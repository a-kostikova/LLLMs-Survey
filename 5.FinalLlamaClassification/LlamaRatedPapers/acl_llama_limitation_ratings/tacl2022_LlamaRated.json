[
    {
        "title": "Word Acquisition in Neural Language Models",
        "authors": [
            "Tyler A. Chang",
            "Benjamin K. Bergen"
        ],
        "published": "2022",
        "summary": "We investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007). Drawing on studies of word acquisition in children, we evaluate multiple predictors for words’ ages of acquisition in LSTMs, BERT, and GPT-2. We find that the effects of concreteness, word length, and lexical class are pointedly different in children and language models, reinforcing the importance of interaction and sensorimotor experience in child language acquisition. Language models rely far more on word frequency than children, but, like children, they exhibit slower learning of words in longer utterances. Interestingly, models follow consistent patterns during training for both unidirectional and bidirectional models, and for both LSTM and Transformer architectures. Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models.",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.1.pdf",
        "keywords": [
            "children",
            "neural language models",
            "language models",
            "language acquisition",
            "word acquisition"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Language models rely far more on word frequency than children, but, like children, they exhibit slower learning of words in longer utterances.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Language models rely far more on word frequency than children, but, like children, they exhibit slower learning of words in longer utterances.\""
    },
    {
        "title": "Predicting Document Coverage for Relation Extraction",
        "authors": [
            "Sneha Singhania",
            "Simon Razniewski",
            "Gerhard Weikum"
        ],
        "published": "2022",
        "summary": "This paper presents a new task of predicting the coverage of a text document for relation extraction (RE): Does the document contain many relational tuples for a given entity? Coverage predictions are useful in selecting the best documents for knowledge base construction with large input corpora. To study this problem, we present a dataset of 31,366 diverse documents for 520 entities. We analyze the correlation of document coverage with features like length, entity mention frequency, Alexa rank, language complexity, and information retrieval scores. Each of these features has only moderate predictive power. We employ methods combining features with statistical models like TF-IDF and language models like BERT. The model combining features and BERT, HERB, achieves an F1 score of up to 46%. We demonstrate the utility of coverage predictions on two use cases: KB construction and claim refutation.",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.12.pdf",
        "keywords": [
            "document coverage",
            "coverage",
            "relation extraction",
            "predicting document coverage",
            "power",
            "language complexity",
            "models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We employ methods combining features with statistical models like TF-IDF and language models like BERT.\"\n\nThis abstract mentions LLMs (BERT) but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"We employ methods combining features with statistical models like TF-IDF and language models like BERT.\"\n\nThis abstract mentions LLMs (BERT) but does not discuss any limitations of LLMs."
    },
    {
        "title": "ABNIRML: Analyzing the Behavior of Neural IR Models",
        "authors": [
            "Sean MacAvaney",
            "Sergey Feldman",
            "Nazli Goharian",
            "Doug Downey",
            "Arman Cohan"
        ],
        "published": "2022",
        "summary": "Pretrained contextualized language models such as BERT and T5 have established a new state-of-the-art for ad-hoc search. However, it is not yet well understood why these methods are so effective, what makes some variants more effective than others, and what pitfalls they may have. We present a new comprehensive framework for Analyzing the Behavior of Neural IR ModeLs (ABNIRML), which includes new types of diagnostic probes that allow us to test several characteristics—such as writing styles, factuality, sensitivity to paraphrasing and word order—that are not addressed by previous techniques. To demonstrate the value of the framework, we conduct an extensive empirical study that yields insights into the factors that contribute to the neural model’s gains, and identify potential unintended biases the models exhibit. Some of our results confirm conventional wisdom, for example, that recent neural ranking models rely less on exact term overlap with the query, and instead leverage richer linguistic information, evidenced by their higher sensitivity to word and sentence order. Other results are more surprising, such as that some models (e.g., T5 and ColBERT) are biased towards factually correct (rather than simply relevant) texts. Further, some characteristics vary even for the same base language model, and other characteristics can appear due to random variations during model training.1",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.13.pdf",
        "keywords": [
            "sensitivity",
            "abnirml",
            "factuality",
            "neural ir"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"what pitfalls they may have\", \"identify potential unintended biases the models exhibit\", \"some models (e.g., T5 and ColBERT) are biased towards factually correct (rather than simply relevant) texts\", \"some characteristics vary even for the same base language model, and other characteristics can appear due to random variations during model training.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"what pitfalls they may have\", \"identify potential unintended biases the models exhibit\", \"some models (e.g., T5 and ColBERT) are biased towards factually correct (rather than simply relevant) texts\", \"some characteristics vary even for the same base language model, and other characteristics can appear due to random variations during model training.\""
    },
    {
        "title": "Time-Aware Language Models as Temporal Knowledge Bases",
        "authors": [
            "Bhuwan Dhingra",
            "Jeremy R. Cole",
            "Julian Martin Eisenschlos",
            "Daniel Gillick",
            "Jacob Eisenstein",
            "William W. Cohen"
        ],
        "published": "2022",
        "summary": "Many facts come with an expiration date, from the name of the President to the basketball team Lebron James plays for. However, most language models (LMs) are trained on snapshots of data collected at a specific moment in time. This can limit their utility, especially in the closed-book setting where the pretraining corpus must contain the facts the model should memorize. We introduce a diagnostic dataset aimed at probing LMs for factual knowledge that changes over time and highlight problems with LMs at either end of the spectrum—those trained on specific slices of temporal data, as well as those trained on a wide range of temporal data. To mitigate these problems, we propose a simple technique for jointly modeling text with its timestamp. This improves memorization of seen facts from the training time period, as well as calibration on predictions about unseen facts from future time periods. We also show that models trained with temporal context can be efficiently “refreshed” as new data arrives, without the need for retraining from scratch.",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.15.pdf",
        "keywords": [
            "temporal data",
            "language models",
            "factual knowledge",
            "calibration"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, most language models (LMs) are trained on snapshots of data collected at a specific moment in time. This can limit their utility, especially in the closed-book setting where the pretraining corpus must contain the facts the model should memorize.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, most language models (LMs) are trained on snapshots of data collected at a specific moment in time. This can limit their utility, especially in the closed-book setting where the pretraining corpus must contain the facts the model should memorize.\""
    },
    {
        "title": "ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models",
        "authors": [
            "Linting Xue",
            "Aditya Barua",
            "Noah Constant",
            "Rami Al-Rfou",
            "Sharan Narang",
            "Mihir Kale",
            "Adam Roberts",
            "Colin Raffel"
        ],
        "published": "2022",
        "summary": "Most widely used pre-trained language models operate on sequences of tokens corresponding to word or subword units. By comparison, token-free models that operate directly on raw text (bytes or characters) have many benefits: They can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Because byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.1",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.17.pdf",
        "keywords": [
            "byte",
            "token sequences",
            "byte models",
            "token free models",
            "process byte sequences",
            "pre trained language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Most widely used pre-trained language models operate on sequences of tokens corresponding to word or subword units... they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Most widely used pre-trained language models operate on sequences of tokens corresponding to word or subword units... they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines.\""
    },
    {
        "title": "Designing an Automatic Agent for Repeated Language–based Persuasion Games",
        "authors": [
            "Maya Raifer",
            "Guy Rotman",
            "Reut Apel",
            "Moshe Tennenholtz",
            "Roi Reichart"
        ],
        "published": "2022",
        "summary": "Persuasion games are fundamental in economics and AI research and serve as the basis for important applications. However, work on this setup assumes communication with stylized messages that do not consist of rich human language. In this paper we consider a repeated sender (expert) – receiver (decision maker) game, where the sender is fully informed about the state of the world and aims to persuade the receiver to accept a deal by sending one of several possible natural language reviews. We design an automatic expert that plays this repeated game, aiming to achieve the maximal payoff. Our expert is implemented within the Monte Carlo Tree Search (MCTS) algorithm, with deep learning models that exploit behavioral and linguistic signals in order to predict the next action of the decision maker, and the future payoff of the expert given the state of the game and a candidate review. We demonstrate the superiority of our expert over strong baselines and its adaptability to different decision makers and potential proposed deals.1",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.18.pdf",
        "keywords": [
            "persuasion games",
            "repeated game",
            "repeated",
            "repeated sender",
            "monte carlo tree"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "VILA: Improving Structured Content Extraction from Scientific PDFs Using Visual Layout Groups",
        "authors": [
            "Zejiang Shen",
            "Kyle Lo",
            "Lucy Lu Wang",
            "Bailey Kuehl",
            "Daniel S. Weld",
            "Doug Downey"
        ],
        "published": "2022",
        "summary": "Accurately extracting structured content from PDFs is a critical first step for NLP over scientific papers. Recent work has improved extraction accuracy by incorporating elementary layout information, for example, each token’s 2D position on the page, into language model pretraining. We introduce new methods that explicitly model VIsual LAyout (VILA) groups, that is, text lines or text blocks, to further improve performance. In our I-VILA approach, we show that simply inserting special tokens denoting layout group boundaries into model inputs can lead to a 1.9% Macro F1 improvement in token classification. In the H-VILA approach, we show that hierarchical encoding of layout-groups can result in up to 47% inference time reduction with less than 0.8% Macro F1 loss. Unlike prior layout-aware approaches, our methods do not require expensive additional pretraining, only fine-tuning, which we show can reduce training cost by up to 95%. Experiments are conducted on a newly curated evaluation suite, S2-VLUE, that unifies existing automatically labeled datasets and includes a new dataset of manual annotations covering diverse papers from 19 scientific disciplines. Pre-trained weights, benchmark datasets, and source code are available at https://github.com/allenai/VILA.",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.22.pdf",
        "keywords": [
            "structured content extraction"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but \"Recent work has improved extraction accuracy by incorporating elementary layout information... into language model pretraining\" implies that LLMs are being discussed and that the paper is building upon previous work, but it does not explicitly mention limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but \"Recent work has improved extraction accuracy by incorporating elementary layout information... into language model pretraining\" implies that LLMs are being discussed and that the paper is building upon previous work, but it does not explicitly mention limitations of LLMs."
    },
    {
        "title": "PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains",
        "authors": [
            "Eyal Ben-David",
            "Nadav Oved",
            "Roi Reichart"
        ],
        "published": "2022",
        "summary": "Natural Language Processing algorithms have made incredible progress, but they still struggle when applied to out-of-distribution examples. We address a challenging and underexplored version of this domain adaptation problem, where an algorithm is trained on several source domains, and then applied to examples from unseen domains that are unknown at training time. Particularly, no examples, labeled or unlabeled, or any other knowledge about the target domain are available to the algorithm at training time. We present PADA: An example-based autoregressive Prompt learning algorithm for on-the-fly Any-Domain Adaptation, based on the T5 language model. Given a test example, PADA first generates a unique prompt for it and then, conditioned on this prompt, labels the example with respect to the NLP prediction task. PADA is trained to generate a prompt that is a token sequence of unrestricted length, consisting of Domain Related Features (DRFs) that characterize each of the source domains. Intuitively, the generated prompt is a unique signature that maps the test example to a semantic space spanned by the source domains. In experiments with 3 tasks (text classification and sequence tagging), for a total of 14 multi-source adaptation scenarios, PADA substantially outperforms strong baselines.1",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.24.pdf",
        "keywords": [
            "adaptation",
            "fly adaptation",
            "domain adaptation",
            "prompt learning",
            "fly any domain adaptation",
            "natural language processing",
            "unseen domains",
            "example"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Natural Language Processing algorithms have made incredible progress, but they still struggle when applied to out-of-distribution examples.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Natural Language Processing algorithms have made incredible progress, but they still struggle when applied to out-of-distribution examples.\""
    },
    {
        "title": "Relational Memory-Augmented Language Models",
        "authors": [
            "Qi Liu",
            "Dani Yogatama",
            "Phil Blunsom"
        ],
        "published": "2022",
        "summary": "We present a memory-augmented approach to condition an autoregressive language model on a knowledge graph. We represent the graph as a collection of relation triples and retrieve relevant relations for a given context to improve text generation. Experiments on WikiText-103, WMT19, and enwik8 English datasets demonstrate that our approach produces a better language model in terms of perplexity and bits per character. We also show that relational memory improves coherence, is complementary to token-based memory, and enables causal interventions. Our model provides a simple yet effective way to combine an autoregressive language model and a knowledge graph for more coherent and logical generation.",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.32.pdf",
        "keywords": [
            "knowledge graph",
            "memory",
            "memory augmented language",
            "language model",
            "coherence"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but the presentation of a new approach to improve text generation implies that existing LLMs may have limitations in this area.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations, but the presentation of a new approach to improve text generation implies that existing LLMs may have limitations in this area."
    },
    {
        "title": "It’s not Rocket Science: Interpreting Figurative Language in Narratives",
        "authors": [
            "Tuhin Chakrabarty",
            "Yejin Choi",
            "Vered Shwartz"
        ],
        "published": "2022",
        "summary": "Figurative language is ubiquitous in English. Yet, the vast majority of NLP research focuses on literal language. Existing text representations by design rely on compositionality, while figurative language is often non- compositional. In this paper, we study the interpretation of two non-compositional figurative languages (idioms and similes). We collected datasets of fictional narratives containing a figurative expression along with crowd-sourced plausible and implausible continuations relying on the correct interpretation of the expression. We then trained models to choose or generate the plausible continuation. Our experiments show that models based solely on pre-trained language models perform substantially worse than humans on these tasks. We additionally propose knowledge-enhanced models, adopting human strategies for interpreting figurative language types: inferring meaning from the context and relying on the constituent words’ literal meanings. The knowledge-enhanced models improve the performance on both the discriminative and generative tasks, further bridging the gap from human performance.",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.34.pdf",
        "keywords": [
            "narratives",
            "figurative language",
            "language",
            "literal language",
            "compositional figurative languages",
            "rocket science",
            "figurative expression",
            "compositionality"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our experiments show that models based solely on pre-trained language models perform substantially worse than humans on these tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our experiments show that models based solely on pre-trained language models perform substantially worse than humans on these tasks.\""
    },
    {
        "title": "Document Summarization with Latent Queries",
        "authors": [
            "Yumo Xu",
            "Mirella Lapata"
        ],
        "published": "2022",
        "summary": "The availability of large-scale datasets has driven the development of neural models that create generic summaries for single or multiple documents. For query-focused summarization (QFS), labeled training data in the form of queries, documents, and summaries is not readily available. We provide a unified modeling framework for any kind of summarization, under the assumption that all summaries are a response to a query, which is observed in the case of QFS and latent in the case of generic summarization. We model queries as discrete latent variables over document tokens, and learn representations compatible with observed and unobserved query verbalizations. Our framework formulates summarization as a generative process, and jointly optimizes a latent query model and a conditional language model. Despite learning from generic summarization data only, our approach outperforms strong comparison systems across benchmarks, query types, document settings, and target domains.1",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.36.pdf",
        "keywords": [
            "summarization",
            "document summarization",
            "document tokens",
            "generic summarization",
            "latent variables"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Is My Model Using the Right Evidence? Systematic Probes for Examining Evidence-Based Tabular Reasoning",
        "authors": [
            "Vivek Gupta",
            "Riyaz A. Bhat",
            "Atreya Ghosal",
            "Manish Shrivastava",
            "Maneesh Singh",
            "Vivek Srikumar"
        ],
        "published": "2022",
        "summary": "Neural models command state-of-the-art performance across NLP tasks, including ones involving “reasoning”. Models claiming to reason about the evidence presented to them should attend to the correct parts of the input while avoiding spurious patterns therein, be self-consistent in their predictions across inputs, and be immune to biases derived from their pre-training in a nuanced, context- sensitive fashion. Do the prevalent *BERT- family of models do so? In this paper, we study this question using the problem of reasoning on tabular data. Tabular inputs are especially well-suited for the study—they admit systematic probes targeting the properties listed above. Our experiments demonstrate that a RoBERTa-based model, representative of the current state-of-the-art, fails at reasoning on the following counts: it (a) ignores relevant parts of the evidence, (b) is over- sensitive to annotation artifacts, and (c) relies on the knowledge encoded in the pre-trained language model rather than the evidence presented in its tabular inputs. Finally, through inoculation experiments, we show that fine- tuning the model on perturbed data does not help it overcome the above challenges.",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.38.pdf",
        "keywords": [
            "systematic probes",
            "tabular reasoning",
            "my model",
            "trained language model"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our experiments demonstrate that a RoBERTa-based model, representative of the current state-of-the-art, fails at reasoning on the following counts: it (a) ignores relevant parts of the evidence, (b) is over- sensitive to annotation artifacts, and (c) relies on the knowledge encoded in the pre-trained language model rather than the evidence presented in its tabular inputs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Our experiments demonstrate that a RoBERTa-based model, representative of the current state-of-the-art, fails at reasoning on the following counts: it (a) ignores relevant parts of the evidence, (b) is over- sensitive to annotation artifacts, and (c) relies on the knowledge encoded in the pre-trained language model rather than the evidence presented in its tabular inputs.\""
    },
    {
        "title": "Uncertainty Estimation and Reduction of Pre-trained Models for Text Regression",
        "authors": [
            "Yuxia Wang",
            "Daniel Beck",
            "Timothy Baldwin",
            "Karin Verspoor"
        ],
        "published": "2022",
        "summary": "State-of-the-art classification and regression models are often not well calibrated, and cannot reliably provide uncertainty estimates, limiting their utility in safety-critical applications such as clinical decision-making. While recent work has focused on calibration of classifiers, there is almost no work in NLP on calibration in a regression setting. In this paper, we quantify the calibration of pre- trained language models for text regression, both intrinsically and extrinsically. We further apply uncertainty estimates to augment training data in low-resource domains. Our experiments on three regression tasks in both self-training and active-learning settings show that uncertainty estimation can be used to increase overall performance and enhance model generalization.",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.39.pdf",
        "keywords": [
            "uncertainty",
            "uncertainty estimation",
            "regression",
            "text regression",
            "self training",
            "model generalization",
            "classification",
            "pre trained language models",
            "active learning",
            "reduction"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"State-of-the-art classification and regression models are often not well calibrated, and cannot reliably provide uncertainty estimates, limiting their utility in safety-critical applications such as clinical decision-making.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"State-of-the-art classification and regression models are often not well calibrated, and cannot reliably provide uncertainty estimates, limiting their utility in safety-critical applications such as clinical decision-making.\""
    },
    {
        "title": "Text-based NP Enrichment",
        "authors": [
            "Yanai Elazar",
            "Victoria Basmov",
            "Yoav Goldberg",
            "Reut Tsarfaty"
        ],
        "published": "2022",
        "summary": "Understanding the relations between entities denoted by NPs in a text is a critical part of human-like natural language understanding. However, only a fraction of such relations is covered by standard NLP tasks and benchmarks nowadays. In this work, we propose a novel task termed text-based NP enrichment (TNE), in which we aim to enrich each NP in a text with all the preposition-mediated relations—either explicit or implicit—that hold between it and other NPs in the text. The relations are represented as triplets, each denoted by two NPs related via a preposition. Humans recover such relations seamlessly, while current state-of-the-art models struggle with them due to the implicit nature of the problem. We build the first large-scale dataset for the problem, provide the formal framing and scope of annotation, analyze the data, and report the results of fine-tuned language models on the task, demonstrating the challenge it poses to current technology. A webpage with a data-exploration UI, a demo, and links to the code, models, and leaderboard, to foster further research into this challenging problem can be found at: yanaiela.github.io/TNE/.",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.44.pdf",
        "keywords": [
            "np enrichment"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"while current state-of-the-art models struggle with them due to the implicit nature of the problem.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"while current state-of-the-art models struggle with them due to the implicit nature of the problem.\""
    },
    {
        "title": "Generate, Annotate, and Learn: NLP with Synthetic Text",
        "authors": [
            "Xuanli He",
            "Islam Nassar",
            "Jamie Kiros",
            "Gholamreza Haffari",
            "Mohammad Norouzi"
        ],
        "published": "2022",
        "summary": "This paper studies the use of language models as a source of synthetic unlabeled text for NLP. We formulate a general framework called “generate, annotate, and learn (GAL)” to take advantage of synthetic text within knowledge distillation, self-training, and few-shot learning applications. To generate high-quality task-specific text, we either fine-tune LMs on inputs from the task of interest, or prompt large LMs with few examples. We use the best available classifier to annotate synthetic text with soft pseudo labels for knowledge distillation and self-training, and use LMs to obtain hard labels for few-shot learning. We train new supervised models on the combination of labeled and pseudo-labeled data, which results in significant gains across several applications. We investigate key components of GAL and present theoretical and empirical arguments against the use of class-conditional LMs to generate synthetic labeled text instead of unlabeled text. GAL achieves new state-of-the-art knowledge distillation results for 6-layer transformers on the GLUE leaderboard.",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.48.pdf",
        "keywords": [
            "knowledge distillation",
            "synthetic text",
            "self training"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but the discussion of \"theoretical and empirical arguments against the use of class-conditional LMs\" implies limitations, however, it is not the main focus of the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None explicitly mentioned, but the discussion of \"theoretical and empirical arguments against the use of class-conditional LMs\" implies limitations, however, it is not the main focus of the abstract."
    },
    {
        "title": "Reducing Conversational Agents’ Overconfidence Through Linguistic Calibration",
        "authors": [
            "Sabrina J. Mielke",
            "Arthur Szlam",
            "Emily Dinan",
            "Y-Lan Boureau"
        ],
        "published": "2022",
        "summary": "While improving neural dialogue agents’ factual accuracy is the object of much research, another important aspect of communication, less studied in the setting of neural dialogue, is transparency about ignorance. In this work, we analyze to what extent state-of-the-art chit-chat models are linguistically calibrated in the sense that their verbalized expression of doubt (or confidence) matches the likelihood that the model’s responses are factually incorrect (or correct). We find that these models are poorly calibrated, yet we show that likelihood of correctness can accurately be predicted. By incorporating such metacognitive features into the training of a controllable generation model, we obtain a dialogue agent with greatly improved linguistic calibration.",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.50.pdf",
        "keywords": [
            "calibrated",
            "linguistic calibration",
            "dialogue agents",
            "conversational agents",
            "likelihood",
            "ignorance"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that these models are poorly calibrated\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We find that these models are poorly calibrated\""
    },
    {
        "title": "A Survey of Text Games for Reinforcement Learning Informed by Natural Language",
        "authors": [
            "Philip Osborne",
            "Heido Nõmm",
            "André Freitas"
        ],
        "published": "2022",
        "summary": "Reinforcement Learning has shown success in a number of complex virtual environments. However, many challenges still exist towards solving problems with natural language as a core component. Interactive Fiction Games (or Text Games) are one such problem type that offer a set of safe, partially observable environments where natural language is required as part of the Reinforcement Learning solution. Therefore, this survey’s aim is to assist in the development of new Text Game problem settings and solutions for Reinforcement Learning informed by natural language. Specifically, this survey: 1) introduces the challenges in Text Game Reinforcement Learning problems, 2) outlines the generation tools for rendering Text Games and the subsequent environments generated, and 3) compares the agent architectures currently applied to provide a systematic review of benchmark methodologies and opportunities for future researchers.",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.51.pdf",
        "keywords": [
            "games",
            "reinforcement learning",
            "text game reinforcement learning",
            "agent architectures",
            "natural language",
            "text games",
            "virtual environments",
            "survey"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Structural Persistence in Language Models: Priming as a Window into Abstract Language Representations",
        "authors": [
            "Arabella Sinclair",
            "Jaap Jumelet",
            "Willem Zuidema",
            "Raquel Fernández"
        ],
        "published": "2022",
        "summary": "We investigate the extent to which modern neural language models are susceptible to structural priming, the phenomenon whereby the structure of a sentence makes the same structure more probable in a follow-up sentence. We explore how priming can be used to study the potential of these models to learn abstract structural information, which is a prerequisite for good performance on tasks that require natural language understanding skills. We introduce a novel metric and release Prime-LM, a large corpus where we control for various linguistic factors that interact with priming strength. We find that Transformer models indeed show evidence of structural priming, but also that the generalizations they learned are to some extent modulated by semantic information. Our experiments also show that the representations acquired by the models may not only encode abstract sequential structure but involve certain level of hierarchical syntactic information. More generally, our study shows that the priming paradigm is a useful, additional tool for gaining insights into the capacities of language models and opens the door to future priming-based investigations that probe the model’s internal states.1",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.60.pdf",
        "keywords": [
            "language",
            "language models",
            "neural language models",
            "structural persistence"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"but also that the generalizations they learned are to some extent modulated by semantic information.\"\n\nThis rating is given because the paper mentions a limitation of the models (i.e., generalizations are modulated by semantic information) but it is not the primary focus of the paper. The main focus is on introducing a novel metric and studying the potential of the models to learn abstract structural information",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"but also that the generalizations they learned are to some extent modulated by semantic information.\"\n\nThis rating is given because the paper mentions a limitation of the models (i.e., generalizations are modulated by semantic information) but it is not the primary focus of the paper. The main focus is on introducing a novel metric and studying the potential of the models to learn abstract structural information"
    },
    {
        "title": "DP-Parse: Finding Word Boundaries from Raw Speech with an Instance Lexicon",
        "authors": [
            "Robin Algayres",
            "Tristan Ricoul",
            "Julien Karadayi",
            "Hugo Laurençon",
            "Salah Zaiem",
            "Abdelrahman Mohamed",
            "Benoît Sagot",
            "Emmanuel Dupoux"
        ],
        "published": "2022",
        "summary": "Finding word boundaries in continuous speech is challenging as there is little or no equivalent of a ‘space’ delimiter between words. Popular Bayesian non-parametric models for text segmentation (Goldwater et al., 2006, 2009) use a Dirichlet process to jointly segment sentences and build a lexicon of word types. We introduce DP-Parse, which uses similar principles but only relies on an instance lexicon of word tokens, avoiding the clustering errors that arise with a lexicon of word types. On the Zero Resource Speech Benchmark 2017, our model sets a new speech segmentation state-of-the-art in 5 languages. The algorithm monotonically improves with better input representations, achieving yet higher scores when fed with weakly supervised inputs. Despite lacking a type lexicon, DP-Parse can be pipelined to a language model and learn semantic and syntactic representations as assessed by a new spoken word embedding benchmark. 1",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.61.pdf",
        "keywords": [
            "speech segmentation",
            "text segmentation",
            "dirichlet process",
            "word boundaries",
            "dp parse"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Evaluating Attribution in Dialogue Systems: The BEGIN Benchmark",
        "authors": [
            "Nouha Dziri",
            "Hannah Rashkin",
            "Tal Linzen",
            "David Reitter"
        ],
        "published": "2022",
        "summary": "Knowledge-grounded dialogue systems powered by large language models often generate responses that, while fluent, are not attributable to a relevant source of information. Progress towards models that do not exhibit this issue requires evaluation metrics that can quantify its prevalence. To this end, we introduce the Benchmark for Evaluation of Grounded INteraction (Begin), comprising 12k dialogue turns generated by neural dialogue systems trained on three knowledge-grounded dialogue corpora. We collect human annotations assessing the extent to which the models’ responses can be attributed to the given background information. We then use Begin to analyze eight evaluation metrics. We find that these metrics rely on spurious correlations, do not reliably distinguish attributable abstractive responses from unattributable ones, and perform substantially worse when the knowledge source is longer. Our findings underscore the need for more sophisticated and robust evaluation metrics for knowledge-grounded dialogue. We make Begin publicly available at https://github.com/google/BEGIN-dataset.",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.62.pdf",
        "keywords": [
            "knowledge",
            "prevalence",
            "dialogue",
            "benchmark",
            "evaluation",
            "dialogue systems",
            "evaluation metrics",
            "language",
            "begin benchmark",
            "attribution"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Knowledge-grounded dialogue systems powered by large language models often generate responses that, while fluent, are not attributable to a relevant source of information.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Knowledge-grounded dialogue systems powered by large language models often generate responses that, while fluent, are not attributable to a relevant source of information.\""
    },
    {
        "title": "Getting BART to Ride the Idiomatic Train: Learning to Represent Idiomatic Expressions",
        "authors": [
            "Ziheng Zeng",
            "Suma Bhat"
        ],
        "published": "2022",
        "summary": "Idiomatic expressions (IEs), characterized by their non-compositionality, are an important part of natural language. They have been a classical challenge to NLP, including pre-trained language models that drive today’s state-of-the-art. Prior work has identified deficiencies in their contextualized representation stemming from the underlying compositional paradigm of representation. In this work, we take a first-principles approach to build idiomaticity into BART using an adapter as a lightweight non-compositional language expert trained on idiomatic sentences. The improved capability over baselines (e.g., BART) is seen via intrinsic and extrinsic methods, where idiom embeddings score 0.19 points higher in homogeneity score for embedding clustering, and up to 25% higher sequence accuracy on the idiom processing tasks of IE sense disambiguation and span detection.",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.65.pdf",
        "keywords": [
            "idiomatic expressions",
            "expressions",
            "idiomaticity",
            "adapter",
            "language",
            "sequence accuracy",
            "natural language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Prior work has identified deficiencies in their contextualized representation stemming from the underlying compositional paradigm of representation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Prior work has identified deficiencies in their contextualized representation stemming from the underlying compositional paradigm of representation.\""
    },
    {
        "title": "Meta-Learning the Difference: Preparing Large Language Models for Efficient Adaptation",
        "authors": [
            "Zejiang Hou",
            "Julian Salazar",
            "George Polovets"
        ],
        "published": "2022",
        "summary": "Large pretrained language models (PLMs) are often domain- or task-adapted via finetuning or prompting. Finetuning requires modifying all of the parameters and having enough data to avoid overfitting while prompting requires no training and few examples but limits performance. Instead, we prepare PLMs for data- and parameter-efficient adaptation by learning to learn the difference between general and adapted PLMs. This difference is expressed in terms of model weights and sublayer structure through our proposed dynamic low-rank reparameterization and learned architecture controller. Experiments on few-shot dialogue completion, low-resource abstractive summarization, and multi-domain language modeling show improvements in adaptation time and performance over direct finetuning or preparation via domain-adaptive pretraining. Ablations show our task-adaptive reparameterization (TARP) and model search (TAMS) components individually improve on other parameter-efficient transfer like adapters and structure-learning methods like learned sparsification.",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.72.pdf",
        "keywords": [
            "finetuning",
            "learned sparsification",
            "summarization",
            "meta learning",
            "language models",
            "large language",
            "adaptation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Finetuning requires modifying all of the parameters and having enough data to avoid overfitting while prompting requires no training and few examples but limits performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Finetuning requires modifying all of the parameters and having enough data to avoid overfitting while prompting requires no training and few examples but limits performance.\""
    },
    {
        "title": "Compositional Evaluation on Japanese Textual Entailment and Similarity",
        "authors": [
            "Hitomi Yanaka",
            "Koji Mineshima"
        ],
        "published": "2022",
        "summary": "Natural Language Inference (NLI) and Semantic Textual Similarity (STS) are widely used benchmark tasks for compositional evaluation of pre-trained language models. Despite growing interest in linguistic universals, most NLI/STS studies have focused almost exclusively on English. In particular, there are no available multilingual NLI/STS datasets in Japanese, which is typologically different from English and can shed light on the currently controversial behavior of language models in matters such as sensitivity to word order and case particles. Against this background, we introduce JSICK, a Japanese NLI/STS dataset that was manually translated from the English dataset SICK. We also present a stress-test dataset for compositional inference, created by transforming syntactic structures of sentences in JSICK to investigate whether language models are sensitive to word order and case particles. We conduct baseline experiments on different pre-trained language models and compare the performance of multilingual models when applied to Japanese and other languages. The results of the stress-test experiments suggest that the current pre-trained language models are insensitive to word order and case marking.",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.73.pdf",
        "keywords": [
            "textual entailment",
            "compositional inference",
            "natural language inference",
            "textual similarity",
            "language models",
            "evaluation"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The results of the stress-test experiments suggest that the current pre-trained language models are insensitive to word order and case marking.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"The results of the stress-test experiments suggest that the current pre-trained language models are insensitive to word order and case marking.\""
    },
    {
        "title": "An End-to-End Contrastive Self-Supervised Learning Framework for Language Understanding",
        "authors": [
            "Hongchao Fang",
            "Pengtao Xie"
        ],
        "published": "2022",
        "summary": "Self-supervised learning (SSL) methods such as Word2vec, BERT, and GPT have shown great effectiveness in language understanding. Contrastive learning, as a recent SSL approach, has attracted increasing attention in NLP. Contrastive learning learns data representations by predicting whether two augmented data instances are generated from the same original data example. Previous contrastive learning methods perform data augmentation and contrastive learning separately. As a result, the augmented data may not be optimal for contrastive learning. To address this problem, we propose a four-level optimization framework that performs data augmentation and contrastive learning end-to-end, to enable the augmented data to be tailored to the contrastive learning task. This framework consists of four learning stages, including training machine translation models for sentence augmentation, pretraining a text encoder using contrastive learning, finetuning a text classification model, and updating weights of translation data by minimizing the validation loss of the classification model, which are performed in a unified way. Experiments on datasets in the GLUE benchmark (Wang et al., 2018a) and on datasets used in Gururangan et al. (2020) demonstrate the effectiveness of our method.",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.76.pdf",
        "keywords": [
            "self supervised",
            "self supervised learning",
            "sentence augmentation",
            "language",
            "machine translation"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Transformer Grammars: Augmenting Transformer Language Models with Syntactic Inductive Biases at Scale",
        "authors": [
            "Laurent Sartran",
            "Samuel Barrett",
            "Adhiguna Kuncoro",
            "Miloš Stanojević",
            "Phil Blunsom",
            "Chris Dyer"
        ],
        "published": "2022",
        "summary": "We introduce Transformer Grammars (TGs), a novel class of Transformer language models that combine (i) the expressive power, scalability, and strong performance of Transformers and (ii) recursive syntactic compositions, which here are implemented through a special attention mask and deterministic transformation of the linearized tree. We find that TGs outperform various strong baselines on sentence-level language modeling perplexity, as well as on multiple syntax-sensitive language modeling evaluation metrics. Additionally, we find that the recursive syntactic composition bottleneck which represents each sentence as a single vector harms perplexity on document-level language modeling, providing evidence that a different kind of memory mechanism—one that is independent of composed syntactic representations—plays an important role in current successful models of long text.",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.81.pdf",
        "keywords": [
            "transformer language",
            "transformer grammars",
            "language modeling",
            "recursive syntactic composition",
            "inductive biases",
            "scale",
            "baselines"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the recursive syntactic composition bottleneck which represents each sentence as a single vector harms perplexity on document-level language modeling\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the recursive syntactic composition bottleneck which represents each sentence as a single vector harms perplexity on document-level language modeling\""
    },
    {
        "title": "Morphology Without Borders: Clause-Level Morphology",
        "authors": [
            "Omer Goldman",
            "Reut Tsarfaty"
        ],
        "published": "2022",
        "summary": "Morphological tasks use large multi-lingual datasets that organize words into inflection tables, which then serve as training and evaluation data for various tasks. However, a closer inspection of these data reveals profound cross-linguistic inconsistencies, which arise from the lack of a clear linguistic and operational definition of what is a word, and which severely impair the universality of the derived tasks. To overcome this deficiency, we propose to view morphology as a clause-level phenomenon, rather than word-level. It is anchored in a fixed yet inclusive set of features, that encapsulates all functions realized in a saturated clause. We deliver MightyMorph, a novel dataset for clause-level morphology covering 4 typologically different languages: English, German, Turkish, and Hebrew. We use this dataset to derive 3 clause-level morphological tasks: inflection, reinflection and analysis. Our experiments show that the clause-level tasks are substantially harder than the respective word-level tasks, while having comparable complexity across languages. Furthermore, redefining morphology to the clause-level provides a neat interface with contextualized language models (LMs) and allows assessing the morphological knowledge encoded in these models and their usability for morphological tasks. Taken together, this work opens up new horizons in the study of computational morphology, leaving ample space for studying neural morphology cross-linguistically.",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.83.pdf",
        "keywords": [
            "morphology",
            "clause level morphology",
            "neural morphology",
            "computational morphology",
            "hebrew",
            "borders",
            "complexity",
            "contextualized language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"redefining morphology to the clause-level provides a neat interface with contextualized language models (LMs) and allows assessing the morphological knowledge encoded in these models and their usability for morphological tasks.\"\n\nThis rating is given because the paper mentions LLMs and their limitations in the context of morphological tasks, but only briefly and as a minor detail. The primary focus of the",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"redefining morphology to the clause-level provides a neat interface with contextualized language models (LMs) and allows assessing the morphological knowledge encoded in these models and their usability for morphological tasks.\"\n\nThis rating is given because the paper mentions LLMs and their limitations in the context of morphological tasks, but only briefly and as a minor detail. The primary focus of the"
    },
    {
        "title": "FaithDial: A Faithful Benchmark for Information-Seeking Dialogue",
        "authors": [
            "Nouha Dziri",
            "Ehsan Kamalloo",
            "Sivan Milton",
            "Osmar Zaiane",
            "Mo Yu",
            "Edoardo M. Ponti",
            "Siva Reddy"
        ],
        "published": "2022",
        "summary": "The goal of information-seeking dialogue is to respond to seeker queries with natural language utterances that are grounded on knowledge sources. However, dialogue systems often produce unsupported utterances, a phenomenon known as hallucination. To mitigate this behavior, we adopt a data-centric solution and create FaithDial, a new benchmark for hallucination-free dialogues, by editing hallucinated responses in the Wizard of Wikipedia (WoW) benchmark. We observe that FaithDial is more faithful than WoW while also maintaining engaging conversations. We show that FaithDial can serve as training signal for: i) a hallucination critic, which discriminates whether an utterance is faithful or not, and boosts the performance by 12.8 F1 score on the BEGIN benchmark compared to existing datasets for dialogue coherence; ii) high-quality dialogue generation. We benchmark a series of state-of-the-art models and propose an auxiliary contrastive objective that achieves the highest level of faithfulness and abstractiveness based on several automated metrics. Further, we find that the benefits of FaithDial generalize to zero-shot transfer on other datasets, such as CMU-Dog and TopicalChat. Finally, human evaluation reveals that responses generated by models trained on FaithDial are perceived as more interpretable, cooperative, and engaging.",
        "pdf_link": "https://aclanthology.org/2022.tacl-1.84.pdf",
        "keywords": [
            "hallucination",
            "benchmark",
            "dialogue",
            "dialogue coherence",
            "hallucination free dialogues",
            "information",
            "interpretable",
            "faithfulness",
            "wikipedia"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, dialogue systems often produce unsupported utterances, a phenomenon known as hallucination.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, dialogue systems often produce unsupported utterances, a phenomenon known as hallucination.\""
    }
]