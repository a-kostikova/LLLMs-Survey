[
    {
        "title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering",
        "authors": [
            "Shamane Siriwardhana",
            "Rivindu Weerasekera",
            "Elliott Wen",
            "Tharindu Kaluarachchi",
            "Rajib Rana",
            "Suranga Nanayakkara"
        ],
        "published": "2023",
        "summary": "Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain Question Answering (ODQA). RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains such as healthcare and news. In this paper, we evaluate the impact of joint training of the retriever and generator components of RAG for the task of domain adaptation in ODQA. We propose RAG-end2end, an extension to RAG that can adapt to a domain-specific knowledge base by updating all components of the external knowledge base during training. In addition, we introduce an auxiliary training signal to inject more domain-specific knowledge. This auxiliary signal forces RAG-end2end to reconstruct a given sentence by accessing the relevant information from the external knowledge base. Our novel contribution is that, unlike RAG, RAG-end2end does joint training of the retriever and generator for the end QA task and domain adaptation. We evaluate our approach with datasets from three domains: COVID-19, News, and Conversations, and achieve significant performance improvements compared to the original RAG model. Our work has been open-sourced through the HuggingFace Transformers library, attesting to our work’s credibility and technical consistency.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.1.pdf",
        "keywords": [
            "domain adaptation",
            "open domain question answering",
            "retrieval augmented generation"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "OPAL: Ontology-Aware Pretrained Language Model for End-to-End Task-Oriented Dialogue",
        "authors": [
            "Zhi Chen",
            "Yuncong Liu",
            "Lu Chen",
            "Su Zhu",
            "Mengyue Wu",
            "Kai Yu"
        ],
        "published": "2023",
        "summary": "This paper presents an ontology-aware pretrained language model (OPAL) for end-to-end task-oriented dialogue (TOD). Unlike chit-chat dialogue models, task-oriented dialogue models fulfill at least two task-specific modules: Dialogue state tracker (DST) and response generator (RG). The dialogue state consists of the domain-slot-value triples, which are regarded as the user’s constraints to search the domain-related databases. The large-scale task-oriented dialogue data with the annotated structured dialogue state usually are inaccessible. It prevents the development of the pretrained language model for the task-oriented dialogue. We propose a simple yet effective pretraining method to alleviate this problem, which consists of two pretraining phases. The first phase is to pretrain on large-scale contextual text data, where the structured information of the text is extracted by the information extracting tool. To bridge the gap between the pretraining method and downstream tasks, we design two pretraining tasks: ontology-like triple recovery and next-text generation, which simulates the DST and RG, respectively. The second phase is to fine-tune the pretrained model on the TOD data. The experimental results show that our proposed method achieves an exciting boost and obtains competitive performance even without any TOD data on CamRest676 and MultiWOZ benchmarks.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.5.pdf",
        "keywords": [
            "ontology",
            "dialogue",
            "language model",
            "dialogue state"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The large-scale task-oriented dialogue data with the annotated structured dialogue state usually are inaccessible. It prevents the development of the pretrained language model for the task-oriented dialogue.\"\n\nThis rating is given because the paper mentions a limitation of LLMs (inaccessibility of large-scale task-oriented dialogue data) but does not explore it in depth. The primary focus of the paper is on the proposed",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The large-scale task-oriented dialogue data with the annotated structured dialogue state usually are inaccessible. It prevents the development of the pretrained language model for the task-oriented dialogue.\"\n\nThis rating is given because the paper mentions a limitation of LLMs (inaccessibility of large-scale task-oriented dialogue data) but does not explore it in depth. The primary focus of the paper is on the proposed"
    },
    {
        "title": "Coreference Resolution through a seq2seq Transition-Based System",
        "authors": [
            "Bernd Bohnet",
            "Chris Alberti",
            "Michael Collins"
        ],
        "published": "2023",
        "summary": "Most recent coreference resolution systems use search algorithms over possible spans to identify mentions and resolve coreference. We instead present a coreference resolution system that uses a text-to-text (seq2seq) paradigm to predict mentions and links jointly. We implement the coreference system as a transition system and use multilingual T5 as an underlying language model. We obtain state-of-the-art accuracy on the CoNLL-2012 datasets with 83.3 F1-score for English (a 2.3 higher F1-score than previous work [Dobrovolskii, 2021]) using only CoNLL data for training, 68.5 F1-score for Arabic (+4.1 higher than previous work), and 74.3 F1-score for Chinese (+5.3). In addition we use the SemEval-2010 data sets for experiments in the zero-shot setting, a few-shot setting, and supervised setting using all available training data. We obtain substantially higher zero-shot F1-scores for 3 out of 4 languages than previous approaches and significantly exceed previous supervised state-of-the-art results for all five tested languages. We provide the code and models as open source.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.13.pdf",
        "keywords": [
            "coreference resolution",
            "coreference",
            "conll data",
            "search algorithms"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We implement the coreference system as a transition system and use multilingual T5 as an underlying language model.\"\n\nThis rating is given because the paper mentions the use of a large language model (multilingual T5) but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"We implement the coreference system as a transition system and use multilingual T5 as an underlying language model.\"\n\nThis rating is given because the paper mentions the use of a large language model (multilingual T5) but does not discuss any limitations of LLMs."
    },
    {
        "title": "Transformers for Tabular Data Representation: A Survey of Models and Applications",
        "authors": [
            "Gilbert Badaro",
            "Mohammed Saeed",
            "Paolo Papotti"
        ],
        "published": "2023",
        "summary": "In the last few years, the natural language processing community has witnessed advances in neural representations of free texts with transformer-based language models (LMs). Given the importance of knowledge available in tabular data, recent research efforts extend LMs by developing neural representations for structured data. In this article, we present a survey that analyzes these efforts. We first abstract the different systems according to a traditional machine learning pipeline in terms of training data, input representation, model training, and supported downstream tasks. For each aspect, we characterize and compare the proposed solutions. Finally, we discuss future work directions.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.14.pdf",
        "keywords": [
            "tabular data representation",
            "transformers",
            "survey",
            "language models",
            "natural language processing"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation is mentioned, but the title and the paper imply that the existing LMs are limited to free text and the paper discusses the extension of LMs to structured data.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation is mentioned, but the title and the paper imply that the existing LMs are limited to free text and the paper discusses the extension of LMs to structured data."
    },
    {
        "title": "Efficient Long-Text Understanding with Short-Text Models",
        "authors": [
            "Maor Ivgi",
            "Uri Shaham",
            "Jonathan Berant"
        ],
        "published": "2023",
        "summary": "Transformer-based pretrained language models (LMs) are ubiquitous across natural language understanding, but cannot be applied to long sequences such as stories, scientific articles, and long documents due to their quadratic complexity. While a myriad of efficient transformer variants have been proposed, they are typically based on custom implementations that require expensive pretraining from scratch. In this work, we propose SLED: SLiding-Encoder and Decoder, a simple approach for processing long sequences that re-uses and leverages battle-tested short-text pretrained LMs. Specifically, we partition the input into overlapping chunks, encode each with a short-text LM encoder and use the pretrained decoder to fuse information across chunks (fusion-in-decoder). We illustrate through controlled experiments that SLED offers a viable strategy for long text understanding and evaluate our approach on SCROLLS, a benchmark with seven datasets across a wide range of language understanding tasks. We find that SLED is competitive with specialized models that are up to 50x larger and require a dedicated and expensive pretraining step.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.17.pdf",
        "keywords": [
            "language models",
            "text models",
            "text pretrained lms"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Transformer-based pretrained language models (LMs)... cannot be applied to long sequences such as stories, scientific articles, and long documents due to their quadratic complexity.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Transformer-based pretrained language models (LMs)... cannot be applied to long sequences such as stories, scientific articles, and long documents due to their quadratic complexity.\""
    },
    {
        "title": "Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?",
        "authors": [
            "Byung-Doh Oh",
            "William Schuler"
        ],
        "published": "2023",
        "summary": "This work presents a linguistic analysis into why larger Transformer-based pre-trained language models with more parameters and lower perplexity nonetheless yield surprisal estimates that are less predictive of human reading times. First, regression analyses show a strictly monotonic, positive log-linear relationship between perplexity and fit to reading times for the more recently released five GPT-Neo variants and eight OPT variants on two separate datasets, replicating earlier results limited to just GPT-2 (Oh et al., 2022). Subsequently, analysis of residual errors reveals a systematic deviation of the larger variants, such as underpredicting reading times of named entities and making compensatory overpredictions for reading times of function words such as modals and conjunctions. These results suggest that the propensity of larger Transformer-based models to ‘memorize’ sequences during training makes their surprisal estimates diverge from humanlike expectations, which warrants caution in using pre-trained language models to study human language processing.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.20.pdf",
        "keywords": [
            "surprisal estimates",
            "residual errors",
            "language models",
            "human language",
            "transformer based language models",
            "human reading",
            "transformer based models",
            "human reading times",
            "poorer fit",
            "perplexity"
        ],
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the propensity of larger Transformer-based models to ‘memorize’ sequences during training makes their surprisal estimates diverge from humanlike expectations, which warrants caution in using pre-trained language models to study human language processing.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"the propensity of larger Transformer-based models to ‘memorize’ sequences during training makes their surprisal estimates diverge from humanlike expectations, which warrants caution in using pre-trained language models to study human language processing.\""
    },
    {
        "title": "Naturalistic Causal Probing for Morpho-Syntax",
        "authors": [
            "Afra Amini",
            "Tiago Pimentel",
            "Clara Meister",
            "Ryan Cotterell"
        ],
        "published": "2023",
        "summary": "Probing has become a go-to methodology for interpreting and analyzing deep neural models in natural language processing. However, there is still a lack of understanding of the limitations and weaknesses of various types of probes. In this work, we suggest a strategy for input-level intervention on naturalistic sentences. Using our approach, we intervene on the morpho-syntactic features of a sentence, while keeping the rest of the sentence unchanged. Such an intervention allows us to causally probe pre-trained models. We apply our naturalistic causal probing framework to analyze the effects of grammatical gender and number on contextualized representations extracted from three pre-trained models in Spanish, the multilingual versions of BERT, RoBERTa, and GPT-2. Our experiments suggest that naturalistic interventions lead to stable estimates of the causal effects of various linguistic properties. Moreover, our experiments demonstrate the importance of naturalistic causal probing when analyzing pre-trained models. https://github.com/rycolab/naturalistic-causal-probing",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.23.pdf",
        "keywords": [
            "probing",
            "morpho",
            "morpho syntax",
            "naturalistic causal probing",
            "naturalistic",
            "contextualized"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, there is still a lack of understanding of the limitations and weaknesses of various types of probes.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, there is still a lack of understanding of the limitations and weaknesses of various types of probes.\""
    },
    {
        "title": "Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval",
        "authors": [
            "Sheng-Chieh Lin",
            "Minghan Li",
            "Jimmy Lin"
        ],
        "published": "2023",
        "summary": "Pre-trained language models have been successful in many knowledge-intensive NLP tasks. However, recent work has shown that models such as BERT are not “structurally ready” to aggregate textual information into a [CLS] vector for dense passage retrieval (DPR). This “lack of readiness” results from the gap between language model pre-training and DPR fine-tuning. Previous solutions call for computationally expensive techniques such as hard negative mining, cross-encoder distillation, and further pre-training to learn a robust DPR model. In this work, we instead propose to fully exploit knowledge in a pre-trained language model for DPR by aggregating the contextualized token embeddings into a dense vector, which we call agg★. By concatenating vectors from the [CLS] token and agg★, our Aggretriever model substantially improves the effectiveness of dense retrieval models on both in-domain and zero-shot evaluations without introducing substantial training overhead. Code is available at https://github.com/castorini/dhr.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.26.pdf",
        "keywords": [
            "dense passage retrieval",
            "dense retrieval",
            "aggretriever",
            "pre trained language models",
            "language model",
            "cross",
            "robust",
            "distillation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, recent work has shown that models such as BERT are not “structurally ready” to aggregate textual information into a [CLS] vector for dense passage retrieval (DPR).\"\n\nThis evidence suggests that the paper mentions a limitation of pre-trained language models in the context of dense passage retrieval, but it is not a major focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, recent work has shown that models such as BERT are not “structurally ready” to aggregate textual information into a [CLS] vector for dense passage retrieval (DPR).\"\n\nThis evidence suggests that the paper mentions a limitation of pre-trained language models in the context of dense passage retrieval, but it is not a major focus of the paper."
    },
    {
        "title": "InSCIt: Information-Seeking Conversations with Mixed-Initiative Interactions",
        "authors": [
            "Zeqiu Wu",
            "Ryu Parish",
            "Hao Cheng",
            "Sewon Min",
            "Prithviraj Ammanabrolu",
            "Mari Ostendorf",
            "Hannaneh Hajishirzi"
        ],
        "published": "2023",
        "summary": "In an information-seeking conversation, a user may ask questions that are under-specified or unanswerable. An ideal agent would interact by initiating different response types according to the available knowledge sources. However, most current studies either fail to or artificially incorporate such agent-side initiative. This work presents InSCIt, a dataset for Information-Seeking Conversations with mixed-initiative Interactions. It contains 4.7K user-agent turns from 805 human-human conversations where the agent searches over Wikipedia and either directly answers, asks for clarification, or provides relevant information to address user queries. The data supports two subtasks, evidence passage identification and response generation, as well as a human evaluation protocol to assess model performance. We report results of two systems based on state-of-the-art models of conversational knowledge identification and open-domain question answering. Both systems significantly underperform humans, suggesting ample room for improvement in future studies.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.27.pdf",
        "keywords": [
            "mixed initiative interactions",
            "conversation",
            "information seeking conversations",
            "agent"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Both systems significantly underperform humans, suggesting ample room for improvement in future studies.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Both systems significantly underperform humans, suggesting ample room for improvement in future studies.\""
    },
    {
        "title": "Sub-Character Tokenization for Chinese Pretrained Language Models",
        "authors": [
            "Chenglei Si",
            "Zhengyan Zhang",
            "Yingfa Chen",
            "Fanchao Qi",
            "Xiaozhi Wang",
            "Zhiyuan Liu",
            "Yasheng Wang",
            "Qun Liu",
            "Maosong Sun"
        ],
        "published": "2023",
        "summary": "Tokenization is fundamental to pretrained language models (PLMs). Existing tokenization methods for Chinese PLMs typically treat each character as an indivisible token. However, they ignore the unique feature of the Chinese writing system where additional linguistic information exists below the character level, i.e., at the sub-character level. To utilize such information, we propose sub-character (SubChar for short) tokenization. Specifically, we first encode the input text by converting each Chinese character into a short sequence based on its glyph or pronunciation, and then construct the vocabulary based on the encoded text with sub-word segmentation. Experimental results show that SubChar tokenizers have two main advantages over existing tokenizers: 1) They can tokenize inputs into much shorter sequences, thus improving the computational efficiency. 2) Pronunciation-based SubChar tokenizers can encode Chinese homophones into the same transliteration sequences and produce the same tokenization output, hence being robust to homophone typos. At the same time, models trained with SubChar tokenizers perform competitively on downstream tasks. We release our code and models at https://github.com/thunlp/SubCharTokenization to facilitate future work.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.28.pdf",
        "keywords": [
            "tokenization",
            "sub character tokenization",
            "pretrained language models",
            "chinese pretrained language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing tokenization methods for Chinese PLMs typically treat each character as an indivisible token. However, they ignore the unique feature of the Chinese writing system where additional linguistic information exists below the character level, i.e., at the sub-character level.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of existing tokenization methods for Chinese PLMs, but it is not",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing tokenization methods for Chinese PLMs typically treat each character as an indivisible token. However, they ignore the unique feature of the Chinese writing system where additional linguistic information exists below the character level, i.e., at the sub-character level.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of existing tokenization methods for Chinese PLMs, but it is not"
    },
    {
        "title": "Understanding and Detecting Hallucinations in Neural Machine Translation via Model Introspection",
        "authors": [
            "Weijia Xu",
            "Sweta Agrawal",
            "Eleftheria Briakou",
            "Marianna J. Martindale",
            "Marine Carpuat"
        ],
        "published": "2023",
        "summary": "Neural sequence generation models are known to “hallucinate”, by producing outputs that are unrelated to the source text. These hallucinations are potentially harmful, yet it remains unclear in what conditions they arise and how to mitigate their impact. In this work, we first identify internal model symptoms of hallucinations by analyzing the relative token contributions to the generation in contrastive hallucinated vs. non-hallucinated outputs generated via source perturbations. We then show that these symptoms are reliable indicators of natural hallucinations, by using them to design a lightweight hallucination detector which outperforms both model-free baselines and strong classifiers based on quality estimation or large pre-trained models on manually annotated English-Chinese and German-English translation test beds.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.32.pdf",
        "keywords": [
            "hallucination",
            "model introspection",
            "neural machine translation"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.\n\nNote: Although the paper discusses hallucinations in neural machine translation, which is related to LLMs, it does not explicitly mention Large Language Models (LLMs) or their limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs.\n\nNote: Although the paper discusses hallucinations in neural machine translation, which is related to LLMs, it does not explicitly mention Large Language Models (LLMs) or their limitations."
    },
    {
        "title": "Questions Are All You Need to Train a Dense Passage Retriever",
        "authors": [
            "Devendra Singh Sachan",
            "Mike Lewis",
            "Dani Yogatama",
            "Luke Zettlemoyer",
            "Joelle Pineau",
            "Manzil Zaheer"
        ],
        "published": "2023",
        "summary": "We introduce ART, a new corpus-level autoencoding approach for training dense retrieval models that does not require any labeled training data. Dense retrieval is a central challenge for open-domain tasks, such as Open QA, where state-of-the-art methods typically require large supervised datasets with custom hard-negative mining and denoising of positive examples. ART, in contrast, only requires access to unpaired inputs and outputs (e.g., questions and potential answer passages). It uses a new passage-retrieval autoencoding scheme, where (1) an input question is used to retrieve a set of evidence passages, and (2) the passages are then used to compute the probability of reconstructing the original question. Training for retrieval based on question reconstruction enables effective unsupervised learning of both passage and question encoders, which can be later incorporated into complete Open QA systems without any further finetuning. Extensive experiments demonstrate that ART obtains state-of-the-art results on multiple QA retrieval benchmarks with only generic initialization from a pre-trained language model, removing the need for labeled data and task-specific losses.1 Our code and model checkpoints are available at: https://github.com/DevSinghSachan/art.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.35.pdf",
        "keywords": [
            "dense retrieval",
            "qa retrieval"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Transparency Helps Reveal When Language Models Learn Meaning",
        "authors": [
            "Zhaofeng Wu",
            "William Merrill",
            "Hao Peng",
            "Iz Beltagy",
            "Noah A. Smith"
        ],
        "published": "2023",
        "summary": "Many current NLP systems are built from language models trained to optimize unsupervised objectives on large amounts of raw text. Under what conditions might such a procedure acquire meaning? Our systematic experiments with synthetic data reveal that, with languages where all expressions have context-independent denotations (i.e., languages with strong transparency), both autoregressive and masked language models successfully learn to emulate semantic relations between expressions. However, when denotations are changed to be context-dependent with the language otherwise unmodified, this ability degrades. Turning to natural language, our experiments with a specific phenomenon—referential opacity—add to the growing body of evidence that current language models do not represent natural language semantics well. We show this failure relates to the context-dependent nature of natural language form-meaning mappings.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.36.pdf",
        "keywords": [
            "languages",
            "natural language semantics",
            "natural language",
            "meaning mappings",
            "language models",
            "context dependent",
            "transparency",
            "strong transparency"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, when denotations are changed to be context-dependent with the language otherwise unmodified, this ability degrades. Turning to natural language, our experiments with a specific phenomenon—referential opacity—add to the growing body of evidence that current language models do not represent natural language semantics well.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, when denotations are changed to be context-dependent with the language otherwise unmodified, this ability degrades. Turning to natural language, our experiments with a specific phenomenon—referential opacity—add to the growing body of evidence that current language models do not represent natural language semantics well.\""
    },
    {
        "title": "Visual Spatial Reasoning",
        "authors": [
            "Fangyu Liu",
            "Guy Emerson",
            "Nigel Collier"
        ],
        "published": "2023",
        "summary": "Spatial relations are a basic part of human cognition. However, they are expressed in natural language in a variety of ways, and previous work has suggested that current vision-and-language models (VLMs) struggle to capture relational information. In this paper, we present Visual Spatial Reasoning (VSR), a dataset containing more than 10k natural text-image pairs with 66 types of spatial relations in English (e.g., under, in front of, facing). While using a seemingly simple annotation format, we show how the dataset includes challenging linguistic phenomena, such as varying reference frames. We demonstrate a large gap between human and model performance: The human ceiling is above 95%, while state-of-the-art models only achieve around 70%. We observe that VLMs’ by-relation performances have little correlation with the number of training examples and the tested models are in general incapable of recognising relations concerning the orientations of objects.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.37.pdf",
        "keywords": [
            "visual spatial reasoning"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"previous work has suggested that current vision-and-language models (VLMs) struggle to capture relational information.\"; \"We demonstrate a large gap between human and model performance: The human ceiling is above 95%, while state-of-the-art models only achieve around 70%.\"; \"We observe that VLMs’ by-relation performances have little correlation with the number of training examples and",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"previous work has suggested that current vision-and-language models (VLMs) struggle to capture relational information.\"; \"We demonstrate a large gap between human and model performance: The human ceiling is above 95%, while state-of-the-art models only achieve around 70%.\"; \"We observe that VLMs’ by-relation performances have little correlation with the number of training examples and"
    },
    {
        "title": "How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN",
        "authors": [
            "R. Thomas McCoy",
            "Paul Smolensky",
            "Tal Linzen",
            "Jianfeng Gao",
            "Asli Celikyilmaz"
        ],
        "published": "2023",
        "summary": "Current language models can generate high-quality text. Are they simply copying text they have seen before, or have they learned generalizable linguistic abstractions? To tease apart these possibilities, we introduce RAVEN, a suite of analyses for assessing the novelty of generated text, focusing on sequential structure (n-grams) and syntactic structure. We apply these analyses to four neural language models trained on English (an LSTM, a Transformer, Transformer-XL, and GPT-2). For local structure—e.g., individual dependencies—text generated with a standard sampling scheme is substantially less novel than our baseline of human-generated text from each model’s test set. For larger-scale structure—e.g., overall sentence structure—model-generated text is as novel or even more novel than the human-generated baseline, but models still sometimes copy substantially, in some cases duplicating passages over 1,000 words long from the training set. We also perform extensive manual analysis, finding evidence that GPT-2 uses both compositional and analogical generalization mechanisms and showing that GPT-2’s novel text is usually well-formed morphologically and syntactically but has reasonably frequent semantic issues (e.g., being self-contradictory).",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.38.pdf",
        "keywords": [
            "novelty",
            "generated",
            "sampling",
            "neural language",
            "language models",
            "human generated text",
            "compositional"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"For local structure—e.g., individual dependencies—text generated with a standard sampling scheme is substantially less novel than our baseline of human-generated text from each model’s test set.... models still sometimes copy substantially, in some cases duplicating passages over 1,000 words long from the training set... and showing that GPT-2’s novel text is usually well-formed morphologically",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"For local structure—e.g., individual dependencies—text generated with a standard sampling scheme is substantially less novel than our baseline of human-generated text from each model’s test set.... models still sometimes copy substantially, in some cases duplicating passages over 1,000 words long from the training set... and showing that GPT-2’s novel text is usually well-formed morphologically"
    },
    {
        "title": "Expectations over Unspoken Alternatives Predict Pragmatic Inferences",
        "authors": [
            "Jennifer Hu",
            "Roger Levy",
            "Judith Degen",
            "Sebastian Schuster"
        ],
        "published": "2023",
        "summary": "Scalar inferences (SI) are a signature example of how humans interpret language based on unspoken alternatives. While empirical studies have demonstrated that human SI rates are highly variable—both within instances of a single scale, and across different scales—there have been few proposals that quantitatively explain both cross- and within-scale variation. Furthermore, while it is generally assumed that SIs arise through reasoning about unspoken alternatives, it remains debated whether humans reason about alternatives as linguistic forms, or at the level of concepts. Here, we test a shared mechanism explaining SI rates within and across scales: context-driven expectations about the unspoken alternatives. Using neural language models to approximate human predictive distributions, we find that SI rates are captured by the expectedness of the strong scalemate as an alternative. Crucially, however, expectedness robustly predicts cross-scale variation only under a meaning-based view of alternatives. Our results suggest that pragmatic inferences arise from context-driven expectations over alternatives, and these expectations operate at the level of concepts.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.50.pdf",
        "keywords": [
            "context driven expectations",
            "language",
            "scale",
            "neural language models",
            "unspoken alternatives",
            "within scale variation",
            "single scale"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Using neural language models to approximate human predictive distributions\"\n\nThis paper mentions the use of neural language models, but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Using neural language models to approximate human predictive distributions\"\n\nThis paper mentions the use of neural language models, but does not discuss any limitations of LLMs."
    },
    {
        "title": "Conditional Generation with a Question-Answering Blueprint",
        "authors": [
            "Shashi Narayan",
            "Joshua Maynez",
            "Reinald Kim Amplayo",
            "Kuzman Ganchev",
            "Annie Louis",
            "Fantine Huot",
            "Anders Sandholm",
            "Dipanjan Das",
            "Mirella Lapata"
        ],
        "published": "2023",
        "summary": "The ability to convey relevant and faithful information is critical for many tasks in conditional generation and yet remains elusive for neural seq-to-seq models whose outputs often reveal hallucinations and fail to correctly cover important details. In this work, we advocate planning as a useful intermediate representation for rendering conditional generation less opaque and more grounded. We propose a new conceptualization of text plans as a sequence of question-answer (QA) pairs and enhance existing datasets (e.g., for summarization) with a QA blueprint operating as a proxy for content selection (i.e., what to say) and planning (i.e., in what order). We obtain blueprints automatically by exploiting state-of-the-art question generation technology and convert input-output pairs into input-blueprint-output tuples. We develop Transformer-based models, each varying in how they incorporate the blueprint in the generated output (e.g., as a global plan or iteratively). Evaluation across metrics and datasets demonstrates that blueprint models are more factual than alternatives which do not resort to planning and allow tighter control of the generation output.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.55.pdf",
        "keywords": [
            "conditional generation",
            "question generation",
            "question answering",
            "neural seq",
            "planning",
            "seq",
            "transformer"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The ability to convey relevant and faithful information is critical for many tasks in conditional generation and yet remains elusive for neural seq-to-seq models whose outputs often reveal hallucinations and fail to correctly cover important details.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The ability to convey relevant and faithful information is critical for many tasks in conditional generation and yet remains elusive for neural seq-to-seq models whose outputs often reveal hallucinations and fail to correctly cover important details.\""
    },
    {
        "title": "Communication Drives the Emergence of Language Universals in Neural Agents: Evidence from the Word-order/Case-marking Trade-off",
        "authors": [
            "Yuchen Lian",
            "Arianna Bisazza",
            "Tessa Verhoef"
        ],
        "published": "2023",
        "summary": "Artificial learners often behave differently from human learners in the context of neural agent-based simulations of language emergence and change. A common explanation is the lack of appropriate cognitive biases in these learners. However, it has also been proposed that more naturalistic settings of language learning and use could lead to more human-like results. We investigate this latter account, focusing on the word-order/case-marking trade-off, a widely attested language universal that has proven particularly hard to simulate. We propose a new Neural-agent Language Learning and Communication framework (NeLLCom) where pairs of speaking and listening agents first learn a miniature language via supervised learning, and then optimize it for communication via reinforcement learning. Following closely the setup of earlier human experiments, we succeed in replicating the trade-off with the new framework without hard-coding specific biases in the agents. We see this as an essential step towards the investigation of language universals with neural learners.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.58.pdf",
        "keywords": [
            "universal",
            "language universals",
            "neural agent language learning"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs."
    },
    {
        "title": "Exploring Contrast Consistency of Open-Domain Question Answering Systems on Minimally Edited Questions",
        "authors": [
            "Zhihan Zhang",
            "Wenhao Yu",
            "Zheng Ning",
            "Mingxuan Ju",
            "Meng Jiang"
        ],
        "published": "2023",
        "summary": "Contrast consistency, the ability of a model to make consistently correct predictions in the presence of perturbations, is an essential aspect in NLP. While studied in tasks such as sentiment analysis and reading comprehension, it remains unexplored in open-domain question answering (OpenQA) due to the difficulty of collecting perturbed questions that satisfy factuality requirements. In this work, we collect minimally edited questions as challenging contrast sets to evaluate OpenQA models. Our collection approach combines both human annotation and large language model generation. We find that the widely used dense passage retriever (DPR) performs poorly on our contrast sets, despite fitting the training set well and performing competitively on standard test sets. To address this issue, we introduce a simple and effective query-side contrastive loss with the aid of data augmentation to improve DPR training. Our experiments on the contrast sets demonstrate that DPR’s contrast consistency is improved without sacrificing its accuracy on the standard test sets.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.61.pdf",
        "keywords": [
            "consistency",
            "contrast consistency",
            "question answering",
            "minimally edited",
            "sentiment analysis"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that the widely used dense passage retriever (DPR) performs poorly on our contrast sets, despite fitting the training set well and performing competitively on standard test sets.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We find that the widely used dense passage retriever (DPR) performs poorly on our contrast sets, despite fitting the training set well and performing competitively on standard test sets.\""
    },
    {
        "title": "T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification",
        "authors": [
            "Inigo Jauregi Unanue",
            "Gholamreza Haffari",
            "Massimo Piccardi"
        ],
        "published": "2023",
        "summary": "Cross-lingual text classification leverages text classifiers trained in a high-resource language to perform text classification in other languages with no or minimal fine-tuning (zero/ few-shots cross-lingual transfer). Nowadays, cross-lingual text classifiers are typically built on large-scale, multilingual language models (LMs) pretrained on a variety of languages of interest. However, the performance of these models varies significantly across languages and classification tasks, suggesting that the superposition of the language modelling and classification tasks is not always effective. For this reason, in this paper we propose revisiting the classic “translate-and-test” pipeline to neatly separate the translation and classification stages. The proposed approach couples 1) a neural machine translator translating from the targeted language to a high-resource language, with 2) a text classifier trained in the high-resource language, but the neural machine translator generates “soft” translations to permit end-to-end backpropagation during fine-tuning of the pipeline. Extensive experiments have been carried out over three cross-lingual text classification datasets (XNLI, MLDoc, and MultiEURLEX), with the results showing that the proposed approach has significantly improved performance over a competitive baseline.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.65.pdf",
        "keywords": [
            "test transfer learning",
            "test",
            "cross lingual text classification",
            "neural machine translator"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the performance of these models varies significantly across languages and classification tasks, suggesting that the superposition of the language modelling and classification tasks is not always effective.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the performance of these models varies significantly across languages and classification tasks, suggesting that the superposition of the language modelling and classification tasks is not always effective.\""
    },
    {
        "title": "Improving Multitask Retrieval by Promoting Task Specialization",
        "authors": [
            "Wenzheng Zhang",
            "Chenyan Xiong",
            "Karl Stratos",
            "Arnold Overwijk"
        ],
        "published": "2023",
        "summary": "In multitask retrieval, a single retriever is trained to retrieve relevant contexts for multiple tasks. Despite its practical appeal, naive multitask retrieval lags behind task-specific retrieval, in which a separate retriever is trained for each task. We show that it is possible to train a multitask retriever that outperforms task-specific retrievers by promoting task specialization. The main ingredients are: (1) a better choice of pretrained model—one that is explicitly optimized for multitasking—along with compatible prompting, and (2) a novel adaptive learning method that encourages each parameter to specialize in a particular task. The resulting multitask retriever is highly performant on the KILT benchmark. Upon analysis, we find that the model indeed learns parameters that are more task-specialized compared to naive multitasking without prompting or adaptive learning.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.68.pdf",
        "keywords": [
            "multitask",
            "adaptive learning",
            "task"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "PASTA: A Dataset for Modeling PArticipant STAtes in Narratives",
        "authors": [
            "Sayontan Ghosh",
            "Mahnaz Koupaee",
            "Isabella Chen",
            "Francis Ferraro",
            "Nathanael Chambers",
            "Niranjan Balasubramanian"
        ],
        "published": "2023",
        "summary": "The events in a narrative are understood as a coherent whole via the underlying states of their participants. Often, these participant states are not explicitly mentioned, instead left to be inferred by the reader. A model that understands narratives should likewise infer these implicit states, and even reason about the impact of changes to these states on the narrative. To facilitate this goal, we introduce a new crowdsourced English-language, Participant States dataset, PASTA. This dataset contains inferable participant states; a counterfactual perturbation to each state; and the changes to the story that would be necessary if the counterfactual were true. We introduce three state-based reasoning tasks that test for the ability to infer when a state is entailed by a story, to revise a story conditioned on a counterfactual state, and to explain the most likely state change given a revised story. Experiments show that today’s LLMs can reason about states to some degree, but there is large room for improvement, especially in problems requiring access and ability to reason with diverse types of knowledge (e.g., physical, numerical, factual).1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.73.pdf",
        "keywords": [
            "narratives",
            "modeling participant states",
            "counterfactual state"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experiments show that today’s LLMs can reason about states to some degree, but there is large room for improvement, especially in problems requiring access and ability to reason with diverse types of knowledge (e.g., physical, numerical, factual).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Experiments show that today’s LLMs can reason about states to some degree, but there is large room for improvement, especially in problems requiring access and ability to reason with diverse types of knowledge (e.g., physical, numerical, factual).\""
    },
    {
        "title": "In-Context Retrieval-Augmented Language Models",
        "authors": [
            "Ori Ram",
            "Yoav Levine",
            "Itay Dalmedigos",
            "Dor Muhlgay",
            "Amnon Shashua",
            "Kevin Leyton-Brown",
            "Yoav Shoham"
        ],
        "published": "2023",
        "summary": "Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.75.pdf",
        "keywords": [
            "language model",
            "context",
            "augmented language modeling",
            "lm"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"mitigate the problem of factually inaccurate text generation\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"mitigate the problem of factually inaccurate text generation\""
    },
    {
        "title": "Learning to Paraphrase Sentences to Different Complexity Levels",
        "authors": [
            "Alison Chi",
            "Li-Kuang Chen",
            "Yi-Chen Chang",
            "Shu-Hui Lee",
            "Jason S. Chang"
        ],
        "published": "2023",
        "summary": "While sentence simplification is an active research topic in NLP, its adjacent tasks of sentence complexification and same-level paraphrasing are not. To train models on all three tasks, we present two new unsupervised datasets. We compare these datasets, one labeled by a weak classifier and the other by a rule-based approach, with a single supervised dataset. Using these three datasets for training, we perform extensive experiments on both multitasking and prompting strategies. Compared to other systems trained on unsupervised parallel data, models trained on our weak classifier labeled dataset achieve state-of-the-art performance on the ASSET simplification benchmark. Our models also outperform previous work on sentence-level targeting. Finally, we establish how a handful of Large Language Models perform on these tasks under a zero-shot setting.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.76.pdf",
        "keywords": [
            "sentence complexification",
            "sentence simplification",
            "sentence level"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Finally, we establish how a handful of Large Language Models perform on these tasks under a zero-shot setting.\"\n\nThis paper mentions LLMs but does not discuss their limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Finally, we establish how a handful of Large Language Models perform on these tasks under a zero-shot setting.\"\n\nThis paper mentions LLMs but does not discuss their limitations."
    },
    {
        "title": "How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure",
        "authors": [
            "Michael Wilson",
            "Jackson Petty",
            "Robert Frank"
        ],
        "published": "2023",
        "summary": "Language models are typically evaluated on their success at predicting the distribution of specific words in specific contexts. Yet linguistic knowledge also encodes relationships between contexts, allowing inferences between word distributions. We investigate the degree to which pre-trained transformer-based large language models (LLMs) represent such relationships, focusing on the domain of argument structure. We find that LLMs perform well in generalizing the distribution of a novel noun argument between related contexts that were seen during pre-training (e.g., the active object and passive subject of the verb spray), succeeding by making use of the semantically organized structure of the embedding space for word embeddings. However, LLMs fail at generalizations between related contexts that have not been observed during pre-training, but which instantiate more abstract, but well-attested structural generalizations (e.g., between the active object and passive subject of an arbitrary verb). Instead, in this case, LLMs show a bias to generalize based on linear order. This finding points to a limitation with current models and points to a reason for which their training is data-intensive.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.78.pdf",
        "keywords": [
            "argument structure",
            "domain of argument structure",
            "linguistic",
            "linguistic knowledge",
            "language models",
            "large language models",
            "abstract",
            "generalize"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, LLMs fail at generalizations between related contexts that have not been observed during pre-training, but which instantiate more abstract, but well-attested structural generalizations (e.g., between the active object and passive subject of an arbitrary verb). Instead, in this case, LLMs show a bias to generalize based on linear order. This finding points to a limitation",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, LLMs fail at generalizations between related contexts that have not been observed during pre-training, but which instantiate more abstract, but well-attested structural generalizations (e.g., between the active object and passive subject of an arbitrary verb). Instead, in this case, LLMs show a bias to generalize based on linear order. This finding points to a limitation"
    },
    {
        "title": "Testing the Predictions of Surprisal Theory in 11 Languages",
        "authors": [
            "Ethan G. Wilcox",
            "Tiago Pimentel",
            "Clara Meister",
            "Ryan Cotterell",
            "Roger P. Levy"
        ],
        "published": "2023",
        "summary": "Surprisal theory posits that less-predictable words should take more time to process, with word predictability quantified as surprisal, i.e., negative log probability in context. While evidence supporting the predictions of surprisal theory has been replicated widely, much of it has focused on a very narrow slice of data: native English speakers reading English texts. Indeed, no comprehensive multilingual analysis exists. We address this gap in the current literature by investigating the relationship between surprisal and reading times in eleven different languages, distributed across five language families. Deriving estimates from language models trained on monolingual and multilingual corpora, we test three predictions associated with surprisal theory: (i) whether surprisal is predictive of reading times, (ii) whether expected surprisal, i.e., contextual entropy, is predictive of reading times, and (iii) whether the linking function between surprisal and reading times is linear. We find that all three predictions are borne out crosslinguistically. By focusing on a more diverse set of languages, we argue that these results offer the most robust link to date between information theory and incremental language processing across languages.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.82.pdf",
        "keywords": [
            "surprisal",
            "surprisal theory",
            "negative log probability",
            "predictable",
            "word predictability",
            "surprisal and reading times",
            "contextual entropy",
            "language models",
            "language processing"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Deriving estimates from language models trained on monolingual and multilingual corpora\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Deriving estimates from language models trained on monolingual and multilingual corpora\""
    },
    {
        "title": "Hallucinations in Large Multilingual Translation Models",
        "authors": [
            "Nuno M. Guerreiro",
            "Duarte M. Alves",
            "Jonas Waldendorf",
            "Barry Haddow",
            "Alexandra Birch",
            "Pierre Colombo",
            "André F. T. Martins"
        ],
        "published": "2023",
        "summary": "Hallucinated translations can severely undermine and raise safety issues when machine translation systems are deployed in the wild. Previous research on the topic focused on small bilingual models trained on high-resource languages, leaving a gap in our understanding of hallucinations in multilingual models across diverse translation scenarios. In this work, we fill this gap by conducting a comprehensive analysis—over 100 language pairs across various resource levels and going beyond English-centric directions—on both the M2M neural machine translation (NMT) models and GPT large language models (LLMs). Among several insights, we highlight that models struggle with hallucinations primarily in low-resource directions and when translating out of English, where, critically, they may reveal toxic patterns that can be traced back to the training data. We also find that LLMs produce qualitatively different hallucinations to those of NMT models. Finally, we show that hallucinations are hard to reverse by merely scaling models trained with the same data. However, employing more diverse models, trained on different data or with different procedures, as fallback systems can improve translation quality and virtually eliminate certain pathologies.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.85.pdf",
        "keywords": [
            "translation",
            "neural machine translation",
            "translation quality",
            "translation models",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"models struggle with hallucinations primarily in low-resource directions and when translating out of English, where, critically, they may reveal toxic patterns that can be traced back to the training data.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"models struggle with hallucinations primarily in low-resource directions and when translating out of English, where, critically, they may reveal toxic patterns that can be traced back to the training data.\""
    },
    {
        "title": "Pre-train, Prompt, and Recommendation: A Comprehensive Survey of Language Modeling Paradigm Adaptations in Recommender Systems",
        "authors": [
            "Peng Liu",
            "Lemei Zhang",
            "Jon Atle Gulla"
        ],
        "published": "2023",
        "summary": "The emergence of Pre-trained Language Models (PLMs) has achieved tremendous success in the field of Natural Language Processing (NLP) by learning universal representations on large corpora in a self-supervised manner. The pre-trained models and the learned representations can be beneficial to a series of downstream NLP tasks. This training paradigm has recently been adapted to the recommendation domain and is considered a promising approach by both academia and industry. In this paper, we systematically investigate how to extract and transfer knowledge from pre-trained models learned by different PLM-related training paradigms to improve recommendation performance from various perspectives, such as generality, sparsity, efficiency and effectiveness. Specifically, we propose a comprehensive taxonomy to divide existing PLM-based recommender systems w.r.t. their training strategies and objectives. Then, we analyze and summarize the connection between PLM-based training paradigms and different input data types for recommender systems. Finally, we elaborate on open issues and future research directions in this vibrant field.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.88.pdf",
        "keywords": [
            "recommender systems",
            "language modeling",
            "adaptations",
            "natural language processing",
            "pre trained models",
            "survey"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Finally, we elaborate on open issues and future research directions in this vibrant field.\"\n\nThis rating is given because the abstract mentions \"open issues\" which implies limitations, but does not elaborate on them in detail, and the primary focus is on the survey and taxonomy of PLM-based recommender systems.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Finally, we elaborate on open issues and future research directions in this vibrant field.\"\n\nThis rating is given because the abstract mentions \"open issues\" which implies limitations, but does not elaborate on them in detail, and the primary focus is on the survey and taxonomy of PLM-based recommender systems."
    },
    {
        "title": "An Efficient Self-Supervised Cross-View Training For Sentence Embedding",
        "authors": [
            "Peerat Limkonchotiwat",
            "Wuttikorn Ponwitayarat",
            "Lalita Lowphansirikul",
            "Can Udomcharoenchaikit",
            "Ekapol Chuangsuwanich",
            "Sarana Nutanong"
        ],
        "published": "2023",
        "summary": "Self-supervised sentence representation learning is the task of constructing an embedding space for sentences without relying on human annotation efforts. One straightforward approach is to finetune a pretrained language model (PLM) with a representation learning method such as contrastive learning. While this approach achieves impressive performance on larger PLMs, the performance rapidly degrades as the number of parameters decreases. In this paper, we propose a framework called Self-supervised Cross-View Training (SCT) to narrow the performance gap between large and small PLMs. To evaluate the effectiveness of SCT, we compare it to 5 baseline and state-of-the-art competitors on seven Semantic Textual Similarity (STS) benchmarks using 5 PLMs with the number of parameters ranging from 4M to 340M. The experimental results show that STC outperforms the competitors for PLMs with less than 100M parameters in 18 of 21 cases.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.89.pdf",
        "keywords": [
            "sentence embedding",
            "self supervised sentence representation",
            "learning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While this approach achieves impressive performance on larger PLMs, the performance rapidly degrades as the number of parameters decreases.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While this approach achieves impressive performance on larger PLMs, the performance rapidly degrades as the number of parameters decreases.\""
    },
    {
        "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation",
        "authors": [
            "Patrick Fernandes",
            "Aman Madaan",
            "Emmy Liu",
            "António Farinhas",
            "Pedro Henrique Martins",
            "Amanda Bertsch",
            "José G. C. de Souza",
            "Shuyan Zhou",
            "Tongshuang Wu",
            "Graham Neubig",
            "André F. T. Martins"
        ],
        "published": "2023",
        "summary": "Natural language generation has witnessed significant advancements due to the training of large language models on vast internet-scale datasets. Despite these advancements, there exists a critical challenge: These models can inadvertently generate content that is toxic, inaccurate, and unhelpful, and existing automatic evaluation metrics often fall short of identifying these shortcomings. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of recent research that has leveraged human feedback to improve natural language generation. First, we introduce a taxonomy distilled from existing research to categorize and organize the varied forms of feedback. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. Finally, we provide an overview of the nascent field of AI feedback, which uses large language models to make judgments based on a set of principles and minimize the need for human intervention. We also release a website of this survey at feedback-gap-survey.info.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.92.pdf",
        "keywords": [
            "natural language generation",
            "feedback",
            "survey",
            "language models",
            "taxonomy"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"These models can inadvertently generate content that is toxic, inaccurate, and unhelpful, and existing automatic evaluation metrics often fall short of identifying these shortcomings.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"These models can inadvertently generate content that is toxic, inaccurate, and unhelpful, and existing automatic evaluation metrics often fall short of identifying these shortcomings.\""
    },
    {
        "title": "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision",
        "authors": [
            "Eugene Kharitonov",
            "Damien Vincent",
            "Zalán Borsos",
            "Raphaël Marinier",
            "Sertan Girgin",
            "Olivier Pietquin",
            "Matt Sharifi",
            "Marco Tagliasacchi",
            "Neil Zeghidour"
        ],
        "published": "2023",
        "summary": "We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to “reading”) and from semantic tokens to low-level acoustic tokens (“speaking”). Decoupling these two tasks enables training of the “speaking” module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the “reading” component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in naturalness and acoustic quality.",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.95.pdf",
        "keywords": [
            "tts",
            "spear tts",
            "text to speech",
            "speech",
            "semantic tokens"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs or their limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs or their limitations."
    },
    {
        "title": "QAmeleon: Multilingual QA with Only 5 Examples",
        "authors": [
            "Priyanka Agrawal",
            "Chris Alberti",
            "Fantine Huot",
            "Joshua Maynez",
            "Ji Ma",
            "Sebastian Ruder",
            "Kuzman Ganchev",
            "Dipanjan Das",
            "Mirella Lapata"
        ],
        "published": "2023",
        "summary": "The availability of large, high-quality datasets has been a major driver of recent progress in question answering (QA). Such annotated datasets, however, are difficult and costly to collect, and rarely exist in languages other than English, rendering QA technology inaccessible to underrepresented languages. An alternative to building large monolingual training datasets is to leverage pre-trained language models (PLMs) under a few-shot learning setting. Our approach, QAmeleon, uses a PLM to automatically generate multilingual data upon which QA models are fine-tuned, thus avoiding costly annotation. Prompt tuning the PLM with only five examples per language delivers accuracy superior to translation-based baselines; it bridges nearly 60% of the gap between an English-only baseline and a fully-supervised upper bound fine-tuned on almost 50,000 hand-labeled examples; and consistently leads to improvements compared to directly fine-tuning a QA model on labeled examples in low resource settings. Experiments on the TyDiqa-GoldP and MLQA benchmarks show that few-shot prompt tuning for data synthesis scales across languages and is a viable alternative to large-scale annotation.1",
        "pdf_link": "https://aclanthology.org/2023.tacl-1.98.pdf",
        "keywords": [
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Such annotated datasets, however, are difficult and costly to collect, and rarely exist in languages other than English, rendering QA technology inaccessible to underrepresented languages.\"\n\nNote: Although the paper does not explicitly discuss limitations of LLMs, it discusses the limitations of current QA technology due to the lack of large, high-quality datasets in languages other than English, which is indirectly related to the",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Such annotated datasets, however, are difficult and costly to collect, and rarely exist in languages other than English, rendering QA technology inaccessible to underrepresented languages.\"\n\nNote: Although the paper does not explicitly discuss limitations of LLMs, it discusses the limitations of current QA technology due to the lack of large, high-quality datasets in languages other than English, which is indirectly related to the"
    }
]