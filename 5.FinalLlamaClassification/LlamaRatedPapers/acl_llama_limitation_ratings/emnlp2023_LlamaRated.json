[
    {
        "title": "IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions",
        "authors": [
            "Zhebin Zhang",
            "Xinyu Zhang",
            "Yuanhang Ren",
            "Saijiang Shi",
            "Meng Han",
            "Yongkang Wu",
            "Ruofei Lai",
            "Zhao Cao"
        ],
        "published": "2023",
        "summary": "Retrieval-Augmented Generation (RAG), by incorporating external knowledge with parametric memory of language models, has become the state-of-the-art architecture for open-domain QA tasks. However, common knowledge bases are inherently constrained by limited coverage and noisy information, making retrieval-based approaches inadequate to answer implicit reasoning questions. In this paper, we propose an Induction-Augmented Generation (IAG) framework that utilizes inductive knowledge along with the retrieved documents for implicit reasoning. We leverage large language models (LLMs) for deriving such knowledge via a novel prompting method based on inductive reasoning patterns. On top of this, we implement two versions of IAG named IAG-GPT and IAG-Student, respectively. IAG-GPT directly utilizes the knowledge generated by GPT-3 for answer prediction, while IAG-Student gets rid of dependencies on GPT service at inference time by incorporating a student inductor model. The inductor is firstly trained via knowledge distillation and further optimized by back-propagating the generator feedback via differentiable beam scores. Experimental results show that IAG outperforms RAG baselines as well as ChatGPT on two Open-Domain QA tasks. Notably, our best models have won the first place in the official leaderboards of CSQA2.0 (since Nov 1, 2022) and StrategyQA (since Jan 8, 2023).",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1.pdf",
        "keywords": [
            "implicit reasoning",
            "inductive reasoning",
            "retrieval augmented generation",
            "inductive knowledge",
            "large language models",
            "iag"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, common knowledge bases are inherently constrained by limited coverage and noisy information, making retrieval-based approaches inadequate to answer implicit reasoning questions.\"\n\nThis paper mentions a limitation of LLMs in the context of knowledge bases, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, common knowledge bases are inherently constrained by limited coverage and noisy information, making retrieval-based approaches inadequate to answer implicit reasoning questions.\"\n\nThis paper mentions a limitation of LLMs in the context of knowledge bases, but it is not the primary focus of the paper."
    },
    {
        "title": "Chinese Lexical Substitution: Dataset and Method",
        "authors": [
            "Jipeng Qiang",
            "Kang Liu",
            "Ying Li",
            "Yun Li",
            "Yi Zhu",
            "Yun-Hao Yuan",
            "Xiaocheng Hu",
            "Xiaoye Ouyang"
        ],
        "published": "2023",
        "summary": "Existing lexical substitution (LS) benchmarks were collected by asking human annotators to think of substitutes from memory, resulting in benchmarks with limited coverage and relatively small scales. To overcome this problem, we propose a novel annotation method to construct an LS dataset based on human and machine collaboration. Based on our annotation method, we construct the first Chinese LS dataset CHNLS which consists of 33,695 instances and 144,708 substitutes, covering three text genres (News, Novel, and Wikipedia). Specifically, we first combine four unsupervised LS methods as an ensemble method to generate the candidate substitutes, and then let human annotators judge these candidates or add new ones. This collaborative process combines the diversity of machine-generated substitutes with the expertise of human annotators. Experimental results that the ensemble method outperforms other LS methods. To our best knowledge, this is the first study for the Chinese LS task.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.3.pdf",
        "keywords": [
            "substitutes",
            "lexical substitution",
            "chinese lexical substitution",
            "collaborative"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Decoding the Silent Majority: Inducing Belief Augmented Social Graph with Large Language Model for Response Forecasting",
        "authors": [
            "Chenkai Sun",
            "Jinning Li",
            "Yi Fung",
            "Hou Chan",
            "Tarek Abdelzaher",
            "ChengXiang Zhai",
            "Heng Ji"
        ],
        "published": "2023",
        "summary": "Automatic response forecasting for news media plays a crucial role in enabling content producers to efficiently predict the impact of news releases and prevent unexpected negative outcomes such as social conflict and moral injury. To effectively forecast responses, it is essential to develop measures that leverage the social dynamics and contextual information surrounding individuals, especially in cases where explicit profiles or historical actions of the users are limited (referred to as lurkers). As shown in a previous study, 97% of all tweets are produced by only the most active 25% of users. However, existing approaches have limited exploration of how to best process and utilize these important features. To address this gap, we propose a novel framework, named SocialSense, that leverages a large language model to induce a belief-centered graph on top of an existent social network, along with graph-based propagation to capture social dynamics. We hypothesize that the induced graph that bridges the gap between distant users who share similar beliefs allows the model to effectively capture the response patterns. Our method surpasses existing state-of-the-art in experimental evaluations for both zero-shot and supervised settings, demonstrating its effectiveness in response forecasting. Moreover, the analysis reveals the framework’s capability to effectively handle unseen user and lurker scenarios, further highlighting its robustness and practical applicability.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.4.pdf",
        "keywords": [
            "language model",
            "forecasting",
            "response forecasting",
            "automatic response forecasting",
            "social network",
            "social graph",
            "belief",
            "belief centered graph"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper does not mention any limitations of LLMs, but rather uses a large language model as a component of the proposed framework.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper does not mention any limitations of LLMs, but rather uses a large language model as a component of the proposed framework."
    },
    {
        "title": "PHD: Pixel-Based Language Modeling of Historical Documents",
        "authors": [
            "Nadav Borenstein",
            "Phillip Rust",
            "Desmond Elliott",
            "Isabelle Augenstein"
        ],
        "published": "2023",
        "summary": "The digitisation of historical documents has provided historians with unprecedented research opportunities. Yet, the conventional approach to analysing historical documents involves converting them from images to text using OCR, a process that overlooks the potential benefits of treating them as images and introduces high levels of noise. To bridge this gap, we take advantage of recent advancements in pixel-based language models trained to reconstruct masked patches of pixels instead of predicting token distributions. Due to the scarcity of real historical scans, we propose a novel method for generating synthetic scans to resemble real historical documents. We then pre-train our model, PHD, on a combination of synthetic scans and real historical newspapers from the 1700-1900 period. Through our experiments, we demonstrate that PHD exhibits high proficiency in reconstructing masked image patches and provide evidence of our model’s noteworthy language understanding capabilities. Notably, we successfully apply our model to a historical QA task, highlighting its usefulness in this domain.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.7.pdf",
        "keywords": [
            "historians",
            "historical documents",
            "masked",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation of LLMs is mentioned in the abstract, but the title and content imply a connection to LLMs, and the focus is on a novel approach to pixel-based language modeling.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation of LLMs is mentioned in the abstract, but the title and content imply a connection to LLMs, and the focus is on a novel approach to pixel-based language modeling."
    },
    {
        "title": "Primacy Effect of ChatGPT",
        "authors": [
            "Yiwei Wang",
            "Yujun Cai",
            "Muhao Chen",
            "Yuxuan Liang",
            "Bryan Hooi"
        ],
        "published": "2023",
        "summary": "Instruction-tuned large language models (LLMs), such as ChatGPT, have led to promising zero-shot performance in discriminative natural language understanding (NLU) tasks. This involves querying the LLM using a prompt containing the question, and the candidate labels to choose from. The question-answering capabilities of ChatGPT arise from its pre-training on large amounts of human-written text, as well as its subsequent fine-tuning on human preferences, which motivates us to ask: Does ChatGPT also inherit humans’ cognitive biases? In this paper, we study the primacy effect of ChatGPT: the tendency of selecting the labels at earlier positions as the answer. We have two main findings: i) ChatGPT’s decision is sensitive to the order of labels in the prompt; ii) ChatGPT has a clearly higher chance to select the labels at earlier positions as the answer. We hope that our experiments and analyses provide additional insights into building more reliable ChatGPT-based solutions. We release the source code at https://github.com/wangywUST/PrimacyEffectGPT.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.8.pdf",
        "keywords": [
            "chatgpt",
            "language models",
            "chatgpt based",
            "instruction tuned"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"i) ChatGPT’s decision is sensitive to the order of labels in the prompt; ii) ChatGPT has a clearly higher chance to select the labels at earlier positions as the answer.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of ChatGPT (primacy effect), but it is not the primary focus of the paper, and the discussion is",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"i) ChatGPT’s decision is sensitive to the order of labels in the prompt; ii) ChatGPT has a clearly higher chance to select the labels at earlier positions as the answer.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of ChatGPT (primacy effect), but it is not the primary focus of the paper, and the discussion is"
    },
    {
        "title": "Evaluating the Rationale Understanding of Critical Reasoning in Logical Reading Comprehension",
        "authors": [
            "Akira Kawabata",
            "Saku Sugawara"
        ],
        "published": "2023",
        "summary": "To precisely evaluate a language model’s capability for logical reading comprehension, we present a dataset for testing the understanding of the rationale behind critical reasoning. For questions taken from an existing multiple-choice logical reading comprehension dataset, we crowdsource rationale texts that explain why we should select or eliminate answer options, resulting in 3,003 multiple-choice subquestions that are associated with 943 main questions. Experiments on our dataset show that recent large language models (e.g., InstructGPT) struggle to answer the subquestions even if they are able to answer the main questions correctly. We find that the models perform particularly poorly in answering subquestions written for the incorrect options of the main questions, implying that the models have a limited capability for explaining why incorrect alternatives should be eliminated. These results suggest that our dataset encourages further investigation into the critical reasoning ability of language models while focusing on the elimination process of relevant alternatives.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.9.pdf",
        "keywords": [
            "critical reasoning",
            "language models",
            "rationale",
            "crowdsource rationale texts",
            "logical reading comprehension",
            "evaluating the rationale understanding",
            "critical reasoning ability",
            "relevant"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experiments on our dataset show that recent large language models (e.g., InstructGPT) struggle to answer the subquestions even if they are able to answer the main questions correctly. We find that the models perform particularly poorly in answering subquestions written for the incorrect options of the main questions, implying that the models have a limited capability for explaining why incorrect alternatives should be eliminated",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Experiments on our dataset show that recent large language models (e.g., InstructGPT) struggle to answer the subquestions even if they are able to answer the main questions correctly. We find that the models perform particularly poorly in answering subquestions written for the incorrect options of the main questions, implying that the models have a limited capability for explaining why incorrect alternatives should be eliminated"
    },
    {
        "title": "Evaluating and Modeling Attribution for Cross-Lingual Question Answering",
        "authors": [
            "Benjamin Muller",
            "John Wieting",
            "Jonathan Clark",
            "Tom Kwiatkowski",
            "Sebastian Ruder",
            "Livio Soares",
            "Roee Aharoni",
            "Jonathan Herzig",
            "Xinyi Wang"
        ],
        "published": "2023",
        "summary": "Trustworthy answer content is abundant in many high-resource languages and is instantly accessible through question answering systems — yet this content can be hard to access for those that do not speak these languages. The leap forward in cross-lingual modeling quality offered by generative language models offers much promise, yet their raw generations often fall short in factuality. To improve trustworthiness in these systems, a promising direction is to attribute the answer to a retrieved source, possibly in a content-rich language different from the query. Our work is the first to study attribution for cross-lingual question answering. First, we collect data in 5 languages to assess the attribution level of a state-of-the-art cross-lingual QA system. To our surprise, we find that a substantial portion of the answers is not attributable to any retrieved passages (up to 50% of answers exactly matching a gold reference) despite the system being able to attend directly to the retrieved text. Second, to address this poor attribution level, we experiment with a wide range of attribution detection techniques. We find that Natural Language Inference models and PaLM 2 fine-tuned on a very small amount of attribution data can accurately detect attribution. With these models, we improve the attribution level of a cross-lingual QA system. Overall, we show that current academic generative cross-lingual QA systems have substantial shortcomings in attribution and we build tooling to mitigate these issues.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.10.pdf",
        "keywords": [
            "attribution",
            "cross lingual question answering",
            "cross lingual modeling",
            "language models",
            "natural language inference models",
            "attribution detection"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"To our surprise, we find that a substantial portion of the answers is not attributable to any retrieved passages (up to 50% of answers exactly matching a gold reference) despite the system being able to attend directly to the retrieved text.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"To our surprise, we find that a substantial portion of the answers is not attributable to any retrieved passages (up to 50% of answers exactly matching a gold reference) despite the system being able to attend directly to the retrieved text.\""
    },
    {
        "title": "Better Quality Pre-training Data and T5 Models for African Languages",
        "authors": [
            "Akintunde Oladipo",
            "Mofetoluwa Adeyemi",
            "Orevaoghene Ahia",
            "Abraham Owodunni",
            "Odunayo Ogundepo",
            "David Adelani",
            "Jimmy Lin"
        ],
        "published": "2023",
        "summary": "In this study, we highlight the importance of enhancing the quality of pretraining data in multilingual language models. Existing web crawls have demonstrated quality issues, particularly in the context of low-resource languages. Consequently, we introduce a new multilingual pretraining corpus for 16 African languages, designed by carefully auditing existing pretraining corpora to understand and rectify prevalent quality issues. To compile this dataset, we undertake a rigorous examination of current data sources for thirteen languages within one of the most extensive multilingual web crawls, mC4, and extract cleaner data through meticulous auditing and improved web crawling strategies. Subsequently, we pretrain a new T5-based model on this dataset and evaluate its performance on multiple downstream tasks. Our model demonstrates better downstream effectiveness over existing pretrained models across four NLP tasks, underscoring the critical role data quality plays in pretraining language models in low-resource scenarios. Specifically, on cross-lingual QA evaluation, our new model is more than twice as effective as multilingual T5. All code, data and models are publicly available at https://github.com/castorini/AfriTeVa-keji.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.11.pdf",
        "keywords": [
            "languages",
            "t5",
            "t5 models",
            "language models",
            "pretraining language",
            "african languages",
            "web crawling",
            "t5 based"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing web crawls have demonstrated quality issues, particularly in the context of low-resource languages.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing web crawls have demonstrated quality issues, particularly in the context of low-resource languages.\""
    },
    {
        "title": "Theory of Mind for Multi-Agent Collaboration via Large Language Models",
        "authors": [
            "Huao Li",
            "Yu Chong",
            "Simon Stepputtis",
            "Joseph Campbell",
            "Dana Hughes",
            "Charles Lewis",
            "Katia Sycara"
        ],
        "published": "2023",
        "summary": "While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents’ planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.13.pdf",
        "keywords": [
            "of mind",
            "theory of mind",
            "language models",
            "large language models",
            "collaboration"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our results reveal limitations in LLM-based agents’ planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Our results reveal limitations in LLM-based agents’ planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state.\""
    },
    {
        "title": "Establishing Trustworthiness: Rethinking Tasks and Model Evaluation",
        "authors": [
            "Robert Litschko",
            "Max Müller-Eberstein",
            "Rob van der Goot",
            "Leon Weber-Genzel",
            "Barbara Plank"
        ],
        "published": "2023",
        "summary": "Language understanding is a multi-faceted cognitive capability, which the Natural Language Processing (NLP) community has striven to model computationally for decades. Traditionally, facets of linguistic intelligence have been compartmentalized into tasks with specialized model architectures and corresponding evaluation protocols. With the advent of large language models (LLMs) the community has witnessed a dramatic shift towards general purpose, task-agnostic approaches powered by generative models. As a consequence, the traditional compartmentalized notion of language tasks is breaking down, followed by an increasing challenge for evaluation and analysis. At the same time, LLMs are being deployed in more real-world scenarios, including previously unforeseen zero-shot setups, increasing the need for trustworthy and reliable systems. Therefore, we argue that it is time to rethink what constitutes tasks and model evaluation in NLP, and pursue a more holistic view on language, placing trustworthiness at the center. Towards this goal, we review existing compartmentalized approaches for understanding the origins of a model’s functional capacity, and provide recommendations for more multi-faceted evaluation protocols.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.14.pdf",
        "keywords": [
            "trustworthiness",
            "model",
            "model evaluation",
            "natural language processing",
            "reliable"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"At the same time, LLMs are being deployed in more real-world scenarios, including previously unforeseen zero-shot setups, increasing the need for trustworthy and reliable systems.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"At the same time, LLMs are being deployed in more real-world scenarios, including previously unforeseen zero-shot setups, increasing the need for trustworthy and reliable systems.\""
    },
    {
        "title": "Let’s Think Frame by Frame with VIP: A Video Infilling and Prediction Dataset for Evaluating Video Chain-of-Thought",
        "authors": [
            "Vaishnavi Himakunthala",
            "Andy Ouyang",
            "Daniel Rose",
            "Ryan He",
            "Alex Mei",
            "Yujie Lu",
            "Chinmay Sonar",
            "Michael Saxon",
            "William Wang"
        ],
        "published": "2023",
        "summary": "Despite exciting recent results showing vision-language systems’ capacity to reason about images using natural language, their capacity for video reasoning remains underexplored. We motivate framing video reasoning as the sequential understanding of a small number of keyframes, thereby leveraging the power and robustness of vision-language while alleviating the computational complexities of processing videos. To evaluate this novel application, we introduce VIP, an inference-time challenge dataset designed to explore models’ reasoning capabilities through video chain-of-thought. Inspired by visually descriptive scene plays, we propose two formats for keyframe description: unstructured dense captions and structured scene descriptions that identify the focus, action, mood, objects, and setting (FAMOuS) of the keyframe. To evaluate video reasoning, we propose two tasks: Video Infilling and Video Prediction, which test abilities to generate multiple intermediate keyframes and predict future keyframes, respectively. We benchmark GPT-4, GPT-3, and VICUNA on VIP, demonstrate the performance gap in these complex video reasoning tasks, and encourage future work to prioritize language models for efficient and generalized video reasoning.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.15.pdf",
        "keywords": [
            "videos",
            "video chain",
            "video infilling",
            "video reasoning",
            "video prediction",
            "video chain of thought"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"demonstrate the performance gap in these complex video reasoning tasks\"\n\nThis paper mentions a limitation of LLMs in passing, specifically their performance gap in complex video reasoning tasks, but does not elaborate on it.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"demonstrate the performance gap in these complex video reasoning tasks\"\n\nThis paper mentions a limitation of LLMs in passing, specifically their performance gap in complex video reasoning tasks, but does not elaborate on it."
    },
    {
        "title": "GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP",
        "authors": [
            "Md Tawkat Islam Khondaker",
            "Abdul Waheed",
            "El Moatez Billah Nagoudi",
            "Muhammad Abdul-Mageed"
        ],
        "published": "2023",
        "summary": "ChatGPT’s emergence heralds a transformative phase in NLP, particularly demonstrated through its excellent performance on many English benchmarks. However, the model’s efficacy across diverse linguistic contexts remains largely uncharted territory. This work aims to bridge this knowledge gap, with a primary focus on assessing ChatGPT’s capabilities on Arabic languages and dialectal varieties. Our comprehensive study conducts a large-scale automated and human evaluation of ChatGPT, encompassing 44 distinct language understanding and generation tasks on over 60 different datasets. To our knowledge, this marks the first extensive performance analysis of ChatGPT’s deployment in Arabic NLP. Our findings indicate that, despite its remarkable performance in English, ChatGPT is consistently surpassed by smaller models that have undergone finetuning on Arabic. We further undertake a meticulous comparison of ChatGPT and GPT-4’s Modern Standard Arabic (MSA) and Dialectal Arabic (DA), unveiling the relative shortcomings of both models in handling Arabic dialects compared to MSA. Although we further explore and confirm the utility of employing GPT-4 as a potential alternative for human evaluation, our work adds to a growing body of research underscoring the limitations of ChatGPT.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.16.pdf",
        "keywords": [
            "arabic nlp",
            "arabic languages",
            "analysis"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our findings indicate that, despite its remarkable performance in English, ChatGPT is consistently surpassed by smaller models that have undergone finetuning on Arabic.\" and \"unveiling the relative shortcomings of both models in handling Arabic dialects compared to MSA.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Our findings indicate that, despite its remarkable performance in English, ChatGPT is consistently surpassed by smaller models that have undergone finetuning on Arabic.\" and \"unveiling the relative shortcomings of both models in handling Arabic dialects compared to MSA.\""
    },
    {
        "title": "Evaluating Object Hallucination in Large Vision-Language Models",
        "authors": [
            "Yifan Li",
            "Yifan Du",
            "Kun Zhou",
            "Jinpeng Wang",
            "Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2023",
        "summary": "Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently proposed by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that they suffer from object hallucinations, i.e., they tend to generate objects inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issues. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently appear in the visual instructions or co-occur with the image objects are obviously prone to be hallucinated by LVLMs. Besides, we further design a polling-based query method called POPE for better evaluation of object hallucination. Experiment results show that our POPE can evaluate object hallucination in a more stable and flexible way.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.20.pdf",
        "keywords": [
            "object hallucination",
            "vision language models",
            "large vision language models",
            "large language models",
            "evaluating object hallucination"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the promising progress on LVLMs, we find that they suffer from object hallucinations, i.e., they tend to generate objects inconsistent with the target images in the descriptions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Despite the promising progress on LVLMs, we find that they suffer from object hallucinations, i.e., they tend to generate objects inconsistent with the target images in the descriptions.\""
    },
    {
        "title": "Event Ontology Completion with Hierarchical Structure Evolution Networks",
        "authors": [
            "Pengfei Cao",
            "Yupu Hao",
            "Yubo Chen",
            "Kang Liu",
            "Jiexin Xu",
            "Huaijun Li",
            "Xiaojian Jiang",
            "Jun Zhao"
        ],
        "published": "2023",
        "summary": "Traditional event detection methods require predefined event schemas. However, manually defining event schemas is expensive and the coverage of schemas is limited. To this end, some works study the event type induction (ETI) task, which discovers new event types via clustering. However, the setting of ETI suffers from two limitations: event types are not linked into the existing hierarchy and have no semantic names. In this paper, we propose a new research task named Event Ontology Completion (EOC), which aims to simultaneously achieve event clustering, hierarchy expansion and type naming. Furthermore, we develop a Hierarchical Structure Evolution Network (HalTon) for this new task. Specifically, we first devise a Neighborhood Contrastive Clustering module to cluster unlabeled event instances. Then, we propose a Hierarchy-Aware Linking module to incorporate the hierarchical information for event expansion. Finally, we generate meaningful names for new types via an In-Context Learning-based Naming module. Extensive experiments indicate that our method achieves the best performance, outperforming the baselines by 8.23%, 8.79% and 8.10% of ARI score on three datasets.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.21.pdf",
        "keywords": [
            "hierarchy aware",
            "event ontology completion",
            "event clustering",
            "hierarchical structure evolution networks",
            "hierarchical information"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Parameter-efficient Tuning for Large Language Model without Calculating Its Gradients",
        "authors": [
            "Feihu Jin",
            "Jiajun Zhang",
            "Chengqing Zong"
        ],
        "published": "2023",
        "summary": "Fine-tuning all parameters of large language models (LLMs) requires significant computational resources and is time-consuming. Recent parameter-efficient tuning methods such as Adapter tuning, Prefix tuning, and LoRA allow for updating a small subset of parameters in large language models. However, they can only save approximately 30% of the training memory requirements, due to the problem that gradient computation and backpropagation are still necessary for these methods. This paper proposes a novel parameter-efficient tuning method for LLMs without calculating their gradients. Leveraging the discernible similarities between the parameter-efficient modules of the same task learned by both large and small language models, we put forward a strategy for transferring the parameter-efficient modules, originally derived from small language models to much larger ones. To ensure a smooth and effective adaptation process, we further introduce a Bridge model to guarantee dimensional consistency while also stimulating a dynamic interaction between the models. We demonstrate the effectiveness of our method using the T5 and GPT-2 series of language models on the SuperGLUE benchmark. Our method achieves comparable performance to both fine-tuning and parameter-efficient tuning on large language models without needing gradient-based optimization. Additionally, our method achieves up to 5.7x memory reduction compared to parameter-efficient tuning.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.22.pdf",
        "keywords": [
            "tuning",
            "parameter efficient tuning",
            "adapter tuning",
            "adaptation",
            "language models",
            "models",
            "fine tuning",
            "large language model",
            "prefix tuning",
            "smooth",
            "consistency"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Fine-tuning all parameters of large language models (LLMs) requires significant computational resources and is time-consuming.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Fine-tuning all parameters of large language models (LLMs) requires significant computational resources and is time-consuming.\""
    },
    {
        "title": "CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models",
        "authors": [
            "Benjamin Minixhofer",
            "Jonas Pfeiffer",
            "Ivan Vulić"
        ],
        "published": "2023",
        "summary": "While many languages possess processes of joining two or more words to create compound words, previous studies have been typically limited only to languages with excessively productive compound formation (e.g., German, Dutch) and there is no public dataset containing compound and non-compound words across a large number of languages. In this work, we systematically study decompounding, the task of splitting compound words into their constituents, at a wide scale. We first address the data gap by introducing a dataset of 255k compound and non-compound words across 56 diverse languages obtained from Wiktionary. We then use this dataset to evaluate an array of Large Language Models (LLMs) on the decompounding task. We find that LLMs perform poorly, especially on words which are tokenized unfavorably by subword tokenization. We thus introduce a novel methodology to train dedicated models for decompounding. The proposed two-stage procedure relies on a fully self-supervised objective in the first stage, while the second, supervised learning stage optionally fine-tunes the model on the annotated Wiktionary data. Our self-supervised models outperform the prior best unsupervised decompounding models by 13.9% accuracy on average. Our fine-tuned models outperform all prior (language-specific) decompounding tools. Furthermore, we use our models to leverage decompounding during the creation of a subword tokenizer, which we refer to as CompoundPiece. CompoundPiece tokenizes compound words more favorably on average, leading to improved performance on decompounding over an otherwise equivalent model using SentencePiece tokenization.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.24.pdf",
        "keywords": [
            "compoundpiece",
            "compound words",
            "decompounding",
            "languages",
            "words",
            "language models",
            "decompounding task",
            "decompounding performance",
            "self supervised",
            "compound formation",
            "subword tokenization",
            "subword",
            "tuned"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that LLMs perform poorly, especially on words which are tokenized unfavorably by subword tokenization.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We find that LLMs perform poorly, especially on words which are tokenized unfavorably by subword tokenization.\""
    },
    {
        "title": "GATITOS: Using a New Multilingual Lexicon for Low-resource Machine Translation",
        "authors": [
            "Alexander Jones",
            "Isaac Caswell",
            "Orhan Firat",
            "Ishank Saxena"
        ],
        "published": "2023",
        "summary": "Modern machine translation models and language models are able to translate without having been trained on parallel data, greatly expanding the set of languages that they can serve. However, these models still struggle in a variety of predictable ways, a problem that cannot be overcome without at least some trusted bilingual data. This work expands on a cheap and abundant resource to combat this problem: bilingual lexica. We test the efficacy of bilingual lexica in a real-world set-up, on 200-language translation models trained on web-crawled text. We present several findings: (1) using lexical data augmentation, we demonstrate sizable performance gains for unsupervised translation; (2) we compare several families of data augmentation, demonstrating that they yield similar improvements, and can be combined for even greater improvements; (3) we demonstrate the importance of carefully curated lexica over larger, noisier ones, especially with larger models; and (4) we compare the efficacy of multilingual lexicon data versus human-translated parallel data. Based on results from (3), we develop and open-source GATITOS, a high-quality, curated dataset in 168 tail languages, one of the first human-translated resources to cover many of these languages.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.26.pdf",
        "keywords": [
            "lexica",
            "language translation",
            "translate",
            "machine translation",
            "bilingual lexica",
            "lexicon",
            "low resource machine translation",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these models still struggle in a variety of predictable ways, a problem that cannot be overcome without at least some trusted bilingual data.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these models still struggle in a variety of predictable ways, a problem that cannot be overcome without at least some trusted bilingual data.\""
    },
    {
        "title": "Bootstrapping Small & High Performance Language Models with Unmasking-Removal Training Policy",
        "authors": [
            "Yahan Yang",
            "Elior Sulem",
            "Insup Lee",
            "Dan Roth"
        ],
        "published": "2023",
        "summary": "BabyBERTa, a language model trained on small-scale child-directed speech while none of the words are unmasked during training, has been shown to achieve a level of grammaticality comparable to that of RoBERTa-base, which is trained on 6,000 times more words and 15 times more parameters. Relying on this promising result, we explore in this paper the performance of BabyBERTa-based models in downstream tasks, focusing on Semantic Role Labeling (SRL) and two Extractive Question Answering tasks, with the aim of building more efficient systems that rely on less data and smaller models. We investigate the influence of these models both alone and as a starting point to larger pre-trained models, separately examining the contribution of the pre-training data, the vocabulary, and the masking policy on the downstream task performance. Our results show that BabyBERTa trained with unmasking-removal policy is a much stronger starting point for downstream tasks compared to the use of RoBERTa masking policy when 10M words are used for training and that this tendency persists, although to a lesser extent, when adding more training data.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.30.pdf",
        "keywords": [
            "masking policy",
            "roberta masking policy",
            "unmasking removal policy",
            "extractive",
            "grammaticality",
            "model"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper discusses the performance of smaller models (BabyBERTa) in comparison to larger models (RoBERTa-base), which can be seen as a minor discussion of a limitation of larger models, but it is not explicitly stated.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper discusses the performance of smaller models (BabyBERTa) in comparison to larger models (RoBERTa-base), which can be seen as a minor discussion of a limitation of larger models, but it is not explicitly stated."
    },
    {
        "title": "How to Enhance Causal Discrimination of Utterances: A Case on Affective Reasoning",
        "authors": [
            "Hang Chen",
            "Xinyu Yang",
            "Jing Luo",
            "Wenjing Zhu"
        ],
        "published": "2023",
        "summary": "Our investigation into the Affective Reasoning in Conversation (ARC) task highlights the challenge of causal discrimination. Almost all existing models, including large language models (LLMs), excel at capturing semantic correlations within utterance embeddings but fall short in determining the specific causal relationships. To overcome this limitation, we propose the incorporation of i.i.d. noise terms into the conversation process, thereby constructing a structural causal model (SCM). It explores how distinct causal relationships of fitted embeddings can be discerned through independent conditions. To facilitate the implementation of deep learning, we introduce the cogn frameworks to handle unstructured conversation data, and employ an autoencoder architecture to regard the unobservable noise as learnable “implicit causes.” Moreover, we curate a synthetic dataset that includes i.i.d. noise. Through comprehensive experiments, we validate the effectiveness and interpretability of our approach. Our code is available in https://github.com/Zodiark-ch/mater-of-our-EMNLP2023-paper.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.33.pdf",
        "keywords": [
            "causal discrimination",
            "affective reasoning",
            "structural causal model",
            "causal relationships",
            "correlations"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Almost all existing models, including large language models (LLMs), excel at capturing semantic correlations within utterance embeddings but fall short in determining the specific causal relationships.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Almost all existing models, including large language models (LLMs), excel at capturing semantic correlations within utterance embeddings but fall short in determining the specific causal relationships.\""
    },
    {
        "title": "Selectively Answering Ambiguous Questions",
        "authors": [
            "Jeremy Cole",
            "Michael Zhang",
            "Daniel Gillick",
            "Julian Eisenschlos",
            "Bhuwan Dhingra",
            "Jacob Eisenstein"
        ],
        "published": "2023",
        "summary": "Trustworthy language models should abstain from answering questions when they do not know the answer. However, the answer to a question can be unknown for a variety of reasons. Prior research has focused on the case in which the question is clear and the answer is unambiguous but possibly unknown. However, the answer to a question can also be unclear due to uncertainty of the questioner’s intent or context. We investigate question answering from this perspective, focusing on answering a subset of questions with a high degree of accuracy, from a set of questions in which many are inherently ambiguous. In this setting, we find that the most reliable approach to calibration involves quantifying repetition within a set of sampled model outputs, rather than the model’s likelihood or self-verification as used in prior work. We find this to be the case across different types of uncertainty, varying model scales and both with or without instruction tuning. Our results suggest that sampling-based confidence scores help calibrate answers to relatively unambiguous questions, with more dramatic improvements on ambiguous questions.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.35.pdf",
        "keywords": [
            "language models"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We investigate question answering from this perspective, focusing on answering a subset of questions with a high degree of accuracy, from a set of questions in which many are inherently ambiguous.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We investigate question answering from this perspective, focusing on answering a subset of questions with a high degree of accuracy, from a set of questions in which many are inherently ambiguous.\""
    },
    {
        "title": "Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning",
        "authors": [
            "Dong-Ho Lee",
            "Kian Ahrabian",
            "Woojeong Jin",
            "Fred Morstatter",
            "Jay Pujara"
        ],
        "published": "2023",
        "summary": "Temporal knowledge graph (TKG) forecasting benchmarks challenge models to predict future facts using knowledge of past facts. In this paper, we develop an approach to use in-context learning (ICL) with large language models (LLMs) for TKG forecasting. Our extensive evaluation compares diverse baselines, including both simple heuristics and state-of-the-art (SOTA) supervised models, against pre-trained LLMs across several popular benchmarks and experimental settings. We observe that naive LLMs perform on par with SOTA models, which employ carefully designed architectures and supervised training for the forecasting task, falling within the (-3.6%, +1.5%) Hits@1 margin relative to the median performance. To better understand the strengths of LLMs for forecasting, we explore different approaches for selecting historical facts, constructing prompts, controlling information propagation, and parsing outputs into a probability distribution. A surprising finding from our experiments is that LLM performance endures (±0.4% Hit@1) even when semantic information is removed by mapping entities/relations to arbitrary numbers, suggesting that prior semantic knowledge is unnecessary; rather, LLMs can leverage the symbolic patterns in the context to achieve such a strong performance. Our analysis also reveals that ICL enables LLMs to learn irregular patterns from the historical context, going beyond frequency and recency biases",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.36.pdf",
        "keywords": [
            "context learning",
            "language models",
            "in context learning",
            "semantic knowledge",
            "temporal knowledge graph",
            "tkg forecasting"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the abstract implies that LLMs are not limited by prior semantic knowledge and can leverage symbolic patterns in the context to achieve strong performance, which is more of a positive aspect of LLMs rather than a limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the abstract implies that LLMs are not limited by prior semantic knowledge and can leverage symbolic patterns in the context to achieve strong performance, which is more of a positive aspect of LLMs rather than a limitation."
    },
    {
        "title": "Knowledge Graph Compression Enhances Diverse Commonsense Generation",
        "authors": [
            "EunJeong Hwang",
            "Veronika Thost",
            "Vered Shwartz",
            "Tengfei Ma"
        ],
        "published": "2023",
        "summary": "Generating commonsense explanations requires reasoning about commonsense knowledge beyond what is explicitly mentioned in the context. Existing models use commonsense knowledge graphs such as ConceptNet to extract a subgraph of relevant knowledge pertaining to concepts in the input. However, due to the large coverage and, consequently, vast scale of ConceptNet, the extracted subgraphs may contain loosely related, redundant and irrelevant information, which can introduce noise into the model. We propose to address this by applying a differentiable graph compression algorithm that focuses on the relevant knowledge for the task. The compressed subgraphs yield considerably more diverse outputs when incorporated into models for the tasks of generating commonsense and abductive explanations. Moreover, our model achieves better quality-diversity tradeoff than a large language model with 100 times the number of parameters. Our generic approach can be applied to additional NLP tasks that can benefit from incorporating external knowledge.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.37.pdf",
        "keywords": [
            "commonsense",
            "commonsense knowledge graphs",
            "commonsense knowledge",
            "diverse commonsense generation",
            "knowledge graph compression",
            "graph compression",
            "abductive explanations",
            "commonsense explanations"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"our model achieves better quality-diversity tradeoff than a large language model with 100 times the number of parameters.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"our model achieves better quality-diversity tradeoff than a large language model with 100 times the number of parameters.\""
    },
    {
        "title": "LLM-FP4: 4-Bit Floating-Point Quantized Transformers",
        "authors": [
            "Shih-yang Liu",
            "Zechun Liu",
            "Xijie Huang",
            "Pingcheng Dong",
            "Kwang-Ting Cheng"
        ],
        "published": "2023",
        "summary": "We propose LLM-FP4 for quantizing both weights and activations in large language models (LLMs) down to 4-bit floating-point values, in a post-training manner. Existing post-training quantization (PTQ) solutions are primarily integer-based and struggle with bit widths below 8 bits. Compared to integer quantization, floating-point (FP) quantization is more flexible and can better handle long-tail or bell-shaped distributions, and it has emerged as a default choice in many hardware platforms. One characteristic of FP quantization is that its performance largely depends on the choice of exponent bits and clipping range. In this regard, we construct a strong FP-PTQ baseline by searching for the optimal quantization parameters. Furthermore, we observe a high inter-channel variance and low intra-channel variance pattern in activation distributions, which adds activation quantization difficulty. We recognize this pattern to be consistent across a spectrum of transformer models designed for diverse tasks such as LLMs, BERT, and Vision Transformer models. To tackle this, we propose per-channel activation quantization and show that these additional scaling factors can be reparameterized as exponential biases of weights, incurring a negligible cost. Our method, for the first time, can quantize both weights and activations in the LLaMA-13B to only 4-bit and achieves an average score of 63.1 on the common sense zero-shot reasoning tasks, which is only 5.8 lower than the full-precision model, significantly outperforming the previous state-of-the-art by 12.7 points. Code is available at: https://github.com/nbasyl/LLM-FP4.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.39.pdf",
        "keywords": [
            "quantization",
            "fp quantization",
            "activation quantization",
            "floating point quantized transformers",
            "floating",
            "fp4"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing post-training quantization (PTQ) solutions are primarily integer-based and struggle with bit widths below 8 bits.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing post-training quantization (PTQ) solutions are primarily integer-based and struggle with bit widths below 8 bits.\""
    },
    {
        "title": "Improving Biomedical Abstractive Summarisation with Knowledge Aggregation from Citation Papers",
        "authors": [
            "Chen Tang",
            "Shun Wang",
            "Tomas Goldsack",
            "Chenghua Lin"
        ],
        "published": "2023",
        "summary": "Abstracts derived from biomedical literature possess distinct domain-specific characteristics, including specialised writing styles and biomedical terminologies, which necessitate a deep understanding of the related literature. As a result, existing language models struggle to generate technical summaries that are on par with those produced by biomedical experts, given the absence of domain-specific background knowledge. This paper aims to enhance the performance of language models in biomedical abstractive summarisation by aggregating knowledge from external papers cited within the source article. We propose a novel attention-based citation aggregation model that integrates domain-specific knowledge from citation papers, allowing neural networks to generate summaries by leveraging both the paper content and relevant knowledge from citation papers. Furthermore, we construct and release a large-scale biomedical summarisation dataset that serves as a foundation for our research. Extensive experiments demonstrate that our model outperforms state-of-the-art approaches and achieves substantial improvements in abstractive biomedical text summarisation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.40.pdf",
        "keywords": [
            "citation aggregation",
            "knowledge aggregation",
            "summarisation",
            "text summarisation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"given the absence of domain-specific background knowledge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"given the absence of domain-specific background knowledge.\""
    },
    {
        "title": "Explanation Selection Using Unlabeled Data for Chain-of-Thought Prompting",
        "authors": [
            "Xi Ye",
            "Greg Durrett"
        ],
        "published": "2023",
        "summary": "Recent work has shown how to prompt large language models with explanations to obtain strong performance on textual reasoning tasks, i.e., the chain-of-thought paradigm. However, subtly different explanations can yield widely varying downstream task accuracy. Explanations that have not been “tuned” for a task, such as off-the-shelf explanations written by non-experts, may lead to mediocre performance. This paper tackles the problem of how to optimize explanation-infused prompts in a blackbox fashion. We first generate sets of candidate explanations for each example in the prompt using a leave-one-out scheme, then find an effective combination of these explanations with a two-stage framework. We first evaluate explanations for each in-context example in isolation according to two proxy metrics, log likelihood and accuracy on new examples. Then, we search over combinations of explanations to find one that yields high performance against a silver-labeled development set. Across four textual reasoning tasks spanning question answering, mathematical reasoning, and natural language inference, results show that our proxy metrics correlate with ground truth accuracy and our overall method can effectively improve prompts over crowdworker annotations and naive search strategies",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.41.pdf",
        "keywords": [
            "prompts",
            "explanations",
            "log likelihood",
            "blackbox",
            "mathematical reasoning",
            "language models",
            "thought prompting",
            "natural language inference",
            "accuracy",
            "isolation",
            "shelf explanations",
            "evaluate explanations",
            "explanation selection",
            "textual"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Explanations that have not been “tuned” for a task, such as off-the-shelf explanations written by non-experts, may lead to mediocre performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Explanations that have not been “tuned” for a task, such as off-the-shelf explanations written by non-experts, may lead to mediocre performance.\""
    },
    {
        "title": "HalOmi: A Manually Annotated Benchmark for Multilingual Hallucination and Omission Detection in Machine Translation",
        "authors": [
            "David Dale",
            "Elena Voita",
            "Janice Lam",
            "Prangthip Hansanti",
            "Christophe Ropers",
            "Elahe Kalbassi",
            "Cynthia Gao",
            "Loic Barrault",
            "Marta Costa-jussà"
        ],
        "published": "2023",
        "summary": "Hallucinations in machine translation are translations that contain information completely unrelated to the input. Omissions are translations that do not include some of the input information. While both cases tend to be catastrophic errors undermining user trust, annotated data with these types of pathologies is extremely scarce and is limited to a few high-resource languages. In this work, we release an annotated dataset for the hallucination and omission phenomena covering 18 translation directions with varying resource levels and scripts. Our annotation covers different levels of partial and full hallucinations as well as omissions both at the sentence and at the word level. Additionally, we revisit previous methods for hallucination and omission detection, show that conclusions made based on a single language pair largely do not hold for a large-scale evaluation, and establish new solid baselines.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.42.pdf",
        "keywords": [
            "hallucination",
            "machine translation",
            "baselines",
            "halomi",
            "multilingual hallucination",
            "detection"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "LLM-powered Data Augmentation for Enhanced Cross-lingual Performance",
        "authors": [
            "Chenxi Whitehouse",
            "Monojit Choudhury",
            "Alham Fikri Aji"
        ],
        "published": "2023",
        "summary": "This paper explores the potential of leveraging Large Language Models (LLMs) for data augmentation in multilingual commonsense reasoning datasets where the available training data is extremely limited. To achieve this, we utilise several LLMs, namely Dolly-v2, StableVicuna, ChatGPT, and GPT-4, to augment three datasets: XCOPA, XWinograd, and XStoryCloze. Subsequently, we evaluate the effectiveness of fine-tuning smaller multilingual models, mBERT and XLMR, using the synthesised data. We compare the performance of training with data generated in English and target languages, as well as translated English-generated data, revealing the overall advantages of incorporating data generated by LLMs, e.g. a notable 13.4 accuracy score improvement for the best case. Furthermore, we conduct a human evaluation by asking native speakers to assess the naturalness and logical coherence of the generated examples across different languages. The results of the evaluation indicate that LLMs such as ChatGPT and GPT-4 excel at producing natural and coherent text in most languages, however, they struggle to generate meaningful text in certain languages like Tamil. We also observe that ChatGPT falls short in generating plausible alternatives compared to the original dataset, whereas examples from GPT-4 exhibit competitive logical consistency.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.44.pdf",
        "keywords": [
            "data augmentation"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"they struggle to generate meaningful text in certain languages like Tamil. We also observe that ChatGPT falls short in generating plausible alternatives compared to the original dataset, whereas examples from GPT-4 exhibit competitive logical consistency.\"\n\nThis rating is given because the paper discusses the limitations of LLMs in generating text in certain languages and in producing plausible alternatives, but the primary focus is on the",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"they struggle to generate meaningful text in certain languages like Tamil. We also observe that ChatGPT falls short in generating plausible alternatives compared to the original dataset, whereas examples from GPT-4 exhibit competitive logical consistency.\"\n\nThis rating is given because the paper discusses the limitations of LLMs in generating text in certain languages and in producing plausible alternatives, but the primary focus is on the"
    },
    {
        "title": "Prompt-based Logical Semantics Enhancement for Implicit Discourse Relation Recognition",
        "authors": [
            "Chenxu Wang",
            "Ping Jian",
            "Mu Huang"
        ],
        "published": "2023",
        "summary": "Implicit Discourse Relation Recognition (IDRR), which infers discourse relations without the help of explicit connectives, is still a crucial and challenging task for discourse parsing. Recent works tend to exploit the hierarchical structure information from the annotated senses, which demonstrate enhanced discourse relation representations can be obtained by integrating sense hierarchy. Nevertheless, the performance and robustness for IDRR are significantly constrained by the availability of annotated data. Fortunately, there is a wealth of unannotated utterances with explicit connectives, that can be utilized to acquire enriched discourse relation features. In light of such motivation, we propose a Prompt-based Logical Semantics Enhancement (PLSE) method for IDRR. Essentially, our method seamlessly injects knowledge relevant to discourse relation into pre-trained language models through prompt-based connective prediction. Furthermore, considering the prompt-based connective prediction exhibits local dependencies due to the deficiency of masked language model (MLM) in capturing global semantics, we design a novel self-supervised learning objective based on mutual information maximization to derive enhanced representations of logical semantics for IDRR. Experimental results on PDTB 2.0 and CoNLL16 datasets demonstrate that our method achieves outstanding and consistent performance against the current state-of-the-art models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.45.pdf",
        "keywords": [
            "relation",
            "connective prediction",
            "discourse relation",
            "discourse relation recognition",
            "logical semantics",
            "mutual information",
            "implicit discourse relation recognition",
            "based connective prediction"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"considering the prompt-based connective prediction exhibits local dependencies due to the deficiency of masked language model (MLM) in capturing global semantics\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"considering the prompt-based connective prediction exhibits local dependencies due to the deficiency of masked language model (MLM) in capturing global semantics\""
    },
    {
        "title": "VLIS: Unimodal Language Models Guide Multimodal Language Generation",
        "authors": [
            "Jiwan Chung",
            "Youngjae Yu"
        ],
        "published": "2023",
        "summary": "Multimodal language generation, which leverages the synergy of language and vision, is a rapidly expanding field. However, existing vision-language models face challenges in tasks that require complex linguistic understanding. To address this issue, we introduce Visual-Language models as Importance Sampling weights (VLIS), a novel framework that combines the visual conditioning capability of vision-language models with the language understanding of unimodal text-only language models without further training. It extracts pointwise mutual information of each image and text from a visual-language model and uses the value as an importance sampling weight to adjust the token likelihood from a text-only model. VLIS improves vision-language models on diverse tasks, including commonsense understanding (WHOOPS, OK-VQA, and ScienceQA) and complex text generation (Concadia, Image Paragraph Captioning, and ROCStories). Our results suggest that VLIS represents a promising new direction for multimodal language generation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.46.pdf",
        "keywords": [
            "language",
            "multimodal language generation",
            "language models",
            "unimodal language models",
            "visual language models",
            "unimodal text",
            "vision language models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing vision-language models face challenges in tasks that require complex linguistic understanding.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, existing vision-language models face challenges in tasks that require complex linguistic understanding.\""
    },
    {
        "title": "Conceptual structure coheres in human cognition but not in large language models",
        "authors": [
            "Siddharth Suresh",
            "Kushin Mukherjee",
            "Xizheng Yu",
            "Wei-Chun Huang",
            "Lisa Padua",
            "Timothy Rogers"
        ],
        "published": "2023",
        "summary": "Neural network models of language have long been used as a tool for developing hypotheses about conceptual representation in the mind and brain. For many years, such use involved extracting vector-space representations of words and using distances among these to predict or understand human behavior in various semantic tasks. In contemporary language models, however, it is possible to interrogate the latent structure of conceptual representations using methods nearly identical to those commonly used with human participants. The current work uses three common techniques borrowed from cognitive psychology to estimate and compare lexical-semantic structure in both humans and a well-known large language model, the DaVinci variant of GPT-3. In humans, we show that conceptual structure is robust to differences in culture, language, and method of estimation. Structures estimated from the LLM behavior, while individually fairly consistent with those estimated from human behavior, depend much more upon the particular task used to generate behavior responses–responses generated by the very same model in the three tasks yield estimates of conceptual structure that cohere less with one another than do human structure estimates. The results suggest one important way that knowledge inhering in contemporary LLMs can differ from human cognition.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.47.pdf",
        "keywords": [
            "cohere",
            "cognition",
            "conceptual structure coheres",
            "language",
            "cognitive psychology",
            "conceptual structure",
            "method of estimation",
            "knowledge inhering",
            "neural network",
            "language model"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Structures estimated from the LLM behavior, while individually fairly consistent with those estimated from human behavior, depend much more upon the particular task used to generate behavior responses–responses generated by the very same model in the three tasks yield estimates of conceptual structure that cohere less with one another than do human structure estimates.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Structures estimated from the LLM behavior, while individually fairly consistent with those estimated from human behavior, depend much more upon the particular task used to generate behavior responses–responses generated by the very same model in the three tasks yield estimates of conceptual structure that cohere less with one another than do human structure estimates.\""
    },
    {
        "title": "Towards LLM-driven Dialogue State Tracking",
        "authors": [
            "Yujie Feng",
            "Zexin Lu",
            "Bo Liu",
            "Liming Zhan",
            "Xiao-Ming Wu"
        ],
        "published": "2023",
        "summary": "Dialogue State Tracking (DST) is of paramount importance in ensuring accurate tracking of user goals and system actions within task-oriented dialogue systems. The emergence of large language models (LLMs) such as GPT3 and ChatGPT has sparked considerable interest in assessing their efficacy across diverse applications. In this study, we conduct an initial examination of ChatGPT’s capabilities in DST. Our evaluation uncovers the exceptional performance of ChatGPT in this task, offering valuable insights to researchers regarding its capabilities and providing useful directions for designing and enhancing dialogue systems. Despite its impressive performance, ChatGPT has significant limitations including its closed-source nature, request restrictions, raising data privacy concerns, and lacking local deployment capabilities. To address these concerns, we present LDST, an LLM-driven DST framework based on smaller, open-source foundation models. By utilizing a novel domain-slot instruction tuning method, LDST achieves performance on par with ChatGPT. Comprehensive evaluations across three distinct experimental settings, we find that LDST exhibits remarkable performance improvements in both zero-shot and few-shot setting compared to previous SOTA methods. The source code is provided for reproducibility.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.48.pdf",
        "keywords": [
            "dialogue",
            "dialogue state tracking",
            "dialogue systems",
            "chatgpt"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite its impressive performance, ChatGPT has significant limitations including its closed-source nature, request restrictions, raising data privacy concerns, and lacking local deployment capabilities.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Despite its impressive performance, ChatGPT has significant limitations including its closed-source nature, request restrictions, raising data privacy concerns, and lacking local deployment capabilities.\""
    },
    {
        "title": "Multitask Multimodal Prompted Training for Interactive Embodied Task Completion",
        "authors": [
            "Georgios Pantazopoulos",
            "Malvina Nikandrou",
            "Amit Parekh",
            "Bhathiya Hemanthage",
            "Arash Eshghi",
            "Ioannis Konstas",
            "Verena Rieser",
            "Oliver Lemon",
            "Alessandro Suglia"
        ],
        "published": "2023",
        "summary": "Interactive and embodied tasks pose at least two fundamental challenges to existing Vision & Language (VL) models, including 1) grounding language in trajectories of actions and observations, and 2) referential disambiguation. To tackle these challenges, we propose an Embodied MultiModal Agent (EMMA): a unified encoder-decoder model that reasons over images and trajectories, and casts action prediction as multimodal text generation. By unifying all tasks as text generation, EMMA learns a language of actions which facilitates transfer across tasks. Different to previous modular approaches with independently trained components, we use a single multitask model where each task contributes to goal completion. EMMA performs on par with similar models on several VL benchmarks and sets a new state-of-the-art performance (36.81% success rate) on the Dialog-guided Task Completion (DTC), a benchmark to evaluate dialog-guided agents in the Alexa Arena.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.50.pdf",
        "keywords": [
            "trajectories",
            "embodied multimodal agent",
            "multitask",
            "embodied task completion"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Interactive and embodied tasks pose at least two fundamental challenges to existing Vision & Language (VL) models, including",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Interactive and embodied tasks pose at least two fundamental challenges to existing Vision & Language (VL) models, including"
    },
    {
        "title": "We’re Afraid Language Models Aren’t Modeling Ambiguity",
        "authors": [
            "Alisa Liu",
            "Zhaofeng Wu",
            "Julian Michael",
            "Alane Suhr",
            "Peter West",
            "Alexander Koller",
            "Swabha Swayamdipta",
            "Noah Smith",
            "Yejin Choi"
        ],
        "published": "2023",
        "summary": "Ambiguity is an intrinsic feature of natural language. Managing ambiguity is a key part of human language understanding, allowing us to anticipate misunderstanding as communicators and revise our interpretations as listeners. As language models are increasingly employed as dialogue interfaces and writing aids, handling ambiguous language is critical to their success. We capture ambiguity in a sentence through its effect on entailment relations with another sentence, and collect AmbiEnt, a linguist-annotated benchmark of 1,645 examples with diverse kinds of ambiguity. We design a suite of tests based on AmbiEnt, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings. We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32% of the time in crowdworker evaluation, compared to 90% for disambiguations in our dataset. Finally, to illustrate the value of ambiguity-sensitive tools, we show that a multilabel NLI model can flag political claims in the wild that are misleading due to ambiguity. We encourage the field to rediscover the importance of ambiguity for NLP.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.51.pdf",
        "keywords": [
            "ambiguity",
            "crowdworker",
            "language models",
            "re afraid language models",
            "linguist"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32% of the time in crowdworker evaluation, compared to 90% for disambiguations in our dataset.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32% of the time in crowdworker evaluation, compared to 90% for disambiguations in our dataset.\""
    },
    {
        "title": "Fidelity-Enriched Contrastive Search: Reconciling the Faithfulness-Diversity Trade-Off in Text Generation",
        "authors": [
            "Wei-Lin Chen",
            "Cheng-Kuang Wu",
            "Hsin-Hsi Chen",
            "Chung-Chi Chen"
        ],
        "published": "2023",
        "summary": "In this paper, we address the hallucination problem commonly found in natural language generation tasks. Language models often generate fluent and convincing content but can lack consistency with the provided source, resulting in potential inaccuracies. We propose a new decoding method called Fidelity-Enriched Contrastive Search (FECS), which augments the contrastive search framework with context-aware regularization terms. FECS promotes tokens that are semantically similar to the provided source while penalizing repetitiveness in the generated text. We demonstrate its effectiveness across two tasks prone to hallucination: abstractive summarization and dialogue generation. Results show that FECS consistently enhances faithfulness across various language model sizes while maintaining output diversity comparable to well-performing decoding algorithms.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.54.pdf",
        "keywords": [
            "faithfulness",
            "hallucination",
            "fidelity",
            "contrastive search",
            "faithfulness diversity",
            "text generation",
            "context aware",
            "fluent",
            "convincing",
            "summarization",
            "natural language generation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Language models often generate fluent and convincing content but can lack consistency with the provided source, resulting in potential inaccuracies.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Language models often generate fluent and convincing content but can lack consistency with the provided source, resulting in potential inaccuracies.\""
    },
    {
        "title": "Analyzing Norm Violations in Live-Stream Chat",
        "authors": [
            "Jihyung Moon",
            "Dong-Ho Lee",
            "Hyundong Cho",
            "Woojeong Jin",
            "Chan Park",
            "Minwoo Kim",
            "Jonathan May",
            "Jay Pujara",
            "Sungjoon Park"
        ],
        "published": "2023",
        "summary": "Toxic language, such as hate speech, can deter users from participating in online communities and enjoying popular platforms. Previous approaches to detecting toxic language and norm violations have been primarily concerned with conversations from online forums and social media, such as Reddit and Twitter. These approaches are less effective when applied to conversations on live-streaming platforms, such as Twitch and YouTube Live, as each comment is only visible for a limited time and lacks a thread structure that establishes its relationship with other comments. In this work, we share the first NLP study dedicated to detecting norm violations in conversations on live-streaming platforms. We define norm violation categories in live-stream chats and annotate 4,583 moderated comments from Twitch. We articulate several facets of live-stream data that differ from other forums, and demonstrate that existing models perform poorly in this setting. By conducting a user study, we identify the informational context humans use in live-stream moderation, and train models leveraging context to identify norm violations. Our results show that appropriate contextual information can boost moderation performance by 35%.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.55.pdf",
        "keywords": [
            "norm violations",
            "live stream chat",
            "live stream moderation",
            "toxic language",
            "moderation",
            "norm violation categories"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality",
        "authors": [
            "Harman Singh",
            "Pengchuan Zhang",
            "Qifan Wang",
            "Mengjiao Wang",
            "Wenhan Xiong",
            "Jingfei Du",
            "Yu Chen"
        ],
        "published": "2023",
        "summary": "Contrastively trained vision-language models have achieved remarkable progress in vision and language representation learning. However, recent research has highlighted severe limitations of these models in their ability to perform compositional reasoning over objects, attributes, and relations. Scene graphs have emerged as an effective way to understand images compositionally. These are graph-structured semantic representations of images that contain objects, their attributes, and relations with other objects in a scene. In this work, we consider the scene graph parsed from text as a proxy for the image scene graph and propose a graph decomposition and augmentation framework along with a coarse-to-fine contrastive learning objective between images and text that aligns sentences of various complexities to the same image. We also introduce novel negative mining techniques in the scene graph space for improving attribute binding and relation understanding. Through extensive experiments, we demonstrate the effectiveness of our approach that significantly improves attribute binding, relation understanding, systematic generalization, and productivity on multiple recently proposed benchmarks (For example, improvements up to 18% for systematic generalization, 16.5% for relation understanding over a strong baseline), while achieving similar or better performance than CLIP on various general multimodal tasks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.56.pdf",
        "keywords": [
            "compositionality",
            "negative mining",
            "trained vision"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, recent research has highlighted severe limitations of these models in their ability to perform compositional reasoning over objects, attributes, and relations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, recent research has highlighted severe limitations of these models in their ability to perform compositional reasoning over objects, attributes, and relations.\""
    },
    {
        "title": "Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms",
        "authors": [
            "Seungju Han",
            "Junhyeok Kim",
            "Jack Hessel",
            "Liwei Jiang",
            "Jiwan Chung",
            "Yejin Son",
            "Yejin Choi",
            "Youngjae Yu"
        ],
        "published": "2023",
        "summary": "Commonsense norms are defeasible by context: reading books is usually great, but not when driving a car. While contexts can be explicitly described in language, in embodied scenarios, contexts are often provided visually. This type of visually grounded reasoning about defeasible commonsense norms is generally easy for humans, but (as we show) poses a challenge for machines, as it necessitates both visual understanding and reasoning about commonsense norms. We construct a new multimodal benchmark for studying commonsense norms: NormLens. NormLens consists of 10K human judgments accompanied by free-form explanations covering 2K multimodal situations, and serves as a probe to address two questions: (1) to what extent can models align with average human judgment? and (2) how well can models explain their predicted judgments? We find that state-of-the-art model judgments and explanations are not well-aligned with human annotation. Additionally, we present a simple yet effective approach to better align models with humans by distilling social commonsense knowledge from large language models. The data and code will be released.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.57.pdf",
        "keywords": [
            "reading books",
            "commonsense norms",
            "commonsense",
            "explanations",
            "defeasible commonsense norms",
            "judgment",
            "defeasible",
            "visually grounded reasoning",
            "reasoning",
            "annotation",
            "context"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that state-of-the-art model judgments and explanations are not well-aligned with human annotation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We find that state-of-the-art model judgments and explanations are not well-aligned with human annotation.\""
    },
    {
        "title": "Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus",
        "authors": [
            "Tianhang Zhang",
            "Lin Qiu",
            "Qipeng Guo",
            "Cheng Deng",
            "Yue Zhang",
            "Zheng Zhang",
            "Chenghu Zhou",
            "Xinbing Wang",
            "Luoyi Fu"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have gained significant popularity for their impressive performance across diverse fields. However, LLMs are prone to hallucinate untruthful or nonsensical outputs that fail to meet user expectations in many real-world applications. Existing works for detecting hallucinations in LLMs either rely on external knowledge for reference retrieval or require sampling multiple responses from the LLM for consistency verification, making these methods costly and inefficient. In this paper, we propose a novel reference-free, uncertainty-based method for detecting hallucinations in LLMs. Our approach imitates human focus in factuality checking from three aspects: 1) focus on the most informative and important keywords in the given text; 2) focus on the unreliable tokens in historical context which may lead to a cascade of hallucinations; and 3) focus on the token properties such as token type and token frequency. Experimental results on relevant datasets demonstrate the effectiveness of our proposed method, which achieves state-of-the-art performance across all the evaluation metrics and eliminates the need for additional information.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.58.pdf",
        "keywords": [
            "hallucination detection",
            "factuality checking",
            "uncertainty"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, LLMs are prone to hallucinate untruthful or nonsensical outputs that fail to meet user expectations in many real-world applications.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, LLMs are prone to hallucinate untruthful or nonsensical outputs that fail to meet user expectations in many real-world applications.\""
    },
    {
        "title": "FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge",
        "authors": [
            "Shangbin Feng",
            "Vidhisha Balachandran",
            "Yuyang Bai",
            "Yulia Tsvetkov"
        ],
        "published": "2023",
        "summary": "Evaluating the factual consistency of automatically generated summaries is essential for the progress and adoption of reliable summarization systems. Despite recent advances, existing factuality evaluation models are not robust, being especially prone to entity and relation errors in new domains. We propose FactKB—a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations. FactKB is based on language models pretrained using facts extracted from external knowledge bases. We introduce three types of complementary factuality pretraining objectives based on entity-specific facts, facts extracted from auxiliary knowledge about entities, and facts constructed compositionally through knowledge base walks. The resulting factuality evaluation model achieves state-of-the-art performance on two in-domain news summarization benchmarks as well as on three out-of-domain scientific literature datasets. Further analysis of FactKB shows improved ability to detect erroneous entities and relations in summaries and is robust and easily generalizable across domains.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.59.pdf",
        "keywords": [
            "factuality",
            "factual consistency",
            "language models",
            "factual knowledge",
            "factuality evaluation",
            "summarization",
            "generalizable factuality",
            "generalizable",
            "factkb"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite recent advances, existing factuality evaluation models are not robust, being especially prone to entity and relation errors in new domains.\"\n\nThis abstract mentions limitations of existing factuality evaluation models, but it does not specifically focus on LLMs limitations, and the discussion of limitations is brief and used to motivate the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite recent advances, existing factuality evaluation models are not robust, being especially prone to entity and relation errors in new domains.\"\n\nThis abstract mentions limitations of existing factuality evaluation models, but it does not specifically focus on LLMs limitations, and the discussion of limitations is brief and used to motivate the proposed solution."
    },
    {
        "title": "Symbol tuning improves in-context learning in language models",
        "authors": [
            "Jerry Wei",
            "Le Hou",
            "Andrew Lampinen",
            "Xiangning Chen",
            "Da Huang",
            "Yi Tay",
            "Xinyun Chen",
            "Yifeng Lu",
            "Denny Zhou",
            "Tengyu Ma",
            "Quoc Le"
        ],
        "published": "2023",
        "summary": "We present symbol tuning - finetuning language models on in-context input-label pairs where natural language labels (e.g., “positive/negative sentiment”) are replaced with arbitrary symbols (e.g., “foo/bar”). Symbol tuning leverages the intuition that when a model cannot use instructions or natural language labels to figure out a task, it must instead do so by learning the input-label mappings. We experiment with symbol tuning across PaLM models up to 540B parameters and observe benefits across various settings. First, symbol tuning boosts performance on unseen in-context learning tasks and is much more robust to underspecified prompts, such as those without instructions or without natural language labels. Second, symbol-tuned models are much stronger at algorithmic reasoning tasks, with up to 18.2% better performance on the List Functions benchmark and up to 15.3% better performance on the Simple Turing Concepts benchmark. Finally, symbol-tuned models show large improvements in following flipped-labels presented in-context, meaning that they are more capable of using in-context information to override prior knowledge.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.61.pdf",
        "keywords": [
            "symbol tuning",
            "natural language labels",
            "context",
            "label mappings",
            "knowledge",
            "language models",
            "algorithmic reasoning",
            "in context",
            "list functions benchmark",
            "robust",
            "concepts benchmark"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the abstract implies that LLMs may struggle with certain tasks or prompts, such as those without instructions or natural language labels, and that they may not be able to override prior knowledge in certain situations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the abstract implies that LLMs may struggle with certain tasks or prompts, such as those without instructions or natural language labels, and that they may not be able to override prior knowledge in certain situations."
    },
    {
        "title": "Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented Large Language Models",
        "authors": [
            "Gangwoo Kim",
            "Sungdong Kim",
            "Byeongguk Jeon",
            "Joonsuk Park",
            "Jaewoo Kang"
        ],
        "published": "2023",
        "summary": "Questions in open-domain question answering are often ambiguous, allowing multiple interpretations. One approach to handling them is to identify all possible interpretations of the ambiguous question (AQ) and to generate a long-form answer addressing them all, as suggested by Stelmakh et al., (2022). While it provides a comprehensive response without bothering the user for clarification, considering multiple dimensions of ambiguity and gathering corresponding knowledge remains a challenge. To cope with the challenge, we propose a novel framework, Tree of Clarifications (ToC): It recursively constructs a tree of disambiguations for the AQ—via few-shot prompting leveraging external knowledge—and uses it to generate a long-form answer. ToC outperforms existing baselines on ASQA in a few-shot setup across the metrics, while surpassing fully-supervised baselines trained on the whole training set in terms of Disambig-F1 and Disambig-ROUGE. Code is available at https://github.com/gankim/tree-of-clarifications.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.63.pdf",
        "keywords": [
            "clarification",
            "tree of clarifications",
            "tree"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"considering multiple dimensions of ambiguity and gathering corresponding knowledge remains a challenge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"considering multiple dimensions of ambiguity and gathering corresponding knowledge remains a challenge.\""
    },
    {
        "title": "Large Language Models Can Self-Improve",
        "authors": [
            "Jiaxin Huang",
            "Shixiang Gu",
            "Le Hou",
            "Yuexin Wu",
            "Xuezhi Wang",
            "Hongkun Yu",
            "Jiawei Han"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate “high-confidence” rationale-augmented answers for unlabeled questions using Chain-of-Though (CoT) prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that without any ground truth label, our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%→82.1% on GSM8K, 90.0%→94.4% on OpenBookQA, and 63.4%→67.9% on ANLI-A3) and can also be adapted to extreme low-resource cases where even training questions and CoT prompts are limited. We conduct ablation studies and show that fine-tuning on diverse reasoning paths is critical for self-improvement.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.67.pdf",
        "keywords": [
            "large language models",
            "self consistency",
            "self improvement"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, fine-tuning an LLM requires extensive supervision.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, fine-tuning an LLM requires extensive supervision.\""
    },
    {
        "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
        "authors": [
            "Yue Wang",
            "Hung Le",
            "Akhilesh Gotmare",
            "Nghi Bui",
            "Junnan Li",
            "Steven Hoi"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks, lacking the flexibility to operate in the optimal architecture for a specific task. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some tasks and hence result in substantial performance degrade. To address these limitations, we propose “CodeT5+”, a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives, which cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) performance on various code-related tasks, and our instruction-tuned CodeT5+ 16B achieves new SoTA results of 35.0% pass@1 and 54.5% pass@10 on the HumanEval code generation task against other open code LLMs, even surpassing the OpenAI code-cushman-001 model.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.68.pdf",
        "keywords": [
            "large language models",
            "code large language models",
            "generation",
            "instruction tuning",
            "code understanding"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing code LLMs have two main limitations. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks, lacking the flexibility to operate in the optimal architecture for a specific task. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some tasks and",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, existing code LLMs have two main limitations. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks, lacking the flexibility to operate in the optimal architecture for a specific task. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some tasks and"
    },
    {
        "title": "Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings",
        "authors": [
            "Andrea W Wen-Yi",
            "David Mimno"
        ],
        "published": "2023",
        "summary": "Cross-lingual transfer learning is an important property of multilingual large language models (LLMs). But how do LLMs represent relationships between languages? Every language model has an input layer that maps tokens to vectors. This ubiquitous layer of language models is often overlooked. We find that similarities between these input embeddings are highly interpretable and that the geometry of these embeddings differs between model families. In one case (XLM-RoBERTa), embeddings encode language: tokens in different writing systems can be linearly separated with an average of 99.2% accuracy. Another family (mT5) represents cross-lingual semantic similarity: the 50 nearest neighbors for any token represent an average of 7.61 writing systems, and are frequently translations. This result is surprising given that there is no explicit parallel cross-lingual training corpora and no explicit incentive for translations in pre-training objectives. Our research opens the door for investigations in 1) The effect of pre-training and model architectures on representations of languages and 2) The applications of cross-lingual representations embedded in language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.71.pdf",
        "keywords": [
            "language model",
            "multilingual large language models",
            "interpretability",
            "token embeddings",
            "input embeddings"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper does mention some surprising results and potential areas for investigation, which could be related to limitations, however it does not explicitly state any limitation of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None, but the paper does mention some surprising results and potential areas for investigation, which could be related to limitations, however it does not explicitly state any limitation of LLMs."
    },
    {
        "title": "SeqXGPT: Sentence-Level AI-Generated Text Detection",
        "authors": [
            "Pengyu Wang",
            "Linyang Li",
            "Ke Ren",
            "Botian Jiang",
            "Dong Zhang",
            "Xipeng Qiu"
        ],
        "published": "2023",
        "summary": "Widely applied large language models (LLMs) can generate human-like content, raising concerns about the abuse of LLMs. Therefore, it is important to build strong AI-generated text (AIGT) detectors. Current works only consider document-level AIGT detection, therefore, in this paper, we first introduce a sentence-level detection challenge by synthesizing a dataset that contains documents that are polished with LLMs, that is, the documents contain sentences written by humans and sentences modified by LLMs. Then we propose Sequence X (Check) GPT, a novel method that utilizes log probability lists from white-box LLMs as features for sentence-level AIGT detection. These features are composed like waves in speech processing and cannot be studied by LLMs. Therefore, we build SeqXGPT based on convolution and self-attention networks. We test it in both sentence and document-level detection challenges. Experimental results show that previous methods struggle in solving sentence-level AIGT detection, while our method not only significantly surpasses baseline methods in both sentence and document-level detection challenges but also exhibits strong generalization capabilities.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.73.pdf",
        "keywords": [
            "detection",
            "sentence level detection",
            "document level detection",
            "sentence level aigt detection",
            "log probability lists",
            "generated text detection",
            "ai generated text",
            "seqxgpt",
            "self attention networks"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Widely applied large language models (LLMs) can generate human-like content, raising concerns about the abuse of LLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Widely applied large language models (LLMs) can generate human-like content, raising concerns about the abuse of LLMs.\""
    },
    {
        "title": "QTSumm: Query-Focused Summarization over Tabular Data",
        "authors": [
            "Yilun Zhao",
            "Zhenting Qi",
            "Linyong Nan",
            "Boyu Mi",
            "Yixin Liu",
            "Weijin Zou",
            "Simeng Han",
            "Ruizhe Chen",
            "Xiangru Tang",
            "Yumo Xu",
            "Dragomir Radev",
            "Arman Cohan"
        ],
        "published": "2023",
        "summary": "People primarily consult tables to conduct data analysis or answer specific questions. Text generation systems that can provide accurate table summaries tailored to users’ information needs can facilitate more efficient access to relevant data insights. Motivated by this, we define a new query-focused table summarization task, where text generation models have to perform human-like reasoning and analysis over the given table to generate a tailored summary. We introduce a new benchmark named QTSumm for this task, which contains 7,111 human-annotated query-summary pairs over 2,934 tables covering diverse topics. We investigate a set of strong baselines on QTSumm, including text generation, table-to-text generation, and large language models. Experimental results and manual analysis reveal that the new task presents significant challenges in table-to-text generation for future research. Moreover, we propose a new approach named ReFactor, to retrieve and reason over query-relevant information from tabular data to generate several natural language facts. Experimental results demonstrate that ReFactor can bring effective improvements to baselines by concatenating the generated facts to the model input. Our data and code are publicly available at https://github.com/yale-nlp/QTSumm.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.74.pdf",
        "keywords": [
            "summarization",
            "table summarization",
            "text generation",
            "query"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experimental results and manual analysis reveal that the new task presents significant challenges in table-to-text generation for future research.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs in the context of table-to-text generation, but it is not the primary focus of the paper and is only briefly mentioned.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Experimental results and manual analysis reveal that the new task presents significant challenges in table-to-text generation for future research.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs in the context of table-to-text generation, but it is not the primary focus of the paper and is only briefly mentioned."
    },
    {
        "title": "From Wrong To Right: A Recursive Approach Towards Vision-Language Explanation",
        "authors": [
            "Jiaxin Ge",
            "Sanjay Subramanian",
            "Trevor Darrell",
            "Boyi Li"
        ],
        "published": "2023",
        "summary": "Addressing the challenge of adapting pre-trained vision-language models for generating insightful explanations for visual reasoning tasks with limited annotations, we present ReVisE: a Recursive Visual Explanation algorithm. Our method iteratively computes visual features (conditioned on the text input), an answer, and an explanation, to improve the explanation quality step by step until the answer converges. We find that this multi-step approach guides the model to correct its own answers and outperforms single-step explanation generation. Furthermore, explanations generated by ReVisE also serve as valuable annotations for few-shot self-training. Our approach outperforms previous methods while utilizing merely 5% of the human-annotated explanations across 10 metrics, demonstrating up to a 4.2 and 1.3 increase in BLEU-1 score on the VCR and VQA-X datasets, underscoring the efficacy and data-efficiency of our method.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.75.pdf",
        "keywords": [
            "explanation",
            "visual explanation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Addressing the challenge of adapting pre-trained vision-language models for generating insightful explanations for visual reasoning tasks with limited annotations\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Addressing the challenge of adapting pre-trained vision-language models for generating insightful explanations for visual reasoning tasks with limited annotations\""
    },
    {
        "title": "LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following",
        "authors": [
            "Cheng-Fu Yang",
            "Yen-Chun Chen",
            "Jianwei Yang",
            "Xiyang Dai",
            "Lu Yuan",
            "Yu-Chiang Wang",
            "Kai-Wei Chang"
        ],
        "published": "2023",
        "summary": "End-to-end Transformers have demonstrated an impressive success rate for Embodied Instruction Following when the environment has been seen in training. However, they tend to struggle when deployed in an unseen environment. This lack of generalizability is due to the agent’s insensitivity to subtle changes in natural language instructions. To mitigate this issue, we propose explicitly aligning the agent’s hidden states with the instructions via contrastive learning. Nevertheless, the semantic gap between high-level language instructions and the agent’s low-level action space remains an obstacle. Therefore, we further introduce a novel concept of meta-actions to bridge the gap. Meta-actions are ubiquitous action patterns that can be parsed from the original action sequence. These patterns represent higher-level semantics that are intuitively aligned closer to the instructions. When meta-actions are applied as additional training signals, the agent generalizes better to unseen environments. Compared to a strong multi-modal Transformer baseline, we achieve a significant 4.5% absolute gain in success rate in unseen environments of ALFRED Embodied Instruction Following. Additional analysis shows that the contrastive objective and meta-actions are complementary in achieving the best results, and the resulting agent better aligns its states with corresponding instructions, making it more suitable for real-world embodied agents.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.77.pdf",
        "keywords": [
            "meta actions",
            "embodied",
            "embodied instruction",
            "language aligning contrastive learning",
            "ubiquitous action patterns"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they tend to struggle when deployed in an unseen environment. This lack of generalizability is due to the agent’s insensitivity to subtle changes in natural language instructions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, they tend to struggle when deployed in an unseen environment. This lack of generalizability is due to the agent’s insensitivity to subtle changes in natural language instructions.\""
    },
    {
        "title": "Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models",
        "authors": [
            "Jianwei Li",
            "Qi Lei",
            "Wei Cheng",
            "Dongkuan Xu"
        ],
        "published": "2023",
        "summary": "The pruning objective has recently extended beyond accuracy and sparsity to robustness in language models. Despite this, existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity and require a retraining process. As humans step into the era of large language models, these issues become increasingly prominent. This paper proposes that the robustness of language models is proportional to the extent of pre-trained knowledge they encompass. Accordingly, we introduce a post-training pruning strategy designed to faithfully replicate the embedding space and feature space of dense language models, aiming to conserve more pre-trained knowledge during the pruning process. In this setup, each layer’s reconstruction error not only originates from itself but also includes cumulative error from preceding layers, followed by an adaptive rectification. Compared to other state-of-art baselines, our approach demonstrates a superior balance between accuracy, sparsity, robustness, and pruning cost with BERT on datasets SST2, IMDB, and AGNews, marking a significant stride towards robust pruning in language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.79.pdf",
        "keywords": [
            "pruning",
            "language models",
            "robust pruning",
            "robustness",
            "adaptive knowledge retention pruning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite this, existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity and require a retraining process.\"\n\nThis abstract mentions a limitation of LLMs (struggling with robustness against adversarial attacks) but does not explore it in depth and focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite this, existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity and require a retraining process.\"\n\nThis abstract mentions a limitation of LLMs (struggling with robustness against adversarial attacks) but does not explore it in depth and focuses on the proposed solution."
    },
    {
        "title": "Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements",
        "authors": [
            "Jiacheng Liu",
            "Wenya Wang",
            "Dianzhuo Wang",
            "Noah Smith",
            "Yejin Choi",
            "Hannaneh Hajishirzi"
        ],
        "published": "2023",
        "summary": "Today’s language models can be remarkably intelligent yet still produce text that contains trivial commonsense errors. Therefore, we seek a retrospective verification approach that can reflect on the commonsense plausibility of the machine text, and introduce Vera, a general-purpose model that learns to estimate the commonsense plausibility of declarative statements. To support diverse commonsense domains, Vera is trained on ~7M commonsense statements that are automatically converted from 19 QA datasets and two commonsense knowledge bases, and using a combination of three training objectives. When applied to solving commonsense problems in the verification format, Vera substantially outperforms existing models that can be repurposed for commonsense verification, even including GPT-3.5/ChatGPT/GPT-4, and it further exhibits generalization capabilities to unseen tasks and provides well-calibrated outputs. We find that Vera excels at filtering machine-generated commonsense knowledge and is useful in detecting erroneous commonsense statements generated by models like ChatGPT in real-world settings.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.81.pdf",
        "keywords": [
            "commonsense",
            "commonsense plausibility",
            "commonsense verification",
            "verification",
            "plausibility estimation model",
            "commonsense errors",
            "retrospective verification"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Today’s language models can be remarkably intelligent yet still produce text that contains trivial commonsense errors.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Today’s language models can be remarkably intelligent yet still produce text that contains trivial commonsense errors.\""
    },
    {
        "title": "Unveiling the Implicit Toxicity in Large Language Models",
        "authors": [
            "Jiaxin Wen",
            "Pei Ke",
            "Hao Sun",
            "Zhexin Zhang",
            "Chengfei Li",
            "Jinfeng Bai",
            "Minlie Huang"
        ],
        "published": "2023",
        "summary": "The open-endedness of large language models (LLMs) combined with their impressive capabilities may lead to new safety issues when being exploited for malicious use. While recent studies primarily focus on probing toxic outputs that can be easily detected with existing toxicity classifiers, we show that LLMs can generate diverse implicit toxic outputs that are exceptionally difficult to detect via simply zero-shot prompting. Moreover, we propose a reinforcement learning (RL) based attacking method to further induce the implicit toxicity in LLMs. Specifically, we optimize the language model with a reward that prefers implicit toxic outputs to explicit toxic and non-toxic ones. Experiments on five widely-adopted toxicity classifiers demonstrate that the attack success rate can be significantly improved through RL fine-tuning. For instance, the RL-finetuned LLaMA-13B model achieves an attack success rate of 90.04% on BAD and 62.85% on Davinci003. Our findings suggest that LLMs pose a significant threat in generating undetectable implicit toxic outputs. We further show that fine-tuning toxicity classifiers on the annotated examples from our attacking method can effectively enhance their ability to detect LLM-generated implicit toxic language.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.84.pdf",
        "keywords": [
            "toxicity",
            "toxic outputs",
            "toxic language",
            "language model",
            "implicit toxicity",
            "large language models",
            "reinforcement learning",
            "undetectable implicit toxic outputs",
            "implicit"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The open-endedness of large language models (LLMs) combined with their impressive capabilities may lead to new safety issues when being exploited for malicious use\"; \"Our findings suggest that LLMs pose a significant threat in generating undetectable implicit toxic outputs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"The open-endedness of large language models (LLMs) combined with their impressive capabilities may lead to new safety issues when being exploited for malicious use\"; \"Our findings suggest that LLMs pose a significant threat in generating undetectable implicit toxic outputs.\""
    },
    {
        "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?",
        "authors": [
            "Chengwei Qin",
            "Aston Zhang",
            "Zhuosheng Zhang",
            "Jiaao Chen",
            "Michihiro Yasunaga",
            "Diyi Yang"
        ],
        "published": "2023",
        "summary": "Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot—i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.85.pdf",
        "keywords": [
            "natural language processing",
            "sequence tagging",
            "shot learning"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging.\""
    },
    {
        "title": "Length is a Curse and a Blessing for Document-level Semantics",
        "authors": [
            "Chenghao Xiao",
            "Yizhi Li",
            "G Hudson",
            "Chenghua Lin",
            "Noura Al Moubayed"
        ],
        "published": "2023",
        "summary": "In recent years, contrastive learning (CL) has been extensively utilized to recover sentence and document-level encoding capability from pre-trained language models. In this work, we question the length generalizability of CL-based models, i.e., their vulnerability towards length-induced semantic shift. We verify not only that length vulnerability is a significant yet overlooked research gap, but we can devise unsupervised CL methods solely depending on the semantic signal provided by document length. We first derive the theoretical foundations underlying length attacks, showing that elongating a document would intensify the high intra-document similarity that is already brought by CL. Moreover, we found that isotropy promised by CL is highly dependent on the length range of text exposed in training. Inspired by these findings, we introduce a simple yet universal document representation learning framework, **LA(SER)3**: length-agnostic self-reference for semantically robust sentence representation learning, achieving state-of-the-art unsupervised performance on the standard information retrieval benchmark. [Our code is publicly available.](https://github.com/gowitheflow-1998/LA-SER-cubed)",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.86.pdf",
        "keywords": [
            "length vulnerability",
            "length",
            "document level semantics",
            "length attacks"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We verify not only that length vulnerability is a significant yet overlooked research gap, but we can devise unsupervised CL methods solely depending on the semantic signal provided by document length.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We verify not only that length vulnerability is a significant yet overlooked research gap, but we can devise unsupervised CL methods solely depending on the semantic signal provided by document length.\""
    },
    {
        "title": "ALCUNA: Large Language Models Meet New Knowledge",
        "authors": [
            "Xunjian Yin",
            "Baizhou Huang",
            "Xiaojun Wan"
        ],
        "published": "2023",
        "summary": "With the rapid development of NLP, large-scale language models (LLMs) excel in various tasks across multiple domains now. However, existing benchmarks may not adequately measure these models’ capabilities, especially when faced with new knowledge. In this paper, we address the lack of benchmarks to evaluate LLMs’ ability to handle new knowledge, an important and challenging aspect in the rapidly evolving world. We propose an approach called KnowGen that generates new knowledge by altering existing entity attributes and relationships, resulting in artificial entities that are distinct from real-world entities. With KnowGen, we introduce a benchmark named ALCUNA to assess LLMs’ abilities in knowledge understanding, differentiation, and association. We benchmark several LLMs, reveals that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge. We also explore the impact of entity similarity on the model’s understanding of entity knowledge and the influence of contextual entities. We appeal to the need for caution when using LLMs in new scenarios or with new knowledge, and hope that our benchmarks can help drive the development of LLMs in face of new knowledge.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.87.pdf",
        "keywords": [
            "alcuna",
            "benchmark",
            "language models",
            "lack of benchmarks",
            "knowgen"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We benchmark several LLMs, reveals that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We benchmark several LLMs, reveals that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge.\""
    },
    {
        "title": "Location-Aware Visual Question Generation with Lightweight Models",
        "authors": [
            "Nicholas Suwono",
            "Justin Chen",
            "Tun Hung",
            "Ting-Hao Huang",
            "I-Bin Liao",
            "Yung-Hui Li",
            "Lun-Wei Ku",
            "Shao-Hua Sun"
        ],
        "published": "2023",
        "summary": "This work introduces a novel task, location-aware visual question generation (LocaVQG), which aims to generate engaging questions from data relevant to a particular geographical location. Specifically, we represent such location-aware information with surrounding images and a GPS coordinate. To tackle this task, we present a dataset generation pipeline that leverages GPT-4 to produce diverse and sophisticated questions. Then, we aim to learn a lightweight model that can address the LocaVQG task and fit on an edge device, such as a mobile phone. To this end, we propose a method which can reliably generate engaging questions from location-aware information. Our proposed method outperforms baselines regarding human evaluation (e.g., engagement, grounding, coherence) and automatic evaluation metrics (e.g., BERTScore, ROUGE-2). Moreover, we conduct extensive ablation studies to justify our proposed techniques for both generating the dataset and solving the task.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.88.pdf",
        "keywords": [
            "visual question generation",
            "lightweight models",
            "aware"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"To tackle this task, we present a dataset generation pipeline that leverages GPT-4 to produce diverse and sophisticated questions.\"\n\n(Note: The paper mentions GPT-4, a type of LLM, but does not discuss any limitations of LLMs.)",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"To tackle this task, we present a dataset generation pipeline that leverages GPT-4 to produce diverse and sophisticated questions.\"\n\n(Note: The paper mentions GPT-4, a type of LLM, but does not discuss any limitations of LLMs.)"
    },
    {
        "title": "Transcending Scaling Laws with 0.1% Extra Compute",
        "authors": [
            "Yi Tay",
            "Jason Wei",
            "Hyung Chung",
            "Vinh Tran",
            "David So",
            "Siamak Shakeri",
            "Xavier Garcia",
            "Steven Zheng",
            "Jinfeng Rao",
            "Aakanksha Chowdhery",
            "Denny Zhou",
            "Donald Metzler",
            "Slav Petrov",
            "Neil Houlsby",
            "Quoc Le",
            "Mostafa Dehghani"
        ],
        "published": "2023",
        "summary": "Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R, a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model on a few more steps with UL2’s mixture-of-denoiser objective. We show that, with almost negligible extra computational costs and no new sources of data, we are able to substantially improve the scaling properties of large language models on downstream metrics. In this paper, we continue training a baseline language model, PaLM, with ULR2, introducing a new set of models at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget (i.e., saving ~4.4 million TPUv4 hours). We further show that this improved scaling curve leads to “emergent abilities” on challenging BIG-Bench tasks—for instance, U-PaLM does much better on some tasks or demonstrates better quality at much smaller scale (62B as opposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many few-shot setups, including reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual tasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.91.pdf",
        "keywords": [
            "scale",
            "transcending scaling laws"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Scaling language models improves performance but comes with significant computational costs.\"\n\nThis abstract mentions a limitation of LLMs (computational costs) but only in passing and does not elaborate on it further.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Scaling language models improves performance but comes with significant computational costs.\"\n\nThis abstract mentions a limitation of LLMs (computational costs) but only in passing and does not elaborate on it further."
    },
    {
        "title": "CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation",
        "authors": [
            "Minzhi Li",
            "Taiwei Shi",
            "Caleb Ziems",
            "Min-Yen Kan",
            "Nancy Chen",
            "Zhengyuan Liu",
            "Diyi Yang"
        ],
        "published": "2023",
        "summary": "Annotated data plays a critical role in Natural Language Processing (NLP) in training models and evaluating their performance. Given recent developments in Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot capability on many text-annotation tasks, comparable with or even exceeding human annotators. Such LLMs can serve as alternatives for manual annotation, due to lower costs and higher scalability. However, limited work has leveraged LLMs as complementary annotators, nor explored how annotation work is best allocated among humans and LLMs to achieve both quality and cost objectives. We propose CoAnnotating, a novel paradigm for Human-LLM co-annotation of unstructured texts at scale. Under this framework, we utilize uncertainty to estimate LLMs’ annotation capability. Our empirical study shows CoAnnotating to be an effective means to allocate work from results on different datasets, with up to 21% performance improvement over random baseline. For code implementation, see https://github.com/SALT-NLP/CoAnnotating.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.92.pdf",
        "keywords": [
            "language models",
            "work allocation",
            "natural language processing",
            "large language models",
            "uncertainty"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, limited work has leveraged LLMs as complementary annotators, nor explored how annotation work is best allocated among humans and LLMs to achieve both quality and cost objectives.\"\n\nThis evidence suggests that the paper mentions a limitation of LLMs in the context of annotation tasks, but it is not a major focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, limited work has leveraged LLMs as complementary annotators, nor explored how annotation work is best allocated among humans and LLMs to achieve both quality and cost objectives.\"\n\nThis evidence suggests that the paper mentions a limitation of LLMs in the context of annotation tasks, but it is not a major focus of the paper."
    },
    {
        "title": "Optimizing Retrieval-augmented Reader Models via Token Elimination",
        "authors": [
            "Moshe Berchansky",
            "Peter Izsak",
            "Avi Caciularu",
            "Ido Dagan",
            "Moshe Wasserblat"
        ],
        "published": "2023",
        "summary": "Fusion-in-Decoder (FiD) is an effective retrieval-augmented language model applied across a variety of open-domain tasks, such as question answering, fact checking, etc. In FiD, supporting passages are first retrieved and then processed using a generative model (Reader), which can cause a significant bottleneck in decoding time, particularly with long outputs. In this work, we analyze the contribution and necessity of all the retrieved passages to the performance of reader models, and propose eliminating some of the retrieved information, at the token level, that might not contribute essential information to the answer generation process. We demonstrate that our method can reduce run-time by up to 62.2%, with only a 2% reduction in performance, and in some cases, even improve the performance results.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.93.pdf",
        "keywords": [
            "token elimination",
            "augmented language model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"In FiD, supporting passages are first retrieved and then processed using a generative model (Reader), which can cause a significant bottleneck in decoding time, particularly with long outputs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"In FiD, supporting passages are first retrieved and then processed using a generative model (Reader), which can cause a significant bottleneck in decoding time, particularly with long outputs.\""
    },
    {
        "title": "Robust Prompt Optimization for Large Language Models Against Distribution Shifts",
        "authors": [
            "Moxin Li",
            "Wenjie Wang",
            "Fuli Feng",
            "Yixin Cao",
            "Jizhi Zhang",
            "Tat-Seng Chua"
        ],
        "published": "2023",
        "summary": "Large Language Model (LLM) has demonstrated significant ability in various Natural Language Processing tasks. However, their effectiveness is highly dependent on the phrasing of the task prompt, leading to research on automatic prompt optimization using labeled task data. We reveal that these prompt optimization techniques are vulnerable to distribution shifts such as subpopulation shifts, which are common for LLMs in real-world scenarios such as customer reviews analysis. In this light, we propose a new problem of robust prompt optimization for LLMs against distribution shifts, which requires the prompt optimized over the labeled source group can simultaneously generalize to an unlabeled target group. To solve this problem, we propose Generalized Prompt Optimization framework , which incorporates the unlabeled data from the target group into prompt optimization. Extensive experimental results demonstrate the effectiveness of the proposed framework with significant performance improvement on the target group and comparable performance on the source group.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.95.pdf",
        "keywords": [
            "distribution shifts",
            "customer reviews",
            "subpopulation shifts",
            "prompt optimization",
            "vulnerable",
            "language"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their effectiveness is highly dependent on the phrasing of the task prompt, leading to research on automatic prompt optimization using labeled task data. We reveal that these prompt optimization techniques are vulnerable to distribution shifts such as subpopulation shifts, which are common for LLMs in real-world scenarios such as customer reviews analysis.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, their effectiveness is highly dependent on the phrasing of the task prompt, leading to research on automatic prompt optimization using labeled task data. We reveal that these prompt optimization techniques are vulnerable to distribution shifts such as subpopulation shifts, which are common for LLMs in real-world scenarios such as customer reviews analysis.\""
    },
    {
        "title": "Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction",
        "authors": [
            "Martin Josifoski",
            "Marija Sakota",
            "Maxime Peyrard",
            "Robert West"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date. We synthetically generate a dataset of 1.8M data points, establish its superior quality compared to existing datasets in a human evaluation, and use it to finetune small models (220M and 770M parameters), termed SynthIE, that outperform the prior state of the art (with equal model size) by a substantial margin of 57 absolute points in micro-F1 and 79 points in macro-F1. Code, data, and models are available at anonymous.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.96.pdf",
        "keywords": [
            "information extraction",
            "synthetic data generation",
            "synthetically generated",
            "synthie",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"for problems that cannot be solved directly by LLMs\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"for problems that cannot be solved directly by LLMs\""
    },
    {
        "title": "Increasing Coverage and Precision of Textual Information in Multilingual Knowledge Graphs",
        "authors": [
            "Simone Conia",
            "Min Li",
            "Daniel Lee",
            "Umar Minhas",
            "Ihab Ilyas",
            "Yunyao Li"
        ],
        "published": "2023",
        "summary": "Recent work in Natural Language Processing and Computer Vision has been using textual information – e.g., entity names and descriptions – available in knowledge graphs to ground neural models to high-quality structured data. However, when it comes to non-English languages, the quantity and quality of textual information are comparatively scarce. To address this issue, we introduce the novel task of automatic Knowledge Graph Completion (KGE) and perform a thorough investigation on bridging the gap in both the quantity and quality of textual information between English and non-English languages. More specifically, we: i) bring to light the problem of increasing multilingual coverage and precision of entity names and descriptions in Wikidata; ii) demonstrate that state-of-the-art methods, namely, Machine Translation (MT), Web Search (WS), and Large Language Models (LLMs), struggle with this task; iii) present M-NTA, a novel unsupervised approach that combines MT, WS, and LLMs to generate high-quality textual information; and, iv) study the impact of increasing multilingual coverage and precision of non-English textual information in Entity Linking, Knowledge Graph Completion, and Question Answering. As part of our effort towards better multilingual knowledge graphs, we also introduce WikiKGE-10, the first human-curated benchmark to evaluate KGE approaches in 10 languages across 7 language families.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.100.pdf",
        "keywords": [
            "knowledge graphs",
            "knowledge graph completion",
            "multilingual knowledge graphs",
            "multilingual coverage",
            "coverage",
            "automatic knowledge graph completion",
            "precision",
            "textual information",
            "language models",
            "quantity and quality"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"demonstrate that state-of-the-art methods, namely, Machine Translation (MT), Web Search (WS), and Large Language Models (LLMs), struggle with this task\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"demonstrate that state-of-the-art methods, namely, Machine Translation (MT), Web Search (WS), and Large Language Models (LLMs), struggle with this task\""
    },
    {
        "title": "Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling",
        "authors": [
            "Xiuying Wei",
            "Yunchen Zhang",
            "Yuhang Li",
            "Xiangguo Zhang",
            "Ruihao Gong",
            "Jinyang Guo",
            "Xianglong Liu"
        ],
        "published": "2023",
        "summary": "Post-training quantization (PTQ) of transformer language models faces significant challenges due to the existence of detrimental outliers in activations. We observe that these outliers are concentrated in specific channels and are asymmetric across channels. To address this issue, we propose the Outlier Suppression+ (OS+) framework, which contains the channel-wise shifting for asymmetry and channel-wise scaling for concentration. We show that these operations can be seamlessly migrated into subsequent modules while maintaining equivalence. Second, we propose a fast and stable scheme to calculate effective shifting and scaling values. The channel-wise shifting aligns the center of each channel for removal of outlier asymmetry. The channel-wise scaling quantitatively evaluates changes brought by migration and quantization for better quantization burden balance. We validate our OS+ under both standard and fine-grained quantization settings with models including BERT, OPT, BLOOM, BLOOMZ, and LLaMA. Comprehensive results across various tasks demonstrate the superiority of our approach. Especially, with standard quantization, OS+ can achieve near-floating-point performance on both small models and large language models on 8-bit and 6-bit. Besides, we establish a new state-of-the-art for 4-bit BERT with 15.5% improvement. Our code is available at https://github.com/ModelTC/Outlier_Suppression_Plus.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.102.pdf",
        "keywords": [
            "language models",
            "outliers",
            "transformer language models",
            "post training quantization"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Post-training quantization (PTQ) of transformer language models faces significant challenges due to the existence of detrimental outliers in activations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Post-training quantization (PTQ) of transformer language models faces significant challenges due to the existence of detrimental outliers in activations.\""
    },
    {
        "title": "Fast and Accurate Factual Inconsistency Detection Over Long Documents",
        "authors": [
            "Barrett Lattimer",
            "Patrick CHen",
            "Xinyuan Zhang",
            "Yi Yang"
        ],
        "published": "2023",
        "summary": "Generative AI models exhibit remarkable potential; however, hallucinations across various tasks present a significant challenge, particularly for longer inputs that current approaches struggle to address effectively. We introduce SCALE (Source Chunking Approach for Large-scale inconsistency Evaluation), a task-agnostic model for detecting factual inconsistencies using a novel chunking strategy. Specifically, SCALE is a Natural Language Inference (NLI) based model that uses large text chunks to condition over long texts. This approach achieves state-of-the-art performance in factual inconsistency detection for diverse tasks and long inputs. Additionally, we leverage the chunking mechanism and employ a novel algorithm to explain SCALE’s decisions through relevant source sentence retrieval. Our evaluations reveal that SCALE outperforms existing methods on both standard benchmarks and a new long-form dialogue dataset ScreenEval we constructed. Moreover, SCALE surpasses competitive systems in efficiency and model explanation evaluations. We have released our code and data publicly to GitHub.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.105.pdf",
        "keywords": [
            "inconsistency evaluation",
            "factual inconsistency detection",
            "natural language inference"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Generative AI models exhibit remarkable potential; however, hallucinations across various tasks present a significant challenge, particularly for longer inputs that current approaches struggle to address effectively.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Generative AI models exhibit remarkable potential; however, hallucinations across various tasks present a significant challenge, particularly for longer inputs that current approaches struggle to address effectively.\""
    },
    {
        "title": "Interpreting Embedding Spaces by Conceptualization",
        "authors": [
            "Adi Simhi",
            "Shaul Markovitch"
        ],
        "published": "2023",
        "summary": "One of the main methods for computational interpretation of a text is mapping it into a vector in some embedding space. Such vectors can then be used for a variety of textual processing tasks. Recently, most embedding spaces are a product of training large language models (LLMs). One major drawback of this type of representation is their incomprehensibility to humans. Understanding the embedding space is crucial for several important needs, including the need to debug the embedding method and compare it to alternatives, and the need to detect biases hidden in the model. In this paper, we present a novel method of understanding embeddings by transforming a latent embedding space into a comprehensible conceptual space. We present an algorithm for deriving a conceptual space with dynamic on-demand granularity. We devise a new evaluation method, using either human rater or LLM-based raters, to show that the conceptualized vectors indeed represent the semantics of the original latent ones. We show the use of our method for various tasks, including comparing the semantics of alternative models and tracing the layers of the LLM. The code is available online https://github.com/adiSimhi/Interpreting-Embedding-Spaces-by-Conceptualization.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.106.pdf",
        "keywords": [
            "conceptualization",
            "embedding space",
            "embeddings",
            "latent embedding space",
            "comprehensible conceptual space",
            "computational interpretation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"One major drawback of this type of representation is their incomprehensibility to humans.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"One major drawback of this type of representation is their incomprehensibility to humans.\""
    },
    {
        "title": "Knowledge-Augmented Language Model Verification",
        "authors": [
            "Jinheon Baek",
            "Soyeong Jeong",
            "Minki Kang",
            "Jong Park",
            "Sung Hwang"
        ],
        "published": "2023",
        "summary": "Recent Language Models (LMs) have shown impressive capabilities in generating texts with the knowledge internalized in parameters. Yet, LMs often generate the factually incorrect responses to the given queries, since their knowledge may be inaccurate, incomplete, and outdated. To address this problem, previous works propose to augment LMs with the knowledge retrieved from an external knowledge source. However, such approaches often show suboptimal text generation performance due to two reasons: 1) the model may fail to retrieve the knowledge relevant to the given query, or 2) the model may not faithfully reflect the retrieved knowledge in the generated text. To overcome these, we propose to verify the output and the knowledge of the knowledge-augmented LMs with a separate verifier, which is a small LM that is trained to detect those two types of errors through instruction-finetuning. Then, when the verifier recognizes an error, we can rectify it by either retrieving new knowledge or generating new text. Further, we use an ensemble of the outputs from different instructions with a single verifier to enhance the reliability of the verification processes. We validate the effectiveness of the proposed verification steps on multiple question answering benchmarks, whose results show that the proposed verifier effectively identifies retrieval and generation errors, allowing LMs to provide more factually correct outputs. Our code is available at https://github.com/JinheonBaek/KALMV.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.107.pdf",
        "keywords": [
            "verification",
            "knowledge augmented lms",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Yet, LMs often generate the factually incorrect responses to the given queries, since their knowledge may be inaccurate, incomplete, and outdated.\"; \"the model may fail to retrieve the knowledge relevant to the given query, or 2) the model may not faithfully reflect the retrieved knowledge in the generated text.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Yet, LMs often generate the factually incorrect responses to the given queries, since their knowledge may be inaccurate, incomplete, and outdated.\"; \"the model may fail to retrieve the knowledge relevant to the given query, or 2) the model may not faithfully reflect the retrieved knowledge in the generated text.\""
    },
    {
        "title": "Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation",
        "authors": [
            "Zeyuan Yang",
            "Peng Li",
            "Yang Liu"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have showcased impressive performance. However, due to their inability to capture relationships among samples, these frozen LLMs inevitably keep repeating similar mistakes. In this work, we propose our Tuning-free Rule Accumulation (TRAN) framework, which guides LLMs in improving their performance by learning from previous mistakes. Considering data arrives sequentially, LLMs gradually accumulate rules from incorrect cases, forming a rule collection. These rules are then utilized by the LLMs to avoid making similar mistakes when processing subsequent inputs. Moreover, the rules remain independent of the primary prompts, seamlessly complementing prompt design strategies. Experimentally, we show that TRAN improves over recent baselines by a large margin.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.109.pdf",
        "keywords": [
            "tuning free rule accumulation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, due to their inability to capture relationships among samples, these frozen LLMs inevitably keep repeating similar mistakes.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, due to their inability to capture relationships among samples, these frozen LLMs inevitably keep repeating similar mistakes.\""
    },
    {
        "title": "Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories",
        "authors": [
            "Suyu Ge",
            "Chenyan Xiong",
            "Corby Rosset",
            "Arnold Overwijk",
            "Jiawei Han",
            "Paul Bennett"
        ],
        "published": "2023",
        "summary": "In this paper we improve the zero-shot generalization ability of language models via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves augmentation documents from multiple information corpora (external memories), with the option to “plug in” unseen memory at inference time. We develop a joint learning mechanism that trains the augmentation component with latent labels derived from the end retrieval task, paired with hard negatives from the memory mixture. We instantiate the model in a zero-shot dense retrieval setting by augmenting strong T5-based retrievers with MoMA. With only T5-base, our model obtains strong zero-shot retrieval accuracy on the eighteen tasks included in the standard BEIR benchmark, outperforming some systems with larger model sizes. As a plug-in-play model, our model can efficiently generalize to any unseen corpus, meanwhile achieving comparable or even better performance than methods relying on target-specific pretraining. Our analysis further illustrates the necessity of augmenting with mixture-of-memory for robust generalization, the benefits of augmentation learning, and how MoMA utilizes the plug-in memory at inference time without changing its parameters. Our code can be found at https://github.com/gesy17/MoMA.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.111.pdf",
        "keywords": [
            "plug in",
            "plug in memory",
            "mixture of memory augmentation",
            "shot retrieval",
            "dense retrieval",
            "augmentation component"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the necessity of augmenting with mixture-of-memory for robust generalization\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the necessity of augmenting with mixture-of-memory for robust generalization\""
    },
    {
        "title": "Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks",
        "authors": [
            "Po-Nien Kung",
            "Fan Yin",
            "Di Wu",
            "Kai-Wei Chang",
            "Nanyun Peng"
        ],
        "published": "2023",
        "summary": "Instruction tuning (IT) achieves impressive zero-shot generalization results by training large language models (LLMs) on a massive amount of diverse tasks with instructions. However, how to select new tasks to improve the performance and generalizability of IT models remains an open question. Training on all existing tasks is impractical due to prohibiting computation requirements, and randomly selecting tasks can lead to suboptimal performance. In this work, we propose active instruction tuning based on prompt uncertainty, a novel framework to identify informative tasks, and then actively tune the models on the selected tasks. We represent the informativeness of new tasks with the disagreement of the current model outputs over perturbed prompts. Our experiments on NIV2 and Self-Instruct datasets demonstrate that our method consistently outperforms other baseline strategies for task selection, achieving better out-of-distribution generalization with fewer training tasks. Additionally, we introduce a task map that categorizes and diagnoses tasks based on prompt uncertainty and prediction probability. We discover that training on ambiguous (prompt-uncertain) tasks improves generalization while training on difficult (prompt-certain and low-probability) tasks offers no benefit, underscoring the importance of task selection for instruction tuning.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.112.pdf",
        "keywords": [
            "tune",
            "instruction tuning",
            "active instruction tuning",
            "task selection"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Training on all existing tasks is impractical due to prohibiting computation requirements, and randomly selecting tasks can lead to suboptimal performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Training on all existing tasks is impractical due to prohibiting computation requirements, and randomly selecting tasks can lead to suboptimal performance.\""
    },
    {
        "title": "DUnE: Dataset for Unified Editing",
        "authors": [
            "Afra Akyürek",
            "Eric Pan",
            "Garry Kuwanto",
            "Derry Wijaya"
        ],
        "published": "2023",
        "summary": "Even the most advanced language models remain susceptible to errors necessitating to modify these models without initiating a comprehensive retraining process. Model editing refers to the modification of a model’s knowledge or representations in a manner that produces the desired outcomes. Prior research primarily centered around editing factual data e.g. “Messi plays for Inter Miami” confining the definition of an edit to a knowledge triplet i.e. (subject, object, relation). However, as the applications of language models expand, so do the diverse ways in which we wish to edit and refine their outputs. In this study, we broaden the scope of the editing problem to include an array of editing cases such as debiasing and rectifying reasoning errors and define an edit as any natural language expression that solicits a change in the model’s outputs. We are introducing DUnE, an editing benchmark where edits are natural language sentences and propose that DUnE presents a challenging yet relevant task. To substantiate this claim, we conduct an extensive series of experiments testing various editing approaches to address DUnE, demonstrating their respective strengths and weaknesses. We argue that retrieval-augmented language modeling can outperform specialized editing techniques and neither set of approaches has fully solved the generalized editing problem covered by our benchmark.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.114.pdf",
        "keywords": [
            "language models",
            "dune",
            "edit"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Even the most advanced language models remain susceptible to errors necessitating to modify these models without initiating a comprehensive retraining process.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Even the most advanced language models remain susceptible to errors necessitating to modify these models without initiating a comprehensive retraining process.\""
    },
    {
        "title": "“Fifty Shades of Bias”: Normative Ratings of Gender Bias in GPT Generated English Text",
        "authors": [
            "Rishav Hada",
            "Agrima Seth",
            "Harshita Diddee",
            "Kalika Bali"
        ],
        "published": "2023",
        "summary": "Language serves as a powerful tool for the manifestation of societal belief systems. In doing so, it also perpetuates the prevalent biases in our society. Gender bias is one of the most pervasive biases in our society and is seen in online and offline discourses. With LLMs increasingly gaining human-like fluency in text generation, gaining a nuanced understanding of the biases these systems can generate is imperative. Prior work often treats gender bias as a binary classification task. However, acknowledging that bias must be perceived at a relative scale; we investigate the generation and consequent receptivity of manual annotators to bias of varying degrees. Specifically, we create the first dataset of GPT-generated English text with normative ratings of gender bias. Ratings were obtained using Best–Worst Scaling – an efficient comparative annotation framework. Next, we systematically analyze the variation of themes of gender biases in the observed ranking and show that identity-attack is most closely related to gender bias. Finally, we show the performance of existing automated models trained on related concepts on our dataset.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.115.pdf",
        "keywords": [
            "bias",
            "gender bias",
            "normative ratings",
            "language",
            "belief"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, acknowledging that bias must be perceived at a relative scale; we investigate the generation and consequent receptivity of manual annotators to bias of varying degrees.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, acknowledging that bias must be perceived at a relative scale; we investigate the generation and consequent receptivity of manual annotators to bias of varying degrees.\""
    },
    {
        "title": "ChatGPT to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness",
        "authors": [
            "Jan Cegin",
            "Jakub Simko",
            "Peter Brusilovsky"
        ],
        "published": "2023",
        "summary": "The emergence of generative large language models (LLMs) raises the question: what will be its impact on crowdsourcing? Traditionally, crowdsourcing has been used for acquiring solutions to a wide variety of human-intelligence tasks, including ones involving text generation, modification or evaluation. For some of these tasks, models like ChatGPT can potentially substitute human workers. In this study, we investigate whether this is the case for the task of paraphrase generation for intent classification. We apply data collection methodology of an existing crowdsourcing study (similar scale, prompts and seed data) using ChatGPT and Falcon-40B. We show that ChatGPT-created paraphrases are more diverse and lead to at least as robust models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.117.pdf",
        "keywords": [
            "crowdsourcing",
            "paraphrases",
            "intent classification",
            "paraphrase generation"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions the potential of LLMs to substitute human workers in certain tasks, implying a limitation of human-based crowdsourcing rather than LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions the potential of LLMs to substitute human workers in certain tasks, implying a limitation of human-based crowdsourcing rather than LLMs."
    },
    {
        "title": "Democratizing Reasoning Ability: Tailored Learning from Large Language Model",
        "authors": [
            "Zhaoyang Wang",
            "Shaohan Huang",
            "Yuxuan Liu",
            "Jiahai Wang",
            "Minghui Song",
            "Zihan Zhang",
            "Haizhen Huang",
            "Furu Wei",
            "Weiwei Deng",
            "Feng Sun",
            "Qi Zhang"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) exhibit impressive emergent abilities in natural language processing, but their democratization is hindered due to huge computation requirements and closed-source nature. Recent research on advancing open-source smaller LMs by distilling knowledge from black-box LLMs has obtained promising results in the instruction-following ability. However, the reasoning ability which is more challenging to foster, is relatively rarely explored. In this paper, we propose a tailored learning approach to distill such reasoning ability to smaller LMs to facilitate the democratization of the exclusive reasoning ability. In contrast to merely employing LLM as a data annotator, we exploit the potential of LLM as a reasoning teacher by building an interactive multi-round learning paradigm. This paradigm enables the student to expose its deficiencies to the black-box teacher who then can provide customized training data in return. Further, to exploit the reasoning potential of the smaller LM, we propose self-reflection learning to motivate the student to learn from self-made mistakes. The learning from self-reflection and LLM are all tailored to the student’s learning status, thanks to the seamless integration with the multi-round learning paradigm. Comprehensive experiments and analysis on mathematical and commonsense reasoning tasks demonstrate the effectiveness of our method. The code will be available at https://github.com/Raibows/Learn-to-Reason.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.120.pdf",
        "keywords": [
            "reasoning",
            "reasoning ability",
            "emergent abilities",
            "language model",
            "large language models",
            "exclusive reasoning ability",
            "multi round learning",
            "commonsense reasoning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models (LLMs) exhibit impressive emergent abilities in natural language processing, but their democratization is hindered due to huge computation requirements and closed-source nature.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large language models (LLMs) exhibit impressive emergent abilities in natural language processing, but their democratization is hindered due to huge computation requirements and closed-source nature.\""
    },
    {
        "title": "OpenAsp: A Benchmark for Multi-document Open Aspect-based Summarization",
        "authors": [
            "Shmuel Amar",
            "Liat Schiff",
            "Ori Ernst",
            "Asi Shefer",
            "Ori Shapira",
            "Ido Dagan"
        ],
        "published": "2023",
        "summary": "The performance of automatic summarization models has improved dramatically in recent years. Yet, there is still a gap in meeting specific information needs of users in real-world scenarios, particularly when a targeted summary is sought, such as in the useful aspect-based summarization setting targeted in this paper. Previous datasets and studies for this setting have predominantly concentrated on a limited set of pre-defined aspects, focused solely on single document inputs, or relied on synthetic data. To advance research on more realistic scenarios, we introduce OpenAsp, a benchmark for multi-document open aspect-based summarization. This benchmark is created using a novel and cost-effective annotation protocol, by which an open aspect dataset is derived from existing generic multi-document summarization datasets. We analyze the properties of OpenAsp showcasing its high-quality content. Further, we show that the realistic open-aspect setting realized in OpenAsp poses a challenge for current state-of-the-art summarization models, as well as for large language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.121.pdf",
        "keywords": [
            "summarization",
            "multi document summarization",
            "multi document",
            "aspect",
            "benchmark"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Further, we show that the realistic open-aspect setting realized in OpenAsp poses a challenge for current state-of-the-art summarization models, as well as for large language models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Further, we show that the realistic open-aspect setting realized in OpenAsp poses a challenge for current state-of-the-art summarization models, as well as for large language models.\""
    },
    {
        "title": "Byte Pair Encoding for Symbolic Music",
        "authors": [
            "Nathan Fradet",
            "Nicolas Gutowski",
            "Fabien Chhel",
            "Jean-Pierre Briot"
        ],
        "published": "2023",
        "summary": "When used with deep learning, the symbolic music modality is often coupled with language model architectures. To do so, the music needs to be tokenized, i.e. converted into a sequence of discrete tokens. This can be achieved by different approaches, as music can be composed of simultaneous tracks, of simultaneous notes with several attributes. Until now, the proposed tokenizations rely on small vocabularies of tokens describing the note attributes and time events, resulting in fairly long token sequences, and a sub-optimal use of the embedding space of language models. Recent research has put efforts on reducing the overall sequence length by merging embeddings or combining tokens. In this paper, we show that Byte Pair Encoding, a compression technique widely used for natural language, significantly decreases the sequence length while increasing the vocabulary size. By doing so, we leverage the embedding capabilities of such models with more expressive tokens, resulting in both better results and faster inference in generation and classification tasks. The [source code is shared on Github](https://github.com/Natooz/bpe-symbolic-music), along with a [companion website](https://Natooz.github.io/BPE-Symbolic-Music). Finally, BPE is directly implemented in [MidiTok](https://github.com/Natooz/MidiTok), allowing the reader to easily benefit from this method.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.123.pdf",
        "keywords": [
            "symbolic music",
            "byte pair encoding"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"resulting in fairly long token sequences, and a sub-optimal use of the embedding space of language models.\"\n\nThis paper mentions a limitation of using language models for symbolic music, specifically the suboptimal use of the embedding space due to long token sequences. However, the focus of the paper is on proposing a solution to this limitation, rather than exploring the limitation itself.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"resulting in fairly long token sequences, and a sub-optimal use of the embedding space of language models.\"\n\nThis paper mentions a limitation of using language models for symbolic music, specifically the suboptimal use of the embedding space due to long token sequences. However, the focus of the paper is on proposing a solution to this limitation, rather than exploring the limitation itself."
    },
    {
        "title": "Self-Influence Guided Data Reweighting for Language Model Pre-training",
        "authors": [
            "Megh Thakkar",
            "Tolga Bolukbasi",
            "Sriram Ganapathy",
            "Shikhar Vashishth",
            "Sarath Chandar",
            "Partha Talukdar"
        ],
        "published": "2023",
        "summary": "Language Models (LMs) pre-trained with selfsupervision on large text corpora have become the default starting point for developing models for various NLP tasks. Once the pre-training corpus has been assembled, all data samples in the corpus are treated with equal importance during LM pre-training. However, due to varying levels of relevance and quality of data, equal importance to all the data samples may not be the optimal choice. While data reweighting has been explored in the context of task-specific supervised learning and LM fine-tuning, model-driven reweighting for pretraining data has not been explored. We fill this important gap and propose PRESENCE, a method for jointly reweighting samples by leveraging self-influence (SI) scores as an indicator of sample importance and pre-training. PRESENCE promotes novelty and stability for model pre-training. Through extensive analysis spanning multiple model sizes, datasets, and tasks, we present PRESENCE as an important first step in the research direction of sample reweighting for pre-training language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.125.pdf",
        "keywords": [
            "data reweighting",
            "sample reweighting",
            "jointly reweighting",
            "model driven reweighting",
            "language models",
            "self",
            "self influence",
            "language model pre training"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, due to varying levels of relevance and quality of data, equal importance to all the data samples may not be the optimal choice.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, due to varying levels of relevance and quality of data, equal importance to all the data samples may not be the optimal choice.\""
    },
    {
        "title": "TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models",
        "authors": [
            "Zorik Gekhman",
            "Jonathan Herzig",
            "Roee Aharoni",
            "Chen Elkind",
            "Idan Szpektor"
        ],
        "published": "2023",
        "summary": "Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the state-of-the-art model with similar capacity, and the LLM teacher. In a systematic study, we compare TrueTeacher to existing synthetic data generation methods and demonstrate its superiority and robustness to domain-shift. We also show that our method generalizes to multilingual scenarios. Lastly, we release our large scale synthetic dataset (1.4M examples), generated using TrueTeacher, and a checkpoint trained on this data.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.127.pdf",
        "keywords": [
            "factual consistency evaluation",
            "factual errors",
            "learning factual consistency",
            "natural language inference",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use.\""
    },
    {
        "title": "Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning",
        "authors": [
            "Namrata Shivagunde",
            "Vladislav Lialin",
            "Anna Rumshisky"
        ],
        "published": "2023",
        "summary": "Language model probing is often used to test specific capabilities of models. However, conclusions from such studies may be limited when the probing benchmarks are small and lack statistical power. In this work, we introduce new, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500) inspired by psycholinguistic studies. We dramatically extend existing NEG-136 and ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44 sentence pairs to 750 each. We also create another version of extended negation dataset (NEG-1500-SIMP-TEMP), created using template-based generation. It consists of 770 sentence pairs. We evaluate 22 models on the extended datasets, seeing model performance dip 20-57% compared to the original smaller benchmarks. We observe high levels of negation sensitivity in models like BERT and ALBERT demonstrating that previous findings might have been skewed due to smaller test sets. Finally, we observe that while GPT3 has generated all the examples in ROLE-1500 is only able to solve 24.6% of them during probing. The datasets and code are available on Github.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.130.pdf",
        "keywords": [
            "probing"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We evaluate 22 models on the extended datasets, seeing model performance dip 20-57% compared to the original smaller benchmarks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We evaluate 22 models on the extended datasets, seeing model performance dip 20-57% compared to the original smaller benchmarks.\""
    },
    {
        "title": "Norm of Word Embedding Encodes Information Gain",
        "authors": [
            "Momose Oyama",
            "Sho Yokoi",
            "Hidetoshi Shimodaira"
        ],
        "published": "2023",
        "summary": "Distributed representations of words encode lexical semantic information, but what type of information is encoded and how? Focusing on the skip-gram with negative-sampling method, we found that the squared norm of static word embedding encodes the information gain conveyed by the word; the information gain is defined by the Kullback-Leibler divergence of the co-occurrence distribution of the word to the unigram distribution. Our findings are explained by the theoretical framework of the exponential family of probability distributions and confirmed through precise experiments that remove spurious correlations arising from word frequency. This theory also extends to contextualized word embeddings in language models or any neural networks with the softmax output layer. We also demonstrate that both the KL divergence and the squared norm of embedding provide a useful metric of the informativeness of a word in tasks such as keyword extraction, proper-noun discrimination, and hypernym discrimination.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.131.pdf",
        "keywords": [
            "hypernym discrimination",
            "norm of word embedding"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This theory also extends to contextualized word embeddings in language models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"This theory also extends to contextualized word embeddings in language models\""
    },
    {
        "title": "CRT-QA: A Dataset of Complex Reasoning Question Answering over Tabular Data",
        "authors": [
            "Zhehao Zhang",
            "Xitao Li",
            "Yan Gao",
            "Jian-Guang Lou"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) show powerful reasoning abilities on various text-based tasks. However, their reasoning capability on structured data such as tables has not been systematically explored. In this work, we first establish a comprehensive taxonomy of reasoning and operation types for tabular data analysis. Then, we construct a complex reasoning QA dataset over tabular data, named CRT-QA dataset (Complex Reasoning QA over Tabular data), with the following unique features: (1) it is the first Table QA dataset with multi-step operation and informal reasoning; (2) it contains fine-grained annotations on questions’ directness, composition types of sub-questions, and human reasoning paths which can be used to conduct a thorough investigation on LLMs’ reasoning ability; (3) it contains a collection of unanswerable and indeterminate questions that commonly arise in real-world situations. We further introduce an efficient and effective tool-augmented method, named ARC (Auto-exemplar-guided Reasoning with Code), to use external tools such as Pandas to solve table reasoning tasks without handcrafted demonstrations. The experiment results show that CRT-QA presents a strong challenge for baseline methods and ARC achieves the best result.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.132.pdf",
        "keywords": [
            "complex reasoning",
            "reasoning",
            "table reasoning",
            "large language models",
            "complex reasoning question answering"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their reasoning capability on structured data such as tables has not been systematically explored.\"\n\nThis abstract mentions a limitation of LLMs in passing, stating that their reasoning capability on structured data has not been explored, but does not elaborate on this limitation and focuses on the proposed solution and dataset.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, their reasoning capability on structured data such as tables has not been systematically explored.\"\n\nThis abstract mentions a limitation of LLMs in passing, stating that their reasoning capability on structured data has not been explored, but does not elaborate on this limitation and focuses on the proposed solution and dataset."
    },
    {
        "title": "MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations",
        "authors": [
            "Arkil Patel",
            "Satwik Bhattamishra",
            "Siva Reddy",
            "Dzmitry Bahdanau"
        ],
        "published": "2023",
        "summary": "Humans possess a remarkable ability to assign novel interpretations to linguistic expressions, enabling them to learn new words and understand community-specific connotations. However, Large Language Models (LLMs) have a knowledge cutoff and are costly to finetune repeatedly. Therefore, it is crucial for LLMs to learn novel interpretations in-context. In this paper, we systematically analyse the ability of LLMs to acquire novel interpretations using in-context learning. To facilitate our study, we introduce MAGNIFICo, an evaluation suite implemented within a text-to-SQL semantic parsing framework that incorporates diverse tokens and prompt settings to simulate real-world complexity. Experimental results on MAGNIFICo demonstrate that LLMs exhibit a surprisingly robust capacity for comprehending novel interpretations from natural language descriptions as well as from discussions within long conversations. Nevertheless, our findings also highlight the need for further improvements, particularly when interpreting unfamiliar words or when composing multiple novel interpretations simultaneously in the same example. Additionally, our analysis uncovers the semantic predispositions in LLMs and reveals the impact of recency bias for information presented in long contexts.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.134.pdf",
        "keywords": [
            "language models",
            "context",
            "in context learning ability",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, Large Language Models (LLMs) have a knowledge cutoff and are costly to finetune repeatedly.... our findings also highlight the need for further improvements, particularly when interpreting unfamiliar words or when composing multiple novel interpretations simultaneously in the same example.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, Large Language Models (LLMs) have a knowledge cutoff and are costly to finetune repeatedly.... our findings also highlight the need for further improvements, particularly when interpreting unfamiliar words or when composing multiple novel interpretations simultaneously in the same example.\""
    },
    {
        "title": "Generating and Evaluating Tests for K-12 Students with Language Model Simulations: A Case Study on Sentence Reading Efficiency",
        "authors": [
            "Eric Zelikman",
            "Wanjing Ma",
            "Jasmine Tran",
            "Diyi Yang",
            "Jason Yeatman",
            "Nick Haber"
        ],
        "published": "2023",
        "summary": "Developing an educational test can be expensive and time-consuming, as each item must be written by experts and then evaluated by collecting hundreds of student responses. Moreover, many tests require multiple distinct sets of questions administered throughout the school year to closely monitor students’ progress, known as parallel tests. In this study, we focus on tests of silent sentence reading efficiency, used to assess students’ reading ability over time. To generate high-quality parallel tests, we propose to fine-tune large language models (LLMs) to simulate how previous students would have responded to unseen items. With these simulated responses, we can estimate each item’s difficulty and ambiguity. We first use GPT-4 to generate new test items following a list of expert-developed rules and then apply a fine-tuned LLM to filter the items based on criteria from psychological measurements. We also propose an optimal-transport-inspired technique for generating parallel tests and show the generated tests closely correspond to the original test’s difficulty and reliability based on crowdworker responses. Our evaluation of a generated test with 234 students from grades 2 to 8 produces test scores highly correlated (r=0.93) to those of a standard test form written by human experts and evaluated across thousands of K-12 students.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.135.pdf",
        "keywords": [
            "parallel tests",
            "tests",
            "language models",
            "language model simulations",
            "sentence reading",
            "sentence reading efficiency",
            "case study",
            "transport inspired",
            "educational test"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned in the abstract."
    },
    {
        "title": "Counter Turing Test (CT2): AI-Generated Text Detection is Not as Easy as You May Think - Introducing AI Detectability Index (ADI)",
        "authors": [
            "Megha Chakraborty",
            "S.M Towhidul Islam Tonmoy",
            "S M Mehedi Zaman",
            "Shreya Gautam",
            "Tanay Kumar",
            "Krish Sharma",
            "Niyar Barman",
            "Chandan Gupta",
            "Vinija Jain",
            "Aman Chadha",
            "Amit Sheth",
            "Amitava Das"
        ],
        "published": "2023",
        "summary": "With the rise of prolific ChatGPT, the risk and consequences of AI-generated text has increased alarmingly. This triggered a series of events, including an open letter, signed by thousands of researchers and tech leaders in March 2023, demanding a six-month moratorium on the training of AI systems more sophisticated than GPT-4. To address the inevitable question of ownership attribution for AI-generated artifacts, the US Copyright Office released a statement stating that “if the content is traditional elements of authorship produced by a machine, the work lacks human authorship and the office will not register it for copyright”. Furthermore, both the US and the EU governments have recently drafted their initial proposals regarding the regulatory framework for AI. Given this cynosural spotlight on generative AI, AI-generated text detection (AGTD) has emerged as a topic that has already received immediate attention in research, with some initial methods having been proposed, soon followed by the emergence of techniques to bypass detection. This paper introduces the Counter Turing Test (CT2), a benchmark consisting of techniques aiming to offer a comprehensive evaluation of the robustness of existing AGTD techniques. Our empirical findings unequivocally highlight the fragility of the proposed AGTD methods under scrutiny. Amidst the extensive deliberations on policy-making for regulating AI development, it is of utmost importance to assess the detectability of content generated by LLMs. Thus, to establish a quantifiable spectrum facilitating the evaluation and ranking of LLMs according to their detectability levels, we propose the AI Detectability Index (ADI). We conduct a thorough examination of 15 contemporary LLMs, empirically demonstrating that larger LLMs tend to have a lower ADI, indicating they are less detectable compared to smaller LLMs. We firmly believe that ADI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.136.pdf",
        "keywords": [
            "detectability",
            "counter turing test",
            "generated text detection",
            "generated text",
            "ai generated text detection",
            "ai detectability index",
            "generative ai",
            "human authorship"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our empirical findings unequivocally highlight the fragility of the proposed AGTD methods under scrutiny\"; \"We conduct a thorough examination of 15 contemporary LLMs, empirically demonstrating that larger LLMs tend to have a lower ADI, indicating they are less detectable compared to smaller LLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Our empirical findings unequivocally highlight the fragility of the proposed AGTD methods under scrutiny\"; \"We conduct a thorough examination of 15 contemporary LLMs, empirically demonstrating that larger LLMs tend to have a lower ADI, indicating they are less detectable compared to smaller LLMs.\""
    },
    {
        "title": "Revisiting the Optimality of Word Lengths",
        "authors": [
            "Tiago Pimentel",
            "Clara Meister",
            "Ethan Wilcox",
            "Kyle Mahowald",
            "Ryan Cotterell"
        ],
        "published": "2023",
        "summary": "Zipf (1935) posited that wordforms are optimized to minimize utterances’ communicative costs. Under the assumption that cost is given by an utterance’s length, he supported this claim by showing that words’ lengths are inversely correlated with their frequencies. Communicative cost, however, can be operationalized in different ways. Piantadosi et al. (2011) claim that cost should be measured as the distance between an utterance’s information rate and channel capacity, which we dub the channel capacity hypothesis (CCH) here. Following this logic, they then proposed that a word’s length should be proportional to the expected value of its surprisal (negative log-probability in context). In this work, we show that Piantadosi et al.’s derivation does not minimize CCH’s cost, but rather a lower bound, which we term CCH-lower. We propose a novel derivation, suggesting an improved way to minimize CCH’s cost. Under this method, we find that a language’s word lengths should instead be proportional to the surprisal’s expectation plus its variance-to-mean ratio. Experimentally, we compare these three communicative cost functions: Zipf’s, CCH-lower , and CCH. Across 13 languages and several experimental settings, we find that length is better predicted by frequency than either of the other hypotheses. In fact, when surprisal’s expectation, or expectation plus variance-to-mean ratio, is estimated using better language models, it leads to worse word length predictions. We take these results as evidence that Zipf’s longstanding hypothesis holds.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.137.pdf",
        "keywords": [
            "lengths",
            "communicative cost",
            "language models",
            "s word lengths",
            "zipf"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Instructed Language Models with Retrievers Are Powerful Entity Linkers",
        "authors": [
            "Zilin Xiao",
            "Ming Gong",
            "Jie Wu",
            "Xingyao Zhang",
            "Linjun Shou",
            "Daxin Jiang"
        ],
        "published": "2023",
        "summary": "Generative approaches powered by large language models (LLMs) have demonstrated emergent abilities in tasks that require complex reasoning abilities. Yet the generative nature still makes the generated content suffer from hallucinations, thus unsuitable for entity-centric tasks like entity linking (EL) requiring precise entity predictions over a large knowledge base. We present Instructed Generative Entity Linker (INSGENEL), the first approach that enables casual language models to perform entity linking over knowledge bases. Several methods of equipping language models with EL ability were proposed in this work, including (i) a sequence-to-sequence training EL objective with instruction-tuning, (ii) a novel generative EL framework based on a light-weight potential mention retriever that frees the model from heavy and non-parallelizable decoding, achieving 4× speedup without compromise on linking metrics. INSGENEL outperforms previous generative alternatives with +6.8 F1 points gain on average, also with a huge advantage in training data efficiency and training compute consumption. In addition, our skillfully-engineered in-context learning (ICL) framework for EL still lags behind INSGENEL significantly, reaffirming that the EL task remains a persistent hurdle for general LLMs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.139.pdf",
        "keywords": [
            "language",
            "language models",
            "entity linking",
            "entity linkers",
            "retrievers",
            "context learning"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Yet the generative nature still makes the generated content suffer from hallucinations, thus unsuitable for entity-centric tasks like entity linking (EL) requiring precise entity predictions over a large knowledge base\"; \"reaffirming that the EL task remains a persistent hurdle for general LLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Yet the generative nature still makes the generated content suffer from hallucinations, thus unsuitable for entity-centric tasks like entity linking (EL) requiring precise entity predictions over a large knowledge base\"; \"reaffirming that the EL task remains a persistent hurdle for general LLMs.\""
    },
    {
        "title": "Does the Correctness of Factual Knowledge Matter for Factual Knowledge-Enhanced Pre-trained Language Models?",
        "authors": [
            "Boxi Cao",
            "Qiaoyu Tang",
            "Hongyu Lin",
            "Xianpei Han",
            "Le Sun"
        ],
        "published": "2023",
        "summary": "In recent years, the injection of factual knowledge has been observed to have a significant positive correlation to the downstream task performance of pre-trained language models. However, existing work neither demonstrates that pre-trained models successfully learn the injected factual knowledge nor proves that there is a causal relation between injected factual knowledge and downstream performance improvements. In this paper, we introduce a counterfactual-based analysis framework to explore the causal effects of factual knowledge injection on the performance of language models within pretrain-finetune paradigm. Instead of directly probing the language model or exhaustively enumerating potential confounding factors, we analyze this issue by perturbing the factual knowledge sources at different scales and comparing the performance of pre-trained language models before and after the perturbation. Surprisingly, throughout our experiments, we find that although the knowledge seems to be successfully injected, the correctness of injected knowledge only has a very limited effect on the models’ downstream performance. This finding strongly challenges previous assumptions that the injected factual knowledge is the key for language models to achieve performance improvements on downstream tasks in pretrain-finetune paradigm.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.143.pdf",
        "keywords": [
            "knowledge",
            "factual knowledge",
            "factual knowledge matter",
            "correctness",
            "pre trained language",
            "injected knowledge",
            "pre trained language models",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Surprisingly, throughout our experiments, we find that although the knowledge seems to be successfully injected, the correctness of injected knowledge only has a very limited effect on the models’ downstream performance. This finding strongly challenges previous assumptions that the injected factual knowledge is the key for language models to achieve performance improvements on downstream tasks in pretrain-finetune paradigm.\"\n\nThis rating is chosen because the paper",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Surprisingly, throughout our experiments, we find that although the knowledge seems to be successfully injected, the correctness of injected knowledge only has a very limited effect on the models’ downstream performance. This finding strongly challenges previous assumptions that the injected factual knowledge is the key for language models to achieve performance improvements on downstream tasks in pretrain-finetune paradigm.\"\n\nThis rating is chosen because the paper"
    },
    {
        "title": "Syntactic Substitutability as Unsupervised Dependency Syntax",
        "authors": [
            "Jasper Jian",
            "Siva Reddy"
        ],
        "published": "2023",
        "summary": "Syntax is a latent hierarchical structure which underpins the robust and compositional nature of human language. In this work, we explore the hypothesis that syntactic dependencies can be represented in language model attention distributions and propose a new method to induce these structures theory-agnostically. Instead of modeling syntactic relations as defined by annotation schemata, we model a more general property implicit in the definition of dependency relations, syntactic substitutability. This property captures the fact that words at either end of a dependency can be substituted with words from the same category. Substitutions can be used to generate a set of syntactically invariant sentences whose representations are then used for parsing. We show that increasing the number of substitutions used improves parsing accuracy on natural data. On long-distance subject-verb agreement constructions, our method achieves 79.5% recall compared to 8.9% using a previous method. Our method also provides improvements when transferred to a different parsing setup, demonstrating that it generalizes.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.144.pdf",
        "keywords": [
            "syntactic substitutability",
            "dependency",
            "language model",
            "agreement",
            "unsupervised dependency syntax"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper is related to LLMs as it discusses language model attention distributions and uses them for parsing.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper is related to LLMs as it discusses language model attention distributions and uses them for parsing."
    },
    {
        "title": "The Shifted and The Overlooked: A Task-oriented Investigation of User-GPT Interactions",
        "authors": [
            "Siru Ouyang",
            "Shuohang Wang",
            "Yang Liu",
            "Ming Zhong",
            "Yizhu Jiao",
            "Dan Iter",
            "Reid Pryzant",
            "Chenguang Zhu",
            "Heng Ji",
            "Jiawei Han"
        ],
        "published": "2023",
        "summary": "Recent progress in Large Language Models (LLMs) has produced models that exhibit remarkable performance across a variety of NLP tasks. However, it remains unclear whether the existing focus of NLP research accurately captures the genuine requirements of human users. This paper provides a comprehensive analysis of the divergence between academic research in NLP and the needs of real-world NLP applications via a large-scale collection of user-GPT conversations. We analyze a large-scale collection of real user queries to GPT. We compare these queries against existing NLP benchmark tasks and identify a significant gap between the tasks that users frequently request from LLMs and the tasks that are commonly studied in academic research. For example, we find that tasks such as “design” and “planning” are prevalent in user interactions but largely neglected or different from traditional NLP benchmarks. We investigate these overlooked tasks, dissect the practical challenges, and provide insights toward a roadmap to make LLMs better aligned with user needs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.146.pdf",
        "keywords": [
            "user interactions",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it remains unclear whether the existing focus of NLP research accurately captures the genuine requirements of human users\"; \"We compare these queries against existing NLP benchmark tasks and identify a significant gap between the tasks that users frequently request from LLMs and the tasks that are commonly studied in academic research.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, it remains unclear whether the existing focus of NLP research accurately captures the genuine requirements of human users\"; \"We compare these queries against existing NLP benchmark tasks and identify a significant gap between the tasks that users frequently request from LLMs and the tasks that are commonly studied in academic research.\""
    },
    {
        "title": "Learning the Visualness of Text Using Large Vision-Language Models",
        "authors": [
            "Gaurav Verma",
            "Ryan Rossi",
            "Christopher Tensmeyer",
            "Jiuxiang Gu",
            "Ani Nenkova"
        ],
        "published": "2023",
        "summary": "Visual text evokes an image in a person’s mind, while non-visual text fails to do so. A method to automatically detect visualness in text will enable text-to-image retrieval and generation models to augment text with relevant images. This is particularly challenging with long-form text as text-to-image generation and retrieval models are often triggered for text that is designed to be explicitly visual in nature, whereas long-form text could contain many non-visual sentences. To this end, we curate a dataset of 3,620 English sentences and their visualness scores provided by multiple human annotators. We also propose a fine-tuning strategy that adapts large vision-language models like CLIP by modifying the model’s contrastive learning objective to map text identified as non-visual to a common NULL image while matching visual text to their corresponding images in the document. We evaluate the proposed approach on its ability to (i) classify visual and non-visual text accurately, and (ii) attend over words that are identified as visual in psycholinguistic studies. Empirical evaluation indicates that our approach performs better than several heuristics and baseline models for the proposed task. Furthermore, to highlight the importance of modeling the visualness of text, we conduct qualitative analyses of text-to-image generation systems like DALL-E.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.147.pdf",
        "keywords": [
            "large vision language models",
            "visual text",
            "visualness"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but mentions \"challenging\" and \"limitation\" is not discussed.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but mentions \"challenging\" and \"limitation\" is not discussed."
    },
    {
        "title": "The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values",
        "authors": [
            "Hannah Kirk",
            "Andrew Bean",
            "Bertie Vidgen",
            "Paul Rottger",
            "Scott Hale"
        ],
        "published": "2023",
        "summary": "Human feedback is increasingly used to steer the behaviours of Large Language Models (LLMs). However, it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values. In this paper, we survey existing approaches for learning from human feedback, drawing on 95 papers primarily from the ACL and arXiv repositories. First, we summarise the past, pre-LLM trends for integrating human feedback into language models. Second, we give an overview of present techniques and practices, as well as the motivations for using feedback; conceptual frameworks for defining values and preferences; and how feedback is collected and from whom. Finally, we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.148.pdf",
        "keywords": [
            "subjective human",
            "feedback",
            "feedback learning",
            "language models",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values.\"; \"Finally, we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values.\"; \"Finally, we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges.\""
    },
    {
        "title": "TempTabQA: Temporal Question Answering for Semi-Structured Tables",
        "authors": [
            "Vivek Gupta",
            "Pranshu Kandoi",
            "Mahek Vora",
            "Shuo Zhang",
            "Yujie He",
            "Ridho Reinanda",
            "Vivek Srikumar"
        ],
        "published": "2023",
        "summary": "Semi-structured data, such as Infobox tables, often include temporal information about entities, either implicitly or explicitly. Can current NLP systems reason about such information in semi-structured tables? To tackle this question, we introduce the task of temporal question answering on semi-structured tables. We present a dataset, TEMPTABQA, which comprises 11,454 question-answer pairs extracted from 1,208 Wikipedia Infobox tables spanning more than 90 distinct domains. Using this dataset, we evaluate several state-of-the-art models for temporal reasoning. We observe that even the top-performing LLMs lag behind human performance by more than 13.5 F1 points. Given these results, our dataset has the potential to serve as a challenging benchmark to improve the temporal reasoning capabilities of NLP models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.149.pdf",
        "keywords": [
            "temporal reasoning",
            "temporal question answering",
            "infobox tables"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We observe that even the top-performing LLMs lag behind human performance by more than 13.5 F1 points.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We observe that even the top-performing LLMs lag behind human performance by more than 13.5 F1 points.\""
    },
    {
        "title": "Task-Level Thinking Steps Help Large Language Models for Challenging Classification Task",
        "authors": [
            "Chunhui Du",
            "Jidong Tian",
            "Haoran Liao",
            "Jindou Chen",
            "Hao He",
            "Yaohui Jin"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have shown incredible performance on many tasks such as dialogue generation, commonsense reasoning and question answering. In-context learning (ICL) is an important paradigm for adapting LLMs to the downstream tasks by prompting few demonstrations. However, the distribution of demonstrations can severely affect the performance, especially for challenging classification tasks. In this paper, we propose the concept of task-level thinking steps that can eliminate bias introduced by demonstrations. Further, to help LLMs distinguish confusing classes, we design a progressive revision framework, which can improve the thinking steps by correcting hard demonstrations. Experimental results prove the superiority of our proposed method, achieving best performance on three kinds of challenging classification tasks in the zero-shot and few-shot settings. Besides, with task-level thinking steps, automatically generated chain-of-thoughts (CoTs) bring more competitive performance.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.150.pdf",
        "keywords": [
            "classification",
            "thinking steps",
            "task level thinking steps",
            "demonstrations",
            "language models",
            "context"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the distribution of demonstrations can severely affect the performance, especially for challenging classification tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the distribution of demonstrations can severely affect the performance, especially for challenging classification tasks.\""
    },
    {
        "title": "RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation",
        "authors": [
            "Fengji Zhang",
            "Bei Chen",
            "Yue Zhang",
            "Jacky Keung",
            "Jin Liu",
            "Daoguang Zan",
            "Yi Mao",
            "Jian-Guang Lou",
            "Weizhu Chen"
        ],
        "published": "2023",
        "summary": "The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoBench, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark will be publicly available after the paper review.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.151.pdf",
        "keywords": [
            "repository",
            "repocoder",
            "code completion",
            "repository level code completion",
            "repository level",
            "iterative retrieval",
            "generation",
            "benchmark",
            "unfinished code"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While for automated code completion tools, it is difficult to utilize the useful information scattered in different files.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"While for automated code completion tools, it is difficult to utilize the useful information scattered in different files.\""
    },
    {
        "title": "Influence Scores at Scale for Efficient Language Data Sampling",
        "authors": [
            "Nikhil Anand",
            "Joshua Tan",
            "Maria Minakova"
        ],
        "published": "2023",
        "summary": "Modern ML systems ingest data aggregated from diverse sources, such as synthetic, human-annotated, and live customer traffic. Understanding which examples are important to the performance of a learning algorithm is crucial for efficient model training. Recently, a growing body of literature has given rise to various “influence scores,” which use training artifacts such as model confidence or checkpointed gradients to identify important subsets of data. However, these methods have primarily been developed in computer vision settings, and it remains unclear how well they generalize to language-based tasks using pretrained models. In this paper, we explore the applicability of influence scores in language classification tasks. We evaluate a diverse subset of these scores on the SNLI dataset by quantifying accuracy changes in response to pruning training data through random and influence-score-based sampling. We then stress-test one of the scores – “variance of gradients” (VoG) from Agarwal and Hooker (2022) – in an NLU model stack that was exposed to dynamic user speech patterns in a voice assistant type of setting. Our experiments demonstrate that in many cases, encoder-based language models can be fine-tuned on roughly 50% of the original data without degradation in performance metrics. Along the way, we summarize lessons learned from applying out-of-the-box implementations of influence scores, quantify the effects of noisy and class-imbalanced data, and offer recommendations on score-based sampling for better accuracy and training efficiency.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.152.pdf",
        "keywords": [
            "score",
            "influence scores",
            "model confidence",
            "language models",
            "language classification",
            "sampling",
            "language data sampling"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations of LLMs, but the paper discusses the efficiency of model training and the potential to fine-tune encoder-based language models on a subset of the original data, implying that the current methods might not be optimal.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: No explicit mention of limitations of LLMs, but the paper discusses the efficiency of model training and the potential to fine-tune encoder-based language models on a subset of the original data, implying that the current methods might not be optimal."
    },
    {
        "title": "G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment",
        "authors": [
            "Yang Liu",
            "Dan Iter",
            "Yichong Xu",
            "Shuohang Wang",
            "Ruochen Xu",
            "Chenguang Zhu"
        ],
        "published": "2023",
        "summary": "The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose analysis on the behavior of LLM-based evaluators, and highlight the potential concern of LLM-based evaluators having a bias towards the LLM-generated texts.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.153.pdf",
        "keywords": [
            "nlg evaluation",
            "bleu",
            "summarization",
            "text summarization",
            "language models",
            "dialogue generation",
            "human alignment",
            "human correspondence",
            "natural language generation"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators.\"; \"We also propose analysis on the behavior of LLM-based evaluators, and highlight the potential concern of LLM-based evaluators having a bias towards the LLM-generated texts.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators.\"; \"We also propose analysis on the behavior of LLM-based evaluators, and highlight the potential concern of LLM-based evaluators having a bias towards the LLM-generated texts.\""
    },
    {
        "title": "Learning Retrieval Augmentation for Personalized Dialogue Generation",
        "authors": [
            "Qiushi Huang",
            "Shuai Fu",
            "Xubo Liu",
            "Wenwu Wang",
            "Tom Ko",
            "Yu Zhang",
            "Lilian Tang"
        ],
        "published": "2023",
        "summary": "Personalized dialogue generation, focusing on generating highly tailored responses by leveraging persona profiles and dialogue context, has gained significant attention in conversational AI applications. However, persona profiles, a prevalent setting in current personalized dialogue datasets, typically composed of merely four to five sentences, may not offer comprehensive descriptions of the persona about the agent, posing a challenge to generate truly personalized dialogues. To handle this problem, we propose Learning Retrieval Augmentation for Personalized DialOgue Generation (LAPDOG), which studies the potential of leveraging external knowledge for persona dialogue generation. Specifically, the proposed LAPDOG model consists of a story retriever and a dialogue generator. The story retriever uses a given persona profile as queries to retrieve relevant information from the story document, which serves as a supplementary context to augment the persona profile. The dialogue generator utilizes both the dialogue history and the augmented persona profile to generate personalized responses. For optimization, we adopt a joint training framework that collaboratively learns the story retriever and dialogue generator, where the story retriever is optimized towards desired ultimate metrics (e.g., BLEU) to retrieve content for the dialogue generator to generate personalized responses. Experiments conducted on the CONVAI2 dataset with ROCStory as a supplementary data source show that the proposed LAPDOG method substantially outperforms the baselines, indicating the effectiveness of the proposed method. The LAPDOG model code is publicly available for further exploration.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.154.pdf",
        "keywords": [
            "personalized",
            "personalized dialogue generation"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations",
        "authors": [
            "Vipula Rawte",
            "Swagata Chakraborty",
            "Agnibh Pathak",
            "Anubhav Sarkar",
            "S.M Towhidul Islam Tonmoy",
            "Aman Chadha",
            "Amit Sheth",
            "Amitava Das"
        ],
        "published": "2023",
        "summary": "The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary LLMs along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank LLMs based on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index (HVI). Amidst the extensive deliberations on policy-making for regulating AI development, it is of utmost importance to assess and measure which LLM is more vulnerable towards hallucination. We firmly believe that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.155.pdf",
        "keywords": [
            "hallucination",
            "categorize hallucination",
            "large language models",
            "hallucination vulnerability"
        ],
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns.\""
    },
    {
        "title": "NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders",
        "authors": [
            "Livio Soares",
            "Daniel Gillick",
            "Jeremy Cole",
            "Tom Kwiatkowski"
        ],
        "published": "2023",
        "summary": "Neural document rerankers are extremely effective in terms of accuracy. However, the best models require dedicated hardware for serving, which is costly and often not feasible. To avoid this servingtime requirement, we present a method of capturing up to 86% of the gains of a Transformer cross-attention model with a lexicalized scoring function that only requires 10-6% of the Transformer’s FLOPs per document and can be served using commodity CPUs. When combined with a BM25 retriever, this approach matches the quality of a state-of-the art dual encoder retriever, that still requires an accelerator for query encoding. We introduce nail (Non-Autoregressive Indexing with Language models) as a model architecture that is compatible with recent encoder-decoder and decoder-only large language models, such as T5, GPT-3 and PaLM. This model architecture can leverage existing pre-trained checkpoints and can be fine-tuned for efficiently constructing document representations that do not require neural processing of queries.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.156.pdf",
        "keywords": [
            "lexical retrieval indices",
            "language",
            "cross attention"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the best models require dedicated hardware for serving, which is costly and often not feasible.\"\n\nThis paper discusses LLMs but mentions a limitation (requirement of dedicated hardware) only in passing, without further analysis or elaboration.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, the best models require dedicated hardware for serving, which is costly and often not feasible.\"\n\nThis paper discusses LLMs but mentions a limitation (requirement of dedicated hardware) only in passing, without further analysis or elaboration."
    },
    {
        "title": "Analyzing Modular Approaches for Visual Question Decomposition",
        "authors": [
            "Apoorv Khandelwal",
            "Ellie Pavlick",
            "Chen Sun"
        ],
        "published": "2023",
        "summary": "Modular neural networks without additional training have recently been shown to surpass end-to-end neural networks on challenging vision–language tasks. The latest such methods simultaneously introduce LLM-based code generation to build programs and a number of skill-specific, task-oriented modules to execute them. In this paper, we focus on ViperGPT and ask where its additional performance comes from and how much is due to the (state-of-art, end-to-end) BLIP-2 model it subsumes vs. additional symbolic components. To do so, we conduct a controlled study (comparing end-to-end, modular, and prompting-based methods across several VQA benchmarks). We find that ViperGPT’s reported gains over BLIP-2 can be attributed to its selection of task-specific modules, and when we run ViperGPT using a more task-agnostic selection of modules, these gains go away. ViperGPT retains much of its performance if we make prominent alterations to its selection of modules: e.g. removing or retaining only BLIP-2. We also compare ViperGPT against a prompting-based decomposition strategy and find that, on some benchmarks, modular approaches significantly benefit by representing subtasks with natural language, instead of code. Our code is fully available at https://github.com/brown-palm/visual-question-decomposition.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.157.pdf",
        "keywords": [
            "visual question decomposition",
            "modular neural networks",
            "modular approaches",
            "vipergpt"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that ViperGPT’s reported gains over BLIP-2 can be attributed to its selection of task-specific modules, and when we run ViperGPT using a more task-agnostic selection of modules, these gains go away.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We find that ViperGPT’s reported gains over BLIP-2 can be attributed to its selection of task-specific modules, and when we run ViperGPT using a more task-agnostic selection of modules, these gains go away.\""
    },
    {
        "title": "Improving Summarization with Human Edits",
        "authors": [
            "Zonghai Yao",
            "Benjamin Schloss",
            "Sai Selvaraj"
        ],
        "published": "2023",
        "summary": "Recent work has shown the promise of learning with human feedback paradigms to produce human-determined high-quality text. Existing works use human feedback to train large language models (LLMs) in general domain abstractive summarization and have obtained summary quality exceeding traditional likelihood training. In this paper, we focus on a less explored form of human feedback – Human Edits. We propose Sequence Alignment (un)Likelihood Training (SALT), a novel technique to use both the human-edited and model-generated data together in the training loop. In addition, we demonstrate simulating Human Edits with ground truth summaries coming from existing training data – Imitation edits, along with the model-generated summaries obtained after the training, to reduce the need for expensive human-edit data. In our experiments, we extend human feedback exploration from general domain summarization to medical domain summarization. Our results demonstrate the effectiveness of SALT in improving the summary quality with Human and Imitation Edits. Through additional experiments, we show that SALT outperforms the conventional RLHF method (designed for human preferences) – DPO, when applied to human-edit data. We hope the evidence in our paper prompts researchers to explore, collect, and better use different human feedback approaches scalably.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.158.pdf",
        "keywords": [
            "summarization",
            "sequence alignment",
            "human feedback paradigms",
            "human preferences"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "The Skipped Beat: A Study of Sociopragmatic Understanding in LLMs for 64 Languages",
        "authors": [
            "Chiyu Zhang",
            "Khai Doan",
            "Qisheng Liao",
            "Muhammad Abdul-Mageed"
        ],
        "published": "2023",
        "summary": "Instruction tuned large language models (LLMs), such as ChatGPT, demonstrate remarkable performance in a wide range of tasks. Despite numerous recent studies that examine the performance of instruction-tuned LLMs on various NLP benchmarks, there remains a lack of comprehensive investigation into their ability to understand cross-lingual sociopragmatic meaning (SM), i.e., meaning embedded within social and interactive contexts. This deficiency arises partly from SM not being adequately represented in any of the existing benchmarks. To address this gap, we present SPARROW, an extensive multilingual benchmark specifically designed for SM understanding. SPARROW comprises 169 datasets covering 13 task types across six primary categories (e.g., anti-social language detection, emotion recognition). SPARROW datasets encompass 64 different languages originating from 12 language families representing 16 writing scripts. We evaluate the performance of various multilingual pretrained language models (e.g., mT5) and instruction-tuned LLMs (e.g., BLOOMZ, ChatGPT) on SPARROW through fine-tuning, zero-shot, and/or few-shot learning. Our comprehensive analysis reveals that existing open-source instruction tuned LLMs still struggle to understand SM across various languages, performing close to a random baseline in some cases. We also find that although ChatGPT outperforms many LLMs, it still falls behind task-specific finetuned models with a gap of 12.19 SPARROW score. Our benchmark is available at: https://github.com/UBC-NLP/SPARROW",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.160.pdf",
        "keywords": [
            "instruction tuned",
            "skipped beat",
            "instruction tuned llms",
            "sociopragmatic meaning"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our comprehensive analysis reveals that existing open-source instruction tuned LLMs still struggle to understand SM across various languages, performing close to a random baseline in some cases.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Our comprehensive analysis reveals that existing open-source instruction tuned LLMs still struggle to understand SM across various languages, performing close to a random baseline in some cases.\""
    },
    {
        "title": "Understanding the Effect of Model Compression on Social Bias in Large Language Models",
        "authors": [
            "Gustavo Gonçalves",
            "Emma Strubell"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model’s predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs. Despite the popularity and need for both approaches, little work has been done to explore the interplay between these two. We perform a carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs. Longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.161.pdf",
        "keywords": [
            "social bias",
            "model compression",
            "social biases",
            "large language models",
            "carefully controlled",
            "knowledge distillation"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model’s predictions in downstream tasks, leading to representational harm.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model’s predictions in downstream tasks, leading to representational harm.\""
    },
    {
        "title": "BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology",
        "authors": [
            "Odhran O’Donoghue",
            "Aleksandar Shtedritski",
            "John Ginger",
            "Ralph Abboud",
            "Ali Ghareeb",
            "Samuel Rodriques"
        ],
        "published": "2023",
        "summary": "The ability to automatically generate accurate protocols for scientific experiments would represent a major step towards the automation of science. Large Language Models (LLMs) have impressive capabilities on a wide range of tasks, such as question answering and the generation of coherent text and code. However, LLMs can struggle with multi-step problems and long-term planning, which are crucial for designing scientific experiments. Moreover, evaluation of the accuracy of scientific protocols is challenging, because experiments can be described correctly in many different ways, require expert knowledge to evaluate, and cannot usually be executed automatically. Here we present an automatic evaluation framework for the task of planning experimental protocols, and we introduce BioProt: a dataset of biology protocols with corresponding pseudocode representations. To measure performance on generating scientific protocols, we use an LLM to convert a natural language protocol into pseudocode, and then evaluate an LLM’s ability to reconstruct the pseudocode from a high-level description and a list of admissible pseudocode functions. We evaluate GPT-3 and GPT-4 on this task and explore their robustness. We externally validate the utility of pseudocode representations of text by generating accurate novel protocols using retrieved pseudocode, and we run a generated protocol successfully in our biological laboratory. Our framework is extensible to the evaluation and improvement of language model",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.162.pdf",
        "keywords": [
            "protocol planning",
            "pseudocode representations"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, LLMs can struggle with multi-step problems and long-term planning, which are crucial for designing scientific experiments.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, LLMs can struggle with multi-step problems and long-term planning, which are crucial for designing scientific experiments.\""
    },
    {
        "title": "Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning across Languages",
        "authors": [
            "Libo Qin",
            "Qiguang Chen",
            "Fuxuan Wei",
            "Shijue Huang",
            "Wanxiang Che"
        ],
        "published": "2023",
        "summary": "Chain-of-thought (CoT) is capable of eliciting models to explicitly generate reasoning paths, thus promoting reasoning accuracy and attracting increasing attention. Specifically, zero-shot CoT achieves remarkable improvements in a wide range of reasoning tasks by simply instructing the LLM with the prompt “Let’s think step by step!”. Despite the success of zero-shot CoT, the existing zero-shot prompting techniques remain limited to a single language, making it challenging to generalize to other languages and hindering global development. In this work, we introduce cross-lingual prompting (CLP), aiming to improve zero-shot CoT reasoning across languages. Specifically, CLP consists of two main components: (1) cross-lingual alignment prompting and (2) task-specific solver prompting. The cross-lingual alignment prompting is responsible for aligning representations across different languages, whereas the task-specific solver prompting is used to generate the final chain of thoughts and results for the reasoning task. In addition, we further introduce cross-lingual self-consistent prompting (CLSP) to ensemble different reasoning paths across languages. Our experimental evaluations on several benchmarks demonstrate that CLP and CLSP significantly outperform the existing prompting methods and achieve state-of-the-art performance. We hope this work will inspire further breakthroughs in cross-lingual CoT.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.163.pdf",
        "keywords": [
            "prompting",
            "cross lingual prompting",
            "languages",
            "chain of thought",
            "shot cot reasoning",
            "self consistent prompting",
            "zero shot prompting"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the success of zero-shot CoT, the existing zero-shot prompting techniques remain limited to a single language, making it challenging to generalize to other languages and hindering global development.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite the success of zero-shot CoT, the existing zero-shot prompting techniques remain limited to a single language, making it challenging to generalize to other languages and hindering global development.\""
    },
    {
        "title": "FinGPT: Large Generative Models for a Small Language",
        "authors": [
            "Risto Luukkonen",
            "Ville Komulainen",
            "Jouni Luoma",
            "Anni Eskelinen",
            "Jenna Kanerva",
            "Hanna-Mari Kupari",
            "Filip Ginter",
            "Veronika Laippala",
            "Niklas Muennighoff",
            "Aleksandra Piktus",
            "Thomas Wang",
            "Nouamane Tazi",
            "Teven Scao",
            "Thomas Wolf",
            "Osma Suominen",
            "Samuli Sairanen",
            "Mikko Merioksa",
            "Jyrki Heinonen",
            "Aija Vahtola",
            "Samuel Antao",
            "Sampo Pyysalo"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) excel in many tasks in NLP and beyond, but most open models have very limited coverage of smaller languages and LLM work tends to focus on languages where nearly unlimited data is available for pretraining. In this work, we study the challenges of creating LLMs for Finnish, a language spoken by less than 0.1% of the world population. We compile an extensive dataset of Finnish combining web crawls, news, social media and eBooks. We pursue two approaches to pretrain models: 1) we train seven monolingual models from scratch (186M to 13B parameters) dubbed FinGPT, 2) we continue the pretraining of the multilingual BLOOM model on a mix of its original training data and Finnish, resulting in a 176 billion parameter model we call BLUUMI. For model evaluation, we introduce FIN-bench, a version of BIG-bench with Finnish tasks. We also assess other model qualities such as toxicity and bias. Our models and tools are openly available at https://turkunlp.org/gpt3-finnish.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.164.pdf",
        "keywords": [
            "large language models",
            "fingpt",
            "ebooks"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"most open models have very limited coverage of smaller languages\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"most open models have very limited coverage of smaller languages\""
    },
    {
        "title": "Indicative Summarization of Long Discussions",
        "authors": [
            "Shahbaz Syed",
            "Dominik Schwabe",
            "Khalid Al-Khatib",
            "Martin Potthast"
        ],
        "published": "2023",
        "summary": "Online forums encourage the exchange and discussion of different stances on many topics. Not only do they provide an opportunity to present one’s own arguments, but may also gather a broad cross-section of others’ arguments. However, the resulting long discussions are difficult to overview. This paper presents a novel unsupervised approach using large language models (LLMs) to generating indicative summaries for long discussions that basically serve as tables of contents. Our approach first clusters argument sentences, generates cluster labels as abstractive summaries, and classifies the generated cluster labels into argumentation frames resulting in a two-level summary. Based on an extensively optimized prompt engineering approach, we evaluate 19 LLMs for generative cluster labeling and frame classification. To evaluate the usefulness of our indicative summaries, we conduct a purpose-driven user study via a new visual interface called **Discussion Explorer**: It shows that our proposed indicative summaries serve as a convenient navigation tool to explore long discussions.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.166.pdf",
        "keywords": [
            "online forums",
            "argumentation"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions using 19 LLMs for generative cluster labeling and frame classification, implying that the paper does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions using 19 LLMs for generative cluster labeling and frame classification, implying that the paper does not discuss any limitations of LLMs."
    },
    {
        "title": "Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts",
        "authors": [
            "Tengxiao Liu",
            "Qipeng Guo",
            "Yuqing Yang",
            "Xiangkun Hu",
            "Yue Zhang",
            "Xipeng Qiu",
            "Zheng Zhang"
        ],
        "published": "2023",
        "summary": "As large language models (LLMs) have shown effectiveness with different prompting methods, such as Chain of Thought, Program of Thought, we find that these methods have formed a great complementarity to each other on math reasoning tasks. In this work, we propose XoT, an integrated problem solving framework by prompting LLMs with diverse reasoning thoughts. For each question, XoT always begins with selecting the most suitable method then executes each method iteratively. Within each iteration, XoT actively checks the validity of the generated answer and incorporates the feedback from external executors, allowing it to dynamically switch among different prompting methods. Through extensive experiments on 10 popular math reasoning datasets, we demonstrate the effectiveness of our proposed approach and thoroughly analyze the strengths of each module. Moreover, empirical results suggest that our framework is orthogonal to recent work that makes improvements on single reasoning methods and can further generalise to logical reasoning domain. By allowing method switching, XoT provides a fresh perspective on the collaborative integration of diverse reasoning thoughts in a unified framework.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.169.pdf",
        "keywords": [
            "prompting",
            "collaborative integration",
            "diverse reasoning",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper discusses improving the effectiveness of LLMs with diverse prompting methods, implying that LLMs may have limitations in certain reasoning tasks without these methods.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper discusses improving the effectiveness of LLMs with diverse prompting methods, implying that LLMs may have limitations in certain reasoning tasks without these methods."
    },
    {
        "title": "Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text Generation",
        "authors": [
            "Mateusz Lango",
            "Ondrej Dusek"
        ],
        "published": "2023",
        "summary": "Hallucination of text ungrounded in the input is a well-known problem in neural data-to-text generation. Many methods have been proposed to mitigate it, but they typically require altering model architecture or collecting additional data, and thus cannot be easily applied to an existing model. In this paper, we explore a new way to mitigate hallucinations by combining the probabilistic output of a generator language model (LM) with the output of a special “text critic” classifier, which guides the generation by assessing the match between the input data and the text generated so far. Our method does not need any changes to the underlying LM’s architecture or training procedure and can thus be combined with any model and decoding operating on word probabilities. The critic does not need any additional training data, using the base LM’s training data and synthetic negative examples. Our experimental results show that our method improves over the baseline on the WebNLG and OpenDialKG benchmarks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.172.pdf",
        "keywords": [
            "hallucination",
            "decoding",
            "benchmarks",
            "generator language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Hallucination of text ungrounded in the input is a well-known problem in neural data-to-text generation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Hallucination of text ungrounded in the input is a well-known problem in neural data-to-text generation.\""
    },
    {
        "title": "DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models",
        "authors": [
            "Xinwei Wu",
            "Junzhuo Li",
            "Minghui Xu",
            "Weilong Dong",
            "Shuangzhi Wu",
            "Chao Bian",
            "Deyi Xiong"
        ],
        "published": "2023",
        "summary": "Pretrained language models have learned a vast amount of human knowledge from large-scale corpora, but their powerful memorization capability also brings the risk of data leakage. Some risks may only be discovered after the model training is completed, such as the model memorizing a specific phone number and frequently outputting it. In such cases, model developers need to eliminate specific data influences from the model to mitigate legal and ethical penalties. To effectively mitigate these risks, people often have to spend a significant amount of time and computational costs to retrain new models instead of finding ways to cure the ‘sick’ models. Therefore, we propose a method to locate and erase risky neurons in order to eliminate the impact of privacy data in the model. We use a new method based on integrated gradients to locate neurons associated with privacy texts, and then erase these neurons by setting their activation values to zero.Furthermore, we propose a risky neuron aggregation method to eliminate the influence of privacy data in the model in batches. Experimental results show that our method can effectively and quickly eliminate the impact of privacy data without affecting the model’s performance. Additionally, we demonstrate the relationship between model memorization and neurons through experiments, further illustrating the robustness of our method.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.174.pdf",
        "keywords": [
            "language models",
            "privacy",
            "risky",
            "risky neuron aggregation"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Pretrained language models have learned a vast amount of human knowledge from large-scale corpora, but their powerful memorization capability also brings the risk of data leakage. Some risks may only be discovered after the model training is completed, such as the model memorizing a specific phone number and frequently outputting it.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Pretrained language models have learned a vast amount of human knowledge from large-scale corpora, but their powerful memorization capability also brings the risk of data leakage. Some risks may only be discovered after the model training is completed, such as the model memorizing a specific phone number and frequently outputting it.\""
    },
    {
        "title": "Investigating Bias in Multilingual Language Models: Cross-Lingual Transfer of Debiasing Techniques",
        "authors": [
            "Manon Reusens",
            "Philipp Borchert",
            "Margot Mieskes",
            "Jochen De Weerdt",
            "Bart Baesens"
        ],
        "published": "2023",
        "summary": "This paper investigates the transferability of debiasing techniques across different languages within multilingual models. We examine the applicability of these techniques in English, French, German, and Dutch. Using multilingual BERT (mBERT), we demonstrate that cross-lingual transfer of debiasing techniques is not only feasible but also yields promising results. Surprisingly, our findings reveal no performance disadvantages when applying these techniques to non-English languages. Using translations of the CrowS-Pairs dataset, our analysis identifies SentenceDebias as the best technique across different languages, reducing bias in mBERT by an average of 13%. We also find that debiasing techniques with additional pretraining exhibit enhanced cross-lingual effectiveness for the languages included in the analyses, particularly in lower-resource languages. These novel insights contribute to a deeper understanding of bias mitigation in multilingual language models and provide practical guidance for debiasing techniques in different language contexts.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.175.pdf",
        "keywords": [
            "debiasing",
            "debiasing techniques",
            "multilingual language models",
            "multilingual models",
            "bias"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No direct evidence, but the paper discusses bias mitigation in multilingual language models, implying that LLMs have bias limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: No direct evidence, but the paper discusses bias mitigation in multilingual language models, implying that LLMs have bias limitations."
    },
    {
        "title": "Can Language Models Laugh at YouTube Short-form Videos?",
        "authors": [
            "Dayoon Ko",
            "Sangho Lee",
            "Gunhee Kim"
        ],
        "published": "2023",
        "summary": "As short-form funny videos on social networks are gaining popularity, it becomes demanding for AI models to understand them for better communication with humans. Unfortunately, previous video humor datasets target specific domains such as speeches or sitcoms, and mostly focus on verbal cues. We curate a user-generated dataset of 10K multimodal funny videos from YouTube, called ExFunTube. Using a video filtering pipeline with GPT-3.5, we verify both verbal and visual elements contributing to humor. After filtering, we annotate each video with timestamps and text explanations for funny moments. Our ExFunTube is unique over existing datasets in that our videos cover a wide range of domains with various types of humor that necessitate a multimodal understanding of the content. Also, we develop a zero-shot video-to-text prompting to maximize video humor understanding of large language models (LLMs). With three different evaluation methods using automatic scores, rationale quality experiments, and human evaluations, we show that our prompting significantly improves LLMs’ ability for humor explanation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.176.pdf",
        "keywords": [
            "funny videos",
            "youtube",
            "exfuntube",
            "video humor",
            "video filtering pipeline",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions that the developed zero-shot video-to-text prompting \"significantly improves LLMs’ ability for humor explanation\", implying that LLMs had limitations in this area before.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions that the developed zero-shot video-to-text prompting \"significantly improves LLMs’ ability for humor explanation\", implying that LLMs had limitations in this area before."
    },
    {
        "title": "HistAlign: Improving Context Dependency in Language Generation by Aligning with History",
        "authors": [
            "David Wan",
            "Shiyue Zhang",
            "Mohit Bansal"
        ],
        "published": "2023",
        "summary": "Language models (LMs) can generate hallucinations and incoherent outputs, which highlights their weak context dependency. Cache-LMs, which augment LMs with a memory of recent history, can increase context dependency and have shown remarkable performance in diverse language generation tasks. However, we find that even with training, the performance gain stemming from the cache component of current cache-LMs is suboptimal due to the misalignment between the current hidden states and those stored in the memory. In this work, we present HistAlign, a new training approach to ensure good cache alignment such that the model receives useful signals from the history. We first prove our concept on a simple and synthetic task where the memory is essential for correct predictions, and we show that the cache component of HistAlign is better aligned and improves overall performance. Next, we evaluate HistAlign on diverse downstream language generation tasks, including prompt continuation, abstractive summarization, and data-to-text. We demonstrate that HistAlign improves text coherence and faithfulness in open-ended and conditional generation settings respectively. HistAlign is also generalizable across different model families, showcasing its strength in improving context dependency of LMs in diverse scenarios.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.179.pdf",
        "keywords": [
            "context dependency",
            "aligning",
            "cache alignment",
            "language",
            "language generation",
            "language models",
            "history"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Language models (LMs) can generate hallucinations and incoherent outputs, which highlights their weak context dependency.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Language models (LMs) can generate hallucinations and incoherent outputs, which highlights their weak context dependency.\""
    },
    {
        "title": "CombLM: Adapting Black-Box Language Models through Small Fine-Tuned Models",
        "authors": [
            "Aitor Ormazabal",
            "Mikel Artetxe",
            "Eneko Agirre"
        ],
        "published": "2023",
        "summary": "Methods for adapting language models (LMs) to new tasks and domains have traditionally assumed white-box access to the model, and work by modifying its parameters. However, this is incompatible with a recent trend in the field, where the highest quality models are only available as black-boxes through inference APIs. Even when the model weights are available, the computational cost of fine-tuning large LMs can be prohibitive for most practitioners. In this work, we present a lightweight method for adapting large LMs to new domains and tasks, assuming no access to their weights or intermediate activations. Our approach fine-tunes a small white-box LM and combines it with the large black-box LM at the probability level through a small network, learned on a small validation set. We validate our approach by adapting a large LM (OPT-30B) to several domains and a downstream task (machine translation), observing improved performance in all cases, of up to 9%, while using a domain expert 23x smaller.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.180.pdf",
        "keywords": [
            "adapting language models",
            "black box language",
            "black box"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Even when the model weights are available, the computational cost of fine-tuning large LMs can be prohibitive for most practitioners.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Even when the model weights are available, the computational cost of fine-tuning large LMs can be prohibitive for most practitioners.\""
    },
    {
        "title": "Image Manipulation via Multi-Hop Instructions - A New Dataset and Weakly-Supervised Neuro-Symbolic Approach",
        "authors": [
            "Harman Singh",
            "Poorva Garg",
            "Mohit Gupta",
            "Kevin Shah",
            "Ashish Goswami",
            "Satyam Modi",
            "Arnab Mondal",
            "Dinesh Khandelwal",
            "Dinesh Garg",
            "Parag Singla"
        ],
        "published": "2023",
        "summary": "We are interested in image manipulation via natural language text – a task that is useful for multiple AI applications but requires complex reasoning over multi-modal spaces. We extend recently proposed Neuro Symbolic Concept Learning (NSCL), which has been quite effective for the task of Visual Question Answering (VQA), for the task of image manipulation. Our system referred to as NeuroSIM can perform complex multi-hop reasoning over multi-object scenes and only requires weak supervision in the form of annotated data for VQA. NeuroSIM parses an instruction into a symbolic program, based on a Domain Specific Language (DSL) comprising of object attributes and manipulation operations, that guides its execution. We create a new dataset for the task, and extensive experiments demonstrate that NeuroSIM is highly competitive with or beats SOTA baselines that make use of supervised data for manipulation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.181.pdf",
        "keywords": [
            "image manipulation",
            "visual question answering",
            "domain specific language",
            "weakly supervised",
            "neurosim"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Generative Spoken Language Model based on continuous word-sized audio tokens",
        "authors": [
            "Robin Algayres",
            "Yossi Adi",
            "Tu Nguyen",
            "Jade Copet",
            "Gabriel Synnaeve",
            "Benoît Sagot",
            "Emmanuel Dupoux"
        ],
        "published": "2023",
        "summary": "In NLP, text language models based on words or subwords are known to outperform their character-based counterparts. Yet, in the speech community, the standard input of spoken LMs are 20ms or 40ms-long discrete units (shorter than a phoneme). Taking inspiration from word-based LM, we introduce a Generative Spoken Language Model (GSLM) based on word-size continuous-valued audio tokens that can generate diverse and expressive language output. This is obtained by replacing lookup table for lexical types with a Lexical Embedding function, the cross entropy loss by a contrastive loss, and multinomial sampling by k-NN sampling. The resulting model is the first generative language model based on word-size continuous tokens. Its performance is on par with discrete unit GSLMs regarding generation quality as measured by automatic metrics and subjective human judgements. Moreover, it is five times more memory efficient thanks to its large 200ms units. In addition, the embeddings before and after the Lexical Embedder are phonetically and semantically interpretable.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.182.pdf",
        "keywords": [
            "audio tokens",
            "spoken language model",
            "continuous tokens",
            "generative language model",
            "continuous",
            "text language models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions that the standard input of spoken LMs are 20ms or 40ms-long discrete units, which can be seen as a limitation of traditional spoken LMs, however, it is not explicitly stated as a limitation in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions that the standard input of spoken LMs are 20ms or 40ms-long discrete units, which can be seen as a limitation of traditional spoken LMs, however, it is not explicitly stated as a limitation in the abstract."
    },
    {
        "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations",
        "authors": [
            "Ning Ding",
            "Yulin Chen",
            "Bokai Xu",
            "Yujia Qin",
            "Shengding Hu",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Bowen Zhou"
        ],
        "published": "2023",
        "summary": "Fine-tuning on instruction data has been widely validated as an effective practice for implementing chat language models like ChatGPT. Scaling the diversity and quality of such data, although straightforward, stands a great chance of leading to improved performance. This paper aims to push the upper bound of open-source models further. We first provide a systematically designed, diverse, informative, large-scale dataset of instructional conversations, UltraChat, which does not involve human queries. Our objective is to capture the breadth of interactions between a human user and an AI assistant and employs a comprehensive framework to generate multi-turn conversation iteratively. UltraChat contains 1.5 million high-quality multi-turn dialogues and covers a wide range of topics and instructions. Our statistical analysis of UltraChat reveals its superiority in various key metrics, including scale, average length, diversity, coherence, etc., solidifying its position as a leading open-source dataset. Building upon UltraChat, we fine-tune a LLaMA model to create a powerful conversational model, UltraLM. Our evaluations indicate that UltraLM consistently outperforms other open-source models, including WizardLM and Vicuna, the previously recognized state-of-the-art open-source models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.183.pdf",
        "keywords": [
            "instructional conversations",
            "fine tune",
            "ultrachat"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned, but the goal of the paper is to \"push the upper bound of open-source models further\", implying that current models have limitations that can be improved upon.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned, but the goal of the paper is to \"push the upper bound of open-source models further\", implying that current models have limitations that can be improved upon."
    },
    {
        "title": "API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs",
        "authors": [
            "Minghao Li",
            "Yingxiu Zhao",
            "Bowen Yu",
            "Feifan Song",
            "Hangyu Li",
            "Haiyang Yu",
            "Zhoujun Li",
            "Fei Huang",
            "Yongbin Li"
        ],
        "published": "2023",
        "summary": "Recent research has demonstrated that Large Language Models (LLMs) can enhance their capabilities by utilizing external tools. However, three pivotal questions remain unanswered: (1) How effective are current LLMs in utilizing tools? (2) How can we enhance LLMs’ ability to utilize tools? (3) What obstacles need to be overcome to leverage tools? To address these questions, we introduce API-Bank, a groundbreaking benchmark, specifically designed for tool-augmented LLMs. For the first question, we develop a runnable evaluation system consisting of 73 API tools. We annotate 314 tool-use dialogues with 753 API calls to assess the existing LLMs’ capabilities in planning, retrieving, and calling APIs. For the second question, we construct a comprehensive training set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000 distinct domains. Using this dataset, we train Lynx, a tool-augmented LLM initialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits improved tool utilization compared to GPT-3, while GPT-4 excels in planning. However, there is still significant potential for further improvement. Moreover, Lynx surpasses Alpaca’s tool utilization performance by more than 26 pts and approaches the effectiveness of GPT-3.5. Through error analysis, we highlight the key challenges for future research in this field to answer the third question.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.187.pdf",
        "keywords": [
            "llms",
            "api bank",
            "augmented llms"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, there is still significant potential for further improvement.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, there is still significant potential for further improvement.\""
    },
    {
        "title": "Lion: Adversarial Distillation of Proprietary Large Language Models",
        "authors": [
            "Yuxin Jiang",
            "Chunkit Chan",
            "Mingyang Chen",
            "Wei Wang"
        ],
        "published": "2023",
        "summary": "The practice of transferring knowledge from a sophisticated, proprietary large language model (LLM) to a compact, open-source LLM has garnered considerable attention. Previous works have focused on a unidirectional knowledge distillation way by aligning the responses of the student model with those of the teacher model to a set of instructions. Nevertheless, they overlooked the possibility of incorporating any “feedback”–identifying challenging instructions where the student model’s performance falls short–to boost the student model’s proficiency iteratively. To this end, we propose a novel adversarial distillation framework for a more efficient knowledge transfer. Leveraging the versatile role adaptability of LLMs, we prompt the teacher model to identify “hard” instructions and generate new “hard” instructions for the student model, creating a three-stage adversarial loop of imitation, discrimination, and generation. By applying this adversarial framework, we successfully transfer knowledge from ChatGPT to a student model (named Lion), using a mere 70k training data. Our results show that Lion-13B not only achieves comparable open-ended generation capabilities to ChatGPT but surpasses conventional state-of-the-art (SOTA) instruction-tuned models like Vicuna-13B by 55.4% in challenging zero-shot reasoning benchmarks such as BIG-Bench Hard (BBH) and 16.7% on AGIEval.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.189.pdf",
        "keywords": [
            "adversarial",
            "adversarial distillation",
            "knowledge transfer",
            "knowledge",
            "proprietary large language models",
            "student model"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Nevertheless, they overlooked the possibility of incorporating any “feedback”–identifying challenging instructions where the student model’s performance falls short–to boost the student model’s proficiency iteratively.\"\n\nThis paper mentions a limitation of existing knowledge distillation methods in passing, but the focus is on the proposed framework rather than the limitation itself.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Nevertheless, they overlooked the possibility of incorporating any “feedback”–identifying challenging instructions where the student model’s performance falls short–to boost the student model’s proficiency iteratively.\"\n\nThis paper mentions a limitation of existing knowledge distillation methods in passing, but the focus is on the proposed framework rather than the limitation itself."
    },
    {
        "title": "Evaluating Large Language Models on Controlled Generation Tasks",
        "authors": [
            "Jiao Sun",
            "Yufei Tian",
            "Wangchunshu Zhou",
            "Nan Xu",
            "Qian Hu",
            "Rahul Gupta",
            "John Wieting",
            "Nanyun Peng",
            "Xuezhe Ma"
        ],
        "published": "2023",
        "summary": "While recent studies have looked into the abilities of large language models in various benchmark tasks, including question generation, reading comprehension, multilingual and etc, there have been few studies looking into the controllability of large language models on generation tasks. We present an extensive analysis of various benchmarks including a sentence planning benchmark with different granularities. After comparing large language models against state-of-the-start finetuned smaller models, we present a spectrum showing large language models falling behind, are comparable, or exceed the ability of smaller models. We conclude that *large language models struggle at meeting fine-grained hard constraints*.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.190.pdf",
        "keywords": [
            "generation",
            "controlled generation tasks",
            "language models",
            "reading comprehension",
            "benchmark",
            "controllability",
            "large language models",
            "evaluating large language models",
            "planning benchmark"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We conclude that *large language models struggle at meeting fine-grained hard constraints*.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We conclude that *large language models struggle at meeting fine-grained hard constraints*.\""
    },
    {
        "title": "DeSIQ: Towards an Unbiased, Challenging Benchmark for Social Intelligence Understanding",
        "authors": [
            "Xiao-Yu Guo",
            "Yuan-Fang Li",
            "Reza Haf"
        ],
        "published": "2023",
        "summary": "Social intelligence is essential for understanding and reasoning about human expressions, intents and interactions. One representative benchmark for its study is Social Intelligence Queries (Social-IQ), a dataset of multiple-choice questions on videos of complex social interactions. We define a comprehensive methodology to study the soundness of Social-IQ, as the soundness of such benchmark datasets is crucial to the investigation of the underlying research problem. We define a comprehensive methodology to study the soundness of Social-IQ, as the soundness of such benchmark datasets is crucial to the investigation of the underlying research problem. Our analysis reveals that Social-IQ contains substantial biases, which can be exploited by a moderately strong language model to learn spurious correlations to achieve perfect performance without being given the context or even the question. We introduce DeSIQ, a new challenging dataset, constructed by applying simple perturbations to Social-IQ. Our empirical analysis shows De-SIQ significantly reduces the biases in the original Social-IQ dataset. Furthermore, we examine and shed light on the effect of model size, model style, learning settings, commonsense knowledge, and multi-modality on the new benchmark performance. Our new dataset, observations and findings open up important research questions for the study of social intelligence.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.191.pdf",
        "keywords": [
            "social intelligence",
            "commonsense knowledge"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our analysis reveals that Social-IQ contains substantial biases, which can be exploited by a moderately strong language model to learn spurious correlations to achieve perfect performance without being given the context or even the question.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (exploiting biases to achieve high performance without understanding the context) but it is not the primary focus of the",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our analysis reveals that Social-IQ contains substantial biases, which can be exploited by a moderately strong language model to learn spurious correlations to achieve perfect performance without being given the context or even the question.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (exploiting biases to achieve high performance without understanding the context) but it is not the primary focus of the"
    },
    {
        "title": "Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional, and Extensional Learning for Faithful Natural Language Generation",
        "authors": [
            "Adam Bouyamourn"
        ],
        "published": "2023",
        "summary": "We show that LLMs hallucinate because their output is not constrained to be synonymous with claims for which they have evidence: a condition that we call evidential closure. Information about the truth or falsity of sentences is not statistically identified in the standard neural language generation setup, and so cannot be conditioned on to generate new strings. We then show how to constrain LLMs to produce output that satisfies evidential closure. A multimodal LLM must learn about the external world (perceptual learning); it must learn a mapping from strings to states of the world (extensional learning); and, to achieve fluency when generalizing beyond a body of evidence, it must learn mappings from strings to their synonyms (intensional learning). The output of a unimodal LLM must be synonymous with strings in a validated evidence set. Finally, we present a heuristic procedure, Learn-Babble-Prune, that yields faithful output from an LLM by rejecting output that is not synonymous with claims for which the LLM has evidence.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.192.pdf",
        "keywords": [
            "language",
            "natural language generation",
            "extensional learning"
        ],
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We show that LLMs hallucinate because their output is not constrained to be synonymous with claims for which they have evidence: a condition that we call evidential closure.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"We show that LLMs hallucinate because their output is not constrained to be synonymous with claims for which they have evidence: a condition that we call evidential closure.\""
    },
    {
        "title": "A Question Answering Framework for Decontextualizing User-facing Snippets from Scientific Documents",
        "authors": [
            "Benjamin Newman",
            "Luca Soldaini",
            "Raymond Fok",
            "Arman Cohan",
            "Kyle Lo"
        ],
        "published": "2023",
        "summary": "Many real-world applications (e.g., note taking, search) require extracting a sentence or paragraph from a document and showing that snippet to a human outside of the source document. Yet, users may find snippets difficult to understand as they lack context from the original document. In this work, we use language models to rewrite snippets from scientific documents to be read on their own. First, we define the requirements and challenges for this user-facing decontextualization task, such as clarifying where edits occur and handling references to other documents. Second, we propose a framework that decomposes the task into three stages: question generation, question answering, and rewriting. Using this framework, we collect gold decontextualizations from experienced scientific article readers. We then conduct a range of experiments across state-of-the-art commercial and open-source language models to identify how to best provide missing-but-relevant information to models for our task. Finally, we develop QaDecontext, a simple prompting strategy inspired by our framework that improves over end-to-end prompting. We conclude with analysis that finds, while rewriting is easy, question generation and answering remain challenging for today’s models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.193.pdf",
        "keywords": [
            "question answering",
            "language",
            "language models",
            "question generation",
            "question answering framework",
            "prompting",
            "qadecontext",
            "snippet",
            "user facing snippets",
            "decontextualization"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we conclude with analysis that finds, while rewriting is easy, question generation and answering remain challenging for today’s models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"we conclude with analysis that finds, while rewriting is easy, question generation and answering remain challenging for today’s models.\""
    },
    {
        "title": "Pushdown Layers: Encoding Recursive Structure in Transformer Language Models",
        "authors": [
            "Shikhar Murty",
            "Pratyusha Sharma",
            "Jacob Andreas",
            "Christopher Manning"
        ],
        "published": "2023",
        "summary": "Recursion is a prominent feature of human language, and fundamentally challenging for self-attention due to the lack of an explicit recursive-state tracking mechanism. Consequently, Transformer language models poorly capture long-tail recursive structure and exhibit sample-inefficient syntactic generalization. This work introduces Pushdown Layers, a new self-attention layer that models recursive state via a stack tape that tracks estimated depths of every token in an incremental parse of the observed prefix. Transformer LMs with Pushdown Layers are syntactic language models that autoregressively and synchronously update this stack tape as they predict new tokens, in turn using the stack tape to softly modulate attention over tokens—for instance, learning to “skip” over closed constituents. When trained on a corpus of strings annotated with silver constituency parses, Transformers equipped with Pushdown Layers achieve dramatically better and 3-5x more sample-efficient syntactic generalization, while maintaining similar perplexities. Pushdown Layers are a drop-in replacement for standard self-attention. We illustrate this by finetuning GPT2-medium with Pushdown Layers on an automatically parsed WikiText-103, leading to improvements on several GLUE text classification tasks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.195.pdf",
        "keywords": [
            "pushdown layers",
            "transformers",
            "transformer language models",
            "encoding recursive structure",
            "tail recursive structure",
            "models recursive state",
            "text classification",
            "syntactic language models",
            "self attention layer",
            "transformer lms",
            "generalization"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Transformer language models poorly capture long-tail recursive structure and exhibit sample-inefficient syntactic generalization.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Transformer language models poorly capture long-tail recursive structure and exhibit sample-inefficient syntactic generalization.\""
    },
    {
        "title": "Can LLMs Facilitate Interpretation of Pre-trained Language Models?",
        "authors": [
            "Basel Mousi",
            "Nadir Durrani",
            "Fahim Dalvi"
        ],
        "published": "2023",
        "summary": "Work done to uncover the knowledge encoded within pre-trained language models rely on annotated corpora or human-in-the-loop methods. However, these approaches are limited in terms of scalability and the scope of interpretation. We propose using a large language model, ChatGPT, as an annotator to enable fine-grained interpretation analysis of pre-trained language models. We discover latent concepts within pre-trained language models by applying agglomerative hierarchical clustering over contextualized representations and then annotate these concepts using ChatGPT. Our findings demonstrate that ChatGPT produces accurate and semantically richer annotations compared to human-annotated concepts. Additionally, we showcase how GPT-based annotations empower interpretation analysis methodologies of which we demonstrate two: probing frameworks and neuron interpretation. To facilitate further exploration and experimentation in the field, we make available a substantial ConceptNet dataset (TCN) comprising 39,000 annotated concepts.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.196.pdf",
        "keywords": [
            "annotated",
            "language models",
            "trained language models",
            "neuron interpretation"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None (The abstract discusses the use of LLMs to facilitate interpretation of pre-trained language models, but does not mention any limitations of LLMs.)",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None (The abstract discusses the use of LLMs to facilitate interpretation of pre-trained language models, but does not mention any limitations of LLMs.)"
    },
    {
        "title": "Oolong: Investigating What Makes Transfer Learning Hard with Controlled Studies",
        "authors": [
            "Zhengxuan Wu",
            "Alex Tamkin",
            "Isabel Papadimitriou"
        ],
        "published": "2023",
        "summary": "When we transfer a pretrained language model to a new language, there are many axes of variation that change at once. To disentangle the impact of different factors like syntactic similarity and vocabulary similarity, we propose a set of controlled transfer studies: we systematically transform the language of the GLUE benchmark, altering one axis of crosslingual variation at a time, and then measure the resulting drops in a pretrained model’s downstream performance. We find that models can largely recover from syntactic-style shifts, but cannot recover from vocabulary misalignment and embedding matrix re-initialization, even with continued pretraining on 15 million tokens. Moreover, good-quality tokenizers in the transfer language do not make vocabulary alignment easier. Our experiments provide insights into the factors of cross-lingual transfer that researchers should most focus on when designing language transfer scenarios.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.198.pdf",
        "keywords": [
            "transfer language",
            "transfer learning",
            "controlled transfer studies",
            "tokenizers",
            "syntactic style shifts"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that models can largely recover from syntactic-style shifts, but cannot recover from vocabulary misalignment and embedding matrix re-initialization, even with continued pretraining on 15 million tokens.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We find that models can largely recover from syntactic-style shifts, but cannot recover from vocabulary misalignment and embedding matrix re-initialization, even with continued pretraining on 15 million tokens.\""
    },
    {
        "title": "TOD-Flow: Modeling the Structure of Task-Oriented Dialogues",
        "authors": [
            "Sungryull Sohn",
            "Yiwei Lyu",
            "Anthony Liu",
            "Lajanugen Logeswaran",
            "Dong-Ki Kim",
            "Dongsub Shim",
            "Honglak Lee"
        ],
        "published": "2023",
        "summary": "Task-Oriented Dialogue (TOD) systems have become crucial components in interactive artificial intelligence applications. While recent advances have capitalized on pre-trained language models (PLMs), they exhibit limitations regarding transparency and controllability. To address these challenges, we propose a novel approach focusing on inferring the TOD-flow graph from dialogue data annotated with dialog acts, uncovering the underlying task structure in the form of a graph. The inferred TOD-flow graph can be easily integrated with any dialogue model to improve its prediction performance, transparency, and controllability. Our TOD-flow graph learns what a model can, should, and should not predict, effectively reducing the search space and providing a rationale for the model’s prediction. We show that the proposed TOD-flow graph better resemble human-annotated graphs compared to prior approaches. Furthermore, when combined with several dialogue policies and end-to-end dialogue models, we demonstrate that our approach significantly improves dialog act classification and end-to-end response generation performance in the MultiWOZ and SGD benchmarks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.204.pdf",
        "keywords": [
            "dialog act",
            "tod flow graph",
            "dialogues",
            "inferred tod flow graph",
            "dialogue model",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"while recent advances have capitalized on pre-trained language models (PLMs), they exhibit limitations regarding transparency and controllability.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"while recent advances have capitalized on pre-trained language models (PLMs), they exhibit limitations regarding transparency and controllability.\""
    },
    {
        "title": "Knowledge Rumination for Pre-trained Language Models",
        "authors": [
            "Yunzhi Yao",
            "Peng Wang",
            "Shengyu Mao",
            "Chuanqi Tan",
            "Fei Huang",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "published": "2023",
        "summary": "Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus, several works have attempted to integrate external knowledge into PLMs. However, despite the promising outcome, we empirically observe that PLMs may have already encoded rich knowledge in their pre-trained parameters but fails to fully utilize them when applying to knowledge-intensive tasks. In this paper, we propose a new paradigm dubbed Knowledge Rumination to help the pre-trained language model utilize that related latent knowledge without retrieving them from the external corpus. By simply adding a prompt like “As far as I know” to the PLMs, we try to review related latent knowledge and inject them back into the model for knowledge consolidation. We apply the proposed knowledge rumination to various language models, including RoBERTa, DeBERTa, and GPT-3. Experimental results on six commonsense reasoning tasks and GLUE benchmarks demonstrate the effectiveness of our proposed approach, which proves that the knowledge stored in PLMs can be better exploited to enhance performance.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.206.pdf",
        "keywords": [
            "knowledge rumination",
            "knowledge consolidation",
            "language models",
            "pre trained language models",
            "latent knowledge",
            "glue benchmarks"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus, several works have attempted to integrate external knowledge into PLMs. However, despite the promising outcome, we empirically observe that PLMs may have already encoded rich knowledge in their pre-trained parameters but fails to fully utilize them when applying to knowledge-intensive",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus, several works have attempted to integrate external knowledge into PLMs. However, despite the promising outcome, we empirically observe that PLMs may have already encoded rich knowledge in their pre-trained parameters but fails to fully utilize them when applying to knowledge-intensive"
    },
    {
        "title": "Struct-XLM: A Structure Discovery Multilingual Language Model for Enhancing Cross-lingual Transfer through Reinforcement Learning",
        "authors": [
            "Linjuan Wu",
            "Weiming Lu"
        ],
        "published": "2023",
        "summary": "Cross-lingual transfer learning heavily relies on well-aligned cross-lingual representations. The syntactic structure is recognized as beneficial for cross-lingual transfer, but limited researches utilize it for aligning representation in multilingual pre-trained language models (PLMs). Additionally, existing methods require syntactic labels that are difficult to obtain and of poor quality for low-resource languages. To address this gap, we propose Struct-XLM, a novel multilingual language model that leverages reinforcement learning (RL) to autonomously discover universal syntactic structures for improving the cross-lingual representation alignment of PLM. Struct-XLM integrates a policy network (PNet) and a translation ranking task. The PNet is designed to discover structural information and integrate it into the last layer of the PLM through the structural multi-head attention module to obtain structural representation. The translation ranking task obtains a delayed reward based on the structural representation to optimize the PNet while improving the alignment of cross-lingual representation. Experiments show the effectiveness of the proposed approach for enhancing cross-lingual transfer of multilingual PLM on the XTREME benchmark.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.207.pdf",
        "keywords": [
            "reinforcement learning",
            "multilingual language model",
            "translation ranking",
            "policy network",
            "pnet",
            "cross lingual transfer learning",
            "struct xlm"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"existing methods require syntactic labels that are difficult to obtain and of poor quality for low-resource languages.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"existing methods require syntactic labels that are difficult to obtain and of poor quality for low-resource languages.\""
    },
    {
        "title": "AdaSent: Efficient Domain-Adapted Sentence Embeddings for Few-Shot Classification",
        "authors": [
            "Yongxin Huang",
            "Kexin Wang",
            "Sourav Dutta",
            "Raj Patel",
            "Goran Glavaš",
            "Iryna Gurevych"
        ],
        "published": "2023",
        "summary": "Recent work has found that few-shot sentence classification based on pre-trained Sentence Encoders (SEs) is efficient, robust, and effective. In this work, we investigate strategies for domain-specialization in the context of few-shot sentence classification with SEs. We first establish that unsupervised Domain-Adaptive Pre-Training (DAPT) of a base Pre-trained Language Model (PLM) (i.e., not an SE) substantially improves the accuracy of few-shot sentence classification by up to 8.4 points. However, applying DAPT on SEs, on the one hand, disrupts the effects of their (general-domain) Sentence Embedding Pre-Training (SEPT). On the other hand, applying general-domain SEPT on top of a domain-adapted base PLM (i.e., after DAPT) is effective but inefficient, since the computationally expensive SEPT needs to be executed on top of a DAPT-ed PLM of each domain. As a solution, we propose AdaSent, which decouples SEPT from DAPT by training a SEPT adapter on the base PLM. The adapter can be inserted into DAPT-ed PLMs from any domain. We demonstrate AdaSent’s effectiveness in extensive experiments on 17 different few-shot sentence classification datasets. AdaSent matches or surpasses the performance of full SEPT on DAPT-ed PLM, while substantially reducing the training costs. The code for AdaSent is available.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.208.pdf",
        "keywords": [
            "sentence classification",
            "shot sentence classification",
            "adapter",
            "domain adapted",
            "pre trained language model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, applying DAPT on SEs, on the one hand, disrupts the effects of their (general-domain) Sentence Embedding Pre-Training (SEPT).\"\n\nThis rating is given because the abstract mentions a limitation of applying DAPT on SEs, but it is not the primary focus of the paper and is used to motivate the proposed solution, AdaSent.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, applying DAPT on SEs, on the one hand, disrupts the effects of their (general-domain) Sentence Embedding Pre-Training (SEPT).\"\n\nThis rating is given because the abstract mentions a limitation of applying DAPT on SEs, but it is not the primary focus of the paper and is used to motivate the proposed solution, AdaSent."
    },
    {
        "title": "Interview Evaluation: A Novel Approach for Automatic Evaluation of Conversational Question Answering Models",
        "authors": [
            "Xibo Li",
            "Bowei Zou",
            "Yifan Fan",
            "Yanling Li",
            "Ai Ti Aw",
            "Yu Hong"
        ],
        "published": "2023",
        "summary": "Conversational Question Answering (CQA) aims to provide natural language answers to users in information-seeking dialogues. Existing CQA benchmarks often evaluate models using pre-collected human-human conversations. However, replacing the model-predicted dialogue history with ground truth compromises the naturalness and sustainability of CQA evaluation. While previous studies proposed using predicted history and rewriting techniques to address unresolved coreferences and incoherencies, this approach renders the question self-contained from the conversation. In this paper, we propose a novel automatic evaluation approach, interview evaluation. Specifically, ChatGPT acts as the interviewer (Q agent) with a set of carefully designed prompts, and the CQA model under test serves as the interviewee (A agent). During the interview evaluation, questions are dynamically generated by the Q agent to guide the A agent in predicting the correct answer through an interactive process. We evaluated four different models on QuAC and two models on CoQA in our experiments. The experiment results demonstrate that our interview evaluation has advantages over previous CQA evaluation approaches, particularly in terms of naturalness and coherence. The source code is made publicly available.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.209.pdf",
        "keywords": [
            "conversational question answering",
            "interview evaluation",
            "automatic evaluation",
            "coqa"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, replacing the model-predicted dialogue history with ground truth compromises the naturalness and sustainability of CQA evaluation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, replacing the model-predicted dialogue history with ground truth compromises the naturalness and sustainability of CQA evaluation.\""
    },
    {
        "title": "Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA",
        "authors": [
            "David Heineman",
            "Yao Dou",
            "Mounica Maddela",
            "Wei Xu"
        ],
        "published": "2023",
        "summary": "Large language models (e.g., GPT-4) are uniquely capable of producing highly rated text simplification, yet current human evaluation methods fail to provide a clear understanding of systems’ specific strengths and weaknesses. To address this limitation, we introduce SALSA, an edit-based human annotation framework that enables holistic and fine-grained text simplification evaluation. We develop twenty one linguistically grounded edit types, covering the full spectrum of success and failure across dimensions of conceptual, syntactic and lexical simplicity. Using SALSA, we collect 19K edit annotations on 840 simplifications, revealing discrepancies in the distribution of simplification strategies performed by fine-tuned models, prompted LLMs and humans, and find GPT-3.5 performs more quality edits than humans, but still exhibits frequent errors. Using our fine-grained annotations, we develop LENS-SALSA, a reference-free automatic simplification metric, trained to predict sentence- and word-level quality simultaneously. Additionally, we introduce word-level quality estimation for simplification and report promising baseline results. Our data, new metric, and annotation toolkit are available at https://salsa-eval.com.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.211.pdf",
        "keywords": [
            "edit",
            "salsa",
            "failure",
            "edit level simplification",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"yet current human evaluation methods fail to provide a clear understanding of systems’ specific strengths and weaknesses\"; \"Using SALSA, we collect 19K edit annotations on 840 simplifications, revealing discrepancies in the distribution of simplification strategies performed by fine-tuned models, prompted LLMs and humans, and find GPT-3.5 performs more quality edits than humans, but",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"yet current human evaluation methods fail to provide a clear understanding of systems’ specific strengths and weaknesses\"; \"Using SALSA, we collect 19K edit annotations on 840 simplifications, revealing discrepancies in the distribution of simplification strategies performed by fine-tuned models, prompted LLMs and humans, and find GPT-3.5 performs more quality edits than humans, but"
    },
    {
        "title": "ToViLaG: Your Visual-Language Generative Model is Also An Evildoer",
        "authors": [
            "Xinpeng Wang",
            "Xiaoyuan Yi",
            "Han Jiang",
            "Shanlin Zhou",
            "Zhihua Wei",
            "Xing Xie"
        ],
        "published": "2023",
        "summary": "Recent large-scale Visual-Language Generative Models (VLGMs) have achieved unprecedented improvement in multimodal image/text generation. However, these models might also generate toxic content, e.g., offensive text and pornography images, raising significant ethical risks. Despite exhaustive studies on toxic degeneration of language models, this problem remains largely unexplored within the context of visual-language generation. This work delves into the propensity for toxicity generation and susceptibility to toxic data across various VLGMs. For this purpose, we built ToViLaG, a dataset comprising 32K co-toxic/mono-toxic text-image pairs and 1K innocuous but evocative text that tends to stimulate toxicity. Furthermore, we propose WInToRe, a novel toxicity metric tailored to visual-language generation, which theoretically reflects different aspects of toxicity considering both input and output. On such a basis, we benchmarked the toxicity of a diverse spectrum of VLGMs and discovered that some models do more evil than expected while some are more vulnerable to infection, underscoring the necessity of VLGMs detoxification. Therefore, we develop an innovative bottleneck-based detoxification method. Our method could reduce toxicity while maintaining comparable generation quality, providing a promising initial solution to this line of research.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.213.pdf",
        "keywords": [
            "detoxification",
            "visual language generative models",
            "language"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these models might also generate toxic content, e.g., offensive text and pornography images, raising significant ethical risks.\"; \"we benchmarked the toxicity of a diverse spectrum of VLGMs and discovered that some models do more evil than expected while some are more vulnerable to infection, underscoring the necessity of VLGMs detoxification.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, these models might also generate toxic content, e.g., offensive text and pornography images, raising significant ethical risks.\"; \"we benchmarked the toxicity of a diverse spectrum of VLGMs and discovered that some models do more evil than expected while some are more vulnerable to infection, underscoring the necessity of VLGMs detoxification.\""
    },
    {
        "title": "GPT-RE: In-context Learning for Relation Extraction using Large Language Models",
        "authors": [
            "Zhen Wan",
            "Fei Cheng",
            "Zhuoyuan Mao",
            "Qianying Liu",
            "Haiyue Song",
            "Jiwei Li",
            "Sadao Kurohashi"
        ],
        "published": "2023",
        "summary": "In spite of the potential for ground-breaking achievements offered by large language models (LLMs) (e.g., GPT-3) via in-context learning (ICL), they still lag significantly behind fully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE). This is due to the two major shortcomings of ICL for RE: (1) low relevance regarding entity and relation in existing sentence-level demonstration retrieval approaches for ICL; and (2) the lack of explaining input-label mappings of demonstrations leading to poor ICL effectiveness. In this paper, we propose GPT-RE to successfully address the aforementioned issues by (1) incorporating task-aware representations in demonstration retrieval; and (2) enriching the demonstrations with gold label-induced reasoning logic. We evaluate GPT-RE on four widely-used RE datasets, and observe that GPT-RE achieves improvements over not only existing GPT-3 baselines, but also fully-supervised baselines as in Figure 1. Specifically, GPT-RE achieves SOTA performances on the Semeval and SciERC datasets, and competitive performances on the TACRED and ACE05 datasets. Additionally, a critical issue of LLMs revealed by previous work, the strong inclination to wrongly classify NULL examples into other pre-defined labels, is substantially alleviated by our method. We show an empirical analysis.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.214.pdf",
        "keywords": [
            "relation extraction",
            "context learning",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"In spite of the potential for ground-breaking achievements offered by large language models (LLMs) (e.g., GPT-3) via in-context learning (ICL), they still lag significantly behind fully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE).\"; \"a critical issue of LLMs revealed by previous work, the",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"In spite of the potential for ground-breaking achievements offered by large language models (LLMs) (e.g., GPT-3) via in-context learning (ICL), they still lag significantly behind fully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE).\"; \"a critical issue of LLMs revealed by previous work, the"
    },
    {
        "title": "Sociocultural Norm Similarities and Differences via Situational Alignment and Explainable Textual Entailment",
        "authors": [
            "Sky CH-Wang",
            "Arkadiy Saakyan",
            "Oliver Li",
            "Zhou Yu",
            "Smaranda Muresan"
        ],
        "published": "2023",
        "summary": "Designing systems that can reason across cultures requires that they are grounded in the norms of the contexts in which they operate. However, current research on developing computational models of social norms has primarily focused on American society. Here, we propose a novel approach to discover and compare descriptive social norms across Chinese and American cultures. We demonstrate our approach by leveraging discussions on a Chinese Q&A platform—Zhihu—and the existing SocialChemistry dataset as proxies for contrasting cultural axes, align social situations cross-culturally, and extract social norms from texts using in-context learning. Embedding Chain-of-Thought prompting in a human-AI collaborative framework, we build a high-quality dataset of 3,069 social norms aligned with social situations across Chinese and American cultures alongside corresponding free-text explanations. To test the ability of models to reason about social norms across cultures, we introduce the task of explainable social norm entailment, showing that existing models under 3B parameters have significant room for improvement in both automatic and human evaluation. Further analysis of cross-cultural norm differences based on our dataset shows empirical alignment with the social orientations framework, revealing several situational and descriptive nuances in norms across these cultures.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.215.pdf",
        "keywords": [
            "norms",
            "social norms",
            "textual entailment",
            "sociocultural norm similarities",
            "social orientations",
            "cultural norm",
            "socialchemistry",
            "cultural",
            "alignment",
            "situational alignment"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"To test the ability of models to reason about social norms across cultures, we introduce the task of explainable social norm entailment, showing that existing models under 3B parameters have significant room for improvement in both automatic and human evaluation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"To test the ability of models to reason about social norms across cultures, we introduce the task of explainable social norm entailment, showing that existing models under 3B parameters have significant room for improvement in both automatic and human evaluation.\""
    },
    {
        "title": "INFORM : Information eNtropy based multi-step reasoning FOR large language Models",
        "authors": [
            "Chuyue Zhou",
            "Wangjie You",
            "Juntao Li",
            "Jing Ye",
            "Kehai Chen",
            "Min Zhang"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have demonstrated exceptional performance in reasoning tasks with dedicated Chain-of-Thought (CoT) prompts. Further enhancing CoT prompts with exquisite exemplars can significantly improve reasoning performance.However, the effectiveness of CoT prompts may fluctuate dramatically with different choices of in-context examples. Additionally, manual construction of rationale steps can be time-consuming, presenting challenges for the widespread adoption of CoT prompting. In this work, we propose a novel approach by introducing information entropy (IE) as a criteria on for CoT prompt selection. We extend this criterion to the CoT generation and inference stages, automatically generating CoT prompts with higher information entropy scores and adaptively determining the number of samples. These three stages together form our proposed information- entropy-based multi-step reasoning for large language models, named INFORM. Our experiments across seven reasoning benchmarks utilizing two language models(GPT-3.5-Turbo and text-davinci-003) demonstrate the superiority of INFORM both in performance and efficiency.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.216.pdf",
        "keywords": [
            "information entropy",
            "multi step reasoning",
            "inform",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the effectiveness of CoT prompts may fluctuate dramatically with different choices of in-context examples. Additionally, manual construction of rationale steps can be time-consuming, presenting challenges for the widespread adoption of CoT prompting.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the effectiveness of CoT prompts may fluctuate dramatically with different choices of in-context examples. Additionally, manual construction of rationale steps can be time-consuming, presenting challenges for the widespread adoption of CoT prompting.\""
    },
    {
        "title": "Adaptive Gating in Mixture-of-Experts based Language Models",
        "authors": [
            "Jiamin Li",
            "Qiang Su",
            "Yitao Yang",
            "Yimin Jiang",
            "Cong Wang",
            "Hong Xu"
        ],
        "published": "2023",
        "summary": "Large language models have demonstrated exceptional language understanding capabilities in many NLP tasks. Sparsely activated mixture-of-experts (MoE) has emerged as a promising solution for scaling models while maintaining a constant number of computational operations. Existing MoE models adopt a fixed gating network where each token is computed by the same number of experts. This contradicts our intuition that the tokens in each sequence vary in terms of their linguistic complexity and, consequently, require different computational costs. Little is discussed in prior research on the trade-off between computation per token and model performance. This paper introduces adaptive gating in MoE, a flexible training strategy that allows tokens to be processed by a variable number of experts based on expert probability distribution. Adaptive gating preserves sparsity while improving training efficiency. We further draw upon curriculum learning to better align the order of training samples and maximize the training time savings. Extensive experiments on diverse NLP tasks show that adaptive gating reduces at most 22.5% training time while maintaining inference quality. Moreover, we conduct a comprehensive analysis of the gating decisions and present our insights on which tokens are inherently difficult to process, depending on the specific language task.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.217.pdf",
        "keywords": [
            "gating",
            "adaptive gating",
            "mixture of experts",
            "language models",
            "probability",
            "curriculum"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This contradicts our intuition that the tokens in each sequence vary in terms of their linguistic complexity and, consequently, require different computational costs. Little is discussed in prior research on the trade-off between computation per token and model performance.\"\n\nNote: The limitation is briefly mentioned as a motivation for the proposed solution, but not explored in detail.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"This contradicts our intuition that the tokens in each sequence vary in terms of their linguistic complexity and, consequently, require different computational costs. Little is discussed in prior research on the trade-off between computation per token and model performance.\"\n\nNote: The limitation is briefly mentioned as a motivation for the proposed solution, but not explored in detail."
    },
    {
        "title": "On the Automatic Generation and Simplification of Children’s Stories",
        "authors": [
            "Maria Valentini",
            "Jennifer Weber",
            "Jesus Salcido",
            "Téa Wright",
            "Eliana Colunga",
            "Katharina von der Wense"
        ],
        "published": "2023",
        "summary": "With recent advances in large language models (LLMs), the concept of automatically generating children’s educational materials has become increasingly realistic. Working toward the goal of age-appropriate simplicity in generated educational texts, we first examine the ability of several popular LLMs to generate stories with properly adjusted lexical and readability levels. We find that, in spite of the growing capabilities of LLMs, they do not yet possess the ability to limit their vocabulary to levels appropriate for younger age groups. As a second experiment, we explore the ability of state-of-the-art lexical simplification models to generalize to the domain of children’s stories and, thus, create an efficient pipeline for their automatic generation. In order to test these models, we develop a dataset of child-directed lexical simplification instances, with examples taken from the LLM-generated stories in our first experiment. We find that, while the strongest-performing current lexical simplification models do not perform as well on material designed for children due to their reliance on large language models behind the scenes, some models that still achieve fairly strong results on general data can mimic or even improve their performance on children-directed data with proper fine-tuning, which we conduct using our newly created child-directed simplification dataset.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.218.pdf",
        "keywords": [
            "children",
            "child directed simplification",
            "language models",
            "automatic generation and simplification",
            "lexical simplification"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that, in spite of the growing capabilities of LLMs, they do not yet possess the ability to limit their vocabulary to levels appropriate for younger age groups.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We find that, in spite of the growing capabilities of LLMs, they do not yet possess the ability to limit their vocabulary to levels appropriate for younger age groups.\""
    },
    {
        "title": "The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models",
        "authors": [
            "Aviv Slobodkin",
            "Omer Goldman",
            "Avi Caciularu",
            "Ido Dagan",
            "Shauli Ravfogel"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have been shown to possess impressive capabilities, while also raising crucial concerns about the faithfulness of their responses. A primary issue arising in this context is the management of (un)answerable queries by LLMs, which often results in hallucinatory behavior due to overconfidence. In this paper, we explore the behavior of LLMs when presented with (un)answerable queries. We ask: do models represent the fact that the question is (un)answerable when generating a hallucinatory answer? Our results show strong indications that such models encode the answerability of an input query, with the representation of the first decoded token often being a strong indicator. These findings shed new light on the spatial organization within the latent representations of LLMs, unveiling previously unexplored facets of these models. Moreover, they pave the way for the development of improved decoding techniques with better adherence to factual generation, particularly in scenarios where query (un)answerability is a concern.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.220.pdf",
        "keywords": [
            "latent representations",
            "language models",
            "hallucinatory"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A primary issue arising in this context is the management of (un)answerable queries by LLMs, which often results in hallucinatory behavior due to overconfidence.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"A primary issue arising in this context is the management of (un)answerable queries by LLMs, which often results in hallucinatory behavior due to overconfidence.\""
    },
    {
        "title": "Retrofitting Light-weight Language Models for Emotions using Supervised Contrastive Learning",
        "authors": [
            "Sapan Shah",
            "Sreedhar Reddy",
            "Pushpak Bhattacharyya"
        ],
        "published": "2023",
        "summary": "We present a novel retrofitting method to induce emotion aspects into pre-trained language models (PLMs) such as BERT and RoBERTa. Our method updates pre-trained network weights using contrastive learning so that the text fragments exhibiting similar emotions are encoded nearby in the representation space, and the fragments with different emotion content are pushed apart. While doing so, it also ensures that the linguistic knowledge already present in PLMs is not inadvertently perturbed. The language models retrofitted by our method, i.e., BERTEmo and RoBERTaEmo, produce emotion-aware text representations, as evaluated through different clustering and retrieval metrics. For the downstream tasks on sentiment analysis and sarcasm detection, they perform better than their pre-trained counterparts (about 1% improvement in F1-score) and other existing approaches. Additionally, a more significant boost in performance is observed for the retrofitted models over pre-trained ones in few-shot learning setting.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.222.pdf",
        "keywords": [
            "emotions",
            "sentiment analysis",
            "light weight language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but the fact that the paper aims to \"induce emotion aspects into pre-trained language models\" implies that pre-trained models lack this capability, which is a limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None explicitly mentioned, but the fact that the paper aims to \"induce emotion aspects into pre-trained language models\" implies that pre-trained models lack this capability, which is a limitation."
    },
    {
        "title": "Longtriever: a Pre-trained Long Text Encoder for Dense Document Retrieval",
        "authors": [
            "Junhan Yang",
            "Zheng Liu",
            "Chaozhuo Li",
            "Guangzhong Sun",
            "Xing Xie"
        ],
        "published": "2023",
        "summary": "Pre-trained language models (PLMs) have achieved the preeminent position in dense retrieval due to their powerful capacity in modeling intrinsic semantics. However, most existing PLM-based retrieval models encounter substantial computational costs and are infeasible for processing long documents. In this paper, a novel retrieval model Longtriever is proposed to embrace three core challenges of long document retrieval: substantial computational cost, incomprehensive document understanding, and scarce annotations. Longtriever splits long documents into short blocks and then efficiently models the local semantics within a block and the global context semantics across blocks in a tightly-coupled manner. A pre-training phase is further proposed to empower Longtriever to achieve a better understanding of underlying semantic correlations. Experimental results on two popular benchmark datasets demonstrate the superiority of our proposal.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.223.pdf",
        "keywords": [
            "core challenges",
            "local semantics",
            "language models",
            "global context semantics",
            "encoder"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, most existing PLM-based retrieval models encounter substantial computational costs and are infeasible for processing long documents.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, most existing PLM-based retrieval models encounter substantial computational costs and are infeasible for processing long documents.\""
    },
    {
        "title": "Revisiting De-Identification of Electronic Medical Records: Evaluation of Within- and Cross-Hospital Generalization",
        "authors": [
            "Yiyang Liu",
            "Jinpeng Li",
            "Enwei Zhu"
        ],
        "published": "2023",
        "summary": "The de-identification task aims to detect and remove the protected health information from electronic medical records (EMRs). Previous studies generally focus on the within-hospital setting and achieve great successes, while the cross-hospital setting has been overlooked. This study introduces a new de-identification dataset comprising EMRs from three hospitals in China, creating a benchmark for evaluating both within- and cross-hospital generalization. We find significant domain discrepancy between hospitals. A model with almost perfect within-hospital performance struggles when transferred across hospitals. Further experiments show that pretrained language models and some domain generalization methods can alleviate this problem. We believe that our data and findings will encourage investigations on the generalization of medical NLP models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.224.pdf",
        "keywords": [
            "electronic medical records",
            "de identification",
            "hospital generalization"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A model with almost perfect within-hospital performance struggles when transferred across hospitals.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"A model with almost perfect within-hospital performance struggles when transferred across hospitals.\""
    },
    {
        "title": "Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning",
        "authors": [
            "Gurusha Juneja",
            "Subhabrata Dutta",
            "Soumen Chakrabarti",
            "Sunny Manchanda",
            "Tanmoy Chakraborty"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) prompted to generate chain-of-thought (CoT) exhibit impressive reasoning capabilities. Recent attempts at prompt decomposition toward solving complex, multi-step reasoning problems depend on the ability of the LLM to simultaneously decompose and solve the problem. A significant disadvantage is that foundational LLMs are typically not available for fine-tuning, making adaptation computationally prohibitive. We believe (and demonstrate) that problem decomposition and solution generation are distinct capabilites, better addressed in separate modules, than by one monolithic LLM. We introduce DaSLaM, which uses a decomposition generator to decompose complex problems into subproblems that require fewer reasoning steps. These subproblems are answered by a solver. We use a relatively small (13B parameters) LM as the decomposition generator, which we train using policy gradient optimization to interact with a solver LM (regarded as black-box) and guide it through subproblems, thereby rendering our method solver-agnostic. Evaluation on multiple different reasoning datasets reveal that with our method, a 175 billion parameter LM (text-davinci-003) can produce competitive or even better performance, compared to its orders-of-magnitude larger successor, GPT-4. Additionally, we show that DaSLaM is not limited by the solver’s capabilities as a function of scale; e.g., solver LMs with diverse sizes give significant performance improvement with our solver-agnostic decomposition technique. Exhaustive ablation studies evince the superiority of our modular finetuning technique over exorbitantly large decomposer LLMs, based on prompting alone.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.225.pdf",
        "keywords": [
            "finetuning",
            "fine tuned",
            "complex reasoning",
            "language models",
            "large language models",
            "decomposer",
            "small"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A significant disadvantage is that foundational LLMs are typically not available for fine-tuning, making adaptation computationally prohibitive.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"A significant disadvantage is that foundational LLMs are typically not available for fine-tuning, making adaptation computationally prohibitive.\""
    },
    {
        "title": "Language Representation Projection: Can We Transfer Factual Knowledge across Languages in Multilingual Language Models?",
        "authors": [
            "Shaoyang Xu",
            "Junzhuo Li",
            "Deyi Xiong"
        ],
        "published": "2023",
        "summary": "Multilingual pretrained language models serve as repositories of multilingual factual knowledge. Nevertheless, a substantial performance gap of factual knowledge probing exists between high-resource languages and low-resource languages, suggesting limited implicit factual knowledge transfer across languages in multilingual pretrained language models. This paper investigates the feasibility of explicitly transferring relatively rich factual knowledge from English to non-English languages. To accomplish this, we propose two parameter-free Language Representation Projection modules (LRP2). The first module converts non-English representations into English-like equivalents, while the second module reverts English-like representations back into representations of the corresponding non-English language. Experimental results on the mLAMA dataset demonstrate that LRP2 significantly improves factual knowledge retrieval accuracy and facilitates knowledge transferability across diverse non-English languages. We further investigate the working mechanism of LRP2 from the perspectives of representation space and cross-lingual knowledge neuron.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.226.pdf",
        "keywords": [
            "language models",
            "multilingual language models",
            "language representation projection",
            "projection",
            "knowledge",
            "knowledge retrieval"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"a substantial performance gap of factual knowledge probing exists between high-resource languages and low-resource languages, suggesting limited implicit factual knowledge transfer across languages in multilingual pretrained language models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"a substantial performance gap of factual knowledge probing exists between high-resource languages and low-resource languages, suggesting limited implicit factual knowledge transfer across languages in multilingual pretrained language models.\""
    },
    {
        "title": "Structural Priming Demonstrates Abstract Grammatical Representations in Multilingual Language Models",
        "authors": [
            "James Michaelov",
            "Catherine Arnett",
            "Tyler Chang",
            "Ben Bergen"
        ],
        "published": "2023",
        "summary": "Abstract grammatical knowledge—of parts of speech and grammatical patterns—is key to the capacity for linguistic generalization in humans. But how abstract is grammatical knowledge in large language models? In the human literature, compelling evidence for grammatical abstraction comes from structural priming. A sentence that shares the same grammatical structure as a preceding sentence is processed and produced more readily. Because confounds exist when using stimuli in a single language, evidence of abstraction is even more compelling from crosslingual structural priming, where use of a syntactic structure in one language primes an analogous structure in another language. We measure crosslingual structural priming in large language models, comparing model behavior to human experimental results from eight crosslingual experiments covering six languages, and four monolingual structural priming experiments in three non-English languages. We find evidence for abstract monolingual and crosslingual grammatical representations in the models that function similarly to those found in humans. These results demonstrate that grammatical representations in multilingual language models are not only similar across languages, but they can causally influence text produced in different languages.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.227.pdf",
        "keywords": [
            "languages",
            "grammatical",
            "language models",
            "multilingual language models",
            "structural priming",
            "grammatical representations",
            "linguistic generalization"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper discusses the capacity of LLMs but does not mention any limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper discusses the capacity of LLMs but does not mention any limitations."
    },
    {
        "title": "ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained Language Models for Question Answering over Knowledge Graph",
        "authors": [
            "Jinhao Jiang",
            "Kun Zhou",
            "Xin Zhao",
            "Yaliang Li",
            "Ji-Rong Wen"
        ],
        "published": "2023",
        "summary": "Question Answering over Knowledge Graph (KGQA) aims to seek answer entities for the natural language question from a large-scale Knowledge Graph (KG). To better perform reasoning on KG, recent work typically adopts a pre-trained language model (PLM) to model the question, and a graph neural network (GNN) based module to perform multi-hop reasoning on the KG. Despite the effectiveness, due to the divergence in model architecture, the PLM and GNN are not closely integrated, limiting the knowledge sharing and fine-grained feature interactions. To solve it, we aim to simplify the above two-module approach, and develop a more capable PLM that can directly support subgraph reasoning for KGQA, namely ReasoningLM. In our approach, we propose a subgraph-aware self-attention mechanism to imitate the GNN for performing structured reasoning, and also adopt an adaptation tuning strategy to adapt the model parameters with 20,000 subgraphs with synthesized questions. After adaptation, the PLM can be parameter-efficient fine-tuned on downstream tasks. Experiments show that ReasoningLM surpasses state-of-the-art models by a large margin, even with fewer updated parameters and less training data. Our codes and data are publicly available at https://github.com/RUCAIBox/ReasoningLM.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.228.pdf",
        "keywords": [
            "knowledge graph",
            "question answering",
            "pre trained language models",
            "graph neural network",
            "structured",
            "structural subgraph reasoning",
            "subgraph reasoning",
            "subgraph aware"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the effectiveness, due to the divergence in model architecture, the PLM and GNN are not closely integrated, limiting the knowledge sharing and fine-grained feature interactions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite the effectiveness, due to the divergence in model architecture, the PLM and GNN are not closely integrated, limiting the knowledge sharing and fine-grained feature interactions.\""
    },
    {
        "title": "Deep Natural Language Feature Learning for Interpretable Prediction",
        "authors": [
            "Felipe Urrutia",
            "Cristian Calderon",
            "Valentin Barriere"
        ],
        "published": "2023",
        "summary": "We propose a general method to break down a main complex task into a set of intermediary easier sub-tasks, which are formulated in natural language as binary questions related to the final target task. Our method allows for representing each example by a vector consisting of the answers to these questions. We call this representation Natural Language Learned Features (NLLF). NLLF is generated by a small transformer language model (e.g., BERT) that has been trained in a Natural Language Inference (NLI) fashion, using weak labels automatically obtained from a Large Language Model (LLM). We show that the LLM normally struggles for the main task using in-context learning, but can handle these easiest subtasks and produce useful weak labels to train a BERT. The NLI-like training of the BERT allows for tackling zero-shot inference with any binary question, and not necessarily the ones seen during the training. We show that this NLLF vector not only helps to reach better performances by enhancing any classifier, but that it can be used as input of an easy-to-interpret machine learning model like a decision tree. This decision tree is interpretable but also reaches high performances, surpassing those of a pre-trained transformer in some cases. We have successfully applied this method to two completely different tasks: detecting incoherence in students’ answers to open-ended mathematics exam questions, and screening abstracts for a systematic literature review of scientific papers on climate change and agroecology.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.229.pdf",
        "keywords": [
            "decision tree",
            "natural language",
            "interpretable prediction",
            "representation natural language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We show that the LLM normally struggles for the main task using in-context learning\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We show that the LLM normally struggles for the main task using in-context learning\""
    },
    {
        "title": "ROBBIE: Robust Bias Evaluation of Large Generative Language Models",
        "authors": [
            "David Esiobu",
            "Xiaoqing Tan",
            "Saghar Hosseini",
            "Megan Ung",
            "Yuchen Zhang",
            "Jude Fernandes",
            "Jane Dwivedi-Yu",
            "Eleonora Presani",
            "Adina Williams",
            "Eric Smith"
        ],
        "published": "2023",
        "summary": "As generative large language models (LLMs) grow more performant and prevalent, we must develop comprehensive enough tools to measure and improve their fairness. Different prompt-based datasets can be used to measure social bias across multiple text domains and demographic axes, meaning that testing LLMs on more datasets can potentially help us characterize their biases more fully, and better ensure equal and equitable treatment of marginalized demographic groups. In this work, our focus is two-fold: (1) Benchmarking: a comparison of 6 different prompt-based bias and toxicity metrics across 12 demographic axes and 5 families of generative LLMs. Out of those 6 metrics, AdvPromptSet and HolisticBiasR are novel datasets proposed in the paper. The comparison of those benchmarks gives us insights about the bias and toxicity of the compared models. Therefore, we explore the frequency of demographic terms in common LLM pre-training corpora and how this may relate to model biases. (2) Mitigation: we conduct a comprehensive study of how well 3 bias/toxicity mitigation techniques perform across our suite of measurements. ROBBIE aims to provide insights for practitioners while deploying a model, emphasizing the need to not only measure potential harms, but also understand how they arise by characterizing the data, mitigate harms once found, and balance any trade-offs. We open-source our analysis code in hopes of encouraging broader measurements of bias in future LLMs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.230.pdf",
        "keywords": [
            "bias evaluation",
            "language models",
            "large language models",
            "social bias"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As generative large language models (LLMs) grow more performant and prevalent, we must develop comprehensive enough tools to measure and improve their fairness... testing LLMs on more datasets can potentially help us characterize their biases more fully, and better ensure equal and equitable treatment of marginalized demographic groups.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"As generative large language models (LLMs) grow more performant and prevalent, we must develop comprehensive enough tools to measure and improve their fairness... testing LLMs on more datasets can potentially help us characterize their biases more fully, and better ensure equal and equitable treatment of marginalized demographic groups.\""
    },
    {
        "title": "Adapting Language Models to Compress Contexts",
        "authors": [
            "Alexis Chevalier",
            "Alexander Wettig",
            "Anirudh Ajith",
            "Danqi Chen"
        ],
        "published": "2023",
        "summary": "Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These language models are capable of compressing long contexts into summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments, and summary vectors from all previous segments are used in language modeling. We fine-tune OPT and Llama-2 models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations and find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference costs. Finally, we explore the benefits of pre-computing summary vectors for large corpora by applying summary vectors to retrieval-augmented language modeling and a passage re-ranking task. Overall, AutoCompressors emerge as a simple and inexpensive solution to extend the context window of LMs while speeding up inference over long contexts.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.232.pdf",
        "keywords": [
            "language models",
            "adapting language models",
            "contexts"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents.\""
    },
    {
        "title": "Lost in Translation, Found in Spans: Identifying Claims in Multilingual Social Media",
        "authors": [
            "Shubham Mittal",
            "Megha Sundriyal",
            "Preslav Nakov"
        ],
        "published": "2023",
        "summary": "Claim span identification (CSI) is an important step in fact-checking pipelines, aiming to identify text segments that contain a check-worthy claim or assertion in a social media post. Despite its importance to journalists and human fact-checkers, it remains a severely understudied problem, and the scarce research on this topic so far has only focused on English. Here we aim to bridge this gap by creating a novel dataset, X-CLAIM, consisting of 7K real-world claims collected from numerous social media platforms in five Indian languages and English. We report strong baselines with state-of-the-art encoder-only language models (e.g., XLM-R) and we demonstrate the benefits of training on multiple languages over alternative cross-lingual transfer methods such as zero-shot transfer, or training on translated data, from a high-resource language such as English. We evaluate generative large language models from the GPT series using prompting methods on the X-CLAIM dataset and we find that they underperform the smaller encoder-only language models for low-resource languages.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.236.pdf",
        "keywords": [
            "translation",
            "spans",
            "social media",
            "claim span identification",
            "multilingual social media"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We evaluate generative large language models from the GPT series using prompting methods on the X-CLAIM dataset and we find that they underperform the smaller encoder-only language models for low-resource languages.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We evaluate generative large language models from the GPT series using prompting methods on the X-CLAIM dataset and we find that they underperform the smaller encoder-only language models for low-resource languages.\""
    },
    {
        "title": "COVID-19 Vaccine Misinformation in Middle Income Countries",
        "authors": [
            "Jongin Kim",
            "Byeo Rhee Bak",
            "Aditya Agrawal",
            "Jiaxi Wu",
            "Veronika Wirtz",
            "Traci Hong",
            "Derry Wijaya"
        ],
        "published": "2023",
        "summary": "This paper introduces a multilingual dataset of COVID-19 vaccine misinformation, consisting of annotated tweets from three middle-income countries: Brazil, Indonesia, and Nigeria. The expertly curated dataset includes annotations for 5,952 tweets, assessing their relevance to COVID-19 vaccines, presence of misinformation, and the themes of the misinformation. To address challenges posed by domain specificity, the low-resource setting, and data imbalance, we adopt two approaches for developing COVID-19 vaccine misinformation detection models: domain-specific pre-training and text augmentation using a large language model. Our best misinformation detection models demonstrate improvements ranging from 2.7 to 15.9 percentage points in macro F1-score compared to the baseline models. Additionally, we apply our misinformation detection models in a large-scale study of 19 million unlabeled tweets from the three countries between 2020 and 2022, showcasing the practical application of our dataset and models for detecting and analyzing vaccine misinformation in multiple countries and languages. Our analysis indicates that percentage changes in the number of new COVID-19 cases are positively associated with COVID-19 vaccine misinformation rates in a staggered manner for Brazil and Indonesia, and there are significant positive associations between the misinformation rates across the three countries.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.237.pdf",
        "keywords": [
            "misinformation",
            "misinformation rates",
            "misinformation detection",
            "vaccine misinformation",
            "vaccine misinformation detection",
            "vaccines",
            "middle income",
            "indonesia"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"text augmentation using a large language model.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"text augmentation using a large language model.\""
    },
    {
        "title": "Contrastive Learning of Sentence Embeddings from Scratch",
        "authors": [
            "Junlei Zhang",
            "Zhenzhong Lan",
            "Junxian He"
        ],
        "published": "2023",
        "summary": "Contrastive learning has been the dominant approach to train state-of-the-art sentence embeddings. Previous studies have typically learned sentence embeddings either through the use of human-annotated natural language inference (NLI) data or via large-scale unlabeled sentences in an unsupervised manner. However, even in the case of unlabeled data, their acquisition presents challenges in certain domains due to various reasons. due to copyright restrictions, data distribution issues, and messy formats, among other factors. To address these issues, we present SynCSE, a contrastive learning framework that trains sentence embeddings with synthetic data. Specifically, we explore utilizing large language models to synthesize the required data samples for contrastive learning, including (1) producing positive and negative annotations given unlabeled sentences SynCSE-partial, and (2) generating sentences along with their corresponding annotations from scratch SynCSE-scratch. Notably, SynCSE-scratch constitutes the first contrastive learning method to learn sentence embeddings from scratch without manually collecting any data sample. Experimental results on sentence similarity and reranking tasks indicate that both SynCSE-partial and SynCSE-scratch greatly outperform unsupervised baselines, and SynCSE-partial even achieves comparable performance to the supervised models in most settings.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.238.pdf",
        "keywords": [
            "sentence embeddings",
            "sentence similarity",
            "learn sentence embeddings",
            "annotations"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"even in the case of unlabeled data, their acquisition presents challenges in certain domains due to various reasons. due to copyright restrictions, data distribution issues, and messy formats, among other factors.\"\n\nThis rating is chosen because the abstract mentions challenges related to data acquisition for training sentence embeddings, but this is not a direct limitation of LLMs. However, it does indicate a limitation in",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"even in the case of unlabeled data, their acquisition presents challenges in certain domains due to various reasons. due to copyright restrictions, data distribution issues, and messy formats, among other factors.\"\n\nThis rating is chosen because the abstract mentions challenges related to data acquisition for training sentence embeddings, but this is not a direct limitation of LLMs. However, it does indicate a limitation in"
    },
    {
        "title": "A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation",
        "authors": [
            "Giuseppe Attanasio",
            "Flor Miriam Plaza del Arco",
            "Debora Nozza",
            "Anne Lauscher"
        ],
        "published": "2023",
        "summary": "Recent instruction fine-tuned models can solve multiple NLP tasks when prompted to do so, with machine translation (MT) being a prominent use case. However, current research often focuses on standard performance benchmarks, leaving compelling fairness and ethical considerations behind. In MT, this might lead to misgendered translations, resulting, among other harms, in the perpetuation of stereotypes and prejudices. In this work, we address this gap by investigating whether and to what extent such models exhibit gender bias in machine translation and how we can mitigate it. Concretely, we compute established gender bias metrics on the WinoMT corpus from English to German and Spanish. We discover that IFT models default to male-inflected translations, even disregarding female occupational stereotypes. Next, using interpretability methods, we unveil that models systematically overlook the pronoun indicating the gender of a target occupation in misgendered translations. Finally, based on this finding, we propose an easy-to-implement and effective bias mitigation solution based on few-shot learning that leads to significantly fairer translations.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.243.pdf",
        "keywords": [
            "gender bias",
            "interpretability",
            "instruction tuned machine translation",
            "machine translation",
            "bias mitigation",
            "instruction fine tuned models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We discover that IFT models default to male-inflected translations, even disregarding female occupational stereotypes.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We discover that IFT models default to male-inflected translations, even disregarding female occupational stereotypes.\""
    },
    {
        "title": "DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining",
        "authors": [
            "Weifeng Jiang",
            "Qianren Mao",
            "Chenghua Lin",
            "Jianxin Li",
            "Ting Deng",
            "Weiyi Yang",
            "Zheng Wang"
        ],
        "published": "2023",
        "summary": "Many text mining models are constructed by fine-tuning a large deep pre-trained language model (PLM) in downstream tasks. However, a significant challenge that arises nowadays is how to maintain performance when we use a lightweight model with limited labeled samples. We present DisCo, a semi-supervised learning (SSL) framework for fine-tuning a cohort of small student models generated from a large PLM using knowledge distillation. Our key insight is to share complementary knowledge among distilled student cohorts to promote their SSL effectiveness. DisCo employs a novel co-training technique to optimize a cohort of multiple small student models by promoting knowledge sharing among students under diversified views: model views produced by different distillation strategies and data views produced by various input augmentations. We evaluate DisCo on both semi-supervised text classification and extractive summarization tasks. Experimental results show that DisCo can produce student models that are 7.6× smaller and 4.8 × faster in inference than the baseline PLMs while maintaining comparable performance. We also show that DisCo-generated student models outperform the similar-sized models elaborately tuned in distinct tasks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.244.pdf",
        "keywords": [
            "co training",
            "text mining",
            "extractive summarization",
            "semi supervised text mining",
            "semi supervised text classification",
            "semi supervised learning",
            "disco",
            "knowledge distillation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, a significant challenge that arises nowadays is how to maintain performance when we use a lightweight model with limited labeled samples.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, a significant challenge that arises nowadays is how to maintain performance when we use a lightweight model with limited labeled samples.\""
    },
    {
        "title": "Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation",
        "authors": [
            "Da Yin",
            "Xiao Liu",
            "Fan Yin",
            "Ming Zhong",
            "Hritik Bansal",
            "Jiawei Han",
            "Kai-Wei Chang"
        ],
        "published": "2023",
        "summary": "Instruction tuning has emerged to enhance the capabilities of large language models (LLMs) to comprehend instructions and generate appropriate responses. Existing methods either manually annotate or employ LLM (e.g., GPT-series) to generate data for instruction tuning. However, they often overlook associating instructions with existing annotated datasets. In this paper, we propose Dynosaur, a dynamic growth paradigm for the automatic curation of instruction-tuning data. Based on the metadata of existing datasets, we use LLMs to automatically construct instruction-tuning data by identifying relevant data fields and generating appropriate instructions. By leveraging the existing annotated datasets, Dynosaur offers several advantages: 1) it reduces the API cost for generating instructions (e.g., it costs less than $12 USD by calling GPT-3.5-turbo for generating 800K instruction tuning samples; 2) it provides high-quality data for instruction tuning (e.g., it performs better than Alpaca and Flan on Super-NI and Longform with comparable data sizes); and 3) it supports the continuous improvement of models by generating instruction-tuning data when a new annotated dataset becomes available. We further investigate a continual learning scheme for learning with the ever-growing instruction-tuning dataset, and demonstrate that replaying tasks with diverse instruction embeddings not only helps mitigate forgetting issues but generalizes to unseen tasks better. Code and data are available at https://github.com/WadeYin9712/Dynosaur.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.245.pdf",
        "keywords": [
            "tuning",
            "instruction tuning",
            "instruction tuning data curation",
            "instruction tuning dataset",
            "dynamic growth",
            "tuning samples",
            "automatic"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing methods... often overlook associating instructions with existing annotated datasets.\"\n\nThis paper mentions a limitation of existing methods for instruction tuning of LLMs, but it does not explicitly discuss limitations of LLMs themselves.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Existing methods... often overlook associating instructions with existing annotated datasets.\"\n\nThis paper mentions a limitation of existing methods for instruction tuning of LLMs, but it does not explicitly discuss limitations of LLMs themselves."
    },
    {
        "title": "Are All Steps Equally Important? Benchmarking Essentiality Detection in Event Processes",
        "authors": [
            "Haoyu Wang",
            "Hongming Zhang",
            "Yueguan Wang",
            "Yuqian Deng",
            "Muhao Chen",
            "Dan Roth"
        ],
        "published": "2023",
        "summary": "Natural language often describes events in different granularities, such that more coarse-grained (goal) events can often be decomposed into fine-grained sequences of (step) events. A critical but overlooked challenge in understanding an event process lies in the fact that the step events are not equally important to the central goal. In this paper, we seek to fill this gap by studying how well current models can understand the essentiality of different step events towards a goal event. As discussed by cognitive studies, such an ability enables the machine to mimic human’s commonsense reasoning about preconditions and necessary efforts of daily-life tasks. Our work contributes with a high-quality corpus of (goal, step) pairs from a community guideline website WikiHow, where the steps are manually annotated with their essentiality w.r.t. the goal. The high IAA indicates that humans have a consistent understanding of the events. Despite evaluating various statistical and massive pre-trained NLU models, we observe that existing SOTA models all perform drastically behind humans, indicating the need for future investigation of this crucial yet challenging task.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.246.pdf",
        "keywords": [
            "event processes",
            "essentiality",
            "benchmarking essentiality detection"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite evaluating various statistical and massive pre-trained NLU models, we observe that existing SOTA models all perform drastically behind humans, indicating the need for future investigation of this crucial yet challenging task.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Despite evaluating various statistical and massive pre-trained NLU models, we observe that existing SOTA models all perform drastically behind humans, indicating the need for future investigation of this crucial yet challenging task.\""
    },
    {
        "title": "Language Model is Suitable for Correction of Handwritten Mathematical Expressions Recognition",
        "authors": [
            "Zui Chen",
            "Jiaqi Han",
            "Chaofan Yang",
            "Yi Zhou"
        ],
        "published": "2023",
        "summary": "Handwritten mathematical expression recognition (HMER) is a multidisciplinary task that generates LaTeX sequences from images. Existing approaches, employing tree decoders within attention-based encoder-decoder architectures, aim to capture the hierarchical tree structure, but are limited by CFGs and pre-generated triplet data, hindering expandability and neglecting visual ambiguity challenges. This article investigates the distinctive language characteristics of LaTeX mathematical expressions, revealing two key observations: 1) the presence of explicit structural symbols, and 2) the treatment of symbols, particularly letters, as minimal units with context-dependent semantics, representing variables or constants. Rooted in these properties, we propose that language models have the potential to synchronously and complementarily provide both structural and semantic information, making them suitable for correction of HMER. To validate our proposition, we propose an architecture called Recognize and Language Fusion Network (RLFN), which integrates recognition and language features to output corrected sequences while jointly optimizing with a string decoder recognition model. Experiments show that RLFN outperforms existing state-of-the-art methods on the CROHME 2014/2016/2019 datasets.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.247.pdf",
        "keywords": [
            "language fusion network",
            "handwritten mathematical expressions recognition",
            "language features",
            "latex mathematical expressions",
            "string decoder recognition"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing approaches... are limited by CFGs and pre-generated triplet data, hindering expandability and neglecting visual ambiguity challenges.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Existing approaches... are limited by CFGs and pre-generated triplet data, hindering expandability and neglecting visual ambiguity challenges.\""
    },
    {
        "title": "mRedditSum: A Multimodal Abstractive Summarization Dataset of Reddit Threads with Images",
        "authors": [
            "Keighley Overbay",
            "Jaewoo Ahn",
            "Fatemeh Pesaran zadeh",
            "Joonsuk Park",
            "Gunhee Kim"
        ],
        "published": "2023",
        "summary": "The growing number of multimodal online discussions necessitates automatic summarization to save time and reduce content overload. However, existing summarization datasets are not suitable for this purpose, as they either do not cover discussions, multiple modalities, or both. To this end, we present mRedditSum, the first multimodal discussion summarization dataset. It consists of 3,033 discussion threads where a post solicits advice regarding an issue described with an image and text, and respective comments express diverse opinions. We annotate each thread with a human-written summary that captures both the essential information from the text, as well as the details available only in the image. Experiments show that popular summarization models—GPT-3.5, BART, and T5—consistently improve in performance when visual information is incorporated. We also introduce a novel method, cluster-based multi-stage summarization, that outperforms existing baselines and serves as a competitive baseline for future work.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.251.pdf",
        "keywords": [
            "mredditsum",
            "cluster"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but it mentions that \"popular summarization models—GPT-3.5, BART, and T5—consistently improve in performance when visual information is incorporated\", implying that they may not perform well without visual information.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None, but it mentions that \"popular summarization models—GPT-3.5, BART, and T5—consistently improve in performance when visual information is incorporated\", implying that they may not perform well without visual information."
    },
    {
        "title": "Sparse Low-rank Adaptation of Pre-trained Language Models",
        "authors": [
            "Ning Ding",
            "Xingtai Lv",
            "Qiaosen Wang",
            "Yulin Chen",
            "Bowen Zhou",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2023",
        "summary": "Fine-tuning pre-trained large language models in a parameter-efficient manner is widely studied for its effectiveness and efficiency. The popular method of low-rank adaptation (LoRA) offers a notable approach, hypothesizing that the adaptation process is intrinsically low-dimensional. Although LoRA has demonstrated commendable performance, it is implemented with a fixed and unalterable intrinsic rank that might not always be the ideal choice. Recognizing the need for more flexible adaptation, we extend the methodology of LoRA to an innovative approach we call sparse low-rank adaptation (SoRA) that enables dynamic adjustments to the intrinsic rank during the adaptation process. We achieve this through the incorporation of a gate unit optimized with proximal gradient method in the training stage, controlling the cardinality of rank under the sparsity of the gate. In the subsequent inference stage, we eliminate the parameter blocks corresponding to the zeroed-out ranks, to reduce each SoRA module back to a concise yet rank-optimal LoRA. Our approach strengthens the representation power of LoRA by initializing it with a higher rank, while efficiently taming a temporarily increased number of parameters via updating in a sparse way. We further introduce a sparsifying scheduler for SoRA, aiming to examine the impact of the number of non-zero parameters on the model’s memorization and generalization. Our experimental results demonstrate that SoRA can outperform other baselines even with 70% retained parameters and 70% training time.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.252.pdf",
        "keywords": [
            "low rank adaptation",
            "sparse low rank adaptation",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although LoRA has demonstrated commendable performance, it is implemented with a fixed and unalterable intrinsic rank that might not always be the ideal choice.\"\n\nThis paper discusses the limitations of low-rank adaptation (LoRA) method for fine-tuning pre-trained large language models, specifically the fixed and unalterable intrinsic rank, but it does not extensively analyze this limitation and focuses on",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although LoRA has demonstrated commendable performance, it is implemented with a fixed and unalterable intrinsic rank that might not always be the ideal choice.\"\n\nThis paper discusses the limitations of low-rank adaptation (LoRA) method for fine-tuning pre-trained large language models, specifically the fixed and unalterable intrinsic rank, but it does not extensively analyze this limitation and focuses on"
    },
    {
        "title": "Human Learning by Model Feedback: The Dynamics of Iterative Prompting with Midjourney",
        "authors": [
            "Shachar Don-Yehiya",
            "Leshem Choshen",
            "Omri Abend"
        ],
        "published": "2023",
        "summary": "Generating images with a Text-to-Image model often requires multiple trials, where human users iteratively update their prompt based on feedback, namely the output image. Taking inspiration from cognitive work on reference games and dialogue alignment, this paper analyzes the dynamics of the user prompts along such iterations. We compile a dataset of iterative interactions of human users with Midjourney. Our analysis then reveals that prompts predictably converge toward specific traits along these iterations. We further study whether this convergence is due to human users, realizing they missed important details, or due to adaptation to the model’s “preferences”, producing better images for a specific language style. We show initial evidence that both possibilities are at play. The possibility that users adapt to the model’s preference raises concerns about reusing user data for further training. The prompts may be biased towards the preferences of a specific model, rather than align with human intentions and natural manner of expression.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.253.pdf",
        "keywords": [
            "midjourney",
            "iterative prompting",
            "model feedback",
            "human learning"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The possibility that users adapt to the model’s preference raises concerns about reusing user data for further training. The prompts may be biased towards the preferences of a specific model, rather than align with human intentions and natural manner of expression.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"The possibility that users adapt to the model’s preference raises concerns about reusing user data for further training. The prompts may be biased towards the preferences of a specific model, rather than align with human intentions and natural manner of expression.\""
    },
    {
        "title": "The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models",
        "authors": [
            "Jingyuan Qi",
            "Zhiyang Xu",
            "Ying Shen",
            "Minqian Liu",
            "Di Jin",
            "Qifan Wang",
            "Lifu Huang"
        ],
        "published": "2023",
        "summary": "Chain-of-Thought (CoT) prompting enables large language models to solve complex reasoning problems by generating intermediate steps. However, confined by its inherent single-pass and sequential generation process, CoT heavily relies on the initial decisions, causing errors in early steps to accumulate and impact the final answers. In contrast, humans adopt recursive thinking when tackling complex reasoning problems, i.e. iteratively breaking the original problem into approachable sub-problems and aggregating their answers to resolve the original one. Inspired by the human cognitive process, we propose SOCRATIC QUESTIONING, a divide-and-conquer style algorithm that mimics the recursive thinking process. Specifically, SOCRATIC QUESTIONING leverages large language models to raise and answer sub-questions until collecting enough information to tackle the original question. Unlike CoT, SOCRATIC QUESTIONING explicitly navigates the thinking space, stimulates effective recursive thinking, and is more robust towards errors in the thinking process. Extensive experiments on several complex reasoning tasks, including MMLU, MATH, LogiQA, and visual question-answering demonstrate significant performance improvements over the state-of-the-art prompting methods, such as CoT, and Tree-of-Thought. The qualitative analysis clearly shows that the intermediate reasoning steps elicited by SOCRATIC QUESTIONING are similar to humans’ recursively thinking process of complex reasoning problems.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.255.pdf",
        "keywords": [
            "thinking",
            "recursive thinking",
            "art",
            "reasoning",
            "complex reasoning",
            "socratic questioning",
            "language models",
            "recursive thinking process",
            "the art"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"confined by its inherent single-pass and sequential generation process, CoT heavily relies on the initial decisions, causing errors in early steps to accumulate and impact the final answers.\"\n\nThis abstract mentions a limitation of LLMs, specifically the Chain-of-Thought (CoT) prompting method, but does not explore it in depth and focuses on the proposed solution, SOCRATIC QUESTIONING",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"confined by its inherent single-pass and sequential generation process, CoT heavily relies on the initial decisions, causing errors in early steps to accumulate and impact the final answers.\"\n\nThis abstract mentions a limitation of LLMs, specifically the Chain-of-Thought (CoT) prompting method, but does not explore it in depth and focuses on the proposed solution, SOCRATIC QUESTIONING"
    },
    {
        "title": "Transductive Learning for Textual Few-Shot Classification in API-based Embedding Models",
        "authors": [
            "Pierre Colombo",
            "Victor Pellegrain",
            "Malik Boudiaf",
            "Myriam Tami",
            "Victor Storchan",
            "Ismail Ayed",
            "Pablo Piantanida"
        ],
        "published": "2023",
        "summary": "Proprietary and closed APIs are becoming increasingly common to process natural language, and are impacting the practical applications of natural language processing, including few-shot classification. Few-shot classification involves training a model to perform a new classification task with a handful of labeled data. This paper presents three contributions. First, we introduce a scenario where the embedding of a pre-trained model is served through a gated API with compute-cost and data-privacy constraints. Second, we propose a transductive inference, a learning paradigm that has been overlooked by the NLP community. Transductive inference, unlike traditional inductive learning, leverages the statistics of unlabelled data. We also introduce a new parameter-free transductive regularizer based on the Fisher-Rao loss, which can be used on top of the gated API embeddings. This method fully utilizes unlabelled data, does not share any label with the third-party API provider and could serve as a baseline for future research. Third, we propose an improved experimental setting and compile a benchmark of eight datasets involving multiclass classification in four different languages, with up to 151 classes. We evaluate our methods using eight backbone models, along with an episodic evaluation over 1,000 episodes, which demonstrate the superiority of transductive inference over the standard inductive setting.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.257.pdf",
        "keywords": [
            "transductive",
            "unlabelled data",
            "gated api",
            "transductive inference",
            "inductive learning",
            "classification",
            "shot classification"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "MEGA: Multilingual Evaluation of Generative AI",
        "authors": [
            "Kabir Ahuja",
            "Harshita Diddee",
            "Rishav Hada",
            "Millicent Ochieng",
            "Krithika Ramesh",
            "Prachi Jain",
            "Akshay Nambi",
            "Tanuja Ganu",
            "Sameer Segal",
            "Mohamed Ahmed",
            "Kalika Bali",
            "Sunayana Sitaram"
        ],
        "published": "2023",
        "summary": "Generative AI models have shown impressive performance on many Natural Language Processing tasks such as language understanding, reasoning, and language generation. An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative LLMs have been restricted to English and it is unclear how capable these models are at understanding and generating text in other languages. We present the first comprehensive benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks, covering 16 NLP datasets across 70 typologically diverse languages. We compare the performance of generative LLMs including Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages. We create a framework for evaluating generative LLMs in the multilingual setting and provide directions for future progress in the field.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.258.pdf",
        "keywords": [
            "generative ai",
            "multilingual evaluation"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages.\""
    },
    {
        "title": "Large Language Models are Temporal and Causal Reasoners for Video Question Answering",
        "authors": [
            "Dohwan Ko",
            "Ji Lee",
            "Woo-Young Kang",
            "Byungseok Roh",
            "Hyunwoo Kim"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have shown remarkable performances on a wide range of natural language understanding and generation tasks. We observe that the LLMs provide effective priors in exploiting linguistic shortcuts for temporal and causal reasoning in Video Question Answering (VideoQA). However, such priors often cause suboptimal results on VideoQA by leading the model to over-rely on questions, i.e., linguistic bias, while ignoring visual content. This is also known as ‘ungrounded guesses’ or ‘hallucinations’. To address this problem while leveraging LLMs’ prior on VideoQA, we propose a novel framework, Flipped-VQA, encouraging the model to predict all the combinations of ⟨V, Q, A⟩ triplet by flipping the source pair and the target label to understand their complex relationships, i.e., predict A, Q, and V given a VQ, VA, and QA pairs, respectively. In this paper, we develop LLaMA-VQA by applying Flipped-VQA to LLaMA, and it outperforms both LLMs-based and non-LLMs-based models on five challenging VideoQA benchmarks. Furthermore, our Flipped-VQA is a general framework that is applicable to various LLMs (OPT and GPT-J) and consistently improves their performances. We empirically demonstrate that Flipped-VQA not only enhances the exploitation of linguistic shortcuts but also mitigates the linguistic bias, which causes incorrect answers over-relying on the question. Code is available at https://github.com/mlvlab/Flipped-VQA.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.261.pdf",
        "keywords": [
            "large language models",
            "video question answering",
            "linguistic",
            "reasoners"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, such priors often cause suboptimal results on VideoQA by leading the model to over-rely on questions, i.e., linguistic bias, while ignoring visual content. This is also known as ‘ungrounded guesses’ or ‘hallucinations’.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, such priors often cause suboptimal results on VideoQA by leading the model to over-rely on questions, i.e., linguistic bias, while ignoring visual content. This is also known as ‘ungrounded guesses’ or ‘hallucinations’.\""
    },
    {
        "title": "Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation",
        "authors": [
            "Yuanyuan Liang",
            "Jianing Wang",
            "Hanlun Zhu",
            "Lei Wang",
            "Weining Qian",
            "Yunshi Lan"
        ],
        "published": "2023",
        "summary": "The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question. For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed. However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation. The emergence of Large Language Models (LLMs) has shown their impressive generalization ability in few-shot tasks. Inspired by Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for reasoning, we formulate KBQG task as a reasoning problem, where the generation of a complete question is splitted into a series of sub-question generation. Our proposed prompting method KQG-CoT first retrieves supportive logical forms from the unlabeled data pool taking account of the characteristics of the logical form. Then, we write a prompt to explicit the reasoning chain of generating complicated questions based on the selected demonstrations. To further ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting the logical forms by their complexity. We conduct extensive experiments over three public KBQG datasets. The results demonstrate that our prompting method consistently outperforms other prompting baselines on the evaluated datasets. Remarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results of the PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4, METEOR, and ROUGE-L, respectively.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.263.pdf",
        "keywords": [
            "thought",
            "chain of thought",
            "language models",
            "prompting",
            "question annotation",
            "question generation",
            "few shot question generation",
            "natural language",
            "knowledge bases"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation.\""
    },
    {
        "title": "TrojanSQL: SQL Injection against Natural Language Interface to Database",
        "authors": [
            "Jinchuan Zhang",
            "Yan Zhou",
            "Binyuan Hui",
            "Yaxin Liu",
            "Ziming Li",
            "Songlin Hu"
        ],
        "published": "2023",
        "summary": "The technology of text-to-SQL has significantly enhanced the efficiency of accessing and manipulating databases. However, limited research has been conducted to study its vulnerabilities emerging from malicious user interaction. By proposing TrojanSQL, a backdoor-based SQL injection framework for text-to-SQL systems, we show how state-of-the-art text-to-SQL parsers can be easily misled to produce harmful SQL statements that can invalidate user queries or compromise sensitive information about the database. The study explores two specific injection attacks, namely boolean-based injection and union-based injection, which use different types of triggers to achieve distinct goals in compromising the parser. Experimental results demonstrate that both medium-sized models based on fine-tuning and LLM-based parsers using prompting techniques are vulnerable to this type of attack, with attack success rates as high as 99% and 89%, respectively. We hope that this study will raise more concerns about the potential security risks of building natural language interfaces to databases.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.264.pdf",
        "keywords": [
            "sql",
            "sql injection",
            "databases",
            "trojansql",
            "natural language interfaces",
            "injection attacks",
            "text",
            "text to sql parsers",
            "boolean based injection",
            "text to sql systems",
            "union based injection",
            "security",
            "manipulating databases",
            "backdoor"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experimental results demonstrate that both medium-sized models based on fine-tuning and LLM-based parsers using prompting techniques are vulnerable to this type of attack, with attack success rates as high as 99% and 89%, respectively.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Experimental results demonstrate that both medium-sized models based on fine-tuning and LLM-based parsers using prompting techniques are vulnerable to this type of attack, with attack success rates as high as 99% and 89%, respectively.\""
    },
    {
        "title": "Preserving Privacy Through Dememorization: An Unlearning Technique For Mitigating Memorization Risks In Language Models",
        "authors": [
            "Aly Kassem",
            "Omar Mahmoud",
            "Sherif Saad"
        ],
        "published": "2023",
        "summary": "Large Language models (LLMs) are trained on vast amounts of data, including sensitive information that poses a risk to personal privacy if exposed. LLMs have shown the ability to memorize and reproduce portions of their training data when prompted by adversaries. Prior research has focused on addressing this memorization issue and preventing verbatim replication through techniques like knowledge unlearning and data pre-processing. However, these methods have limitations regarding the number of protected samples, limited privacy types, and potentially lower-quality generative models. To tackle this challenge more effectively, we propose “DeMem,” a novel unlearning approach that utilizes an efficient reinforcement learning feedback loop via proximal policy optimization. By fine-tuning the language model with a negative similarity score as a reward signal, we incentivize the LLMs to learn a paraphrasing policy to unlearn the pre-training data. Our experiments demonstrate that DeMem surpasses strong baselines and state-of-the-art methods in terms of its ability to generalize and strike a balance between maintaining privacy and LLM performance.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.265.pdf",
        "keywords": [
            "memorize",
            "memorization risks",
            "language models",
            "unlearning",
            "reinforcement learning",
            "privacy",
            "proximal policy optimization"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these methods have limitations regarding the number of protected samples, limited privacy types, and potentially lower-quality generative models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, these methods have limitations regarding the number of protected samples, limited privacy types, and potentially lower-quality generative models.\""
    },
    {
        "title": "Meta-Learning Online Adaptation of Language Models",
        "authors": [
            "Nathan Hu",
            "Eric Mitchell",
            "Christopher Manning",
            "Chelsea Finn"
        ],
        "published": "2023",
        "summary": "Large language models encode impressively broad world knowledge in their parameters. However, the knowledge in static language models falls out of date, limiting the model’s effective “shelf life.” While online fine-tuning can reduce this degradation, we find that naively fine-tuning on a stream of documents leads to a low level of information uptake. We hypothesize that online fine-tuning does not sufficiently attend to important information. That is, the gradient signal from important tokens representing factual information is drowned out by the gradient from inherently noisy tokens, suggesting that a dynamic, context-aware learning rate may be beneficial. We therefore propose learning which tokens to upweight. We meta-train a small, autoregressive model to reweight the language modeling loss for each token during online fine-tuning, with the objective of maximizing the out-of-date base question-answering model’s ability to answer questions about a document after a single weighted gradient step. We call this approach Context-aware Meta-learned Loss Scaling (CaMeLS). Across three different distributions of documents, our experiments find that CaMeLS provides substantially improved information uptake on streams of thousands of documents compared with standard fine-tuning and baseline heuristics for reweighting token losses.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.268.pdf",
        "keywords": [
            "tuning",
            "language models",
            "fine tuning",
            "online fine tuning",
            "meta learning online adaptation",
            "large language models",
            "context aware"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the knowledge in static language models falls out of date, limiting the model’s effective “shelf life.”; \"We hypothesize that online fine-tuning does not sufficiently attend to important information.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, the knowledge in static language models falls out of date, limiting the model’s effective “shelf life.”; \"We hypothesize that online fine-tuning does not sufficiently attend to important information.\""
    },
    {
        "title": "Self-Detoxifying Language Models via Toxification Reversal",
        "authors": [
            "Chak Tou Leong",
            "Yi Cheng",
            "Jiashuo Wang",
            "Jian Wang",
            "Wenjie Li"
        ],
        "published": "2023",
        "summary": "Language model detoxification aims to minimize the risk of generating offensive or harmful content in pretrained language models (PLMs) for safer deployment. Existing methods can be roughly categorized as finetuning-based and decoding-based. However, the former is often resource-intensive, while the latter relies on additional components and potentially compromises the generation fluency. In this paper, we propose a more lightweight approach that enables the PLM itself to achieve “self-detoxification”. Our method is built upon the observation that prepending a negative steering prompt can effectively induce PLMs to generate toxic content. At the same time, we are inspired by the recent research in the interpretability field, which formulates the evolving contextualized representations within the PLM as an information stream facilitated by the attention layers. Drawing on this idea, we devise a method to identify the toxification direction from the normal generation process to the one prompted with the negative prefix, and then steer the generation to the reversed direction by manipulating the information movement within the attention layers. Experimental results show that our approach, without any fine-tuning or extra components, can achieve comparable performance with state-of-the-art methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.269.pdf",
        "keywords": [
            "language models",
            "self detoxification",
            "detoxification",
            "toxification reversal"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Language model detoxification aims to minimize the risk of generating offensive or harmful content in pretrained language models (PLMs) for safer deployment.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Language model detoxification aims to minimize the risk of generating offensive or harmful content in pretrained language models (PLMs) for safer deployment.\""
    },
    {
        "title": "POE: Process of Elimination for Multiple Choice Reasoning",
        "authors": [
            "Chenkai Ma",
            "Xinya Du"
        ],
        "published": "2023",
        "summary": "Language models (LMs) are capable of conducting in-context learning for multiple choice reasoning tasks, but the options in these tasks are treated equally. As humans often first eliminate wrong options before picking the final correct answer, we argue a similar two-step strategy can make LMs better at these tasks. To this end, we present the Process of Elimination (POE), a two-step scoring method. In the first step, POE scores each option, and eliminates seemingly wrong options. In the second step, POE masks these wrong options, and makes the final prediction from the remaining options. Zero-shot experiments on 8 reasoning tasks illustrate the effectiveness of POE, and a following analysis finds our method to be especially performant on logical reasoning tasks. We further analyze the effect of masks, and show that POE applies to few-shot settings and large language models (LLMs) like ChatGPT.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.273.pdf",
        "keywords": [
            "multiple choice reasoning",
            "language models",
            "elimination"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Language models (LMs) are capable of conducting in-context learning for multiple choice reasoning tasks, but the options in these tasks are treated equally.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Language models (LMs) are capable of conducting in-context learning for multiple choice reasoning tasks, but the options in these tasks are treated equally.\""
    },
    {
        "title": "Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis",
        "authors": [
            "Hongyi Zheng",
            "Abulhair Saparov"
        ],
        "published": "2023",
        "summary": "Recent advances in prompt engineering enable large language models (LLMs) to solve multi-hop logical reasoning problems with impressive accuracy. However, there is little existing work investigating the robustness of LLMs with few-shot prompting techniques. Therefore, we introduce a systematic approach to test the robustness of LLMs in multi-hop reasoning tasks via domain-agnostic perturbations. We include perturbations at multiple levels of abstractions (e.g. lexical perturbations such as typos, and semantic perturbations such as the inclusion of intermediate reasoning steps in the questions) to conduct behavioral analysis on the LLMs. Throughout our experiments, we find that models are more sensitive to certain perturbations such as replacing words with their synonyms. We also demonstrate that increasing the proportion of perturbed exemplars in the prompts improves the robustness of few-shot prompting methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.277.pdf",
        "keywords": [
            "domain agnostic",
            "behavioral analysis",
            "language"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Throughout our experiments, we find that models are more sensitive to certain perturbations such as replacing words with their synonyms.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Throughout our experiments, we find that models are more sensitive to certain perturbations such as replacing words with their synonyms.\""
    },
    {
        "title": "Can Large Language Models Capture Dissenting Human Voices?",
        "authors": [
            "Noah Lee",
            "Na Min An",
            "James Thorne"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have shown impressive achievements in solving a broad range of tasks. Augmented by instruction fine-tuning, LLMs have also been shown to generalize in zero-shot settings as well. However, whether LLMs closely align with the human disagreement distribution has not been well-studied, especially within the scope of natural language inference (NLI). In this paper, we evaluate the performance and alignment of LLM distribution with humans using two different techniques to estimate the multinomial distribution: Monte Carlo Estimation (MCE) and Log Probability Estimation (LPE). As a result, we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution. The inference and human alignment performances plunge even further on data samples with high human disagreement levels, raising concerns about their natural language understanding (NLU) ability and their representativeness to a larger human population.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.278.pdf",
        "keywords": [
            "log probability estimation",
            "multinomial distribution",
            "monte carlo estimation",
            "natural language inference",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As a result, we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution. The inference and human alignment performances plunge even further on data samples with high human disagreement levels, raising concerns about their natural language understanding (NLU) ability and their representativeness to a larger human population.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"As a result, we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution. The inference and human alignment performances plunge even further on data samples with high human disagreement levels, raising concerns about their natural language understanding (NLU) ability and their representativeness to a larger human population.\""
    },
    {
        "title": "DecoMT: Decomposed Prompting for Machine Translation Between Related Languages using Large Language Models",
        "authors": [
            "Ratish Puduppully",
            "Anoop Kunchukuttan",
            "Raj Dabre",
            "Ai Ti Aw",
            "Nancy Chen"
        ],
        "published": "2023",
        "summary": "This study investigates machine translation between related languages i.e., languages within the same family that share linguistic characteristics such as word order and lexical similarity. Machine translation through few-shot prompting leverages a small set of translation pair examples to generate translations for test sentences. This procedure requires the model to learn how to generate translations while simultaneously ensuring that token ordering is maintained to produce a fluent and accurate translation. We propose that for related languages, the task of machine translation can be simplified by leveraging the monotonic alignment characteristic of such languages. We introduce DecoMT, a novel approach of few-shot prompting that decomposes the translation process into a sequence of word chunk translations. Through automatic and human evaluation conducted on multiple related language pairs across various language families, we demonstrate that our proposed approach of decomposed prompting surpasses multiple established few-shot baseline approaches. For example, DecoMT outperforms the strong few-shot prompting BLOOM model with an average improvement of 8 chrF++ scores across the examined languages.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.279.pdf",
        "keywords": [
            "machine translation",
            "languages",
            "language models",
            "prompting",
            "related language",
            "language families",
            "decomposed prompting"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the title mentions \"Large Language Models\" and the paper discusses the challenges of machine translation using LLMs, but does not explicitly mention any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the title mentions \"Large Language Models\" and the paper discusses the challenges of machine translation using LLMs, but does not explicitly mention any limitations of LLMs."
    },
    {
        "title": "Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning",
        "authors": [
            "Hao Zhao",
            "Jie Fu",
            "Zhaofeng He"
        ],
        "published": "2023",
        "summary": "Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in adapting the pre-trained language models to downstream tasks while only updating a small number of parameters. Despite the success, most existing methods independently adapt to each task without considering knowledge transfer between tasks and are limited to low-data regimes. To overcome this issue, we propose Prototype-based HyperAdapter (PHA), a novel framework built on the adapter-tuning and hypernetwork. It introduces an instance-dense retriever and a prototypical hypernetwork to generate the conditional modules in a sample-efficient manner. This leads to comparable performance improvements against existing PEFT methods on multi-task learning and few-shot transfer learning. More importantly, when the available data size gets smaller, our method outperforms other strong baselines by a large margin. Based on our extensive empirical experiments across various datasets, we demonstrate that PHA strikes a better trade-off between trainable parameters, accuracy on stream tasks, and sample efficiency. Our code is publicly available at https://github.com/Bumble666/PHA",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.280.pdf",
        "keywords": [
            "hyperadapter",
            "sample",
            "sample efficient",
            "parameter efficient fine tuning",
            "adapter tuning",
            "multi task learning"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper discusses limitations of existing methods (e.g., \"most existing methods independently adapt to each task without considering knowledge transfer between tasks and are limited to low-data regimes\"), but not specifically Large Language Models (LLMs).",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper discusses limitations of existing methods (e.g., \"most existing methods independently adapt to each task without considering knowledge transfer between tasks and are limited to low-data regimes\"), but not specifically Large Language Models (LLMs)."
    },
    {
        "title": "Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for Knowledge-Grounded Dialogue",
        "authors": [
            "Lang Qin",
            "Yao Zhang",
            "Hongru Liang",
            "Jun Wang",
            "Zhenglu Yang"
        ],
        "published": "2023",
        "summary": "Accurate knowledge selection is critical in knowledge-grounded dialogue systems. Towards a closer look at it, we offer a novel perspective to organize existing literature, i.e., knowledge selection coupled with, after, and before generation. We focus on the third under-explored category of study, which can not only select knowledge accurately in advance, but has the advantage to reduce the learning, adjustment, and interpretation burden of subsequent response generation models, especially LLMs. We propose \\tt{GATE}, a generator-agnostic knowledge selection method, to prepare knowledge for subsequent response generation models by selecting context-related knowledge among different knowledge structures and variable knowledge requirements. Experimental results demonstrate the superiority of \\tt{GATE}, and indicate that knowledge selection before generation is a lightweight yet effective way to facilitate LLMs (e.g., ChatGPT) to generate more informative responses.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.285.pdf",
        "keywords": [
            "knowledge",
            "knowledge selection",
            "generator",
            "knowledge grounded dialogue",
            "generator agnostic knowledge selection"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"has the advantage to reduce the learning, adjustment, and interpretation burden of subsequent response generation models, especially LLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"has the advantage to reduce the learning, adjustment, and interpretation burden of subsequent response generation models, especially LLMs.\""
    },
    {
        "title": "Merging Generated and Retrieved Knowledge for Open-Domain QA",
        "authors": [
            "Yunxiang Zhang",
            "Muhammad Khalifa",
            "Lajanugen Logeswaran",
            "Moontae Lee",
            "Honglak Lee",
            "Lu Wang"
        ],
        "published": "2023",
        "summary": "Open-domain question answering (QA) systems are often built with retrieval modules. However, retrieving passages from a given source is known to suffer from insufficient knowledge coverage. Alternatively, prompting large language models (LLMs) to generate contextual passages based on their parametric knowledge has been shown to improve QA performance. Yet, LLMs tend to “hallucinate” content that conflicts with the retrieved knowledge. Based on the intuition that answers supported by both sources are more likely to be correct, we propose COMBO, a Compatibility-Oriented knowledge Merging for Better Open-domain QA framework, to effectively leverage the two sources of information. Concretely, we match LLM-generated passages with retrieved counterparts into compatible pairs, based on discriminators trained with silver compatibility labels. Then a Fusion-in-Decoder-based reader model handles passage pairs to arrive at the final answer. Experiments show that COMBO outperforms competitive baselines on three out of four tested open-domain QA benchmarks. Further analysis reveals that our proposed framework demonstrates greater efficacy in scenarios with a higher degree of knowledge conflicts.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.286.pdf",
        "keywords": [
            "passages",
            "merging",
            "knowledge merging",
            "knowledge",
            "qa",
            "retrieved knowledge",
            "open domain question answering",
            "passage pairs"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Yet, LLMs tend to “hallucinate” content that conflicts with the retrieved knowledge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Yet, LLMs tend to “hallucinate” content that conflicts with the retrieved knowledge.\""
    },
    {
        "title": "Best of Both Worlds: Towards Improving Temporal Knowledge Base Question Answering via Targeted Fact Extraction",
        "authors": [
            "Nithish Kannen",
            "Udit Sharma",
            "Sumit Neelam",
            "Dinesh Khandelwal",
            "Shajith Ikbal",
            "Hima Karanam",
            "L Subramaniam"
        ],
        "published": "2023",
        "summary": "Temporal question answering (QA) is a special category of complex question answering task that requires reasoning over facts asserting time intervals of events. Previous works have predominately relied on Knowledge Base Question Answering (KBQA) for temporal QA. One of the major challenges faced by these systems is their inability to retrieve all relevant facts due to factors such as incomplete KB and entity/relation linking errors. A failure to fetch even a single fact will block KBQA from computing the answer. Such cases of KB incompleteness are even more profound in the temporal context. To address this issue, we explore an interesting direction where a targeted temporal fact extraction technique is used to assist KBQA whenever it fails to retrieve temporal facts from the KB. We model the extraction problem as an open-domain question answering task using off-the-shelf language models. This way, we target to extract from textual resources those facts that failed to get retrieved from the KB. Experimental results on two temporal QA benchmarks show promising ~30% & ~10% relative improvements in answer accuracies without any additional training cost.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.287.pdf",
        "keywords": [
            "temporal question answering",
            "open domain question answering",
            "temporal knowledge base question",
            "knowledge base question answering"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"One of the major challenges faced by these systems is their inability to retrieve all relevant facts due to factors such as incomplete KB and entity/relation linking errors.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"One of the major challenges faced by these systems is their inability to retrieve all relevant facts due to factors such as incomplete KB and entity/relation linking errors.\""
    },
    {
        "title": "Text Fact Transfer",
        "authors": [
            "Nishant Balepur",
            "Jie Huang",
            "Kevin Chang"
        ],
        "published": "2023",
        "summary": "Text style transfer is a prominent task that aims to control the style of text without inherently changing its factual content. To cover more text modification applications, such as adapting past news for current events and repurposing educational materials, we propose the task of text fact transfer, which seeks to transfer the factual content of a source text between topics without modifying its style. We find that existing language models struggle with text fact transfer, due to their inability to preserve the specificity and phrasing of the source text, and tendency to hallucinate errors. To address these issues, we design ModQGA, a framework that minimally modifies a source text with a novel combination of end-to-end question generation and specificity-aware question answering. Through experiments on four existing datasets adapted for text fact transfer, we show that ModQGA can accurately transfer factual content without sacrificing the style of the source text.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.288.pdf",
        "keywords": [
            "text fact transfer",
            "factual content"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that existing language models struggle with text fact transfer, due to their inability to preserve the specificity and phrasing of the source text, and tendency to hallucinate errors.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We find that existing language models struggle with text fact transfer, due to their inability to preserve the specificity and phrasing of the source text, and tendency to hallucinate errors.\""
    },
    {
        "title": "A Cheaper and Better Diffusion Language Model with Soft-Masked Noise",
        "authors": [
            "Jiaao Chen",
            "Aston Zhang",
            "Mu Li",
            "Alex Smola",
            "Diyi Yang"
        ],
        "published": "2023",
        "summary": "Diffusion models that are based on iterative denoising have been recently proposed and leveraged in various generation tasks like image generation. Whereas, as a way inherently built for continuous data, existing diffusion models still have some limitations in modeling discrete data, e.g., languages. For example, the generally used Gaussian noise can not handle the discrete corruption well, and the objectives in continuous spaces fail to be stable for textual data in the diffusion process especially when the dimension is high. To alleviate these issues, we introduce a novel diffusion model for language modeling, Masked-Diffuse LM, with lower training cost and better performances, inspired by linguistic features in languages. Specifically, we design a linguistic-informed forward process which adds corruptions to the text through strategically soft-masking to better noise the textual data. Also, we directly predict the categorical distribution with cross-entropy loss function in every diffusion step to connect the continuous space and discrete space in a more efficient and straightforward way. Through experiments on 5 controlled generation tasks, we demonstrate that our Masked-Diffuse LM can achieve better generation quality than the state-of-the-art diffusion models with better efficiency.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.289.pdf",
        "keywords": [
            "soft masking",
            "soft masked noise",
            "denoising",
            "language modeling"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"existing diffusion models still have some limitations in modeling discrete data, e.g., languages. For example, the generally used Gaussian noise can not handle the discrete corruption well, and the objectives in continuous spaces fail to be stable for textual data in the diffusion process especially when the dimension is high.\"\n\nThis rating is based on the fact that the abstract mentions limitations of existing diffusion models in modeling discrete",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"existing diffusion models still have some limitations in modeling discrete data, e.g., languages. For example, the generally used Gaussian noise can not handle the discrete corruption well, and the objectives in continuous spaces fail to be stable for textual data in the diffusion process especially when the dimension is high.\"\n\nThis rating is based on the fact that the abstract mentions limitations of existing diffusion models in modeling discrete"
    },
    {
        "title": "Cognitive Dissonance: Why Do Language Model Outputs Disagree with Internal Representations of Truthfulness?",
        "authors": [
            "Kevin Liu",
            "Stephen Casper",
            "Dylan Hadfield-Menell",
            "Jacob Andreas"
        ],
        "published": "2023",
        "summary": "Neural language models (LMs) can be used to evaluate the truth of factual statements in two ways: they can be either queried for statement probabilities, or probed for internal representations of truthfulness. Past work has found that these two procedures sometimes disagree, and that probes tend to be more accurate than LM outputs. This has led some researchers to conclude that LMs “lie’ or otherwise encode non-cooperative communicative intents. Is this an accurate description of today’s LMs, or can query–probe disagreement arise in other ways? We identify three different classes of disagreement, which we term confabulation, deception, and heterogeneity. In many cases, the superiority of probes is simply attributable to better calibration on uncertain answers rather than a greater fraction of correct, high-confidence answers. In some cases, queries and probes perform better on different subsets of inputs, and accuracy can further be improved by ensembling the two.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.291.pdf",
        "keywords": [
            "truthfulness",
            "communicative",
            "disagree",
            "language model",
            "neural language models",
            "disagreement",
            "cognitive dissonance"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Past work has found that these two procedures sometimes disagree, and that probes tend to be more accurate than LM outputs. This has led some researchers to conclude that LMs “lie’ or otherwise encode non-cooperative communicative intents.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Past work has found that these two procedures sometimes disagree, and that probes tend to be more accurate than LM outputs. This has led some researchers to conclude that LMs “lie’ or otherwise encode non-cooperative communicative intents.\""
    },
    {
        "title": "Post-hoc Utterance Refining Method by Entity Mining for Faithful Knowledge Grounded Conversations",
        "authors": [
            "Yoonna Jang",
            "Suhyune Son",
            "Jeongwoo Lee",
            "Junyoung Son",
            "Yuna Hur",
            "Jungwoo Lim",
            "Hyeonseok Moon",
            "Kisu Yang",
            "Heuiseok Lim"
        ],
        "published": "2023",
        "summary": "Despite the striking advances in recent language generation performance, model-generated responses have suffered from the chronic problem of hallucinations that are either untrue or unfaithful to a given source. Especially in the task of knowledge grounded conversation, the models are required to generate informative responses, but hallucinated utterances lead to miscommunication. In particular, entity-level hallucination that causes critical misinformation and undesirable conversation is one of the major concerns. To address this issue, we propose a post-hoc refinement method called REM. It aims to enhance the quality and faithfulness of hallucinated utterances by refining them based on the source knowledge. If the generated utterance has a low source-faithfulness score with the given knowledge, REM mines the key entities in the knowledge and implicitly uses them for refining the utterances. We verify that our method reduces entity hallucination in the utterance. Also, we show the adaptability and efficacy of REM with extensive experiments and generative results. Our code is available at https://github.com/YOONNAJANG/REM.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.295.pdf",
        "keywords": [
            "hallucination",
            "entity hallucination",
            "knowledge grounded conversations",
            "knowledge",
            "entity mining",
            "utterance refining method"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the striking advances in recent language generation performance, model-generated responses have suffered from the chronic problem of hallucinations that are either untrue or unfaithful to a given source.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Despite the striking advances in recent language generation performance, model-generated responses have suffered from the chronic problem of hallucinations that are either untrue or unfaithful to a given source.\""
    },
    {
        "title": "Can We Edit Factual Knowledge by In-Context Learning?",
        "authors": [
            "Ce Zheng",
            "Lei Li",
            "Qingxiu Dong",
            "Yuxuan Fan",
            "Zhiyong Wu",
            "Jingjing Xu",
            "Baobao Chang"
        ],
        "published": "2023",
        "summary": "Previous studies have shown that large language models (LLMs) like GPTs store massive factual knowledge in their parameters. However, the stored knowledge could be false or outdated. Traditional knowledge editing methods refine LLMs via fine-tuning on texts containing specific knowledge. However, with the increasing scales of LLMs, these gradient-based approaches bring large computation costs. The trend of model-as-a-service also makes it impossible to modify knowledge in black-box LMs. Inspired by in-context learning (ICL), a new paradigm based on demonstration contexts without parameter updating, we explore whether ICL can edit factual knowledge. To answer this question, we give a comprehensive empirical study of ICL strategies. Experiments show that in-context knowledge editing (IKE), without any gradient and parameter updating, achieves a competitive success rate compared to gradient-based methods on GPT-J (6B) but with much fewer side effects, including less over-editing on similar but unrelated facts and less knowledge forgetting on previously stored knowledge. We also apply the method to larger LMs with tens or hundreds of parameters like OPT-175B, which shows the scalability of our method. The code is available at https://github.com/pkunlp-icler/IKE.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.296.pdf",
        "keywords": [
            "knowledge editing",
            "edit factual knowledge",
            "context knowledge editing",
            "context learning",
            "parameter updating",
            "demonstration contexts",
            "language"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the stored knowledge could be false or outdated.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, the stored knowledge could be false or outdated.\""
    },
    {
        "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
        "authors": [
            "Joshua Ainslie",
            "James Lee-Thorp",
            "Michiel de Jong",
            "Yury Zemlyanskiy",
            "Federico Lebron",
            "Sumit Sanghai"
        ],
        "published": "2023",
        "summary": "Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.298.pdf",
        "keywords": [
            "multi query attention",
            "grouped query attention",
            "multi head checkpoints"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"MQA can lead to quality degradation\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"MQA can lead to quality degradation\""
    },
    {
        "title": "Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models",
        "authors": [
            "Yifan Hou",
            "Jiaoda Li",
            "Yu Fei",
            "Alessandro Stolfo",
            "Wangchunshu Zhou",
            "Guangtao Zeng",
            "Antoine Bosselut",
            "Mrinmaya Sachan"
        ],
        "published": "2023",
        "summary": "Recent work has shown that language models (LMs) have strong multi-step (i.e., procedural) reasoning capabilities. However, it is unclear whether LMs perform these tasks by cheating with answers memorized from pretraining corpus, or, via a multi-step reasoning mechanism. In this paper, we try to answer this question by exploring a mechanistic interpretation of LMs for multi-step reasoning tasks. Concretely, we hypothesize that the LM implicitly embeds a reasoning tree resembling the correct reasoning process within it. We test this hypothesis by introducing a new probing approach (called MechanisticProbe) that recovers the reasoning tree from the model’s attention patterns. We use our probe to analyze two LMs: GPT-2 on a synthetic task (k-th smallest element), and LLaMA on two simple language-based reasoning tasks (ProofWriter & AI2 Reasoning Challenge). We show that MechanisticProbe is able to detect the information of the reasoning tree from the model’s attentions for most examples, suggesting that the LM indeed is going through a process of multi-step reasoning within its architecture in many cases.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.299.pdf",
        "keywords": [
            "language models",
            "mechanistic interpretation",
            "reasoning",
            "multi step reasoning",
            "multi step"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it is unclear whether LMs perform these tasks by cheating with answers memorized from pretraining corpus, or, via a multi-step reasoning mechanism.\"\n\nThis rating is chosen because the abstract mentions a limitation of LMs (the uncertainty about how they perform multi-step reasoning tasks) but does not elaborate on it and focuses on the proposed solution (MechanisticProbe) to address",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, it is unclear whether LMs perform these tasks by cheating with answers memorized from pretraining corpus, or, via a multi-step reasoning mechanism.\"\n\nThis rating is chosen because the abstract mentions a limitation of LMs (the uncertainty about how they perform multi-step reasoning tasks) but does not elaborate on it and focuses on the proposed solution (MechanisticProbe) to address"
    },
    {
        "title": "Text encoders bottleneck compositionality in contrastive vision-language models",
        "authors": [
            "Amita Kamath",
            "Jack Hessel",
            "Kai-Wei Chang"
        ],
        "published": "2023",
        "summary": "Performant vision-language (VL) models like CLIP represent captions using a single vector. How much information about language is lost in this bottleneck? We first curate CompPrompts, a set of increasingly compositional image captions that VL models should be able to capture (e.g., single object, to object+property, to multiple interacting objects). Then, we train text-only recovery probes that aim to reconstruct captions from single-vector text representations produced by several VL models. This approach does not require images, allowing us to test on a broader range of scenes compared to prior work. We find that: 1) CLIP’s text encoder falls short on more compositional inputs, including object relationships, attribute-object association, counting, and negations; 2) some text encoders work significantly better than others; and 3) text-only recovery performance predicts multimodal matching performance on ControlledImCaps: a new evaluation benchmark we collect and release consisting of fine-grained compositional images and captions. Specifically, our results suggest text-only recoverability is a necessary (but not sufficient) condition for modeling compositional factors in contrastive VL models. We release our datasets and code.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.301.pdf",
        "keywords": [
            "bottleneck compositionality",
            "compositional images",
            "captions",
            "text encoders"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition",
        "authors": [
            "Sander Schulhoff",
            "Jeremy Pinto",
            "Anaum Khan",
            "Louis-François Bouchard",
            "Chenglei Si",
            "Svetlina Anati",
            "Valen Tagliabue",
            "Anson Kost",
            "Christopher Carnahan",
            "Jordan Boyd-Graber"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) are increasingly being deployed in interactive contexts that involve direct user engagement, such as chatbots and writing assistants. These deployments are increasingly plagued by prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and instead follow potentially malicious ones. Although widely acknowledged as a significant security threat, there is a dearth of a large-scale resource and quantitative study on prompt hacking. To address this lacuna, we launch a global prompt hacking competition, which allows for free-form human input attacks. We elicit 600K+ adversarial prompts against three state-of-the-art LLMs. We describe the dataset, which empirically verifies that current LLMs can indeed be manipulated via prompt hacking. We also present a comprehensive ontology of the types of adversarial prompts.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.302.pdf",
        "keywords": [
            "security threat",
            "language models",
            "ontology"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"These deployments are increasingly plagued by prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and instead follow potentially malicious ones.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"These deployments are increasingly plagued by prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and instead follow potentially malicious ones.\""
    },
    {
        "title": "Localizing Active Objects from Egocentric Vision with Symbolic World Knowledge",
        "authors": [
            "Te-Lin Wu",
            "Yu Zhou",
            "Nanyun Peng"
        ],
        "published": "2023",
        "summary": "The ability to actively ground task instructions from an egocentric view is crucial for AI agents to accomplish tasks or assist humans virtually. One important step towards this goal is to localize and track key active objects that undergo major state change as a consequence of human actions/interactions to the environment without being told exactly what/where to ground (e.g., localizing and tracking the ‘sponge‘ in video from the instruction “Dip the sponge into the bucket.”). While existing works approach this problem from a pure vision perspective, we investigate to which extent the textual modality (i.e., task instructions) and their interaction with visual modality can be beneficial. Specifically, we propose to improve phrase grounding models’ ability on localizing the active objects by: (1) learning the role of ‘objects undergoing change‘ and extracting them accurately from the instructions, (2) leveraging pre- and post-conditions of the objects during actions, and (3) recognizing the objects more robustly with descriptional knowledge. We leverage large language models (LLMs) to extract the aforementioned action-object knowledge, and design a per-object aggregation masking technique to effectively perform joint inference on object phrases and symbolic knowledge. We evaluate our framework on Ego4D and Epic-Kitchens datasets. Extensive experiments demonstrate the effectiveness of our proposed framework, which leads to>54% improvements in all standard metrics on the TREK-150-OPE-Det localization + tracking task, >7% improvements in all standard metrics on the TREK-150-OPE tracking task, and >3% improvements in average precision (AP) on the Ego4D SCOD task.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.304.pdf",
        "keywords": [
            "egocentric vision",
            "ego4d",
            "language models",
            "grounding",
            "localizing active",
            "’",
            "pure"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We leverage large language models (LLMs) to extract the aforementioned action-object knowledge\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"We leverage large language models (LLMs) to extract the aforementioned action-object knowledge\""
    },
    {
        "title": "Prompting is not a substitute for probability measurements in large language models",
        "authors": [
            "Jennifer Hu",
            "Roger Levy"
        ],
        "published": "2023",
        "summary": "Prompting is now a dominant method for evaluating the linguistic knowledge of large language models (LLMs). While other methods directly read out models’ probability distributions over strings, prompting requires models to access this internal information by processing linguistic input, thereby implicitly testing a new type of emergent ability: metalinguistic judgment. In this study, we compare metalinguistic prompting and direct probability measurements as ways of measuring models’ linguistic knowledge. Broadly, we find that LLMs’ metalinguistic judgments are inferior to quantities directly derived from representations. Furthermore, consistency gets worse as the prompt query diverges from direct measurements of next-word probabilities. Our findings suggest that negative results relying on metalinguistic prompts cannot be taken as conclusive evidence that an LLM lacks a particular linguistic generalization. Our results also highlight the value that is lost with the move to closed APIs where access to probability distributions is limited.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.306.pdf",
        "keywords": [
            "prompting",
            "language models",
            "metalinguistic prompts",
            "direct probability",
            "linguistic"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Broadly, we find that LLMs’ metalinguistic judgments are inferior to quantities directly derived from representations. Furthermore, consistency gets worse as the prompt query diverges from direct measurements of next-word probabilities. Our findings suggest that negative results relying on metalinguistic prompts cannot be taken as conclusive evidence that an LLM lacks a particular linguistic generalization.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Broadly, we find that LLMs’ metalinguistic judgments are inferior to quantities directly derived from representations. Furthermore, consistency gets worse as the prompt query diverges from direct measurements of next-word probabilities. Our findings suggest that negative results relying on metalinguistic prompts cannot be taken as conclusive evidence that an LLM lacks a particular linguistic generalization.\""
    },
    {
        "title": "Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings",
        "authors": [
            "Josip Jukić",
            "Jan Snajder"
        ],
        "published": "2023",
        "summary": "Pre-trained language models (PLMs) have ignited a surge in demand for effective fine-tuning techniques, particularly in low-resource domains and languages. Active learning (AL), a set of algorithms designed to decrease labeling costs by minimizing label complexity, has shown promise in confronting the labeling bottleneck. In parallel, adapter modules designed for parameter-efficient fine-tuning (PEFT) have demonstrated notable potential in low-resource settings. However, the interplay between AL and adapter-based PEFT remains unexplored. We present an empirical study of PEFT behavior with AL in low-resource settings for text classification tasks. Our findings affirm the superiority of PEFT over full-fine tuning (FFT) in low-resource settings and demonstrate that this advantage persists in AL setups. We further examine the properties of PEFT and FFT through the lens of forgetting dynamics and instance-level representations, where we find that PEFT yields more stable representations of early and middle layers compared to FFT. Our research underscores the synergistic potential of AL and PEFT in low-resource settings, paving the way for advancements in efficient and effective fine-tuning.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.307.pdf",
        "keywords": [
            "active learning",
            "adapter",
            "tuning",
            "model tuning",
            "parameter efficient fine tuning",
            "pre trained language models",
            "parameter efficient",
            "classification"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Pre-trained language models (PLMs) have ignited a surge in demand for effective fine-tuning techniques, particularly in low-resource domains and languages.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Pre-trained language models (PLMs) have ignited a surge in demand for effective fine-tuning techniques, particularly in low-resource domains and languages.\""
    },
    {
        "title": "Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks",
        "authors": [
            "Alon Jacovi",
            "Avi Caciularu",
            "Omer Goldman",
            "Yoav Goldberg"
        ],
        "published": "2023",
        "summary": "Data contamination has become prevalent and challenging with the rise of models pretrained on large automatically-crawled corpora. For closed models, the training data becomes a trade secret, and even for open models, it is not trivial to detect contamination. Strategies such as leaderboards with hidden answers, or using test data which is guaranteed to be unseen, are expensive and become fragile with time. Assuming that all relevant actors value clean test data and will cooperate to mitigate data contamination, what can be done? We propose three strategies that can make a difference: (1) Test data made public should be encrypted with a public key and licensed to disallow derivative distribution; (2) demand training exclusion controls from closed API holders, and protect your test data by refusing to evaluate without them; (3) avoid data which appears with its solution on the internet, and release the web-page context of internet-derived data along with the data. These strategies are practical and can be effective in preventing data contamination.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.308.pdf",
        "keywords": [
            "data contamination",
            "evaluation benchmarks",
            "uploading test",
            "crawled corpora"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Data contamination has become prevalent and challenging with the rise of models pretrained on large automatically-crawled corpora.\"\n\nThis evidence suggests that the paper discusses a limitation of large language models, specifically the challenge of data contamination, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Data contamination has become prevalent and challenging with the rise of models pretrained on large automatically-crawled corpora.\"\n\nThis evidence suggests that the paper discusses a limitation of large language models, specifically the challenge of data contamination, but it is not the primary focus of the paper."
    },
    {
        "title": "DiSTRICT: Dialogue State Tracking with Retriever Driven In-Context Tuning",
        "authors": [
            "Praveen Venkateswaran",
            "Evelyn Duesterwald",
            "Vatche Isahagian"
        ],
        "published": "2023",
        "summary": "Dialogue State Tracking (DST), a key component of task-oriented conversation systems, represents user intentions by determining the values of pre-defined slots in an ongoing dialogue. Existing approaches use hand-crafted templates and additional slot information to fine-tune and prompt large pre-trained language models and elicit slot values from the dialogue context. Significant manual effort and domain knowledge is required to design effective prompts, limiting the generalizability of these approaches to new domains and tasks. In this work, we propose DiSTRICT, a generalizable in-context tuning approach for DST that retrieves highly relevant training examples for a given dialogue to fine-tune the model without any hand-crafted templates. Experiments with the MultiWOZ benchmark datasets show that DiSTRICT outperforms existing approaches in various zero-shot and few-shot settings using a much smaller model, thereby providing an important advantage for real-world deployments that often have limited resource availability.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.310.pdf",
        "keywords": [
            "context tuning",
            "dialogue state tracking"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Significant manual effort and domain knowledge is required to design effective prompts, limiting the generalizability of these approaches to new domains and tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Significant manual effort and domain knowledge is required to design effective prompts, limiting the generalizability of these approaches to new domains and tasks.\""
    },
    {
        "title": "LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers",
        "authors": [
            "Theo Olausson",
            "Alex Gu",
            "Ben Lipkin",
            "Cedegao Zhang",
            "Armando Solar-Lezama",
            "Joshua Tenenbaum",
            "Roger Levy"
        ],
        "published": "2023",
        "summary": "Logical reasoning, i.e., deductively inferring the truth value of a conclusion from a set of premises, is an important task for artificial intelligence with wide potential impacts on science, mathematics, and society. While many prompting-based strategies have been proposed to enable Large Language Models (LLMs) to do such reasoning more effectively, they still appear unsatisfactory, often failing in subtle and unpredictable ways. In this work, we investigate the validity of instead reformulating such tasks as modular neurosymbolic programming, which we call LINC: Logical Inference via Neurosymbolic Computation. In LINC, the LLM acts as a semantic parser, translating premises and conclusions from natural language to expressions in first-order logic. These expressions are then offloaded to an external theorem prover, which symbolically performs deductive inference. Leveraging this approach, we observe significant performance gains on FOLIO and a balanced subset of ProofWriter for three different models in nearly all experimental conditions we evaluate. On ProofWriter, augmenting the comparatively small open-source StarCoder+ (15.5B parameters) with LINC even outperforms GPT-3.5 and GPT-4 with Chain-of-Thought (CoT) prompting by an absolute 38% and 10%, respectively. When used with GPT-4, LINC scores 26% higher than CoT on ProofWriter while performing comparatively on FOLIO. Further analysis reveals that although both methods on average succeed roughly equally often on this dataset, they exhibit distinct and complementary failure modes. We thus provide promising evidence for how logical reasoning over natural language can be tackled through jointly leveraging LLMs alongside symbolic provers. All corresponding code is publicly available.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.313.pdf",
        "keywords": [
            "logical reasoning",
            "neurosymbolic",
            "first order logic",
            "first order logic provers",
            "logical inference",
            "modular neurosymbolic programming",
            "theorem prover",
            "language models",
            "neurosymbolic computation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While many prompting-based strategies have been proposed to enable Large Language Models (LLMs) to do such reasoning more effectively, they still appear unsatisfactory, often failing in subtle and unpredictable ways.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While many prompting-based strategies have been proposed to enable Large Language Models (LLMs) to do such reasoning more effectively, they still appear unsatisfactory, often failing in subtle and unpredictable ways.\""
    },
    {
        "title": "ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing",
        "authors": [
            "Nam Nguyen",
            "Thang Phan",
            "Duc-Vu Nguyen",
            "Kiet Nguyen"
        ],
        "published": "2023",
        "summary": "English and Chinese, known as resource-rich languages, have witnessed the strong development of transformer-based language models for natural language processing tasks. Although Vietnam has approximately 100M people speaking Vietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELECTRA, performed well on general Vietnamese NLP tasks, including POS tagging and named entity recognition. These pre-trained language models are still limited to Vietnamese social media tasks. In this paper, we present the first monolingual pre-trained language model for Vietnamese social media texts, ViSoBERT, which is pre-trained on a large-scale corpus of high-quality and diverse Vietnamese social media texts using XLM-R architecture. Moreover, we explored our pre-trained model on five important natural language downstream tasks on Vietnamese social media texts: emotion recognition, hate speech detection, sentiment analysis, spam reviews detection, and hate speech spans detection. Our experiments demonstrate that ViSoBERT, with far fewer parameters, surpasses the previous state-of-the-art models on multiple Vietnamese social media tasks. Our ViSoBERT model is available only for research purposes. Disclaimer: This paper contains actual comments on social networks that might be construed as abusive, offensive, or obscene.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.315.pdf",
        "keywords": [
            "natural language",
            "vietnamese social media",
            "vietnamese",
            "vietnamese social media texts",
            "natural language processing",
            "social networks",
            "named entity recognition",
            "emotion recognition",
            "language model",
            "model",
            "trained models",
            "spam reviews detection",
            "tagging",
            "sentiment analysis"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"These pre-trained language models are still limited to Vietnamese social media tasks.\"\n\nThis paper mentions a limitation of pre-trained language models in passing, specifically their limitations in Vietnamese social media tasks.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"These pre-trained language models are still limited to Vietnamese social media tasks.\"\n\nThis paper mentions a limitation of pre-trained language models in passing, specifically their limitations in Vietnamese social media tasks."
    },
    {
        "title": "GeoLM: Empowering Language Models for Geospatially Grounded Language Understanding",
        "authors": [
            "Zekun Li",
            "Wenxuan Zhou",
            "Yao-Yi Chiang",
            "Muhao Chen"
        ],
        "published": "2023",
        "summary": "Humans subconsciously engage in geospatial reasoning when reading articles. We recognize place names and their spatial relations in text and mentally associate them with their physical locations on Earth. Although pretrained language models can mimic this cognitive process using linguistic context, they do not utilize valuable geospatial information in large, widely available geographical databases, e.g., OpenStreetMap. This paper introduces GeoLM, a geospatially grounded language model that enhances the understanding of geo-entities in natural language. GeoLM leverages geo-entity mentions as anchors to connect linguistic information in text corpora with geospatial information extracted from geographical databases. GeoLM connects the two types of context through contrastive learning and masked language modeling. It also incorporates a spatial coordinate embedding mechanism to encode distance and direction relations to capture geospatial context. In the experiment, we demonstrate that GeoLM exhibits promising capabilities in supporting toponym recognition, toponym linking, relation extraction, and geo-entity typing, which bridge the gap between natural language processing and geospatial sciences. The code is publicly available at https://github.com/knowledge-computing/geolm.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.317.pdf",
        "keywords": [
            "grounded language",
            "grounded language model",
            "language models",
            "processing"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although pretrained language models can mimic this cognitive process using linguistic context, they do not utilize valuable geospatial information in large, widely available geographical databases, e.g., OpenStreetMap.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (not utilizing geospatial information) in passing, but the primary focus is on the proposed solution (GeoLM) rather",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although pretrained language models can mimic this cognitive process using linguistic context, they do not utilize valuable geospatial information in large, widely available geographical databases, e.g., OpenStreetMap.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (not utilizing geospatial information) in passing, but the primary focus is on the proposed solution (GeoLM) rather"
    },
    {
        "title": "LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models",
        "authors": [
            "Zhiqiang Hu",
            "Lei Wang",
            "Yihuai Lan",
            "Wanyu Xu",
            "Ee-Peng Lim",
            "Lidong Bing",
            "Xing Xu",
            "Soujanya Poria",
            "Roy Lee"
        ],
        "published": "2023",
        "summary": "The success of large language models (LLMs), like GPT-4 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire LLMs while achieving comparable or even better performance. To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks. The framework includes state-of-the-art open-access LLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such as Series adapters, Parallel adapter, Prompt-based learning and Reparametrization-based methods. Moreover, we conduct extensive empirical studies on the impact of adapter types, placement locations, and hyper-parameters to the best design for each adapter-based methods. We evaluate the effectiveness of the adapters on fourteen datasets from two different reasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results demonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few extra trainable parameters yields comparable, and in some cases superior, performance to powerful LLMs (175B) in zero-shot inference on simple math reasoning datasets.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.319.pdf",
        "keywords": [
            "adapters",
            "tuning",
            "fine tuning",
            "series adapters",
            "parameter efficient fine tuning",
            "parameters",
            "finetuning",
            "large language models",
            "llm adapters"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the title and paper imply that the method is proposed to address the cost-ineffectiveness of fine-tuning large language models, which can be seen as a limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the title and paper imply that the method is proposed to address the cost-ineffectiveness of fine-tuning large language models, which can be seen as a limitation."
    },
    {
        "title": "Query Rewriting in Retrieval-Augmented Large Language Models",
        "authors": [
            "Xinbei Ma",
            "Yeyun Gong",
            "Pengcheng He",
            "Hai Zhao",
            "Nan Duan"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) play powerful, black-box readers in the retrieve-then-read pipeline, making remarkable progress in knowledge-intensive tasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of the previous retrieve-then-read for the retrieval-augmented LLMs from the perspective of the query rewriting. Unlike prior studies focusing on adapting either the retriever or the reader, our approach pays attention to the adaptation of the search query itself, for there is inevitably a gap between the input text and the needed knowledge in retrieval. We first prompt an LLM to generate the query, then use a web search engine to retrieve contexts. Furthermore, to better align the query to the frozen modules, we propose a trainable scheme for our pipeline. A small language model is adopted as a trainable rewriter to cater to the black-box LLM reader. The rewriter is trained using the feedback of the LLM reader by reinforcement learning. Evaluation is conducted on downstream tasks, open-domain QA and multiple-choice QA. Experiments results show consistent performance improvement, indicating that our framework is proven effective and scalable, and brings a new framework for retrieval-augmented LLM.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.322.pdf",
        "keywords": [
            "query rewriting",
            "rewriter",
            "augmented large language models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"there is inevitably a gap between the input text and the needed knowledge in retrieval.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"there is inevitably a gap between the input text and the needed knowledge in retrieval.\""
    },
    {
        "title": "PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation",
        "authors": [
            "Gaurav Sahu",
            "Olga Vechtomova",
            "Dzmitry Bahdanau",
            "Issam Laradji"
        ],
        "published": "2023",
        "summary": "Data augmentation is a widely used technique to address the problem of text classification when there is a limited amount of training data. Recent work often tackles this problem using large language models (LLMs) like GPT3 that can generate new examples given already available ones. In this work, we propose a method to generate more helpful augmented data by utilizing the LLM’s abilities to follow instructions and perform few-shot classifications. Our specific PromptMix method consists of two steps: 1) generate challenging text augmentations near class boundaries; however, generating borderline examples increases the risk of false positives in the dataset, so we 2) relabel the text augmentations using a prompting-based LLM classifier to enhance the correctness of labels in the generated data. We evaluate the proposed method in challenging 2-shot and zero-shot settings on four text classification datasets: Banking77, TREC6, Subjectivity (SUBJ), and Twitter Complaints. Our experiments show that generating and, crucially, relabeling borderline examples facilitates the transfer of knowledge of a massive LLM like GPT3.5-turbo into smaller and cheaper classifiers like DistilBERT-base and BERT-base. Furthermore, 2-shot PromptMix outperforms multiple 5-shot data augmentation methods on the four datasets. Our code is available at https://github.com/ServiceNow/PromptMix-EMNLP-2023.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.323.pdf",
        "keywords": [
            "data augmentation",
            "language model distillation",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"generating borderline examples increases the risk of false positives in the dataset\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"generating borderline examples increases the risk of false positives in the dataset\""
    },
    {
        "title": "QUDeval: The Evaluation of Questions Under Discussion Discourse Parsing",
        "authors": [
            "Yating Wu",
            "Ritika Mangla",
            "Greg Durrett",
            "Junyi Jessy Li"
        ],
        "published": "2023",
        "summary": "Questions Under Discussion (QUD) is a versatile linguistic framework in which discourse progresses as continuously asking questions and answering them. Automatic parsing of a discourse to produce a QUD structure thus entails a complex question generation task: given a document and an answer sentence, generate a question that satisfies linguistic constraints of QUD and can be grounded in an anchor sentence in prior context. These questions are known to be curiosity-driven and open-ended. This work introduces the first framework for the automatic evaluation of QUD parsing, instantiating the theoretical constraints of QUD in a concrete protocol. We present QUDeval, a dataset of fine-grained evaluation of 2,190 QUD questions generated from both fine-tuned systems and LLMs. Using QUDeval, we show that satisfying all constraints of QUD is still challenging for modern LLMs, and that existing evaluation metrics poorly approximate parser quality. Encouragingly, human-authored QUDs are scored highly by our human evaluators, suggesting that there is headroom for further progress on language modeling to improve both QUD parsing and QUD evaluation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.325.pdf",
        "keywords": [
            "automatic evaluation",
            "evaluation",
            "discussion discourse parsing"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Using QUDeval, we show that satisfying all constraints of QUD is still challenging for modern LLMs, and that existing evaluation metrics poorly approximate parser quality.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Using QUDeval, we show that satisfying all constraints of QUD is still challenging for modern LLMs, and that existing evaluation metrics poorly approximate parser quality.\""
    },
    {
        "title": "PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter",
        "authors": [
            "Haoyan Yang",
            "Zhitao Li",
            "Yong Zhang",
            "Jianzong Wang",
            "Ning Cheng",
            "Ming Li",
            "Jing Xiao"
        ],
        "published": "2023",
        "summary": "The Retrieval Question Answering (ReQA) task employs the retrieval-augmented framework, composed of a retriever and generator. The generators formulate the answer based on the documents retrieved by the retriever. Incorporating Large Language Models (LLMs) as generators is beneficial due to their advanced QA capabilities, but they are typically too large to be fine-tuned with budget constraints while some of them are only accessible via APIs. To tackle this issue and further improve ReQA performance, we propose a trainable Pluggable Reward-Driven Contextual Adapter (PRCA), keeping the generator as a black box. Positioned between the retriever and generator in a Pluggable manner, PRCA refines the retrieved information by operating in a token-autoregressive strategy via maximizing rewards of the reinforcement learning phase. Our experiments validate PRCA’s effectiveness in enhancing ReQA performance on three datasets by up to 20% improvement to fit black-box LLMs into existing frameworks, demonstrating its considerable potential in the LLMs era.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.326.pdf",
        "keywords": [
            "contextual adapter",
            "retrieval question answering",
            "large language models",
            "pluggable",
            "pluggable reward"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Incorporating Large Language Models (LLMs) as generators is beneficial due to their advanced QA capabilities, but they are typically too large to be fine-tuned with budget constraints while some of them are only accessible via APIs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Incorporating Large Language Models (LLMs) as generators is beneficial due to their advanced QA capabilities, but they are typically too large to be fine-tuned with budget constraints while some of them are only accessible via APIs.\""
    },
    {
        "title": "Exploring Chain of Thought Style Prompting for Text-to-SQL",
        "authors": [
            "Chang-Yu Tai",
            "Ziru Chen",
            "Tianshu Zhang",
            "Xiang Deng",
            "Huan Sun"
        ],
        "published": "2023",
        "summary": "In-context learning with large language models (LLMs) has recently caught increasing attention due to its superior few-shot performance on various tasks. However, its performance on text-to-SQL parsing still has much room for improvement. In this paper, we hypothesize that a crucial aspect of LLMs to improve for text-to-SQL parsing is their multi-step reasoning ability. Thus, we systematically study how to enhance LLMs’ reasoning ability through chain of thought (CoT) style prompting, including the original chain-of-thought prompting and least-to-most prompting. Our experiments demonstrate that iterative prompting as in least-to-most prompting may be unnecessary for text-to-SQL parsing, and using detailed reasoning steps tends to have more error propagation issues. Based on these findings, we propose a new CoT-style prompting method for text-to-SQL parsing. It brings 5.2 and 6.5 point absolute gains on the Spider development set and the Spider Realistic set, respectively, compared to the standard prompting method without reasoning steps; 2.4 and 1.5 point absolute gains, compared to the least-to-most prompting method.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.327.pdf",
        "keywords": [
            "prompting",
            "thought prompting",
            "text to sql",
            "text to sql parsing",
            "thought style prompting",
            "style prompting",
            "prompting method",
            "cot style prompting",
            "chain of thought",
            "language models",
            "iterative prompting",
            "context learning",
            "reasoning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, its performance on text-to-SQL parsing still has much room for improvement.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, its performance on text-to-SQL parsing still has much room for improvement.\""
    },
    {
        "title": "Harnessing Black-Box Control to Boost Commonsense in LM’s Generation",
        "authors": [
            "Yufei Tian",
            "Felix Zhang",
            "Nanyun Peng"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) such as GPT-3 have demonstrated a strong capability to generate coherent and contextually relevant text. However, amidst their successes, a crucial issue persists: their generated outputs still lack commonsense at times. Moreover, fine-tuning the entire LLM towards more commonsensical outputs is computationally expensive if not infeasible. In this paper, we present a computation-efficient framework that steers a frozen Pre-Trained Language Model (PTLM) towards more commonsensical generation (i.e., producing a plausible output that incorporates a list of concepts in a meaningful way). Specifically, we first construct a reference-free evaluator that assigns a sentence with a commonsensical score by grounding the sentence to a dynamic commonsense knowledge base from four different relational aspects. We then use the scorer as the oracle for commonsense knowledge, and extend the controllable generation method called NADO to train an auxiliary head that guides a fixed PTLM to better satisfy the oracle. We test our framework on a series of GPT-2-, Flan-T5-, and Alpaca-based language models (LMs) on two constrained concept-to-sentence benchmarks. Human evaluation results demonstrate that our method consistently leads to the most commonsensical outputs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.329.pdf",
        "keywords": [
            "commonsense",
            "black box control",
            "language models",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, amidst their successes, a crucial issue persists: their generated outputs still lack commonsense at times.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, amidst their successes, a crucial issue persists: their generated outputs still lack commonsense at times.\""
    },
    {
        "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
        "authors": [
            "Katherine Tian",
            "Eric Mitchell",
            "Allan Zhou",
            "Archit Sharma",
            "Rafael Rafailov",
            "Huaxiu Yao",
            "Chelsea Finn",
            "Christopher Manning"
        ],
        "published": "2023",
        "summary": "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model’s conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.330.pdf",
        "keywords": [
            "calibration",
            "calibrated confidence",
            "language",
            "language models",
            "trustworthy"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated.\""
    },
    {
        "title": "Representative Demonstration Selection for In-Context Learning with Two-Stage Determinantal Point Process",
        "authors": [
            "Zhao Yang",
            "Yuanzhe Zhang",
            "Dianbo Sui",
            "Cao Liu",
            "Jun Zhao",
            "Kang Liu"
        ],
        "published": "2023",
        "summary": "Although In-Context Learning has proven effective across a broad array of tasks, its efficiency is noticeably influenced by the selection of demonstrations. Existing methods tend to select different demonstrations for each test instance, which is time-consuming and poses limitations in practical scenarios. Therefore, this study aims to address the challenge of selecting a representative subset of in-context demonstrations that can effectively prompt different test instances in a specific task. We propose that this representative subset should be of high quality and diversity. Our empirical analyses confirm that demonstrations that meet these criteria can indeed bolster model performance. To satisfy these criteria, this paper further introduces a two-stage Determinantal Point Process (DPP) method designed to incorporate both quality and diversity in the process of demonstration selection, thereby obtaining representative in-context demonstrations. Through comprehensive experimentation, we have confirmed the efficacy of our proposed method, paving the way for more practical and effective In-Context Learning.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.331.pdf",
        "keywords": [
            "context learning"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models",
        "authors": [
            "Lovisa Hagström",
            "Denitsa Saynova",
            "Tobias Norlund",
            "Moa Johansson",
            "Richard Johansson"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions. For example, a model might supply the answer “Edinburgh” to “Anne Redpath passed away in X.” and “London” to “Anne Redpath’s life ended in X.” In this work, we identify potential causes of inconsistency and evaluate the effectiveness of two mitigation strategies: up-scaling and augmenting the LM with a passage retrieval database. Our results on the LLaMA and Atlas models show that both strategies reduce inconsistency but that retrieval augmentation is considerably more efficient. We further consider and disentangle the consistency contributions of different components of Atlas. For all LMs evaluated we find that syntactical form and task artifacts impact consistency. Taken together, our results provide a better understanding of the factors affecting the factual consistency of language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.332.pdf",
        "keywords": [
            "factual consistency",
            "factual knowledge",
            "lm",
            "language models",
            "up scaling",
            "large language models",
            "task artifacts impact",
            "syntactical form"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions.\""
    },
    {
        "title": "ViPE: Visualise Pretty-much Everything",
        "authors": [
            "Hassan Shahmohammadi",
            "Adhiraj Ghosh",
            "Hendrik Lensch"
        ],
        "published": "2023",
        "summary": "Figurative and non-literal expressions are profoundly integrated in human communication. Visualising such expressions allow us to convey our creative thoughts, and evoke nuanced emotions. Recent text-to-image models like Stable Diffusion, on the other hand, struggle to depict non-literal expressions. Recent works primarily deal with this issue by compiling humanly annotated datasets on a small scale, which not only demands specialized expertise but also proves highly inefficient. To address this issue, we introduce ViPE: Visualise Pretty-much Everything. ViPE offers a series of lightweight and robust language models that have been trained on a large-scale set of lyrics with noisy visual descriptions that represent their implicit meaning. The synthetic visual descriptions are generated by GPT3.5 relying on neither human annotations nor images. ViPE effectively expresses any arbitrary piece of text into a visualisable description, enabling meaningful and high-quality image generation. We provide compelling evidence that ViPE is more robust than GPT3.5 in synthesising visual elaborations. ViPE also exhibits an understanding of figurative expressions comparable to human experts, providing a powerful and open-source backbone to many downstream applications such as music video and caption generation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.333.pdf",
        "keywords": [
            "expressions",
            "non literal expressions",
            "lyrics",
            "pretty",
            "pretty much everything"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent text-to-image models like Stable Diffusion, on the other hand, struggle to depict non-literal expressions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Recent text-to-image models like Stable Diffusion, on the other hand, struggle to depict non-literal expressions.\""
    },
    {
        "title": "Semi-automatic Data Enhancement for Document-Level Relation Extraction with Distant Supervision from Large Language Models",
        "authors": [
            "Junpeng Li",
            "Zixia Jia",
            "Zilong Zheng"
        ],
        "published": "2023",
        "summary": "Document-level Relation Extraction (DocRE), which aims to extract relations from a long context, is a critical challenge in achieving fine-grained structural comprehension and generating interpretable document representations. Inspired by recent advances in in-context learning capabilities emergent from large language models (LLMs), such as ChatGPT, we aim to design an automated annotation method for DocRE with minimum human effort. Unfortunately, vanilla in-context learning is infeasible for DocRE due to the plenty of predefined fine-grained relation types and the uncontrolled generations of LLMs. To tackle this issue, we propose a method integrating an LLM and a natural language inference (NLI) module to generate relation triples, thereby augmenting document-level relation datasets. We demonstrate the effectiveness of our approach by introducing an enhanced dataset known as DocGNRE, which excels in re-annotating numerous long-tail relation types. We are confident that our method holds the potential for broader applications in domain-specific relation type definitions and offers tangible benefits in advancing generalized language semantic comprehension.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.334.pdf",
        "keywords": [
            "document level relation extraction",
            "large language models",
            "natural language inference"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unfortunately, vanilla in-context learning is infeasible for DocRE due to the plenty of predefined fine-grained relation types and the uncontrolled generations of LLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Unfortunately, vanilla in-context learning is infeasible for DocRE due to the plenty of predefined fine-grained relation types and the uncontrolled generations of LLMs.\""
    },
    {
        "title": "Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models",
        "authors": [
            "Kaitlyn Zhou",
            "Dan Jurafsky",
            "Tatsunori Hashimoto"
        ],
        "published": "2023",
        "summary": "The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs. Here, we study an aspect of model epistemology: how epistemic markers of certainty, uncertainty, or evidentiality like “I’m sure it’s”, “I think it’s”, or “Wikipedia says it’s” affect models, and whether they contribute to model failures. We develop a typology of epistemic markers and inject 50 markers into prompts for question answering. We find that LMs are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80%. Surprisingly, we find that expressions of high certainty result in a 7% decrease in accuracy as compared to low certainty expressions; similarly, factive verbs hurt performance, while evidentials benefit performance. Our analysis of a popular pretraining dataset shows that these markers of uncertainty are associated with answers on question-answering websites, while markers of certainty are associated with questions. These associations may suggest that the behavior of LMs is based on mimicking observed language use, rather than truly reflecting epistemic uncertainty.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.335.pdf",
        "keywords": [
            "epistemic markers",
            "language models",
            "overconfidence",
            "certainty expressions",
            "grey area",
            "uncertainty",
            "epistemology"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Surprisingly, we find that expressions of high certainty result in a 7% decrease in accuracy as compared to low certainty expressions; similarly, factive verbs hurt performance, while evidentials benefit performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Surprisingly, we find that expressions of high certainty result in a 7% decrease in accuracy as compared to low certainty expressions; similarly, factive verbs hurt performance, while evidentials benefit performance.\""
    },
    {
        "title": "EntSUMv2: Dataset, Models and Evaluation for More Abstractive Entity-Centric Summarization",
        "authors": [
            "Dhruv Mehra",
            "Lingjue Xie",
            "Ella Hofmann-Coyle",
            "Mayank Kulkarni",
            "Daniel Preotiuc-Pietro"
        ],
        "published": "2023",
        "summary": "Entity-centric summarization is a form of controllable summarization that aims to generate a summary for a specific entity given a document. Concise summaries are valuable in various real-life applications, as they enable users to quickly grasp the main points of the document focusing on an entity of interest. This paper presents ENTSUMV2, a more abstractive version of the original entity-centric ENTSUM summarization dataset. In ENTSUMV2 the annotated summaries are intentionally made shorter to benefit more specific and useful entity-centric summaries for downstream users. We conduct extensive experiments on this dataset using multiple abstractive summarization approaches that employ supervised fine-tuning or large-scale instruction tuning. Additionally, we perform comprehensive human evaluation that incorporates metrics for measuring crucial facets. These metrics provide a more fine-grained interpretation of the current state-of-the-art systems and highlight areas for future improvement.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.337.pdf",
        "keywords": [
            "summarization",
            "evaluation",
            "human evaluation",
            "entity centric summarization",
            "entsumv2",
            "entsum summarization dataset",
            "models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"highlight areas for future improvement.\"\n\nThis abstract mentions LLMs indirectly through the discussion of large-scale instruction tuning, and it mentions limitations of current state-of-the-art systems in passing, but does not elaborate on these limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"highlight areas for future improvement.\"\n\nThis abstract mentions LLMs indirectly through the discussion of large-scale instruction tuning, and it mentions limitations of current state-of-the-art systems in passing, but does not elaborate on these limitations."
    },
    {
        "title": "Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents",
        "authors": [
            "Hyungjoo Chae",
            "Yongho Song",
            "Kai Ong",
            "Taeyoon Kwon",
            "Minjin Kim",
            "Youngjae Yu",
            "Dongha Lee",
            "Dongyeop Kang",
            "Jinyoung Yeo"
        ],
        "published": "2023",
        "summary": "Human-like chatbots necessitate the use of commonsense reasoning in order to effectively comprehend and respond to implicit information present within conversations. Achieving such coherence and informativeness in responses, however, is a non-trivial task. Even for large language models (LLMs), the task of identifying and aggregating key evidence within a single hop presents a substantial challenge. This complexity arises because such evidence is scattered across multiple turns in a conversation, thus necessitating integration over multiple hops. Hence, our focus is to facilitate such multi-hop reasoning over a dialogue context, namely dialogue chain-of-thought (CoT) reasoning. To this end, we propose a knowledge distillation framework that leverages LLMs as unreliable teachers and selectively distills consistent and helpful rationales via alignment filters. We further present DOCTOR, a DialOgue Chain-of-ThOught Reasoner that provides reliable CoT rationales for response generation. We conduct extensive experiments to show that enhancing dialogue agents with high-quality rationales from DOCTOR significantly improves the quality of their responses.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.342.pdf",
        "keywords": [
            "commonsense",
            "knowledge distillation",
            "dialogue agents",
            "thought distillation",
            "dialogue chain",
            "dialogue chain of thought",
            "commonsense reasoning",
            "thought reasoner",
            "conversational agents"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Even for large language models (LLMs), the task of identifying and aggregating key evidence within a single hop presents a substantial challenge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Even for large language models (LLMs), the task of identifying and aggregating key evidence within a single hop presents a substantial challenge.\""
    },
    {
        "title": "C-STS: Conditional Semantic Textual Similarity",
        "authors": [
            "Ameet Deshpande",
            "Carlos Jimenez",
            "Howard Chen",
            "Vishvak Murahari",
            "Victoria Graf",
            "Tanmay Rajpurohit",
            "Ashwin Kalyan",
            "Danqi Chen",
            "Karthik Narasimhan"
        ],
        "published": "2023",
        "summary": "Semantic textual similarity (STS) has been a cornerstone task in NLP that measures the degree of similarity between a pair of sentences, with applications in information retrieval, question answering, and embedding methods. However, it is an inherently ambiguous task, with the sentence similarity depending on the specific aspect of interest. We resolve this ambiguity by proposing a novel task called conditional STS (C-STS) which measures similarity conditioned on an aspect elucidated in natural language (hereon, condition). As an example, the similarity between the sentences “The NBA player shoots a three-pointer.” and “A man throws a tennis ball into the air to serve.” is higher for the condition “The motion of the ball.” (both upward) and lower for “The size of the ball.” (one large and one small). C-STS’s advantages are two-fold: (1) it reduces the subjectivity and ambiguity of STS, and (2) enables fine-grained similarity evaluation using diverse conditions. C-STS contains almost 20,000 instances from diverse domains and we evaluate several state-of-the-art models to demonstrate that even the most performant fine-tuning and in-context learning models (GPT-4, Flan, SimCSE) find it challenging, with Spearman correlation scores of <50. We encourage the community to evaluate their models on C-STS to provide a more holistic view of semantic similarity and natural language understanding.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.345.pdf",
        "keywords": [
            "similarity",
            "semantic textual similarity",
            "semantic similarity",
            "language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"even the most performant fine-tuning and in-context learning models (GPT-4, Flan, SimCSE) find it challenging, with Spearman correlation scores of <50.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"even the most performant fine-tuning and in-context learning models (GPT-4, Flan, SimCSE) find it challenging, with Spearman correlation scores of <50.\""
    },
    {
        "title": "Rumor Detection on Social Media with Crowd Intelligence and ChatGPT-Assisted Networks",
        "authors": [
            "Chang Yang",
            "Peng Zhang",
            "Wenbo Qiao",
            "Hui Gao",
            "Jiaming Zhao"
        ],
        "published": "2023",
        "summary": "In the era of widespread dissemination through social media, the task of rumor detection plays a pivotal role in establishing a trustworthy and reliable information environment. Nonetheless, existing research on rumor detection confronts several challenges: the limited expressive power of text encoding sequences, difficulties in domain knowledge coverage and effective information extraction with knowledge graph-based methods, and insufficient mining of semantic structural information. To address these issues, we propose a Crowd Intelligence and ChatGPT-Assisted Network(CICAN) for rumor classification. Specifically, we present a crowd intelligence-based semantic feature learning module to capture textual content’s sequential and hierarchical features. Then, we design a knowledge-based semantic structural mining module that leverages ChatGPT for knowledge enhancement. Finally, we construct an entity-sentence heterogeneous graph and design Entity-Aware Heterogeneous Attention to effectively integrate diverse structural information meta-paths. Experimental results demonstrate that CICAN achieves performance improvement in rumor detection tasks, validating the effectiveness and rationality of using large language models as auxiliary tools.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.347.pdf",
        "keywords": [
            "crowd intelligence",
            "rumor detection",
            "chatgpt",
            "social media",
            "chatgpt assisted networks",
            "rumor classification"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"validating the effectiveness and rationality of using large language models as auxiliary tools.\"\n\nThis paper discusses LLMs but does not explicitly mention any limitations of the models in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"validating the effectiveness and rationality of using large language models as auxiliary tools.\"\n\nThis paper discusses LLMs but does not explicitly mention any limitations of the models in the abstract."
    },
    {
        "title": "Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?",
        "authors": [
            "Yichi Zhang",
            "Jiayi Pan",
            "Yuchen Zhou",
            "Rui Pan",
            "Joyce Chai"
        ],
        "published": "2023",
        "summary": "Vision-Language Models (VLMs) are trained on vast amounts of data captured by humans emulating our understanding of the world. However, known as visual illusions, human’s perception of reality isn’t always faithful to the physical world. This raises a key question: do VLMs have the similar kind of illusions as humans do, or do they faithfully learn to represent reality? To investigate this question, we build a dataset containing five types of visual illusions and formulate four tasks to examine visual illusions in state-of-the-art VLMs. Our findings have shown that although the overall alignment is low, larger models are closer to human perception and more susceptible to visual illusions. Our dataset and initial findings will promote a better understanding of visual illusions in humans and machines and provide a stepping stone for future computational models that can better align humans and machines in perceiving and communicating about the shared visual world. The code and data are available at [github.com/vl-illusion/dataset](https://github.com/vl-illusion/dataset).",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.348.pdf",
        "keywords": [
            "models",
            "computational models",
            "vision language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, known as visual illusions, human’s perception of reality isn’t always faithful to the physical world.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, known as visual illusions, human’s perception of reality isn’t always faithful to the physical world.\""
    },
    {
        "title": "HyperRouter: Towards Efficient Training and Inference of Sparse Mixture of Experts",
        "authors": [
            "Truong Giang Do",
            "Le Khiem",
            "Quang Pham",
            "TrungTin Nguyen",
            "Thanh-Nam Doan",
            "Binh Nguyen",
            "Chenghao Liu",
            "Savitha Ramasamy",
            "Xiaoli Li",
            "Steven Hoi"
        ],
        "published": "2023",
        "summary": "By routing input tokens to only a few split experts, Sparse Mixture-of-Experts has enabled efficient training of large language models. Recent findings suggest that fixing the routers can achieve competitive performance by alleviating the collapsing problem, where all experts eventually learn similar representations. However, this strategy has two key limitations: (i) the policy derived from random routers might be sub-optimal, and (ii) it requires extensive resources during training and evaluation, leading to limited efficiency gains. This work introduces HyperRouter, which dynamically generates the router’s parameters through a fixed hypernetwork and trainable embeddings to achieve a balance between training the routers and freezing them to learn an improved routing policy. Extensive experiments across a wide range of tasks demonstrate the superior performance and efficiency gains of HyperRouter compared to existing routing methods. Our implementation is publicly available at https://github.com/giangdip2410/HyperRouter.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.351.pdf",
        "keywords": [
            "mixture of experts",
            "sparse mixture"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, this strategy has two key limitations: (i) the policy derived from random routers might be sub-optimal, and (ii) it requires extensive resources during training and evaluation, leading to limited efficiency gains.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, this strategy has two key limitations: (i) the policy derived from random routers might be sub-optimal, and (ii) it requires extensive resources during training and evaluation, leading to limited efficiency gains.\""
    },
    {
        "title": "Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata",
        "authors": [
            "Silei Xu",
            "Shicheng Liu",
            "Theo Culhane",
            "Elizaveta Pertseva",
            "Meng-Hsi Wu",
            "Sina Semnani",
            "Monica Lam"
        ],
        "published": "2023",
        "summary": "While large language models (LLMs) can answer many questions correctly, they can also hallucinate and give wrong answers. Wikidata, with its over 12 billion facts, can be used to ground LLMs to improve their factuality. This paper presents WikiWebQuestions, a high-quality question answering benchmark for Wikidata. Ported over from WebQuestions for Freebase, it consists of real-world data with SPARQL annotation. This paper presents a few-shot sequence-to-sequence semantic parser for Wikidata. We modify SPARQL to use the unique domain and property names instead of their IDs. We train the parser to use either the results from an entity linker or mentions in the query. We fine-tune LLaMA by adding the few-shot training data to that used to fine-tune Alpaca. Our experimental results demonstrate the effectiveness of this methodology, establishing a strong baseline of 76% and 65% answer accuracy in the dev and test sets of WikiWebQuestions, respectively. By pairing our semantic parser with GPT-3, we combine verifiable results with qualified GPT-3 guesses to provide useful answers to 96% of the questions in dev. We also show that our method outperforms the state-of-the-art for the QALD-7 Wikidata dataset by 3.6% in F1 score.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.353.pdf",
        "keywords": [
            "wikidata",
            "hallucinate",
            "sparql",
            "sparql annotation",
            "semantic parsing",
            "tuned"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While large language models (LLMs) can answer many questions correctly, they can also hallucinate and give wrong answers.\"\n\nThis paper briefly mentions a limitation of LLMs (hallucination) but does not explore it in depth, focusing instead on a solution to improve their factuality.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While large language models (LLMs) can answer many questions correctly, they can also hallucinate and give wrong answers.\"\n\nThis paper briefly mentions a limitation of LLMs (hallucination) but does not explore it in depth, focusing instead on a solution to improve their factuality."
    },
    {
        "title": "ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models",
        "authors": [
            "Dheeraj Mekala",
            "Jason Wolfe",
            "Subhro Roy"
        ],
        "published": "2023",
        "summary": "We explore the use of large language models (LLMs) for zero-shot semantic parsing. Semantic parsing involves mapping natural language utterances to task-specific meaning representations. LLMs are generally trained on publicly available text and code and cannot be expected to directly generalize to domain-specific parsing tasks in a zero-shot setting. In this work, we propose ZEROTOP, a zero-shot task-oriented parsing method that decomposes semantic parsing problem into a set of abstractive and extractive question-answering (QA) problems. For each utterance, we prompt the LLM with questions corresponding to its top-level intent and a set of slots and use the LLM generations to construct the target meaning representation. We observe that current LLMs fail to detect unanswerable questions; and as a result, cannot handle questions corresponding to missing slots. We address this by fine-tuning a language model on public QA datasets using synthetic negative samples. Experimental results show that our QA-based decomposition paired with the fine-tuned LLM can zero-shot parse ≈ 16% of utterances in the MTOP dataset.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.354.pdf",
        "keywords": [
            "semantic parsing",
            "large language models",
            "negative samples",
            "zero shot semantic parsing",
            "extractive question answering"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"LLMs are generally trained on publicly available text and code and cannot be expected to directly generalize to domain-specific parsing tasks in a zero-shot setting.\"; \"We observe that current LLMs fail to detect unanswerable questions; and as a result, cannot handle questions corresponding to missing slots.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"LLMs are generally trained on publicly available text and code and cannot be expected to directly generalize to domain-specific parsing tasks in a zero-shot setting.\"; \"We observe that current LLMs fail to detect unanswerable questions; and as a result, cannot handle questions corresponding to missing slots.\""
    },
    {
        "title": "The BLA Benchmark: Investigating Basic Language Abilities of Pre-Trained Multimodal Models",
        "authors": [
            "Xinyi Chen",
            "Raquel Fernández",
            "Sandro Pezzelle"
        ],
        "published": "2023",
        "summary": "Despite the impressive performance achieved by pre-trained language-and-vision models in downstream tasks, it remains an open question whether this reflects a proper understanding of image-text interaction. In this work, we explore to what extent they handle basic linguistic constructions—active-passive voice, coordination, and relative clauses—that even preschool children can typically master. We present BLA, a novel, automatically constructed benchmark to evaluate multimodal models on these Basic Language Abilities. We show that different types of Transformer-based systems, such as CLIP, ViLBERT, and BLIP2, generally struggle with BLA in a zero-shot setting, in line with previous findings. Our experiments, in particular, show that most of the tested models only marginally benefit when fine-tuned or prompted with construction-specific samples. Yet, the generative BLIP2 shows promising trends, especially in an in-context learning setting. This opens the door to using BLA not only as an evaluation benchmark but also to improve models’ basic language abilities.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.356.pdf",
        "keywords": [
            "bla",
            "multimodal",
            "bla benchmark",
            "benchmark",
            "evaluation benchmark",
            "preschool children",
            "basic language abilities",
            "multimodal models",
            "voice"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We show that different types of Transformer-based systems, such as CLIP, ViLBERT, and BLIP2, generally struggle with BLA in a zero-shot setting, in line with previous findings.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We show that different types of Transformer-based systems, such as CLIP, ViLBERT, and BLIP2, generally struggle with BLA in a zero-shot setting, in line with previous findings.\""
    },
    {
        "title": "Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings",
        "authors": [
            "Qian Chen",
            "Wen Wang",
            "Qinglin Zhang",
            "Siqi Zheng",
            "Chong Deng",
            "Hai Yu",
            "Jiaqing Liu",
            "Yukun Ma",
            "Chong Zhang"
        ],
        "published": "2023",
        "summary": "Prior studies diagnose the anisotropy problem in sentence representations from pre-trained language models, e.g., BERT, without fine-tuning. Our analysis reveals that the sentence embeddings from BERT suffer from a bias towards uninformative words, limiting the performance in semantic textual similarity (STS) tasks. To address this bias, we propose a simple and efficient unsupervised approach, Diagonal Attention Pooling (Ditto), which weights words with model-based importance estimations and computes the weighted average of word representations from pre-trained models as sentence embeddings. Ditto can be easily applied to any pre-trained language model as a postprocessing operation. Compared to prior sentence embedding approaches, Ditto does not add parameters nor requires any learning. Empirical evaluations demonstrate that our proposed Ditto can alleviate the anisotropy problem and improve various pre-trained models on the STS benchmarks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.359.pdf",
        "keywords": [
            "sentence embeddings"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Prior studies diagnose the anisotropy problem in sentence representations from pre-trained language models, e.g., BERT, without fine-tuning. Our analysis reveals that the sentence embeddings from BERT suffer from a bias towards uninformative words, limiting the performance in semantic textual similarity (STS) tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Prior studies diagnose the anisotropy problem in sentence representations from pre-trained language models, e.g., BERT, without fine-tuning. Our analysis reveals that the sentence embeddings from BERT suffer from a bias towards uninformative words, limiting the performance in semantic textual similarity (STS) tasks.\""
    },
    {
        "title": "Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction",
        "authors": [
            "Ji Qi",
            "Chuchun Zhang",
            "Xiaozhi Wang",
            "Kaisheng Zeng",
            "Jifan Yu",
            "Jinxin Liu",
            "Lei Hou",
            "Juanzi Li",
            "Xu Bin"
        ],
        "published": "2023",
        "summary": "The robustness to distribution changes ensures that NLP models can be successfully applied in the realistic world, especially for information extraction tasks. However, most prior evaluation benchmarks have been devoted to validating pairwise matching correctness, ignoring the crucial validation of robustness. In this paper, we present the first benchmark that simulates the evaluation of open information extraction models in the real world, where the syntactic and expressive distributions under the same knowledge meaning may drift variously. We design and annotate a large-scale testbed in which each example is a knowledge-invariant clique that consists of sentences with structured knowledge of the same meaning but with different syntactic and expressive forms. By further elaborating the robustness metric, a model is judged to be robust if its performance is consistently accurate on the overall cliques. We perform experiments on typical models published in the last decade as well as a representative large language model, and the results show that the existing successful models exhibit a frustrating degradation, with a maximum drop of 23.43 F1 score. Our resources and code will be publicly available.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.360.pdf",
        "keywords": [
            "knowledge invariance",
            "robustness",
            "robustness evaluation",
            "information extraction",
            "benchmark",
            "open information extraction"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We perform experiments on typical models published in the last decade as well as a representative large language model, and the results show that the existing successful models exhibit a frustrating degradation, with a maximum drop of 23.43 F1 score.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We perform experiments on typical models published in the last decade as well as a representative large language model, and the results show that the existing successful models exhibit a frustrating degradation, with a maximum drop of 23.43 F1 score.\""
    },
    {
        "title": "Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding",
        "authors": [
            "Sangmin Bae",
            "Jongwoo Ko",
            "Hwanjun Song",
            "Se-Young Yun"
        ],
        "published": "2023",
        "summary": "To tackle the high inference latency exhibited by autoregressive language models, previous studies have proposed an early-exiting framework that allocates adaptive computation paths for each token based on the complexity of generating the subsequent token. However, we observed several shortcomings, including performance degradation caused by a state copying mechanism or numerous exit paths, and sensitivity to exit confidence thresholds. Consequently, we propose a Fast and Robust Early-Exiting (FREE) framework, which incorporates a shallow-deep module and a synchronized parallel decoding. Our framework enables faster inference by synchronizing the decoding process of the current token with previously stacked early-exited tokens. Furthermore, as parallel decoding allows us to observe predictions from both shallow and deep models, we present a novel adaptive threshold estimator that exploits a Beta mixture model to determine suitable confidence thresholds. We empirically demonstrated the superiority of our proposed framework on extensive generation tasks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.362.pdf",
        "keywords": [
            "adaptive threshold estimator",
            "autoregressive language models",
            "synchronized parallel",
            "beta mixture model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, we observed several shortcomings, including performance degradation caused by a state copying mechanism or numerous exit paths, and sensitivity to exit confidence thresholds.\"\n\nThis abstract mentions some limitations of autoregressive language models (a type of LLM), but does not elaborate on them and focuses on the proposed solution to address these limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, we observed several shortcomings, including performance degradation caused by a state copying mechanism or numerous exit paths, and sensitivity to exit confidence thresholds.\"\n\nThis abstract mentions some limitations of autoregressive language models (a type of LLM), but does not elaborate on them and focuses on the proposed solution to address these limitations."
    },
    {
        "title": "Answering Questions by Meta-Reasoning over Multiple Chains of Thought",
        "authors": [
            "Ori Yoran",
            "Tomer Wolfson",
            "Ben Bogin",
            "Uri Katz",
            "Daniel Deutch",
            "Jonathan Berant"
        ],
        "published": "2023",
        "summary": "Modern systems for multi-hop question answering (QA) typically break questions into a sequence of reasoning steps, termed chain-of-thought (CoT), before arriving at a final answer. Often, multiple chains are sampled and aggregated through a voting mechanism over the final answers, but the intermediate steps themselves are discarded. While such approaches improve performance, they do not consider the relations between intermediate steps across chains and do not provide a unified explanation for the predicted answer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts large language models to meta-reason over multiple chains of thought, rather than aggregate their answers. MCR examines different reasoning chains, mixes information between them and selects the most relevant facts in generating an explanation and predicting the answer. MCR outperforms strong baselines on 7 multi-hop QA datasets. Moreover, our analysis reveals that MCR explanations exhibit high quality, enabling humans to verify its answers.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.364.pdf",
        "keywords": [
            "chain of thought",
            "multi chain reasoning",
            "meta reasoning",
            "multiple chains of thought",
            "multi hop question answering"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation of LLMs is mentioned, but the paper proposes a method to improve their performance in multi-hop QA tasks, implying that current LLMs may have limitations in this area.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation of LLMs is mentioned, but the paper proposes a method to improve their performance in multi-hop QA tasks, implying that current LLMs may have limitations in this area."
    },
    {
        "title": "INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback",
        "authors": [
            "Wenda Xu",
            "Danqing Wang",
            "Liangming Pan",
            "Zhenqiao Song",
            "Markus Freitag",
            "William Wang",
            "Lei Li"
        ],
        "published": "2023",
        "summary": "Automatically evaluating the quality of language generation is critical. Although recent learned metrics show high correlation with human judgement, these metrics do not provide explicit explanation of their verdict, nor associate the scores with defects in the generated text. To address this limitation, we present INSTRUCTSCORE, a fine-grained explainable evaluation metric for text generation. By harnessing both explicit human instruction and the implicit knowledge of GPT-4, we fine-tune a text evaluation metric based on LLaMA, producing both a score for generated text and a human readable diagnostic report. We evaluate INSTRUCTSCORE on a variety of generation tasks, including translation, captioning, data-to-text, and commonsense generation. Experiments show that our 7B model surpasses all other unsupervised metrics, including those based on 175B GPT-3 and GPT-4. Surprisingly, our INSTRUCTSCORE, even without direct supervision from human-rated data, achieves performance levels on par with state-of-the-art metrics like COMET22, which were fine-tuned on human ratings.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.365.pdf",
        "keywords": [
            "text generation",
            "instructscore",
            "generation",
            "automatic feedback",
            "text generation evaluation",
            "text evaluation",
            "language generation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although recent learned metrics show high correlation with human judgement, these metrics do not provide explicit explanation of their verdict, nor associate the scores with defects in the generated text.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although recent learned metrics show high correlation with human judgement, these metrics do not provide explicit explanation of their verdict, nor associate the scores with defects in the generated text.\""
    },
    {
        "title": "Multi-level Contrastive Learning for Script-based Character Understanding",
        "authors": [
            "Dawei Li",
            "Hengyuan Zhang",
            "Yanran Li",
            "Shiping Yang"
        ],
        "published": "2023",
        "summary": "In this work, we tackle the scenario of understanding characters in scripts, which aims to learn the characters’ personalities and identities from their utterances. We begin by analyzing several challenges in this scenario, and then propose a multi-level contrastive learning framework to capture characters’ global information in a fine-grained manner. To validate the proposed framework, we conduct extensive experiments on three character understanding sub-tasks by comparing with strong pre-trained language models, including SpanBERT, Longformer, BigBird and ChatGPT-3.5. Experimental results demonstrate that our method improves the performances by a considerable margin. Through further in-depth analysis, we show the effectiveness of our method in addressing the challenges and provide more hints on the scenario of character understanding. We will open-source our work in this URL.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.366.pdf",
        "keywords": [
            "character understanding",
            "contrastive learning",
            "script based character understanding"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We begin by analyzing several challenges in this scenario...\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"We begin by analyzing several challenges in this scenario...\""
    },
    {
        "title": "CHEF in the Language Kitchen: A Generative Data Augmentation Leveraging Korean Morpheme Ingredients",
        "authors": [
            "Jaehyung Seo",
            "Hyeonseok Moon",
            "Jaewook Lee",
            "Sugyeong Eo",
            "Chanjun Park",
            "Heuiseok Lim"
        ],
        "published": "2023",
        "summary": "Korean morphological variations present unique opportunities and challenges in natural language processing (NLP), necessitating an advanced understanding of morpheme-based sentence construction. The complexity of morphological variations allows for diverse sentence forms based on the syntactic-semantic integration of functional morphemes (i.e., affixes) to lexical morphemes (i.e., roots). With this in mind, we propose a method - CHEF, replicating the morphological transformations inherent in sentences based on lexical and functional morpheme combinations through generative data augmentation. CHEF operates using a morpheme blender and a label discriminator, thereby enhancing the diversity of Korean sentence forms by capturing the properties of agglutination while maintaining label consistency. We conduct experiments on Korean multiple classification datasets, improving model performance in full- and few-shot settings. Our proposed method boosts performance beyond the preceding data augmentation methods without incurring external data usage. We demonstrate that our approach achieves comparable results yielded by augmentation techniques that use large language models (LLMs).",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.367.pdf",
        "keywords": [
            "morpheme ingredients",
            "data augmentation",
            "generative data augmentation",
            "natural language processing",
            "morphological transformations"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We demonstrate that our approach achieves comparable results yielded by augmentation techniques that use large language models (LLMs).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We demonstrate that our approach achieves comparable results yielded by augmentation techniques that use large language models (LLMs).\""
    },
    {
        "title": "Towards Interpretable Mental Health Analysis with Large Language Models",
        "authors": [
            "Kailai Yang",
            "Shaoxiong Ji",
            "Tianlin Zhang",
            "Qianqian Xie",
            "Ziyan Kuang",
            "Sophia Ananiadou"
        ],
        "published": "2023",
        "summary": "The latest large language models (LLMs) such as ChatGPT, exhibit strong capabilities in automated mental health analysis. However, existing relevant studies bear several limitations, including inadequate evaluations, lack of prompting strategies, and ignorance of exploring LLMs for explainability. To bridge these gaps, we comprehensively evaluate the mental health analysis and emotional reasoning ability of LLMs on 11 datasets across 5 tasks. We explore the effects of different prompting strategies with unsupervised and distantly supervised emotional information. Based on these prompts, we explore LLMs for interpretable mental health analysis by instructing them to generate explanations for each of their decisions. We convey strict human evaluations to assess the quality of the generated explanations, leading to a novel dataset with 163 human-assessed explanations. We benchmark existing automatic evaluation metrics on this dataset to guide future related works. According to the results, ChatGPT shows strong in-context learning ability but still has a significant gap with advanced task-specific methods. Careful prompt engineering with emotional cues and expert-written few-shot examples can also effectively improve performance on mental health analysis. In addition, ChatGPT generates explanations that approach human performance, showing its great potential in explainable mental health analysis.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.370.pdf",
        "keywords": [
            "mental health",
            "mental health analysis",
            "language models",
            "large language models",
            "explanations",
            "emotional reasoning",
            "prompting"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"existing relevant studies bear several limitations, including inadequate evaluations, lack of prompting strategies, and ignorance of exploring LLMs for explainability\"; \"ChatGPT shows strong in-context learning ability but still has a significant gap with advanced task-specific methods.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"existing relevant studies bear several limitations, including inadequate evaluations, lack of prompting strategies, and ignorance of exploring LLMs for explainability\"; \"ChatGPT shows strong in-context learning ability but still has a significant gap with advanced task-specific methods.\""
    },
    {
        "title": "Spoiler Detection as Semantic Text Matching",
        "authors": [
            "Ryan Tran",
            "Canwen Xu",
            "Julian McAuley"
        ],
        "published": "2023",
        "summary": "Engaging with discussion of TV shows online often requires individuals to refrain from consuming show-related content for extended periods to avoid spoilers. While existing research on spoiler detection shows promising results in safeguarding viewers from general spoilers, it fails to address the issue of users abstaining from show-related content during their watch. This is primarily because the definition of a spoiler varies depending on the viewer’s progress in the show, and conventional spoiler detection methods lack the granularity to capture this complexity. To tackle this challenge, we propose the task of spoiler matching, which involves assigning an episode number to a spoiler given a specific TV show. We frame this task as semantic text matching and introduce a dataset comprised of comments and episode summaries to evaluate model performance. Given the length of each example, our dataset can also serve as a benchmark for long-range language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.373.pdf",
        "keywords": [
            "spoilers",
            "spoiler matching",
            "spoiler detection",
            "text matching",
            "semantic text matching"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Given the length of each example, our dataset can also serve as a benchmark for long-range language models.\"\n\nThis abstract mentions LLMs briefly and implies that existing models may have limitations in handling long-range tasks, but it does not elaborate on these limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Given the length of each example, our dataset can also serve as a benchmark for long-range language models.\"\n\nThis abstract mentions LLMs briefly and implies that existing models may have limitations in handling long-range tasks, but it does not elaborate on these limitations."
    },
    {
        "title": "Multimodal Embodied Plan Prediction Augmented with Synthetic Embodied Dialogue",
        "authors": [
            "Aishwarya Padmakumar",
            "Mert Inan",
            "Spandana Gella",
            "Patrick Lange",
            "Dilek Hakkani-Tur"
        ],
        "published": "2023",
        "summary": "Embodied task completion is a challenge where an agent in a simulated environment must predict environment actions to complete tasks based on natural language instructions and ego-centric visual observations. We propose a variant of this problem where the agent predicts actions at a higher level of abstraction called a plan, which helps make agent actions more interpretable and can be obtained from the appropriate prompting of large language models. We show that multimodal transformer models can outperform language-only models for this problem but fall significantly short of oracle plans. Since collecting human-human dialogues for embodied environments is expensive and time-consuming, we propose a method to synthetically generate such dialogues, which we then use as training data for plan prediction. We demonstrate that multimodal transformer models can attain strong zero-shot performance from our synthetic data, outperforming language-only models trained on human-human data.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.374.pdf",
        "keywords": [
            "embodied",
            "embodied dialogue",
            "language",
            "multimodal embodied plan prediction"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We show that multimodal transformer models can outperform language-only models for this problem but fall significantly short of oracle plans.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of the multimodal transformer models (falling short of oracle plans), but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We show that multimodal transformer models can outperform language-only models for this problem but fall significantly short of oracle plans.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of the multimodal transformer models (falling short of oracle plans), but it is not the primary focus of the paper."
    },
    {
        "title": "GEM: Gestalt Enhanced Markup Language Model for Web Understanding via Render Tree",
        "authors": [
            "Zirui Shao",
            "Feiyu Gao",
            "Zhongda Qi",
            "Hangdi Xing",
            "Jiajun Bu",
            "Zhi Yu",
            "Qi Zheng",
            "Xiaozhong Liu"
        ],
        "published": "2023",
        "summary": "Inexhaustible web content carries abundant perceptible information beyond text. Unfortunately, most prior efforts in pre-trained Language Models (LMs) ignore such cyber-richness, while few of them only employ plain HTMLs, and crucial information in the rendered web, such as visual, layout, and style, are excluded. Intuitively, those perceptible web information can provide essential intelligence to facilitate content understanding tasks. This study presents an innovative Gestalt Enhanced Markup (GEM) Language Model inspired by Gestalt psychological theory for hosting heterogeneous visual information from the render tree into the language model without requiring additional visual input. Comprehensive experiments on multiple downstream tasks, i.e., web question answering and web information extraction, validate GEM superiority.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.375.pdf",
        "keywords": [
            "language",
            "language models",
            "gestalt",
            "visual",
            "web",
            "render tree",
            "markup"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unfortunately, most prior efforts in pre-trained Language Models (LMs) ignore such cyber-richness, while few of them only employ plain HTMLs, and crucial information in the rendered web, such as visual, layout, and style, are excluded.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Unfortunately, most prior efforts in pre-trained Language Models (LMs) ignore such cyber-richness, while few of them only employ plain HTMLs, and crucial information in the rendered web, such as visual, layout, and style, are excluded.\""
    },
    {
        "title": "SentiStream: A Co-Training Framework for Adaptive Online Sentiment Analysis in Evolving Data Streams",
        "authors": [
            "Yuhao Wu",
            "Karthick Sharma",
            "Chun Seah",
            "Shuhao Zhang"
        ],
        "published": "2023",
        "summary": "Online sentiment analysis has emerged as a crucial component in numerous data-driven applications, including social media monitoring, customer feedback analysis, and online reputation management. Despite their importance, current methodologies falter in effectively managing the continuously evolving nature of data streams, largely due to their reliance on substantial, pre-existing labelled datasets. This paper presents sentistream, a novel co-training framework specifically designed for efficient sentiment analysis within dynamic data streams. Comprising unsupervised, semi-supervised, and stream merge modules, sentistream guarantees constant adaptability to evolving data landscapes. This research delves into the continuous adaptation of language models for online sentiment analysis, focusing on real-world applications. Experimental evaluations using data streams derived from three benchmark sentiment analysis datasets confirm that our proposed methodology surpasses existing approaches in terms of both accuracy and computational efficiency.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.380.pdf",
        "keywords": [
            "sentiment analysis",
            "co training",
            "online sentiment analysis",
            "social media",
            "reputation management",
            "data streams"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite their importance, current methodologies falter in effectively managing the continuously evolving nature of data streams, largely due to their reliance on substantial, pre-existing labelled datasets.\"\n\nThis abstract mentions a limitation of current methodologies (reliance on substantial, pre-existing labelled datasets) in the context of language models for online sentiment analysis, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite their importance, current methodologies falter in effectively managing the continuously evolving nature of data streams, largely due to their reliance on substantial, pre-existing labelled datasets.\"\n\nThis abstract mentions a limitation of current methodologies (reliance on substantial, pre-existing labelled datasets) in the context of language models for online sentiment analysis, but it is not the primary focus of the paper."
    },
    {
        "title": "Solving Hard Analogy Questions with Relation Embedding Chains",
        "authors": [
            "Nitesh Kumar",
            "Steven Schockaert"
        ],
        "published": "2023",
        "summary": "Modelling how concepts are related is a central topic in Lexical Semantics. A common strategy is to rely on knowledge graphs (KGs) such as ConceptNet, and to model the relation between two concepts as a set of paths. However, KGs are limited to a fixed set of relation types, and they are incomplete and often noisy. Another strategy is to distill relation embeddings from a fine-tuned language model. However, this is less suitable for words that are only indirectly related and it does not readily allow us to incorporate structured domain knowledge. In this paper, we aim to combine the best of both worlds. We model relations as paths but associate their edges with relation embeddings. The paths are obtained by first identifying suitable intermediate words and then selecting those words for which informative relation embeddings can be obtained. We empirically show that our proposed representations are useful for solving hard analogy questions.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.382.pdf",
        "keywords": [
            "relation embeddings",
            "relation embedding chains",
            "hard analogy",
            "semantics"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, this is less suitable for words that are only indirectly related and it does not readily allow us to incorporate structured domain knowledge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, this is less suitable for words that are only indirectly related and it does not readily allow us to incorporate structured domain knowledge.\""
    },
    {
        "title": "Tree Prompting: Efficient Task Adaptation without Fine-Tuning",
        "authors": [
            "Chandan Singh",
            "John Morris",
            "Alexander Rush",
            "Jianfeng Gao",
            "Yuntian Deng"
        ],
        "published": "2023",
        "summary": "Prompting language models (LMs) is the main interface for applying them to new tasks. However, for smaller LMs, prompting provides low accuracy compared to gradient-based fine-tuning. Tree Prompting is an approach to prompting which builds a decision tree of prompts, linking multiple prompt-LM calls together to solve a task. At inference time, each call to the LM is determined by efficiently routing the outcome of the previous call using the tree. Experiments on classification datasets show that Tree Prompting improves accuracy over competing methods and is competitive with fine-tuning. We also show that variants of Tree Prompting allow inspection of a model’s decision-making process.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.384.pdf",
        "keywords": [
            "tree",
            "tree prompting",
            "prompting language models",
            "fine tuning",
            "decision tree",
            "task adaptation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, for smaller LMs, prompting provides low accuracy compared to gradient-based fine-tuning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, for smaller LMs, prompting provides low accuracy compared to gradient-based fine-tuning.\""
    },
    {
        "title": "Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data",
        "authors": [
            "Canwen Xu",
            "Daya Guo",
            "Nan Duan",
            "Julian McAuley"
        ],
        "published": "2023",
        "summary": "Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance LLaMA, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. Additionally, we propose a new technique called Self-Distill with Feedback, to further improve the performance of the Baize models with feedback from ChatGPT.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.385.pdf",
        "keywords": [
            "parameter efficient tuning",
            "open source chat",
            "self chat",
            "chat corpus"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field.\""
    },
    {
        "title": "Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators",
        "authors": [
            "Liang Chen",
            "Yang Deng",
            "Yatao Bian",
            "Zeyu Qin",
            "Bingzhe Wu",
            "Tat-Seng Chua",
            "Kam-Fai Wong"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) outperform information retrieval techniques for downstream knowledge-intensive tasks when being prompted to generate world knowledge. However, community concerns abound regarding the factuality and potential implications of using this uncensored knowledge. In light of this, we introduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to systematically and automatically evaluate generated knowledge from six important perspectives – Factuality, Relevance, Coherence, Informativeness, Helpfulness and Validity. We conduct an extensive empirical analysis of the generated knowledge from three different types of LLMs on two widely studied knowledge-intensive tasks, i.e., open-domain question answering and knowledge-grounded dialogue. Surprisingly, our study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks. Instead, the relevance and coherence of the outputs are more important than small factual mistakes. Further, we show how to use CONNER to improve knowledge-intensive tasks by designing two strategies: Prompt Engineering and Knowledge Selection. Our evaluation code and LLM-generated knowledge with human annotations will be released to facilitate future research.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.390.pdf",
        "keywords": [
            "factuality",
            "conner",
            "language models",
            "large language models",
            "knowledge",
            "knowledge evaluation"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, community concerns abound regarding the factuality and potential implications of using this uncensored knowledge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, community concerns abound regarding the factuality and potential implications of using this uncensored knowledge.\""
    },
    {
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
            "Yucheng Li",
            "Bo Dong",
            "Frank Guerin",
            "Chenghua Lin"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM’s fixed context length. This paper proposes a method called Selective Context that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the input context to make the input more compact. We test our approach using common data sources requiring long context processing: arXiv papers, news articles, and long conversations, on tasks of summarisation, question answering, and response generation. Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used. Specifically, we achieve a 50% reduction in context cost, resulting in a 36% reduction in inference memory usage and a 32% reduction in inference time, while observing only a minor drop of .023 in BERTscore and .038 in faithfulness on four downstream applications, indicating that our method strikes a good balance between efficiency and performance.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.391.pdf",
        "keywords": [
            "language models",
            "inference",
            "compressing",
            "selective context",
            "response generation"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM’s fixed context length.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM’s fixed context length.\""
    },
    {
        "title": "MoT: Memory-of-Thought Enables ChatGPT to Self-Improve",
        "authors": [
            "Xiaonan Li",
            "Xipeng Qiu"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have shown impressive abilities on various tasks. However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning. On the contrary, humans can easily improve themselves by self-thinking and memory, without external resources. In this paper, we propose a framework, **MoT**, to let the LLM self-improve through **M**emory **o**f **T**houghts, without annotated datasets and parameter updates. Specifically, MoT is divided into two stages: 1. before the test stage, the LLM pre-thinks on the unlabeled dataset and saves the high-confidence thoughts as external memory; 2. During the test stage, given a test question, the LLM recalls relevant memory to help itself reason and answer it. Experimental results show that MoT can help ChatGPT significantly improve its abilities in arithmetic reasoning, commonsense reasoning, factual reasoning, and natural language inference. Further analyses show that each component contributes critically to the improvements and MoT can lead to consistent improvements across various CoT methods and LLMs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.392.pdf",
        "keywords": [
            "memory",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning.\""
    },
    {
        "title": "Can You Follow Me? Testing Situational Understanding for ChatGPT",
        "authors": [
            "Chenghao Yang",
            "Allyson Ettinger"
        ],
        "published": "2023",
        "summary": "Understanding sentence meanings and updating information states appropriately across time—what we call “situational understanding” (SU)—is a critical ability for human-like AI agents. SU is essential in particular for chat models, such as ChatGPT, to enable consistent, coherent, and effective dialogue between humans and AI. Previous works have identified certain SU limitations in non-chatbot Large Language models (LLMs), but the extent and causes of these limitations are not well understood, and capabilities of current chat-based models in this domain have not been explored. In this work we tackle these questions, proposing a novel synthetic environment for SU testing which allows us to do controlled and systematic testing of SU in chat-oriented models, through assessment of models’ ability to track and enumerate environment states. Our environment also allows for close analysis of dynamics of model performance, to better understand underlying causes for performance patterns. We apply our test to ChatGPT, the state-of-the-art chatbot, and find that despite the fundamental simplicity of the task, the model’s performance reflects an inability to retain correct environment states across time. Our follow-up analyses suggest that performance degradation is largely because ChatGPT has non-persistent in-context memory (although it can access the full dialogue history) and it is susceptible to hallucinated updates—including updates that artificially inflate accuracies. Our findings suggest overall that ChatGPT is not currently equipped for robust tracking of situation states, and that trust in the impressive dialogue performance of ChatGPT comes with risks. We release the codebase for reproducing our test environment, as well as all prompts and API responses from ChatGPT, at https://github.com/yangalan123/SituationalTesting.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.394.pdf",
        "keywords": [
            "situationaltesting",
            "critical ability",
            "chat models",
            "chat oriented models",
            "codebase",
            "chat based models",
            "situational understanding",
            "testing",
            "su testing",
            "test environment",
            "chatgpt",
            "robust tracking",
            "synthetic environment",
            "risks"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Previous works have identified certain SU limitations in non-chatbot Large Language models (LLMs)...Our follow-up analyses suggest that performance degradation is largely because ChatGPT has non-persistent in-context memory...and it is susceptible to hallucinated updates—including updates that artificially inflate accuracies.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Previous works have identified certain SU limitations in non-chatbot Large Language models (LLMs)...Our follow-up analyses suggest that performance degradation is largely because ChatGPT has non-persistent in-context memory...and it is susceptible to hallucinated updates—including updates that artificially inflate accuracies.\""
    },
    {
        "title": "Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4",
        "authors": [
            "Kellin Pelrine",
            "Anne Imouza",
            "Camille Thibault",
            "Meilina Reksoprodjo",
            "Caleb Gupta",
            "Joel Christoph",
            "Jean-François Godbout",
            "Reihaneh Rabbany"
        ],
        "published": "2023",
        "summary": "Misinformation poses a critical societal challenge, and current approaches have yet to produce an effective solution. We propose focusing on generalization, uncertainty, and how to leverage recent large language models, in order to create more practical tools to evaluate information veracity in contexts where perfect classification is impossible. We first demonstrate that GPT-4 can outperform prior methods in multiple settings and languages. Next, we explore generalization, revealing that GPT-4 and RoBERTa-large exhibit differences in failure modes. Third, we propose techniques to handle uncertainty that can detect impossible examples and strongly improve outcomes. We also discuss results on other language models, temperature, prompting, versioning, explainability, and web retrieval, each one providing practical insights and directions for future research. Finally, we publish the LIAR-New dataset with novel paired English and French misinformation data and Possibility labels that indicate if there is sufficient context for veracity evaluation. Overall, this research lays the groundwork for future tools that can drive real-world progress to combat misinformation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.395.pdf",
        "keywords": [
            "veracity",
            "misinformation",
            "generalization",
            "reliable misinformation mitigation",
            "veracity evaluation",
            "uncertainty",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Next, we explore generalization, revealing that GPT-4 and RoBERTa-large exhibit differences in failure modes.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Next, we explore generalization, revealing that GPT-4 and RoBERTa-large exhibit differences in failure modes.\""
    },
    {
        "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
        "authors": [
            "Junyi Li",
            "Xiaoxue Cheng",
            "Xin Zhao",
            "Jian-Yun Nie",
            "Ji-Rong Wen"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation for Large Language Models (HaluEval) benchmark, a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about 19.5% user queries). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. While, our experiments also prove that the hallucination recognition can be improved by providing external knowledge or adding reasoning steps.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.397.pdf",
        "keywords": [
            "hallucination",
            "halueval",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge... Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge... Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts.\""
    },
    {
        "title": "Enabling Large Language Models to Generate Text with Citations",
        "authors": [
            "Tianyu Gao",
            "Howard Yen",
            "Jiatong Yu",
            "Danqi Chen"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs’ Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions—fluency, correctness, and citation quality—and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement—For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.398.pdf",
        "keywords": [
            "citations",
            "eli5 dataset",
            "language models",
            "text",
            "corpora"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"but their generated outputs are prone to hallucination\"; \"Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement—For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"but their generated outputs are prone to hallucination\"; \"Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement—For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time.\""
    },
    {
        "title": "Pre-Trained Language Models Augmented with Synthetic Scanpaths for Natural Language Understanding",
        "authors": [
            "Shuwen Deng",
            "Paul Prasse",
            "David Reich",
            "Tobias Scheffer",
            "Lena Jäger"
        ],
        "published": "2023",
        "summary": "Human gaze data offer cognitive information that reflects natural language comprehension. Indeed, augmenting language models with human scanpaths has proven beneficial for a range of NLP tasks, including language understanding. However, the applicability of this approach is hampered because the abundance of text corpora is contrasted by a scarcity of gaze data. Although models for the generation of human-like scanpaths during reading have been developed, the potential of synthetic gaze data across NLP tasks remains largely unexplored. We develop a model that integrates synthetic scanpath generation with a scanpath-augmented language model, eliminating the need for human gaze data. Since the model’s error gradient can be propagated throughout all parts of the model, the scanpath generator can be fine-tuned to downstream tasks. We find that the proposed model not only outperforms the underlying language model, but achieves a performance that is comparable to a language model augmented with real human gaze data. Our code is publicly available.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.400.pdf",
        "keywords": [
            "scanpaths",
            "scanpath augmented language",
            "augmenting language",
            "language",
            "synthetic scanpath",
            "synthetic gaze data",
            "natural language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the applicability of this approach is hampered because the abundance of text corpora is contrasted by a scarcity of gaze data.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the applicability of this approach is hampered because the abundance of text corpora is contrasted by a scarcity of gaze data.\""
    },
    {
        "title": "Counting the Bugs in ChatGPT’s Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model",
        "authors": [
            "Leonie Weissweiler",
            "Valentin Hofmann",
            "Anjali Kantharuban",
            "Anna Cai",
            "Ritam Dutt",
            "Amey Hengle",
            "Anubha Kabra",
            "Atharva Kulkarni",
            "Abhishek Vijayakumar",
            "Haofei Yu",
            "Hinrich Schuetze",
            "Kemal Oflazer",
            "David Mortensen"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have recently reached an impressive level of linguistic capability, prompting comparisons with human language skills. However, there have been relatively few systematic inquiries into the linguistic capabilities of the latest generation of LLMs, and those studies that do exist (i) ignore the remarkable ability of humans to generalize, (ii) focus only on English, and (iii) investigate syntax or semantics and overlook other capabilities that lie at the heart of human language, like morphology. Here, we close these gaps by conducting the first rigorous analysis of the morphological capabilities of ChatGPT in four typologically varied languages (specifically, English, German, Tamil, and Turkish). We apply a version of Berko’s (1958) wug test to ChatGPT, using novel, uncontaminated datasets for the four examined languages. We find that ChatGPT massively underperforms purpose-built systems, particularly in English. Overall, our results—through the lens of morphology—cast a new light on the linguistic capabilities of ChatGPT, suggesting that claims of human-like language skills are premature and misleading.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.401.pdf",
        "keywords": [
            "linguistic",
            "language",
            "language model",
            "large language models",
            "linguistic capabilities"
        ],
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that ChatGPT massively underperforms purpose-built systems, particularly in English. Overall, our results—through the lens of morphology—cast a new light on the linguistic capabilities of ChatGPT, suggesting that claims of human-like language skills are premature and misleading.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"We find that ChatGPT massively underperforms purpose-built systems, particularly in English. Overall, our results—through the lens of morphology—cast a new light on the linguistic capabilities of ChatGPT, suggesting that claims of human-like language skills are premature and misleading.\""
    },
    {
        "title": "Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning",
        "authors": [
            "Quanyu Long",
            "Wenya Wang",
            "Sinno Pan"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have showcased their capability with few-shot inference known as in-context learning. However, in-domain demonstrations are not always readily available in real scenarios, leading to cross-domain in-context learning. Besides, LLMs are still facing challenges in long-tail knowledge in unseen and unfamiliar domains. The above limitations demonstrate the necessity of Unsupervised Domain Adaptation (UDA). In this paper, we study the UDA problem under an in-context learning setting to adapt language models from the source domain to the target domain without any target labels. The core idea is to retrieve a subset of cross-domain elements that are the most similar to the query, and elicit language model to adapt in an in-context manner by learning both target domain distribution and the discriminative task signal simultaneously with the augmented cross-domain in-context examples. We devise different prompting and training strategies, accounting for different LM architectures to learn the target distribution via language modeling. With extensive experiments on Sentiment Analysis (SA) and Named Entity Recognition (NER) tasks, we thoroughly study the effectiveness of ICL for domain transfer and demonstrate significant improvements over baseline models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.402.pdf",
        "keywords": [
            "adapt",
            "domain adaptation",
            "language",
            "adapt language models",
            "language modeling",
            "context",
            "named entity recognition",
            "augmented domain adaptation",
            "in context learning"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, in-domain demonstrations are not always readily available in real scenarios, leading to cross-domain in-context learning. Besides, LLMs are still facing challenges in long-tail knowledge in unseen and unfamiliar domains.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, in-domain demonstrations are not always readily available in real scenarios, leading to cross-domain in-context learning. Besides, LLMs are still facing challenges in long-tail knowledge in unseen and unfamiliar domains.\""
    },
    {
        "title": "Understanding the Inner-workings of Language Models Through Representation Dissimilarity",
        "authors": [
            "Davis Brown",
            "Charles Godfrey",
            "Nicholas Konz",
            "Jonathan Tu",
            "Henry Kvinge"
        ],
        "published": "2023",
        "summary": "As language models are applied to an increasing number of real-world applications, understanding their inner workings has become an important issue in model trust, interpretability, and transparency. In this work we show that representation dissimilarity measures, which are functions that measure the extent to which two model’s internal representations differ, can be a valuable tool for gaining insight into the mechanics of language models. Among our insights are: (i) an apparent asymmetry in the internal representations of model using SoLU and GeLU activation functions, (ii) evidence that dissimilarity measures can identify and locate generalization properties of models that are invisible via in-distribution test set performance, and (iii) new evaluations of how language model features vary as width and depth are increased. Our results suggest that dissimilarity measures are a promising set of tools for shedding light on the inner workings of language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.403.pdf",
        "keywords": [
            "language models",
            "representation dissimilarity",
            "dissimilarity measures"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"evidence that dissimilarity measures can identify and locate generalization properties of models that are invisible via in-distribution test set performance\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"evidence that dissimilarity measures can identify and locate generalization properties of models that are invisible via in-distribution test set performance\""
    },
    {
        "title": "Construction Artifacts in Metaphor Identification Datasets",
        "authors": [
            "Joanne Boisson",
            "Luis Espinosa-Anke",
            "Jose Camacho-Collados"
        ],
        "published": "2023",
        "summary": "Metaphor identification aims at understanding whether a given expression is used figuratively in context. However, in this paper we show how existing metaphor identification datasets can be gamed by fully ignoring the potential metaphorical expression or the context in which it occurs. We test this hypothesis in a variety of datasets and settings, and show that metaphor identification systems based on language models without complete information can be competitive with those using the full context. This is due to the construction procedures to build such datasets, which introduce unwanted biases for positive and negative classes. Finally, we test the same hypothesis on datasets that are carefully sampled from natural corpora and where this bias is not present, making these datasets more challenging and reliable.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.406.pdf",
        "keywords": [
            "metaphor identification",
            "metaphor identification systems",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"metaphor identification systems based on language models without complete information can be competitive with those using the full context.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"metaphor identification systems based on language models without complete information can be competitive with those using the full context.\""
    },
    {
        "title": "MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models",
        "authors": [
            "Deepak Nathani",
            "David Wang",
            "Liangming Pan",
            "William Wang"
        ],
        "published": "2023",
        "summary": "Language Models (LMs) have shown impressive performance in various natural language tasks. However, when it comes to natural language reasoning, LMs still face challenges such as hallucination, generating incorrect intermediate reasoning steps, and making mathematical errors. Recent research has focused on enhancing LMs through *self-improvement* using feedback. Nevertheless, existing approaches relying on a single generic feedback source fail to address the diverse error types found in LM-generated reasoning chains. In this work, we propose **Multi-Aspect Feedback**, an iterative refinement framework that integrates multiple feedback modules, including frozen LMs and external tools, each focusing on a specific error category. Our experimental results demonstrate the efficacy of our approach to addressing several errors in the LM-generated reasoning chain and thus improving the overall performance of an LM in several reasoning tasks. We see an improvement of up to 20% in Mathematical Reasoning and up to 18% in Logical Entailment.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.407.pdf",
        "keywords": [
            "language models",
            "large language models",
            "aspect feedback"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, when it comes to natural language reasoning, LMs still face challenges such as hallucination, generating incorrect intermediate reasoning steps, and making mathematical errors.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, when it comes to natural language reasoning, LMs still face challenges such as hallucination, generating incorrect intermediate reasoning steps, and making mathematical errors.\""
    },
    {
        "title": "Rethinking Model Selection and Decoding for Keyphrase Generation with Pre-trained Sequence-to-Sequence Models",
        "authors": [
            "Di Wu",
            "Wasi Ahmad",
            "Kai-Wei Chang"
        ],
        "published": "2023",
        "summary": "Keyphrase Generation (KPG) is a longstanding task in NLP with widespread applications. The advent of sequence-to-sequence (seq2seq) pre-trained language models (PLMs) has ushered in a transformative era for KPG, yielding promising performance improvements. However, many design decisions remain unexplored and are often made arbitrarily. This paper undertakes a systematic analysis of the influence of model selection and decoding strategies on PLM-based KPG. We begin by elucidating why seq2seq PLMs are apt for KPG, anchored by an attention-driven hypothesis. We then establish that conventional wisdom for selecting seq2seq PLMs lacks depth: (1) merely increasing model size or performing task-specific adaptation is not parameter-efficient; (2) although combining in-domain pre-training with task adaptation benefits KPG, it does partially hinder generalization. Regarding decoding, we demonstrate that while greedy search achieves strong F1 scores, it lags in recall compared with sampling-based methods. Based on these insights, we propose DeSel, a likelihood-based decode-select algorithm for seq2seq PLMs. DeSel improves greedy search by an average of 4.7% semantic F1 across five datasets. Our collective findings pave the way for deeper future investigations into PLM-based KPG.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.410.pdf",
        "keywords": [
            "keyphrase generation",
            "decoding",
            "model selection",
            "language models",
            "desel"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"conventional wisdom for selecting seq2seq PLMs lacks depth: (1) merely increasing model size or performing task-specific adaptation is not parameter-efficient; (2) although combining in-domain pre-training with task adaptation benefits KPG, it does partially hinder generalization.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"conventional wisdom for selecting seq2seq PLMs lacks depth: (1) merely increasing model size or performing task-specific adaptation is not parameter-efficient; (2) although combining in-domain pre-training with task adaptation benefits KPG, it does partially hinder generalization.\""
    },
    {
        "title": "Personalized Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation",
        "authors": [
            "Hailin Chen",
            "Amrita Saha",
            "Steven Hoi",
            "Shafiq Joty"
        ],
        "published": "2023",
        "summary": "With the rise of powerful closed-sourced LLMs (ChatGPT, GPT-4), there are increasing interests in distilling the capabilies of close-sourced LLMs to smaller open-sourced LLMs. Previous distillation methods usually prompt ChatGPT to generate a set of instructions and answers, for the student model to learn. However, such standard distillation approach neglects the merits and conditions of the student model. Inspired by modern teaching principles, we design a personalised distillation process, in which the student attempts to solve a task first, then the teacher provides an adaptive refinement for the student to improve. Instead of feeding the student with teacher’s prior, personalised distillation enables personalised learning for the student model, as it only learns on examples it makes mistakes upon and learns to improve its own solution. On code generation, personalised distillation consistently outperforms standard distillation with only one third of the data. With only 2.5-3K personalised examples that incur a data-collection cost of 4-6$, we boost CodeGen-mono-16B by 7% to achieve 36.4% pass@1 and StarCoder by 12.2% to achieve 45.8% pass@1 on HumanEval.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.417.pdf",
        "keywords": [
            "personalized distillation",
            "personalised distillation",
            "code generation",
            "adaptive"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Previous distillation methods... neglects the merits and conditions of the student model.\"\n\nThis abstract mentions a limitation of previous distillation methods for LLMs, but it is a minor detail and not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Previous distillation methods... neglects the merits and conditions of the student model.\"\n\nThis abstract mentions a limitation of previous distillation methods for LLMs, but it is a minor detail and not the primary focus of the paper."
    },
    {
        "title": "Do Language Models Have a Common Sense regarding Time? Revisiting Temporal Commonsense Reasoning in the Era of Large Language Models",
        "authors": [
            "Raghav Jain",
            "Daivik Sojitra",
            "Arkadeep Acharya",
            "Sriparna Saha",
            "Adam Jatowt",
            "Sandipan Dandapat"
        ],
        "published": "2023",
        "summary": "Temporal reasoning represents a vital component of human communication and understanding, yet remains an underexplored area within the context of Large Language Models (LLMs). Despite LLMs demonstrating significant proficiency in a range of tasks, a comprehensive, large-scale analysis of their temporal reasoning capabilities is missing. Our paper addresses this gap, presenting the first extensive benchmarking of LLMs on temporal reasoning tasks. We critically evaluate 8 different LLMs across 6 datasets using 3 distinct prompting strategies. Additionally, we broaden the scope of our evaluation by including in our analysis 2 Code Generation LMs. Beyond broad benchmarking of models and prompts, we also conduct a fine-grained investigation of performance across different categories of temporal tasks. We further analyze the LLMs on varying temporal aspects, offering insights into their proficiency in understanding and predicting the continuity, sequence, and progression of events over time. Our findings reveal a nuanced depiction of the capabilities and limitations of the models within temporal reasoning, offering a comprehensive reference for future research in this pivotal domain.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.418.pdf",
        "keywords": [
            "temporal reasoning",
            "benchmarking",
            "temporal commonsense reasoning",
            "language models",
            "proficiency",
            "time"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our findings reveal a nuanced depiction of the capabilities and limitations of the models within temporal reasoning\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Our findings reveal a nuanced depiction of the capabilities and limitations of the models within temporal reasoning\""
    },
    {
        "title": "Evaluation of African American Language Bias in Natural Language Generation",
        "authors": [
            "Nicholas Deas",
            "Jessica Grieser",
            "Shana Kleiner",
            "Desmond Patton",
            "Elsbeth Turcan",
            "Kathleen McKeown"
        ],
        "published": "2023",
        "summary": "While biases disadvantaging African American Language (AAL) have been uncovered in models for tasks such as speech recognition and toxicity detection, there has been little investigation of these biases for language generation models like ChatGPT. We evaluate how well LLMs understand AAL in comparison to White Mainstream English (WME), the encouraged “standard” form of English taught in American classrooms. We measure large language model performance on two tasks: a counterpart generation task, where a model generates AAL given WME and vice versa, and a masked span prediction (MSP) task, where models predict a phrase hidden from their input. Using a novel dataset of AAL texts from a variety of regions and contexts, we present evidence of dialectal bias for six pre-trained LLMs through performance gaps on these tasks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.421.pdf",
        "keywords": [
            "bias",
            "language bias",
            "language model performance",
            "performance gaps",
            "model generates",
            "natural language generation",
            "masked span prediction"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We present evidence of dialectal bias for six pre-trained LLMs through performance gaps on these tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We present evidence of dialectal bias for six pre-trained LLMs through performance gaps on these tasks.\""
    },
    {
        "title": "A Systematic Study of Performance Disparities in Multilingual Task-Oriented Dialogue Systems",
        "authors": [
            "Songbo Hu",
            "Han Zhou",
            "Moy Yuan",
            "Milan Gritta",
            "Guchun Zhang",
            "Ignacio Iacobacci",
            "Anna Korhonen",
            "Ivan Vulić"
        ],
        "published": "2023",
        "summary": "Achieving robust language technologies that can perform well across the world’s many languages is a central goal of multilingual NLP. In this work, we take stock of and empirically analyse task performance disparities that exist between multilingual task-oriented dialogue (ToD) systems. We first define new quantitative measures of absolute and relative equivalence in system performance, capturing disparities across languages and within individual languages. Through a series of controlled experiments, we demonstrate that performance disparities depend on a number of factors: the nature of the ToD task at hand, the underlying pretrained language model, the target language, and the amount of ToD annotated data. We empirically prove the existence of the adaptation and intrinsic biases in current ToD systems: e.g., ToD systems trained for Arabic or Turkish using annotated ToD data fully parallel to English ToD data still exhibit diminished ToD task performance. Beyond providing a series of insights into the performance disparities of ToD systems in different languages, our analyses offer practical tips on how to approach ToD data collection and system development for new languages.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.422.pdf",
        "keywords": [
            "performance disparities",
            "dialogue systems",
            "task performance disparities",
            "multilingual task oriented dialogue"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We empirically prove the existence of the adaptation and intrinsic biases in current ToD systems: e.g., ToD systems trained for Arabic or Turkish using annotated ToD data fully parallel to English ToD data still exhibit diminished ToD task performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We empirically prove the existence of the adaptation and intrinsic biases in current ToD systems: e.g., ToD systems trained for Arabic or Turkish using annotated ToD data fully parallel to English ToD data still exhibit diminished ToD task performance.\""
    },
    {
        "title": "Cognate Transformer for Automated Phonological Reconstruction and Cognate Reflex Prediction",
        "authors": [
            "V.S.D.S.Mahesh Akavarapu",
            "Arnab Bhattacharya"
        ],
        "published": "2023",
        "summary": "Phonological reconstruction is one of the central problems in historical linguistics where a proto-word of an ancestral language is determined from the observed cognate words of daughter languages. Computational approaches to historical linguistics attempt to automate the task by learning models on available linguistic data. Several ideas and techniques drawn from computational biology have been successfully applied in this area of computational historical linguistics. Following these lines, we adapt MSA Transformer, a protein language model, to the problem of automated phonological reconstruction. MSA Transformer trains on multiple sequence alignments as input and is, thus, apt for application on aligned cognate words. We, hence, name our model as Cognate Transformer. We also apply the model on another associated task, namely, cognate reflex prediction where a reflex word in a daughter language is predicted based on cognate words from other daughter languages. We show that our model outperforms the existing models on both the tasks, especially when it is pre-trained on masked word prediction task.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.423.pdf",
        "keywords": [
            "reflex",
            "computational biology",
            "daughter languages",
            "transformer",
            "cognate reflex prediction",
            "phonological reconstruction",
            "automated phonological"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning",
        "authors": [
            "Ximing Lu",
            "Faeze Brahman",
            "Peter West",
            "Jaehun Jung",
            "Khyathi Chandu",
            "Abhilasha Ravichander",
            "Prithviraj Ammanabrolu",
            "Liwei Jiang",
            "Sahana Ramnath",
            "Nouha Dziri",
            "Jillian Fisher",
            "Bill Lin",
            "Skyler Hallinan",
            "Lianhui Qin",
            "Xiang Ren",
            "Sean Welleck",
            "Yejin Choi"
        ],
        "published": "2023",
        "summary": "While extreme-scale language models have demonstrated exceptional performance on a variety of language tasks, the degree of control over these language models through pure prompting can often be limited. Directly fine-tuning such language models can be effective for tailoring them, but it can be either extremely costly (e.g., GPT-3) or not even feasible for the broader community (e.g., GPT-4). We propose Inference-time Policy Adapters (IPA), which efficiently tailors a language model such as GPT-3 without fine-tuning it. IPA guides a large base model during decoding time through a lightweight policy adapter trained to optimize an arbitrary user objective with reinforcement learning. On five challenging text generation tasks, such as toxicity reduction and lexically constrained generation, IPA consistently brings significant improvements over off-the-shelf language models. It outperforms competitive baseline methods, sometimes even including expensive fine-tuning. In particular, tailoring GPT-2 with IPA can outperform GPT-3, while tailoring GPT-3 with IPA brings a major performance boost over GPT-3 (and sometimes even over GPT-4). Our promising results highlight the potential of IPA as a lightweight alternative to tailoring extreme-scale language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.424.pdf",
        "keywords": [
            "ipa",
            "extreme scale language",
            "extreme scale language models",
            "language models",
            "extreme scale lms",
            "tuning",
            "fine tuning",
            "adapter",
            "inference time policy adapters",
            "inference time"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While extreme-scale language models have demonstrated exceptional performance on a variety of language tasks, the degree of control over these language models through pure prompting can often be limited. Directly fine-tuning such language models can be effective for tailoring them, but it can be either extremely costly (e.g., GPT-3) or not even feasible for the broader community (e.g.,",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While extreme-scale language models have demonstrated exceptional performance on a variety of language tasks, the degree of control over these language models through pure prompting can often be limited. Directly fine-tuning such language models can be effective for tailoring them, but it can be either extremely costly (e.g., GPT-3) or not even feasible for the broader community (e.g.,"
    },
    {
        "title": "Weakly Supervised Semantic Parsing with Execution-based Spurious Program Filtering",
        "authors": [
            "Kang-il Lee",
            "Segwang Kim",
            "Kyomin Jung"
        ],
        "published": "2023",
        "summary": "The problem of spurious programs is a longstanding challenge when training a semantic parser from weak supervision. To eliminate such programs that have wrong semantics but correct denotation, existing methods focus on exploiting similarities between examples based on domain-specific knowledge. In this paper, we propose a domain-agnostic filtering mechanism based on program execution results. Specifically, for each program obtained through the search process, we first construct a representation that captures the program’s semantics as execution results under various inputs. Then, we run a majority vote on these representations to identify and filter out programs with significantly different semantics from the other programs. In particular, our method is orthogonal to the program search process so that it can easily augment any of the existing weakly supervised semantic parsing frameworks. Empirical evaluations on the Natural Language Visual Reasoning and WikiTableQuestions demonstrate that applying our method to the existing semantic parsers induces significantly improved performances.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.425.pdf",
        "keywords": [
            "semantic parsers",
            "weakly supervised semantic parsing",
            "spurious program filtering",
            "domain agnostic filtering",
            "language visual reasoning"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "EtiCor: Corpus for Analyzing LLMs for Etiquettes",
        "authors": [
            "Ashutosh Dwivedi",
            "Pradhyumna Lavania",
            "Ashutosh Modi"
        ],
        "published": "2023",
        "summary": "Etiquettes are an essential ingredient of day-to-day interactions among people. Moreover, etiquettes are region-specific, and etiquettes in one region might contradict those in other regions. In this paper, we propose EtiCor, an Etiquettes Corpus, having texts about social norms from five different regions across the globe. The corpus provides a test bed for evaluating LLMs for knowledge and understanding of region-specific etiquettes. Additionally, we propose the task of Etiquette Sensitivity. We experiment with state-of-the-art LLMs (Delphi, Falcon40B, and GPT-3.5). Initial results indicate that LLMs, mostly fail to understand etiquettes from regions from non-Western world.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.428.pdf",
        "keywords": [
            "corpus",
            "social norms"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Initial results indicate that LLMs, mostly fail to understand etiquettes from regions from non-Western world.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Initial results indicate that LLMs, mostly fail to understand etiquettes from regions from non-Western world.\""
    },
    {
        "title": "An Investigation of LLMs’ Inefficacy in Understanding Converse Relations",
        "authors": [
            "Chengwen Qi",
            "Bowen Li",
            "Binyuan Hui",
            "Bailin Wang",
            "Jinyang Li",
            "Jinwang Wu",
            "Yuanjun Laili"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have achieved remarkable success in many formal language oriented tasks, such as structural data-to-text and semantic parsing. However current benchmarks mostly follow the data distribution of the pre-training data of LLMs. Therefore, a natural question rises that do LLMs really understand the structured semantics of formal languages. In this paper, we investigate this problem on a special case, converse binary relation. We introduce a new benchmark ConvRe focusing on converse relations, which contains 17 relations and 1240 triples extracted from popular knowledge graph completion datasets. Our ConvRE features two tasks, Re2Text and Text2Re, which are formulated as multi-choice question answering to evaluate LLMs’ ability to determine the matching between relations and associated text. For the evaluation protocol, apart from different prompting methods, we further introduce variants to the test text and few-shot example text. We conduct experiments on three popular LLM families and have observed various scaling trends. The results suggest that LLMs often resort to shortcut learning and still face challenges on our proposed benchmark.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.429.pdf",
        "keywords": [
            "converse binary relation",
            "converse relations",
            "large language models",
            "formal languages",
            "inefficacy"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However current benchmarks mostly follow the data distribution of the pre-training data of LLMs. Therefore, a natural question rises that do LLMs really understand the structured semantics of formal languages.\"; \"The results suggest that LLMs often resort to shortcut learning and still face challenges on our proposed benchmark.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However current benchmarks mostly follow the data distribution of the pre-training data of LLMs. Therefore, a natural question rises that do LLMs really understand the structured semantics of formal languages.\"; \"The results suggest that LLMs often resort to shortcut learning and still face challenges on our proposed benchmark.\""
    },
    {
        "title": "Towards Low-Resource Automatic Program Repair with Meta-Learning and Pretrained Language Models",
        "authors": [
            "Weishi Wang",
            "Yue Wang",
            "Steven Hoi",
            "Shafiq Joty"
        ],
        "published": "2023",
        "summary": "Automatic program repair (APR) has gained increasing attention as an essential technique in software development to reduce manual debugging efforts and boost developers’ productivity. Recent advances in deep learning (DL) based models have demonstrated promising results by learning from large-scale bug-fix examples in a data-driven manner. However, in practical scenarios, software bugs have an imbalanced distribution, and the fixing knowledge learned by APR models often only capture the patterns of frequent error types, making it inapplicable to handle the rare error types. To address this limitation, we investigate a novel task of low-resource APR, and propose Meta-APR, a new meta-learning framework integrated with code pretrained language models to generate fixes for low-resource bugs with limited training samples. Our Meta-APR learns better error-specific knowledge from high-resource bugs through efficient first-order meta-learning optimization, which allows for a faster adaptation to the target low-resource bugs. Besides, while we adopt CodeT5, a pretrained code-aware encoder-decoder Transformer, as the backbone model for Meta-APR, it is a model-agnostic framework that can be integrated with any neural models. Extensive experimental results on three benchmarks in various programming languages verify the superiority of our method over existing DL-based APR approaches.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.430.pdf",
        "keywords": [
            "language models",
            "meta learning",
            "automatic program repair",
            "low resource"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, in practical scenarios, software bugs have an imbalanced distribution, and the fixing knowledge learned by APR models often only capture the patterns of frequent error types, making it inapplicable to handle the rare error types.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, in practical scenarios, software bugs have an imbalanced distribution, and the fixing knowledge learned by APR models often only capture the patterns of frequent error types, making it inapplicable to handle the rare error types.\""
    },
    {
        "title": "ZGUL: Zero-shot Generalization to Unseen Languages using Multi-source Ensembling of Language Adapters",
        "authors": [
            "Vipul Rathore",
            "Rajdeep Dhingra",
            "Parag Singla",
            "Mausam"
        ],
        "published": "2023",
        "summary": "We tackle the problem of zero-shot cross-lingual transfer in NLP tasks via the use of language adapters (LAs). Most of the earlier works have explored training with adapter of a single source (often English), and testing either using the target LA or LA of another related language. Training target LA requires unlabeled data, which may not be readily available for low resource *unseen* languages: those that are neither seen by the underlying multilingual language model (e.g., mBERT), nor do we have any (labeled or unlabeled) data for them. We posit that for more effective cross-lingual transfer, instead of just one source LA, we need to leverage LAs of multiple (linguistically or geographically related) source languages, both at train and test-time - which we investigate via our novel neural architecture, ZGUL. Extensive experimentation across four language groups, covering 15 unseen target languages, demonstrates improvements of up to 3.2 average F1 points over standard fine-tuning and other strong baselines on POS tagging and NER tasks. We also extend ZGUL to settings where either (1) some unlabeled data or (2) few-shot training examples are available for the target language. We find that ZGUL continues to outperform baselines in these settings too.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.431.pdf",
        "keywords": [
            "adapter",
            "language adapters"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Training target LA requires unlabeled data, which may not be readily available for low resource *unseen* languages: those that are neither seen by the underlying multilingual language model (e.g., mBERT), nor do we have any (labeled or unlabeled) data for them.\"\n\nThis evidence indicates that the paper mentions a limitation of LLMs, specifically the need for",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Training target LA requires unlabeled data, which may not be readily available for low resource *unseen* languages: those that are neither seen by the underlying multilingual language model (e.g., mBERT), nor do we have any (labeled or unlabeled) data for them.\"\n\nThis evidence indicates that the paper mentions a limitation of LLMs, specifically the need for"
    },
    {
        "title": "Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning",
        "authors": [
            "Sarkar Snigdha Sarathi Das",
            "Ranran Haoran Zhang",
            "Peng Shi",
            "Wenpeng Yin",
            "Rui Zhang"
        ],
        "published": "2023",
        "summary": "Unified Sequence Labeling that articulates different sequence labeling problems such as Named Entity Recognition, Relation Extraction, Semantic Role Labeling, etc. in a generalized sequence-to-sequence format opens up the opportunity to make the maximum utilization of large language model knowledge toward structured prediction. Unfortunately, this requires formatting them into specialized augmented format unknown to the base pretrained language model (PLMs) necessitating finetuning to the target format. This significantly bounds its usefulness in data-limited settings where finetuning large models cannot properly generalize to the target format. To address this challenge and leverage PLM knowledge effectively, we propose FISH-DIP, a sample-aware dynamic sparse finetuning strategy that selectively focuses on a fraction of parameters, informed by feedback from highly regressing examples, during the fine-tuning process. By leveraging the dynamism of sparsity, our approach mitigates the impact of well-learned samples and prioritizes underperforming instances for improvement in generalization. Across five tasks of sequence labeling, we demonstrate that FISH-DIP can smoothly optimize the model in low resource settings offering upto 40% performance improvements over full fine-tuning depending on target evaluation settings. Also, compared to in-context learning and other parameter-efficient fine-tuning approaches, FISH-DIP performs comparably or better, notably in extreme low-resource settings. The source code of FISH-DIP will be available at [this URL](https://github.com/psunlpgroup/FISH-DIP)",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.433.pdf",
        "keywords": [
            "finetuning",
            "fine tuning",
            "sequence labeling",
            "sparse finetuning",
            "resource sequence labeling"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unfortunately, this requires formatting them into specialized augmented format unknown to the base pretrained language model (PLMs) necessitating finetuning to the target format. This significantly bounds its usefulness in data-limited settings where finetuning large models cannot properly generalize to the target format.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Unfortunately, this requires formatting them into specialized augmented format unknown to the base pretrained language model (PLMs) necessitating finetuning to the target format. This significantly bounds its usefulness in data-limited settings where finetuning large models cannot properly generalize to the target format.\""
    },
    {
        "title": "On the Representational Capacity of Recurrent Neural Language Models",
        "authors": [
            "Franz Nowak",
            "Anej Svete",
            "Li Du",
            "Ryan Cotterell"
        ],
        "published": "2023",
        "summary": "This work investigates the computational expressivity of language models (LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992) famously showed that RNNs with rational weights and hidden states and unbounded computation time are Turing complete. However, LMs define weightings over strings in addition to just (unweighted) language membership and the analysis of the computational power of RNN LMs (RLMs) should reflect this. We extend the Turing completeness result to the probabilistic case, showing how a rationally weighted RLM with unbounded computation time can simulate any deterministic probabilistic Turing machine (PTM) with rationally weighted transitions. Since, in practice, RLMs work in real-time, processing a symbol at every time step, we treat the above result as an upper bound on the expressivity of RLMs. We also provide a lower bound by showing that under the restriction to real-time computation, such models can simulate deterministic real-time rational PTMs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.434.pdf",
        "keywords": [
            "recurrent neural networks",
            "recurrent neural language",
            "probabilistic turing machine",
            "expressivity"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Since, in practice, RLMs work in real-time, processing a symbol at every time step, we treat the above result as an upper bound on the expressivity of RLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Since, in practice, RLMs work in real-time, processing a symbol at every time step, we treat the above result as an upper bound on the expressivity of RLMs.\""
    },
    {
        "title": "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis",
        "authors": [
            "Alessandro Stolfo",
            "Yonatan Belinkov",
            "Mrinmaya Sachan"
        ],
        "published": "2023",
        "summary": "Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture. In order to improve our understanding of this aspect of language models, we present a mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework. By intervening on the activations of specific model components and measuring the resulting changes in predicted probabilities, we identify the subset of parameters responsible for specific predictions. This provides insights into how information related to arithmetic is processed by LMs. Our experimental results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism. Then, this information is processed by a set of MLP modules, which generate result-related information that is incorporated into the residual stream. To assess the specificity of the observed activation dynamics, we compare the effects of different model components on arithmetic queries with other tasks, including number retrieval from prompts and factual knowledge questions.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.435.pdf",
        "keywords": [
            "arithmetic",
            "arithmetic reasoning",
            "causal mediation analysis",
            "mathematical reasoning",
            "language models",
            "large language models",
            "mechanistic interpretation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture.\"\n\n(Note: The abstract mentions a limitation of LLMs, which is the limited understanding of how they process and store information related to arithmetic tasks, but it is not the",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture.\"\n\n(Note: The abstract mentions a limitation of LLMs, which is the limited understanding of how they process and store information related to arithmetic tasks, but it is not the"
    },
    {
        "title": "Benchmarking and Improving Text-to-SQL Generation under Ambiguity",
        "authors": [
            "Adithya Bhaskar",
            "Tushar Tomar",
            "Ashutosh Sathe",
            "Sunita Sarawagi"
        ],
        "published": "2023",
        "summary": "Research in Text-to-SQL conversion has been largely benchmarked against datasets where each text query corresponds to one correct SQL. However, natural language queries over real-life databases frequently involve significant ambiguity about the intended SQL due to overlapping schema names and multiple confusing relationship paths. To bridge this gap, we develop a novel benchmark called AmbiQT with over 3000 examples where each text is interpretable as two plausible SQLs due to lexical and/or structural ambiguity. When faced with ambiguity, an ideal top-k decoder should generate all valid interpretations for possible disambiguation by the user. We evaluate several Text-to-SQL systems and decoding algorithms, including those employing state-of-the-art LLMs, and find them to be far from this ideal. The primary reason is that the prevalent beam search algorithm and its variants, treat SQL queries as a string and produce unhelpful token-level diversity in the top-k. We propose LogicalBeam, a new decoding algorithm that navigates the SQL logic space using a blend of plan-based template generation and constrained infilling. Counterfactually generated plans diversify templates while in-filling with a beam-search that branches solely on schema names provides value diversity. LogicalBeam is up to 2.5 times more effective than state-of-the-art models at generating all candidate SQLs in the top-k ranked outputs. It also enhances the top-5 Exact and Execution Match Accuracies on SPIDER and Kaggle DBQA.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.436.pdf",
        "keywords": [
            "benchmarking",
            "ambiguity",
            "text to sql systems",
            "sqls",
            "sql generation",
            "beam search",
            "accuracies"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We evaluate several Text-to-SQL systems and decoding algorithms, including those employing state-of-the-art LLMs, and find them to be far from this ideal.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We evaluate several Text-to-SQL systems and decoding algorithms, including those employing state-of-the-art LLMs, and find them to be far from this ideal.\""
    },
    {
        "title": "Translating away Translationese without Parallel Data",
        "authors": [
            "Rricha Jalota",
            "Koel Chowdhury",
            "Cristina España-Bonet",
            "Josef van Genabith"
        ],
        "published": "2023",
        "summary": "Translated texts exhibit systematic linguistic differences compared to original texts in the same language, and these differences are referred to as translationese. Translationese has effects on various cross-lingual natural language processing tasks, potentially leading to biased results. In this paper, we explore a novel approach to reduce translationese in translated texts: translation-based style transfer. As there are no parallel human-translated and original data in the same language, we use a self-supervised approach that can learn from comparable (rather than parallel) mono-lingual original and translated data. However, even this self-supervised approach requires some parallel data for validation. We show how we can eliminate the need for parallel validation data by combining the self-supervised loss with an unsupervised loss. This unsupervised loss leverages the original language model loss over the style-transferred output and a semantic similarity loss between the input and style-transferred output. We evaluate our approach in terms of original vs. translationese binary classification in addition to measuring content preservation and target-style fluency. The results show that our approach is able to reduce translationese classifier accuracy to a level of a random classifier after style transfer while adequately preserving the content and fluency in the target original style.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.438.pdf",
        "keywords": [
            "translated",
            "translationese",
            "translationese classifier accuracy",
            "parallel data",
            "language model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, even this self-supervised approach requires some parallel data for validation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, even this self-supervised approach requires some parallel data for validation.\""
    },
    {
        "title": "Prompt-Based Monte-Carlo Tree Search for Goal-oriented Dialogue Policy Planning",
        "authors": [
            "Xiao Yu",
            "Maximillian Chen",
            "Zhou Yu"
        ],
        "published": "2023",
        "summary": "Planning for goal-oriented dialogue often requires simulating future dialogue interactions and estimating task progress. Many approaches thus consider training neural networks to perform look-ahead search algorithms such as A* search and Monte Carlo Tree Search (MCTS). However, this training often require abundant annotated data, which creates challenges when faced with noisy annotations or low-resource settings. We introduce GDP-Zero, an approach using Open-Loop MCTS to perform goal-oriented dialogue policy planning without any model training. GDP-Zero prompts a large language model to act as a policy prior, value function, user simulator, and system model during the tree search. We evaluate GDP-Zero on the goal-oriented task PersuasionForGood, and find that its responses are preferred over ChatGPT up to 59.32% of the time, and are rated more persuasive than ChatGPT during interactive evaluations.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.439.pdf",
        "keywords": [
            "tree search",
            "dialogue policy planning",
            "dialogue policy",
            "future",
            "monte carlo tree search",
            "goal"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but the paper proposes an approach that utilizes a large language model in a specific way, implying that existing methods may have limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations, but the paper proposes an approach that utilizes a large language model in a specific way, implying that existing methods may have limitations."
    },
    {
        "title": "UniMath: A Foundational and Multimodal Mathematical Reasoner",
        "authors": [
            "Zhenwen Liang",
            "Tianyu Yang",
            "Jipeng Zhang",
            "Xiangliang Zhang"
        ],
        "published": "2023",
        "summary": "While significant progress has been made in natural language processing (NLP), existing methods exhibit limitations in effectively interpreting and processing diverse mathematical modalities. Therefore, we introduce UniMath, a versatile and unified system designed for multimodal mathematical reasoning tasks. Tackling complex problem-solving in arithmetic, geometry, and table-based math, UniMath utilizes a fine-tuned T5 model augmented with a variational autoencoder (VAE)-based image tokenizer. By jointly training and evaluating the model on three diverse datasets - SVAMP, GeoQA, and TableMWP, UniMath achieves state-of-the-art performance. The model’s generalization ability is further demonstrated via fine-tuning on two additional datasets, MathQA and Geo-Proving. Through comprehensive evaluations, we showcase that joint training across diverse math tasks improves overall model performance and enhances its ability to generalize across different mathematical reasoning tasks. This pioneering approach provides a blueprint and inspires further efforts on unified mathematical reasoning with deep learning systems.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.440.pdf",
        "keywords": [
            "unimath",
            "multimodal mathematical reasoner",
            "natural language processing"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While significant progress has been made in natural language processing (NLP), existing methods exhibit limitations in effectively interpreting and processing diverse mathematical modalities.\"\n\nNote that this paper is rated 1 because it mentions limitations of existing methods in NLP, which can be inferred to include LLMs, but does not explicitly discuss limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"While significant progress has been made in natural language processing (NLP), existing methods exhibit limitations in effectively interpreting and processing diverse mathematical modalities.\"\n\nNote that this paper is rated 1 because it mentions limitations of existing methods in NLP, which can be inferred to include LLMs, but does not explicitly discuss limitations of LLMs."
    },
    {
        "title": "CaseEncoder: A Knowledge-enhanced Pre-trained Model for Legal Case Encoding",
        "authors": [
            "Yixiao Ma",
            "Yueyue Wu",
            "Weihang Su",
            "Qingyao Ai",
            "Yiqun Liu"
        ],
        "published": "2023",
        "summary": "Legal case retrieval is a critical process for modern legal information systems. While recent studies have utilized pre-trained language models (PLMs) based on the general domain self-supervised pre-training paradigm to build models for legal case retrieval, there are limitations in using general domain PLMs as backbones. Specifically, these models may not fully capture the underlying legal features in legal case documents. To address this issue, we propose CaseEncoder, a legal document encoder that leverages fine-grained legal knowledge in both the data sampling and pre-training phases. In the data sampling phase, we enhance the quality of the training data by utilizing fine-grained law article information to guide the selection of positive and negative examples. In the pre-training phase, we design legal-specific pre-training tasks that align with the judging criteria of relevant legal cases. Based on these tasks, we introduce an innovative loss function called Biased Circle Loss to enhance the model’s ability to recognize case relevance in fine grains. Experimental results on multiple benchmarks demonstrate that CaseEncoder significantly outperforms both existing general pre-training models and legal-specific pre-training models in zero-shot legal case retrieval. The source code of CaseEncoder can be found at https://github.com/Anonymous-EMNLP2023/CaseEncoder.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.441.pdf",
        "keywords": [
            "pre trained language models",
            "legal case encoding"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"there are limitations in using general domain PLMs as backbones. Specifically, these models may not fully capture the underlying legal features in legal case documents.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"there are limitations in using general domain PLMs as backbones. Specifically, these models may not fully capture the underlying legal features in legal case documents.\""
    },
    {
        "title": "HiddenTables and PyQTax: A Cooperative Game and Dataset For TableQA to Ensure Scale and Data Privacy Across a Myriad of Taxonomies",
        "authors": [
            "William Watson",
            "Nicole Cho",
            "Tucker Balch",
            "Manuela Veloso"
        ],
        "published": "2023",
        "summary": "A myriad of different Large Language Models (LLMs) face a common challenge in contextually analyzing table question-answering tasks. These challenges are engendered from (1) finite context windows for large tables, (2) multi-faceted discrepancies amongst tokenization patterns against cell boundaries, and (3) various limitations stemming from data confidentiality in the process of using external models such as gpt-35-turbo. We propose a cooperative game dubbed “HiddenTables” as a potential resolution to this challenge. In essence, “HiddenTables” is played between the code-generating LLM “Solver” and the “Oracle” which evaluates the ability of the LLM agents to solve TableQA tasks. This game is based on natural language schemas and importantly, ensures the security of the underlying data. We provide evidential experiments on a diverse set of tables that demonstrate an LLM’s collective inability to generalize and perform on complex queries, handle compositional dependencies, and align natural language to programmatic commands when concrete table schemas are provided. Unlike encoder-based models, we have pushed the boundaries of “HiddenTables” to not be limited by the number of rows - therefore we exhibit improved efficiency in prompt and completion tokens. Our infrastructure has spawned a new dataset “PyQTax” that spans across 116,671 question-table-answer triplets and provides additional fine-grained breakdowns and labels for varying question taxonomies. Therefore, in tandem with our academic contributions regarding LLMs’ deficiency in TableQA tasks, “HiddenTables” is a tactile manifestation of how LLMs can interact with massive datasets while ensuring data security and minimizing generation costs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.442.pdf",
        "keywords": [
            "pyqtax",
            "cooperative game",
            "hiddentables",
            "schemas",
            "natural language schemas",
            "question taxonomies"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A myriad of different Large Language Models (LLMs) face a common challenge in contextually analyzing table question-answering tasks. These challenges are engendered from (1) finite context windows for large tables, (2) multi-faceted discrepancies amongst tokenization patterns against cell boundaries, and (3) various limitations stemming from data confidentiality in the process of using external",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"A myriad of different Large Language Models (LLMs) face a common challenge in contextually analyzing table question-answering tasks. These challenges are engendered from (1) finite context windows for large tables, (2) multi-faceted discrepancies amongst tokenization patterns against cell boundaries, and (3) various limitations stemming from data confidentiality in the process of using external"
    },
    {
        "title": "Causal Document-Grounded Dialogue Pre-training",
        "authors": [
            "Yingxiu Zhao",
            "Bowen Yu",
            "Bowen Li",
            "Haiyang Yu",
            "Jinyang Li",
            "Chao Wang",
            "Fei Huang",
            "Yongbin Li",
            "Nevin Zhang"
        ],
        "published": "2023",
        "summary": "The goal of document-grounded dialogue (DocGD) is to generate a response by anchoring the evidence in a supporting document in accordance with the dialogue context. This entails four causally interconnected variables. While task-specific pre-training has significantly enhanced performances on numerous downstream tasks, existing DocGD methods still rely on general pre-trained language models without a specifically tailored pre-training approach that explicitly captures the causal relationships. To address this, we present the first causally-complete dataset construction strategy for developing million-scale DocGD pre-training corpora. Additionally, we propose a causally-perturbed pre-training strategy to better capture causality by introducing perturbations on the variables and optimizing the overall causal effect. Experiments conducted on three benchmark datasets demonstrate that our causal pre-training yields substantial and consistent improvements in fully-supervised, low-resource, few-shot, and zero-shot settings.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.443.pdf",
        "keywords": [
            "training",
            "pre training",
            "dialogue pre training",
            "dialogue",
            "document"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"existing DocGD methods still rely on general pre-trained language models without a specifically tailored pre-training approach that explicitly captures the causal relationships.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"existing DocGD methods still rely on general pre-trained language models without a specifically tailored pre-training approach that explicitly captures the causal relationships.\""
    },
    {
        "title": "DUMB: A Benchmark for Smart Evaluation of Dutch Models",
        "authors": [
            "Wietse de Vries",
            "Martijn Wieling",
            "Malvina Nissim"
        ],
        "published": "2023",
        "summary": "We introduce the Dutch Model Benchmark: DUMB. The benchmark includes a diverse set of datasets for low-, medium- and high-resource tasks. The total set of nine tasks includes four tasks that were previously not available in Dutch. Instead of relying on a mean score across tasks, we propose Relative Error Reduction (RER), which compares the DUMB performance of language models to a strong baseline which can be referred to in the future even when assessing different sets of language models. Through a comparison of 14 pre-trained language models (mono- and multi-lingual, of varying sizes), we assess the internal consistency of the benchmark tasks, as well as the factors that likely enable high performance. Our results indicate that current Dutch monolingual models under-perform and suggest training larger Dutch models with other architectures and pre-training objectives. At present, the highest performance is achieved by DeBERTaV3 (large), XLM-R (large) and mDeBERTaV3 (base). In addition to highlighting best strategies for training larger Dutch models, DUMB will foster further research on Dutch. A public leaderboard is available at https://dumbench.nl.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.447.pdf",
        "keywords": [
            "smart evaluation",
            "language models",
            "models",
            "relative error reduction",
            "model benchmark",
            "benchmark",
            "dumb"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our results indicate that current Dutch monolingual models under-perform and suggest training larger Dutch models with other architectures and pre-training objectives.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our results indicate that current Dutch monolingual models under-perform and suggest training larger Dutch models with other architectures and pre-training objectives.\""
    },
    {
        "title": "Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4",
        "authors": [
            "Kent Chang",
            "Mackenzie Cramer",
            "Sandeep Soni",
            "David Bamman"
        ],
        "published": "2023",
        "summary": "In this work, we carry out a data archaeology to infer books that are known to ChatGPT and GPT-4 using a name cloze membership inference query. We find that OpenAI models have memorized a wide collection of copyrighted materials, and that the degree of memorization is tied to the frequency with which passages of those books appear on the web. The ability of these models to memorize an unknown set of books complicates assessments of measurement validity for cultural analytics by contaminating test data; we show that models perform much better on memorized books than on non-memorized books for downstream tasks. We argue that this supports a case for open models whose training data is known.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.453.pdf",
        "keywords": [
            "archaeology",
            "memorized books"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The ability of these models to memorize an unknown set of books complicates assessments of measurement validity for cultural analytics by contaminating test data; we show that models perform much better on memorized books than on non-memorized books for downstream tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"The ability of these models to memorize an unknown set of books complicates assessments of measurement validity for cultural analytics by contaminating test data; we show that models perform much better on memorized books than on non-memorized books for downstream tasks.\""
    },
    {
        "title": "A Study on Accessing Linguistic Information in Pre-Trained Language Models by Using Prompts",
        "authors": [
            "Marion Di Marco",
            "Katharina Hämmerl",
            "Alexander Fraser"
        ],
        "published": "2023",
        "summary": "We study whether linguistic information in pre-trained multilingual language models can be accessed by human language: So far, there is no easy method to directly obtain linguistic information and gain insights into the linguistic principles encoded in such models. We use the technique of prompting and formulate linguistic tasks to test the LM’s access to explicit grammatical principles and study how effective this method is at providing access to linguistic features. Our experiments on German, Icelandic and Spanish show that some linguistic properties can in fact be accessed through prompting, whereas others are harder to capture.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.454.pdf",
        "keywords": [
            "linguistic",
            "grammatical",
            "multilingual language models",
            "prompts"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"So far, there is no easy method to directly obtain linguistic information and gain insights into the linguistic principles encoded in such models... whereas others are harder to capture.\"\n\nThis rating is chosen because the paper discusses a limitation of LLMs, specifically the difficulty in accessing linguistic information, and provides some analysis and examples of this limitation. However, the primary focus of the paper is on the",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"So far, there is no easy method to directly obtain linguistic information and gain insights into the linguistic principles encoded in such models... whereas others are harder to capture.\"\n\nThis rating is chosen because the paper discusses a limitation of LLMs, specifically the difficulty in accessing linguistic information, and provides some analysis and examples of this limitation. However, the primary focus of the paper is on the"
    },
    {
        "title": "From Heuristic to Analytic: Cognitively Motivated Strategies for Coherent Physical Commonsense Reasoning",
        "authors": [
            "Zheyuan Zhang",
            "Shane Storks",
            "Fengyuan Hu",
            "Sungryull Sohn",
            "Moontae Lee",
            "Honglak Lee",
            "Joyce Chai"
        ],
        "published": "2023",
        "summary": "Pre-trained language models (PLMs) have shown impressive performance in various language tasks. However, they are prone to spurious correlations, and often generate illusory information. In real-world applications, PLMs should justify decisions with formalized, coherent reasoning chains, but this challenge remains under-explored. Cognitive psychology theorizes that humans are capable of utilizing fast and intuitive *heuristic* thinking to make decisions based on past experience, then rationalizing the decisions through slower and deliberative *analytic* reasoning. We incorporate these interlinked dual processes in fine-tuning and in-context learning with PLMs, applying them to two language understanding tasks that require coherent physical commonsense reasoning. We show that our proposed Heuristic-Analytic Reasoning (HAR) strategies drastically improve the coherence of rationalizations for model decisions, yielding state-of-the-art results on Tiered Reasoning for Intuitive Physics (TRIP). We also find that this improved coherence is a direct result of more faithful attention to relevant language context in each step of reasoning. Our findings suggest that human-like reasoning strategies can effectively improve the coherence and reliability of PLM reasoning.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.456.pdf",
        "keywords": [
            "commonsense",
            "heuristic",
            "coherent",
            "heuristic analytic reasoning",
            "cognitive psychology",
            "tiered reasoning",
            "pre trained language models",
            "physical commonsense reasoning",
            "reasoning strategies"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they are prone to spurious correlations, and often generate illusory information.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, they are prone to spurious correlations, and often generate illusory information.\""
    },
    {
        "title": "Copyright Violations and Large Language Models",
        "authors": [
            "Antonia Karamolegkou",
            "Jiaang Li",
            "Li Zhou",
            "Anders Søgaard"
        ],
        "published": "2023",
        "summary": "Language models may memorize more than just facts, including entire chunks of texts seen during training. Fair use exemptions to copyright laws typically allow for limited use of copyrighted material without permission from the copyright holder, but typically for extraction of information from copyrighted materials, rather than verbatim reproduction. This work explores the issue of copyright violations and large language models through the lens of verbatim memorization, focusing on possible redistribution of copyrighted text. We present experiments with a range of language models over a collection of popular books and coding problems, providing a conservative characterization of the extent to which language models can redistribute these materials. Overall, this research highlights the need for further examination and the potential impact on future developments in natural language processing to ensure adherence to copyright regulations. Code is at https://github.com/coastalcph/CopyrightLLMs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.458.pdf",
        "keywords": [
            "copyright",
            "language models",
            "copyright violations",
            "copyright laws",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Language models may memorize more than just facts, including entire chunks of texts seen during training... We present experiments with a range of language models over a collection of popular books and coding problems, providing a conservative characterization of the extent to which language models can redistribute these materials.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Language models may memorize more than just facts, including entire chunks of texts seen during training... We present experiments with a range of language models over a collection of popular books and coding problems, providing a conservative characterization of the extent to which language models can redistribute these materials.\""
    },
    {
        "title": "Effects of sub-word segmentation on performance of transformer language models",
        "authors": [
            "Jue Hou",
            "Anisia Katinskaia",
            "Anh-Duc Vu",
            "Roman Yangarber"
        ],
        "published": "2023",
        "summary": "Language modeling is a fundamental task in natural language processing, which has been thoroughly explored with various architectures and hyperparameters. However, few studies focus on the effect of sub-word segmentation on the performance of language models (LMs). In this paper, we compare GPT and BERT models trained with the statistical segmentation algorithm BPE vs. two unsupervised algorithms for morphological segmentation — Morfessor and StateMorph. We train the models for several languages — including ones with very rich morphology — and compare their performance with different segmentation algorithms, vocabulary sizes, and model sizes. The results show that training with morphological segmentation allows the LMs to: (1) achieve lower perplexity, (2) converge more efficiently in terms of training time, and (3) achieve equivalent or better evaluation scores on downstream tasks. Lastly, we show that (4) LMs of smaller size using morphological segmentation can perform comparably to models of larger size trained with BPE — both in terms of (1) perplexity and (3) scores on downstream tasks. Points (2) and (4) impact on sustainability, since they reduce the model cost; and while 2 reduces cost only in the training phase, 4 does so also in the inference phase.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.459.pdf",
        "keywords": [
            "language modeling",
            "transformer language models",
            "sub word segmentation",
            "morphological segmentation",
            "morphological"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Points (2) and (4) impact on sustainability, since they reduce the model cost; and while 2 reduces cost only in the training phase, 4 does so also in the inference phase.\"\n\nThis rating is given because the paper mentions a limitation of LLMs (high model cost) but does not explore it in depth and focuses on the benefits of morphological segmentation",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Points (2) and (4) impact on sustainability, since they reduce the model cost; and while 2 reduces cost only in the training phase, 4 does so also in the inference phase.\"\n\nThis rating is given because the paper mentions a limitation of LLMs (high model cost) but does not explore it in depth and focuses on the benefits of morphological segmentation"
    },
    {
        "title": "Symbolic Planning and Code Generation for Grounded Dialogue",
        "authors": [
            "Justin Chiu",
            "Wenting Zhao",
            "Derek Chen",
            "Saujas Vaduguru",
            "Alexander Rush",
            "Daniel Fried"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) excel at processing and generating text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and interpretable grounded dialogue system that addresses these shortcomings by composing LLMs with a symbolic planner and grounded code execution. Our system, consists of a reader and planner: the reader leverages an LLM to convert partner utterances into executable code, calling functions that perform grounding. The translated code’s output is stored to track dialogue state, while a symbolic planner determines the next appropriate response. We evaluate our system’s performance on the demanding OneCommon dialogue task, involving collaborative reference resolution on abstract images of scattered dots. Our system substantially outperforms the previous state-of-the-art, including improving task success in human evaluations from 56% to 69% in the most challenging setting.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.460.pdf",
        "keywords": [
            "dialogue",
            "symbolic planning",
            "code generation",
            "grounded dialogue",
            "grounded code",
            "grounding",
            "grounded dialogue system",
            "large language models",
            "grounded task"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding.\""
    },
    {
        "title": "Universal Self-Adaptive Prompting",
        "authors": [
            "Xingchen Wan",
            "Ruoxi Sun",
            "Hootan Nakhost",
            "Hanjun Dai",
            "Julian Eisenschlos",
            "Sercan Arik",
            "Tomas Pfister"
        ],
        "published": "2023",
        "summary": "A hallmark of modern large language models (LLMs) is their impressive general zero-shot and few-shot abilities, often elicited through in-context learning (ICL) via prompting. However, while highly coveted and being the most general, zero-shot performances in LLMs are still typically weaker due to the lack of guidance and the difficulty of applying existing automatic prompt design methods in general tasks when ground-truth labels are unavailable. In this study, we address this by presenting Universal Self-Adaptive Prompting (USP), an automatic prompt design approach specifically tailored for zero-shot learning (while compatible with few-shot). Requiring only a small amount of unlabeled data and an inference-only LLM, USP is highly versatile: to achieve universal prompting, USP categorizes a possible NLP task into one of the three possible task types and then uses a corresponding selector to select the most suitable queries and zero-shot model-generated responses as pseudo-demonstrations, thereby generalizing ICL to the zero-shot setup in a fully automated way. We evaluate USP with PaLM and PaLM 2 models and demonstrate performances that are considerably stronger than standard zero-shot baselines and often comparable to or even superior to few-shot baselines across more than 40 natural language understanding, natural language generation, and reasoning tasks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.461.pdf",
        "keywords": [
            "universal prompting",
            "universal self adaptive prompting"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"while highly coveted and being the most general, zero-shot performances in LLMs are still typically weaker due to the lack of guidance and the difficulty of applying existing automatic prompt design methods in general tasks when ground-truth labels are unavailable.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"while highly coveted and being the most general, zero-shot performances in LLMs are still typically weaker due to the lack of guidance and the difficulty of applying existing automatic prompt design methods in general tasks when ground-truth labels are unavailable.\""
    },
    {
        "title": "Beat LLMs at Their Own Game: Zero-Shot LLM-Generated Text Detection via Querying ChatGPT",
        "authors": [
            "Biru Zhu",
            "Lifan Yuan",
            "Ganqu Cui",
            "Yangyi Chen",
            "Chong Fu",
            "Bingxiang He",
            "Yangdong Deng",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Ming Gu"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs), e.g., ChatGPT, have revolutionized the domain of natural language processing because of their excellent performance on various tasks. Despite their great potential, LLMs also incur serious concerns as they are likely to be misused. There are already reported cases of academic cheating by using LLMs. Thus, it is a pressing problem to identify LLM-generated texts. In this work, we design a zero-shot black-box method for detecting LLM-generated texts. The key idea is to revise the text to be detected using the ChatGPT model. Our method is based on the intuition that the ChatGPT model will make fewer revisions to LLM-generated texts than it does to human-written texts, because the texts generated by LLMs are more in accord with the generation logic and statistical patterns learned by LLMs like ChatGPT. Thus, if the text to be detected and its ChatGPT-revised version have a higher degree of similarity, the text is more likely to be LLM-generated. Extensive experiments on various datasets and tasks show that our method can effectively detect LLM-generated texts. Moreover, compared with other detection methods, our method has better generalization ability and is more stable across various datasets. The codes are publicly available at https://github.com/thunlp/LLM-generated-text-detection.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.463.pdf",
        "keywords": [
            "zero shot black box",
            "large language models",
            "chatgpt model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite their great potential, LLMs also incur serious concerns as they are likely to be misused. There are already reported cases of academic cheating by using LLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite their great potential, LLMs also incur serious concerns as they are likely to be misused. There are already reported cases of academic cheating by using LLMs.\""
    },
    {
        "title": "Faithful Model Evaluation for Model-Based Metrics",
        "authors": [
            "Qian Hu",
            "Palash Goyal",
            "Rahul Gupta"
        ],
        "published": "2023",
        "summary": "Statistical significance testing is used in natural language processing (NLP) to determine whether the results of a study or experiment are likely to be due to chance or if they reflect a genuine relationship. A key step in significance testing is the estimation of confidence interval which is a function of sample variance. Sample variance calculation is straightforward when evaluating against ground truth. However, in many cases, a metric model is often used for evaluation. For example, to compare toxicity of two large language models, a toxicity classifier is used for evaluation. Existing works usually do not consider the variance change due to metric model errors, which can lead to wrong conclusions. In this work, we establish the mathematical foundation of significance testing for model-based metrics. With experiments on public benchmark datasets and a production system, we show that considering metric model errors to calculate sample variances for model-based metrics changes the conclusions in certain experiments.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.464.pdf",
        "keywords": [
            "significance testing",
            "sample variance",
            "model based metrics",
            "metric model",
            "model",
            "statistical significance testing",
            "faithful model evaluation",
            "natural language processing"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"For example, to compare toxicity of two large language models, a toxicity classifier is used for evaluation. Existing works usually do not consider the variance change due to metric model errors, which can lead to wrong conclusions.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs (metric model errors) but does not elaborate on it and uses it to justify the paper's contribution",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"For example, to compare toxicity of two large language models, a toxicity classifier is used for evaluation. Existing works usually do not consider the variance change due to metric model errors, which can lead to wrong conclusions.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs (metric model errors) but does not elaborate on it and uses it to justify the paper's contribution"
    },
    {
        "title": "Content- and Topology-Aware Representation Learning for Scientific Multi-Literature",
        "authors": [
            "Kai Zhang",
            "Kaisong Song",
            "Yangyang Kang",
            "Xiaozhong Liu"
        ],
        "published": "2023",
        "summary": "Representation learning forms an essential building block in the development of natural language processing architectures. To date, mainstream approaches focus on learning textual information at the sentence- or document-level, unfortunately, overlooking the inter-document connections. This omission decreases the potency of downstream applications, particularly in multi-document settings. To address this issue, embeddings equipped with latent semantic and rich relatedness information are needed. In this paper, we propose SMRC2, which extends representation learning to the multi-document level. Our model jointly learns latent semantic information from content and rich relatedness information from topological networks. Unlike previous studies, our work takes multi-document as input and integrates both semantic and relatedness information using a shared space via language model and graph structure. Our extensive experiments confirm the superiority and effectiveness of our approach. To encourage further research in scientific multi-literature representation learning, we will release our code and a new dataset from the biomedical domain.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.465.pdf",
        "keywords": [
            "topology",
            "topological networks",
            "language model",
            "representation learning",
            "latent semantic",
            "natural language processing",
            "graph structure",
            "multi document"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"unlike previous studies, our work takes multi-document as input and integrates both semantic and relatedness information using a shared space via language model\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"unlike previous studies, our work takes multi-document as input and integrates both semantic and relatedness information using a shared space via language model\""
    },
    {
        "title": "Language Model Quality Correlates with Psychometric Predictive Power in Multiple Languages",
        "authors": [
            "Ethan Wilcox",
            "Clara Meister",
            "Ryan Cotterell",
            "Tiago Pimentel"
        ],
        "published": "2023",
        "summary": "Surprisal theory (Hale, 2001; Levy, 2008) posits that a word’s reading time is proportional to its surprisal (i.e., to its negative log probability given the proceeding context). Since we are unable to access a word’s ground-truth probability, surprisal theory has been empirically tested using surprisal estimates from language models (LMs). Under the premise that surprisal theory holds, we would expect that higher quality language models provide more powerful predictors of human reading behavior—a conjecture we dub the quality–power (QP) hypothesis. Unfortunately, empirical support for the QP hypothesis is mixed. Some studies in English have found correlations between LM quality and predictive power, but other studies using Japanese data, as well as using larger English LMs, find no such correlations. In this work, we conduct a systematic crosslinguistic assessment of the QP hypothesis. We train LMs from scratch on small- and medium-sized datasets from 13 languages (across five language families) and assess their ability to predict eye tracking data. We find correlations between LM quality and power in eleven of these thirteen languages, suggesting that, within the range of model classes and sizes tested, better language models are indeed better predictors of human language processing behaviors.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.466.pdf",
        "keywords": [
            "predictive power",
            "power",
            "psychometric predictive power",
            "language models",
            "quality language",
            "surprisal",
            "quality",
            "surprisal theory",
            "predict eye tracking",
            "surprisal estimates",
            "language model quality correlates",
            "multiple languages",
            "negative log probability",
            "quality–power"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unfortunately, empirical support for the QP hypothesis is mixed. Some studies in English have found correlations between LM quality and predictive power, but other studies using Japanese data, as well as using larger English LMs, find no such correlations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Unfortunately, empirical support for the QP hypothesis is mixed. Some studies in English have found correlations between LM quality and predictive power, but other studies using Japanese data, as well as using larger English LMs, find no such correlations.\""
    },
    {
        "title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models",
        "authors": [
            "Daman Arora",
            "Himanshu Singh",
            "Mausam"
        ],
        "published": "2023",
        "summary": "The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompting, GPT-4 is unable to assess risk introduced by negative marking for incorrect answers. For this, we develop a post-hoc confidence-thresholding method over self-consistency, which enables effective response selection. We hope that our challenging benchmark will guide future re-search in problem-solving using LLMs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.468.pdf",
        "keywords": [
            "benchmark",
            "language models",
            "reasoning benchmarks"
        ],
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompting, GPT-4 is unable to assess risk introduced by negative marking for incorrect answers.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompting, GPT-4 is unable to assess risk introduced by negative marking for incorrect answers.\""
    },
    {
        "title": "WiCE: Real-World Entailment for Claims in Wikipedia",
        "authors": [
            "Ryo Kamoi",
            "Tanya Goyal",
            "Juan Diego Rodriguez",
            "Greg Durrett"
        ],
        "published": "2023",
        "summary": "Textual entailment models are increasingly applied in settings like fact-checking, presupposition verification in question answering, or summary evaluation. However, these represent a significant domain shift from existing entailment datasets, and models underperform as a result. We propose WiCE, a new fine-grained textual entailment dataset built on natural claim and evidence pairs extracted from Wikipedia. In addition to standard claim-level entailment, WiCE provides entailment judgments over sub-sentence units of the claim, and a minimal subset of evidence sentences that support each subclaim. To support this, we propose an automatic claim decomposition strategy using GPT-3.5 which we show is also effective at improving entailment models’ performance on multiple datasets at test time. Finally, we show that real claims in our dataset involve challenging verification and retrieval problems that existing models fail to address.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.470.pdf",
        "keywords": [
            "entailment",
            "textual entailment",
            "wikipedia",
            "verification",
            "presupposition verification",
            "claim level entailment",
            "retrieval",
            "claim decomposition"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Finally, we show that real claims in our dataset involve challenging verification and retrieval problems that existing models fail to address.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Finally, we show that real claims in our dataset involve challenging verification and retrieval problems that existing models fail to address.\""
    },
    {
        "title": "On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research",
        "authors": [
            "Luiza Pozzobon",
            "Beyza Ermis",
            "Patrick Lewis",
            "Sara Hooker"
        ],
        "published": "2023",
        "summary": "Perception of toxicity evolves over time and often differs between geographies and cultural backgrounds. Similarly, black-box commercially available APIs for detecting toxicity, such as the Perspective API, are not static, but frequently retrained to address any unattended weaknesses and biases. We evaluate the implications of these changes on the reproducibility of findings that compare the relative merits of models and methods that aim to curb toxicity. Our findings suggest that research that relied on inherited automatic toxicity scores to compare models and techniques may have resulted in inaccurate findings. Rescoring all models from HELM, a widely respected living benchmark, for toxicity with the recent version of the API led to a different ranking of widely used foundation models. We suggest caution in applying apples-to-apples comparisons between studies and call for a more structured approach to evaluating toxicity over time.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.472.pdf",
        "keywords": [
            "toxicity evaluation",
            "black box"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Instructive Dialogue Summarization with Query Aggregations",
        "authors": [
            "Bin Wang",
            "Zhengyuan Liu",
            "Nancy Chen"
        ],
        "published": "2023",
        "summary": "Conventional dialogue summarization methods directly generate summaries and do not consider user’s specific interests. This poses challenges in cases where the users are more focused on particular topics or aspects. With the advancement of instruction-finetuned language models, we introduce instruction-tuning to dialogues to expand the capability set of dialogue summarization models. To overcome the scarcity of instructive dialogue summarization data, we propose a three-step approach to synthesize high-quality query-based summarization triples. This process involves summary-anchored query generation, query filtering and query-based summary generation. By training a unified model called InstructDS (Instructive Dialogue Summarization) on three summarization datasets with multi-purpose instructive triples, we expand the capability of dialogue summarization models. We evaluate our method on four datasets, including dialogue summarization and dialogue reading comprehension. Experimental results show that our approach outperforms the state-of-the-art models and even models with larger sizes. Additionally, our model exhibits higher generalizability and faithfulness, as confirmed by human subjective evaluations.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.474.pdf",
        "keywords": [
            "summarization",
            "dialogue summarization",
            "dialogues",
            "query aggregations",
            "query based summarization",
            "instructive dialogue summarization",
            "instructive triples",
            "dialogue reading"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"With the advancement of instruction-finetuned language models...\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"With the advancement of instruction-finetuned language models...\""
    },
    {
        "title": "GLEN: Generative Retrieval via Lexical Index Learning",
        "authors": [
            "Sunkyung Lee",
            "Minjin Choi",
            "Jongwuk Lee"
        ],
        "published": "2023",
        "summary": "Generative retrieval shed light on a new paradigm of document retrieval, aiming to directly generate the identifier of a relevant document for a query. While it takes advantage of bypassing the construction of auxiliary index structures, existing studies face two significant challenges: (i) the discrepancy between the knowledge of pre-trained language models and identifiers and (ii) the gap between training and inference that poses difficulty in learning to rank. To overcome these challenges, we propose a novel generative retrieval method, namely Generative retrieval via LExical iNdex learning (GLEN). For training, GLEN effectively exploits a dynamic lexical identifier using a two-phase index learning strategy, enabling it to learn meaningful lexical identifiers and relevance signals between queries and documents. For inference, GLEN utilizes collision-free inference, using identifier weights to rank documents without additional overhead. Experimental results prove that GLEN achieves state-of-the-art or competitive performance against existing generative retrieval methods on various benchmark datasets, e.g., NQ320k, MS MARCO, and BEIR. The code is available at https://github.com/skleee/GLEN.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.477.pdf",
        "keywords": [
            "lexical index learning",
            "document retrieval"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the discrepancy between the knowledge of pre-trained language models and identifiers\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the discrepancy between the knowledge of pre-trained language models and identifiers\""
    },
    {
        "title": "Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study",
        "authors": [
            "Boxin Wang",
            "Wei Ping",
            "Peng Xu",
            "Lawrence McAfee",
            "Zihan Liu",
            "Mohammad Shoeybi",
            "Yi Dong",
            "Oleksii Kuchaiev",
            "Bo Li",
            "Chaowei Xiao",
            "Anima Anandkumar",
            "Bryan Catanzaro"
        ],
        "published": "2023",
        "summary": "Large decoder-only language models (LMs) can be largely improved in terms of perplexity by retrieval (e.g., RETRO), but its impact on text generation quality and downstream task accuracy is unclear. Thus, it is still an open question: shall we pretrain large autoregressive LMs with retrieval? To answer it, we perform a comprehensive study on a scalable pre-trained retrieval-augmented LM (i.e., RETRO) compared with standard GPT and retrieval-augmented GPT incorporated at fine-tuning or inference stages. We first provide the recipe to reproduce RETRO up to 9.5B parameters while retrieving a text corpus with 330B tokens. Based on that, we have the following novel findings: i) RETRO outperforms GPT on text generation with much less degeneration (i.e., repetition), moderately higher factual accuracy, and slightly lower toxicity with a nontoxic retrieval database. ii) On the LM Evaluation Harness benchmark, RETRO largely outperforms GPT on knowledge-intensive tasks, but is on par with GPT on other tasks. Furthermore, we introduce a simple variant of the model, RETRO++, which largely improves open-domain QA results of original RETRO (e.g., EM score +8.6 on Natural Question) and significantly outperforms retrieval-augmented GPT across different model sizes. Our findings highlight the promising direction of pretraining autoregressive LMs with retrieval as future foundation models. We release our implementation at: https://github.com/NVIDIA/Megatron-LM/tree/main/tools/retro.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.482.pdf",
        "keywords": [
            "perplexity"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large decoder-only language models (LMs) can be largely improved in terms of perplexity by retrieval... but its impact on text generation quality and downstream task accuracy is unclear.\"\n\nThis rating is given because the abstract mentions a limitation of large decoder-only language models (LMs) in terms of text generation quality and downstream task accuracy when using retrieval, but it is not the primary focus",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large decoder-only language models (LMs) can be largely improved in terms of perplexity by retrieval... but its impact on text generation quality and downstream task accuracy is unclear.\"\n\nThis rating is given because the abstract mentions a limitation of large decoder-only language models (LMs) in terms of text generation quality and downstream task accuracy when using retrieval, but it is not the primary focus"
    },
    {
        "title": "SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables",
        "authors": [
            "Xinyuan Lu",
            "Liangming Pan",
            "Qian Liu",
            "Preslav Nakov",
            "Min-Yen Kan"
        ],
        "published": "2023",
        "summary": "Current scientific fact-checking benchmarks exhibit several shortcomings, such as biases arising from crowd-sourced claims and an over-reliance on text-based evidence. We present SCITAB, a challenging evaluation dataset consisting of 1.2K expert-verified scientific claims that 1) originate from authentic scientific publications and 2) require compositional reasoning for verification. The claims are paired with evidence-containing scientific tables annotated with labels. Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of-Thought, do not achieve much performance gains on SCITAB. Our analysis uncovers several unique challenges posed by SCITAB, including table grounding, claim ambiguity, and compositional reasoning. Our codes and data are publicly available at https://github.com/XinyuanLu00/SciTab.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.483.pdf",
        "keywords": [
            "scitab",
            "claim verification",
            "verification",
            "compositional reasoning",
            "fact checking",
            "benchmark"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing.\""
    },
    {
        "title": "Task-Agnostic Low-Rank Adapters for Unseen English Dialects",
        "authors": [
            "Zedian Xiao",
            "William Held",
            "Yanchen Liu",
            "Diyi Yang"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) are trained on corpora disproportionally weighted in favor of Standard American English. As a result, speakers of other dialects experience significantly more failures when interacting with these technologies. In practice, these speakers often accommodate their speech to be better understood. Our work shares the belief that language technologies should be designed to accommodate the diversity in English dialects and not the other way around. However, prior work on dialect struggle with generalizing to evolving and emerging dialects in a scalable manner. To fill this gap, our method, HyperLoRA, leverages expert linguistic knowledge to enable resource-efficient adaptation via hypernetworks. By disentangling dialect-specific and cross-dialectal information, HyperLoRA improves generalization to unseen dialects in a task-agnostic fashion. Not only is HyperLoRA more scalable in the number of parameters, but it also achieves the best or most competitive performance across 5 dialects in a zero-shot setting. In this way, our approach facilitates access to language technology for billions of English dialect speakers who are traditionally underrepresented.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.487.pdf",
        "keywords": [
            "language models"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large Language Models (LLMs) are trained on corpora disproportionally weighted in favor of Standard American English. As a result, speakers of other dialects experience significantly more failures when interacting with these technologies.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Large Language Models (LLMs) are trained on corpora disproportionally weighted in favor of Standard American English. As a result, speakers of other dialects experience significantly more failures when interacting with these technologies.\""
    },
    {
        "title": "Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization",
        "authors": [
            "Tianshi Che",
            "Ji Liu",
            "Yang Zhou",
            "Jiaxiang Ren",
            "Jiwen Zhou",
            "Victor Sheng",
            "Huaiyu Dai",
            "Dejing Dou"
        ],
        "published": "2023",
        "summary": "Federated learning (FL) is a promising paradigm to enable collaborative model training with decentralized data. However, the training process of Large Language Models (LLMs) generally incurs the update of significant parameters, which limits the applicability of FL techniques to tackle the LLMs in real scenarios. Prompt tuning can significantly reduce the number of parameters to update, but it either incurs performance degradation or low training efficiency. The straightforward utilization of prompt tuning in the FL often raises non-trivial communication costs and dramatically degrades performance. In addition, the decentralized data is generally non-Independent and Identically Distributed (non-IID), which brings client drift problems and thus poor performance. This paper proposes a Parameter-efficient prompt Tuning approach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and effective FL of LLMs. First, an efficient partial prompt tuning approach is proposed to improve performance and efficiency simultaneously. Second, a novel adaptive optimization method is developed to address the client drift problems on both the device and server sides to enhance performance further. Extensive experiments based on 10 datasets demonstrate the superb performance (up to 60.8% in terms of accuracy) and efficiency (up to 97.59% in terms of training time) of FedPepTAO compared with 9 baseline approaches. Our code is available at https://github.com/llm-eff/FedPepTAO.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.488.pdf",
        "keywords": [
            "adaptive optimization",
            "tuning",
            "federated learning"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the training process of Large Language Models (LLMs) generally incurs the update of significant parameters, which limits the applicability of FL techniques to tackle the LLMs in real scenarios.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, the training process of Large Language Models (LLMs) generally incurs the update of significant parameters, which limits the applicability of FL techniques to tackle the LLMs in real scenarios.\""
    },
    {
        "title": "TheoremQA: A Theorem-driven Question Answering Dataset",
        "authors": [
            "Wenhu Chen",
            "Ming Yin",
            "Max Ku",
            "Pan Lu",
            "Yixin Wan",
            "Xueguang Ma",
            "Jianyu Xu",
            "Xinyi Wang",
            "Tony Xia"
        ],
        "published": "2023",
        "summary": "The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in solving fundamental math problems like GSM8K by achieving over 90% accuracy. However, their capabilities to solve more challenging math problems which require domain-specific knowledge (i.e. theorem) have yet to be investigated. In this paper, we introduce TheoremQA, the first theorem-driven question-answering dataset designed to evaluate AI models’ capabilities to apply theorems to solve challenging science problems. TheoremQA is curated by domain experts containing 800 high-quality questions covering 350 theorems from Math, Physics, EE&CS, and Finance. We evaluate a wide spectrum of 16 large language and code models with different prompting strategies like Chain-of-Thoughts and Program-of-Thoughts. We found that GPT-4’s capabilities to solve these problems are unparalleled, achieving an accuracy of 51% with Program-of-Thoughts Prompting. All the existing open-sourced models are below 15%, barely surpassing the random-guess baseline. Given the diversity and broad coverage of TheoremQA, we believe it can be used as a better benchmark to evaluate LLMs’ capabilities to solve challenging science problems.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.489.pdf",
        "keywords": [
            "question answering dataset"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"All the existing open-sourced models are below 15%, barely surpassing the random-guess baseline.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"All the existing open-sourced models are below 15%, barely surpassing the random-guess baseline.\""
    },
    {
        "title": "Don’t Trust ChatGPT when your Question is not in English: A Study of Multilingual Abilities and Types of LLMs",
        "authors": [
            "Xiang Zhang",
            "Senyu Li",
            "Bradley Hauer",
            "Ning Shi",
            "Grzegorz Kondrak"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have demonstrated exceptional natural language understanding abilities, and have excelled in a variety of natural language processing (NLP) tasks. Despite the fact that most LLMs are trained predominantly on English, multiple studies have demonstrated their capabilities in a variety of languages. However, fundamental questions persist regarding how LLMs acquire their multilingual abilities and how performance varies across different languages. These inquiries are crucial for the study of LLMs since users and researchers often come from diverse language backgrounds, potentially influencing how they use LLMs and interpret their output. In this work, we propose a systematic way of qualitatively and quantitatively evaluating the multilingual capabilities of LLMs. We investigate the phenomenon of cross-language generalization in LLMs, wherein limited multilingual training data leads to advanced multilingual capabilities. To accomplish this, we employ a novel prompt back-translation method. The results demonstrate that LLMs, such as GPT, can effectively transfer learned knowledge across different languages, yielding relatively consistent results in translation-equivariant tasks, in which the correct output does not depend on the language of the input. However, LLMs struggle to provide accurate results in translation-variant tasks, which lack this property, requiring careful user judgment to evaluate the answers.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.491.pdf",
        "keywords": [
            "natural language processing",
            "multilingual capabilities",
            "large language models",
            "abilities"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, LLMs struggle to provide accurate results in translation-variant tasks, which lack this property, requiring careful user judgment to evaluate the answers.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, LLMs struggle to provide accurate results in translation-variant tasks, which lack this property, requiring careful user judgment to evaluate the answers.\""
    },
    {
        "title": "M3Seg: A Maximum-Minimum Mutual Information Paradigm for Unsupervised Topic Segmentation in ASR Transcripts",
        "authors": [
            "Ke Wang",
            "Xiutian Zhao",
            "Yanghui Li",
            "Wei Peng"
        ],
        "published": "2023",
        "summary": "Topic segmentation aims to detect topic boundaries and split automatic speech recognition transcriptions (e.g., meeting transcripts) into segments that are bounded by thematic meanings. In this work, we propose M3Seg, a novel Maximum-Minimum Mutual information paradigm for linear topic segmentation without using any parallel data. Specifically, by employing sentence representations provided by pre-trained language models, M3Seg first learns a region-based segment encoder based on the maximization of mutual information between the global segment representation and the local contextual sentence representation. Secondly, an edge-based boundary detection module aims to segment the whole by topics based on minimizing the mutual information between different segments. Experiment results on two public datasets demonstrate the effectiveness of M3Seg, which outperform the state-of-the-art methods by a significant (18%–37% improvement) margin.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.492.pdf",
        "keywords": [
            "segments",
            "topic segmentation",
            "mutual information",
            "minimum mutual information",
            "maximum minimum mutual information",
            "boundary detection",
            "asr"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"by employing sentence representations provided by pre-trained language models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"by employing sentence representations provided by pre-trained language models\""
    },
    {
        "title": "Empirical Study of Zero-Shot NER with ChatGPT",
        "authors": [
            "Tingyu Xie",
            "Qi Li",
            "Jian Zhang",
            "Yan Zhang",
            "Zuozhu Liu",
            "Hongwei Wang"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) exhibited powerful capability in various natural language processing tasks. This work focuses on exploring LLM performance on zero-shot information extraction, with a focus on the ChatGPT and named entity recognition (NER) task. Inspired by the remarkable reasoning capability of LLM on symbolic and arithmetic reasoning, we adapt the prevalent reasoning methods to NER and propose reasoning strategies tailored for NER. First, we explore a decomposed question-answering paradigm by breaking down the NER task into simpler subproblems by labels. Second, we propose syntactic augmentation to stimulate the model’s intermediate thinking in two ways: syntactic prompting, which encourages the model to analyze the syntactic structure itself, and tool augmentation, which provides the model with the syntactic information generated by a parsing tool. Besides, we adapt self-consistency to NER by proposing a two-stage majority voting strategy, which first votes for the most consistent mentions, then the most consistent types. The proposed methods achieve remarkable improvements for zero-shot NER across seven benchmarks, including Chinese and English datasets, and on both domain-specific and general-domain scenarios. In addition, we present a comprehensive analysis of the error types with suggestions for optimization directions. We also verify the effectiveness of the proposed methods on the few-shot setting and other LLMs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.493.pdf",
        "keywords": [
            "zero shot ner"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions \"error types with suggestions for optimization directions\" which implies limitations, however it is not the main focus of the paper.\n\nNote that the paper does not explicitly discuss limitations of LLMs, but it does mention error types and optimization directions, which could be seen as a minor hint at limitations. However, the main focus is on the proposed methods and their",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions \"error types with suggestions for optimization directions\" which implies limitations, however it is not the main focus of the paper.\n\nNote that the paper does not explicitly discuss limitations of LLMs, but it does mention error types and optimization directions, which could be seen as a minor hint at limitations. However, the main focus is on the proposed methods and their"
    },
    {
        "title": "Automatic Prompt Optimization with “Gradient Descent” and Beam Search",
        "authors": [
            "Reid Pryzant",
            "Dan Iter",
            "Jerry Li",
            "Yin Lee",
            "Chenguang Zhu",
            "Michael Zeng"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Prompt Optimization with Textual Gradients (ProTeGi), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language “gradients” that criticize the current prompt, much like how numerical gradients point in the direction of error ascent. The natural language gradients are then “propagated” into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing techniques and improve an initial prompt’s performance by up to 31%, by using data to rewrite vague task descriptions into more precise annotation instructions.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.494.pdf",
        "keywords": [
            "beam search",
            "error ascent",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort.\""
    },
    {
        "title": "Active Retrieval Augmented Generation",
        "authors": [
            "Zhengbao Jiang",
            "Frank Xu",
            "Luyu Gao",
            "Zhiqing Sun",
            "Qian Liu",
            "Jane Dwivedi-Yu",
            "Yiming Yang",
            "Jamie Callan",
            "Graham Neubig"
        ],
        "published": "2023",
        "summary": "Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.495.pdf",
        "keywords": [
            "augmenting lms",
            "augmented generation",
            "retrieve and generate"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output.\""
    },
    {
        "title": "DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models",
        "authors": [
            "Chengcheng Han",
            "Xiaowei Du",
            "Che Zhang",
            "Yixin Lian",
            "Xiang Li",
            "Ming Gao",
            "Baoyuan Wang"
        ],
        "published": "2023",
        "summary": "Chain-of-Thought (CoT) prompting has successfully enhanced the reasoning capabilities of Large Language Models (LLMs) with at least 100 billion parameters. However, it is ineffective, or even detrimental, to the performance on reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion parameters. In this paper, we propose Dialogue-guided Chain-of-Thought (DialCoT) to improve the reasoning capabilities of SLMs, with the aim of generating intermediate reasoning steps in a dialogue format to guide the model to the final answer. Furthermore, we optimize the model to choose the optimal reasoning path through the Proximal Policy Optimization (PPO) algorithm, further enhancing its reasoning capabilities. Compared to previous methods, our advantages lie in: 1) We transform the process of solving complex reasoning problems into decomposing problems and solving a series of simpler sub-questions, significantly reducing task difficulty and making it more suitable for SLMs. 2) We optimize the model to choose the optimal reasoning path through the PPO algorithm. Comprehensive experiments on four arithmetic reasoning datasets show that our method can achieve significant performance gains over state-of-the-art competitors.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.501.pdf",
        "keywords": [
            "smaller language models",
            "large language models",
            "dialcot",
            "reasoning",
            "ppo",
            "arithmetic reasoning",
            "optimize",
            "optimal reasoning path",
            "proximal policy optimization"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Chain-of-Thought (CoT) prompting has successfully enhanced the reasoning capabilities of Large Language Models (LLMs) with at least 100 billion parameters. However, it is ineffective, or even detrimental, to the performance on reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion parameters.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Chain-of-Thought (CoT) prompting has successfully enhanced the reasoning capabilities of Large Language Models (LLMs) with at least 100 billion parameters. However, it is ineffective, or even detrimental, to the performance on reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion parameters.\""
    },
    {
        "title": "Recurrent Neural Language Models as Probabilistic Finite-state Automata",
        "authors": [
            "Anej Svete",
            "Ryan Cotterell"
        ],
        "published": "2023",
        "summary": "Studying language models (LMs) in terms of well-understood formalisms allows us to precisely characterize their abilities and limitations. Previous work has investigated the expressive power of recurrent neural network (RNN) LMs in terms of their capacity to recognize unweighted formal languages. However, LMs do not describe unweighted formal languages—rather, they define probability distributions over strings. In this work, we study what classes of such probability distributions RNN LMs can represent, which allows us to make more direct statements about their capabilities. We show that simple RNNs are equivalent to a subclass of probabilistic finite-state automata, and can thus model a strict subset of probability distributions expressible by finite-state models. Furthermore, we study the space complexity of representing finite-state LMs with RNNs. We show that, to represent an arbitrary deterministic finite-state LM with N states over an alphabet 𝛴, an RNN requires 𝛺\\left(N |𝛴|\\right) neurons. These results present a first step towards characterizing the classes of distributions RNN LMs can represent and thus help us understand their capabilities and limitations.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.502.pdf",
        "keywords": [
            "state",
            "finite state lms",
            "recurrent neural network",
            "recurrent neural language",
            "rnn lms",
            "probabilistic finite state automata"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, LMs do not describe unweighted formal languages—rather, they define probability distributions over strings.\"; \"We show that simple RNNs are equivalent to a subclass of probabilistic finite-state automata, and can thus model a strict subset of probability distributions expressible by finite-state models.\"; \"These results present a first step towards characterizing the classes of distributions RNN L",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, LMs do not describe unweighted formal languages—rather, they define probability distributions over strings.\"; \"We show that simple RNNs are equivalent to a subclass of probabilistic finite-state automata, and can thus model a strict subset of probability distributions expressible by finite-state models.\"; \"These results present a first step towards characterizing the classes of distributions RNN L"
    },
    {
        "title": "Background Summarization of Event Timelines",
        "authors": [
            "Adithya Pratapa",
            "Kevin Small",
            "Markus Dreyer"
        ],
        "published": "2023",
        "summary": "Generating concise summaries of news events is a challenging natural language processing task. While journalists often curate timelines to highlight key sub-events, newcomers to a news event face challenges in catching up on its historical context. In this paper, we address this need by introducing the task of background news summarization, which complements each timeline update with a background summary of relevant preceding events. We construct a dataset by merging existing timeline datasets and asking human annotators to write a background summary for each timestep of each news event. We establish strong baseline performance using state-of-the-art summarization systems and propose a query-focused variant to generate background summaries. To evaluate background summary quality, we present a question-answering-based evaluation metric, Background Utility Score (BUS), which measures the percentage of questions about a current event timestep that a background summary answers. Our experiments show the effectiveness of instruction fine-tuned systems such as Flan-T5, in addition to strong zero-shot performance using GPT-3.5.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.505.pdf",
        "keywords": [
            "background summarization",
            "background news summarization",
            "background utility score",
            "news event",
            "question answering"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We establish strong baseline performance using state-of-the-art summarization systems and propose a query-focused variant to generate background summaries.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"We establish strong baseline performance using state-of-the-art summarization systems and propose a query-focused variant to generate background summaries.\""
    },
    {
        "title": "Superlim: A Swedish Language Understanding Evaluation Benchmark",
        "authors": [
            "Aleksandrs Berdicevskis",
            "Gerlof Bouma",
            "Robin Kurtz",
            "Felix Morger",
            "Joey Öhman",
            "Yvonne Adesam",
            "Lars Borin",
            "Dana Dannélls",
            "Markus Forsberg",
            "Tim Isbister",
            "Anna Lindahl",
            "Martin Malmsten",
            "Faton Rekathati",
            "Magnus Sahlgren",
            "Elena Volodina",
            "Love Börjeson",
            "Simon Hengchen",
            "Nina Tahmasebi"
        ],
        "published": "2023",
        "summary": "We present Superlim, a multi-task NLP benchmark and analysis platform for evaluating Swedish language models, a counterpart to the English-language (Super)GLUE suite. We describe the dataset, the tasks, the leaderboard and report the baseline results yielded by a reference implementation. The tested models do not approach ceiling performance on any of the tasks, which suggests that Superlim is truly difficult, a desirable quality for a benchmark. We address methodological challenges, such as mitigating the Anglocentric bias when creating datasets for a less-resourced language; choosing the most appropriate measures; documenting the datasets and making the leaderboard convenient and transparent. We also highlight other potential usages of the dataset, such as, for instance, the evaluation of cross-lingual transfer learning.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.506.pdf",
        "keywords": [
            "superlim",
            "nlp benchmark",
            "language",
            "english language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The tested models do not approach ceiling performance on any of the tasks, which suggests that Superlim is truly difficult, a desirable quality for a benchmark.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The tested models do not approach ceiling performance on any of the tasks, which suggests that Superlim is truly difficult, a desirable quality for a benchmark.\""
    },
    {
        "title": "Reasoning with Language Model is Planning with World Model",
        "authors": [
            "Shibo Hao",
            "Yi Gu",
            "Haodi Ma",
            "Joshua Hong",
            "Zhen Wang",
            "Daisy Wang",
            "Zhiting Hu"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have shown remarkable reasoning capabilities, particularly with Chain-of-Thought-style prompts. However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks or performing complex math or logical reasoning. This is due to LLMs’ absence of an internal world model for predicting world states (e.g., environment status, variable values) and simulating long-term action outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, Reasoning via Planning (RAP). RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monte Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, properly balancing exploration v.s. exploitation to achieve a high-reward reasoning path efficiently. We apply RAP to a variety of challenging reasoning problems, such as plan generation, math reasoning, and logical inference. Empirical results demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency, e.g., RAP on LLaMA-33B surpasses CoT on GPT-4 with 33% relative improvement in plan generation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.507.pdf",
        "keywords": [
            "world model",
            "reward reasoning",
            "language model",
            "planning"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks or performing complex math or logical reasoning. This is due to LLMs’ absence of an internal world model for predicting world states (e.g., environment status, variable values) and simulating long-term action outcomes of actions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks or performing complex math or logical reasoning. This is due to LLMs’ absence of an internal world model for predicting world states (e.g., environment status, variable values) and simulating long-term action outcomes of actions.\""
    },
    {
        "title": "LLM-enhanced Self-training for Cross-domain Constituency Parsing",
        "authors": [
            "Jianling Li",
            "Meishan Zhang",
            "Peiming Guo",
            "Min Zhang",
            "Yue Zhang"
        ],
        "published": "2023",
        "summary": "Self-training has proven to be an effective approach for cross-domain tasks, and in this study, we explore its application to cross-domain constituency parsing. Traditional self-training methods rely on limited and potentially low-quality raw corpora. To overcome this limitation, we propose enhancing self-training with the large language model (LLM) to generate domain-specific raw corpora iteratively. For the constituency parsing, we introduce grammar rules that guide the LLM in generating raw corpora and establish criteria for selecting pseudo instances. Our experimental results demonstrate that self-training for constituency parsing, equipped with an LLM, outperforms traditional methods regardless of the LLM’s performance. Moreover, the combination of grammar rules and confidence criteria for pseudo-data selection yields the highest performance in the cross-domain constituency parsing.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.508.pdf",
        "keywords": [
            "constituency parsing",
            "self training",
            "cross domain constituency parsing",
            "pseudo data",
            "grammar"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Traditional self-training methods rely on limited and potentially low-quality raw corpora.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Traditional self-training methods rely on limited and potentially low-quality raw corpora.\""
    },
    {
        "title": "Editing Common Sense in Transformers",
        "authors": [
            "Anshita Gupta",
            "Debanjan Mondal",
            "Akshay Sheshadri",
            "Wenlong Zhao",
            "Xiang Li",
            "Sarah Wiegreffe",
            "Niket Tandon"
        ],
        "published": "2023",
        "summary": "Editing model parameters directly in Transformers makes updating open-source transformer-based models possible without re-training. However, these editing methods have only been evaluated on statements about encyclopedic knowledge with a single correct answer. Commonsense knowledge with multiple correct answers, e.g., an apple can be green or red but not transparent, has not been studied but is as essential for enhancing transformers’ reliability and usefulness. In this paper, we investigate whether commonsense judgments are causally associated with localized, editable parameters in Transformers, and we provide an affirmative answer. We find that directly applying the MEMIT editing algorithm results in sub-par performance and improve it for the commonsense domain by varying edit tokens and improving the layer selection strategy, i.e., MEMITCSK. GPT-2 Large and XL models edited using MEMITCSK outperform best-fine-tuned baselines by 10.97% and 10.73% F1 scores on PEP3k and 20Q datasets. In addition, we propose a novel evaluation dataset, PROBE\\ SET, that contains unaffected and affected neighborhoods, affected paraphrases, and affected reasoning challenges. MEMITCSK performs well across the metrics while fine-tuning baselines show significant trade-offs between unaffected and affected metrics. These results suggest a compelling future direction for incorporating feedback about common sense into Transformers through direct model editing.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.511.pdf",
        "keywords": [
            "transformers",
            "editing common sense",
            "memitcsk",
            "editing model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these editing methods have only been evaluated on statements about encyclopedic knowledge with a single correct answer. Commonsense knowledge with multiple correct answers... has not been studied but is as essential for enhancing transformers’ reliability and usefulness.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these editing methods have only been evaluated on statements about encyclopedic knowledge with a single correct answer. Commonsense knowledge with multiple correct answers... has not been studied but is as essential for enhancing transformers’ reliability and usefulness.\""
    },
    {
        "title": "Air-Decoding: Attribute Distribution Reconstruction for Decoding-Time Controllable Text Generation",
        "authors": [
            "Tianqi Zhong",
            "Quan Wang",
            "Jingxuan Han",
            "Yongdong Zhang",
            "Zhendong Mao"
        ],
        "published": "2023",
        "summary": "Controllable text generation (CTG) aims to generate text with desired attributes, and decoding-time-based methods have shown promising performance on this task. However, in this paper, we identify the phenomenon of Attribute Collapse for the first time. It causes the fluency of generated text to rapidly decrease when the control strength exceeds a critical value, rendering the text completely unusable. This limitation hinders the effectiveness of decoding methods in achieving high levels of controllability. To address this problem, we propose a novel lightweight decoding framework named Air-Decoding. Its main idea is reconstructing the attribute distributions to balance the weights between attribute words and non-attribute words to generate more fluent text. Specifically, we train prefixes by prefix-tuning to obtain attribute distributions. Then we design a novel attribute distribution reconstruction method to balance the obtained distributions and use the reconstructed distributions to guide language models for generation, effectively avoiding the issue of Attribute Collapse. Experiments on multiple CTG tasks prove that our method achieves a new state-of-the-art control performance.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.512.pdf",
        "keywords": [
            "attribute distribution reconstruction",
            "controllable text generation",
            "attribute collapse",
            "fluency"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This limitation hinders the effectiveness of decoding methods in achieving high levels of controllability.\"\n\nThis rating is chosen because the abstract mentions a limitation of decoding-time controllable text generation methods (Attribute Collapse), which is related to LLMs, but it is not the primary focus of the paper. The limitation is mentioned briefly to motivate the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"This limitation hinders the effectiveness of decoding methods in achieving high levels of controllability.\"\n\nThis rating is chosen because the abstract mentions a limitation of decoding-time controllable text generation methods (Attribute Collapse), which is related to LLMs, but it is not the primary focus of the paper. The limitation is mentioned briefly to motivate the proposed solution."
    },
    {
        "title": "IfQA: A Dataset for Open-domain Question Answering under Counterfactual Presuppositions",
        "authors": [
            "Wenhao Yu",
            "Meng Jiang",
            "Peter Clark",
            "Ashish Sabharwal"
        ],
        "published": "2023",
        "summary": "Although counterfactual reasoning is a fundamental aspect of intelligence, the lack of large-scale counterfactual open-domain question-answering (QA) benchmarks makes it difficult to evaluate and improve models on this ability. To address this void, we introduce the first such dataset, named IfQA, where each question is based on a counterfactual presupposition via an “if” clause. Such questions require models to go beyond retrieving direct factual knowledge from the Web: they must identify the right information to retrieve and reason about an imagined situation that may even go against the facts built into their parameters. The IfQA dataset contains 3,800 questions that were annotated by crowdworkers on relevant Wikipedia passages. Empirical analysis reveals that the IfQA dataset is highly challenging for existing open-domain QA methods, including supervised retrieve-then-read pipeline methods (F1 score 44.5), as well as recent few-shot approaches such as chain-of-thought prompting with ChatGPT (F1 score 57.2). We hope the unique challenges posed by IfQA will push open-domain QA research on both retrieval and reasoning fronts, while also helping endow counterfactual reasoning abilities to today’s language understanding models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.515.pdf",
        "keywords": [
            "counterfactual reasoning",
            "counterfactual presupposition",
            "open domain question answering"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Empirical analysis reveals that the IfQA dataset is highly challenging for existing open-domain QA methods, including supervised retrieve-then-read pipeline methods (F1 score 44.5), as well as recent few-shot approaches such as chain-of-thought prompting with ChatGPT (F1 score 57.2).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Empirical analysis reveals that the IfQA dataset is highly challenging for existing open-domain QA methods, including supervised retrieve-then-read pipeline methods (F1 score 44.5), as well as recent few-shot approaches such as chain-of-thought prompting with ChatGPT (F1 score 57.2).\""
    },
    {
        "title": "How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances",
        "authors": [
            "Zihan Zhang",
            "Meng Fang",
            "Ling Chen",
            "Mohammad-Reza Namazi-Rad",
            "Jun Wang"
        ],
        "published": "2023",
        "summary": "Although large language models (LLMs) are impressive in solving various tasks, they can quickly be outdated after deployment. Maintaining their up-to-date status is a pressing concern in the current era. This paper provides a comprehensive review of recent advances in aligning deployed LLMs with the ever-changing world knowledge. We categorize research works systemically and provide in-depth comparisons and discussions. We also discuss existing challenges and highlight future directions to facilitate research in this field.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.516.pdf",
        "keywords": [
            "world knowledge",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although large language models (LLMs) are impressive in solving various tasks, they can quickly be outdated after deployment. Maintaining their up-to-date status is a pressing concern in the current era.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although large language models (LLMs) are impressive in solving various tasks, they can quickly be outdated after deployment. Maintaining their up-to-date status is a pressing concern in the current era.\""
    },
    {
        "title": "DecipherPref: Analyzing Influential Factors in Human Preference Judgments via GPT-4",
        "authors": [
            "Yebowen Hu",
            "Kaiqiang Song",
            "Sangwoo Cho",
            "Xiaoyang Wang",
            "Hassan Foroosh",
            "Fei Liu"
        ],
        "published": "2023",
        "summary": "Human preference judgments are pivotal in guiding large language models (LLMs) to produce outputs that align with human values. Human evaluations are also used in summarization tasks to compare outputs from various systems, complementing existing automatic metrics. Despite their significance, however, there has been limited research probing these pairwise or k-wise comparisons. The collective impact and relative importance of factors such as output length, informativeness, fluency, and factual consistency are still not well understood. It is also unclear if there are other hidden factors influencing human judgments. In this paper, we conduct an in-depth examination of a collection of pairwise human judgments released by OpenAI. Utilizing the Bradley-Terry-Luce (BTL) model, we reveal the inherent preferences embedded in these human judgments. We find that the most favored factors vary across tasks and genres, whereas the least favored factors tend to be consistent, e.g., outputs are too brief, contain excessive off-focus content or hallucinated facts. Our findings have implications on the construction of balanced datasets in human preference evaluations, which is a crucial step in shaping the behaviors of future LLMs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.519.pdf",
        "keywords": [
            "genres",
            "preference",
            "language models",
            "fluency",
            "decipherpref",
            "factors",
            "preference judgments",
            "preference evaluations",
            "evaluations",
            "consistency"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"outputs are too brief, contain excessive off-focus content or hallucinated facts.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"outputs are too brief, contain excessive off-focus content or hallucinated facts.\""
    },
    {
        "title": "QA-NatVer: Question Answering for Natural Logic-based Fact Verification",
        "authors": [
            "Rami Aly",
            "Marek Strong",
            "Andreas Vlachos"
        ],
        "published": "2023",
        "summary": "Fact verification systems assess a claim’s veracity based on evidence. An important consideration in designing them is faithfulness, i.e. generating explanations that accurately reflect the reasoning of the model. Recent works have focused on natural logic, which operates directly on natural language by capturing the semantic relation of spans between an aligned claim with its evidence via set-theoretic operators. However, these approaches rely on substantial resources for training, which are only available for high-resource languages. To this end, we propose to use question answering to predict natural logic operators, taking advantage of the generalization capabilities of instruction-tuned language models. Thus, we obviate the need for annotated training data while still relying on a deterministic inference system. In a few-shot setting on FEVER, our approach outperforms the best baseline by 4.3 accuracy points, including a state-of-the-art pre-trained seq2seq natural logic system, as well as a state-of-the-art prompt-based classifier. Our system demonstrates its robustness and portability, achieving competitive performance on a counterfactual dataset and surpassing all approaches without further annotation on a Danish verification dataset. A human evaluation indicates that our approach produces more plausible proofs with fewer erroneous natural logic operators than previous natural logic-based systems.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.521.pdf",
        "keywords": [
            "natural logic",
            "natural logic system",
            "fact verification",
            "question answering"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these approaches rely on substantial resources for training, which are only available for high-resource languages.\"\n\nThis paper mentions a limitation of LLMs in passing, specifically that they require substantial resources for training which are not available for low-resource languages.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these approaches rely on substantial resources for training, which are only available for high-resource languages.\"\n\nThis paper mentions a limitation of LLMs in passing, specifically that they require substantial resources for training which are not available for low-resource languages."
    },
    {
        "title": "Increasing Probability Mass on Answer Choices Does Not Always Improve Accuracy",
        "authors": [
            "Sarah Wiegreffe",
            "Matthew Finlayson",
            "Oyvind Tafjord",
            "Peter Clark",
            "Ashish Sabharwal"
        ],
        "published": "2023",
        "summary": "When pretrained language models (LMs) are applied to discriminative tasks such as multiple-choice questions, they place probability mass on vocabulary tokens that aren’t among the given answer choices. Spreading probability mass across multiple surface forms with identical meaning (such as “bath” and “bathtub”) is thought to cause an underestimation of a model’s true performance, referred to as the “surface form competition” (SFC) hypothesis. This has motivated the introduction of various probability normalization methods. However, many core questions remain unanswered. How do we measure SFC? Are there direct ways of reducing it, and does doing so improve task performance? We propose a mathematical formalism for SFC which allows us to quantify and bound its impact for the first time. We identify a simple method for reducing it—namely, increasing probability mass on the given answer choices by a) including them in the prompt and b) using in-context learning with even just one example. We show this method eliminates the impact of SFC in the majority of instances. Our experiments on three diverse datasets and six LMs reveal several additional surprising findings. For example, both normalization and prompting methods for reducing SFC can be ineffective or even detrimental to task performance for some LMs. We conclude with practical insights for effectively prompting LMs for multiple-choice tasks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.522.pdf",
        "keywords": [
            "answer choices",
            "probability",
            "probability mass",
            "spreading probability mass",
            "probability normalization"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, many core questions remain unanswered... We show this method eliminates the impact of SFC in the majority of instances... For example, both normalization and prompting methods for reducing SFC can be ineffective or even detrimental to task performance for some LMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, many core questions remain unanswered... We show this method eliminates the impact of SFC in the majority of instances... For example, both normalization and prompting methods for reducing SFC can be ineffective or even detrimental to task performance for some LMs.\""
    },
    {
        "title": "Generating Data for Symbolic Language with Large Language Models",
        "authors": [
            "Jiacheng Ye",
            "Chengzu Li",
            "Lingpeng Kong",
            "Tao Yu"
        ],
        "published": "2023",
        "summary": "While large language models (LLMs) bring not only performance but also complexity, recent work has started to turn LLMs into data generators rather than task inferencers, where another affordable task model is trained for efficient deployment and inference. However, such an approach has primarily been applied to natural language tasks, and has not yet been explored for symbolic language tasks with complex structured outputs (e.g., semantic parsing and code generation). In this paper, we propose SymGen which utilizes LLMs for generating various annotation-expensive symbolic language data. SymGen consists of an informative prompt to steer generation and an agreement-based verifier to improve data correctness. We conduct extensive experiments on six symbolic language tasks across various settings. Compared with the LLMs, we demonstrate the 1%-sized task model can achieve comparable or better performance, largely cutting inference and deployment costs. We also show that generated data with only a few human demonstrations can be as effective as over 10 times the amount of human-annotated data when training the task model, saving a considerable amount of annotation effort. SymGen takes a step toward data generation for annotation-expensive complex tasks, and we release the code at URL.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.523.pdf",
        "keywords": [
            "language models",
            "language",
            "task model",
            "symbolic language",
            "inference",
            "code generation",
            "complexity",
            "data generators"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While large language models (LLMs) bring not only performance but also complexity\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"While large language models (LLMs) bring not only performance but also complexity\""
    },
    {
        "title": "Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models",
        "authors": [
            "Laura Cabello",
            "Emanuele Bugliarello",
            "Stephanie Brandl",
            "Desmond Elliott"
        ],
        "published": "2023",
        "summary": "Pretrained machine learning models are known to perpetuate and even amplify existing biases in data, which can result in unfair outcomes that ultimately impact user experience. Therefore, it is crucial to understand the mechanisms behind those prejudicial biases to ensure that model performance does not result in discriminatory behaviour toward certain groups or populations. In this work, we define gender bias as our case study. We quantify bias amplification in pretraining and after fine-tuning on three families of vision-and-language models. We investigate the connection, if any, between the two learning stages, and evaluate how bias amplification reflects on model performance. Overall, we find that bias amplification in pretraining and after fine-tuning are independent. We then examine the effect of continued pretraining on gender-neutral data, finding that this reduces group disparities, i.e., promotes fairness, on VQAv2 and retrieval tasks without significantly compromising task performance.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.525.pdf",
        "keywords": [
            "language models",
            "gender bias",
            "bias",
            "gender neutral",
            "fairness",
            "biases",
            "unfair"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Pretrained machine learning models are known to perpetuate and even amplify existing biases in data, which can result in unfair outcomes that ultimately impact user experience.\"\n\nNote that the paper discusses limitations of pre-trained vision-and-language models (which can be considered a type of LLMs), specifically the amplification of biases, but does not delve deeply into this limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Pretrained machine learning models are known to perpetuate and even amplify existing biases in data, which can result in unfair outcomes that ultimately impact user experience.\"\n\nNote that the paper discusses limitations of pre-trained vision-and-language models (which can be considered a type of LLMs), specifically the amplification of biases, but does not delve deeply into this limitation."
    },
    {
        "title": "Improving Dialogue Discourse Parsing via Reply-to Structures of Addressee Recognition",
        "authors": [
            "Yaxin Fan",
            "Feng Jiang",
            "Peifeng Li",
            "Fang Kong",
            "Qiaoming Zhu"
        ],
        "published": "2023",
        "summary": "Dialogue discourse parsing aims to reflect the relation-based structure of dialogue by establishing discourse links according to discourse relations. To alleviate data sparsity, previous studies have adopted multitasking approaches to jointly learn dialogue discourse parsing with related tasks (e.g., reading comprehension) that require additional human annotation, thus limiting their generality. In this paper, we propose a multitasking framework that integrates dialogue discourse parsing with its neighboring task addressee recognition. Addressee recognition reveals the reply-to structure that partially overlaps with the relation-based structure, which can be exploited to facilitate relation-based structure learning. To this end, we first proposed a reinforcement learning agent to identify training examples from addressee recognition that are most helpful for dialog discourse parsing. Then, a task-aware structure transformer is designed to capture the shared and private dialogue structure of different tasks, thereby further promoting dialogue discourse parsing. Experimental results on both the Molweni and STAC datasets show that our proposed method can outperform the SOTA baselines. The code will be available at https://github.com/yxfanSuda/RLTST.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.526.pdf",
        "keywords": [
            "addressee recognition",
            "parsing",
            "dialogue",
            "dialogue discourse parsing",
            "reinforcement learning agent",
            "dialog discourse parsing"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Improving Language Models’ Meaning Understanding and Consistency by Learning Conceptual Roles from Dictionary",
        "authors": [
            "Myeongjun Jang",
            "Thomas Lukasiewicz"
        ],
        "published": "2023",
        "summary": "The non-humanlike behaviour of contemporary pre-trained language models (PLMs) is a leading cause undermining their trustworthiness. A striking phenomenon of such faulty behaviours is the generation of inconsistent predictions, which produces logically contradictory results, such as generating different predictions for texts delivering the same meaning or violating logical properties. Previous studies exploited data augmentation or implemented specialised loss functions to alleviate the issue. However, their usage is limited, because they consume expensive training resources for large-sized PLMs and can only handle a certain consistency type. To this end, we propose a practical approach that alleviates the inconsistent behaviour issue by fundamentally improving PLMs’ meaning awareness. Based on the conceptual role theory, our method allows PLMs to capture accurate meaning by learning precise interrelationships between concepts from word-definition pairs in a dictionary. Next, we propose an efficient parameter integration technique that updates only a few additional parameters to combine the learned interrelationship with PLMs’ pre-trained knowledge. Our experimental results reveal that the approach can concurrently improve multiple types of consistency, enables efficient knowledge integration, and easily applies to other languages.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.527.pdf",
        "keywords": [
            "language models",
            "roles",
            "pre trained language models",
            "inconsistent",
            "conceptual role theory",
            "knowledge integration",
            "meaning awareness",
            "plms"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The non-humanlike behaviour of contemporary pre-trained language models (PLMs) is a leading cause undermining their trustworthiness... A striking phenomenon of such faulty behaviours is the generation of inconsistent predictions, which produces logically contradictory results, such as generating different predictions for texts delivering the same meaning or violating logical properties.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"The non-humanlike behaviour of contemporary pre-trained language models (PLMs) is a leading cause undermining their trustworthiness... A striking phenomenon of such faulty behaviours is the generation of inconsistent predictions, which produces logically contradictory results, such as generating different predictions for texts delivering the same meaning or violating logical properties.\""
    },
    {
        "title": "DALE: Generative Data Augmentation for Low-Resource Legal NLP",
        "authors": [
            "Sreyan Ghosh",
            "Chandra Kiran Reddy Evuru",
            "Sonal Kumar",
            "S Ramaneswaran",
            "S Sakshi",
            "Utkarsh Tyagi",
            "Dinesh Manocha"
        ],
        "published": "2023",
        "summary": "We present DALE, a novel and effective generative Data Augmentation framework for low-resource LEgal NLP. DALE addresses the challenges existing frameworks pose in generating effective data augmentations of legal documents - legal language, with its specialized vocabulary and complex semantics, morphology, and syntax, does not benefit from data augmentations that merely rephrase the source sentence. To address this, DALE, built on an Encoder-Decoder Language Model, is pre-trained on a novel unsupervised text denoising objective based on selective masking - our masking strategy exploits the domain-specific language characteristics of templatized legal documents to mask collocated spans of text. Denoising these spans help DALE acquire broad legal knowledge and develop the ability to generate coherent and diverse augmentations with novel contexts. Finally, DALE performs conditional generation to generate synthetic augmentations for low-resource Legal NLP tasks. We demonstrate the effectiveness of DALE on 13 datasets spanning 6 tasks and 4 low-resource settings. DALE outperforms all our baselines, including LLMs, qualitatively and quantitatively, with absolute improvements of 1%-50%.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.528.pdf",
        "keywords": [
            "generative data augmentation",
            "language"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"DALE outperforms all our baselines, including LLMs, qualitatively and quantitatively, with absolute improvements of 1%-50%.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"DALE outperforms all our baselines, including LLMs, qualitatively and quantitatively, with absolute improvements of 1%-50%.\""
    },
    {
        "title": "FedID: Federated Interactive Distillation for Large-Scale Pretraining Language Models",
        "authors": [
            "Xinge Ma",
            "Jiangming Liu",
            "Jin Wang",
            "Xuejie Zhang"
        ],
        "published": "2023",
        "summary": "The growing concerns and regulations surrounding the protection of user data privacy have necessitated decentralized training paradigms. To this end, federated learning (FL) is widely studied in user-related natural language processing (NLP). However, it suffers from several critical limitations including extensive communication overhead, inability to handle heterogeneity, and vulnerability to white-box inference attacks. Federated distillation (FD) is proposed to alleviate these limitations, but its performance is faded by confirmation bias. To tackle this issue, we propose Federated Interactive Distillation (FedID), which utilizes a small amount of labeled data retained by the server to further rectify the local models during knowledge transfer. Additionally, based on the GLUE benchmark, we develop a benchmarking framework across multiple tasks with diverse data distributions to contribute to the research of FD in NLP community. Experiments show that our proposed FedID framework achieves the best results in homogeneous and heterogeneous federated scenarios. The code for this paper is available at: https://github.com/maxinge8698/FedID.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.529.pdf",
        "keywords": [
            "federated distillation",
            "federated interactive distillation",
            "pretraining language models",
            "natural language processing",
            "federated learning",
            "vulnerability"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it suffers from several critical limitations including extensive communication overhead, inability to handle heterogeneity, and vulnerability to white-box inference attacks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, it suffers from several critical limitations including extensive communication overhead, inability to handle heterogeneity, and vulnerability to white-box inference attacks.\""
    },
    {
        "title": "trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback",
        "authors": [
            "Alexander Havrilla",
            "Maksym Zhuravinskyi",
            "Duy Phung",
            "Aman Tiwari",
            "Jonathan Tow",
            "Stella Biderman",
            "Quentin Anthony",
            "Louis Castricato"
        ],
        "published": "2023",
        "summary": "Reinforcement learning from human feedback (RLHF) utilizes human feedback to better align large language models with human preferences via online optimization against a learned reward model. Current RLHF paradigms rely on Proximal Policy Optimization (PPO), which quickly becomes a challenge to implement and scale up to large architectures. To address this difficulty we present the AutoRLHF library as a feature complete open-source framework for RLHF fine-tuning of models up to and exceeding 70 billion parameters. To do so we implement support for multiple types of distributed training including distributed data parallel, model sharded, as well as tensor, sequential, and pipeline parallelism. Additionally, we implement compute and memory saving features, giving AutoRLHF the flexibility to support users with a wide range of compute resources. This includes offline RL methods like Implicit Language Q Learning (ILQL) as a compute efficient alternative to PPO. We find offline fine-tuning offers competitive performance relative to online algorithms while being easier to implement, train, and scale. To evaluate our framework we train RLHF models on two separate well-known tasks using publicly available human preference data. Models trained with AutoRLHF achieve preference win-rates over baselines at rates comparable to the original works.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.530.pdf",
        "keywords": [
            "reinforcement learning",
            "human feedback",
            "proximal policy optimization",
            "framework"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Current RLHF paradigms rely on Proximal Policy Optimization (PPO), which quickly becomes a challenge to implement and scale up to large architectures.\"\n\nThis paper discusses the limitations of Proximal Policy Optimization (PPO) in large language models, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Current RLHF paradigms rely on Proximal Policy Optimization (PPO), which quickly becomes a challenge to implement and scale up to large architectures.\"\n\nThis paper discusses the limitations of Proximal Policy Optimization (PPO) in large language models, but it is not the primary focus of the paper."
    },
    {
        "title": "This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models",
        "authors": [
            "Iker García-Ferrero",
            "Begoña Altuna",
            "Javier Alvez",
            "Itziar Gonzalez-Dios",
            "German Rigau"
        ],
        "published": "2023",
        "summary": "Although large language models (LLMs) have apparently acquired a certain level of grammatical knowledge and the ability to make generalizations, they fail to interpret negation, a crucial step in Natural Language Processing. We try to clarify the reasons for the sub-optimal performance of LLMs understanding negation. We introduce a large semi-automatically generated dataset of circa 400,000 descriptive sentences about commonsense knowledge that can be true or false in which negation is present in about 2/3 of the corpus in different forms. We have used our dataset with the largest available open LLMs in a zero-shot approach to grasp their generalization and inference capability and we have also fine-tuned some of the models to assess whether the understanding of negation can be trained. Our findings show that, while LLMs are proficient at classifying affirmative sentences, they struggle with negative sentences and lack a deep understanding of negation, often relying on superficial cues. Although fine-tuning the models on negative sentences improves their performance, the lack of generalization in handling negation is persistent, highlighting the ongoing challenges of LLMs regarding negation understanding and generalization. The dataset and code are publicly available.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.531.pdf",
        "keywords": [
            "negation",
            "negation benchmark",
            "language models",
            "large language models",
            "grammatical knowledge",
            "commonsense knowledge",
            "natural language processing",
            "sentences"
        ],
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"they fail to interpret negation, a crucial step in Natural Language Processing... Our findings show that, while LLMs are proficient at classifying affirmative sentences, they struggle with negative sentences and lack a deep understanding of negation, often relying on superficial cues.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"they fail to interpret negation, a crucial step in Natural Language Processing... Our findings show that, while LLMs are proficient at classifying affirmative sentences, they struggle with negative sentences and lack a deep understanding of negation, often relying on superficial cues.\""
    },
    {
        "title": "MT2: Towards a Multi-Task Machine Translation Model with Translation-Specific In-Context Learning",
        "authors": [
            "Chunyou Li",
            "Mingtong Liu",
            "Hongxiao Zhang",
            "Yufeng Chen",
            "Jinan Xu",
            "Ming Zhou"
        ],
        "published": "2023",
        "summary": "Sentence-level translation, document-level translation, translation memory, and terminology constrained translation play an important role in machine translation. Most of the previous work uses separate models or methods to solve these tasks, which is not conducive to knowledge transfer of different tasks and increases the complexity of system construction. In this work, we explore the potential of pre-trained language model in machine translation tasks and propose a Multi-Task Machine Translation (MT2) model to integrate these translation tasks. We design a novel translation-specific In-Context Learning (ICL) paradigm for model training, in which all of the translation tasks can be modeled as context-learning tasks that integrate contextual information for performance improvement. Specifically, we propose a retrieval and alignment method to obtain a large scale context-enhancement training data, then we train the model in an in-context learning manner. Furthermore, we adopt two context-dependent training strategies to encourage the model to better understand and utilize contextual information for translation. Extensive experiments on translation memory, terminology constrained translation, document-level translation, and few-shot domain-adaptation tasks demonstrate the superior performance of our model, verifying the effectiveness of our proposed approach.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.532.pdf",
        "keywords": [
            "domain adaptation",
            "translation",
            "machine translation",
            "multi task machine translation",
            "sentence level translation",
            "document level translation",
            "context learning"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Most of the previous work uses separate models or methods to solve these tasks, which is not conducive to knowledge transfer of different tasks and increases the complexity of system construction.\"\n\n(Note: Although this paper discusses pre-trained language models, it does not explicitly mention limitations of LLMs, but rather the limitations of previous work in machine translation tasks. However, it can be inferred that the complexity",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Most of the previous work uses separate models or methods to solve these tasks, which is not conducive to knowledge transfer of different tasks and increases the complexity of system construction.\"\n\n(Note: Although this paper discusses pre-trained language models, it does not explicitly mention limitations of LLMs, but rather the limitations of previous work in machine translation tasks. However, it can be inferred that the complexity"
    },
    {
        "title": "Disentangling Transformer Language Models as Superposed Topic Models",
        "authors": [
            "Jia Peng Lim",
            "Hady Lauw"
        ],
        "published": "2023",
        "summary": "Topic Modelling is an established research area where the quality of a given topic is measured using coherence metrics. Often, we infer topics from Neural Topic Models (NTM) by interpreting their decoder weights, consisting of top-activated words projected from individual neurons. Transformer-based Language Models (TLM) similarly consist of decoder weights. However, due to its hypothesised superposition properties, the final logits originating from the residual path are considered uninterpretable. Therefore, we posit that we can interpret TLM as superposed NTM by proposing a novel weight-based, model-agnostic and corpus-agnostic approach to search and disentangle decoder-only TLM, potentially mapping individual neurons to multiple coherent topics. Our results show that it is empirically feasible to disentangle coherent topics from GPT-2 models using the Wikipedia corpus. We validate this approach for GPT-2 models using Zero-Shot Topic Modelling. Finally, we extend the proposed approach to disentangle and analyse LLaMA models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.534.pdf",
        "keywords": [
            "decoder weights",
            "topic models",
            "language models",
            "wikipedia",
            "transformer",
            "corpus"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, due to its hypothesised superposition properties, the final logits originating from the residual path are considered uninterpretable.\"\n\nThis rating is given because the abstract mentions a limitation of Transformer-based Language Models (TLMs) in passing, but it is not the primary focus of the paper. The limitation is mentioned to justify the proposed approach, but it is not explained or",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, due to its hypothesised superposition properties, the final logits originating from the residual path are considered uninterpretable.\"\n\nThis rating is given because the abstract mentions a limitation of Transformer-based Language Models (TLMs) in passing, but it is not the primary focus of the paper. The limitation is mentioned to justify the proposed approach, but it is not explained or"
    },
    {
        "title": "Not all quantifiers are equal: Probing Transformer-based language models’ understanding of generalised quantifiers",
        "authors": [
            "Tharindu Madusanka",
            "Iqra Zahid",
            "Hao Li",
            "Ian Pratt-Hartmann",
            "Riza Batista-Navarro"
        ],
        "published": "2023",
        "summary": "How do different generalised quantifiers affect the behaviour of transformer-based language models (TLMs)? The recent popularity of TLMs and the central role generalised quantifiers have traditionally played in linguistics and logic bring this question into particular focus. The current research investigating this subject has not utilised a task defined purely in a logical sense, and thus, has not captured the underlying logical significance of generalised quantifiers. Consequently, they have not answered the aforementioned question faithfully or adequately. Therefore, we investigate how different generalised quantifiers affect TLMs by employing a textual entailment problem defined in a purely logical sense, namely, model-checking with natural language. Our approach permits the automatic construction of datasets with respect to which we can assess the ability of TLMs to learn the meanings of generalised quantifiers. Our investigation reveals that TLMs generally can comprehend the logical semantics of the most common generalised quantifiers, but that distinct quantifiers influence TLMs in varying ways.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.536.pdf",
        "keywords": [
            "generalised quantifiers",
            "transformer",
            "probing transformer",
            "language models",
            "checking"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our investigation reveals that TLMs generally can comprehend the logical semantics of the most common generalised quantifiers, but that distinct quantifiers influence TLMs in varying ways.\"\n\nThis rating is given because the paper discusses the limitations of TLMs in understanding generalized quantifiers, specifically how different quantifiers influence TLMs in varying ways, but the primary focus of the paper is",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Our investigation reveals that TLMs generally can comprehend the logical semantics of the most common generalised quantifiers, but that distinct quantifiers influence TLMs in varying ways.\"\n\nThis rating is given because the paper discusses the limitations of TLMs in understanding generalized quantifiers, specifically how different quantifiers influence TLMs in varying ways, but the primary focus of the paper is"
    },
    {
        "title": "Structure-aware Knowledge Graph-to-text Generation with Planning Selection and Similarity Distinction",
        "authors": [
            "Feng Zhao",
            "Hongzhi Zou",
            "Cheng Yan"
        ],
        "published": "2023",
        "summary": "The knowledge graph-to-text (KG-to-text) generation task aims to synthesize coherent and engaging sentences that accurately convey the complex information derived from an input knowledge graph. One of the primary challenges in this task is bridging the gap between the diverse structures of the KG and the target text, while preserving the details of the input KG. To address this, we propose a novel approach that efficiently integrates graph structure-aware modules with pre-trained language models. Unlike conventional techniques, which only consider direct connections between first-order neighbors, our method delves deeper by incorporating Relative Distance Encoding as a bias within the graph structure-aware module. This enables our model to better capture the intricate topology information present in the KG. To further elevate the fidelity of the generated text, Planning Selection and Similarity Distinction are introduced. Our approach filters the most relevant linearized sequences by employing a planning scorer, while simultaneously distinguishing similar input KGs through contrastive learning techniques. Experiments on two datasets demonstrate the superiority of our model.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.537.pdf",
        "keywords": [
            "knowledge graph",
            "similarity distinction",
            "pre trained language models",
            "planning selection",
            "graph structure aware modules"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unlike conventional techniques, which only consider direct connections between first-order neighbors, our method delves deeper by incorporating Relative Distance Encoding as a bias within the graph structure-aware module.\"\n\nThis paper is rated with 1 since it talks about pre-trained language models, but does not mention any explicit limitation of the models in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Unlike conventional techniques, which only consider direct connections between first-order neighbors, our method delves deeper by incorporating Relative Distance Encoding as a bias within the graph structure-aware module.\"\n\nThis paper is rated with 1 since it talks about pre-trained language models, but does not mention any explicit limitation of the models in the abstract."
    },
    {
        "title": "SOUL: Towards Sentiment and Opinion Understanding of Language",
        "authors": [
            "Yue Deng",
            "Wenxuan Zhang",
            "Sinno Pan",
            "Lidong Bing"
        ],
        "published": "2023",
        "summary": "Sentiment analysis is a well-established natural language processing task, with sentiment polarity classification being one of its most popular and representative tasks. However, despite the success of pre-trained language models in this area, they often fall short of capturing the broader complexities of sentiment analysis. To address this issue, we propose a new task called Sentiment and Opinion Understanding of Language (SOUL). SOUL aims to evaluate sentiment understanding through two subtasks: Review Comprehension (RC) and Justification Generation (JG). RC seeks to validate statements that focus on subjective information based on a review text, while JG requires models to provide explanations for their sentiment predictions. To enable comprehensive evaluation, we annotate a new dataset comprising 15,028 statements from 3,638 reviews. Experimental results indicate that SOUL is a challenging task for both small and large language models, with a performance gap of up to 27% when compared to human performance. Furthermore, evaluations conducted with both human experts and GPT-4 highlight the limitations of the small language model in generating reasoning-based justifications. These findings underscore the challenging nature of the SOUL task for existing models, emphasizing the need for further advancements in sentiment analysis to address its complexities. The new dataset and code are available at https://github.com/DAMO-NLP-SG/SOUL.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.538.pdf",
        "keywords": [
            "sentiment",
            "sentiment analysis",
            "opinion",
            "sentiment understanding",
            "opinion understanding",
            "sentiment polarity classification",
            "sentiment predictions",
            "justification generation"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experimental results indicate that SOUL is a challenging task for both small and large language models, with a performance gap of up to 27% when compared to human performance. Furthermore, evaluations conducted with both human experts and GPT-4 highlight the limitations of the small language model in generating reasoning-based justifications.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Experimental results indicate that SOUL is a challenging task for both small and large language models, with a performance gap of up to 27% when compared to human performance. Furthermore, evaluations conducted with both human experts and GPT-4 highlight the limitations of the small language model in generating reasoning-based justifications.\""
    },
    {
        "title": "Regulation and NLP (RegNLP): Taming Large Language Models",
        "authors": [
            "Catalina Goanta",
            "Nikolaos Aletras",
            "Ilias Chalkidis",
            "Sofia Ranchordás",
            "Gerasimos Spanakis"
        ],
        "published": "2023",
        "summary": "The scientific innovation in Natural Language Processing (NLP) and more broadly in artificial intelligence (AI) is at its fastest pace to date. As large language models (LLMs) unleash a new era of automation, important debates emerge regarding the benefits and risks of their development, deployment and use. Currently, these debates have been dominated by often polarized narratives mainly led by the AI Safety and AI Ethics movements. This polarization, often amplified by social media, is swaying political agendas on AI regulation and governance and posing issues of regulatory capture. Capture occurs when the regulator advances the interests of the industry it is supposed to regulate, or of special interest groups rather than pursuing the general public interest. Meanwhile in NLP research, attention has been increasingly paid to the discussion of regulating risks and harms. This often happens without systematic methodologies or sufficient rooting in the disciplines that inspire an extended scope of NLP research, jeopardizing the scientific integrity of these endeavors. Regulation studies are a rich source of knowledge on how to systematically deal with risk and uncertainty, as well as with scientific evidence, to evaluate and compare regulatory options. This resource has largely remained untapped so far. In this paper, we argue how NLP research on these topics can benefit from proximity to regulatory studies and adjacent fields. We do so by discussing basic tenets of regulation, and risk and uncertainty, and by highlighting the shortcomings of current NLP discussions dealing with risk assessment. Finally, we advocate for the development of a new multidisciplinary research space on regulation and NLP (RegNLP), focused on connecting scientific knowledge to regulatory processes based on systematic methodologies.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.539.pdf",
        "keywords": [
            "natural language processing",
            "governance",
            "regulation",
            "proximity",
            "language models",
            "artificial intelligence",
            "risk assessment",
            "nlp"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Currently, these debates have been dominated by often polarized narratives mainly led by the AI Safety and AI Ethics movements... posing issues of regulatory capture... Meanwhile in NLP research, attention has been increasingly paid to the discussion of regulating risks and harms... We do so by discussing basic tenets of regulation, and risk and uncertainty, and by highlighting the shortcomings of current NLP discussions dealing",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Currently, these debates have been dominated by often polarized narratives mainly led by the AI Safety and AI Ethics movements... posing issues of regulatory capture... Meanwhile in NLP research, attention has been increasingly paid to the discussion of regulating risks and harms... We do so by discussing basic tenets of regulation, and risk and uncertainty, and by highlighting the shortcomings of current NLP discussions dealing"
    },
    {
        "title": "MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation",
        "authors": [
            "Zexue He",
            "Yu Wang",
            "An Yan",
            "Yao Liu",
            "Eric Chang",
            "Amilcare Gentili",
            "Julian McAuley",
            "Chun-Nan Hsu"
        ],
        "published": "2023",
        "summary": "Curated datasets for healthcare are often limited due to the need of human annotations from experts. In this paper, we present MedEval, a multi-level, multi-task, and multi-domain medical benchmark to facilitate the development of language models for healthcare. MedEval is comprehensive and consists of data from several healthcare systems and spans 35 human body regions from 8 examination modalities. With 22,779 collected sentences and 21,228 reports, we provide expert annotations at multiple levels, offering a granular potential usage of the data and supporting a wide range of tasks. Moreover, we systematically evaluated 10 generic and domain-specific language models under zero-shot and finetuning settings, from domain-adapted baselines in healthcare to general-purposed state-of-the-art large language models (e.g., ChatGPT). Our evaluations reveal varying effectiveness of the two categories of language models across different tasks, from which we notice the importance of instruction tuning for few-shot usage of large language models. Our investigation paves the way toward benchmarking language models for healthcare and provides valuable insights into the strengths and limitations of adopting large language models in medical domains, informing their practical applications and future advancements.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.540.pdf",
        "keywords": [
            "language models",
            "multi task",
            "language model evaluation",
            "multi level",
            "medical benchmark"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our investigation paves the way toward benchmarking language models for healthcare and provides valuable insights into the strengths and limitations of adopting large language models in medical domains\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our investigation paves the way toward benchmarking language models for healthcare and provides valuable insights into the strengths and limitations of adopting large language models in medical domains\""
    },
    {
        "title": "Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks",
        "authors": [
            "Andrea Sottana",
            "Bin Liang",
            "Kai Zou",
            "Zheng Yuan"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) evaluation is a patchy and inconsistent landscape, and it is becoming clear that the quality of automatic evaluation metrics is not keeping up with the pace of development of generative models. We aim to improve the understanding of current models’ performance by providing a preliminary and hybrid evaluation on a range of open and closed-source generative LLMs on three NLP benchmarks: text summarisation, text simplification and grammatical error correction (GEC), using both automatic and human evaluation. We also explore the potential of the recently released GPT-4 to act as an evaluator. We find that ChatGPT consistently outperforms many other popular models according to human reviewers on the majority of metrics, while scoring much more poorly when using classic automatic evaluation metrics. We also find that human reviewers rate the gold reference as much worse than the best models’ outputs, indicating the poor quality of many popular benchmarks. Finally, we find that GPT-4 is capable of ranking models’ outputs in a way which aligns reasonably closely to human judgement despite task-specific variations, with a lower alignment in the GEC task.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.543.pdf",
        "keywords": [
            "large language models",
            "evaluation metrics"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that human reviewers rate the gold reference as much worse than the best models’ outputs, indicating the poor quality of many popular benchmarks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We find that human reviewers rate the gold reference as much worse than the best models’ outputs, indicating the poor quality of many popular benchmarks.\""
    },
    {
        "title": "Learning to Predict Task Transferability via Soft Prompt",
        "authors": [
            "Lingyun Feng"
        ],
        "published": "2023",
        "summary": "Fine-tuning pretrained language models on helpful intermediate tasks often greatly improves the performance of target tasks. However, how to efficiently find the source tasks that can successfully transfer still remains under-explored. In this work, we propose to learn an affinity scoring function to predict transferability between tasks. Specifically, we conduct prompt tuning and regard soft prompts as task embeddings that summarize task-specific information. Then we randomly sample task pairs to train an affinity scoring function. The goal is to predict the transfer gain (i.e., affinity) between a task pair, by conditioning on their task embeddings. Once the scoring function is trained, given a novel target task, we use it to predict the most transferable source tasks, without a brute-force search for all possible source-target pairs. Experimental results across 50 tasks show that our method efficiently identifies beneficial tasks for transfer learning.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.546.pdf",
        "keywords": [
            "scoring function",
            "affinity scoring function",
            "transferability",
            "task transferability",
            "soft prompts"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "Chain-of-Questions Training with Latent Answers for Robust Multistep Question Answering",
        "authors": [
            "Wang Zhu",
            "Jesse Thomason",
            "Robin Jia"
        ],
        "published": "2023",
        "summary": "We propose Chain-of-Questions, a framework that trains a model to robustly answer multistep questions by generating and answering sub-questions. We obtain supervision for sub-questions from human-annotated question decomposition meaning representation (QDMR), but QDMR does not include annotated answers to sub-questions. To overcome this technical challenge, we treat sub-answers as latent variables and infer them with a novel dynamic mixture of Hard-EM and MAPO. Chain-of-Questions is effective and robust, greatly outperforming strong neuro-symbolic methods by 9.0 F1 on a DROP contrast set and GPT-3.5 by 24.3 F1 on a HotpotQA adversarial set.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.547.pdf",
        "keywords": [
            "question answering",
            "infer"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"GPT-3.5\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"GPT-3.5\""
    },
    {
        "title": "“Mistakes Help Us Grow”: Facilitating and Evaluating Growth Mindset Supportive Language in Classrooms",
        "authors": [
            "Kunal Handa",
            "Margarett Clapper",
            "Jessica Boyle",
            "Rose Wang",
            "Diyi Yang",
            "David Yeager",
            "Dorottya Demszky"
        ],
        "published": "2023",
        "summary": "Teachers’ growth mindset supportive language (GMSL)—rhetoric emphasizing that one’s skills can be improved over time—has been shown to significantly reduce disparities in academic achievement and enhance students’ learning outcomes. Although teachers espouse growth mindset principles, most find it difficult to adopt GMSL in their practice due the lack of effective coaching in this area. We explore whether large language models (LLMs) can provide automated, personalized coaching to support teachers’ use of GMSL. We establish an effective coaching tool to reframe unsupportive utterances to GMSL by developing (i) a parallel dataset containing GMSL-trained teacher reframings of unsupportive statements with an accompanying annotation guide, (ii) a GMSL prompt framework to revise teachers’ unsupportive language, and (iii) an evaluation framework grounded in psychological theory for evaluating GMSL with the help of students and teachers. We conduct a large-scale evaluation involving 174 teachers and 1,006 students, finding that both teachers and students perceive GMSL-trained teacher and model reframings as more effective in fostering a growth mindset and promoting challenge-seeking behavior, among other benefits. We also find that model-generated reframings outperform those from the GMSL-trained teachers. These results show promise for harnessing LLMs to provide automated GMSL feedback for teachers and, more broadly, LLMs’ potentiality for supporting students’ learning in the classroom. Our findings also demonstrate the benefit of large-scale human evaluations when applying LLMs in educational domains.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.549.pdf",
        "keywords": [
            "supportive language",
            "reframings",
            "growth mindset supportive language",
            "language models",
            "personalized coaching",
            "coaching",
            "psychological theory"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, but mentions \"although teachers espouse growth mindset principles, most find it difficult to adopt GMSL in their practice\" which is not directly related to LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, but mentions \"although teachers espouse growth mindset principles, most find it difficult to adopt GMSL in their practice\" which is not directly related to LLMs."
    },
    {
        "title": "Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text",
        "authors": [
            "Qi Cao",
            "Takeshi Kojima",
            "Yutaka Matsuo",
            "Yusuke Iwasawa"
        ],
        "published": "2023",
        "summary": "While Large Language Models (LLMs) have achieved remarkable performance in many tasks, much about their inner workings remains unclear. In this study, we present novel experimental insights into the resilience of LLMs, particularly GPT-4, when subjected to extensive character-level permutations. To investigate this, we first propose the Scrambled Bench, a suite designed to measure the capacity of LLMs to handle scrambled input, in terms of both recovering scrambled sentences and answering questions given scrambled context. The experimental results indicate that multiple advanced LLMs demonstrate the capability akin to typoglycemia, a phenomenon where humans can understand the meaning of words even when the letters within those words are scrambled, as long as the first and last letters remain in place. More surprisingly, we found that only GPT-4 nearly flawlessly processes inputs with unnatural errors, a task that poses significant challenges for other LLMs and often even for humans. Specifically, GPT-4 can almost perfectly reconstruct the original sentences from scrambled ones, decreasing the edit distance by 95%, even when all letters within each word are entirely scrambled. It is counter-intuitive that LLMs can exhibit such resilience despite severe disruption to input tokenization caused by scrambled text.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.550.pdf",
        "keywords": [
            "unnatural error correction",
            "scrambled",
            "resilience"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"It is counter-intuitive that LLMs can exhibit such resilience despite severe disruption to input tokenization caused by scrambled text.\"\n\nThis abstract mentions a limitation of LLMs in passing, specifically the potential disruption caused by scrambled text, but does not explore this limitation in detail and instead focuses on the surprising resilience of GPT-4 in handling scrambled input.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"It is counter-intuitive that LLMs can exhibit such resilience despite severe disruption to input tokenization caused by scrambled text.\"\n\nThis abstract mentions a limitation of LLMs in passing, specifically the potential disruption caused by scrambled text, but does not explore this limitation in detail and instead focuses on the surprising resilience of GPT-4 in handling scrambled input."
    },
    {
        "title": "Detecting and Mitigating Hallucinations in Multilingual Summarisation",
        "authors": [
            "Yifu Qiu",
            "Yftah Ziser",
            "Anna Korhonen",
            "Edoardo Ponti",
            "Shay Cohen"
        ],
        "published": "2023",
        "summary": "Hallucinations pose a significant challenge to the reliability of neural models for abstractive summarisation. While automatically generated summaries may be fluent, they often lack faithfulness to the original document. This issue becomes even more pronounced in low-resource languages, where summarisation requires cross-lingual transfer. With the existing faithful metrics focusing on English, even measuring the extent of this phenomenon in cross-lingual settings is hard. To address this, we first develop a novel metric, mFACT, evaluating the faithfulness of non-English summaries, leveraging translation-based transfer from multiple English faithfulness metrics. Through extensive experiments in multiple languages, we demonstrate that mFACT is best suited to detect hallucinations compared to alternative metrics. With mFACT, we assess a broad range of multilingual large language models, and find that they all tend to hallucinate often in languages different from English. We then propose a simple but effective method to reduce hallucinations in cross-lingual transfer, which weighs the loss of each training example by its faithfulness score. This method drastically increases both performance and faithfulness according to both automatic and human evaluation when compared to strong baselines for cross-lingual transfer such as MAD-X. Our code and dataset are available at https://github.com/yfqiu-nlp/mfact-summ.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.551.pdf",
        "keywords": [
            "summarisation",
            "faithfulness"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We then propose a simple but effective method to reduce hallucinations in cross-lingual transfer, which weighs the loss of each training example by its faithfulness score.... We assess a broad range of multilingual large language models, and find that they all tend to hallucinate often in languages different from English.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We then propose a simple but effective method to reduce hallucinations in cross-lingual transfer, which weighs the loss of each training example by its faithfulness score.... We assess a broad range of multilingual large language models, and find that they all tend to hallucinate often in languages different from English.\""
    },
    {
        "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
        "authors": [
            "Potsawee Manakul",
            "Adian Liusie",
            "Mark Gales"
        ],
        "published": "2023",
        "summary": "Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose “SelfCheckGPT”, a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.557.pdf",
        "keywords": [
            "large language models",
            "hallucination detection",
            "selfcheckgpt",
            "generative large language models",
            "black box",
            "factuality assessment"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output.\""
    },
    {
        "title": "Revisiting Instruction Fine-tuned Model Evaluation to Guide Industrial Applications",
        "authors": [
            "Manuel Faysse",
            "Gautier Viaud",
            "Céline Hudelot",
            "Pierre Colombo"
        ],
        "published": "2023",
        "summary": "Instruction Fine-Tuning (IFT) is a powerful paradigm that strengthens the zero-shot capabilities of Large Language Models (LLMs), but in doing so induces new evaluation metric requirements. We show LLM-based metrics to be well adapted to these requirements, and leverage them to conduct an investigation of task-specialization strategies, quantifying the trade-offs that emerge in practical industrial settings. Our findings offer practitioners actionable insights for real-world IFT model deployment.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.559.pdf",
        "keywords": [
            "fine tuned",
            "instruction fine tuning",
            "evaluation"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Instruction Fine-Tuning (IFT) is a powerful paradigm that strengthens the zero-shot capabilities of Large Language Models (LLMs), but in doing so induces new evaluation metric requirements.\"\n\nThis evidence does not explicitly discuss limitations of LLMs, but rather mentions that IFT induces new evaluation metric requirements, which can be seen as a limitation of the IFT method rather than LLMs",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Instruction Fine-Tuning (IFT) is a powerful paradigm that strengthens the zero-shot capabilities of Large Language Models (LLMs), but in doing so induces new evaluation metric requirements.\"\n\nThis evidence does not explicitly discuss limitations of LLMs, but rather mentions that IFT induces new evaluation metric requirements, which can be seen as a limitation of the IFT method rather than LLMs"
    },
    {
        "title": "Understanding the Role of Input Token Characters in Language Models: How Does Information Loss Affect Performance?",
        "authors": [
            "Ahmed Alajrami",
            "Katerina Margatina",
            "Nikolaos Aletras"
        ],
        "published": "2023",
        "summary": "Understanding how and what pre-trained language models (PLMs) learn about language is an open challenge in natural language processing. Previous work has focused on identifying whether they capture semantic and syntactic information, and how the data or the pre-training objective affects their performance. However, to the best of our knowledge, no previous work has specifically examined how information loss in input token characters affects the performance of PLMs. In this study, we address this gap by pre-training language models using small subsets of characters from individual tokens. Surprisingly, we find that pre-training even under extreme settings, i.e. using only one character of each token, the performance retention in standard NLU benchmarks and probing tasks compared to full-token models is high. For instance, a model pre-trained only on single first characters from tokens achieves performance retention of approximately 90% and 77% of the full-token model in SuperGLUE and GLUE tasks, respectively.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.563.pdf",
        "keywords": [
            "language",
            "language models",
            "token models",
            "token characters",
            "retention",
            "performance retention",
            "information loss",
            "input token characters"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, to the best of our knowledge, no previous work has specifically examined how information loss in input token characters affects the performance of PLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, to the best of our knowledge, no previous work has specifically examined how information loss in input token characters affects the performance of PLMs.\""
    },
    {
        "title": "Improved Unsupervised Chinese Word Segmentation Using Pre-trained Knowledge and Pseudo-labeling Transfer",
        "authors": [
            "Hsiu-Wen Li",
            "Ying-Jia Lin",
            "Yi-Ting Li",
            "Chun Lin",
            "Hung-Yu Kao"
        ],
        "published": "2023",
        "summary": "Unsupervised Chinese word segmentation (UCWS) has made progress by incorporating linguistic knowledge from pre-trained language models using parameter-free probing techniques. However, such approaches suffer from increased training time due to the need for multiple inferences using a pre-trained language model to perform word segmentation. This work introduces a novel way to enhance UCWS performance while maintaining training efficiency. Our proposed method integrates the segmentation signal from the unsupervised segmental language model to the pre-trained BERT classifier under a pseudo-labeling framework. Experimental results demonstrate that our approach achieves state-of-the-art performance on the eight UCWS tasks while considerably reducing the training time compared to previous approaches.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.564.pdf",
        "keywords": [
            "pseudo labeling",
            "word segmentation",
            "unsupervised chinese word segmentation",
            "pseudo labeling transfer"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, such approaches suffer from increased training time due to the need for multiple inferences using a pre-trained language model to perform word segmentation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, such approaches suffer from increased training time due to the need for multiple inferences using a pre-trained language model to perform word segmentation.\""
    },
    {
        "title": "EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs",
        "authors": [
            "Hanlin Tang",
            "Yifu Sun",
            "Decheng Wu",
            "Kai Liu",
            "Jianchen Zhu",
            "Zhanhui Kang"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have proven to be very superior to conventional methods in various tasks. However, their expensive computations and high memory requirements are prohibitive for deployment. Model quantization is an effective method for reducing this overhead. The problem is that in most previous works, the quantized model was calibrated using few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks. Hence in this work, we explore an important question: Can we design a data-independent quantization method for LLMs to guarantee its generalization performance? In this work, we propose EasyQuant, a training-free and data-independent weight-only quantization algorithm for LLMs. Our observation indicates that two factors: outliers in the weight and quantization ranges, are essential for reducing the quantization error. Therefore, in EasyQuant, we leave the outliers (less than 1%) unchanged and optimize the quantization range to reduce the reconstruction error. With these methods, we surprisingly find that EasyQuant achieves comparable performance to the original model. Since EasyQuant does not depend on any training data, the generalization performance of quantized LLMs is safely guaranteed. Moreover, EasyQuant can be implemented in parallel so that the quantized model could be attained in a few minutes even for LLMs over 100B. To our best knowledge, we are the first work that achieves almost lossless quantization performance for LLMs under a data-independent setting and our algorithm runs over 10 times faster than the data-dependent methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.565.pdf",
        "keywords": [
            "quantized",
            "lossless quantization",
            "model quantization",
            "outliers",
            "large language models",
            "data free quantization",
            "quantization error",
            "quantization algorithm"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their expensive computations and high memory requirements are prohibitive for deployment.\"\n\nThis abstract mentions a limitation of LLMs (high computational and memory requirements) but does not elaborate on it and focuses on the proposed solution to mitigate this limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, their expensive computations and high memory requirements are prohibitive for deployment.\"\n\nThis abstract mentions a limitation of LLMs (high computational and memory requirements) but does not elaborate on it and focuses on the proposed solution to mitigate this limitation."
    },
    {
        "title": "APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models",
        "authors": [
            "Qifan Wang",
            "Yuning Mao",
            "Jingang Wang",
            "Hanchao Yu",
            "Shaoliang Nie",
            "Sinong Wang",
            "Fuli Feng",
            "Lifu Huang",
            "Xiaojun Quan",
            "Zenglin Xu",
            "Dongfang Liu"
        ],
        "published": "2023",
        "summary": "With the continuous growth of large language models, the process of fine-tuning these models for new tasks has become increasingly parameter-intensive. Prompt tuning, a method that involves tuning a small set of soft prompts, has emerged as an effective and efficient approach for adapting large pre-trained language models. However, most existing prompt tuning approaches only introduce prompts at the input layer, limiting their performance and leaving large rooms for improvement. In this work, we propose a novel Attention Prompt tuning method, namely APrompt, for efficient adaptation of pre-trained language models. We first demonstrate that existing prompt tuning can be considered as a special case of attention prompt tuning. We then formally introduce APrompt, which incorporates query, key, and value prompts into the attention layer to guide the attention computation during fine-tuning. Experimental results on the SuperGLUE benchmark consistently demonstrate that our proposed approach outperforms state-of-the-art baselines and full fine-tuning method with pre-trained models at different scales. In addition, a comprehensive set of ablation studies validate the effectiveness of the prompt design, as well as the efficiency of our approach.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.567.pdf",
        "keywords": [
            "tuning",
            "adaptation",
            "attention",
            "language models",
            "baselines",
            "fine tuning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, most existing prompt tuning approaches only introduce prompts at the input layer, limiting their performance and leaving large rooms for improvement.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, most existing prompt tuning approaches only introduce prompts at the input layer, limiting their performance and leaving large rooms for improvement.\""
    },
    {
        "title": "What’s “up” with vision-language models? Investigating their struggle with spatial reasoning",
        "authors": [
            "Amita Kamath",
            "Jack Hessel",
            "Kai-Wei Chang"
        ],
        "published": "2023",
        "summary": "Recent vision-language (VL) models are powerful, but can they reliably distinguish “right” from “left”? We curate three new corpora to quantify model comprehension of such basic spatial relations. These tests isolate spatial reasoning more precisely than existing datasets like VQAv2, e.g., our What’sUp benchmark contains sets of photographs varying only the spatial relations of objects, keeping their identity fixed (see Figure 1: models must comprehend not only the usual case of a dog under a table, but also, the same dog on top of the same table). We evaluate 18 VL models, finding that all perform poorly, e.g., BLIP finetuned on VQAv2, which nears human parity on VQAv2, achieves 56% accuracy on our benchmarks vs. humans at 99%. We conclude by studying causes of this surprising behavior, finding: 1) that popular vision-language pretraining corpora like LAION-2B contain little reliable data for learning spatial relationships; and 2) that basic modeling interventions like up-weighting preposition-containing instances or fine-tuning on our corpora are not sufficient to address the challenges our benchmarks pose. We are hopeful that these corpora will facilitate further research, and we release our data and code at https://github.com/amitakamath/whatsup_vlms.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.568.pdf",
        "keywords": [
            "vision language",
            "model comprehension",
            "spatial reasoning",
            "perform",
            "struggle"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We evaluate 18 VL models, finding that all perform poorly... We conclude by studying causes of this surprising behavior, finding: 1) that popular vision-language pretraining corpora like LAION-2B contain little reliable data for learning spatial relationships; and 2) that basic modeling interventions like up-weighting preposition-containing instances or fine-tuning on our corpora are",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We evaluate 18 VL models, finding that all perform poorly... We conclude by studying causes of this surprising behavior, finding: 1) that popular vision-language pretraining corpora like LAION-2B contain little reliable data for learning spatial relationships; and 2) that basic modeling interventions like up-weighting preposition-containing instances or fine-tuning on our corpora are"
    },
    {
        "title": "Learning Preference Model for LLMs via Automatic Preference Data Generation",
        "authors": [
            "Shijia Huang",
            "Jianqiao Zhao",
            "Yanyang Li",
            "Liwei Wang"
        ],
        "published": "2023",
        "summary": "Despite the advanced capacities of the state-of-the-art large language models (LLMs), they suffer from issues of hallucination, stereotype, etc. Preference models play an important role in LLM alignment, yet training preference models predominantly rely on human-annotated data. This reliance limits their versatility and scalability. In this paper, we propose learning the preference model for LLMs via automatic preference data generation (AutoPM). Our approach involves both In-Breadth Data Generation, which elicits pairwise preference data from LLMs following the helpful-honest-harmless (HHH) criteria, and In-Depth Data Generation, which enriches the dataset with responses spanning a wide quality range. With HHH-guided preference data, our approach simultaneously enables the LLMs to learn human preferences and align with human values. Quantitative assessments on five benchmark datasets demonstrate the reliability and potential of AutoPM, pointing out a more general and scalable way to improve LLM performance.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.570.pdf",
        "keywords": [
            "preference data generation",
            "automatic preference data generation",
            "preference models",
            "learning preference model",
            "hallucination"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the advanced capacities of the state-of-the-art large language models (LLMs), they suffer from issues of hallucination, stereotype, etc.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Despite the advanced capacities of the state-of-the-art large language models (LLMs), they suffer from issues of hallucination, stereotype, etc.\""
    },
    {
        "title": "StructGPT: A General Framework for Large Language Model to Reason over Structured Data",
        "authors": [
            "Jinhao Jiang",
            "Kun Zhou",
            "Zican Dong",
            "Keming Ye",
            "Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2023",
        "summary": "In this paper, we aim to improve the reasoning ability of large language models (LLMs) over structured data in a unified way. Inspired by the studies on tool augmentation for LLMs, we develop an Iterative Reading-then-Reasoning (IRR) framework to solve question answering tasks based on structured data, called StructGPT. In this framework, we construct the specialized interfaces to collect relevant evidence from structured data (i.e., reading), and let LLMs concentrate on the reasoning task based on the collected information (i.e., reasoning). Specially, we propose an invoking-linearization-generation procedure to support LLMs in reasoning on the structured data with the help of the interfaces. By iterating this procedure with provided interfaces, our approach can gradually approach the target answers to a given query. Experiments conducted on three types of structured data show that StructGPT greatly improves the performance of LLMs, under the few-shot and zero-shot settings.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.574.pdf",
        "keywords": [
            "large language model",
            "iterative reading then reasoning",
            "tool augmentation",
            "reasoning ability",
            "linearization"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the abstract implies that LLMs need improvement in reasoning over structured data, which suggests a limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the abstract implies that LLMs need improvement in reasoning over structured data, which suggests a limitation."
    },
    {
        "title": "Modeling Legal Reasoning: LM Annotation at the Edge of Human Agreement",
        "authors": [
            "Rosamond Thalken",
            "Edward Stiglitz",
            "David Mimno",
            "Matthew Wilkens"
        ],
        "published": "2023",
        "summary": "Generative language models (LMs) are increasingly used for document class-prediction tasks and promise enormous improvements in cost and efficiency. Existing research often examines simple classification tasks, but the capability of LMs to classify on complex or specialized tasks is less well understood. We consider a highly complex task that is challenging even for humans: the classification of legal reasoning according to jurisprudential philosophy. Using a novel dataset of historical United States Supreme Court opinions annotated by a team of domain experts, we systematically test the performance of a variety of LMs. We find that generative models perform poorly when given instructions (i.e. prompts) equal to the instructions presented to human annotators through our codebook. Our strongest results derive from fine-tuning models on the annotated dataset; the best performing model is an in-domain model, LEGAL-BERT. We apply predictions from this fine-tuned model to study historical trends in jurisprudence, an exercise that both aligns with prominent qualitative historical accounts and points to areas of possible refinement in those accounts. Our findings generally sound a note of caution in the use of generative LMs on complex tasks without fine-tuning and point to the continued relevance of human annotation-intensive classification methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.575.pdf",
        "keywords": [
            "legal reasoning",
            "generative language models",
            "generative models",
            "human agreement"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that generative models perform poorly when given instructions (i.e. prompts) equal to the instructions presented to human annotators through our codebook.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We find that generative models perform poorly when given instructions (i.e. prompts) equal to the instructions presented to human annotators through our codebook.\""
    },
    {
        "title": "Model-tuning Via Prompts Makes NLP Models Adversarially Robust",
        "authors": [
            "Mrigank Raman",
            "Pratyush Maini",
            "J Kolter",
            "Zachary Lipton",
            "Danish Pruthi"
        ],
        "published": "2023",
        "summary": "In recent years, NLP practitioners have converged on the following practice: (i) import an off-the-shelf pretrained (masked) language model; (ii) append a multilayer perceptron atop the CLS token’s hidden representation (with randomly initialized weights); and (iii) fine-tune the entire model on a downstream task (MLP-FT). This procedure has produced massive gains on standard NLP benchmarks, but these models remain brittle, even to mild adversarial perturbations. In this work, we demonstrate surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks. Rather than appending an MLP head to make output prediction, MVP appends a prompt template to the input, and makes prediction via text infilling/completion. Across 5 NLP datasets, 4 adversarial attacks, and 3 different models, MVP improves performance against adversarial substitutions by an average of 8% over standard methods and even outperforms adversarial training-based state-of-art defenses by 3.5%. By combining MVP with adversarial training, we achieve further improvements in adversarial robustness while maintaining performance on unperturbed examples. Finally, we conduct ablations to investigate the mechanism underlying these gains. Notably, we find that the main causes of vulnerability of MLP-FT can be attributed to the misalignment between pre-training and fine-tuning tasks, and the randomly initialized MLP parameters.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.576.pdf",
        "keywords": [
            "tuning",
            "model tuning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"these models remain brittle, even to mild adversarial perturbations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"these models remain brittle, even to mild adversarial perturbations.\""
    },
    {
        "title": "Describe Me an Auklet: Generating Grounded Perceptual Category Descriptions",
        "authors": [
            "Bill Noble",
            "Nikolai Ilinykh"
        ],
        "published": "2023",
        "summary": "Human speakers can generate descriptions of perceptual concepts, abstracted from the instance-level. Moreover, such descriptions can be used by other speakers to learn provisional representations of those concepts. Learning and using abstract perceptual concepts is under-investigated in the language-and-vision field. The problem is also highly relevant to the field of representation learning in multi-modal NLP. In this paper, we introduce a framework for testing category-level perceptual grounding in multi-modal language models. In particular, we train separate neural networks to **generate** and **interpret** descriptions of visual categories. We measure the *communicative success* of the two models with the zero-shot classification performance of the interpretation model, which we argue is an indicator of perceptual grounding. Using this framework, we compare the performance of *prototype*- and *exemplar*-based representations. Finally, we show that communicative success exposes performance issues in the generation model, not captured by traditional intrinsic NLG evaluation metrics, and argue that these issues stem from a failure to properly ground language in vision at the category level.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.580.pdf",
        "keywords": [
            "human speakers",
            "category descriptions"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we show that communicative success exposes performance issues in the generation model, not captured by traditional intrinsic NLG evaluation metrics, and argue that these issues stem from a failure to properly ground language in vision at the category level.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"we show that communicative success exposes performance issues in the generation model, not captured by traditional intrinsic NLG evaluation metrics, and argue that these issues stem from a failure to properly ground language in vision at the category level.\""
    },
    {
        "title": "Revisiting Automated Topic Model Evaluation with Large Language Models",
        "authors": [
            "Dominik Stammbach",
            "Vilém Zouhar",
            "Alexander Hoyle",
            "Mrinmaya Sachan",
            "Elliott Ash"
        ],
        "published": "2023",
        "summary": "Topic models help us make sense of large text collections. Automatically evaluating their output and determining the optimal number of topics are both longstanding challenges, with no effective automated solutions to date. This paper proposes using large language models (LLMs) for these tasks. We find that LLMs appropriately assess the resulting topics, correlating more strongly with human judgments than existing automated metrics. However, the setup of the evaluation task is crucial — LLMs perform better on coherence ratings of word sets than on intrustion detection. We find that LLMs can also assist us in guiding us towards a reasonable number of topics. In actual applications, topic models are typically used to answer a research question related to a collection of texts. We can incorporate this research question in the prompt to the LLM, which helps estimating the optimal number of topics.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.581.pdf",
        "keywords": [
            "topic model",
            "language models",
            "perform",
            "coherence"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the setup of the evaluation task is crucial — LLMs perform better on coherence ratings of word sets than on intrustion detection.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the setup of the evaluation task is crucial — LLMs perform better on coherence ratings of word sets than on intrustion detection.\""
    },
    {
        "title": "ORCHID: A Chinese Debate Corpus for Target-Independent Stance Detection and Argumentative Dialogue Summarization",
        "authors": [
            "Xiutian Zhao",
            "Ke Wang",
            "Wei Peng"
        ],
        "published": "2023",
        "summary": "Dialogue agents have been receiving increasing attention for years, and this trend has been further boosted by the recent progress of large language models (LLMs). Stance detection and dialogue summarization are two core tasks of dialogue agents in application scenarios that involve argumentative dialogues. However, research on these tasks is limited by the insufficiency of public datasets, especially for non-English languages. To address this language resource gap in Chinese, we present ORCHID (Oral Chinese Debate), the first Chinese dataset for benchmarking target-independent stance detection and debate summarization. Our dataset consists of 1,218 real-world debates that were conducted in Chinese on 476 unique topics, containing 2,436 stance-specific summaries and 14,133 fully annotated utterances. Besides providing a versatile testbed for future research, we also conduct an empirical study on the dataset and propose an integrated task. The results show the challenging nature of the dataset and suggest a potential of incorporating stance detection in summarization for argumentative dialogue.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.582.pdf",
        "keywords": [
            "dialogue agents",
            "argumentative",
            "argumentative dialogue summarization",
            "dialogue summarization",
            "corpus",
            "orchid",
            "stance detection"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, research on these tasks is limited by the insufficiency of public datasets, especially for non-English languages.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, research on these tasks is limited by the insufficiency of public datasets, especially for non-English languages.\""
    },
    {
        "title": "Query2doc: Query Expansion with Large Language Models",
        "authors": [
            "Liang Wang",
            "Nan Yang",
            "Furu Wei"
        ],
        "published": "2023",
        "summary": "This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo documents. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.585.pdf",
        "keywords": [
            "language models",
            "query expansion",
            "query disambiguation"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"LLMs are trained on web-scale text corpora and are adept at knowledge memorization.\"\n\nThis paper discusses LLMs, but it does not mention any limitations of the models in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"LLMs are trained on web-scale text corpora and are adept at knowledge memorization.\"\n\nThis paper discusses LLMs, but it does not mention any limitations of the models in the abstract."
    },
    {
        "title": "Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration",
        "authors": [
            "Fanqi Wan",
            "Xinting Huang",
            "Tao Yang",
            "Xiaojun Quan",
            "Wei Bi",
            "Shuming Shi"
        ],
        "published": "2023",
        "summary": "Instruction-tuning can be substantially optimized through enhanced diversity, resulting in models capable of handling a broader spectrum of tasks. However, existing data employed for such tuning often exhibit an inadequate coverage of individual domains, limiting the scope for nuanced comprehension and interactions within these areas. To address this deficiency, we propose Explore-Instruct, a novel approach to enhance the data coverage to be used in domain-specific instruction-tuning through active exploration via Large Language Models (LLMs). Built upon representative domain use cases, Explore-Instruct explores a multitude of variations or possibilities by implementing a search algorithm to obtain diversified and domain-focused instruction-tuning data. Our data-centric analysis validates the effectiveness of this proposed approach in improving domain-specific instruction coverage. Moreover, our model’s performance demonstrates considerable advancements over multiple baselines, including those utilizing domain-specific data enhancement. Our findings offer a promising opportunity to improve instruction coverage, especially in domain-specific contexts, thereby advancing the development of adaptable language models. Our code, model weights, and data are public at https://github.com/fanqiwan/Explore-Instruct.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.587.pdf",
        "keywords": [
            "instruct",
            "domain specific instruction tuning",
            "instruction tuning",
            "domain specific instruction coverage",
            "tuning",
            "instruct explores",
            "active exploration",
            "domain specific data",
            "language",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing data employed for such tuning often exhibit an inadequate coverage of individual domains, limiting the scope for nuanced comprehension and interactions within these areas.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, existing data employed for such tuning often exhibit an inadequate coverage of individual domains, limiting the scope for nuanced comprehension and interactions within these areas.\""
    },
    {
        "title": "Just Adjust One Prompt: Enhancing In-Context Dialogue Scoring via Constructing the Optimal Subgraph of Demonstrations and Prompts",
        "authors": [
            "Jiashu Pu",
            "Ling Cheng",
            "Lu Fan",
            "Tangjie Lv",
            "Rongsheng Zhang"
        ],
        "published": "2023",
        "summary": "The use of modern Large Language Models (LLMs) as chatbots still has some problems such as hallucinations and lack of empathy. Identifying these issues can help improve chatbot performance. The community has been continually iterating on reference-free dialogue evaluation methods based on large language models (LLMs) that can be readily applied. However, many of these LLM-based metrics require selecting specific datasets and developing specialized training tasks for different evaluation dimensions (e.g., coherence, informative). The developing step can be time-consuming and may need to be repeated for new evaluation dimensions. To enable efficient and flexible adaptation to diverse needs of dialogue evaluation, we propose a dimension-agnostic scoring method that leverages the in-context learning (ICL) capability of LLMs to learn from human scoring to the fullest extent. Our method has three key features. To begin with, rather than manual prompt crafting, we propose automatically generating prompts, allowing the LLM to observe human labels and summarize the most suitable prompt. Additionally, since the LLM has a token limit and ICL is sensitive to demonstration variations, we train a selector to finely customize demonstrations and prompts for each dialogue input. Finally, during inference, we propose to request the LLM multiple times with a subgraph of demonstrations and prompts that are diverse and suitable to maximize ICL from various human scoring. We validate the efficacy of our method on five datasets, even with a small amount of annotated data, our method outperforms all strong baselines. Code is available at https://github.com/iamlxb3/EMNLP2023-ADOROR.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.590.pdf",
        "keywords": [
            "demonstrations",
            "dialogue",
            "dialogue evaluation",
            "prompts",
            "dialogue input"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The use of modern Large Language Models (LLMs) as chatbots still has some problems such as hallucinations and lack of empathy.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The use of modern Large Language Models (LLMs) as chatbots still has some problems such as hallucinations and lack of empathy.\""
    },
    {
        "title": "EpiK-Eval: Evaluation for Language Models as Epistemic Models",
        "authors": [
            "Gabriele Prato",
            "Jerry Huang",
            "Prasanna Parthasarathi",
            "Shagun Sodhani",
            "Sarath Chandar"
        ],
        "published": "2023",
        "summary": "In the age of artificial intelligence, the role of large language models (LLMs) is becoming increasingly central. Despite their growing prevalence, their capacity to consolidate knowledge from different training documents—a crucial ability in numerous applications—remains unexplored. This paper presents the first study examining the capability of LLMs to effectively combine such information within their parameter space. We introduce EpiK-Eval, a novel question-answering benchmark tailored to evaluate LLMs’ proficiency in formulating a coherent and consistent knowledge representation from segmented narratives. Evaluations across various LLMs reveal significant weaknesses in this domain. We contend that these shortcomings stem from the intrinsic nature of prevailing training objectives. Consequently, we advocate for refining the approach towards knowledge consolidation, as it harbors the potential to dramatically improve their overall effectiveness and performance. The findings from this study offer insights for developing more robust and reliable LLMs. Our code and benchmark are available at https://github.com/chandar-lab/EpiK-Eval",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.593.pdf",
        "keywords": [
            "language models",
            "epistemic models",
            "epik eval",
            "knowledge consolidation"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Evaluations across various LLMs reveal significant weaknesses in this domain.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Evaluations across various LLMs reveal significant weaknesses in this domain.\""
    },
    {
        "title": "On Bilingual Lexicon Induction with Large Language Models",
        "authors": [
            "Yaoyiran Li",
            "Anna Korhonen",
            "Ivan Vulić"
        ],
        "published": "2023",
        "summary": "Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that still, to a large extent, relies on calculating cross-lingual word representations. Inspired by the global paradigm shift in NLP towards Large Language Models (LLMs), we examine the potential of the latest generation of LLMs for the development of bilingual lexicons. We ask the following research question: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for BLI, and how does this approach compare against and complement current BLI approaches? To this end, we systematically study 1) zero-shot prompting for unsupervised BLI and 2) few-shot in-context prompting with a set of seed translation pairs, both without any LLM fine-tuning, as well as 3) standard BLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source text-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two standard BLI benchmarks covering a range of typologically diverse languages. Our work is the first to demonstrate strong BLI capabilities of text-to-text mLLMs. The results reveal that few-shot prompting with in-context examples from nearest neighbours achieves the best performance, establishing new state-of-the-art BLI scores for many language pairs. We also conduct a series of in-depth analyses and ablation studies, providing more insights on BLI with (m)LLMs, also along with their limitations.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.595.pdf",
        "keywords": [
            "language models",
            "bilingual lexicon induction",
            "bilingual lexicons",
            "prompting"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"also along with their limitations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"also along with their limitations.\""
    },
    {
        "title": "Statistical Depth for Ranking and Characterizing Transformer-Based Text Embeddings",
        "authors": [
            "Parker Seegmiller",
            "Sarah Preum"
        ],
        "published": "2023",
        "summary": "The popularity of transformer-based text embeddings calls for better statistical tools for measuring distributions of such embeddings. One such tool would be a method for ranking texts within a corpus by centrality, i.e. assigning each text a number signifying how representative that text is of the corpus as a whole. However, an intrinsic center-outward ordering of high-dimensional text representations is not trivial. A statistical depth is a function for ranking k-dimensional objects by measuring centrality with respect to some observed k-dimensional distribution. We adopt a statistical depth to measure distributions of transformer-based text embeddings, transformer-based text embedding (TTE) depth, and introduce the practical use of this depth for both modeling and distributional inference in NLP pipelines. We first define TTE depth and an associated rank sum test for determining whether two corpora differ significantly in embedding space. We then use TTE depth for the task of in-context learning prompt selection, showing that this approach reliably improves performance over statistical baseline approaches across six text classification tasks. Finally, we use TTE depth and the associated rank sum test to characterize the distributions of synthesized and human-generated corpora, showing that five recent synthetic data augmentation processes cause a measurable distributional shift away from associated human-generated text.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.596.pdf",
        "keywords": [
            "transformer",
            "text embeddings",
            "transformer based text embeddings",
            "statistical depth",
            "rank sum test",
            "text classification"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model",
        "authors": [
            "Kaiyan Zhang",
            "Ning Ding",
            "Biqing Qi",
            "Xuekai Zhu",
            "Xinwei Long",
            "Bowen Zhou"
        ],
        "published": "2023",
        "summary": "Instruction tuning has recently been recognized as an effective way of aligning Large Language Models (LLMs) to enhance their generalization ability across various tasks. However, when tuning publicly accessible, centralized LLMs with private instruction data, privacy concerns are inevitable. While direct transfer of parameterized modules between models is a plausible approach to address this, its implications and effectiveness need further exploration. This paper focuses on Offsite-Tuning (OFT), a representative technique that transfers transformer blocks between centralized LLMs and downstream emulators. Given the limited understanding of the underlying mechanism of OFT, we perform an empirical analysis on LLMs from the perspectives of representation and functional similarity. Interestingly, our findings reveal a unique modular structure within the layers of LLMs that appears to emerge as the model size expands. Simultaneously, we note subtle but potentially significant changes in representation and intermediate predictions across the layers. Inspired by these observations, we propose CRaSh, involving Clustering, Removing, and Sharing, a training-free strategy to derive improved emulators from LLMs. CRaSh significantly boosts performance of OFT with billions of parameters. Furthermore, we investigate the optimal solutions yielded by fine-tuning with and without full model through the lens of loss landscape. Our findings demonstrate a linear connectivity among these optima falling over the same basin, thereby highlighting the effectiveness of CRaSh and OFT.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.597.pdf",
        "keywords": [
            "clustering",
            "language model",
            "performance",
            "tuning",
            "instruction tuning",
            "offsite tuning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, when tuning publicly accessible, centralized LLMs with private instruction data, privacy concerns are inevitable.\"\n\nThis abstract mentions a limitation of LLMs related to privacy concerns when tuning with private instruction data, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, when tuning publicly accessible, centralized LLMs with private instruction data, privacy concerns are inevitable.\"\n\nThis abstract mentions a limitation of LLMs related to privacy concerns when tuning with private instruction data, but it is not the primary focus of the paper."
    },
    {
        "title": "Large Language Models are biased to overestimate profoundness",
        "authors": [
            "Eugenio Herrera-Berg",
            "Tomás Browne",
            "Pablo León-Villagrá",
            "Marc-Lluís Vives",
            "Cristian Calderon"
        ],
        "published": "2023",
        "summary": "Recent advancements in natural language processing by large language models (LLMs), such as GPT-4, have been suggested to approach Artificial General Intelligence. And yet, it is still under dispute whether LLMs possess similar reasoning abilities to humans. This study evaluates GPT-4 and various other LLMs in judging the profoundness of mundane, motivational, and pseudo-profound statements. We found a significant statement-to-statement correlation between the LLMs and humans, irrespective of the type of statements and the prompting technique used. However, LLMs systematically overestimate the profoundness of nonsensical statements, with the exception of Tk-instruct, which uniquely underestimates the profoundness of statements. Only few-shot learning prompts, as opposed to chain-of-thought prompting, draw LLMs ratings closer to humans. Furthermore, this work provides insights into the potential biases induced by Reinforcement Learning from Human Feedback (RLHF), inducing an increase in the bias to overestimate the profoundness of statements.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.599.pdf",
        "keywords": [
            "human feedback",
            "language",
            "large language models",
            "reinforcement learning"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, LLMs systematically overestimate the profoundness of nonsensical statements, with the exception of Tk-instruct, which uniquely underestimates the profoundness of statements.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, LLMs systematically overestimate the profoundness of nonsensical statements, with the exception of Tk-instruct, which uniquely underestimates the profoundness of statements.\""
    },
    {
        "title": "SummEdits: Measuring LLM Ability at Factual Reasoning Through The Lens of Summarization",
        "authors": [
            "Philippe Laban",
            "Wojciech Kryscinski",
            "Divyansh Agarwal",
            "Alexander Fabbri",
            "Caiming Xiong",
            "Shafiq Joty",
            "Chien-Sheng Wu"
        ],
        "published": "2023",
        "summary": "With the recent appearance of LLMs in practical settings, having methods that can effectively detect factual inconsistencies is crucial to reduce the propagation of misinformation and improve trust in model outputs. When testing on existing factual consistency benchmarks, we find that a few large language models (LLMs) perform competitively on classification benchmarks for factual inconsistency detection compared to traditional non-LLM methods. However, a closer analysis reveals issues with existing evaluation benchmarks, affecting evaluation precision. To address this, we propose a new protocol for inconsistency detection benchmark creation and implement it in a 10-domain benchmark called SummEdits. This new benchmark is 20 times more cost-effective per sample than previous benchmarks and highly reproducible, as we estimate inter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with performance close to random chance. The best-performing model, GPT-4, is still 8% below estimated human performance, highlighting the gaps in LLMs’ ability to reason about facts and detect inconsistencies when they occur.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.600.pdf",
        "keywords": [
            "inconsistency detection",
            "factual inconsistency detection",
            "factual reasoning",
            "summarization",
            "classification benchmarks",
            "factual consistency"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, a closer analysis reveals issues with existing evaluation benchmarks, affecting evaluation precision... Most LLMs struggle on SummEdits, with performance close to random chance. The best-performing model, GPT-4, is still 8% below estimated human performance, highlighting the gaps in LLMs’ ability to reason about facts and detect inconsistencies when they occur.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, a closer analysis reveals issues with existing evaluation benchmarks, affecting evaluation precision... Most LLMs struggle on SummEdits, with performance close to random chance. The best-performing model, GPT-4, is still 8% below estimated human performance, highlighting the gaps in LLMs’ ability to reason about facts and detect inconsistencies when they occur.\""
    },
    {
        "title": "Quantifying the redundancy between prosody and text",
        "authors": [
            "Lukas Wolf",
            "Tiago Pimentel",
            "Evelina Fedorenko",
            "Ryan Cotterell",
            "Alex Warstadt",
            "Ethan Wilcox",
            "Tamar Regev"
        ],
        "published": "2023",
        "summary": "Prosody—the suprasegmental component of speech, including pitch, loudness, and tempo—carries critical aspects of meaning. However, the relationship between the information conveyed by prosody vs. by the words themselves remains poorly understood. We use large language models (LLMs) to estimate how much information is redundant between prosody and the words themselves. Using a large spoken corpus of English audiobooks, we extract prosodic features aligned to individual words and test how well they can be predicted from LLM embeddings, compared to non-contextual word embeddings. We find a high degree of redundancy between the information carried by the words and prosodic information across several prosodic features, including intensity, duration, pauses, and pitch contours. Furthermore, a word’s prosodic information is redundant with both the word itself and the context preceding as well as following it. Still, we observe that prosodic features can not be fully predicted from text, suggesting that prosody carries information above and beyond the words. Along with this paper, we release a general-purpose data processing pipeline for quantifying the relationship between linguistic information and extra-linguistic features.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.606.pdf",
        "keywords": [
            "prosody",
            "redundancy",
            "prosodic",
            "spoken corpus",
            "prosodic features",
            "speech",
            "audiobooks",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Still, we observe that prosodic features can not be fully predicted from text, suggesting that prosody carries information above and beyond the words.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Still, we observe that prosodic features can not be fully predicted from text, suggesting that prosody carries information above and beyond the words.\""
    },
    {
        "title": "CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks",
        "authors": [
            "Mete Ismayilzada",
            "Debjit Paul",
            "Syrielle Montariol",
            "Mor Geva",
            "Antoine Bosselut"
        ],
        "published": "2023",
        "summary": "Recent efforts in natural language processing (NLP) commonsense reasoning research have yielded a considerable number of new datasets and benchmarks. However, most of these datasets formulate commonsense reasoning challenges in artificial scenarios that are not reflective of the tasks which real-world NLP systems are designed to solve. In this work, we present CRoW, a manually-curated, multi-task benchmark that evaluates the ability of models to apply commonsense reasoning in the context of six real-world NLP tasks. CRoW is constructed using a multi-stage data collection pipeline that rewrites examples from existing datasets using commonsense-violating perturbations. We use CRoW to study how NLP systems perform across different dimensions of commonsense knowledge, such as physical, temporal, and social reasoning. We find a significant performance gap when NLP systems are evaluated on CRoW compared to humans, showcasing that commonsense reasoning is far from being solved in real-world task settings. We make our dataset and leaderboard available to the research community.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.607.pdf",
        "keywords": [
            "commonsense",
            "benchmarks",
            "benchmarking commonsense reasoning",
            "commonsense reasoning",
            "natural language processing",
            "task benchmark"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.\n\n(Note: Although the paper discusses NLP systems, which may include LLMs, it does not explicitly mention LLMs or their limitations.)\n\nPlease provide the next abstract for evaluation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs.\n\n(Note: Although the paper discusses NLP systems, which may include LLMs, it does not explicitly mention LLMs or their limitations.)\n\nPlease provide the next abstract for evaluation."
    },
    {
        "title": "A Video Is Worth 4096 Tokens: Verbalize Videos To Understand Them In Zero Shot",
        "authors": [
            "Aanisha Bhattacharyya",
            "Yaman K Singla",
            "Balaji Krishnamurthy",
            "Rajiv Ratn Shah",
            "Changyou Chen"
        ],
        "published": "2023",
        "summary": "Multimedia content, such as advertisements and story videos, exhibit a rich blend of creativity and multiple modalities. They incorporate elements like text, visuals, audio, and storytelling techniques, employing devices like emotions, symbolism, and slogans to convey meaning. There is a dearth of large annotated training datasets in the multimedia domain hindering the development of supervised learning models with satisfactory performance for real-world applications. On the other hand, the rise of large language models (LLMs) has witnessed remarkable zero-shot performance in various natural language processing (NLP) tasks, such as emotion classification, question answering, and topic classification. To leverage such advanced techniques to bridge this performance gap in multimedia understanding, we propose verbalizing long videos to generate their descriptions in natural language, followed by performing video-understanding tasks on the generated story as opposed to the original video. Through extensive experiments on fifteen video-understanding tasks, we demonstrate that our method, despite being zero-shot, achieves significantly better results than supervised baselines for video understanding. Furthermore, to alleviate a lack of story understanding benchmarks, we publicly release the first dataset on a crucial task in computational social science on persuasion strategy identification.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.608.pdf",
        "keywords": [
            "language models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"There is a dearth of large annotated training datasets in the multimedia domain hindering the development of supervised learning models with satisfactory performance for real-world applications.\"\n\nThis evidence is brief and does not elaborate on the limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"There is a dearth of large annotated training datasets in the multimedia domain hindering the development of supervised learning models with satisfactory performance for real-world applications.\"\n\nThis evidence is brief and does not elaborate on the limitations of LLMs."
    },
    {
        "title": "Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning",
        "authors": [
            "Lean Wang",
            "Lei Li",
            "Damai Dai",
            "Deli Chen",
            "Hao Zhou",
            "Fandong Meng",
            "Jie Zhou",
            "Xu Sun"
        ],
        "published": "2023",
        "summary": "In-context learning (ICL) emerges as a promising capability of large language models (LLMs) by providing them with demonstration examples to perform diverse tasks. However, the underlying mechanism of how LLMs learn from the provided context remains under-explored. In this paper, we investigate the working mechanism of ICL through an information flow lens. Our findings reveal that label words in the demonstration examples function as anchors: (1) semantic information aggregates into label word representations during the shallow computation layers’ processing; (2) the consolidated information in label words serves as a reference for LLMs’ final predictions. Based on these insights, we introduce an anchor re-weighting method to improve ICL performance, a demonstration compression technique to expedite inference, and an analysis framework for diagnosing ICL errors in GPT2-XL. The promising applications of our findings again validate the uncovered ICL working mechanism and pave the way for future studies.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.609.pdf",
        "keywords": [
            "information flow",
            "context learning",
            "language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the underlying mechanism of how LLMs learn from the provided context remains under-explored.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the underlying mechanism of how LLMs learn from the provided context remains under-explored.\""
    },
    {
        "title": "Prompting Scientific Names for Zero-Shot Species Recognition",
        "authors": [
            "Shubham Parashar",
            "Zhiqiu Lin",
            "Yanan Li",
            "Shu Kong"
        ],
        "published": "2023",
        "summary": "Trained on web-scale image-text pairs, Vision-Language Models (VLMs) such as CLIP can recognize images of common objects in a zero-shot fashion. However, it is underexplored how to use CLIP for zero-shot recognition of highly specialized concepts, e.g., species of birds, plants, and animals, for which their scientific names are written in Latin or Greek. Indeed, CLIP performs poorly for zero-shot species recognition with prompts that use scientific names, e.g., “a photo of Lepus Timidus” (which is a scientific name in Latin). This is because these names are usually not included in CLIP’s training set. To improve performance, we explore using large-language models (LLMs) to generate descriptions (e.g., of species color and shape) and additionally use them in prompts. However, this method improves only marginally. Instead, we are motivated to translate scientific names (e.g., Lepus Timidus) to common English names (e.g., mountain hare) and use such in the prompts. We find that common names are more likely to be included in CLIP’s training set, and prompting them achieves 2~5 times higher accuracy on benchmarking datasets of fine-grained species recognition.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.610.pdf",
        "keywords": [
            "scientific names",
            "species",
            "shot species recognition",
            "language models",
            "zero shot recognition",
            "common english names",
            "common names",
            "prompts"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, this method improves only marginally.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs, but only briefly and in passing, without exploring it in depth. The primary focus of the paper is on proposing a solution to improve performance in zero-shot species recognition.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, this method improves only marginally.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs, but only briefly and in passing, without exploring it in depth. The primary focus of the paper is on proposing a solution to improve performance in zero-shot species recognition."
    },
    {
        "title": "Active Learning for Natural Language Generation",
        "authors": [
            "Yotam Perlitz",
            "Ariel Gera",
            "Michal Shmueli-Scheuer",
            "Dafna Sheinwald",
            "Noam Slonim",
            "Liat Ein-Dor"
        ],
        "published": "2023",
        "summary": "The field of Natural Language Generation (NLG) suffers from a severe shortage of labeled data due to the extremely expensive and time-consuming process involved in manual annotation. A natural approach for coping with this problem is active learning (AL), a well-known machine learning technique for improving annotation efficiency by selectively choosing the most informative examples to label. However, while AL has been well-researched in the context of text classification, its application to NLG remains largely unexplored. In this paper, we present a first systematic study of active learning for NLG, considering a diverse set of tasks and multiple leading selection strategies, and harnessing a strong instruction-tuned model. Our results indicate that the performance of existing AL strategies is inconsistent, surpassing the baseline of random example selection in some cases but not in others. We highlight some notable differences between the classification and generation scenarios, and analyze the selection behaviors of existing AL strategies. Our findings motivate exploring novel approaches for applying AL to generation tasks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.611.pdf",
        "keywords": [
            "natural language generation",
            "active learning"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models",
        "authors": [
            "Orevaoghene Ahia",
            "Sachin Kumar",
            "Hila Gonen",
            "Jungo Kasai",
            "David Mortensen",
            "Noah Smith",
            "Yulia Tsvetkov"
        ],
        "published": "2023",
        "summary": "Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of “tokens” processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API’s pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI’s language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less affordable, to begin with. Through these analyses, we aim to increase transparency around language model APIs’ pricing policies and encourage the vendors to make them more equitable.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.614.pdf",
        "keywords": [
            "language models",
            "supported languages",
            "tokenization",
            "systematic analysis",
            "languages cost",
            "s language model",
            "multilingual benchmarks",
            "transparency"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages.\"; \"We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages.\"; \"We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results.\""
    },
    {
        "title": "Characterizing Mechanisms for Factual Recall in Language Models",
        "authors": [
            "Qinan Yu",
            "Jack Merullo",
            "Ellie Pavlick"
        ],
        "published": "2023",
        "summary": "Language Models (LMs) often must integrate facts they memorized in pretraining with new information that appears in a given context. These two sources can disagree, causing competition within the model, and it is unclear how an LM will resolve the conflict. On a dataset that queries for knowledge of world capitals, we investigate both distributional and mechanistic determinants of LM behavior in such situations. Specifically, we measure the proportion of the time an LM will use a counterfactual prefix (e.g., “The capital of Poland is London”) to overwrite what it learned in pretraining (“Warsaw”). On Pythia and GPT2, the training frequency of both the query country (”Poland”) and the in-context city (”London”) highly affect the models’ likelihood of using the counterfactual. We then use head attribution to identify individual attention heads that either promote the memorized answer or the in-context answer in the logits. By scaling up or down the value vector of these heads, we can control the likelihood of using the in-context answer on new data. This method can increase the rate of generating the in-context answer to 88% of the time simply by scaling a single head at runtime. Our work contributes to a body of evidence showing that we can often localize model behaviors to specific components and provides a proof of concept for how future methods might control model behavior dynamically at runtime.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.615.pdf",
        "keywords": [
            "pretraining",
            "factual recall",
            "memorized",
            "language models",
            "head attribution"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"These two sources can disagree, causing competition within the model, and it is unclear how an LM will resolve the conflict.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"These two sources can disagree, causing competition within the model, and it is unclear how an LM will resolve the conflict.\""
    },
    {
        "title": "MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark",
        "authors": [
            "Dominik Macko",
            "Robert Moro",
            "Adaku Uchendu",
            "Jason Lucas",
            "Michiharu Yamashita",
            "Matúš Pikuliak",
            "Ivan Srba",
            "Thai Le",
            "Dongwon Lee",
            "Jakub Simko",
            "Maria Bielikova"
        ],
        "published": "2023",
        "summary": "There is a lack of research into capabilities of recent LLMs to generate convincing text in languages other than English and into performance of detectors of machine-generated text in multilingual settings. This is also reflected in the available benchmarks which lack authentic texts in languages other than English and predominantly cover older generators. To fill this gap, we introduce MULTITuDE, a novel benchmarking dataset for multilingual machine-generated text detection comprising of 74,081 authentic and machine-generated texts in 11 languages (ar, ca, cs, de, en, es, nl, pt, ru, uk, and zh) generated by 8 multilingual LLMs. Using this benchmark, we compare the performance of zero-shot (statistical and black-box) and fine-tuned detectors. Considering the multilinguality, we evaluate 1) how these detectors generalize to unseen languages (linguistically similar as well as dissimilar) and unseen LLMs and 2) whether the detectors improve their performance when trained on multiple languages.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.616.pdf",
        "keywords": [
            "benchmarking",
            "machine generated text detection",
            "multitude"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"There is a lack of research into capabilities of recent LLMs to generate convincing text in languages other than English and into performance of detectors of machine-generated text in multilingual settings.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"There is a lack of research into capabilities of recent LLMs to generate convincing text in languages other than English and into performance of detectors of machine-generated text in multilingual settings.\""
    },
    {
        "title": "Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference?",
        "authors": [
            "Cheng Zhang",
            "Jianyi Cheng",
            "Ilia Shumailov",
            "George Constantinides",
            "Yiren Zhao"
        ],
        "published": "2023",
        "summary": "The inference of Large language models (LLMs) requires immense computation and memory resources. To curtail these costs, quantisation has emerged as a promising solution, but existing LLM quantisation mainly focuses on 8-bit. In this work, we explore the statistical and learning properties of the LLM layer and attribute the bottleneck of LLM quantisation to numerical scaling offsets. To address this, we adapt block quantisations for LLMs, a family of methods that share scaling factors across packed numbers. Block quantisations efficiently reduce the numerical scaling offsets solely from an arithmetic perspective, without additional treatments in the computational path. Our nearly-lossless quantised 6-bit LLMs achieve a 19× higher arithmetic density and 5× memory density than the float32 baseline, surpassing the prior art 8-bit quantisation by 2.5× in arithmetic density and 1.2× in memory density, without requiring any data calibration or re-training. We also share our insights into sub-8-bit LLM quantisation, including the mismatch between activation and weight distributions, optimal fine-tuning strategies, and a lower quantisation granularity inherent in the statistical properties of LLMs. The latter two tricks enable nearly-lossless 4-bit LLMs on downstream tasks. Our code is open-sourced.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.617.pdf",
        "keywords": [
            "scaling offsets",
            "quantisation",
            "block quantisations"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The inference of Large language models (LLMs) requires immense computation and memory resources.\"\n\nThis evidence indicates that the paper mentions a limitation of LLMs, but it is not the primary focus of the paper. The limitation is mentioned to motivate the proposed solution, but it is not explored in detail.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The inference of Large language models (LLMs) requires immense computation and memory resources.\"\n\nThis evidence indicates that the paper mentions a limitation of LLMs, but it is not the primary focus of the paper. The limitation is mentioned to motivate the proposed solution, but it is not explored in detail."
    },
    {
        "title": "Reducing Sequence Length by Predicting Edit Spans with Large Language Models",
        "authors": [
            "Masahiro Kaneko",
            "Naoaki Okazaki"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in various tasks and gained significant attention. LLMs are also used for local sequence transduction tasks, including grammatical error correction (GEC) and formality style transfer, where most tokens in a source text are kept unchanged. However, the models that generate all target tokens in such tasks have a tendency to simply copy the input text as is, without making needed changes, because the difference between input and output texts is minimal in the training data. This is also inefficient because the computational cost grows quadratically with the target sequence length with Transformer. This paper proposes predicting edit spans for the source text for local sequence transduction tasks. Representing an edit span with a position of the source text and corrected tokens, we can reduce the length of the target sequence and the computational cost for inference. We apply instruction tuning for LLMs on the supervision data of edit spans. Experiments show that the proposed method achieves comparable performance to the baseline in four tasks, paraphrasing, formality style transfer, GEC, and text simplification, despite reducing the length of the target text by as small as 21%. Furthermore, we report that the task-specific fine-tuning with the proposed method achieved state-of-the-art performance in the four tasks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.619.pdf",
        "keywords": [
            "edit",
            "language models",
            "edit span",
            "predicting edit spans",
            "large language models",
            "grammatical error correction"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the models that generate all target tokens in such tasks have a tendency to simply copy the input text as is, without making needed changes, because the difference between input and output texts is minimal in the training data. This is also inefficient because the computational cost grows quadratically with the target sequence length with Transformer.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the models that generate all target tokens in such tasks have a tendency to simply copy the input text as is, without making needed changes, because the difference between input and output texts is minimal in the training data. This is also inefficient because the computational cost grows quadratically with the target sequence length with Transformer.\""
    },
    {
        "title": "Instruct and Extract: Instruction Tuning for On-Demand Information Extraction",
        "authors": [
            "Yizhu Jiao",
            "Ming Zhong",
            "Sha Li",
            "Ruining Zhao",
            "Siru Ouyang",
            "Heng Ji",
            "Jiawei Han"
        ],
        "published": "2023",
        "summary": "Large language models with instruction-following capabilities open the door to a wider group of users. However, when it comes to information extraction – a classic task in natural language processing – most task-specific systems cannot align well with long-tail ad hoc extraction use cases for non-expert users. To address this, we propose a novel paradigm, termed On-Demand Information Extraction, to fulfill the personalized demands of real-world users. Our task aims to follow the instructions to extract the desired content from the associated text and present it in a structured tabular format. The table headers can either be user-specified or inferred contextually by the model. To facilitate research in this emerging area, we present a benchmark named InstructIE, inclusive of both automatically generated training data, as well as the human-annotated test set. Building on InstructIE, we further develop an On-Demand Information Extractor, ODIE. Comprehensive evaluations on our benchmark reveal that ODIE substantially outperforms the existing open-source models of similar size.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.620.pdf",
        "keywords": [
            "extract",
            "on demand information extraction",
            "instruction tuning",
            "language models",
            "instruct",
            "natural language processing"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"most task-specific systems cannot align well with long-tail ad hoc extraction use cases for non-expert users.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"most task-specific systems cannot align well with long-tail ad hoc extraction use cases for non-expert users.\""
    },
    {
        "title": "Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models",
        "authors": [
            "Xiaolei Wang",
            "Xinyu Tang",
            "Xin Zhao",
            "Jingyuan Wang",
            "Ji-Rong Wen"
        ],
        "published": "2023",
        "summary": "The recent success of large language models (LLMs) has shown great potential to develop more powerful conversational recommender systems (CRSs), which rely on natural language conversations to satisfy user needs. In this paper, we embark on an investigation into the utilization of ChatGPT for CRSs, revealing the inadequacy of the existing evaluation protocol. It might overemphasize the matching with ground-truth items annotated by humans while neglecting the interactive nature of CRSs. To overcome the limitation, we further propose an **i**nteractive **Eva**luation approach based on **L**L**M**s, named **iEvaLM**, which harnesses LLM-based user simulators. Our evaluation approach can simulate various system-user interaction scenarios. Through the experiments on two public CRS datasets, we demonstrate notable improvements compared to the prevailing evaluation protocol. Furthermore, we emphasize the evaluation of explainability, and ChatGPT showcases persuasive explanation generation for its recommendations. Our study contributes to a deeper comprehension of the untapped potential of LLMs for CRSs and provides a more flexible and realistic evaluation approach for future research about LLM-based CRSs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.621.pdf",
        "keywords": [
            "language models",
            "recommendations",
            "conversational recommender systems",
            "evaluation",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"revealing the inadequacy of the existing evaluation protocol. It might overemphasize the matching with ground-truth items annotated by humans while neglecting the interactive nature of CRSs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"revealing the inadequacy of the existing evaluation protocol. It might overemphasize the matching with ground-truth items annotated by humans while neglecting the interactive nature of CRSs.\""
    },
    {
        "title": "Expand, Highlight, Generate: RL-driven Document Generation for Passage Reranking",
        "authors": [
            "Arian Askari",
            "Mohammad Aliannejadi",
            "Chuan Meng",
            "Evangelos Kanoulas",
            "Suzan Verberne"
        ],
        "published": "2023",
        "summary": "Generating synthetic training data based on large language models (LLMs) for ranking models has gained attention recently. Prior studies use LLMs to build pseudo query-document pairs by generating synthetic queries from documents in a corpus. In this paper, we propose a new perspective of data augmentation: generating synthetic documents from queries. To achieve this, we propose DocGen, that consists of a three-step pipeline that utilizes the few-shot capabilities of LLMs. DocGen pipeline performs synthetic document generation by (i) expanding, (ii) highlighting the original query, and then (iii) generating a synthetic document that is likely to be relevant to the query. To further improve the relevance between generated synthetic documents and their corresponding queries, we propose DocGen-RL, which regards the estimated relevance of the document as a reward and leverages reinforcement learning (RL) to optimize DocGen pipeline. Extensive experiments demonstrate that DocGen pipeline and DocGen-RL significantly outperform existing state-of-theart data augmentation methods, such as InPars, indicating that our new perspective of generating documents leverages the capacity of LLMs in generating synthetic data more effectively. We release the code, generated data, and model checkpoints to foster research in this area.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.623.pdf",
        "keywords": [
            "passage reranking",
            "document generation",
            "ranking",
            "reinforcement learning",
            "synthetic document generation",
            "synthetic data"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper discusses the capabilities and applications of LLMs but does not mention any limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper discusses the capabilities and applications of LLMs but does not mention any limitations."
    },
    {
        "title": "Transformer-based Live Update Generation for Soccer Matches from Microblog Posts",
        "authors": [
            "Masashi Oshika",
            "Kosuke Yamada",
            "Ryohei Sasano",
            "Koichi Takeda"
        ],
        "published": "2023",
        "summary": "It has been known to be difficult to generate adequate sports updates from a sequence of vast amounts of diverse live tweets, although the live sports viewing experience with tweets is gaining the popularity. In this paper, we focus on soccer matches and work on building a system to generate live updates for soccer matches from tweets so that users can instantly grasp a match’s progress and enjoy the excitement of the match from raw tweets. Our proposed system is based on a large pre-trained language model and incorporates a mechanism to control the number of updates and a mechanism to reduce the redundancy of duplicate and similar updates.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.624.pdf",
        "keywords": [
            "live update generation",
            "soccer matches",
            "microblog",
            "transformer"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"It has been known to be difficult to generate adequate sports updates from a sequence of vast amounts of diverse live tweets\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"It has been known to be difficult to generate adequate sports updates from a sequence of vast amounts of diverse live tweets\""
    },
    {
        "title": "Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews",
        "authors": [
            "Hye Yun",
            "Iain Marshall",
            "Thomas Trikalinos",
            "Byron Wallace"
        ],
        "published": "2023",
        "summary": "Medical systematic reviews play a vital role in healthcare decision making and policy. However, their production is time-consuming, limiting the availability of high-quality and up-to-date evidence summaries. Recent advancements in LLMs offer the potential to automatically generate literature reviews on demand, addressing this issue. However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucination or omission. In healthcare, this can make LLMs unusable at best and dangerous at worst. We conducted 16 interviews with international systematic review experts to characterize the perceived utility and risks of LLMs in the specific context of medical evidence reviews. Experts indicated that LLMs can assist in the writing process by drafting summaries, generating templates, distilling information, and crosschecking information. They also raised concerns regarding confidently composed but inaccurate LLM outputs and other potential downstream harms, including decreased accountability and proliferation of low-quality reviews. Informed by this qualitative analysis, we identify criteria for rigorous evaluation of biomedical LLMs aligned with domain expert views.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.626.pdf",
        "keywords": [
            "systematic review",
            "risks",
            "medical systematic reviews",
            "composed"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucination or omission. In healthcare, this can make LLMs unusable at best and dangerous at worst.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucination or omission. In healthcare, this can make LLMs unusable at best and dangerous at worst.\""
    },
    {
        "title": "Text Rendering Strategies for Pixel Language Models",
        "authors": [
            "Jonas Lotz",
            "Elizabeth Salesky",
            "Phillip Rust",
            "Desmond Elliott"
        ],
        "published": "2023",
        "summary": "Pixel-based language models process text rendered as images, which allows them to handle any script, making them a promising approach to open vocabulary language modelling. However, recent approaches use text renderers that produce a large set of almost-equivalent input patches, which may prove sub-optimal for downstream tasks, due to redundancy in the input representations. In this paper, we investigate four approaches to rendering text in the PIXEL model (Rust et al., 2023), and find that simple character bigram rendering brings improved performance on sentence-level tasks without compromising performance on token-level or multilingual tasks. This new rendering strategy also makes it possible to train a more compact model with only 22M parameters that performs on par with the original 86M parameter model. Our analyses show that character bigram rendering leads to a consistently better model but with an anisotropic patch embedding space, driven by a patch frequency bias, highlighting the connections between image patch- and tokenization-based language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.628.pdf",
        "keywords": [
            "language",
            "text rendering",
            "rendering text",
            "language models",
            "pixel language models",
            "pixel model",
            "tokenization"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, recent approaches use text renderers that produce a large set of almost-equivalent input patches, which may prove sub-optimal for downstream tasks, due to redundancy in the input representations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, recent approaches use text renderers that produce a large set of almost-equivalent input patches, which may prove sub-optimal for downstream tasks, due to redundancy in the input representations.\""
    },
    {
        "title": "APoLLo : Unified Adapter and Prompt Learning for Vision Language Models",
        "authors": [
            "Sanjoy Chowdhury",
            "Sayan Nag",
            "Dinesh Manocha"
        ],
        "published": "2023",
        "summary": "The choice of input text prompt plays a critical role in the performance of Vision-Language Pretrained (VLP) models such as CLIP. We present APoLLo, a unified multi-modal approach that combines Adapter and Prompt learning for Vision-Language models. Our method is designed to substantially improve the generalization capabilities of VLP models when they are fine-tuned in a few-shot setting. We introduce trainable cross-attention-based adapter layers in conjunction with vision and language encoders to strengthen the alignment between the two modalities. We enforce consistency between the respective encoder branches (receiving augmented inputs) to prevent overfitting in downstream tasks. Our method is evaluated on three representative tasks: generalization to novel classes, cross-dataset evaluation, and unseen domain shifts. In practice, APoLLo achieves a relative gain up to 6.03% over MaPLe (SOTA) on novel classes for 10 diverse image recognition datasets.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.629.pdf",
        "keywords": [
            "adapter",
            "apollo",
            "vision language models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation of LLMs is mentioned in the abstract, but the focus on improving generalization capabilities implies that existing models may have limitations in this regard.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation of LLMs is mentioned in the abstract, but the focus on improving generalization capabilities implies that existing models may have limitations in this regard."
    },
    {
        "title": "SAMRank: Unsupervised Keyphrase Extraction using Self-Attention Map in BERT and GPT-2",
        "authors": [
            "Byungha Kang",
            "Youhyun Shin"
        ],
        "published": "2023",
        "summary": "We propose a novel unsupervised keyphrase extraction approach, called SAMRank, which uses only a self-attention map in a pre-trained language model (PLM) to determine the importance of phrases. Most recent approaches for unsupervised keyphrase extraction mainly utilize contextualized embeddings to capture semantic relevance between words, sentences, and documents. However, due to the anisotropic nature of contextual embeddings, these approaches may not be optimal for semantic similarity measurements. SAMRank as proposed here computes the importance of phrases solely leveraging a self-attention map in a PLM, in this case BERT and GPT-2, eliminating the need to measure embedding similarities. To assess the level of importance, SAMRank combines both global and proportional attention scores through calculations using a self-attention map. We evaluate the SAMRank on three keyphrase extraction datasets: Inspec, SemEval2010, and SemEval2017. The experimental results show that SAMRank outperforms most embedding-based models on both long and short documents and demonstrating that it is possible to use only a self-attention map for keyphrase extraction without relying on embeddings. Source code is available at https://github.com/kangnlp/SAMRank.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.630.pdf",
        "keywords": [
            "keyphrase extraction",
            "self attention",
            "self",
            "self attention map",
            "language"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "Contrastive Learning for Inference in Dialogue",
        "authors": [
            "Etsuko Ishii",
            "Yan Xu",
            "Bryan Wilie",
            "Ziwei Ji",
            "Holy Lovenia",
            "Willy Chung",
            "Pascale Fung"
        ],
        "published": "2023",
        "summary": "Inference, especially those derived from inductive processes, is a crucial component in our conversation to complement the information implicitly or explicitly conveyed by a speaker. While recent large language models show remarkable advances in inference tasks, their performance in inductive reasoning, where not all information is present in the context, is far behind deductive reasoning. In this paper, we analyze the behavior of the models based on the task difficulty defined by the semantic information gap – which distinguishes inductive and deductive reasoning. Our analysis reveals that the information gap between dialogue contexts and desired inferences renders the inductive inference process more challenging. To mitigate this information gap, we investigate a contrastive learning approach by feeding negative samples. Our experiments suggest negative samples help models understand what is wrong and improve their inference generations.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.631.pdf",
        "keywords": [
            "inference",
            "inductive inference",
            "inductive reasoning",
            "deductive reasoning",
            "dialogue",
            "learning",
            "contrastive learning"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While recent large language models show remarkable advances in inference tasks, their performance in inductive reasoning, where not all information is present in the context, is far behind deductive reasoning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"While recent large language models show remarkable advances in inference tasks, their performance in inductive reasoning, where not all information is present in the context, is far behind deductive reasoning.\""
    },
    {
        "title": "Editing Large Language Models: Problems, Methods, and Opportunities",
        "authors": [
            "Yunzhi Yao",
            "Peng Wang",
            "Bozhong Tian",
            "Siyuan Cheng",
            "Zhoubo Li",
            "Shumin Deng",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "published": "2023",
        "summary": "Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to alter the behavior of LLMs efficiently within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in making informed decisions on the selection of the most appropriate method for a specific task or context.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.632.pdf",
        "keywords": [
            "benchmark",
            "methods",
            "editing large language models",
            "errors",
            "evaluation",
            "progressive"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive.\""
    },
    {
        "title": "Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT",
        "authors": [
            "Xiaoshuai Song",
            "Keqing He",
            "Pei Wang",
            "Guanting Dong",
            "Yutao Mou",
            "Jingang Wang",
            "Yunsen Xian",
            "Xunliang Cai",
            "Weiran Xu"
        ],
        "published": "2023",
        "summary": "The tasks of out-of-domain (OOD) intent discovery and generalized intent discovery (GID) aim to extend a closed intent classifier to open-world intent sets, which is crucial to task-oriented dialogue (TOD) systems. Previous methods address them by fine-tuning discriminative models. Recently, although some studies has been exploring the application of large language models (LLMs) represented by ChatGPT to various downstream tasks, it is still unclear for the ability of ChatGPT to discover and incrementally extent OOD intents. In this paper, we comprehensively evaluate ChatGPT on OOD intent discovery and GID, and then outline the strengths and weaknesses of ChatGPT. Overall, ChatGPT exhibits consistent advantages under zero-shot settings, but is still at a disadvantage compared to fine-tuned models. More deeply, through a series of analytical experiments, we summarize and discuss the challenges faced by LLMs including clustering, domain-specific understanding, and cross-domain in-context learning scenarios. Finally, we provide empirical guidance for future directions to address these challenges.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.636.pdf",
        "keywords": [
            "intent discovery",
            "generalized intent discovery",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"More deeply, through a series of analytical experiments, we summarize and discuss the challenges faced by LLMs including clustering, domain-specific understanding, and cross-domain in-context learning scenarios.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"More deeply, through a series of analytical experiments, we summarize and discuss the challenges faced by LLMs including clustering, domain-specific understanding, and cross-domain in-context learning scenarios.\""
    },
    {
        "title": "The Distributional Hypothesis Does Not Fully Explain the Benefits of Masked Language Model Pretraining",
        "authors": [
            "Ting-Rui Chiang",
            "Dani Yogatama"
        ],
        "published": "2023",
        "summary": "We analyze the masked language modeling pretraining objective function from the perspective of the Distributional Hypothesis. We investigate whether the better sample efficiency and the better generalization capability of models pretrained with masked language modeling can be attributed to the semantic similarity encoded in the pretraining data’s distributional property. Via a synthetic dataset, our analysis suggests that distributional property indeed leads to the better sample efficiency of pretrained masked language models, but does not fully explain the generalization capability. We also conduct an analysis over two real-world datasets and demonstrate that the distributional property does not explain the generalization ability of pretrained natural language models either. Our results illustrate our limited understanding of model pretraining and provide future research directions.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.637.pdf",
        "keywords": [
            "masked",
            "pretraining",
            "masked language",
            "masked language model pretraining",
            "distributional property",
            "sample efficiency"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Via a synthetic dataset, our analysis suggests that distributional property indeed leads to the better sample efficiency of pretrained masked language models, but does not fully explain the generalization capability.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Via a synthetic dataset, our analysis suggests that distributional property indeed leads to the better sample efficiency of pretrained masked language models, but does not fully explain the generalization capability.\""
    },
    {
        "title": "Simple and Effective Input Reformulations for Translation",
        "authors": [
            "Brian Yu",
            "Hansen Lillemark",
            "Kurt Keutzer"
        ],
        "published": "2023",
        "summary": "Foundation language models learn from their finetuning input context in different ways. In this paper, we reformulate inputs during finetuning for challenging translation tasks, leveraging model strengths from pretraining in novel ways to improve downstream performance. These reformulations are simple data level modifications, require no additional collection of training data or modification of data at inference time. They can be applied either on single language pair translation tasks or massively multilingual translation tasks. Experiments with these techniques demonstrate significant performance improvements up to 3.5 chrF++ on the Flores200 translation benchmark. We hope our research accessibly improves finetuning data efficiency, enabling more effective training to scalably improve state-of-the-art performance. Our code is released here.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.638.pdf",
        "keywords": [
            "finetuning",
            "reformulations",
            "input reformulations",
            "translation"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Using Artificial French Data to Understand the Emergence of Gender Bias in Transformer Language Models",
        "authors": [
            "Lina Conti",
            "Guillaume Wisniewski"
        ],
        "published": "2023",
        "summary": "Numerous studies have demonstrated the ability of neural language models to learn various linguistic properties without direct supervision. This work takes an initial step towards exploring the less researched topic of how neural models discover linguistic properties of words, such as gender, as well as the rules governing their usage. We propose to use an artificial corpus generated by a PCFG based on French to precisely control the gender distribution in the training data and determine under which conditions a model correctly captures gender information or, on the contrary, appears gender-biased.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.641.pdf",
        "keywords": [
            "transformer language models",
            "artificial french data",
            "artificial corpus",
            "neural language models",
            "gender bias"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"or, on the contrary, appears gender-biased.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"or, on the contrary, appears gender-biased.\""
    },
    {
        "title": "Learning to Rank Context for Named Entity Recognition Using a Synthetic Dataset",
        "authors": [
            "Arthur Amalvy",
            "Vincent Labatut",
            "Richard Dufour"
        ],
        "published": "2023",
        "summary": "While recent pre-trained transformer-based models can perform named entity recognition (NER) with great accuracy, their limited range remains an issue when applied to long documents such as whole novels. To alleviate this issue, a solution is to retrieve relevant context at the document level. Unfortunately, the lack of supervision for such a task means one has to settle for unsupervised approaches. Instead, we propose to generate a synthetic context retrieval training dataset using Alpaca, an instruction-tuned large language model (LLM). Using this dataset, we train a neural context retriever based on a BERT model that is able to find relevant context for NER. We show that our method outperforms several retrieval baselines for the NER task on an English literary dataset composed of the first chapter of 40 books.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.642.pdf",
        "keywords": [
            "named entity recognition",
            "synthetic dataset",
            "learning to rank context"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While recent pre-trained transformer-based models can perform named entity recognition (NER) with great accuracy, their limited range remains an issue when applied to long documents such as whole novels.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While recent pre-trained transformer-based models can perform named entity recognition (NER) with great accuracy, their limited range remains an issue when applied to long documents such as whole novels.\""
    },
    {
        "title": "Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting",
        "authors": [
            "Preethi Lahoti",
            "Nicholas Blumm",
            "Xiao Ma",
            "Raghavendra Kotikalapudi",
            "Sahitya Potluri",
            "Qijun Tan",
            "Hansa Srinivasan",
            "Ben Packer",
            "Ahmad Beirami",
            "Alex Beutel",
            "Jilin Chen"
        ],
        "published": "2023",
        "summary": "A crucial challenge for generative large language models (LLMs) is diversity: when a user’s prompt is under-specified, models may follow implicit assumptions while generating a response, which may result in homogenization of the responses, as well as certain demographic groups being under-represented or even erased from the generated responses. In this paper, we formalize the problem diversity of representation in LLM generations. We present evaluation datasets and propose metrics to measure diversity in generated responses along people and culture axes. We find that LLMs understand the notion of diversity, and that they can reason and critique their own responses for that goal. This finding motivated a new prompting technique called collective-critique and self-voting (CCSV) to self-improve people diversity of LLMs by tapping into its diversity reasoning capabilities, without relying on handcrafted examples or prompt tuning. Extensive empirical experiments with both human and automated evaluations show that our proposed approach is effective at improving people and culture diversity, and outperforms all baseline methods by a large margin.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.643.pdf",
        "keywords": [
            "self voting",
            "diversity",
            "collective critique",
            "large language models",
            "demographic groups",
            "demographic representation",
            "generative large language models",
            "metrics"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"when a user’s prompt is under-specified, models may follow implicit assumptions while generating a response, which may result in homogenization of the responses, as well as certain demographic groups being under-represented or even erased from the generated responses.\"\n\nThis abstract mentions a limitation of LLMs (lack of diversity in responses and potential for under-representation or erasure of certain",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"when a user’s prompt is under-specified, models may follow implicit assumptions while generating a response, which may result in homogenization of the responses, as well as certain demographic groups being under-represented or even erased from the generated responses.\"\n\nThis abstract mentions a limitation of LLMs (lack of diversity in responses and potential for under-representation or erasure of certain"
    },
    {
        "title": "Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection",
        "authors": [
            "Xinlin Peng",
            "Ying Zhou",
            "Ben He",
            "Le Sun",
            "Yingfei Sun"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have exhibited remarkable capabilities in text generation tasks. However, the utilization of these models carries inherent risks, including but not limited to plagiarism, the dissemination of fake news, and issues in educational exercises. Although several detectors have been proposed to address these concerns, their effectiveness against adversarial perturbations, specifically in the context of student essay writing, remains largely unexplored. This paper aims to bridge this gap by constructing AIG-ASAP, an AI-generated student essay dataset, employing a range of text perturbation methods that are expected to generate high-quality essays while evading detection. Through empirical experiments, we assess the performance of current AIGC detectors on the AIG-ASAP dataset. The results reveal that the existing detectors can be easily circumvented using straightforward automatic adversarial attacks. Specifically, we explore word substitution and sentence substitution perturbation methods that effectively evade detection while maintaining the quality of the generated essays. This highlights the urgent need for more accurate and robust methods to detect AI-generated student essays in the education domain. Code and data are released for public use.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.644.pdf",
        "keywords": [
            "ghostwriters",
            "sentence substitution",
            "language models",
            "aig asap",
            "adversarial"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the utilization of these models carries inherent risks, including but not limited to plagiarism, the dissemination of fake news, and issues in educational exercises.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, the utilization of these models carries inherent risks, including but not limited to plagiarism, the dissemination of fake news, and issues in educational exercises.\""
    },
    {
        "title": "Contextual Interaction for Argument Post Quality Assessment",
        "authors": [
            "Yiran Wang",
            "Xuanang Chen",
            "Ben He",
            "Le Sun"
        ],
        "published": "2023",
        "summary": "Recently, there has been an increased emphasis on assessing the quality of natural language arguments. Existing approaches primarily focus on evaluating the quality of individual argument posts. However, they often fall short when it comes to effectively distinguishing arguments that possess a narrow quality margin. To address this limitation, this paper delves into two alternative methods for modeling the relative quality of different arguments. These approaches include: 1) Supervised contrastive learning that captures the intricate interactions between arguments. By incorporating this approach, we aim to enhance the assessment of argument quality by effectively distinguishing between arguments with subtle differences in quality. 2) Large language models (LLMs) with in-context examples that harness the power of LLMs and enrich them with in-context examples. Through extensive evaluation and analysis on the publicly available IBM-Rank-30k dataset, we demonstrate the superiority of our contrastive argument quality assessment approach over state-of-the-art baselines. On the other hand, while LLMs with in-context examples showcase a commendable ability to identify high-quality argument posts, they exhibit relatively limited efficacy in discerning between argument posts with a narrow quality gap.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.645.pdf",
        "keywords": [
            "quality",
            "argument quality assessment",
            "argument quality",
            "argument post quality assessment",
            "arguments",
            "argument posts",
            "contextual interaction",
            "context"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"they exhibit relatively limited efficacy in discerning between argument posts with a narrow quality gap.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"they exhibit relatively limited efficacy in discerning between argument posts with a narrow quality gap.\""
    },
    {
        "title": "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations",
        "authors": [
            "Zhuoyan Li",
            "Hangxiao Zhu",
            "Zhuoran Lu",
            "Ming Yin"
        ],
        "published": "2023",
        "summary": "The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.647.pdf",
        "keywords": [
            "text classification"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks\"; \"We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks\"; \"We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation.\""
    },
    {
        "title": "People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection",
        "authors": [
            "Indira Sen",
            "Dennis Assenmacher",
            "Mattia Samory",
            "Isabelle Augenstein",
            "Wil Aalst",
            "Claudia Wagner"
        ],
        "published": "2023",
        "summary": "NLP models are used in a variety of critical social computing tasks, such as detecting sexist, racist, or otherwise hateful content. Therefore, it is imperative that these models are robust to spurious features. Past work has attempted to tackle such spurious features using training data augmentation, including Counterfactually Augmented Data (CADs). CADs introduce minimal changes to existing training data points and flip their labels; training on them may reduce model dependency on spurious features. However, manually generating CADs can be time-consuming and expensive. Hence in this work, we assess if this task can be automated using generative NLP models. We automatically generate CADs using Polyjuice, ChatGPT, and Flan-T5, and evaluate their usefulness in improving model robustness compared to manually-generated CADs. By testing both model performance on multiple out-of-domain test sets and individual data point efficacy, our results show that while manual CADs are still the most effective, CADs generated by ChatGPT come a close second. One key reason for the lower performance of automated methods is that the changes they introduce are often insufficient to flip the original label.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.649.pdf",
        "keywords": [
            "language detection",
            "model robustness",
            "augmented data"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"One key reason for the lower performance of automated methods is that the changes they introduce are often insufficient to flip the original label.\"\n\nThis abstract mentions a limitation of LLMs in generating Counterfactually Augmented Data (CADs), specifically that the changes they introduce are often insufficient to flip the original label, but the primary focus is on the evaluation of automated methods for generating CADs",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"One key reason for the lower performance of automated methods is that the changes they introduce are often insufficient to flip the original label.\"\n\nThis abstract mentions a limitation of LLMs in generating Counterfactually Augmented Data (CADs), specifically that the changes they introduce are often insufficient to flip the original label, but the primary focus is on the evaluation of automated methods for generating CADs"
    },
    {
        "title": "Unraveling Feature Extraction Mechanisms in Neural Networks",
        "authors": [
            "Xiaobing Sun",
            "Jiaxi Li",
            "Wei Lu"
        ],
        "published": "2023",
        "summary": "The underlying mechanism of neural networks in capturing precise knowledge has been the subject of consistent research efforts. In this work, we propose a theoretical approach based on Neural Tangent Kernels (NTKs) to investigate such mechanisms. Specifically, considering the infinite network width, we hypothesize the learning dynamics of target models may intuitively unravel the features they acquire from training data, deepening our insights into their internal mechanisms. We apply our approach to several fundamental models and reveal how these models leverage statistical features during gradient descent and how they are integrated into final decisions. We also discovered that the choice of activation function can affect feature extraction. For instance, the use of the ReLU activation function could potentially introduce a bias in features, providing a plausible explanation for its replacement with alternative functions in recent pre-trained language models. Additionally, we find that while self-attention and CNN models may exhibit limitations in learning n-grams, multiplication-based models seem to excel in this area. We verify these theoretical findings through experiments and find that they can be applied to analyze language modeling tasks, which can be regarded as a special variant of classification. Our work may offer insights into the roles and capacities of fundamental modules within deep neural networks including large language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.650.pdf",
        "keywords": [
            "feature extraction",
            "neural tangent kernels",
            "neural networks",
            "activation function"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We also discovered that the choice of activation function can affect feature extraction. For instance, the use of the ReLU activation function could potentially introduce a bias in features, providing a plausible explanation for its replacement with alternative functions in recent pre-trained language models. Additionally, we find that while self-attention and CNN models may exhibit limitations in learning n-grams, multiplication-based models seem to",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We also discovered that the choice of activation function can affect feature extraction. For instance, the use of the ReLU activation function could potentially introduce a bias in features, providing a plausible explanation for its replacement with alternative functions in recent pre-trained language models. Additionally, we find that while self-attention and CNN models may exhibit limitations in learning n-grams, multiplication-based models seem to"
    },
    {
        "title": "What do Deck Chairs and Sun Hats Have in Common? Uncovering Shared Properties in Large Concept Vocabularies",
        "authors": [
            "Amit Gajbhiye",
            "Zied Bouraoui",
            "Na Li",
            "Usashi Chatterjee",
            "Luis Espinosa-Anke",
            "Steven Schockaert"
        ],
        "published": "2023",
        "summary": "Concepts play a central role in many applications. This includes settings where concepts have to be modelled in the absence of sentence context. Previous work has therefore focused on distilling decontextualised concept embeddings from language models. But concepts can be modelled from different perspectives, whereas concept embeddings typically mostly capture taxonomic structure. To address this issue, we propose a strategy for identifying what different concepts, from a potentially large concept vocabulary, have in common with others. We then represent concepts in terms of the properties they share with the other concepts. To demonstrate the practical usefulness of this way of modelling concepts, we consider the task of ultra-fine entity typing, which is a challenging multi-label classification problem. We show that by augmenting the label set with shared properties, we can improve the performance of the state-of-the-art models for this task.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.654.pdf",
        "keywords": [
            "deck chairs",
            "concept embeddings",
            "fine entity typing",
            "classification"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"But concepts can be modelled from different perspectives, whereas concept embeddings typically mostly capture taxonomic structure.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"But concepts can be modelled from different perspectives, whereas concept embeddings typically mostly capture taxonomic structure.\""
    },
    {
        "title": "Goal-Driven Explainable Clustering via Language Descriptions",
        "authors": [
            "Zihan Wang",
            "Jingbo Shang",
            "Ruiqi Zhong"
        ],
        "published": "2023",
        "summary": "Unsupervised clustering is widely used to explore large corpora, but existing formulations neither consider the users’ goals nor explain clusters’ meanings. We propose a new task formulation, “Goal-Driven Clustering with Explanations” (GoalEx), which represents both the goal and the explanations as free-form language descriptions. For example, to categorize the errors made by a summarization system, the input to GoalEx is a corpus of annotator-written comments for system-generated summaries and a goal description “cluster the comments based on why the annotators think the summary is imperfect.”; the outputs are text clusters each with an explanation (“this cluster mentions that the summary misses important context information.”), which relates to the goal and accurately explains which comments should (not) belong to a cluster. To tackle GoalEx, we prompt a language model with “[corpus subset] + [goal] + Brainstorm a list of explanations each representing a cluster.”; then we classify whether each sample belongs to a cluster based on its explanation; finally, we use integer linear programming to select a subset of candidate clusters to cover most samples while minimizing overlaps. Under both automatic and human evaluation on corpora with or without labels, our method produces more accurate and goal-related explanations than prior methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.657.pdf",
        "keywords": [
            "clustering",
            "categorize",
            "goal description",
            "language"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"prompt a language model with [corpus subset] + [goal] + Brainstorm a list of explanations each representing a cluster.\"\n\nThis paper does not explicitly discuss limitations of LLMs, but it does mention using a language model in the context of the proposed method, which is why it is rated as 1.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"prompt a language model with [corpus subset] + [goal] + Brainstorm a list of explanations each representing a cluster.\"\n\nThis paper does not explicitly discuss limitations of LLMs, but it does mention using a language model in the context of the proposed method, which is why it is rated as 1."
    },
    {
        "title": "Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models",
        "authors": [
            "Jirui Qi",
            "Raquel Fernández",
            "Arianna Bisazza"
        ],
        "published": "2023",
        "summary": "Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts inserted in English reveal a clear pattern whereby the new piece of knowledge transfers only to languages with which English has a high RankC score. All code and data are released at https://github.com/Betswish/Cross-Lingual-Consistency.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.658.pdf",
        "keywords": [
            "language",
            "language models",
            "cross lingual consistency",
            "factual knowledge"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"large variations are observed across languages\"; \"increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"large variations are observed across languages\"; \"increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency.\""
    },
    {
        "title": "Learning from Mistakes via Cooperative Study Assistant for Large Language Models",
        "authors": [
            "Danqing Wang",
            "Lei Li"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have demonstrated their potential to refine their generation based on their own feedback. However, the feedback from LLM itself is often inaccurate, thereby limiting its benefits. In this paper, we propose Study Assistant for Large LAnguage Model (SALAM), a novel framework with an auxiliary agent to assist the main LLM in learning from mistakes through interactive cooperation. In the gathering phase, the student assistant agent probes the main LLM, analyzes its errors, and collects the interaction in a mistake memory. During the examination phase, the study assistant provides guidelines by retrieving relevant cases to help the main LLM anticipate and avoid similar errors. We first investigate the effectiveness of a general study assistant and then customize it to provide LLM-specific guidance through imitation learning from successful guidance experiences. Our experiments on three LLMs using two challenging frameworks demonstrate that SALAM can significantly boost LLMs by an accuracy margin of up to 6.6 on BBH and 12.6 on BBQ.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.659.pdf",
        "keywords": [
            "large language models",
            "agent"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the feedback from LLM itself is often inaccurate, thereby limiting its benefits.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the feedback from LLM itself is often inaccurate, thereby limiting its benefits.\""
    },
    {
        "title": "Bridging the Digital Divide: Performance Variation across Socio-Economic Factors in Vision-Language Models",
        "authors": [
            "Joan Nwatu",
            "Oana Ignat",
            "Rada Mihalcea"
        ],
        "published": "2023",
        "summary": "Despite the impressive performance of current AI models reported across various tasks, performance reports often do not include evaluations of how these models perform on the specific groups that will be impacted by these technologies. Among the minority groups under-represented in AI, data from low-income households are often overlooked in data collection and model evaluation. We evaluate the performance of a state-of-the-art vision-language model (CLIP) on a geo-diverse dataset containing household images associated with different income values (DollarStreet) and show that performance inequality exists among households of different income levels. Our results indicate that performance for the poorer groups is consistently lower than the wealthier groups across various topics and countries. We highlight insights that can help mitigate these issues and propose actionable steps for economic-level inclusive AI development.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.660.pdf",
        "keywords": [
            "digital divide",
            "socio economic"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our results indicate that performance for the poorer groups is consistently lower than the wealthier groups across various topics and countries.\"\n\nThis paper discusses the limitations of vision-language models, a type of multimodal LLM, in terms of performance inequality across different socioeconomic groups, specifically households of different income levels.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Our results indicate that performance for the poorer groups is consistently lower than the wealthier groups across various topics and countries.\"\n\nThis paper discusses the limitations of vision-language models, a type of multimodal LLM, in terms of performance inequality across different socioeconomic groups, specifically households of different income levels."
    },
    {
        "title": "Conceptor-Aided Debiasing of Large Language Models",
        "authors": [
            "Li Yifei",
            "Lyle Ungar",
            "João Sedoc"
        ],
        "published": "2023",
        "summary": "Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus. Many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy. We use *conceptors*–a soft projection method–to identify and remove the bias subspace in LLMs such as BERT and GPT. We propose two methods of applying conceptors (1) bias subspace projection by post-processing by the conceptor NOT operation; and (2) a new architecture, conceptor-intervened BERT (CI-BERT), which explicitly incorporates the conceptor projection into all layers during training. We find that conceptor post-processing achieves state-of-the-art (SoTA) debiasing results while maintaining LLMs’ performance on the GLUE benchmark. Further, it is robust in various scenarios and can mitigate intersectional bias efficiently by its AND operation on the existing bias subspaces. Although CI-BERT’s training takes all layers’ bias into account and can beat its post-processing counterpart in bias mitigation, CI-BERT reduces the language model accuracy. We also show the importance of carefully constructing the bias subspace. The best results are obtained by removing outliers from the list of biased words, combining them (via the OR operation), and computing their embeddings using the sentences from a cleaner corpus.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.661.pdf",
        "keywords": [
            "debiasing",
            "large language models",
            "bias subspace projection"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus. Many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus. Many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy.\""
    },
    {
        "title": "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations",
        "authors": [
            "Myra Cheng",
            "Tiziano Piccardi",
            "Diyi Yang"
        ],
        "published": "2023",
        "summary": "Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations’ susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.669.pdf",
        "keywords": [
            "caricature",
            "exaggeration",
            "llm",
            "llm simulations"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes.\""
    },
    {
        "title": "FaMeSumm: Investigating and Improving Faithfulness of Medical Summarization",
        "authors": [
            "Nan Zhang",
            "Yusen Zhang",
            "Wu Guo",
            "Prasenjit Mitra",
            "Rui Zhang"
        ],
        "published": "2023",
        "summary": "Summaries of medical text shall be faithful by being consistent and factual with source inputs, which is an important but understudied topic for safety and efficiency in healthcare. In this paper, we investigate and improve faithfulness in summarization on a broad range of medical summarization tasks. Our investigation reveals that current summarization models often produce unfaithful outputs for medical input text. We then introduce FaMeSumm, a framework to improve faithfulness by fine-tuning pre-trained language models based on medical knowledge. FaMeSumm performs contrastive learning on designed sets of faithful and unfaithful summaries, and it incorporates medical terms and their contexts to encourage faithful generation of medical terms. We conduct comprehensive experiments on three datasets in two languages: health question and radiology report summarization datasets in English, and a patient-doctor dialogue dataset in Chinese. Results demonstrate that FaMeSumm is flexible and effective by delivering consistent improvements over mainstream language models such as BART, T5, mT5, and PEGASUS, yielding state-of-the-art performances on metrics for faithfulness and general quality. Human evaluation by doctors also shows that FaMeSumm generates more faithful outputs. Our code is available at https://github.com/psunlpgroup/FaMeSumm.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.673.pdf",
        "keywords": [
            "faithfulness",
            "summarization",
            "language",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our investigation reveals that current summarization models often produce unfaithful outputs for medical input text.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our investigation reveals that current summarization models often produce unfaithful outputs for medical input text.\""
    },
    {
        "title": "Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning",
        "authors": [
            "Saibo Geng",
            "Martin Josifoski",
            "Maxime Peyrard",
            "Robert West"
        ],
        "published": "2023",
        "summary": "Despite their impressive performance, large language models (LMs) still struggle with reliably generating complex output structures when not finetuned to follow the required output format exactly. To address this issue, grammar-constrained decoding (GCD) can be used to control the generation of LMs, guaranteeing that the output follows a given structure. Most existing GCD methods are, however, limited to specific tasks, such as parsing or code generation. In this work, we demonstrate that formal grammars can describe the output space for a much wider range of tasks and argue that GCD can serve as a unified framework for structured NLP tasks in general. For increased flexibility, we introduce input-dependent grammars, which allow the grammar to depend on the input and thus enable the generation of different output structures for different inputs. We then empirically demonstrate the power and flexibility of GCD-enhanced LMs on (1) information extraction, (2) entity disambiguation, and (3) constituency parsing. Our results indicate that grammar-constrained LMs substantially outperform unconstrained LMs or even beat task-specific finetuned models. Grammar constraints thus hold great promise for harnessing off-the-shelf LMs for a wide range of structured NLP tasks, especially where training data is scarce or finetuning is expensive. Code and data: https://github.com/epfl-dlab/GCD.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.674.pdf",
        "keywords": [
            "finetuning",
            "grammar constrained decoding",
            "entity disambiguation",
            "grammar constraints"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"large language models (LMs) still struggle with reliably generating complex output structures when not finetuned to follow the required output format exactly.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"large language models (LMs) still struggle with reliably generating complex output structures when not finetuned to follow the required output format exactly.\""
    },
    {
        "title": "Systematic word meta-sense extension",
        "authors": [
            "Lei Yu"
        ],
        "published": "2023",
        "summary": "The meaning of polysemous words often varies in a highly productive yet predictable way. Generalizing the regularity between conventional senses to derive novel word meaning is crucial for automated processing of non-literal language uses such as figurative expressions. We introduce a novel task called systematic word meta-sense extension (SWORME) to test and improve language models’ ability to extend word meaning to denote new semantic domains (also called meta-senses) that bear regular semantic relations with existing senses. We found that language models prefer incremental lexical semantic change toward conceptually similar meta-senses such as logical metonymy, and are much worse at predicting highly non-literal meaning extensions such as metaphors. We propose a novel analogy-based method of word meaning extension, and show that it effectively improves language model systematicity in making both gradual and radical types of meta-sense extension. We further demonstrate that learning systematic meta-sense extensions benefits language models on multiple benchmarks of figurative language understanding.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.675.pdf",
        "keywords": [
            "language",
            "meta sense extension",
            "meta senses",
            "language model systematicity",
            "word meaning extension",
            "language models",
            "semantic change"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We found that language models prefer incremental lexical semantic change toward conceptually similar meta-senses such as logical metonymy, and are much worse at predicting highly non-literal meaning extensions such as metaphors.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We found that language models prefer incremental lexical semantic change toward conceptually similar meta-senses such as logical metonymy, and are much worse at predicting highly non-literal meaning extensions such as metaphors.\""
    },
    {
        "title": "Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory",
        "authors": [
            "Ziang Xiao",
            "Susu Zhang",
            "Vivian Lai",
            "Q. Vera Liao"
        ],
        "published": "2023",
        "summary": "We address a fundamental challenge in Natural Language Generation (NLG) model evaluation—the design and evaluation of evaluation metrics. Recognizing the limitations of existing automatic metrics and noises from how current human evaluation was conducted, we propose MetricEval, a framework informed by measurement theory, the foundation of educational test design, for conceptualizing and evaluating the reliability and validity of NLG evaluation metrics. The framework formalizes the source of measurement error and offers statistical tools for evaluating evaluation metrics based on empirical data. With our framework, one can quantify the uncertainty of the metrics to better interpret the result. To exemplify the use of our framework in practice, we analyzed a set of evaluation metrics for summarization and identified issues related to conflated validity structure in human-eval and reliability in LLM-based metrics. Through MetricEval, we aim to promote the design, evaluation, and interpretation of valid and reliable metrics to advance robust and effective NLG models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.676.pdf",
        "keywords": [
            "reliability",
            "evaluation",
            "evaluation metrics",
            "measurement theory",
            "nlg evaluation metrics",
            "validity",
            "measurement error",
            "and reliable",
            "metriceval",
            "natural language generation",
            "framework"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"issues related to conflated validity structure in human-eval and reliability in LLM-based metrics.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"issues related to conflated validity structure in human-eval and reliability in LLM-based metrics.\""
    },
    {
        "title": "Revisiting the Knowledge Injection Frameworks",
        "authors": [
            "Peng Fu",
            "Yiming Zhang",
            "Haobo Wang",
            "Weikang Qiu",
            "Junbo Zhao"
        ],
        "published": "2023",
        "summary": "In recent years, large language models (LLMs), such as GPTs, have attained great impact worldwide. However, how to adapt these LLMs to better suit the vertical domain-specific tasks by utilizing external knowledge remains not completely solved. Indeed, there have emerged a few works on this line where most of them rely on an alignment heuristic that is built to inject the corresponding knowledge tuple into the associated text sample. However, despite the promise, we identify a pivotal problem in this work ubiquitously. Simply put, we find that injecting unaligned (i.e., random) knowledge tuple into the LLMs achieves comparable (and sometimes better) results than the aligned knowledge being injected. We therefore take a thorough investigation of this frustrating finding on a variety of related prior work and further provide a chain of potential interpretations for the phenomenon. Based on all that, we offer a simple remediated technique. Briefly, the core of this technique roots in an ideological emphasis on the pruning and purification of the external knowledge base to be injected into LLMs. At last, we show that by integrating this technique into most (if not all) knowledge injection frameworks and recent LLMs, it manages to overcome the aforementioned sanity problem and further pushes the boundary of the performance of the domain-adaptive LLMs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.677.pdf",
        "keywords": [
            "knowledge injection frameworks"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, how to adapt these LLMs to better suit the vertical domain-specific tasks by utilizing external knowledge remains not completely solved.\"; \"Simply put, we find that injecting unaligned (i.e., random) knowledge tuple into the LLMs achieves comparable (and sometimes better) results than the aligned knowledge being injected.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, how to adapt these LLMs to better suit the vertical domain-specific tasks by utilizing external knowledge remains not completely solved.\"; \"Simply put, we find that injecting unaligned (i.e., random) knowledge tuple into the LLMs achieves comparable (and sometimes better) results than the aligned knowledge being injected.\""
    },
    {
        "title": "We Are What We Repeatedly Do: Inducing and Deploying Habitual Schemas in Persona-Based Responses",
        "authors": [
            "Benjamin Kane",
            "Lenhart Schubert"
        ],
        "published": "2023",
        "summary": "Many practical applications of dialogue technology require the generation of responses according to a particular developer-specified persona. While a variety of personas can be elicited from recent large language models, the opaqueness and unpredictability of these models make it desirable to be able to specify personas in an explicit form. In previous work, personas have typically been represented as sets of one-off pieces of self-knowledge that are retrieved by the dialogue system for use in generation. However, in realistic human conversations, personas are often revealed through story-like narratives that involve rich habitual knowledge – knowledge about kinds of events that an agent often participates in (e.g., work activities, hobbies, sporting activities, favorite entertainments, etc.), including typical goals, sub-events, preconditions, and postconditions of those events. We capture such habitual knowledge using an explicit schema representation, and propose an approach to dialogue generation that retrieves relevant schemas to condition a large language model to generate persona-based responses. Furthermore, we demonstrate a method for bootstrapping the creation of such schemas by first generating generic passages from a set of simple facts, and then inducing schemas from the generated passages.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.678.pdf",
        "keywords": [
            "habitual",
            "personas",
            "habitual knowledge",
            "habitual schemas",
            "dialogue"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the opaqueness and unpredictability of these models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"the opaqueness and unpredictability of these models\""
    },
    {
        "title": "Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model",
        "authors": [
            "Qi Jia",
            "Siyu Ren",
            "Yizhu Liu",
            "Kenny Zhu"
        ],
        "published": "2023",
        "summary": "Despite tremendous improvements in natural language generation, summarization models still suffer from the unfaithfulness issue. Previous work evaluates faithfulness either using models trained on the other tasks or in-domain synthetic data, or prompting a large model such as ChatGPT. This paper proposes to do zero-shot faithfulness evaluation simply with a moderately-sized foundation language model. We introduce a new metric FFLM, which is a combination of probability changes based on the intuition that prefixing a piece of text that is consistent with the output will increase the probability of predicting the output. Experiments show that FFLM performs competitively with or even outperforms ChatGPT on both inconsistency detection and faithfulness rating with 24x fewer parameters. FFLM also achieves improvements over other strong baselines.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.679.pdf",
        "keywords": [
            "faithfulness",
            "summarization",
            "text summarization",
            "inconsistency detection",
            "shot faithfulness",
            "zero shot faithfulness",
            "foundation language model",
            "prompting"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"summarization models still suffer from the unfaithfulness issue.\"\n\nNote: Although the paper discusses limitations of summarization models, which may be related to LLMs, it does not explicitly discuss limitations of LLMs. However, it does mention a limitation of summarization models, which is a related area, so the rating is 1.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"summarization models still suffer from the unfaithfulness issue.\"\n\nNote: Although the paper discusses limitations of summarization models, which may be related to LLMs, it does not explicitly discuss limitations of LLMs. However, it does mention a limitation of summarization models, which is a related area, so the rating is 1."
    },
    {
        "title": "A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models",
        "authors": [
            "Yi Zhou",
            "Jose Camacho-Collados",
            "Danushka Bollegala"
        ],
        "published": "2023",
        "summary": "Various types of social biases have been reported with pretrained Masked Language Models (MLMs) in prior work. However, multiple underlying factors are associated with an MLM such as its model size, size of the training data, training objectives, the domain from which pretraining data is sampled, tokenization, and languages present in the pretrained corpora, to name a few. It remains unclear as to which of those factors influence social biases that are learned by MLMs. To study the relationship between model factors and the social biases learned by an MLM, as well as the downstream task performance of the model, we conduct a comprehensive study over 39 pretrained MLMs covering different model sizes, training objectives, tokenization methods, training data domains and languages. Our results shed light on important factors often neglected in prior literature, such as tokenization or model objectives.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.683.pdf",
        "keywords": [
            "social biases",
            "task performance",
            "predictive factor analysis",
            "masked"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, multiple underlying factors are associated with an MLM such as its model size, size of the training data, training objectives, the domain from which pretraining data is sampled, tokenization, and languages present in the pretrained corpora, to name a few. It remains unclear as to which of those factors influence social biases that are learned by MLMs.\"\n\nThis paper discusses limitations of",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, multiple underlying factors are associated with an MLM such as its model size, size of the training data, training objectives, the domain from which pretraining data is sampled, tokenization, and languages present in the pretrained corpora, to name a few. It remains unclear as to which of those factors influence social biases that are learned by MLMs.\"\n\nThis paper discusses limitations of"
    },
    {
        "title": "Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation",
        "authors": [
            "Wanrong Zhu",
            "Xinyi Wang",
            "Yujie Lu",
            "Tsu-Jui Fu",
            "Xin Wang",
            "Miguel Eckstein",
            "William Wang"
        ],
        "published": "2023",
        "summary": "The field of text-to-image (T2I) generation has garnered significant attention both within the research community and among everyday users. Despite the advancements of T2I models, a common issue encountered by users is the need for repetitive editing of input prompts in order to receive a satisfactory image, which is time-consuming and labor-intensive. Given the demonstrated text generation power of large-scale language models, such as GPT-k, we investigate the potential of utilizing such models to improve the prompt editing process for T2I generation. We conduct a series of experiments to compare the common edits made by humans and GPT-k, evaluate the performance of GPT-k in prompting T2I, and examine factors that may influence this process. We found that GPT-k models focus more on inserting modifiers while humans tend to replace words and phrases, which includes changes to the subject matter. Experimental results show that GPT-k are more effective in adjusting modifiers rather than predicting spontaneous changes in the primary subject matters. Adopting the edit suggested by GPT-k models may reduce the percentage of remaining edits by 20-30%.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.685.pdf",
        "keywords": [
            "gpt k",
            "text to image",
            "text to image generation",
            "text generation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experimental results show that GPT-k are more effective in adjusting modifiers rather than predicting spontaneous changes in the primary subject matters.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Experimental results show that GPT-k are more effective in adjusting modifiers rather than predicting spontaneous changes in the primary subject matters.\""
    },
    {
        "title": "Explicit Planning Helps Language Models in Logical Reasoning",
        "authors": [
            "Hongyu Zhao",
            "Kangrui Wang",
            "Mo Yu",
            "Hongyuan Mei"
        ],
        "published": "2023",
        "summary": "Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose LEAP, a novel system that uses language models to perform multi-step logical reasoning and incorporates explicit planning into the inference procedure. Explicit planning enables the system to make more informed reasoning decisions at each step by looking ahead into their future effects. Moreover, we propose a training strategy that safeguards the planning process from being led astray by spurious features. Our full system significantly outperforms other competing methods on multiple standard datasets. When using small T5 models as its core selection and deduction components, our system performs competitively compared to GPT-3 despite having only about 1B parameters (i.e., 175 times smaller than GPT-3). When using GPT-3.5, it significantly outperforms chain-of-thought prompting on the challenging PrOntoQA dataset. We have conducted extensive empirical studies to demonstrate that explicit planning plays a crucial role in the system’s performance.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.688.pdf",
        "keywords": [
            "explicit planning",
            "logical reasoning",
            "multi step logical reasoning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned, but the paper implies that LLMs may benefit from explicit planning to improve their logical reasoning, suggesting that they may not perform well in this area without additional assistance.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: No explicit limitations of LLMs are mentioned, but the paper implies that LLMs may benefit from explicit planning to improve their logical reasoning, suggesting that they may not perform well in this area without additional assistance."
    },
    {
        "title": "clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents",
        "authors": [
            "Kranti Chalamalasetti",
            "Jana Götze",
            "Sherzod Hakimov",
            "Brielen Madureira",
            "Philipp Sadler",
            "David Schlangen"
        ],
        "published": "2023",
        "summary": "Recent work has proposed a methodology for the systematic evaluation of “Situated Language Understanding Agents” — agents that operate in rich linguistic and non-linguistic contexts — through testing them in carefully constructed interactive settings. Other recent work has argued that Large Language Models (LLMs), if suitably set up, can be understood as (simulators of) such agents. A connection suggests itself, which this paper explores: Can LLMs be evaluated meaningfully by exposing them to constrained game-like settings that are built to challenge specific capabilities? As a proof of concept, this paper investigates five interaction settings, showing that current chat-optimised LLMs are, to an extent, capable of following game-play instructions. Both this capability and the quality of the game play, measured by how well the objectives of the different games are met, follows the development cycle, with newer models generally performing better. The metrics even for the comparatively simple example games are far from being saturated, suggesting that the proposed instrument will remain to have diagnostic value.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.689.pdf",
        "keywords": [
            "games",
            "game play",
            "language models",
            "chat optimised",
            "chat optimized language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The metrics even for the comparatively simple example games are far from being saturated, suggesting that the proposed instrument will remain to have diagnostic value.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs in passing, specifically that the evaluation metrics are \"far from being saturated\", implying that there is still room for improvement in the performance of LLMs in game-like settings.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The metrics even for the comparatively simple example games are far from being saturated, suggesting that the proposed instrument will remain to have diagnostic value.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs in passing, specifically that the evaluation metrics are \"far from being saturated\", implying that there is still room for improvement in the performance of LLMs in game-like settings."
    },
    {
        "title": "UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers",
        "authors": [
            "Jon Saad-Falcon",
            "Omar Khattab",
            "Keshav Santhanam",
            "Radu Florian",
            "Martin Franz",
            "Salim Roukos",
            "Avirup Sil",
            "Md Sultan",
            "Christopher Potts"
        ],
        "published": "2023",
        "summary": "Many information retrieval tasks require large labeled datasets for fine-tuning. However, such datasets are often unavailable, and their utility for real-world applications can diminish quickly due to domain shifts. To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply. The method begins by generating a small number of synthetic queries using an expensive LLM. After that, a much less expensive one is used to create large numbers of synthetic queries, which are used to fine-tune a family of reranker models. These rerankers are then distilled into a single efficient retriever for use in the target domain. We show that this technique boosts zero-shot accuracy in long-tail domains and achieves substantially lower latency than standard reranking methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.693.pdf",
        "keywords": [
            "reranking",
            "unsupervised domain adaptation",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions the high cost of using LLMs and the need for a cheaper alternative.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions the high cost of using LLMs and the need for a cheaper alternative."
    },
    {
        "title": "Data Similarity is Not Enough to Explain Language Model Performance",
        "authors": [
            "Gregory Yauney",
            "Emily Reif",
            "David Mimno"
        ],
        "published": "2023",
        "summary": "Large language models achieve high performance on many but not all downstream tasks. The interaction between pretraining data and task data is commonly assumed to determine this variance: a task with data that is more similar to a model’s pretraining data is assumed to be easier for that model. We test whether distributional and example-specific similarity measures (embedding-, token- and model-based) correlate with language model performance through a large-scale comparison of the Pile and C4 pretraining datasets with downstream benchmarks. Similarity correlates with performance for multilingual datasets, but in other benchmarks, we surprisingly find that similarity metrics are not correlated with accuracy or even each other. This suggests that the relationship between pretraining data and downstream tasks is more complex than often assumed.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.695.pdf",
        "keywords": [
            "similar",
            "language models",
            "language model performance",
            "data similarity",
            "pretraining data",
            "performance",
            "more complex",
            "benchmarks"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models achieve high performance on many but not all downstream tasks... This suggests that the relationship between pretraining data and downstream tasks is more complex than often assumed.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Large language models achieve high performance on many but not all downstream tasks... This suggests that the relationship between pretraining data and downstream tasks is more complex than often assumed.\""
    },
    {
        "title": "Zero-shot Sharpness-Aware Quantization for Pre-trained Language Models",
        "authors": [
            "Miaoxi Zhu",
            "Qihuang Zhong",
            "Li Shen",
            "Liang Ding",
            "Juhua Liu",
            "Bo Du",
            "Dacheng Tao"
        ],
        "published": "2023",
        "summary": "Quantization is a promising approach for reducing memory overhead and accelerating inference, especially in large pre-trained language model (PLM) scenarios. While having no access to original training data due to security and privacy concerns has emerged the demand for zero-shot quantization. Most of the cutting-edge zero-shot quantization methods primarily 1) apply to computer vision tasks, and 2) neglect of overfitting problem in the generative adversarial learning process, leading to sub-optimal performance. Motivated by this, we propose a novel zero-shot sharpness-aware quantization (ZSAQ) framework for the zero-shot quantization of various PLMs. The key algorithm in solving ZSAQ is the SAM-SGA optimization, which aims to improve the quantization accuracy and model generalization via optimizing a minimax problem. We theoretically prove the convergence rate for the minimax optimization problem and this result can be applied to other nonconvex-PL minimax optimization frameworks. Extensive experiments on 11 tasks demonstrate that our method brings consistent and significant performance gains on both discriminative and generative PLMs, i.e., up to +6.98 average score. Furthermore, we empirically validate that our method can effectively improve the model generalization.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.696.pdf",
        "keywords": [
            "quantization",
            "shot quantization",
            "sharpness aware quantization",
            "zero shot quantization",
            "pre trained language model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Most of the cutting-edge zero-shot quantization methods primarily 1) apply to computer vision tasks, and 2) neglect of overfitting problem in the generative adversarial learning process, leading to sub-optimal performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Most of the cutting-edge zero-shot quantization methods primarily 1) apply to computer vision tasks, and 2) neglect of overfitting problem in the generative adversarial learning process, leading to sub-optimal performance.\""
    },
    {
        "title": "Deciphering Stereotypes in Pre-Trained Language Models",
        "authors": [
            "Weicheng Ma",
            "Henry Scheible",
            "Brian Wang",
            "Goutham Veeramachaneni",
            "Pratim Chowdhary",
            "Alan Sun",
            "Andrew Koulogeorge",
            "Lili Wang",
            "Diyi Yang",
            "Soroush Vosoughi"
        ],
        "published": "2023",
        "summary": "Warning: This paper contains content that is stereotypical and may be upsetting. This paper addresses the issue of demographic stereotypes present in Transformer-based pre-trained language models (PLMs) and aims to deepen our understanding of how these biases are encoded in these models. To accomplish this, we introduce an easy-to-use framework for examining the stereotype-encoding behavior of PLMs through a combination of model probing and textual analyses. Our findings reveal that a small subset of attention heads within PLMs are primarily responsible for encoding stereotypes and that stereotypes toward specific minority groups can be identified using attention maps on these attention heads. Leveraging these insights, we propose an attention-head pruning method as a viable approach for debiasing PLMs, without compromising their language modeling capabilities or adversely affecting their performance on downstream tasks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.697.pdf",
        "keywords": [
            "pre trained language models",
            "language modeling",
            "stereotypes"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This paper addresses the issue of demographic stereotypes present in Transformer-based pre-trained language models (PLMs) and aims to deepen our understanding of how these biases are encoded in these models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"This paper addresses the issue of demographic stereotypes present in Transformer-based pre-trained language models (PLMs) and aims to deepen our understanding of how these biases are encoded in these models.\""
    },
    {
        "title": "An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives",
        "authors": [
            "Young Min Cho",
            "Sunny Rai",
            "Lyle Ungar",
            "João Sedoc",
            "Sharath Guntuku"
        ],
        "published": "2023",
        "summary": "Mental health conversational agents (a.k.a. chatbots) are widely studied for their potential to offer accessible support to those experiencing mental health challenges. Previous surveys on the topic primarily consider papers published in either computer science or medicine, leading to a divide in understanding and hindering the sharing of beneficial knowledge between both domains. To bridge this gap, we conduct a comprehensive literature review using the PRISMA framework, reviewing 534 papers published in both computer science and medicine. Our systematic review reveals 136 key papers on building mental health-related conversational agents with diverse characteristics of modeling and experimental design techniques. We find that computer science papers focus on LLM techniques and evaluating response quality using automated metrics with little attention to the application while medical papers use rule-based conversational agents and outcome metrics to measure the health outcomes of participants. Based on our findings on transparency, ethics, and cultural heterogeneity in this review, we provide a few recommendations to help bridge the disciplinary divide and enable the cross-disciplinary development of mental health conversational agents.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.698.pdf",
        "keywords": [
            "mental health",
            "mental health conversational agents",
            "integrative survey",
            "conversational agents"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that computer science papers focus on LLM techniques and evaluating response quality using automated metrics with little attention to the application\"\n\nThis abstract mentions a limitation of LLMs in the context of mental health conversational agents, specifically that they are often evaluated using automated metrics with little attention to the application, but it does not elaborate on this limitation and focuses more on the survey and recommendations",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We find that computer science papers focus on LLM techniques and evaluating response quality using automated metrics with little attention to the application\"\n\nThis abstract mentions a limitation of LLMs in the context of mental health conversational agents, specifically that they are often evaluated using automated metrics with little attention to the application, but it does not elaborate on this limitation and focuses more on the survey and recommendations"
    },
    {
        "title": "Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark",
        "authors": [
            "Minje Choi",
            "Jiaxin Pei",
            "Sagar Kumar",
            "Chang Shu",
            "David Jurgens"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have been shown to perform well at a variety of syntactic, discourse, and reasoning tasks. While LLMs are increasingly deployed in many forms including conversational agents that interact with humans, we lack a grounded benchmark to measure how well LLMs understand social language. Here, we introduce a new theory-driven benchmark, SocKET, that contains 58 NLP tasks testing social knowledge which we group into five categories: humor & sarcasm, offensiveness, sentiment & emotion, and trustworthiness. In tests on the benchmark, we demonstrate that current models attain only moderate performance but reveal significant potential for task transfer among different types and categories of tasks, which were predicted from theory. Through zero-shot evaluations, we show that pretrained models already possess some innate but limited capabilities of social language understanding and training on one category of tasks can improve zero-shot testing on others. Our benchmark provides a systematic way to analyze model performance on an important dimension of language and points to clear room for improvement to build more socially-aware LLMs. The resources are released at https://github.com/minjechoi/SOCKET.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.699.pdf",
        "keywords": [
            "social knowledge",
            "social language",
            "language models",
            "socket",
            "agents"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"In tests on the benchmark, we demonstrate that current models attain only moderate performance but reveal significant potential for task transfer among different types and categories of tasks... Our benchmark provides a systematic way to analyze model performance on an important dimension of language and points to clear room for improvement to build more socially-aware LLMs.\"\n\nThis rating is given because the paper extensively analyzes the limitations of LLM",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"In tests on the benchmark, we demonstrate that current models attain only moderate performance but reveal significant potential for task transfer among different types and categories of tasks... Our benchmark provides a systematic way to analyze model performance on an important dimension of language and points to clear room for improvement to build more socially-aware LLMs.\"\n\nThis rating is given because the paper extensively analyzes the limitations of LLM"
    },
    {
        "title": "Axiomatic Preference Modeling for Longform Question Answering",
        "authors": [
            "Corby Rosset",
            "Guoqing Zheng",
            "Victor Dibia",
            "Ahmed Awadallah",
            "Paul Bennett"
        ],
        "published": "2023",
        "summary": "The remarkable abilities of large language models (LLMs) like ChatGPT and GPT-4 partially stem from the post-training processes involving human preferences encoded within a reward model as part of a Reinforcement Learning from Human Feedback (RLHF) regimen. These reward models (RMs) often lack direct knowledge of why, or under what principles, the preferences annotations were made. In this study, we identify principles that guide RMs to better align with human preferences, and then develop an axiomatic framework to generate a rich variety of preference signals to uphold them. We use these axiomatic signals to train a model for the scoring answers to longform questions. Our approach yields a Preference Model with only about 220M parameters that agrees with gold human-annotated preference labels more often than GPT-4. The contributions of this work include: training a standalone preference model that can score human- and LLM-generated answers on the same scale; developing an axiomatic framework for generating training data pairs tailored to certain principles; and showing that a small amount of axiomatic signals can help small models outperform GPT-4 in preference scoring. We intend to release our axiomatic data and model.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.702.pdf",
        "keywords": [
            "axiomatic",
            "axiomatic preference modeling",
            "question answering",
            "large language models",
            "preference",
            "preference model",
            "reinforcement learning",
            "axiomatic signals",
            "preference signals",
            "axiomatic data"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The remarkable abilities of large language models (LLMs) like ChatGPT and GPT-4 partially stem from the post-training processes involving human preferences encoded within a reward model as part of a Reinforcement Learning from Human Feedback (RLHF) regimen.\"\n\nThis abstract mentions the limitation of LLMs in the context of the post-training process, but does not elaborate on the limitation",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The remarkable abilities of large language models (LLMs) like ChatGPT and GPT-4 partially stem from the post-training processes involving human preferences encoded within a reward model as part of a Reinforcement Learning from Human Feedback (RLHF) regimen.\"\n\nThis abstract mentions the limitation of LLMs in the context of the post-training process, but does not elaborate on the limitation"
    },
    {
        "title": "Countering Misinformation via Emotional Response Generation",
        "authors": [
            "Daniel Russo",
            "Shane Kaszefski-Yaschuk",
            "Jacopo Staiano",
            "Marco Guerini"
        ],
        "published": "2023",
        "summary": "The proliferation of misinformation on social media platforms (SMPs) poses a significant danger to public health, social cohesion and ultimately democracy. Previous research has shown how social correction can be an effective way to curb misinformation, by engaging directly in a constructive dialogue with users who spread – often in good faith – misleading messages. Although professional fact-checkers are crucial to debunking viral claims, they usually do not engage in conversations on social media. Thereby, significant effort has been made to automate the use of fact-checker material in social correction; however, no previous work has tried to integrate it with the style and pragmatics that are commonly employed in social media communication. To fill this gap, we present VerMouth, the first large-scale dataset comprising roughly 12 thousand claim-response pairs (linked to debunking articles), accounting for both SMP-style and basic emotions, two factors which have a significant role in misinformation credibility and spreading. To collect this dataset we used a technique based on an author-reviewer pipeline, which efficiently combines LLMs and human annotators to obtain high-quality data. We also provide comprehensive experiments showing how models trained on our proposed dataset have significant improvements in terms of output quality and generalization capabilities.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.703.pdf",
        "keywords": [
            "smp style",
            "misinformation",
            "social media",
            "social media communication",
            "social correction",
            "cohesion",
            "emotional"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we used a technique based on an author-reviewer pipeline, which efficiently combines LLMs and human annotators to obtain high-quality data.\"\n\nThis paper mentions the use of LLMs but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"we used a technique based on an author-reviewer pipeline, which efficiently combines LLMs and human annotators to obtain high-quality data.\"\n\nThis paper mentions the use of LLMs but does not discuss any limitations of LLMs."
    },
    {
        "title": "Integrating Language Models into Direct Speech Translation: An Inference-Time Solution to Control Gender Inflection",
        "authors": [
            "Dennis Fucci",
            "Marco Gaido",
            "Sara Papi",
            "Mauro Cettolo",
            "Matteo Negri",
            "Luisa Bentivogli"
        ],
        "published": "2023",
        "summary": "When translating words referring to the speaker, speech translation (ST) systems should not resort to default masculine generics nor rely on potentially misleading vocal traits. Rather, they should assign gender according to the speakers’ preference. The existing solutions to do so, though effective, are hardly feasible in practice as they involve dedicated model re-training on gender-labeled ST data. To overcome these limitations, we propose the first inference-time solution to control speaker-related gender inflections in ST. Our approach partially replaces the (biased) internal language model (LM) implicitly learned by the ST decoder with gender-specific external LMs. Experiments on en→es/fr/it show that our solution outperforms the base models and the best training-time mitigation strategy by up to 31.0 and 1.6 points in gender accuracy, respectively, for feminine forms. The gains are even larger (up to 32.0 and 3.4) in the challenging condition where speakers’ vocal traits conflict with their gender.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.705.pdf",
        "keywords": [
            "speech translation",
            "direct speech translation",
            "integrating language models",
            "gender inflection"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the (biased) internal language model (LM) implicitly learned by the ST decoder\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the (biased) internal language model (LM) implicitly learned by the ST decoder\""
    },
    {
        "title": "StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding",
        "authors": [
            "Cheng Jiayang",
            "Lin Qiu",
            "Tsz Chan",
            "Tianqing Fang",
            "Weiqi Wang",
            "Chunkit Chan",
            "Dongyu Ru",
            "Qipeng Guo",
            "Hongming Zhang",
            "Yangqiu Song",
            "Yue Zhang",
            "Zheng Zhang"
        ],
        "published": "2023",
        "summary": "Analogy-making between narratives is crucial for human reasoning. In this paper, we evaluate the ability to identify and generate analogies by constructing a first-of-its-kind large-scale story-level analogy corpus, StoryAnalogy, which contains 24K story pairs from diverse domains with human annotations on two similarities from the extended Structure-Mapping Theory. We design a set of tests on StoryAnalogy, presenting the first evaluation of story-level analogy identification and generation. Interestingly, we find that the analogy identification tasks are incredibly difficult not only for sentence embedding models but also for the recent large language models (LLMs) such as ChatGPT and LLaMa. ChatGPT, for example, only achieved around 30% accuracy in multiple-choice questions (compared to over 85% accuracy for humans). Furthermore, we observe that the data in StoryAnalogy can improve the quality of analogy generation in LLMs, where a fine-tuned FlanT5-xxl model achieves comparable performance to zero-shot ChatGPT.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.706.pdf",
        "keywords": [
            "narratives",
            "language models",
            "analogy identification",
            "analogies",
            "large language models",
            "storyanalogy",
            "story",
            "generation"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Interestingly, we find that the analogy identification tasks are incredibly difficult not only for sentence embedding models but also for the recent large language models (LLMs) such as ChatGPT and LLaMa. ChatGPT, for example, only achieved around 30% accuracy in multiple-choice questions (compared to over 85% accuracy for humans).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Interestingly, we find that the analogy identification tasks are incredibly difficult not only for sentence embedding models but also for the recent large language models (LLMs) such as ChatGPT and LLaMa. ChatGPT, for example, only achieved around 30% accuracy in multiple-choice questions (compared to over 85% accuracy for humans).\""
    },
    {
        "title": "Crystal: Introspective Reasoners Reinforced with Self-Feedback",
        "authors": [
            "Jiacheng Liu",
            "Ramakanth Pasunuru",
            "Hannaneh Hajishirzi",
            "Yejin Choi",
            "Asli Celikyilmaz"
        ],
        "published": "2023",
        "summary": "Extensive work has shown that the performance and interpretability of commonsense reasoning can be improved via knowledge-augmented reasoning methods, where the knowledge that underpins the reasoning process is explicitly verbalized and utilized. However, existing implementations, including “chain-of-thought” and its variants, fall short in capturing the *introspective* nature of knowledge required in commonsense reasoning, and in accounting for the mutual adaptation between the generation and utilization of knowledge. We propose a novel method to develop an introspective commonsense reasoner, **Crystal**. To tackle commonsense problems, it first introspects for knowledge statements related to the given question, and subsequently makes an informed prediction that is grounded in the previously introspected knowledge. The knowledge introspection and knowledge-grounded reasoning modes of the model are tuned via reinforcement learning to mutually adapt, where the reward derives from the feedback given by the model itself. Experiments show that Crystal significantly outperforms both the standard supervised finetuning and chain-of-thought distilled methods, and enhances the transparency of the commonsense reasoning process. Our work ultimately validates the feasibility and potential of reinforcing a neural model with self-feedback.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.708.pdf",
        "keywords": [
            "commonsense",
            "commonsense reasoning",
            "self feedback",
            "knowledge introspection",
            "reinforcement learning",
            "knowledge grounded reasoning",
            "crystal",
            "adapt"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models",
        "authors": [
            "Jing Xiong",
            "Jianhao Shen",
            "Ye Yuan",
            "Haiming Wang",
            "Yichun Yin",
            "Zhengying Liu",
            "Lin Li",
            "Zhijiang Guo",
            "Qingxing Cao",
            "Yinya Huang",
            "Chuanyang Zheng",
            "Xiaodan Liang",
            "Ming Zhang",
            "Qun Liu"
        ],
        "published": "2023",
        "summary": "Automated theorem proving (ATP) has become an appealing domain for exploring the reasoning ability of the recent successful generative language models. However, current ATP benchmarks are mainly focus on symbolic inference, but rarely involve the understanding of complex number combination reasoning. In this work, we propose TRIGO, an ATP benchmark that not only requires a model to reduce a trigonometric expression with step-by-step proof but also evaluates a generative LM’s reasoning ability on formulas and capability to manipulate, group, and factor number terms. We gather trigonometric expressions and their reduced forms from web, annotate the simplification process manually, and translate it into the “Lean” formal language system. We then automatically generate additional examples from the annotated samples to expand the dataset. Furthermore, we also create three automatically generated training and testing datasets of varying difficulty and distributions. Our extensive experiments show our proposed TRIGO poses a new challenge for advanced generative LM’s including GPT-4 which is pre-trained on a considerable amount of open-source formal theorem-proving language data, and provide a new tool to study the generative LM’s ability on both formal and mathematical reasoning.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.711.pdf",
        "keywords": [
            "generative language models",
            "generative lm",
            "automated theorem proving",
            "s ability"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our extensive experiments show our proposed TRIGO poses a new challenge for advanced generative LM’s including GPT-4 which is pre-trained on a considerable amount of open-source formal theorem-proving language data,\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Our extensive experiments show our proposed TRIGO poses a new challenge for advanced generative LM’s including GPT-4 which is pre-trained on a considerable amount of open-source formal theorem-proving language data,\""
    },
    {
        "title": "Vicarious Offense and Noise Audit of Offensive Speech Classifiers: Unifying Human and Machine Disagreement on What is Offensive",
        "authors": [
            "Tharindu Weerasooriya",
            "Sujan Dutta",
            "Tharindu Ranasinghe",
            "Marcos Zampieri",
            "Christopher Homan",
            "Ashiqur KhudaBukhsh"
        ],
        "published": "2023",
        "summary": "Offensive speech detection is a key component of content moderation. However, what is offensive can be highly subjective. This paper investigates how machine and human moderators disagree on what is offensive when it comes to real-world social web political discourse. We show that (1) there is extensive disagreement among the moderators (humans and machines); and (2) human and large-language-model classifiers are unable to predict how other human raters will respond, based on their political leanings. For (1), we conduct a ***noise audit*** at an unprecedented scale that combines both machine and human responses. For (2), we introduce a first-of-its-kind dataset of ***vicarious offense***. Our noise audit reveals that moderation outcomes vary wildly across different machine moderators. Our experiments with human moderators suggest that political leanings combined with sensitive issues affect both first-person and vicarious offense. The dataset is available through https://github.com/Homan-Lab/voiced.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.713.pdf",
        "keywords": [
            "political leanings",
            "disagreement",
            "vicarious offense",
            "offensive",
            "offensive speech detection",
            "human and machine disagreement",
            "offensive speech classifiers"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our noise audit reveals that moderation outcomes vary wildly across different machine moderators.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Our noise audit reveals that moderation outcomes vary wildly across different machine moderators.\""
    },
    {
        "title": "CESAR: Automatic Induction of Compositional Instructions for Multi-turn Dialogs",
        "authors": [
            "Taha Aksu",
            "Devamanyu Hazarika",
            "Shikib Mehri",
            "Seokhwan Kim",
            "Dilek Hakkani-Tur",
            "Yang Liu",
            "Mahdi Namazifar"
        ],
        "published": "2023",
        "summary": "Instruction-based multitasking has played a critical role in the success of large language models (LLMs) in multi-turn dialog applications. While publicly available LLMs have shown promising performance, when exposed to complex instructions with multiple constraints, they lag against state-of-the-art models like ChatGPT. In this work, we hypothesize that the availability of large-scale complex demonstrations is crucial in bridging this gap. Focusing on dialog applications, we propose a novel framework, CESAR, that unifies a large number of dialog tasks in the same format and allows programmatic induction of complex instructions without any manual effort. We apply CESAR on InstructDial, a benchmark for instruction-based dialog tasks. We further enhance InstructDial with new datasets and tasks and utilize CESAR to induce complex tasks with compositional instructions. This results in a new benchmark called InstructDial++, which includes 63 datasets with 86 basic tasks and 68 composite tasks. Through rigorous experiments, we demonstrate the scalability of CESAR in providing rich instructions. Models trained on InstructDial++ can follow compositional prompts, such as prompts that ask for multiple stylistic constraints.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.717.pdf",
        "keywords": [
            "dialogs",
            "multi turn dialog"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While publicly available LLMs have shown promising performance, when exposed to complex instructions with multiple constraints, they lag against state-of-the-art models like ChatGPT.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While publicly available LLMs have shown promising performance, when exposed to complex instructions with multiple constraints, they lag against state-of-the-art models like ChatGPT.\""
    },
    {
        "title": "From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser for Complex Question Answering over Knowledge Base",
        "authors": [
            "Wangzhen Guo",
            "Linyin Luo",
            "Hanjiang Lai",
            "Jian Yin"
        ],
        "published": "2023",
        "summary": "Parsing questions into executable logical forms has showed impressive results for knowledge-base question answering (KBQA). However, complex KBQA is a more challenging task that requires to perform complex multi-step reasoning. Recently, a new semantic parser called KoPL has been proposed to explicitly model the reasoning processes, which achieved the state-of-the-art on complex KBQA. In this paper, we further explore how to unlock the reasoning ability of semantic parsers by a simple proposed parse-execute-refine paradigm. We refine and improve the KoPL parser by demonstrating the executed intermediate reasoning steps to the KBQA model. We show that such simple strategy can significantly improve the ability of complex reasoning. Specifically, we propose three components: a parsing stage, an execution stage and a refinement stage, to enhance the ability of complex reasoning. The parser uses the KoPL to generate the transparent logical forms. Then, the execution stage aligns and executes the logical forms over knowledge base to obtain intermediate reasoning processes. Finally, the intermediate step-by-step reasoning processes are demonstrated to the KBQA model in the refinement stage. With the explicit reasoning processes, it is much easier to answer the complex questions. Experiments on benchmark dataset shows that the proposed PER-KBQA performs significantly better than the stage-of-the-art baselines on the complex KBQA.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.720.pdf",
        "keywords": [
            "question answering",
            "knowledge base question answering",
            "semantic parser"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs."
    },
    {
        "title": "Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model",
        "authors": [
            "Haikang Deng",
            "Colin Raffel"
        ],
        "published": "2023",
        "summary": "While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute. In this paper, we introduce Reward-Augmented Decoding (RAD), a text generation procedure that uses a small unidirectional reward model to encourage a language model to generate text that has certain properties. Specifically, RAD uses the reward model to score generations as they are produced and rescales sampling probabilities to favor high-reward tokens. By using a unidirectional reward model, RAD can cache activations from prior generation steps to decrease computational overhead. Through experiments on generating non-toxic and sentiment-controlled text, we demonstrate that RAD performs best among methods that change only the generation procedure and matches the performance of state-of-the-art methods that involve re-training the language model. We further validate that RAD is effective on very large language models while incurring a minimal computational overhead.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.721.pdf",
        "keywords": [
            "text generation",
            "controlled text generation",
            "augmented"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute.\""
    },
    {
        "title": "Models See Hallucinations: Evaluating the Factuality in Video Captioning",
        "authors": [
            "Hui Liu",
            "Xiaojun Wan"
        ],
        "published": "2023",
        "summary": "Video captioning aims to describe events in a video with natural language. In recent years, many works have focused on improving captioning models’ performance. However, like other text generation tasks, it risks introducing factual errors not supported by the input video. Factual errors can seriously affect the quality of the generated text, sometimes making it completely unusable. Although factual consistency has received much research attention in text-to-text tasks (e.g., summarization), it is less studied in vision-based text generation. In this work, we conduct the first human evaluation of the factuality in video captioning and annotate two factuality datasets. We find that 56% of the model-generated sentences have factual errors, indicating it is a severe problem in this field, but existing evaluation metrics show little correlation with human factuality annotation. We further propose a weakly-supervised, model-based factuality metric FactVC, which outperforms previous metrics on factuality evaluation of video captioning.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.723.pdf",
        "keywords": [
            "factuality",
            "captioning",
            "factual consistency",
            "video captioning",
            "factual errors"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, like other text generation tasks, it risks introducing factual errors not supported by the input video. Factual errors can seriously affect the quality of the generated text, sometimes making it completely unusable.\"; \"We find that 56% of the model-generated sentences have factual errors, indicating it is a severe problem in this field, but existing evaluation metrics show little correlation with human fact",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, like other text generation tasks, it risks introducing factual errors not supported by the input video. Factual errors can seriously affect the quality of the generated text, sometimes making it completely unusable.\"; \"We find that 56% of the model-generated sentences have factual errors, indicating it is a severe problem in this field, but existing evaluation metrics show little correlation with human fact"
    },
    {
        "title": "Cabbage Sweeter than Cake? Analysing the Potential of Large Language Models for Learning Conceptual Spaces",
        "authors": [
            "Usashi Chatterjee",
            "Amit Gajbhiye",
            "Steven Schockaert"
        ],
        "published": "2023",
        "summary": "The theory of Conceptual Spaces is an influential cognitive-linguistic framework for representing the meaning of concepts. Conceptual spaces are constructed from a set of quality dimensions, which essentially correspond to primitive perceptual features (e.g. hue or size). These quality dimensions are usually learned from human judgements, which means that applications of conceptual spaces tend to be limited to narrow domains (e.g. modelling colour or taste). Encouraged by recent findings about the ability of Large Language Models (LLMs) to learn perceptually grounded representations, we explore the potential of such models for learning conceptual spaces. Our experiments show that LLMs can indeed be used for learning meaningful representations to some extent. However, we also find that fine-tuned models of the BERT family are able to match or even outperform the largest GPT-3 model, despite being 2 to 3 orders of magnitude smaller.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.725.pdf",
        "keywords": [
            "conceptual spaces",
            "learning conceptual spaces",
            "language models",
            "perceptually grounded representations"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, we also find that fine-tuned models of the BERT family are able to match or even outperform the largest GPT-3 model, despite being 2 to 3 orders of magnitude smaller.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (in this case, the large size of the GPT-3 model compared to smaller models",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, we also find that fine-tuned models of the BERT family are able to match or even outperform the largest GPT-3 model, despite being 2 to 3 orders of magnitude smaller.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (in this case, the large size of the GPT-3 model compared to smaller models"
    },
    {
        "title": "Can Language Models Understand Physical Concepts?",
        "authors": [
            "Lei Li",
            "Jingjing Xu",
            "Qingxiu Dong",
            "Ce Zheng",
            "Xu Sun",
            "Lingpeng Kong",
            "Qi Liu"
        ],
        "published": "2023",
        "summary": "Language models (LMs) gradually become general-purpose interfaces in the interactive and embodied world, where the understanding of physical concepts is an essential prerequisite. However, it is unclear whether LMs can understand physical concepts in the human world. To investigate this, we design a benchmark VEC that covers the tasks of (i) Visual concepts, such as the shape and material of objects, and (ii) Embodied Concepts, learned from the interaction with the world such as the temperature of objects. Our zero (few)-shot prompting results show that the understanding of certain visual concepts emerges as scaling up LMs, but there are still basic concepts to which the scaling law does not apply. For example, OPT-175B performs close to humans with a zero-shot accuracy of 85% on the material concept, yet behaves like random guessing on the mass concept. Instead, vision-augmented LMs such as CLIP and BLIP achieve a human-level understanding of embodied concepts. Analysis indicates that the rich semantics in visual representation can serve as a valuable source of embodied knowledge. Inspired by this, we propose a distillation method to transfer embodied knowledge from VLMs to LMs, achieving performance gain comparable with that by scaling up parameters of LMs 134×. Our dataset is available at https://github.com/TobiasLee/VEC.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.726.pdf",
        "keywords": [
            "language models",
            "physical concepts"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it is unclear whether LMs can understand physical concepts in the human world.\" and \"yet behaves like random guessing on the mass concept.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, it is unclear whether LMs can understand physical concepts in the human world.\" and \"yet behaves like random guessing on the mass concept.\""
    },
    {
        "title": "Once Upon a Time in Graph: Relative-Time Pretraining for Complex Temporal Reasoning",
        "authors": [
            "Sen Yang",
            "Xin Li",
            "Lidong Bing",
            "Wai Lam"
        ],
        "published": "2023",
        "summary": "Our physical world is constantly evolving over time, rendering challenges for pre-trained language models to understand and reason over the temporal contexts of texts. Existing work focuses on strengthening the direct association between a piece of text and its time-stamp. However, the knowledge-time association is usually insufficient for the downstream tasks that require reasoning over temporal dependencies between knowledge. In this work, we make use of the underlying nature of time, all temporally-scoped sentences are strung together through a one-dimensional time axis, and suggest creating a graph structure based on the relative placements of events along the time axis. Inspired by the graph view, we propose RemeMo ( ̲Relative Ti ̲me  ̲Modeling), which explicitly connects all temporally-scoped facts by modeling the time relations between any two sentences. Experimental results show that RemeMo outperforms the baseline T5 on multiple temporal question answering datasets under various settings. Further analysis suggests that RemeMo is especially good at modeling long-range complex temporal dependencies.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.728.pdf",
        "keywords": [
            "complex temporal reasoning",
            "pre trained language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our physical world is constantly evolving over time, rendering challenges for pre-trained language models to understand and reason over the temporal contexts of texts.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our physical world is constantly evolving over time, rendering challenges for pre-trained language models to understand and reason over the temporal contexts of texts.\""
    },
    {
        "title": "Enhancing Textbooks with Visuals from the Web for Improved Learning",
        "authors": [
            "Janvijay Singh",
            "Vilém Zouhar",
            "Mrinmaya Sachan"
        ],
        "published": "2023",
        "summary": "Textbooks are one of the main mediums for delivering high-quality education to students. In particular, explanatory and illustrative visuals play a key role in retention, comprehension and general transfer of knowledge. However, many textbooks lack these interesting visuals to support student learning. In this paper, we investigate the effectiveness of vision-language models to automatically enhance textbooks with images from the web. We collect a dataset of e-textbooks in the math, science, social science and business domains. We then set up a text-image matching task that involves retrieving and appropriately assigning web images to textbooks, which we frame as a matching optimization problem. Through a crowd-sourced evaluation, we verify that (1) while the original textbook images are rated higher, automatically assigned ones are not far behind, and (2) the precise formulation of the optimization problem matters. We release the dataset of textbooks with an associated image bank to inspire further research in this intersectional area of computer vision and NLP for education.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.731.pdf",
        "keywords": [
            "textbooks",
            "e textbooks"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "An Empirical Study of Translation Hypothesis Ensembling with Large Language Models",
        "authors": [
            "António Farinhas",
            "José de Souza",
            "Andre Martins"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) are becoming a one-fits-many solution, but they sometimes hallucinate or produce unreliable output. In this paper, we investigate how hypothesis ensembling can improve the quality of the generated text for the specific problem of LLM-based machine translation. We experiment with several techniques for ensembling hypotheses produced by LLMs such as ChatGPT, LLaMA, and Alpaca. We provide a comprehensive study along multiple dimensions, including the method to generate hypotheses (multiple prompts, temperature-based sampling, and beam search) and the strategy to produce the final translation (instruction-based, quality-based reranking, and minimum Bayes risk (MBR) decoding). Our results show that MBR decoding is a very effective method, that translation quality can be improved using a small number of samples, and that instruction tuning has a strong impact on the relation between the diversity of the hypotheses and the sampling temperature.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.733.pdf",
        "keywords": [
            "language models",
            "large language models",
            "translation",
            "machine translation",
            "empirical study",
            "translation quality",
            "quality based reranking",
            "risk"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models (LLMs) are becoming a one-fits-many solution, but they sometimes hallucinate or produce unreliable output.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large language models (LLMs) are becoming a one-fits-many solution, but they sometimes hallucinate or produce unreliable output.\""
    },
    {
        "title": "FedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning",
        "authors": [
            "Jaemin Shin",
            "Hyungjun Yoon",
            "Seungjoo Lee",
            "Sungjoon Park",
            "Yunxin Liu",
            "Jinho Choi",
            "Sung-Ju Lee"
        ],
        "published": "2023",
        "summary": "Psychiatrists diagnose mental disorders via the linguistic use of patients. Still, due to data privacy, existing passive mental health monitoring systems use alternative features such as activity, app usage, and location via mobile devices. We propose FedTherapist, a mobile mental health monitoring system that utilizes continuous speech and keyboard input in a privacy-preserving way via federated learning. We explore multiple model designs by comparing their performance and overhead for FedTherapist to overcome the complex nature of on-device language model training on smartphones. We further propose a Context-Aware Language Learning (CALL) methodology to effectively utilize smartphones’ large and noisy text for mental health signal sensing. Our IRB-approved evaluation of the prediction of self-reported depression, stress, anxiety, and mood from 46 participants shows higher accuracy of FedTherapist compared with the performance with non-language features, achieving 0.15 AUROC improvement and 8.21% MAE reduction.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.734.pdf",
        "keywords": [
            "mental health",
            "mental health monitoring",
            "smartphones",
            "context aware language learning"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models",
        "authors": [
            "Geewook Kim",
            "Hodong Lee",
            "Daehee Kim",
            "Haeji Jung",
            "Sanghee Park",
            "Yoonsik Kim",
            "Sangdoo Yun",
            "Taeho Kil",
            "Bado Lee",
            "Seunghyun Park"
        ],
        "published": "2023",
        "summary": "Recent advances in Large Language Models (LLMs) have stimulated a surge of research aimed at extending their applications to the visual domain. While these models exhibit promise in generating abstract image captions and facilitating natural conversations, their performance on text-rich images still requires improvement. In this paper, we introduce Contrastive Reading Model (Cream), a novel neural architecture designed to enhance the language-image understanding capability of LLMs by capturing intricate details that are often overlooked in existing methods. Cream combines vision and auxiliary encoders, fortified by a contrastive feature alignment technique, to achieve a more effective comprehension of language information in visually situated contexts within the images. Our approach bridges the gap between vision and language understanding, paving the way for the development of more sophisticated Document Intelligence Assistants. Through rigorous evaluations across diverse visually-situated language understanding tasks that demand reasoning capabilities, we demonstrate the compelling performance of Cream, positioning it as a prominent model in the field of visual document understanding. We provide our codebase and newly-generated datasets at https://github.com/naver-ai/cream.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.735.pdf",
        "keywords": [
            "natural language",
            "language models",
            "situated language understanding",
            "frozen large language models",
            "contrastive reading",
            "language understanding",
            "contrastive feature"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While these models exhibit promise in generating abstract image captions and facilitating natural conversations, their performance on text-rich images still requires improvement.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While these models exhibit promise in generating abstract image captions and facilitating natural conversations, their performance on text-rich images still requires improvement.\""
    },
    {
        "title": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs",
        "authors": [
            "Jiaao Chen",
            "Diyi Yang"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our proposed methods compared to the state-of-the-art baselines.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.738.pdf",
        "keywords": [
            "unlearn",
            "forget",
            "unlearning framework",
            "classification",
            "baselines",
            "large language models",
            "update llms"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"however, this process might suffer from privacy issues and violations of data protection regulations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"however, this process might suffer from privacy issues and violations of data protection regulations.\""
    },
    {
        "title": "Precedent-Enhanced Legal Judgment Prediction with LLM and Domain-Model Collaboration",
        "authors": [
            "Yiquan Wu",
            "Siying Zhou",
            "Yifei Liu",
            "Weiming Lu",
            "Xiaozhong Liu",
            "Yating Zhang",
            "Changlong Sun",
            "Fei Wu",
            "Kun Kuang"
        ],
        "published": "2023",
        "summary": "Legal Judgment Prediction (LJP) has become an increasingly crucial task in Legal AI, i.e., predicting the judgment of the case in terms of case fact description. Precedents are the previous legal cases with similar facts, which are the basis for the judgment of the subsequent case in national legal systems. Thus, it is worthwhile to explore the utilization of precedents in the LJP. Recent advances in deep learning have enabled a variety of techniques to be used to solve the LJP task. These can be broken down into two categories: large language models (LLMs) and domain-specific models. LLMs are capable of interpreting and generating complex natural language, while domain models are efficient in learning task-specific information. In this paper, we propose the precedent-enhanced LJP framework (PLJP) – a system that leverages the strength of both LLM and domain models in the context of precedents. Specifically, the domain models are designed to provide candidate labels and find the proper precedents efficiently, and the large models will make the final prediction with an in-context precedents comprehension. Experiments on the real-world dataset demonstrate the effectiveness of our PLJP. Moreover, our work shows a promising direction for LLM and domain-model collaboration that can be generalized to other vertical domains.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.740.pdf",
        "keywords": [
            "domain models",
            "domain model collaboration",
            "domain specific models",
            "legal judgment prediction",
            "precedent"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the abstract implies LLMs have limitations in the legal domain by stating that domain models are more \"efficient in learning task-specific information\", suggesting that LLMs might not be as efficient in this aspect.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the abstract implies LLMs have limitations in the legal domain by stating that domain models are more \"efficient in learning task-specific information\", suggesting that LLMs might not be as efficient in this aspect."
    },
    {
        "title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
        "authors": [
            "Sewon Min",
            "Kalpesh Krishna",
            "Xinxi Lyu",
            "Mike Lewis",
            "Wen-tau Yih",
            "Pang Koh",
            "Mohit Iyyer",
            "Luke Zettlemoyer",
            "Hannaneh Hajishirzi"
        ],
        "published": "2023",
        "summary": "Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs—InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI—and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2% error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost $26K if evaluated by humans, with various findings: GPT-4 and ChatGPT are more factual than public models, and Vicuna and Alpaca are some of the best public models. FACTSCORE is available for public use via ‘pip install factscore‘.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.741.pdf",
        "keywords": [
            "factscore",
            "atomic evaluation",
            "human evaluation",
            "factual precision",
            "strong language model",
            "large language models",
            "extensive human evaluation",
            "factuality",
            "automated"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly.\""
    },
    {
        "title": "Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems",
        "authors": [
            "Marek Kadlčík",
            "Michal Štefánik",
            "Ondrej Sotolar",
            "Vlastimil Martinek"
        ],
        "published": "2023",
        "summary": "Despite outstanding performance in many tasks, language models are notoriously inclined to make factual errors in tasks requiring arithmetic computation. We address this deficiency by creating Calc-X, a collection of datasets that demonstrates the appropriate use of a calculator in reasoning chains. Calc-X is suitable for teaching language models to offload computations to a symbolic system. We survey and unify several existing chain-of-thought datasets into a proposed format, resulting in a standard collection of over 300,000 samples requiring arithmetic reasoning. Finally, we use the new Calc-X collection to train open-source calculator-using models and show that these models approximately double the accuracy of generating correct results compared to vanilla language model baselines.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.742.pdf",
        "keywords": [
            "symbolic systems",
            "arithmetical chain"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite outstanding performance in many tasks, language models are notoriously inclined to make factual errors in tasks requiring arithmetic computation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite outstanding performance in many tasks, language models are notoriously inclined to make factual errors in tasks requiring arithmetic computation.\""
    },
    {
        "title": "CoF-CoT: Enhancing Large Language Models with Coarse-to-Fine Chain-of-Thought Prompting for Multi-domain NLU Tasks",
        "authors": [
            "Hoang Nguyen",
            "Ye Liu",
            "Chenwei Zhang",
            "Tao Zhang",
            "Philip Yu"
        ],
        "published": "2023",
        "summary": "While Chain-of-Thought prompting is popular in reasoning tasks, its application to Large Language Models (LLMs) in Natural Language Understanding (NLU) is under-explored. Motivated by multi-step reasoning of LLMs, we propose Coarse-to-Fine Chain-of-Thought (CoF-CoT) approach that breaks down NLU tasks into multiple reasoning steps where LLMs can learn to acquire and leverage essential concepts to solve tasks from different granularities. Moreover, we propose leveraging semantic-based Abstract Meaning Representation (AMR) structured knowledge as an intermediate step to capture the nuances and diverse structures of utterances, and to understand connections between their varying levels of granularity. Our proposed approach is demonstrated effective in assisting the LLMs adapt to the multi-grained NLU tasks under both zero-shot and few-shot multi-domain settings.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.743.pdf",
        "keywords": [
            "thought",
            "language models",
            "large language models",
            "natural language understanding"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Motivated by multi-step reasoning of LLMs\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Motivated by multi-step reasoning of LLMs\""
    },
    {
        "title": "When Language Models Fall in Love: Animacy Processing in Transformer Language Models",
        "authors": [
            "Michael Hanna",
            "Yonatan Belinkov",
            "Sandro Pezzelle"
        ],
        "published": "2023",
        "summary": "Animacy—whether an entity is alive and sentient—is fundamental to cognitive processing, impacting areas such as memory, vision, and language. However, animacy is not always expressed directly in language: in English it often manifests indirectly, in the form of selectional constraints on verbs and adjectives. This poses a potential issue for transformer language models (LMs): they often train only on text, and thus lack access to extralinguistic information from which humans learn about animacy. We ask: how does this impact LMs’ animacy processing—do they still behave as humans do? We answer this question using open-source LMs. Like previous studies, we find that LMs behave much like humans when presented with entities whose animacy is typical. However, we also show that even when presented with stories about atypically animate entities, such as a peanut in love, LMs adapt: they treat these entities as animate, though they do not adapt as well as humans. Even when the context indicating atypical animacy is very short, LMs pick up on subtle clues and change their behavior. We conclude that despite the limited signal through which LMs can learn about animacy, they are indeed sensitive to the relevant lexical semantic nuances available in English.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.744.pdf",
        "keywords": [
            "language",
            "transformer language models",
            "language models fall",
            "open source",
            "cognitive processing"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, we also show that even when presented with stories about atypically animate entities, such as a peanut in love, LMs adapt: they treat these entities as animate, though they do not adapt as well as humans.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, we also show that even when presented with stories about atypically animate entities, such as a peanut in love, LMs adapt: they treat these entities as animate, though they do not adapt as well as humans.\""
    },
    {
        "title": "Target-to-Source Augmentation for Aspect Sentiment Triplet Extraction",
        "authors": [
            "Yice Zhang",
            "Yifan Yang",
            "Meng Li",
            "Bin Liang",
            "Shiwei Chen",
            "Ruifeng Xu"
        ],
        "published": "2023",
        "summary": "Aspect Sentiment Triplet Extraction (ASTE) is an important task in sentiment analysis, aiming to extract aspect-level opinions and sentiments from user-generated reviews. The fine-grained nature of ASTE incurs a high annotation cost, while the scarcity of annotated data limits the performance of existing methods. This paper exploits data augmentation to address this issue. Traditional augmentation methods typically modify the input sentences of existing samples via heuristic rules or language models, which have shown success in text classification tasks. However, applying these methods to fine-grained tasks like ASTE poses challenges in generating diverse augmented samples while maintaining alignment between modified sentences and origin labels. Therefore, this paper proposes a target-to-source augmentation approach for ASTE. Our approach focuses on learning a generator that can directly generate new sentences based on labels and syntactic templates. With this generator, we can generate a substantial number of diverse augmented samples by mixing labels and syntactic templates from different samples. Besides, to ensure the quality of the generated sentence, we introduce fluency and alignment discriminators to provide feedback on the generated sentence and then use this feedback to optimize the generator via a reinforcement learning framework. Experiments demonstrate that our approach significantly enhances the performance of existing ASTE models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.747.pdf",
        "keywords": [
            "data augmentation",
            "source augmentation",
            "aspect sentiment triplet extraction",
            "target to source augmentation",
            "sentiment analysis",
            "reinforcement learning"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "PAC-tuning: Fine-tuning Pre-trained Language Models with PAC-driven Perturbed Gradient Descent",
        "authors": [
            "Guangliang Liu",
            "Zhiyu Xue",
            "Xitong Zhang",
            "Kristen Johnson",
            "Rongrong Wang"
        ],
        "published": "2023",
        "summary": "Fine-tuning pretrained language models (PLMs) for downstream tasks is a large-scale optimization problem, in which the choice of the training algorithm critically determines how well the trained model can generalize to unseen test data, especially in the context of few-shot learning. To achieve good generalization performance and avoid overfitting, techniques such as data augmentation and pruning are often applied. However, adding these regularizations necessitates heavy tuning of the hyperparameters of optimization algorithms, such as the popular Adam optimizer. In this paper, we propose a two-stage fine-tuning method, PAC-tuning, to address this optimization challenge. First, based on PAC-Bayes training, PAC-tuning directly minimizes the PAC-Bayes generalization bound to learn proper parameter distribution. Second, PAC-tuning modifies the gradient by injecting noise with the variance learned in the first stage into the model parameters during training, resulting in a variant of perturbed gradient descent (PGD). In the past, the few-shot scenario posed difficulties for PAC-Bayes training because the PAC-Bayes bound, when applied to large models with limited training data, might not be stringent. Our experimental results across 5 GLUE benchmark tasks demonstrate that PAC-tuning successfully handles the challenges of fine-tuning tasks and outperforms strong baseline methods by a visible margin, further confirming the potential to apply PAC training for any other settings where the Adam optimizer is currently used for training.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.748.pdf",
        "keywords": [
            "perturbed gradient descent",
            "pac tuning",
            "pac bayes",
            "pac training"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"in the context of few-shot learning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"in the context of few-shot learning.\""
    },
    {
        "title": "Emergence of Abstract State Representations in Embodied Sequence Modeling",
        "authors": [
            "Tian Yun",
            "Zilai Zeng",
            "Kunal Handa",
            "Ashish Thapliyal",
            "Bo Pang",
            "Ellie Pavlick",
            "Chen Sun"
        ],
        "published": "2023",
        "summary": "Decision making via sequence modeling aims to mimic the success of language models, where actions taken by an embodied agent are modeled as tokens to predict. Despite their promising performance, it remains unclear if embodied sequence modeling leads to the emergence of internal representations that represent the environmental state information. A model that lacks abstract state representations would be liable to make decisions based on surface statistics which fail to generalize. We take the BabyAI environment, a grid world in which language-conditioned navigation tasks are performed, and build a sequence modeling Transformer, which takes a language instruction, a sequence of actions, and environmental observations as its inputs. In order to investigate the emergence of abstract state representations, we design a “blindfolded” navigation task, where only the initial environmental layout, the language instruction, and the action sequence to complete the task are available for training. Our probing results show that intermediate environmental layouts can be reasonably reconstructed from the internal activations of a trained model, and that language instructions play a role in the reconstruction accuracy. Our results suggest that many key features of state representations can emerge via embodied sequence modeling, supporting an optimistic outlook for applications of sequence modeling objectives to more complex embodied decision-making domains.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.749.pdf",
        "keywords": [
            "sequence modeling",
            "state representations",
            "abstract state representations",
            "embodied sequence modeling",
            "embodied decision making",
            "emergence",
            "language"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Dissecting Recall of Factual Associations in Auto-Regressive Language Models",
        "authors": [
            "Mor Geva",
            "Jasmijn Bastings",
            "Katja Filippova",
            "Amir Globerson"
        ],
        "published": "2023",
        "summary": "Transformer-based language models (LMs) are known to capture factual knowledge in their parameters. While previous work looked into where factual associations are stored, only little is known about how they are retrieved internally during inference. We investigate this question through the lens of information flow. Given a subject-relation query, we study how the model aggregates information about the subject and relation to predict the correct attribute. With interventions on attention edges, we first identify two critical points where information propagates to the prediction: one from the relation positions followed by another from the subject positions. Next, by analyzing the information at these points, we unveil a three-step internal mechanism for attribute extraction. First, the representation at the last-subject position goes through an enrichment process, driven by the early MLP sublayers, to encode many subject-related attributes. Second, information from the relation propagates to the prediction. Third, the prediction representation “queries” the enriched subject to extract the attribute. Perhaps surprisingly, this extraction is typically done via attention heads, which often encode subject-attribute mappings in their parameters. Overall, our findings introduce a comprehensive view of how factual associations are stored and extracted internally in LMs, facilitating future research on knowledge localization and editing.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.751.pdf",
        "keywords": [
            "auto regressive language models",
            "enrichment",
            "transformer based language models",
            "factual associations",
            "associations"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper implies that there are limitations in understanding how factual associations are retrieved internally during inference in LMs, which is the focus of the study.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None, but the paper implies that there are limitations in understanding how factual associations are retrieved internally during inference in LMs, which is the focus of the study."
    },
    {
        "title": "StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models",
        "authors": [
            "Sullam Jeoung",
            "Yubin Ge",
            "Jana Diesner"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have been observed to encode and perpetuate harmful associations present in the training data. We propose a theoretically grounded framework called StereoMap to gain insights into their perceptions of how demographic groups have been viewed by society. The framework is grounded in the Stereotype Content Model (SCM); a well-established theory from psychology. According to SCM, stereotypes are not all alike. Instead, the dimensions of Warmth and Competence serve as the factors that delineate the nature of stereotypes. Based on the SCM theory, StereoMap maps LLMs’ perceptions of social groups (defined by socio-demographic features) using the dimensions of Warmth and Competence. Furthermore, the framework enables the investigation of keywords and verbalizations of reasoning of LLMs’ judgments to uncover underlying factors influencing their perceptions. Our results show that LLMs exhibit a diverse range of perceptions towards these groups, characterized by mixed evaluations along the dimensions of Warmth and Competence. Furthermore, analyzing the reasonings of LLMs, our findings indicate that LLMs demonstrate an awareness of social disparities, often stating statistical data and research findings to support their reasoning. This study contributes to the understanding of how LLMs perceive and represent social groups, shedding light on their potential biases and the perpetuation of harmful associations.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.752.pdf",
        "keywords": [
            "stereotypes",
            "language models",
            "stereotype content model",
            "stereomap",
            "awareness"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large Language Models (LLMs) have been observed to encode and perpetuate harmful associations present in the training data.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Large Language Models (LLMs) have been observed to encode and perpetuate harmful associations present in the training data.\""
    },
    {
        "title": "Select, Prompt, Filter: Distilling Large Language Models for Summarizing Conversations",
        "authors": [
            "Minh-Quang Pham",
            "Sathish Indurthi",
            "Shamil Chollampatt",
            "Marco Turchi"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) like ChatGPT can be expensive to train, deploy, and use for specific natural language generation tasks such as text summarization and for certain domains. A promising alternative is to fine-tune relatively smaller language models (LMs) on a particular task using high-quality, in-domain datasets. However, it can be prohibitively expensive to get such high-quality training data. This issue has been mitigated by generating weakly supervised data via knowledge distillation (KD) of LLMs. We propose a three-step approach to distill ChatGPT and fine-tune smaller LMs for summarizing forum conversations. More specifically, we design a method to selectively sample a large unannotated corpus of forum conversation using a semantic similarity metric. Then, we use the same metric to retrieve suitable prompts for ChatGPT from a small annotated validation set in the same domain. The generated dataset is then filtered to remove low-quality instances. Our proposed select-prompt-filter KD approach leads to significant improvements of up to 6.6 ROUGE-2 score by leveraging sufficient in-domain pseudo-labeled data over a standard KD approach given the same size of training data.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.753.pdf",
        "keywords": [
            "summarizing forum conversations",
            "knowledge distillation",
            "language models",
            "filter"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models (LLMs) like ChatGPT can be expensive to train, deploy, and use for specific natural language generation tasks such as text summarization and for certain domains.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large language models (LLMs) like ChatGPT can be expensive to train, deploy, and use for specific natural language generation tasks such as text summarization and for certain domains.\""
    },
    {
        "title": "Human Raters Cannot Distinguish English Translations from Original English Texts",
        "authors": [
            "Shira Wein"
        ],
        "published": "2023",
        "summary": "The term translationese describes the set of linguistic features unique to translated texts, which appear regardless of translation quality. Though automatic classifiers designed to distinguish translated texts achieve high accuracy and prior work has identified common hallmarks of translationese, human accuracy of identifying translated text is understudied. In this work, we perform a human evaluation of English original/translated texts in order to explore raters’ ability to classify texts as being original or translated English and the features that lead a rater to judge text as being translated. Ultimately, we find that, regardless of the annotators’ native language or the source language of the text, annotators are unable to distinguish translations from original English texts and also have low agreement. Our results provide critical insight into work in translation studies and context for assessments of translationese classifiers.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.754.pdf",
        "keywords": [
            "translationese classifiers",
            "translated texts",
            "translated english"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models",
        "authors": [
            "Shuai Zhao",
            "Jinming Wen",
            "Anh Luu",
            "Junbo Zhao",
            "Jie Fu"
        ],
        "published": "2023",
        "summary": "The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose ProAttack, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text classification tasks, we empirically validate ProAttack’s competitive performance in textual backdoor attacks. Notably, in the rich-resource setting, ProAttack achieves state-of-the-art attack success rates in the clean-label backdoor attack benchmark without external triggers.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.757.pdf",
        "keywords": [
            "backdoor attack",
            "language models",
            "vulnerability",
            "label modification",
            "targeted vulnerabilities",
            "based learning"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks.\""
    },
    {
        "title": "UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation",
        "authors": [
            "Daixuan Cheng",
            "Shaohan Huang",
            "Junyu Bi",
            "Yuefeng Zhan",
            "Jianfeng Liu",
            "Yujing Wang",
            "Hao Sun",
            "Furu Wei",
            "Weiwei Deng",
            "Qi Zhang"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) are popular for their impressive abilities, but the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization. We propose UPRISE (Universal Prompt Retrieval for Improving zero-Shot Evaluation), which tunes a lightweight and versatile retriever that automatically retrieves prompts for a given zero-shot task input. Specifically, we demonstrate universality in a cross-task and cross-model scenario: the retriever is tuned on diverse tasks, but tested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for tuning the retriever, but test the retriever on different LLMs of much larger scales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that UPRISE mitigates the hallucination problem in our experiments with ChatGPT, suggesting its potential to improve even the strongest LLMs. Our model and code are available at https://github.com/microsoft/LMOps.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.758.pdf",
        "keywords": [
            "shot evaluation",
            "universal prompt retrieval",
            "zero shot evaluation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"but the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization\"; \"mitigates the hallucination problem in our experiments with ChatGPT\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"but the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization\"; \"mitigates the hallucination problem in our experiments with ChatGPT\""
    },
    {
        "title": "KRLS: Improving End-to-End Response Generation in Task Oriented Dialog with Reinforced Keywords Learning",
        "authors": [
            "Xiao Yu",
            "Qingyang Wu",
            "Kun Qian",
            "Zhou Yu"
        ],
        "published": "2023",
        "summary": "In task-oriented dialogs (TOD), reinforcement learning (RL) algorithms train a model to directly optimize response for task-related metrics. However, RL often needs to perform exploration, which can be time-consuming due to the slow auto-regressive sequence generation process. We investigate an approach to create a more efficient RL-based algorithm to improve TOD performance in an offline setting. First, we use a faster generation procedure that samples from independent next-word distributions after training the language model (LM) with supervised learning. We then introduce a fine-grained reward function to help the model focus on learning key information in a dialog, by measuring the importance and semantic closeness of each generated token. Experiments on the MultiWoZ dataset show our new training algorithm, Keywords Reinforcement Learning with Next-word Sampling (KRLS), achieves state-of-the-art performance on the end-to-end response generation task, with a 15% training time reduction compared to a standard RL algorithm using auto-regressive generation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.759.pdf",
        "keywords": [
            "task oriented dialog",
            "next word sampling",
            "language model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, RL often needs to perform exploration, which can be time-consuming due to the slow auto-regressive sequence generation process.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, RL often needs to perform exploration, which can be time-consuming due to the slow auto-regressive sequence generation process.\""
    },
    {
        "title": "Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU",
        "authors": [
            "Fajri Koto",
            "Nurul Aisyah",
            "Haonan Li",
            "Timothy Baldwin"
        ],
        "published": "2023",
        "summary": "Although large language models (LLMs) are often pre-trained on large-scale multilingual texts, their reasoning abilities and real-world knowledge are mainly evaluated based on English datasets. Assessing LLM capabilities beyond English is increasingly vital but hindered due to the lack of suitable datasets. In this work, we introduce IndoMMLU, the first multi-task language understanding benchmark for Indonesian culture and languages, which consists of questions from primary school to university entrance exams in Indonesia. By employing professional teachers, we obtain 14,981 questions across 64 tasks and education levels, with 46% of the questions focusing on assessing proficiency in the Indonesian language and knowledge of nine local languages and cultures in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture. Other smaller models such as BLOOMZ and Falcon perform at even lower levels.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.760.pdf",
        "keywords": [
            "indonesia",
            "large language models",
            "indommlu"
        ],
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture. Other smaller models such as BLOOMZ and Falcon perform at even lower levels.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture. Other smaller models such as BLOOMZ and Falcon perform at even lower levels.\""
    },
    {
        "title": "Let’s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs",
        "authors": [
            "Pranjal Aggarwal",
            "Aman Madaan",
            "Yiming Yang",
            "Mausam"
        ],
        "published": "2023",
        "summary": "A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency - poll the LLM multiple times and output the most frequent solution. Existing Self-Consistency techniques always generate a constant number of samples per question, where a better approach will be to non-uniformly distribute the available budget based on the amount of agreement in the samples generated so far. In response, we introduce Adaptive-Consistency, a cost-efficient, model-agnostic technique that dynamically adjusts the number of samples per question using a lightweight stopping criterion. Our experiments over 17 reasoning and code generation datasets and three LLMs demonstrate that Adaptive-Consistency reduces sample budget by up to 7.9 times with an average accuracy drop of less than 0.1%",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.761.pdf",
        "keywords": [
            "adaptive consistency",
            "self consistency",
            "efficient reasoning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency - poll the LLM multiple times and output the most frequent solution.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency - poll the LLM multiple times and output the most frequent solution.\""
    },
    {
        "title": "Bridging Information-Theoretic and Geometric Compression in Language Models",
        "authors": [
            "Emily Cheng",
            "Corentin Kervadec",
            "Marco Baroni"
        ],
        "published": "2023",
        "summary": "For a language model (LM) to faithfully model human language, it must compress vast, potentially infinite information into relatively few dimensions. We propose analyzing compression in (pre-trained) LMs from two points of view: geometric and information-theoretic. We demonstrate that the two views are highly correlated, such that the intrinsic geometric dimension of linguistic data predicts their coding length under the LM. We then show that, in turn, high compression of a linguistic dataset predicts rapid adaptation to that dataset, confirming that being able to compress linguistic information is an important part of successful LM performance. As a practical byproduct of our analysis, we evaluate a battery of intrinsic dimension estimators for the first time on linguistic data, showing that only some encapsulate the relationship between information-theoretic compression, geometric compression, and ease-of-adaptation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.762.pdf",
        "keywords": [
            "language models",
            "compress",
            "geometric compression"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"For a language model (LM) to faithfully model human language, it must compress vast, potentially infinite information into relatively few dimensions.\"\n\nNote: Although the paper does not explicitly discuss limitations of LLMs, it implies a challenge for LMs in compressing vast information, which can be considered a limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"For a language model (LM) to faithfully model human language, it must compress vast, potentially infinite information into relatively few dimensions.\"\n\nNote: Although the paper does not explicitly discuss limitations of LLMs, it implies a challenge for LMs in compressing vast information, which can be considered a limitation."
    },
    {
        "title": "Pre-training Language Models for Comparative Reasoning",
        "authors": [
            "Mengxia Yu",
            "Zhihan Zhang",
            "Wenhao Yu",
            "Meng Jiang"
        ],
        "published": "2023",
        "summary": "Comparative reasoning is a process of comparing objects, concepts, or entities to draw conclusions, which constitutes a fundamental cognitive ability. In this paper, we propose a novel framework to pre-train language models for enhancing their abilities of comparative reasoning over texts. While there have been approaches for NLP tasks that require comparative reasoning, they suffer from costly manual data labeling and limited generalizability to different tasks. Our approach introduces a novel method of collecting scalable data for text-based entity comparison, which leverages both structured and unstructured data. Moreover, we present a framework of pre-training language models via three novel objectives on comparative reasoning. Evaluation on downstream tasks including comparative question answering, question generation, and summarization shows that our pre-training framework significantly improves the comparative reasoning abilities of language models, especially under low-resource conditions. This work also releases the first integrated benchmark for comparative reasoning.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.763.pdf",
        "keywords": [
            "comparative reasoning",
            "comparative question answering",
            "pre training language models",
            "language models",
            "benchmark"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While there have been approaches for NLP tasks that require comparative reasoning, they suffer from costly manual data labeling and limited generalizability to different tasks.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of previous approaches for NLP tasks that require comparative reasoning, but it does not specifically discuss the limitations of LLMs. The focus of the paper is on proposing",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While there have been approaches for NLP tasks that require comparative reasoning, they suffer from costly manual data labeling and limited generalizability to different tasks.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of previous approaches for NLP tasks that require comparative reasoning, but it does not specifically discuss the limitations of LLMs. The focus of the paper is on proposing"
    },
    {
        "title": "AutoTrial: Prompting Language Models for Clinical Trial Design",
        "authors": [
            "Zifeng Wang",
            "Cao Xiao",
            "Jimeng Sun"
        ],
        "published": "2023",
        "summary": "Clinical trials are critical for drug development. Constructing the appropriate eligibility criteria (i.e., the inclusion/exclusion criteria for patient recruitment) is essential for the trial’s success. Proper design of clinical trial protocols should consider similar precedent trials and their eligibility criteria to ensure sufficient patient coverage. In this paper, we present a method named AutoTrial to aid the design of clinical eligibility criteria using language models. It allows (1) controllable generation under instructions via a hybrid of discrete and neural prompting, (2) scalable knowledge incorporation via in-context learning, and (3) explicit reasoning chains to provide rationales for understanding the outputs. Experiments on over 70K clinical trials verify that AutoTrial generates high-quality criteria texts that are fluent and coherent and with high accuracy in capturing the relevant clinical concepts to the target trial. It is noteworthy that our method, with a much smaller parameter size, gains around 60% winning rate against the GPT-3.5 baselines via human evaluations.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.766.pdf",
        "keywords": [
            "language models",
            "patient coverage",
            "clinical trials",
            "clinical trial design",
            "clinical eligibility",
            "clinical trial protocols",
            "prompting"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations mentioned, but the comparison with a smaller parameter size model that gains a high winning rate against the GPT-3.5 baseline implies some limitations of large models like GPT-3.5, but this is not the focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations mentioned, but the comparison with a smaller parameter size model that gains a high winning rate against the GPT-3.5 baseline implies some limitations of large models like GPT-3.5, but this is not the focus of the paper."
    },
    {
        "title": "Hallucination Mitigation in Natural Language Generation from Large-Scale Open-Domain Knowledge Graphs",
        "authors": [
            "Xiao Shi",
            "Zhengyuan Zhu",
            "Zeyu Zhang",
            "Chengkai Li"
        ],
        "published": "2023",
        "summary": "In generating natural language descriptions for knowledge graph triples, prior works used either small-scale, human-annotated datasets or datasets with limited variety of graph shapes, e.g., those having mostly star graphs. Graph-to-text models trained and evaluated on such datasets are largely not assessed for more realistic large-scale, open-domain settings. We introduce a new dataset, GraphNarrative, to fill this gap. Fine-tuning transformer-based pre-trained language models has achieved state-of-the-art performance among graph-to-text models. However, this method suffers from information hallucination—the generated text may contain fabricated facts not present in input graphs. We propose a novel approach that, given a graph-sentence pair in GraphNarrative, trims the sentence to eliminate portions that are not present in the corresponding graph, by utilizing the sentence’s dependency parse tree. Our experiment results verify this approach using models trained on GraphNarrative and existing datasets. The dataset, source code, and trained models are released at https://github.com/idirlab/graphnarrator.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.770.pdf",
        "keywords": [
            "graphnarrator",
            "hallucination",
            "natural language generation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, this method suffers from information hallucination—the generated text may contain fabricated facts not present in input graphs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, this method suffers from information hallucination—the generated text may contain fabricated facts not present in input graphs.\""
    },
    {
        "title": "Multi-Source Multi-Type Knowledge Exploration and Exploitation for Dialogue Generation",
        "authors": [
            "Xuanfan Ni",
            "Hongliang Dai",
            "Zhaochun Ren",
            "Piji Li"
        ],
        "published": "2023",
        "summary": "Open-domain multi-turn dialogue generation encounters the significant challenge of lacking various types of knowledge from diverse sources. Existing models typically focus on identifying specific types of dialogue knowledge and utilize corresponding datasets for training. However, this approach often leads to limited generalization capabilities and increased computational resource requirements. Recently, large language models (LLMs) have shown impressive performance on natural language processing tasks. To harness the knowledge storage of LLMs, we propose a framework named KnowEE that explores multi-source multi-type knowledge from LLMs by leveraging diverse datasets and then exploits the obtained knowledge for response generation. Our framework comprises two phases: First, we leverage five external datasets encompassing various types of knowledge to extract the most relevant samples to the dialogue context which are served as prompts to generate corresponding type of knowledge; Second, we inject the acquired knowledge into the ongoing dialogue context in fine-grained and coarse-grained manners, which is then fed into LLMs to generate the final dialogue response. Both automatic and manual evaluation results validate the effectiveness of our framework in exploring and exploiting multi-source multi-type knowledge to generate coherent, informative, and fluent responses.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.771.pdf",
        "keywords": [
            "knowledge",
            "knowledge exploration",
            "dialogue generation",
            "knowee",
            "exploitation",
            "language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing models typically focus on identifying specific types of dialogue knowledge and utilize corresponding datasets for training. However, this approach often leads to limited generalization capabilities and increased computational resource requirements.\"\n\nNote: Although the paper mentions limitations of existing models, it does not directly discuss the limitations of LLMs. However, it implies that the proposed framework is designed to overcome the limitations of existing models by",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing models typically focus on identifying specific types of dialogue knowledge and utilize corresponding datasets for training. However, this approach often leads to limited generalization capabilities and increased computational resource requirements.\"\n\nNote: Although the paper mentions limitations of existing models, it does not directly discuss the limitations of LLMs. However, it implies that the proposed framework is designed to overcome the limitations of existing models by"
    },
    {
        "title": "Multilingual Large Language Models Are Not (Yet) Code-Switchers",
        "authors": [
            "Ruochen Zhang",
            "Samuel Cahyawijaya",
            "Jan Christian Blaise Cruz",
            "Genta Winata",
            "Alham Fikri Aji"
        ],
        "published": "2023",
        "summary": "Multilingual Large Language Models (LLMs) have recently shown great capabilities in a wide range of tasks, exhibiting state-of-the-art performance through zero-shot or few-shot prompting methods. While there have been extensive studies on their abilities in monolingual tasks, the investigation of their potential in the context of code-switching (CSW), the practice of alternating languages within an utterance, remains relatively uncharted. In this paper, we provide a comprehensive empirical analysis of various multilingual LLMs, benchmarking their performance across four tasks: sentiment analysis, machine translation, summarization and word-level language identification. Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales. We argue that current “multilingualism’ in LLMs does not inherently imply proficiency with code-switching texts, calling for future research to bridge this discrepancy.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.774.pdf",
        "keywords": [
            "languages",
            "large language models",
            "multilingual large language models",
            "switching",
            "code switching",
            "prompting",
            "multilingualism",
            "sentiment analysis",
            "machine translation"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales. We argue that current “multilingualism’ in LLMs does not inherently imply proficiency with code-switching texts, calling for future research to bridge this discrepancy.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales. We argue that current “multilingualism’ in LLMs does not inherently imply proficiency with code-switching texts, calling for future research to bridge this discrepancy.\""
    },
    {
        "title": "LM vs LM: Detecting Factual Errors via Cross Examination",
        "authors": [
            "Roi Cohen",
            "May Hamri",
            "Mor Geva",
            "Amir Globerson"
        ],
        "published": "2023",
        "summary": "A prominent weakness of modern language models (LMs) is their tendency to generate factually incorrect text, which hinders their usability. A natural question is whether such factual errors can be detected automatically. Inspired by truth-seeking mechanisms in law, we propose a factuality evaluation framework for LMs that is based on cross-examination. Our key idea is that an incorrect claim is likely to result in inconsistency with other claims that the model generates. To discover such inconsistencies, we facilitate a multi-turn interaction between the LM that generated the claim and another LM (acting as an examiner) which introduces questions to discover inconsistencies. We empirically evaluate our method on factual claims made by multiple recent LMs on four benchmarks, finding that it outperforms existing methods and baselines, often by a large gap. Our results demonstrate the potential of using interacting LMs for capturing factual errors.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.778.pdf",
        "keywords": [
            "cross examination",
            "factuality",
            "lm",
            "factual errors",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A prominent weakness of modern language models (LMs) is their tendency to generate factually incorrect text, which hinders their usability.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"A prominent weakness of modern language models (LMs) is their tendency to generate factually incorrect text, which hinders their usability.\""
    },
    {
        "title": "Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding",
        "authors": [
            "Bram van Dijk",
            "Tom Kouwenhoven",
            "Marco Spruit",
            "Max Johannes van Duijn"
        ],
        "published": "2023",
        "summary": "Current Large Language Models (LLMs) are unparalleled in their ability to generate grammatically correct, fluent text. LLMs are appearing rapidly, and debates on LLM capacities have taken off, but reflection is lagging behind. Thus, in this position paper, we first zoom in on the debate and critically assess three points recurring in critiques of LLM capacities: i) that LLMs only parrot statistical patterns in the training data; ii) that LLMs master formal but not functional language competence; and iii) that language learning in LLMs cannot inform human language learning. Drawing on empirical and theoretical arguments, we show that these points need more nuance. Second, we outline a pragmatic perspective on the issue of ‘real’ understanding and intentionality in LLMs. Understanding and intentionality pertain to unobservable mental states we attribute to other humans because they have pragmatic value: they allow us to abstract away from complex underlying mechanics and predict behaviour effectively. We reflect on the circumstances under which it would make sense for humans to similarly attribute mental states to LLMs, thereby outlining a pragmatic philosophical context for LLMs as an increasingly prominent technology in society.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.779.pdf",
        "keywords": [
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"three points recurring in critiques of LLM capacities: i) that LLMs only parrot statistical patterns in the training data; ii) that LLMs master formal but not functional language competence; and iii) that language learning in LLMs cannot inform human language learning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"three points recurring in critiques of LLM capacities: i) that LLMs only parrot statistical patterns in the training data; ii) that LLMs master formal but not functional language competence; and iii) that language learning in LLMs cannot inform human language learning.\""
    },
    {
        "title": "PIEClass: Weakly-Supervised Text Classification with Prompting and Noise-Robust Iterative Ensemble Training",
        "authors": [
            "Yunyi Zhang",
            "Minhao Jiang",
            "Yu Meng",
            "Yu Zhang",
            "Jiawei Han"
        ],
        "published": "2023",
        "summary": "Weakly-supervised text classification trains a classifier using the label name of each target class as the only supervision, which largely reduces human annotation efforts. Most existing methods first use the label names as static keyword-based features to generate pseudo labels, which are then used for final classifier training. While reasonable, such a commonly adopted framework suffers from two limitations: (1) keywords can have different meanings in different contexts and some text may not have any keyword, so keyword matching can induce noisy and inadequate pseudo labels; (2) the errors made in the pseudo label generation stage will directly propagate to the classifier training stage without a chance of being corrected. In this paper, we propose a new method, PIEClass, consisting of two modules: (1) a pseudo label acquisition module that uses zero-shot prompting of pre-trained language models (PLM) to get pseudo labels based on contextualized text understanding beyond static keyword matching, and (2) a noise-robust iterative ensemble training module that iteratively trains classifiers and updates pseudo labels by utilizing two PLM fine-tuning methods that regularize each other. Extensive experiments show that PIEClass achieves overall better performance than existing strong baselines on seven benchmark datasets and even achieves similar performance to fully-supervised classifiers on sentiment classification tasks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.780.pdf",
        "keywords": [
            "ensemble",
            "ensemble training",
            "weakly supervised text classification",
            "prompting",
            "sentiment classification",
            "noise"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"keywords can have different meanings in different contexts and some text may not have any keyword, so keyword matching can induce noisy and inadequate pseudo labels;\"\n\nNote that this paper does not explicitly discuss limitations of LLMs, but rather the limitations of a specific method that uses LLMs. However, it does mention the limitations of using keywords in the context of LLMs, which",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"keywords can have different meanings in different contexts and some text may not have any keyword, so keyword matching can induce noisy and inadequate pseudo labels;\"\n\nNote that this paper does not explicitly discuss limitations of LLMs, but rather the limitations of a specific method that uses LLMs. However, it does mention the limitations of using keywords in the context of LLMs, which"
    },
    {
        "title": "MeaeQ: Mount Model Extraction Attacks with Efficient Queries",
        "authors": [
            "Chengwei Dai",
            "Minxuan Lv",
            "Kun Li",
            "Wei Zhou"
        ],
        "published": "2023",
        "summary": "We study model extraction attacks in natural language processing (NLP) where attackers aim to steal victim models by repeatedly querying the open Application Programming Interfaces (APIs). Recent works focus on limited-query budget settings and adopt random sampling or active learning-based sampling strategies on publicly available, unannotated data sources. However, these methods often result in selected queries that lack task relevance and data diversity, leading to limited success in achieving satisfactory results with low query costs. In this paper, we propose MeaeQ (Model extraction attack with efficient Queries), a straightforward yet effective method to address these issues. Specifically, we initially utilize a zero-shot sequence inference classifier, combined with API service information, to filter task-relevant data from a public text corpus instead of a problem domain-specific dataset. Furthermore, we employ a clustering-based data reduction technique to obtain representative data as queries for the attack. Extensive experiments conducted on four benchmark datasets demonstrate that MeaeQ achieves higher functional similarity to the victim model than baselines while requiring fewer queries.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.781.pdf",
        "keywords": [
            "model extraction",
            "victim model",
            "mount model extraction",
            "natural language processing",
            "efficient queries",
            "data reduction"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning",
        "authors": [
            "Seungone Kim",
            "Se Joo",
            "Doyoung Kim",
            "Joel Jang",
            "Seonghyeon Ye",
            "Jamin Shin",
            "Minjoon Seo"
        ],
        "published": "2023",
        "summary": "Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks. In this work, we aim to equip smaller LMs with the step-by-step reasoning capability by instruction tuning with CoT rationales. In order to achieve this goal, we first introduce a new instruction-tuning dataset called the CoT Collection, which augments the existing Flan Collection (including only 9 CoT tasks) with additional 1.84 million rationales across 1,060 tasks. We show that CoT fine-tuning Flan-T5 (3B & 11B) with CoT Collection enables smaller LMs to have better CoT capabilities on unseen tasks. On the BIG-Bench-Hard (BBH) benchmark, we report an average improvement of +4.34% (Flan-T5 3B) and +2.60% (Flan-T5 11B), in terms of zero-shot task accuracy. Furthermore, we show that instruction tuning with CoT Collection allows LMs to possess stronger few-shot learning capabilities on 4 domain-specific tasks, resulting in an improvement of +2.24% (Flan-T5 3B) and +2.37% (Flan-T5 11B), even outperforming ChatGPT utilizing demonstrations until the max length by a +13.98% margin. Our code, the CoT Collection data, and model checkpoints are publicly available.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.782.pdf",
        "keywords": [
            "language models",
            "chain of thought",
            "shot learning",
            "cot collection"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks.\""
    },
    {
        "title": "Explaining Interactions Between Text Spans",
        "authors": [
            "Sagnik Ray Choudhury",
            "Pepa Atanasova",
            "Isabelle Augenstein"
        ],
        "published": "2023",
        "summary": "Reasoning over spans of tokens from different parts of the input is essential for natural language understanding (NLU) tasks such as fact-checking (FC), machine reading comprehension (MRC) or natural language inference (NLI). However, existing highlight-based explanations primarily focus on identifying individual important features or interactions only between adjacent tokens or tuples of tokens. Most notably, there is a lack of annotations capturing the human decision-making process with respect to the necessary interactions for informed decision-making in such tasks. To bridge this gap, we introduce SpanEx, a multi-annotator dataset of human span interaction explanations for two NLU tasks: NLI and FC. We then investigate the decision-making processes of multiple fine-tuned large language models in terms of the employed connections between spans in separate parts of the input and compare them to the human reasoning processes. Finally, we present a novel community detection based unsupervised method to extract such interaction explanations. We make the code and the dataset available on [Github](https://github.com/copenlu/spanex). The dataset is also available on [Huggingface datasets](https://huggingface.co/datasets/copenlu/spanex).",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.783.pdf",
        "keywords": [
            "spans",
            "text spans",
            "community detection",
            "interactions"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"investigate the decision-making processes of multiple fine-tuned large language models in terms of the employed connections between spans in separate parts of the input and compare them to the human reasoning processes.\"\n\nThis rating is based on the fact that the paper mentions the limitations of large language models in terms of their decision-making processes and compares them to human reasoning processes, but does not extensively analyze or elaborate",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"investigate the decision-making processes of multiple fine-tuned large language models in terms of the employed connections between spans in separate parts of the input and compare them to the human reasoning processes.\"\n\nThis rating is based on the fact that the paper mentions the limitations of large language models in terms of their decision-making processes and compares them to human reasoning processes, but does not extensively analyze or elaborate"
    },
    {
        "title": "Question Answering as Programming for Solving Time-Sensitive Questions",
        "authors": [
            "Xinyu Zhu",
            "Cheng Yang",
            "Bei Chen",
            "Siheng Li",
            "Jian-Guang Lou",
            "Yujiu Yang"
        ],
        "published": "2023",
        "summary": "Question answering plays a pivotal role in human daily life because it involves our acquisition of knowledge about the world. However, due to the dynamic and ever-changing nature of real-world facts, the answer can be completely different when the time constraint in the question changes. Recently, Large Language Models (LLMs) have shown remarkable intelligence in question answering, while our experiments reveal that the aforementioned problems still pose a significant challenge to existing LLMs. This can be attributed to the LLMs’ inability to perform rigorous reasoning based on surface-level text semantics. To overcome this limitation, rather than requiring LLMs to directly answer the question, we propose a novel approach where we reframe the Question Answering task as Programming (QAaP). Concretely, by leveraging modern LLMs’ superior capability in understanding both natural language and programming language, we endeavor to harness LLMs to represent diversely expressed text as well-structured code and select the best matching answer from multiple candidates through programming. We evaluate our QAaP framework on several time-sensitive question answering datasets and achieve decent improvement, up to 14.5% over strong baselines.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.787.pdf",
        "keywords": [
            "question answering",
            "time sensitive question answering",
            "time constraint",
            "question answering as programming",
            "as programming"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"our experiments reveal that the aforementioned problems still pose a significant challenge to existing LLMs. This can be attributed to the LLMs’ inability to perform rigorous reasoning based on surface-level text semantics.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"our experiments reveal that the aforementioned problems still pose a significant challenge to existing LLMs. This can be attributed to the LLMs’ inability to perform rigorous reasoning based on surface-level text semantics.\""
    },
    {
        "title": "Controlling Pre-trained Language Models for Grade-Specific Text Simplification",
        "authors": [
            "Sweta Agrawal",
            "Marine Carpuat"
        ],
        "published": "2023",
        "summary": "Text simplification systems rewrite text to make it more readable while preserving its content. However, what makes a text easy to read depends on the intended readers. Recent work has shown that pre-trained language models can simplify text using a wealth of techniques to control output simplicity, ranging from specifying only the desired reading grade level, to directly specifying low-level edit operations. Yet it remains unclear how to set these control parameters in practice. Existing approaches set them at the corpus level, disregarding the complexity of individual inputs and considering only one level of output complexity. In this work, we conduct an empirical study to understand how different control mechanisms impact the adequacy and simplicity of text simplification systems. Based on these insights, we introduce a simple method that predicts the edit operations required for simplifying a text for a specific grade level on an instance-per-instance basis. This approach improves the quality of the simplified outputs over corpus-level search-based heuristics.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.790.pdf",
        "keywords": [
            "grade specific text",
            "pre trained language models",
            "text simplification",
            "grade level"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Yet it remains unclear how to set these control parameters in practice. Existing approaches set them at the corpus level, disregarding the complexity of individual inputs and considering only one level of output complexity.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Yet it remains unclear how to set these control parameters in practice. Existing approaches set them at the corpus level, disregarding the complexity of individual inputs and considering only one level of output complexity.\""
    },
    {
        "title": "Context Compression for Auto-regressive Transformers with Sentinel Tokens",
        "authors": [
            "Siyu Ren",
            "Qi Jia",
            "Kenny Zhu"
        ],
        "published": "2023",
        "summary": "The quadratic complexity of the attention module makes it gradually become the bulk of compute in Transformer-based LLMs during generation. Moreover, the excessive key-value cache that arises when dealing with long inputs also brings severe issues on memory footprint and inference latency. In this work, we propose a plug-and-play approach that is able to incrementally compress the intermediate activation of a specified span of tokens into compact ones, thereby reducing both memory and computational cost when processing subsequent context. Experiments on both in-domain language modeling and zero-shot open-ended document generation demonstrate the advantage of our approach over sparse attention baselines in terms of fluency, n-gram matching, and semantic similarity. At last, we comprehensively profile the benefit of context compression on improving the system throughout. Code is available at https://github.com/DRSY/KV_Compression.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.794.pdf",
        "keywords": [
            "sentinel tokens",
            "context compression",
            "auto regressive transformers"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The quadratic complexity of the attention module makes it gradually become the bulk of compute in Transformer-based LLMs during generation. Moreover, the excessive key-value cache that arises when dealing with long inputs also brings severe issues on memory footprint and inference latency.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The quadratic complexity of the attention module makes it gradually become the bulk of compute in Transformer-based LLMs during generation. Moreover, the excessive key-value cache that arises when dealing with long inputs also brings severe issues on memory footprint and inference latency.\""
    },
    {
        "title": "SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization",
        "authors": [
            "Hyunwoo Kim",
            "Jack Hessel",
            "Liwei Jiang",
            "Peter West",
            "Ximing Lu",
            "Youngjae Yu",
            "Pei Zhou",
            "Ronan Bras",
            "Malihe Alikhani",
            "Gunhee Kim",
            "Maarten Sap",
            "Yejin Choi"
        ],
        "published": "2023",
        "summary": "Data scarcity has been a long standing issue in the field of open-domain social dialogue. To quench this thirst, we present SODA: the first publicly available, million-scale high-quality social dialogue dataset. By contextualizing social commonsense knowledge from a knowledge graph, we are able to distill an exceptionally broad spectrum of social interactions from a large language model. Human evaluation shows that conversations in SODA are more consistent, specific, and (surprisingly) natural than those in prior human-authored datasets. Using SODA, we train COSMO: a generalizable conversation model that is significantly more natural and consistent on unseen datasets than best-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna). Experiments reveal COSMO is sometimes even preferred to the original human-written gold responses. Additionally, our results shed light on the distinction between knowledge-enriched conversations and natural social chitchats. We plan to make our data, model, and code public.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.799.pdf",
        "keywords": [
            "dialogue",
            "dialogue distillation",
            "social commonsense contextualization",
            "social dialogue",
            "social chitchats",
            "knowledge"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"By contextualizing social commonsense knowledge from a knowledge graph, we are able to distill an exceptionally broad spectrum of social interactions from a large language model.\"\n\nThis paper discusses LLMs but does not explicitly mention any limitations of the models in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"By contextualizing social commonsense knowledge from a knowledge graph, we are able to distill an exceptionally broad spectrum of social interactions from a large language model.\"\n\nThis paper discusses LLMs but does not explicitly mention any limitations of the models in the abstract."
    },
    {
        "title": "MailEx: Email Event and Argument Extraction",
        "authors": [
            "Saurabh Srivastava",
            "Gaurav Singh",
            "Shou Matsumoto",
            "Ali Raz",
            "Paulo Costa",
            "Joshua Poore",
            "Ziyu Yao"
        ],
        "published": "2023",
        "summary": "In this work, we present the first dataset, MailEx, for performing event extraction from conversational email threads. To this end, we first proposed a new taxonomy covering 10 event types and 76 arguments in the email domain. Our final dataset includes 1.5K email threads and ~4K emails, which are annotated with a total of ~8K event instances. To understand the task challenges, we conducted a series of experiments comparing three types of approaches, i.e., fine-tuned sequence labeling, fine-tuned generative extraction, and few-shot in-context learning. Our results showed that the task of email event extraction is far from being addressed, due to challenges lying in, e.g., extracting non-continuous, shared trigger spans, extracting non-named entity arguments, and modeling the email conversational history. Our work thus suggests more future investigations in this domain-specific event extraction task.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.801.pdf",
        "keywords": [
            "event",
            "event extraction",
            "arguments",
            "argument extraction",
            "email event extraction",
            "email"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Beware of Model Collapse! Fast and Stable Test-time Adaptation for Robust Question Answering",
        "authors": [
            "Yi Su",
            "Yixin Ji",
            "Juntao Li",
            "Hai Ye",
            "Min Zhang"
        ],
        "published": "2023",
        "summary": "Although pre-trained language models (PLM) have achieved great success in question answering (QA), their robustness is still insufficient to support their practical applications, especially in the face of distribution shifts. Recently, test-time adaptation (TTA) has shown great potential for solving this problem, which adapts the model to fit the test samples at test time. However, TTA sometimes causes model collapse, making almost all the model outputs incorrect, which has raised concerns about its stability and reliability. In this paper, we delve into why TTA causes model collapse and find that the imbalanced label distribution inherent in QA is the reason for it. To address this problem, we propose Anti-Collapse Fast test-time adaptation (Anti-CF), which utilizes the source model‘s output to regularize the update of the adapted model during test time. We further design an efficient side block to reduce its inference time. Extensive experiments on various distribution shift scenarios and pre-trained language models (e.g., XLM-RoBERTa, BLOOM) demonstrate that our method can achieve comparable or better results than previous TTA methods at a speed close to vanilla forward propagation, which is 1.8× to 4.4× speedup compared to previous TTA methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.803.pdf",
        "keywords": [
            "test time adaptation",
            "model collapse",
            "beware",
            "answering",
            "robust question answering",
            "language models",
            "anti collapse",
            "language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although pre-trained language models (PLM) have achieved great success in question answering (QA), their robustness is still insufficient to support their practical applications, especially in the face of distribution shifts.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although pre-trained language models (PLM) have achieved great success in question answering (QA), their robustness is still insufficient to support their practical applications, especially in the face of distribution shifts.\""
    },
    {
        "title": "Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation",
        "authors": [
            "Anastasia Kritharoula",
            "Maria Lymperaiou",
            "Giorgos Stamou"
        ],
        "published": "2023",
        "summary": "Visual Word Sense Disambiguation (VWSD) is a novel challenging task with the goal of retrieving an image among a set of candidates, which better represents the meaning of an ambiguous word within a given context. In this paper, we make a substantial step towards unveiling this interesting task by applying a varying set of approaches. Since VWSD is primarily a text-image retrieval task, we explore the latest transformer-based methods for multimodal retrieval. Additionally, we utilize Large Language Models (LLMs) as knowledge bases to enhance the given phrases and resolve ambiguity related to the target word. We also study VWSD as a unimodal problem by converting to text-to-text and image-to-image retrieval, as well as question-answering (QA), to fully explore the capabilities of relevant models. To tap into the implicit knowledge of LLMs, we experiment with Chain-of-Thought (CoT) prompting to guide explainable answer generation. On top of all, we train a learn to rank (LTR) model in order to combine our different modules, achieving competitive ranking results. Extensive experiments on VWSD demonstrate valuable insights to effectively drive future directions.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.807.pdf",
        "keywords": [
            "visual word sense disambiguation",
            "multimodal retrieval",
            "text image retrieval",
            "large language models",
            "question answering"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"To tap into the implicit knowledge of LLMs, we experiment with Chain-of-Thought (CoT) prompting to guide explainable answer generation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"To tap into the implicit knowledge of LLMs, we experiment with Chain-of-Thought (CoT) prompting to guide explainable answer generation.\""
    },
    {
        "title": "Be Selfish, But Wisely: Investigating the Impact of Agent Personality in Mixed-Motive Human-Agent Interactions",
        "authors": [
            "Kushal Chawla",
            "Ian Wu",
            "Yu Rong",
            "Gale Lucas",
            "Jonathan Gratch"
        ],
        "published": "2023",
        "summary": "A natural way to design a negotiation dialogue system is via self-play RL: train an agent that learns to maximize its performance by interacting with a simulated user that has been designed to imitate human-human dialogue data. Although this procedure has been adopted in prior work, we find that it results in a fundamentally flawed system that fails to learn the value of compromise in a negotiation, which can often lead to no agreements (i.e., the partner walking away without a deal), ultimately hurting the model’s overall performance. We investigate this observation in the context of DealOrNoDeal task, a multi-issue negotiation over books, hats, and balls. Grounded in negotiation theory from Economics, we modify the training procedure in two novel ways to design agents with diverse personalities and analyze their performance with human partners. We find that although both techniques show promise, a selfish agent, which maximizes its own performance while also avoiding walkaways, performs superior to other variants by implicitly learning to generate value for both itself and the negotiation partner. We discuss the implications of our findings for what it means to be a successful negotiation dialogue system and how these systems should be designed in the future.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.808.pdf",
        "keywords": [
            "agent",
            "negotiation",
            "selfish agent",
            "be selfish",
            "agent personality",
            "negotiation theory",
            "investigating"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Doolittle: Benchmarks and Corpora for Academic Writing Formalization",
        "authors": [
            "Shizhe Diao",
            "Yongyu Lei",
            "Liangming Pan",
            "Tianqing Fang",
            "Wangchunshu Zhou",
            "Sedrick Keh",
            "Min-Yen Kan",
            "Tong Zhang"
        ],
        "published": "2023",
        "summary": "Improving the quality of academic writing is a meaningful but challenging task. Conventional methods of language refinement focus on narrow, specific linguistic features within isolated sentences, such as grammatical errors and improper word use. We propose a more general task, Academic Writing Formalization (AWF), to improve the overall quality of formal academic writing at the paragraph level. We formulate this language refinement task as a formal text style transfer task which transfers informal-academic text to formal-academic and contribute a large-scale non-parallel dataset, Doolittle, for this purpose. Concurrently, we apply a method named metric-oriented reinforcement learning (MORL) to two large language models (LLM) where we incorporate different levels of automatic feedback into the training process. Our experiments reveal that existing text transfer models and grammatical error correction models address certain aspects of AWF but still have a significant performance gap compared to human performance. Meanwhile, language models fine-tuned with our MORL method exhibit considerably improved performance, rivaling the latest chatbot ChatGPT, but still have a non-negligible gap compared to the ground truth formal-academic texts in Doolittle.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.809.pdf",
        "keywords": [
            "corpora",
            "refinement",
            "language refinement",
            "academic writing formalization",
            "text transfer",
            "texts",
            "language",
            "grammatical error"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our experiments reveal that existing text transfer models and grammatical error correction models address certain aspects of AWF but still have a significant performance gap compared to human performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our experiments reveal that existing text transfer models and grammatical error correction models address certain aspects of AWF but still have a significant performance gap compared to human performance.\""
    },
    {
        "title": "Token Prediction as Implicit Classification to Identify LLM-Generated Text",
        "authors": [
            "Yutian Chen",
            "Hao Kang",
            "Vivian Zhai",
            "Liangze Li",
            "Rita Singh",
            "Bhiksha Raj"
        ],
        "published": "2023",
        "summary": "This paper introduces a novel approach for identifying the possible large language models (LLMs) involved in text generation. Instead of adding an additional classification layer to a base LM, we reframe the classification task as a next-token prediction task and directly fine-tune the base LM to perform it. We utilize the Text-to-Text Transfer Transformer (T5) model as the backbone for our experiments. We compared our approach to the more direct approach of utilizing hidden states for classification. Evaluation shows the exceptional performance of our method in the text classification task, highlighting its simplicity and efficiency. Furthermore, interpretability studies on the features extracted by our model reveal its ability to differentiate distinctive writing styles among various LLMs even in the absence of an explicit classifier. We also collected a dataset named OpenLLMText, containing approximately 340k text samples from human and LLMs, including GPT3.5, PaLM, LLaMA, and GPT2.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.810.pdf",
        "keywords": [
            "token prediction",
            "text classification",
            "implicit classification"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit evidence of LLM limitations, but the paper discusses identifying LLM-generated text, which implies limitations in LLMs' detectability.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: No explicit evidence of LLM limitations, but the paper discusses identifying LLM-generated text, which implies limitations in LLMs' detectability."
    },
    {
        "title": "XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models",
        "authors": [
            "Davis Liang",
            "Hila Gonen",
            "Yuning Mao",
            "Rui Hou",
            "Naman Goyal",
            "Marjan Ghazvininejad",
            "Luke Zettlemoyer",
            "Madian Khabsa"
        ],
        "published": "2023",
        "summary": "Large multilingual language models typically rely on a single vocabulary shared across 100+ languages. As these models have increased in parameter count and depth, vocabulary size has remained largely unchanged. This vocabulary bottleneck limits the representational capabilities of multilingual models like XLM-R. In this paper, we introduce a new approach for scaling to very large multilingual vocabularies by de-emphasizing token sharing between languages with little lexical overlap and assigning vocabulary capacity to achieve sufficient coverage for each individual language. Tokenizations using our vocabulary are typically more semantically meaningful and shorter compared to XLM-R. Leveraging this improved vocabulary, we train XLM-V, a multilingual language model with a one million token vocabulary. XLM-V outperforms XLM-R on every task we tested on ranging from natural language inference (XNLI), question answering (MLQA, XQuAD, TyDiQA), to named entity recognition (WikiAnn). XLM-V is particularly effective on low-resource language tasks and outperforms XLM-R by 11.2% and 5.8% absolute on MasakhaNER and Americas NLI, respectively.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.813.pdf",
        "keywords": [
            "masked language models",
            "language",
            "multilingual language model",
            "large multilingual language models",
            "vocabularies",
            "vocabulary bottleneck",
            "entity recognition"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This vocabulary bottleneck limits the representational capabilities of multilingual models like XLM-R.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"This vocabulary bottleneck limits the representational capabilities of multilingual models like XLM-R.\""
    },
    {
        "title": "Character-LLM: A Trainable Agent for Role-Playing",
        "authors": [
            "Yunfan Shao",
            "Linyang Li",
            "Junqi Dai",
            "Xipeng Qiu"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc. Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences. To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents memorize their characters and experiences. Experimental results show interesting observations that help build future simulacra of humankind.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.814.pdf",
        "keywords": [
            "trainable agent",
            "role playing",
            "behaviors",
            "simulate",
            "character"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, the abstract only discusses the potential of LLMs in simulating human behaviors.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, the abstract only discusses the potential of LLMs in simulating human behaviors."
    },
    {
        "title": "Natural Language Decompositions of Implicit Content Enable Better Text Representations",
        "authors": [
            "Alexander Hoyle",
            "Rupak Sarkar",
            "Pranav Goel",
            "Philip Resnik"
        ],
        "published": "2023",
        "summary": "When people interpret text, they rely on inferences that go beyond the observed language itself. Inspired by this observation, we introduce a method for the analysis of text that takes implicitly communicated content explicitly into account. We use a large language model to produce sets of propositions that are inferentially related to the text that has been observed, then validate the plausibility of the generated content via human judgments. Incorporating these explicit representations of implicit content proves useful in multiple problem settings that involve the human interpretation of utterances: assessing the similarity of arguments, making sense of a body of opinion data, and modeling legislative behavior. Our results suggest that modeling the meanings behind observed language, rather than the literal text alone, is a valuable direction for NLP and particularly its applications to social science.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.815.pdf",
        "keywords": [
            "natural language",
            "language model"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned, but the paper uses an LLM to produce sets of propositions and relies on human judgments to validate the plausibility of the generated content, implying some limitations in the model's ability to generate accurate inferences.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned, but the paper uses an LLM to produce sets of propositions and relies on human judgments to validate the plausibility of the generated content, implying some limitations in the model's ability to generate accurate inferences."
    },
    {
        "title": "How do languages influence each other? Studying cross-lingual data sharing during LM fine-tuning",
        "authors": [
            "Rochelle Choenni",
            "Dan Garrette",
            "Ekaterina Shutova"
        ],
        "published": "2023",
        "summary": "Multilingual language models (MLMs) are jointly trained on data from many different languages such that representation of individual languages can benefit from other languages’ data. Impressive performance in zero-shot cross-lingual transfer shows that these models are able to exploit this property. Yet, it remains unclear to what extent, and under which conditions, languages rely on each other’s data. To answer this question, we use TracIn (Pruthi et al., 2020), a training data attribution (TDA) method, to retrieve training samples from multilingual data that are most influential for test predictions in a given language. This allows us to analyse cross-lingual sharing mechanisms of MLMs from a new perspective. While previous work studied cross-lingual sharing at the model parameter level, we present the first approach to study it at the data level. We find that MLMs rely on data from multiple languages during fine-tuning and this reliance increases as fine-tuning progresses. We further find that training samples from other languages can both reinforce and complement the knowledge acquired from data of the test language itself.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.818.pdf",
        "keywords": [
            "tuning",
            "fine tuning",
            "lm",
            "languages",
            "tracin"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations mentioned, but \"it remains unclear to what extent, and under which conditions, languages rely on each other’s data\" implies a knowledge gap in understanding how MLMs work.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: No explicit limitations mentioned, but \"it remains unclear to what extent, and under which conditions, languages rely on each other’s data\" implies a knowledge gap in understanding how MLMs work."
    },
    {
        "title": "COFFEE: Counterfactual Fairness for Personalized Text Generation in Explainable Recommendation",
        "authors": [
            "Nan Wang",
            "Qifan Wang",
            "Yi-Chia Wang",
            "Maziar Sanjabi",
            "Jingzhou Liu",
            "Hamed Firooz",
            "Hongning Wang",
            "Shaoliang Nie"
        ],
        "published": "2023",
        "summary": "As language models become increasingly integrated into our digital lives, Personalized Text Generation (PTG) has emerged as a pivotal component with a wide range of applications. However, the bias inherent in user written text, often used for PTG model training, can inadvertently associate different levels of linguistic quality with users’ protected attributes. The model can inherit the bias and perpetuate inequality in generating text w.r.t. users’ protected attributes, leading to unfair treatment when serving users. In this work, we investigate fairness of PTG in the context of personalized explanation generation for recommendations. We first discuss the biases in generated explanations and their fairness implications. To promote fairness, we introduce a general framework to achieve measure-specific counterfactual fairness in explanation generation. Extensive experiments and human evaluations demonstrate the effectiveness of our method.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.819.pdf",
        "keywords": [
            "fairness",
            "counterfactual fairness",
            "coffee",
            "inequality",
            "personalized text generation",
            "personalized explanation",
            "recommendations",
            "explanations",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the bias inherent in user written text, often used for PTG model training, can inadvertently associate different levels of linguistic quality with users’ protected attributes. The model can inherit the bias and perpetuate inequality in generating text w.r.t. users’ protected attributes, leading to unfair treatment when serving users.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the bias inherent in user written text, often used for PTG model training, can inadvertently associate different levels of linguistic quality with users’ protected attributes. The model can inherit the bias and perpetuate inequality in generating text w.r.t. users’ protected attributes, leading to unfair treatment when serving users.\""
    },
    {
        "title": "NameGuess: Column Name Expansion for Tabular Data",
        "authors": [
            "Jiani Zhang",
            "Zhengyuan Shen",
            "Balasubramaniam Srinivasan",
            "Shen Wang",
            "Huzefa Rangwala",
            "George Karypis"
        ],
        "published": "2023",
        "summary": "Recent advances in large language models have revolutionized many sectors, including the database industry. One common challenge when dealing with large volumes of tabular data is the pervasive use of abbreviated column names, which can negatively impact performance on various data search, access, and understanding tasks. To address this issue, we introduce a new task, called NameGuess, to expand column names (used in database schema) as a natural language generation problem. We create a training dataset of 384K abbreviated-expanded column pairs using a new data fabrication method and a human-annotated evaluation benchmark that includes 9.2K examples from real-world tables. To tackle the complexities associated with polysemy and ambiguity in NameGuess, we enhance auto-regressive language models by conditioning on table content and column header names – yielding a fine-tuned model (with 2.7B parameters) that matches human performance. Furthermore, we conduct a comprehensive analysis (on multiple LLMs) to validate the effectiveness of table content in NameGuess and identify promising future opportunities. Code has been made available at https://github.com/amazon-science/nameguess.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.820.pdf",
        "keywords": [
            "column name expansion",
            "language models",
            "column",
            "nameguess",
            "auto regressive language models",
            "language generation",
            "database schema"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"To tackle the complexities associated with polysemy and ambiguity in NameGuess, we enhance auto-regressive language models...\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"To tackle the complexities associated with polysemy and ambiguity in NameGuess, we enhance auto-regressive language models...\""
    },
    {
        "title": "BLESS: Benchmarking Large Language Models on Sentence Simplification",
        "authors": [
            "Tannon Kew",
            "Alison Chi",
            "Laura Vásquez-Rodríguez",
            "Sweta Agrawal",
            "Dennis Aumiller",
            "Fernando Alva-Manchego",
            "Matthew Shardlow"
        ],
        "published": "2023",
        "summary": "We present BLESS, a comprehensive performance benchmark of the most recent state-of-the-art Large Language Models (LLMs) on the task of text simplification (TS). We examine how well off-the-shelf LLMs can solve this challenging task, assessing a total of 44 models, differing in size, architecture, pre-training methods, and accessibility, on three test sets from different domains (Wikipedia, news, and medical) under a few-shot setting. Our analysis considers a suite of automatic metrics, as well as a large-scale quantitative investigation into the types of common edit operations performed by the different models. Furthermore, we perform a manual qualitative analysis on a subset of model outputs to better gauge the quality of the generated simplifications. Our evaluation indicates that the best LLMs, despite not being trained on TS perform comparably with state-of-the-art TS baselines. Additionally, we find that certain LLMs demonstrate a greater range and diversity of edit operations. Our performance benchmark will be available as a resource for the development of future TS methods and evaluation metrics.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.821.pdf",
        "keywords": [
            "benchmarking",
            "language models",
            "sentence simplification",
            "large language models",
            "performance benchmark",
            "test"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our evaluation indicates that the best LLMs, despite not being trained on TS perform comparably with state-of-the-art TS baselines.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (not being trained on TS) in passing, but it is not the primary focus of the paper. The paper is more focused on presenting a benchmark and",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our evaluation indicates that the best LLMs, despite not being trained on TS perform comparably with state-of-the-art TS baselines.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (not being trained on TS) in passing, but it is not the primary focus of the paper. The paper is more focused on presenting a benchmark and"
    },
    {
        "title": "ViStruct: Visual Structural Knowledge Extraction via Curriculum Guided Code-Vision Representation",
        "authors": [
            "Yangyi Chen",
            "Xingyao Wang",
            "Manling Li",
            "Derek Hoiem",
            "Heng Ji"
        ],
        "published": "2023",
        "summary": "State-of-the-art vision-language models (VLMs) still have limited performance in structural knowledge extraction, such as relations between objects. In this work, we present ViStruct, a training framework to learn VLMs for effective visual structural knowledge extraction. Two novel designs are incorporated. First, we propose to leverage the inherent structure of programming language to depict visual structural information. This approach enables explicit and consistent representation of visual structural information of multiple granularities, such as concepts, relations, and events, in a well-organized structured format. Second, we introduce curriculum-based learning for VLMs to progressively comprehend visual structures, from fundamental visual concepts to intricate event structures. Our intuition is that lower-level knowledge may contribute to complex visual structure understanding. Furthermore, we compile and release a collection of datasets tailored for visual structural knowledge extraction. We adopt a weakly-supervised approach to directly generate visual event structures from captions for ViStruct training, capitalizing on abundant image-caption pairs from the web. In experiments, we evaluate ViStruct on visual structure prediction tasks, demonstrating its effectiveness in improving the understanding of visual structures. The code will be made public to facilitate future research.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.824.pdf",
        "keywords": [
            "knowledge extraction",
            "weakly supervised",
            "curriculum",
            "structural knowledge extraction",
            "visual structural knowledge extraction",
            "code vision"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"State-of-the-art vision-language models (VLMs) still have limited performance in structural knowledge extraction, such as relations between objects.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"State-of-the-art vision-language models (VLMs) still have limited performance in structural knowledge extraction, such as relations between objects.\""
    },
    {
        "title": "LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models",
        "authors": [
            "Huiqiang Jiang",
            "Qianhui Wu",
            "Chin-Yew Lin",
            "Yuqing Yang",
            "Lili Qiu"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have been applied in various applications due to their astonishing capabilities. With advancements in technologies such as chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of tokens. To accelerate model inference and reduce cost, this paper presents LLMLingua, a coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models. We conduct experiments and analysis over four datasets from different scenarios, i.e., GSM8K, BBH, ShareGPT, and Arxiv-March23; showing that the proposed approach yields state-of-the-art performance and allows for up to 20x compression with little performance loss.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.825.pdf",
        "keywords": [
            "language models",
            "large language models",
            "compressing prompts"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"With advancements in technologies such as chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of tokens.\"\n\nThis rating is given because the paper mentions a limitation of LLMs (lengthy prompts) but does not elaborate on it and instead focuses on the proposed solution to",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"With advancements in technologies such as chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of tokens.\"\n\nThis rating is given because the paper mentions a limitation of LLMs (lengthy prompts) but does not elaborate on it and instead focuses on the proposed solution to"
    },
    {
        "title": "Characterizing and Verifying Scientific Claims: Qualitative Causal Structure is All You Need",
        "authors": [
            "Jinxuan Wu",
            "Wenhan Chao",
            "Xian Zhou",
            "Zhunchen Luo"
        ],
        "published": "2023",
        "summary": "A scientific claim typically begins with the formulation of a research question or hypothesis, which is a tentative statement or proposition about a phenomenon or relationship between variables. Within the realm of scientific claim verification, considerable research efforts have been dedicated to attention architectures and leveraging the text comprehension capabilities of Pre-trained Language Models (PLMs), yielding promising performances. However, these models overlook the causal structure information inherent in scientific claims, thereby failing to establish a comprehensive chain of causal inference. This paper delves into the exploration to highlight the crucial role of qualitative causal structure in characterizing and verifying scientific claims based on evidence. We organize the qualitative causal structure into a heterogeneous graph and propose a novel attention-based graph neural network model to facilitate causal reasoning across relevant causally-potent factors. Our experiments demonstrate that by solely utilizing the qualitative causal structure, the proposed model achieves comparable performance to PLM-based models. Furthermore, by incorporating semantic features, our model outperforms state-of-the-art approaches comprehensively.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.828.pdf",
        "keywords": [
            "causal structure",
            "scientific claims",
            "causal inference",
            "text comprehension",
            "qualitative causal structure",
            "language models",
            "neural network",
            "causal reasoning",
            "architectures"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these models overlook the causal structure information inherent in scientific claims, thereby failing to establish a comprehensive chain of causal inference.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these models overlook the causal structure information inherent in scientific claims, thereby failing to establish a comprehensive chain of causal inference.\""
    },
    {
        "title": "FOCUS: Effective Embedding Initialization for Monolingual Specialization of Multilingual Models",
        "authors": [
            "Konstantin Dobler",
            "Gerard de Melo"
        ],
        "published": "2023",
        "summary": "Using model weights pretrained on a high-resource language as a warm start can reduce the need for data and compute to obtain high-quality language models for other, especially low-resource, languages. However, if we want to use a new tokenizer specialized for the target language, we cannot transfer the source model’s embedding matrix. In this paper, we propose FOCUS - **F**ast **O**verlapping Token **C**ombinations **U**sing **S**parsemax, a novel embedding initialization method that effectively initializes the embedding matrix for a new tokenizer based on information in the source model’s embedding matrix. FOCUS represents newly added tokens as combinations of tokens in the overlap of the source and target vocabularies. The overlapping tokens are selected based on semantic similarity in an auxiliary static token embedding space. We focus our study on using the multilingual XLM-R as a source model and empirically show that FOCUS outperforms random initialization and previous work on language modeling and on a range of downstream tasks (NLI, QA, and NER). We publish our model checkpoints and code on GitHub.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.829.pdf",
        "keywords": [
            "initialization",
            "vocabularies",
            "model weights",
            "monolingual specialization"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, if we want to use a new tokenizer specialized for the target language, we cannot transfer the source model’s embedding matrix.\"\n\nThis evidence shows that the abstract mentions a limitation of LLMs, specifically the difficulty in transferring the source model's embedding matrix when using a new tokenizer specialized for the target language. However, this limitation is not the primary focus of the paper, which",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, if we want to use a new tokenizer specialized for the target language, we cannot transfer the source model’s embedding matrix.\"\n\nThis evidence shows that the abstract mentions a limitation of LLMs, specifically the difficulty in transferring the source model's embedding matrix when using a new tokenizer specialized for the target language. However, this limitation is not the primary focus of the paper, which"
    },
    {
        "title": "ByteSized32: A Corpus and Challenge Task for Generating Task-Specific World Models Expressed as Text Games",
        "authors": [
            "Ruoyao Wang",
            "Graham Todd",
            "Xingdi Yuan",
            "Ziang Xiao",
            "Marc-Alexandre Côté",
            "Peter Jansen"
        ],
        "published": "2023",
        "summary": "In this work we investigate the capacity of language models to generate explicit, interpretable, and interactive world models of scientific and common-sense reasoning tasks. We operationalize this as a task of generating text games, expressed as hundreds of lines of Python code. To facilitate this task, we introduce ByteSized32, a corpus of 32 reasoning-focused text games totalling 20k lines of Python code. We empirically demonstrate that GPT-4 can use these games as templates for single-shot in-context learning, successfully producing runnable games on unseen topics in 28% of cases. When allowed to self-reflect on program errors, game runnability substantially increases to 58%. While evaluating simulation fidelity is labor intensive, we introduce a suite of automated metrics to assess game fidelity, technical validity, adherence to task specifications, and winnability, showing a high-degree of agreement with expert human ratings. We pose this as a challenge task to spur further development at the juncture of world modeling and code generation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.830.pdf",
        "keywords": [
            "world modeling",
            "games",
            "text games",
            "challenge task",
            "task specific world models",
            "corpus",
            "python"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While evaluating simulation fidelity is labor intensive\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While evaluating simulation fidelity is labor intensive\""
    },
    {
        "title": "Skill-Based Few-Shot Selection for In-Context Learning",
        "authors": [
            "Shengnan An",
            "Bo Zhou",
            "Zeqi Lin",
            "Qiang Fu",
            "Bei Chen",
            "Nanning Zheng",
            "Weizhu Chen",
            "Jian-Guang Lou"
        ],
        "published": "2023",
        "summary": "*In-context learning* is the paradigm that adapts large language models to downstream tasks by providing a few examples. *Few-shot selection*—selecting appropriate examples for each test instance separately—is important for in-context learning. In this paper, we propose **Skill-KNN**, a skill-based few-shot selection method for in-context learning. The key advantages of Skill-KNN include: (1) it addresses the problem that existing methods based on pre-trained embeddings can be easily biased by surface natural language features that are not important for the target task; (2) it does not require training or fine-tuning of any models, making it suitable for frequently expanding or changing example banks. The key insight is to optimize the inputs fed into the embedding model, rather than tuning the model itself. Technically, Skill-KNN generates the skill-based descriptions for each test case and candidate example by utilizing a pre-processing few-shot prompting, thus eliminating unimportant surface features. Experimental results across five cross-domain semantic parsing datasets and six backbone models show that Skill-KNN significantly outperforms existing methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.831.pdf",
        "keywords": [
            "shot selection",
            "pre trained embeddings",
            "in context learning",
            "context learning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"it addresses the problem that existing methods based on pre-trained embeddings can be easily biased by surface natural language features that are not important for the target task\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"it addresses the problem that existing methods based on pre-trained embeddings can be easily biased by surface natural language features that are not important for the target task\""
    },
    {
        "title": "PTP: Boosting Stability and Performance of Prompt Tuning with Perturbation-Based Regularizer",
        "authors": [
            "Lichang Chen",
            "Jiuhai Chen",
            "Heng Huang",
            "Minhao Cheng"
        ],
        "published": "2023",
        "summary": "Recent studies show that prompt tuning can better leverage the power of large language models than fine-tuning on downstream natural language understanding tasks. However, the existing prompt tuning methods have training instability issues, as the variance of scores under different random seeds is quite large. To address this critical problem, we first investigate and find that the loss landscape of vanilla prompt tuning is precipitous when it is visualized, where a slight change of input data can cause a big fluctuation in the loss landscape. This is an essential factor that leads to the instability of prompt tuning. Based on this observation, we introduce perturbation-based regularizers, which can smooth the loss landscape, into prompt tuning. We propose a new algorithm, called Prompt Tuning with Perturbation-based regularizer (PTP), which can not only alleviate training instability dramatically but also boost the performance of prompt tuning. We design two kinds of perturbation-based regularizers, including random-noise-based and adversarial-based. In particular, our proposed perturbations are flexible on both text space and embedding space. Extensive experiments show the effectiveness of our proposed methods in stabilizing the training. Our new algorithms improve the state-of-the-art prompt tuning methods by 1.94% and 2.34% on SuperGLUE and FewGLUE benchmarks, respectively.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.833.pdf",
        "keywords": [
            "regularizer",
            "stabilizing",
            "performance",
            "perturbation",
            "instability",
            "boosting stability",
            "ptp",
            "tuning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the existing prompt tuning methods have training instability issues, as the variance of scores under different random seeds is quite large.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the existing prompt tuning methods have training instability issues, as the variance of scores under different random seeds is quite large.\""
    },
    {
        "title": "Towards Unsupervised Recognition of Token-level Semantic Differences in Related Documents",
        "authors": [
            "Jannis Vamvas",
            "Rico Sennrich"
        ],
        "published": "2023",
        "summary": "Automatically highlighting words that cause semantic differences between two documents could be useful for a wide range of applications. We formulate recognizing semantic differences (RSD) as a token-level regression task and study three unsupervised approaches that rely on a masked language model. To assess the approaches, we begin with basic English sentences and gradually move to more complex, cross-lingual document pairs. Our results show that an approach based on word alignment and sentence-level contrastive learning has a robust correlation to gold labels. However, all unsupervised approaches still leave a large margin of improvement.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.835.pdf",
        "keywords": [
            "word alignment",
            "token level regression"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, all unsupervised approaches still leave a large margin of improvement.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, all unsupervised approaches still leave a large margin of improvement.\""
    },
    {
        "title": "SEER : A Knapsack approach to Exemplar Selection for In-Context HybridQA",
        "authors": [
            "Jonathan Tonglet",
            "Manon Reusens",
            "Philipp Borchert",
            "Bart Baesens"
        ],
        "published": "2023",
        "summary": "Question answering over hybrid contexts is a complex task, which requires the combination of information extracted from unstructured texts and structured tables in various ways. Recently, In-Context Learning demonstrated significant performance advances for reasoning tasks. In this paradigm, a large language model performs predictions based on a small set of supporting exemplars. The performance of In-Context Learning depends heavily on the selection procedure of the supporting exemplars, particularly in the case of HybridQA, where considering the diversity of reasoning chains and the large size of the hybrid contexts becomes crucial. In this work, we present Selection of ExEmplars for hybrid Reasoning (SEER), a novel method for selecting a set of exemplars that is both representative and diverse. The key novelty of SEER is that it formulates exemplar selection as a Knapsack Integer Linear Program. The Knapsack framework provides the flexibility to incorporate diversity constraints that prioritize exemplars with desirable attributes, and capacity constraints that ensure that the prompt size respects the provided capacity budgets. The effectiveness of SEER is demonstrated on FinQA and TAT-QA, two real-world benchmarks for HybridQA, where it outperforms previous exemplar selection methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.837.pdf",
        "keywords": [
            "selection",
            "exemplar selection",
            "hybrid contexts",
            "hybrid reasoning",
            "knapsack",
            "question answering"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The performance of In-Context Learning depends heavily on the selection procedure of the supporting exemplars, particularly in the case of HybridQA, where considering the diversity of reasoning chains and the large size of the hybrid contexts becomes crucial.\"\n\nThis rating is given because the abstract mentions a limitation of In-Context Learning with LLMs in the context of HybridQA, but it does not elaborate",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The performance of In-Context Learning depends heavily on the selection procedure of the supporting exemplars, particularly in the case of HybridQA, where considering the diversity of reasoning chains and the large size of the hybrid contexts becomes crucial.\"\n\nThis rating is given because the abstract mentions a limitation of In-Context Learning with LLMs in the context of HybridQA, but it does not elaborate"
    },
    {
        "title": "Conversation Chronicles: Towards Diverse Temporal and Relational Dynamics in Multi-Session Conversations",
        "authors": [
            "Jihyoung Jang",
            "Minseong Boo",
            "Hyounghun Kim"
        ],
        "published": "2023",
        "summary": "In the field of natural language processing, open-domain chatbots have emerged as an important research topic. However, a major limitation of existing open-domain chatbot research is its singular focus on short single-session dialogue, neglecting the potential need for understanding contextual information in multiple consecutive sessions that precede an ongoing dialogue. Among the elements that compose the context in multi-session conversation settings, the time intervals between sessions and the relationships between speakers would be particularly important. Despite their importance, current research efforts have not sufficiently addressed these dialogical components. In this paper, we introduce a new 1M multi-session dialogue dataset, called Conversation Chronicles, for implementing a long-term conversation setup in which time intervals and fine-grained speaker relationships are incorporated. Following recent works, we exploit a large language model to produce the data. The extensive human evaluation shows that dialogue episodes in Conversation Chronicles reflect those properties while maintaining coherent and consistent interactions across all the sessions. We also propose a dialogue model, called ReBot, which consists of chronological summarization and dialogue generation modules using only around 630M parameters. When trained on Conversation Chronicles, ReBot demonstrates long-term context understanding with a high human engagement score.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.838.pdf",
        "keywords": [
            "dialogue"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"a major limitation of existing open-domain chatbot research is its singular focus on short single-session dialogue, neglecting the potential need for understanding contextual information in multiple consecutive sessions that precede an ongoing dialogue.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"a major limitation of existing open-domain chatbot research is its singular focus on short single-session dialogue, neglecting the potential need for understanding contextual information in multiple consecutive sessions that precede an ongoing dialogue.\""
    },
    {
        "title": "DueT: Image-Text Contrastive Transfer Learning with Dual-adapter Tuning",
        "authors": [
            "Taku Hasegawa",
            "Kyosuke Nishida",
            "Koki Maeda",
            "Kuniko Saito"
        ],
        "published": "2023",
        "summary": "This paper presents DueT, a novel transfer learning method for vision and language models built by contrastive learning. In DueT, adapters are inserted into the image and text encoders, which have been initialized using models pre-trained on uni-modal corpora and then frozen. By training only these adapters, DueT enables efficient learning with a reduced number of trainable parameters. Moreover, unlike traditional adapters, those in DueT are equipped with a gating mechanism, enabling effective transfer and connection of knowledge acquired from pre-trained uni-modal encoders while preventing catastrophic forgetting. We report that DueT outperformed simple fine-tuning, the conventional method fixing only the image encoder and training only the text encoder, and the LoRA-based adapter method in accuracy and parameter efficiency for 0-shot image and text retrieval in both English and Japanese domains.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.839.pdf",
        "keywords": [
            "adapters",
            "transfer learning",
            "language models",
            "dual adapter tuning"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but the use of adapters and a gating mechanism to prevent \"catastrophic forgetting\" implies a limitation in traditional transfer learning methods for LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None explicitly mentioned, but the use of adapters and a gating mechanism to prevent \"catastrophic forgetting\" implies a limitation in traditional transfer learning methods for LLMs."
    },
    {
        "title": "Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation",
        "authors": [
            "Yeongseo Jung",
            "Eunseo Jung",
            "Lei Chen"
        ],
        "published": "2023",
        "summary": "In Conversational Recommendation System (CRS), an agent is asked to recommend a set of items to users within natural language conversations. To address the need for both conversational capability and personalized recommendations, prior works have utilized separate recommendation and dialogue modules. However, such approach inevitably results in a discrepancy between recommendation results and generated responses. To bridge the gap, we propose a multi-task learning for a unified CRS, where a single model jointly learns both tasks via Contextualized Knowledge Distillation (ConKD). We introduce two versions of ConKD: hard gate and soft gate. The former selectively gates between two task-specific teachers, while the latter integrates knowledge from both teachers. Our gates are computed on-the-fly in a context-specific manner, facilitating flexible integration of relevant knowledge. Extensive experiments demonstrate that our single model significantly improves recommendation performance while enhancing fluency, and achieves comparable results in terms of diversity.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.840.pdf",
        "keywords": [
            "multi task learning",
            "contextualized knowledge distillation",
            "personalized",
            "fluency",
            "conversational recommendation system",
            "conversational"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "CLAIR: Evaluating Image Captions with Large Language Models",
        "authors": [
            "David Chan",
            "Suzanne Petryk",
            "Joseph Gonzalez",
            "Trevor Darrell",
            "John Canny"
        ],
        "published": "2023",
        "summary": "The evaluation of machine-generated image captions poses an interesting yet persistent challenge. Effective evaluation measures must consider numerous dimensions of similarity, including semantic relevance, visual structure, object interactions, caption diversity, and specificity. Existing highly-engineered measures attempt to capture specific aspects, but fall short in providing a holistic score that aligns closely with human judgments. Here, we propose CLAIR, a novel method that leverages the zero-shot language modeling capabilities of large language models (LLMs) to evaluate candidate captions. In our evaluations, CLAIR demonstrates a stronger correlation with human judgments of caption quality compared to existing measures. Notably, on Flickr8K-Expert, CLAIR achieves relative correlation improvements over SPICE of 39.6% and over image-augmented methods such as RefCLIP-S of 18.3%. Moreover, CLAIR provides noisily interpretable results by allowing the language model to identify the underlying reasoning behind its assigned score.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.841.pdf",
        "keywords": [
            "captions",
            "language",
            "large language models",
            "generated image captions"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper discusses the capabilities of LLMs but does not mention any limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper discusses the capabilities of LLMs but does not mention any limitations."
    },
    {
        "title": "MoPe: Model Perturbation based Privacy Attacks on Language Models",
        "authors": [
            "Marvin Li",
            "Jason Wang",
            "Jeffrey Wang",
            "Seth Neel"
        ],
        "published": "2023",
        "summary": "Recent work has shown that Large Language Models (LLMs) can unintentionally leak sensitive information present in their training data. In this paper, we present Model Perturbations (MoPe), a new method to identify with high confidence if a given text is in the training data of a pre-trained language model, given white-box access to the models parameters. MoPe adds noise to the model in parameter space and measures the drop in log-likelihood at a given point x, a statistic we show approximates the trace of the Hessian matrix with respect to model parameters. Across language models ranging from 70M to 12B parameters, we show that MoPe is more effective than existing loss-based attacks and recently proposed perturbation-based methods. We also examine the role of training point order and model size in attack success, and empirically demonstrate that MoPe accurately approximate the trace of the Hessian in practice. Our results show that the loss of a point alone is insufficient to determine extractability—there are training points we can recover using our method that have average loss. This casts some doubt on prior works that use the loss of a point as evidence of memorization or unlearning.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.842.pdf",
        "keywords": [
            "language models",
            "privacy attacks",
            "perturbation"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent work has shown that Large Language Models (LLMs) can unintentionally leak sensitive information present in their training data.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Recent work has shown that Large Language Models (LLMs) can unintentionally leak sensitive information present in their training data.\""
    },
    {
        "title": "q2d: Turning Questions into Dialogs to Teach Models How to Search",
        "authors": [
            "Yonatan Bitton",
            "Shlomi Cohen-Ganor",
            "Ido Hakimi",
            "Yoad Lewenberg",
            "Roee Aharoni",
            "Enav Weinreb"
        ],
        "published": "2023",
        "summary": "One of the exciting capabilities of recent language models for dialog is their ability to independently search for relevant information to ground a given dialog response. However, obtaining training data to teach models how to issue search queries is time and resource consuming. In this work, we propose q2d: an automatic data generation pipeline that generates information-seeking dialogs from questions. We prompt a large language model (PaLM) to create conversational versions of question answering datasets, and use it to improve query generation models that communicate with external search APIs to ground dialog responses. Unlike previous approaches which relied on human written dialogs with search queries, our method allows to automatically generate query-based grounded dialogs with better control and scale. Our experiments demonstrate that: (1) For query generation on the QReCC dataset, models trained on our synthetically-generated data achieve 90%-97% of the performance of models trained on the human-generated data; (2) We can successfully generate data for training dialog models in new domains without any existing dialog data as demonstrated on the multi-hop MuSiQue and Bamboogle QA datasets. (3) We perform a thorough analysis of the generated dialogs showing that humans find them of high quality and struggle to distinguish them from human-written dialogs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.843.pdf",
        "keywords": [
            "dialogs",
            "data generation pipeline"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, obtaining training data to teach models how to issue search queries is time and resource consuming.\"\n\nThis rating is given because the paper mentions a limitation of LLMs, but only in passing and as a minor detail to justify the paper's goals and methods. The primary focus of the paper is on the proposed solution, q2d, and its effectiveness in generating information-seeking dialogs",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, obtaining training data to teach models how to issue search queries is time and resource consuming.\"\n\nThis rating is given because the paper mentions a limitation of LLMs, but only in passing and as a minor detail to justify the paper's goals and methods. The primary focus of the paper is on the proposed solution, q2d, and its effectiveness in generating information-seeking dialogs"
    },
    {
        "title": "Aligning Large Language Models through Synthetic Feedback",
        "authors": [
            "Sungdong Kim",
            "Sanghwan Bae",
            "Jamin Shin",
            "Soyoung Kang",
            "Donghyun Kwak",
            "Kang Yoo",
            "Minjoon Seo"
        ],
        "published": "2023",
        "summary": "Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs. However, it requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT. In this work, we propose a novel alignment learning framework with synthetic feedback not dependent on extensive human annotations and proprietary LLMs. First, we perform reward modeling (RM) with synthetic feedback by contrasting responses from vanilla LLMs with various sizes and prompts. Then, we use the RM to simulate high-quality demonstrations to train a supervised policy and further optimize the model with reinforcement learning. Our resulting model, Aligned Language Model with Synthetic Training dataset (ALMoST), outperforms recent open-sourced models, which are trained on the outputs of InstructGPT or human-annotated demonstrations, in alignment benchmarks. In human evaluation, our model is preferred to Alpaca and Dolly-v2, 55.0% and 58.5% of the time, respectively. Further analyses demonstrate the efficacy and importance of synthetic feedback in our framework.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.844.pdf",
        "keywords": [
            "feedback",
            "synthetic feedback",
            "language models",
            "aligning large language models",
            "alignment"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, it requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT.\""
    },
    {
        "title": "You Told Me That Joke Twice: A Systematic Investigation of Transferability and Robustness of Humor Detection Models",
        "authors": [
            "Alexander Baranov",
            "Vladimir Kniazhevsky",
            "Pavel Braslavski"
        ],
        "published": "2023",
        "summary": "In this study, we focus on automatic humor detection, a highly relevant task for conversational AI. To date, there are several English datasets for this task, but little research on how models trained on them generalize and behave in the wild. To fill this gap, we carefully analyze existing datasets, train RoBERTa-based and Naïve Bayes classifiers on each of them, and test on the rest. Training and testing on the same dataset yields good results, but the transferability of the models varies widely. Models trained on datasets with jokes from different sources show better transferability, while the amount of training data has a smaller impact. The behavior of the models on out-of-domain data is unstable, suggesting that some of the models overfit, while others learn non-specific humor characteristics. An adversarial attack shows that models trained on pun datasets are less robust. We also evaluate the sense of humor of the chatGPT and Flan-UL2 models in a zero-shot scenario. The LLMs demonstrate competitive results on humor datasets and a more stable behavior on out-of-domain data. We believe that the obtained results will facilitate the development of new datasets and evaluation methodologies in the field of computational humor. We’ve made all the data from the study and the trained models publicly available at https://github.com/Humor-Research/Humor-detection.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.845.pdf",
        "keywords": [
            "humor",
            "transferability",
            "automatic humor detection",
            "humor detection models",
            "computational humor",
            "conversational ai"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The behavior of the models on out-of-domain data is unstable, suggesting that some of the models overfit, while others learn non-specific humor characteristics.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The behavior of the models on out-of-domain data is unstable, suggesting that some of the models overfit, while others learn non-specific humor characteristics.\""
    },
    {
        "title": "Empower Nested Boolean Logic via Self-Supervised Curriculum Learning",
        "authors": [
            "Hongqiu Wu",
            "Linfeng Liu",
            "Hai Zhao",
            "Min Zhang"
        ],
        "published": "2023",
        "summary": "Beyond the great cognitive powers showcased by language models, it is crucial to scrutinize whether their reasoning capabilities stem from strong generalization or merely exposure to relevant data. As opposed to constructing increasingly complex logic, this paper probes into the boolean logic, the root capability of a logical reasoner. We find that any pre-trained language models even including large language models only behave like a random selector in the face of multi-nested boolean logic, a task that humans can handle with ease. To empower language models with this fundamental capability, this paper proposes a new self-supervised learning method Curriculum Logical Reasoning (Clr), where we augment the training data with nested boolean logic chain step-by-step, and program the training from simpler logical patterns gradually to harder ones. This new training paradigm allows language models to effectively generalize to much harder and longer-hop logic, which can hardly be learned through naive training. Furthermore, we show that boolean logic is a great foundation for improving the subsequent general logical tasks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.847.pdf",
        "keywords": [
            "self supervised curriculum",
            "language",
            "curriculum logical reasoning",
            "logical reasoner",
            "boolean logic",
            "language models",
            "nested boolean logic",
            "self supervised learning",
            "root capability"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we find that any pre-trained language models even including large language models only behave like a random selector in the face of multi-nested boolean logic, a task that humans can handle with ease.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"we find that any pre-trained language models even including large language models only behave like a random selector in the face of multi-nested boolean logic, a task that humans can handle with ease.\""
    },
    {
        "title": "DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules",
        "authors": [
            "Yanchen Liu",
            "William Held",
            "Diyi Yang"
        ],
        "published": "2023",
        "summary": "Existing large language models (LLMs) that mainly focus on Standard American English (SAE) often lead to significantly worse performance when being applied to other English dialects. While existing mitigations tackle discrepancies for individual target dialects, they assume access to high-accuracy dialect identification systems. The boundaries between dialects are inherently flexible, making it difficult to categorize language into discrete predefined categories. In this paper, we propose DADA (Dialect Adaptation via Dynamic Aggregation), a modular approach to imbue SAE-trained models with multi-dialectal robustness by composing adapters which handle specific linguistic features. The compositional architecture of DADA allows for both targeted adaptation to specific dialect variants and simultaneous adaptation to various dialects. We show that DADA is effective for both single task and instruction finetuned language models, offering an extensible and interpretable framework for adapting existing LLMs to different English dialects.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.850.pdf",
        "keywords": [
            "dialect adaptation",
            "categorize language",
            "aggregation",
            "dynamic aggregation",
            "dialects",
            "linguistic rules"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing large language models (LLMs) that mainly focus on Standard American English (SAE) often lead to significantly worse performance when being applied to other English dialects.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing large language models (LLMs) that mainly focus on Standard American English (SAE) often lead to significantly worse performance when being applied to other English dialects.\""
    },
    {
        "title": "The Benefits of Label-Description Training for Zero-Shot Text Classification",
        "authors": [
            "Lingyu Gao",
            "Debanjan Ghosh",
            "Kevin Gimpel"
        ],
        "published": "2023",
        "summary": "Pretrained language models have improved zero-shot text classification by allowing the transfer of semantic knowledge from the training data in order to classify among specific label sets in downstream tasks. We propose a simple way to further improve zero-shot accuracies with minimal effort. We curate small finetuning datasets intended to describe the labels for a task. Unlike typical finetuning data, which has texts annotated with labels, our data simply describes the labels in language, e.g., using a few related terms, dictionary/encyclopedia entries, and short templates. Across a range of topic and sentiment datasets, our method is more accurate than zero-shot by 17-19% absolute. It is also more robust to choices required for zero-shot classification, such as patterns for prompting the model to classify and mappings from labels to tokens in the model’s vocabulary. Furthermore, since our data merely describes the labels but does not use input texts, finetuning on it yields a model that performs strongly on multiple text domains for a given label set, even improving over few-shot out-of-domain classification in multiple settings.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.853.pdf",
        "keywords": [
            "label description training",
            "shot text classification",
            "classification",
            "finetuning",
            "zero shot text classification"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"It is also more robust to choices required for zero-shot classification, such as patterns for prompting the model to classify and mappings from labels to tokens in the model’s vocabulary.\"\n\nThis evidence suggests that the paper mentions some limitations of LLMs, specifically the sensitivity to prompting patterns and label-token mappings, but it is a minor detail and not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"It is also more robust to choices required for zero-shot classification, such as patterns for prompting the model to classify and mappings from labels to tokens in the model’s vocabulary.\"\n\nThis evidence suggests that the paper mentions some limitations of LLMs, specifically the sensitivity to prompting patterns and label-token mappings, but it is a minor detail and not the primary focus of the paper."
    },
    {
        "title": "Can We Edit Multimodal Large Language Models?",
        "authors": [
            "Siyuan Cheng",
            "Bozhong Tian",
            "Qingbin Liu",
            "Xi Chen",
            "Yongheng Wang",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "published": "2023",
        "summary": "In this paper, we focus on editing multimodal Large Language Models (LLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.856.pdf",
        "keywords": [
            "editing",
            "multimodal",
            "edit multimodal large language",
            "multimodal model editing",
            "multimodal large language models"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process.\"; \"Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process.\"; \"Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task.\""
    },
    {
        "title": "ClusterLLM: Large Language Models as a Guide for Text Clustering",
        "authors": [
            "Yuwei Zhang",
            "Zihan Wang",
            "Jingbo Shang"
        ],
        "published": "2023",
        "summary": "We introduce ClusterLLM, a novel text clustering framework that leverages feedback from an instruction-tuned large language model, such as ChatGPT. Compared with traditional unsupervised methods that builds upon “small” embedders, ClusterLLM exhibits two intriguing advantages: (1) it enjoys the emergent capability of LLM even if its embeddings are inaccessible; and (2) it understands the user’s preference on clustering through textual instruction and/or a few annotated data. First, we prompt ChatGPT for insights on clustering perspective by constructing hard triplet questions <does A better correspond to B than C>, where A, B and C are similar data points that belong to different clusters according to small embedder. We empirically show that this strategy is both effective for fine-tuning small embedder and cost-efficient to query ChatGPT. Second, we prompt ChatGPT for helps on clustering granularity by carefully designed pairwise questions <do A and B belong to the same category>, and tune the granularity from cluster hierarchies that is the most consistent with the ChatGPT answers. Extensive experiments on 14 datasets show that ClusterLLM consistently improves clustering quality, at an average cost of ~$0.6 per dataset.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.858.pdf",
        "keywords": [
            "text clustering",
            "clustering",
            "language model",
            "large language models",
            "instruction tuned"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned in the abstract."
    },
    {
        "title": "Syllogistic Reasoning for Legal Judgment Analysis",
        "authors": [
            "Wentao Deng",
            "Jiahuan Pei",
            "Keyi Kong",
            "Zhe Chen",
            "Furu Wei",
            "Yujun Li",
            "Zhaochun Ren",
            "Zhumin Chen",
            "Pengjie Ren"
        ],
        "published": "2023",
        "summary": "Legal judgment assistants are developing fast due to impressive progress of large language models (LLMs). However, people can hardly trust the results generated by a model without reliable analysis of legal judgement. For legal practitioners, it is common practice to utilize syllogistic reasoning to select and evaluate the arguments of the parties as part of the legal decision-making process. But the development of syllogistic reasoning for legal judgment analysis is hindered by the lack of resources: (1) there is no large-scale syllogistic reasoning dataset for legal judgment analysis, and (2) there is no set of established benchmarks for legal judgment analysis. In this paper, we construct and manually correct a syllogistic reasoning dataset for legal judgment analysis. The dataset contains 11,239 criminal cases which cover 4 criminal elements, 80 charges and 124 articles. We also select a set of large language models as benchmarks, and conduct a in-depth analysis of the capacity of their legal judgment analysis.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.864.pdf",
        "keywords": [
            "legal judgment analysis",
            "language models",
            "syllogistic reasoning",
            "legal decision making",
            "legal judgment assistants"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, people can hardly trust the results generated by a model without reliable analysis of legal judgement.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, people can hardly trust the results generated by a model without reliable analysis of legal judgement.\""
    },
    {
        "title": "KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection",
        "authors": [
            "Sehyun Choi",
            "Tianqing Fang",
            "Zhaowei Wang",
            "Yangqiu Song"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable human-level natural language generation capabilities. However, their potential to generate misinformation, often called the *hallucination* problem, poses a significant risk to their deployment. A common approach to address this issue is to retrieve relevant knowledge and fine-tune the LLM with the knowledge in its input. Unfortunately, this method incurs high training costs and may cause catastrophic forgetting for multi-tasking models. To overcome these limitations, we propose a knowledge-constrained decoding method called KCTS (Knowledge-Constrained Tree Search), which guides a frozen LM to generate text aligned with the reference knowledge at each decoding step using a knowledge classifier score and MCTS (Monte-Carlo Tree Search). To adapt the sequence-level knowledge classifier to token-level guidance, we also propose a novel token-level hallucination detection method called RIPA (Reward Inflection Point Approximation). Our empirical results on knowledge-grounded dialogue and abstractive summarization demonstrate the strength of KCTS as a plug-and-play, model-agnostic decoding method that can effectively reduce hallucinations in natural language generation.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.867.pdf",
        "keywords": [
            "hallucination",
            "token level hallucination",
            "knowledge constrained tree search",
            "knowledge constrained",
            "summarization",
            "kcts"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their potential to generate misinformation, often called the *hallucination* problem, poses a significant risk to their deployment.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, their potential to generate misinformation, often called the *hallucination* problem, poses a significant risk to their deployment.\""
    },
    {
        "title": "CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL",
        "authors": [
            "Mayank Kothyari",
            "Dhruva Dhingra",
            "Sunita Sarawagi",
            "Soumen Chakrabarti"
        ],
        "published": "2023",
        "summary": "Existing Text-to-SQL generators require the entire schema to be encoded with the user text. This is expensive or impractical for large databases with tens of thousands of columns. Standard dense retrieval techniques are inadequate for schema subsetting of a large structured database, where the correct semantics of retrieval demands that we rank sets of schema elements rather than individual documents. In response, we propose a two-stage process for effective coverage during retrieval. First, we use an LLM to hallucinate a minimal DB schema that it deems adequate to answer the query. We use the hallucinated schema to retrieve a subset of the actual schema, by composing the results from multiple dense retrievals. Remarkably, hallucination — generally considered a nuisance — turns out to be actually useful as a bridging mechanism. Since no existing benchmarks exist for schema subsetting on large databases, we introduce two benchmarks: (1) A semi-synthetic dataset of 4502 schema elements, by taking a union of schema on the well-known SPIDER dataset, and (2) A real-life benchmark called SocialDB sourced from an actual large data warehouse comprising of 17844 schema elements. We show that our method leads to significantly higher recall than SOTA retrieval-based augmentation methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.868.pdf",
        "keywords": [
            "hallucination",
            "schema hallucination",
            "text2sql",
            "crush4sql",
            "text to sql",
            "collective retrieval",
            "spider"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing Text-to-SQL generators require the entire schema to be encoded with the user text.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Existing Text-to-SQL generators require the entire schema to be encoded with the user text.\""
    },
    {
        "title": "Incorporating Structured Representations into Pretrained Vision & Language Models Using Scene Graphs",
        "authors": [
            "Roei Herzig",
            "Alon Mendelson",
            "Leonid Karlinsky",
            "Assaf Arbelle",
            "Rogerio Feris",
            "Trevor Darrell",
            "Amir Globerson"
        ],
        "published": "2023",
        "summary": "Vision and language models (VLMs) have demonstrated remarkable zero-shot (ZS) performance in a variety of tasks. However, recent works have shown that even the best VLMs struggle to capture aspects of compositional scene understanding, such as object attributes, relations, and action states. In contrast, obtaining structured annotations, such as scene graphs (SGs), that could improve these models is time-consuming and costly, and thus cannot be used on a large scale. Here we ask whether small SG datasets can provide sufficient information for enhancing structured understanding of pretrained VLMs. We show that it is indeed possible to improve VLMs when learning from SGs by integrating components that incorporate structured information into both visual and textual representations. For the visual side, we incorporate a special “SG Component” in the image transformer trained to predict SG information, while for the textual side, we utilize SGs to generate fine-grained captions that highlight different compositional aspects of the scene. Our method improves the performance of several popular VLMs on multiple VL datasets with only a mild degradation in ZS capabilities.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.870.pdf",
        "keywords": [
            "scene graphs",
            "scene understanding",
            "compositional",
            "structured"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, recent works have shown that even the best VLMs struggle to capture aspects of compositional scene understanding, such as object attributes, relations, and action states.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, recent works have shown that even the best VLMs struggle to capture aspects of compositional scene understanding, such as object attributes, relations, and action states.\""
    },
    {
        "title": "DetGPT: Detect What You Need via Reasoning",
        "authors": [
            "Renjie Pi",
            "Jiahui Gao",
            "Shizhe Diao",
            "Rui Pan",
            "Hanze Dong",
            "Jipeng Zhang",
            "Lewei Yao",
            "Jianhua Han",
            "Hang Xu",
            "Lingpeng Kong",
            "Tong Zhang"
        ],
        "published": "2023",
        "summary": "In recent years, the field of computer vision has seen significant advancements thanks to the development of large language models (LLMs). These models have enabled more effective and sophisticated interactions between humans and machines, paving the way for novel techniques that blur the lines between human and machine intelligence. In this paper, we introduce a new paradigm for object detection that we call reasoning-based object detection. Unlike conventional object detection methods that rely on specific object names, our approach enables users to interact with the system using natural language instructions, allowing for a higher level of interactivity. Our proposed method, called DetGPT, leverages state-of-the-art multi-modal models and open-vocabulary object detectors to perform reasoning within the context of the user’s instructions and the visual scene. This enables DetGPT to automatically locate the object of interest based on the user’s expressed desires, even if the object is not explicitly mentioned. For instance, if a user expresses a desire for a cold beverage, DetGPT can analyze the image, identify a fridge, and use its knowledge of typical fridge contents to locate the beverage. This flexibility makes our system applicable across a wide range of fields, from robotics and automation to autonomous driving. Overall, our proposed paradigm and DetGPT demonstrate the potential for more sophisticated and intuitive interactions between humans and machines. We hope that our proposed paradigm and approach will provide inspiration to the community and open the door to more interactive and versatile object detection systems.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.876.pdf",
        "keywords": [
            "detgpt",
            "object detection"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but mentions \"large language models (LLMs)\" in the context of their advancements and capabilities, without discussing any limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but mentions \"large language models (LLMs)\" in the context of their advancements and capabilities, without discussing any limitations."
    },
    {
        "title": "Language Models with Rationality",
        "authors": [
            "Nora Kassner",
            "Oyvind Tafjord",
            "Ashish Sabharwal",
            "Kyle Richardson",
            "Hinrich Schuetze",
            "Peter Clark"
        ],
        "published": "2023",
        "summary": "While large language models (LLMs) are proficient at question-answering (QA), it is not always clear how (or even if) an answer follows from their latent “beliefs”. This lack of interpretability is a growing impediment to widespread use of LLMs. To address this, our goals are to make model beliefs and their inferential relationships explicit, and to resolve inconsistencies that may exist, so that answers are supported by interpretable chains of reasoning drawn from a consistent network of beliefs. Our approach, which we call REFLEX, is to add a **rational, self-reflecting layer** on top of the LLM. First, given a question, we construct a **belief graph** using a backward-chaining process to materialize relevant model beliefs (including beliefs about answer candidates) and their inferential relationships. Second, we identify and minimize contradictions in that graph using a formal constraint reasoner. We find that REFLEX significantly improves consistency (by 8%-11% absolute) without harming overall answer accuracy, resulting in answers supported by faithful chains of reasoning drawn from a more consistent belief system. This suggests a new style of system architecture in which an LLM extended with a rational layer can provide an interpretable window into system beliefs, add a systematic reasoning capability, and repair latent inconsistencies present in the LLM.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.877.pdf",
        "keywords": [
            "reflex",
            "latent inconsistencies",
            "rationality",
            "language models",
            "question"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This lack of interpretability is a growing impediment to widespread use of LLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"This lack of interpretability is a growing impediment to widespread use of LLMs.\""
    },
    {
        "title": "Mitigating Temporal Misalignment by Discarding Outdated Facts",
        "authors": [
            "Michael Zhang",
            "Eunsol Choi"
        ],
        "published": "2023",
        "summary": "While large language models are able to retain vast amounts of world knowledge seen during pretraining, such knowledge is prone to going out of date and is nontrivial to update. Furthermore, these models are often used under temporal misalignment, tasked with answering questions about the present, despite having only been trained on data collected in the past. To mitigate the effects of temporal misalignment, we propose fact duration prediction: the task of predicting how long a given fact will remain true. In our experiments, we demonstrate that identifying which facts are prone to rapid change can help models avoid reciting outdated information and determine which predictions require seeking out up-to-date knowledge sources. We also show how modeling fact duration improves calibration for knowledge-intensive tasks, such as open-retrieval question answering, under temporal misalignment, by discarding volatile facts.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.879.pdf",
        "keywords": [
            "predicting",
            "temporal misalignment",
            "misalignment",
            "fact duration prediction",
            "facts",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"such knowledge is prone to going out of date and is nontrivial to update\"; \"these models are often used under temporal misalignment, tasked with answering questions about the present, despite having only been trained on data collected in the past.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"such knowledge is prone to going out of date and is nontrivial to update\"; \"these models are often used under temporal misalignment, tasked with answering questions about the present, despite having only been trained on data collected in the past.\""
    },
    {
        "title": "IEKG: A Commonsense Knowledge Graph for Idiomatic Expressions",
        "authors": [
            "Ziheng Zeng",
            "Kellen Cheng",
            "Srihari Nanniyur",
            "Jianing Zhou",
            "Suma Bhat"
        ],
        "published": "2023",
        "summary": "Idiomatic expression (IE) processing and comprehension have challenged pre-trained language models (PTLMs) because their meanings are non-compositional. Unlike prior works that enable IE comprehension through fine-tuning PTLMs with sentences containing IEs, in this work, we construct IEKG, a commonsense knowledge graph for figurative interpretations of IEs. This extends the established ATOMIC2020 converting PTLMs into knowledge models (KMs) that encode and infer commonsense knowledge related to IE use. Experiments show that various PTLMs can be converted into KMs with IEKG. We verify the quality of IEKG and the ability of the trained KMs with automatic and human evaluation. Through applications in natural language understanding, we show that a PTLM injected with knowledge from IEKG exhibits improved IE comprehension ability and can generalize to IEs unseen during training.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.881.pdf",
        "keywords": [
            "commonsense",
            "idiomatic expressions",
            "iekg",
            "commonsense knowledge graph",
            "knowledge models",
            "language models",
            "natural language",
            "ie comprehension"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Idiomatic expression (IE) processing and comprehension have challenged pre-trained language models (PTLMs) because their meanings are non-compositional.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Idiomatic expression (IE) processing and comprehension have challenged pre-trained language models (PTLMs) because their meanings are non-compositional.\""
    },
    {
        "title": "Bias Neutralization in Non-Parallel Texts: A Cyclic Approach with Auxiliary Guidance",
        "authors": [
            "Karthic Madanagopal",
            "James Caverlee"
        ],
        "published": "2023",
        "summary": "Objectivity is a goal for Wikipedia and many news sites, as well as a guiding principle of many large language models. Indeed, several methods have recently been developed for automatic subjective bias neutralization. These methods, however, typically rely on parallel text for training (i.e. a biased sentence coupled with a non-biased sentence), demonstrate poor transfer to new domains, and can lose important bias-independent context. Toward expanding the reach of bias neutralization, we propose in this paper a new approach called FairBalance. Three of its unique features are: i) a cycle consistent adversarial network enables bias neutralization without the need for parallel text; ii) the model design preserves bias-independent content; and iii) through auxiliary guidance, the model highlights sequences of bias-inducing words, yielding strong results in terms of bias neutralization quality. Extensive experiments demonstrate how FairBalance significantly improves subjective bias neutralization compared to other methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.882.pdf",
        "keywords": [
            "bias",
            "subjective bias",
            "parallel text",
            "wikipedia",
            "non parallel texts",
            "bias neutralization",
            "fairbalance",
            "guidance"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Indeed, several methods have recently been developed for automatic subjective bias neutralization... however, typically rely on parallel text for training... and can lose important bias-independent context.\"\n\nThis abstract mentions a limitation of existing bias neutralization methods, which are likely used in LLMs, but does not focus on LLMs specifically and does not elaborate on the limitation in detail.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Indeed, several methods have recently been developed for automatic subjective bias neutralization... however, typically rely on parallel text for training... and can lose important bias-independent context.\"\n\nThis abstract mentions a limitation of existing bias neutralization methods, which are likely used in LLMs, but does not focus on LLMs specifically and does not elaborate on the limitation in detail."
    },
    {
        "title": "Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation",
        "authors": [
            "Jason Lucas",
            "Adaku Uchendu",
            "Michiharu Yamashita",
            "Jooyoung Lee",
            "Shaurya Rohatgi",
            "Dongwon Lee"
        ],
        "published": "2023",
        "summary": "Recent ubiquity and disruptive impacts of large language models (LLMs) have raised concerns about their potential to be misused (*.i.e, generating large-scale harmful and misleading content*). To combat this emerging risk of LLMs, we propose a novel “***Fighting Fire with Fire***” (F3) strategy that harnesses modern LLMs’ generative and emergent reasoning capabilities to counter human-written and LLM-generated disinformation. First, we leverage GPT-3.5-turbo to synthesize authentic and deceptive LLM-generated content through paraphrase-based and perturbation-based prefix-style prompts, respectively. Second, we apply zero-shot in-context semantic reasoning techniques with cloze-style prompts to discern genuine from deceptive posts and news articles. In our extensive experiments, we observe GPT-3.5-turbo’s zero-shot superiority for both in-distribution and out-of-distribution datasets, where GPT-3.5-turbo consistently achieved accuracy at 68-72%, unlike the decline observed in previous customized and fine-tuned disinformation detectors. Our codebase and dataset are available at https://github.com/mickeymst/F3.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.883.pdf",
        "keywords": [
            "codebase",
            "disinformation",
            "context semantic reasoning",
            "emergent reasoning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent ubiquity and disruptive impacts of large language models (LLMs) have raised concerns about their potential to be misused (*.i.e, generating large-scale harmful and misleading content*).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Recent ubiquity and disruptive impacts of large language models (LLMs) have raised concerns about their potential to be misused (*.i.e, generating large-scale harmful and misleading content*).\""
    },
    {
        "title": "SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts",
        "authors": [
            "Joon-Young Choi",
            "Junho Kim",
            "Jun-Hyung Park",
            "Wing-Lam Mok",
            "SangKeun Lee"
        ],
        "published": "2023",
        "summary": "Prompt tuning has emerged as a successful parameter-efficient alternative to the full fine-tuning of language models. However, prior works on prompt tuning often utilize long soft prompts of up to 100 tokens to improve performance, overlooking the inefficiency associated with extended inputs. In this paper, we propose a novel prompt tuning method SMoP (Sparse Mixture-of-Prompts) that utilizes short soft prompts for efficient training and inference while maintaining performance gains typically induced from longer soft prompts. To achieve this, SMoP employs a gating mechanism to train multiple short soft prompts specialized in handling different subsets of the data, providing an alternative to relying on a single long soft prompt to cover the entire data. Experimental results demonstrate that SMoP outperforms baseline methods while reducing training and inference costs. We release our code at https://github.com/jyjohnchoi/SMoP.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.884.pdf",
        "keywords": [
            "smop",
            "prompt tuning",
            "soft prompts",
            "short soft prompts",
            "sparse mixture of"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, prior works on prompt tuning often utilize long soft prompts of up to 100 tokens to improve performance, overlooking the inefficiency associated with extended inputs.\"\n\n(Note: The paper mentions a limitation of prior works on prompt tuning, which is related to LLMs, but it's not a limitation of LLMs themselves. However, it's still relevant to the topic, so",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, prior works on prompt tuning often utilize long soft prompts of up to 100 tokens to improve performance, overlooking the inefficiency associated with extended inputs.\"\n\n(Note: The paper mentions a limitation of prior works on prompt tuning, which is related to LLMs, but it's not a limitation of LLMs themselves. However, it's still relevant to the topic, so"
    },
    {
        "title": "BRAINTEASER: Lateral Thinking Puzzles for Large Language Models",
        "authors": [
            "Yifan Jiang",
            "Filip Ilievski",
            "Kaixin Ma",
            "Zhivar Sourati"
        ],
        "published": "2023",
        "summary": "The success of language models has inspired the NLP community to attend to tasks that require implicit and complex reasoning, relying on human-like commonsense mechanisms. While such vertical thinking tasks have been relatively popular, lateral thinking puzzles have received little attention. To bridge this gap, we devise BrainTeaser: a multiple-choice Question Answering task designed to test the model’s ability to exhibit lateral thinking and defy default commonsense associations. We design a three-step procedure for creating the first lateral thinking benchmark, consisting of data collection, distractor generation, and generation of adversarial examples, leading to 1,100 puzzles with high-quality annotations. To assess the consistency of lateral reasoning by models, we enrich BrainTeaser based on a semantic and contextual reconstruction of its questions. Our experiments with state-of-the-art instruction- and commonsense language models reveal a significant gap between human and model performance, which is further widened when consistency across adversarial formats is considered. We make all of our code and data available to stimulate work on developing and evaluating lateral thinking models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.885.pdf",
        "keywords": [
            "puzzles",
            "lateral thinking puzzles",
            "language models",
            "lateral thinking models",
            "commonsense language models",
            "commonsense",
            "lateral reasoning"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our experiments with state-of-the-art instruction- and commonsense language models reveal a significant gap between human and model performance, which is further widened when consistency across adversarial formats is considered.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Our experiments with state-of-the-art instruction- and commonsense language models reveal a significant gap between human and model performance, which is further widened when consistency across adversarial formats is considered.\""
    },
    {
        "title": "When are Lemons Purple? The Concept Association Bias of Vision-Language Models",
        "authors": [
            "Yingtian Tang",
            "Yutaro Yamada",
            "Yoyo Zhang",
            "Ilker Yildirim"
        ],
        "published": "2023",
        "summary": "Large-scale vision-language models such as CLIP have shown impressive performance on zero-shot image classification and image-to-text retrieval. However, such performance does not realize in tasks that require a finer-grained correspondence between vision and language, such as Visual Question Answering (VQA). We investigate why this is the case, and report an interesting phenomenon of vision-language models, which we call the Concept Association Bias (CAB), as a potential cause of the difficulty of applying these models to VQA and similar tasks. We find that models with CAB tend to treat input as a bag of concepts and attempt to fill in the other missing concept crossmodally, leading to an unexpected zero-shot prediction. We demonstrate CAB by showing that CLIP’s zero-shot classification performance greatly suffers when there is a strong concept association between an object (e.g. eggplant) and an attribute (e.g. color purple). We also show that the strength of CAB predicts the performance on VQA. We observe that CAB is prevalent in vision-language models trained with contrastive losses, even when autoregressive losses are jointly employed. However, a model that solely relies on autoregressive loss seems to exhibit minimal or no signs of CAB.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.886.pdf",
        "keywords": [
            "lemons",
            "concept association bias",
            "vision language models",
            "shot prediction",
            "to",
            "retrieval"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, such performance does not realize in tasks that require a finer-grained correspondence between vision and language, such as Visual Question Answering (VQA).\"; \"We find that models with CAB tend to treat input as a bag of concepts and attempt to fill in the other missing concept crossmodally, leading to an unexpected zero-shot prediction.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, such performance does not realize in tasks that require a finer-grained correspondence between vision and language, such as Visual Question Answering (VQA).\"; \"We find that models with CAB tend to treat input as a bag of concepts and attempt to fill in the other missing concept crossmodally, leading to an unexpected zero-shot prediction.\""
    },
    {
        "title": "Text Representation Distillation via Information Bottleneck Principle",
        "authors": [
            "Yanzhao Zhang",
            "Dingkun Long",
            "Zehan Li",
            "Pengjun Xie"
        ],
        "published": "2023",
        "summary": "Pre-trained language models (PLMs) have recently shown great success in text representation field. However, the high computational cost and high-dimensional representation of PLMs pose significant challenges for practical applications. To make models more accessible, an effective method is to distill large models into smaller representation models. In order to relieve the issue of performance degradation after distillation, we propose a novel Knowledge Distillation method called IBKD. This approach is motivated by the Information Bottleneck principle and aims to maximize the mutual information between the final representation of the teacher and student model, while simultaneously reducing the mutual information between the student model’s representation and the input data. This enables the student model to preserve important learned information while avoiding unnecessary information, thus reducing the risk of over-fitting. Empirical studies on two main downstream applications of text representation (Semantic Textual Similarity and Dense Retrieval tasks) demonstrate the effectiveness of our proposed approach.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.888.pdf",
        "keywords": [
            "information bottleneck principle",
            "text representation distillation",
            "pre trained language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the high computational cost and high-dimensional representation of PLMs pose significant challenges for practical applications.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the high computational cost and high-dimensional representation of PLMs pose significant challenges for practical applications.\""
    },
    {
        "title": "Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation",
        "authors": [
            "Zhenwen Liang",
            "Wenhao Yu",
            "Tanmay Rajpurohit",
            "Peter Clark",
            "Xiangliang Zhang",
            "Ashwin Kalyan"
        ],
        "published": "2023",
        "summary": "In this paper, we present a novel approach for distilling math word problem solving capabilities from large language models (LLMs) into smaller, more efficient student models. Our approach is designed to consider the student model’s weaknesses and foster a tailored learning experience by generating targeted exercises aligned with educational science principles, such as knowledge tracing and personalized learning. Concretely, we let GPT-3 be a math tutor and run two steps iteratively: 1) assessing the student model’s current learning status on a GPT-generated exercise book, and 2) improving the student model by training it with tailored exercise samples generated by GPT-3. Experimental results reveal that our approach outperforms LLMs (e.g., GPT-3 and PaLM) in accuracy across three distinct benchmarks while employing significantly fewer parameters. Furthermore, we provide a comprehensive analysis of the various components within our methodology to substantiate their efficacy.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.889.pdf",
        "keywords": [],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experimental results reveal that our approach outperforms LLMs (e.g., GPT-3 and PaLM) in accuracy across three distinct benchmarks while employing significantly fewer parameters.\"\n\nThis paper talks about LLMs and mentions a limitation (large size and parameters) in passing, but does not elaborate on it.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Experimental results reveal that our approach outperforms LLMs (e.g., GPT-3 and PaLM) in accuracy across three distinct benchmarks while employing significantly fewer parameters.\"\n\nThis paper talks about LLMs and mentions a limitation (large size and parameters) in passing, but does not elaborate on it."
    },
    {
        "title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
        "authors": [
            "Hyunwoo Kim",
            "Melanie Sclar",
            "Xuhui Zhou",
            "Ronan Bras",
            "Gunhee Kim",
            "Yejin Choi",
            "Maarten Sap"
        ],
        "published": "2023",
        "summary": "Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.890.pdf",
        "keywords": [
            "theory of mind",
            "stress test",
            "testing models",
            "benchmark",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.\""
    },
    {
        "title": "Exploring the Boundaries of GPT-4 in Radiology",
        "authors": [
            "Qianchu Liu",
            "Stephanie Hyland",
            "Shruthi Bannur",
            "Kenza Bouzid",
            "Daniel Castro",
            "Maria Wetscherek",
            "Robert Tinn",
            "Harshita Sharma",
            "Fernando Pérez-García",
            "Anton Schwaighofer",
            "Pranav Rajpurkar",
            "Sameer Khanna",
            "Hoifung Poon",
            "Naoto Usuyama",
            "Anja Thieme",
            "Aditya Nori",
            "Matthew Lungren",
            "Ozan Oktay",
            "Javier Alvarez-Valle"
        ],
        "published": "2023",
        "summary": "The recent success of general-domain large language models (LLMs) has significantly changed the natural language processing paradigm towards a unified foundation model across domains and applications. In this paper, we focus on assessing the performance of GPT-4, the most capable LLM so far, on the text-based applications for radiology reports, comparing against state-of-the-art (SOTA) radiology-specific models. Exploring various prompting strategies, we evaluated GPT-4 on a diverse range of common radiology tasks and we found GPT-4 either outperforms or is on par with current SOTA radiology models. With zero-shot prompting, GPT-4 already obtains substantial gains (≈ 10% absolute improvement) over radiology models in temporal sentence similarity classification (accuracy) and natural language inference (F1). For tasks that require learning dataset-specific style or schema (e.g. findings summarisation), GPT-4 improves with example-based prompting and matches supervised SOTA. Our extensive error analysis with a board-certified radiologist shows GPT-4 has a sufficient level of radiology knowledge with only occasional errors in complex context that require nuanced domain knowledge. For findings summarisation, GPT-4 outputs are found to be overall comparable with existing manually-written impressions.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.891.pdf",
        "keywords": [
            "natural language processing",
            "gpt 4",
            "gpt 4 in radiology",
            "classification"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"For tasks that require learning dataset-specific style or schema (e.g. findings summarisation), GPT-4 improves with example-based prompting and matches supervised SOTA. Our extensive error analysis with a board-certified radiologist shows GPT-4 has a sufficient level of radiology knowledge with only occasional errors in complex context that require nuanced domain knowledge.\"\n\nThis rating is given because the",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"For tasks that require learning dataset-specific style or schema (e.g. findings summarisation), GPT-4 improves with example-based prompting and matches supervised SOTA. Our extensive error analysis with a board-certified radiologist shows GPT-4 has a sufficient level of radiology knowledge with only occasional errors in complex context that require nuanced domain knowledge.\"\n\nThis rating is given because the"
    },
    {
        "title": "A Frustratingly Easy Post-Training Quantization Scheme for LLMs",
        "authors": [
            "Yongkweon Jeon",
            "Chungman Lee",
            "Kyungphil Park",
            "Ho-young Kim"
        ],
        "published": "2023",
        "summary": "Efficient inference has become crucial for hyper-scale AI models, including large language models, as their parameter count continues to increase for enhanced performance. This necessity holds true regardless of the computing environment, whether it be mobile devices or cloud servers. Quantization emerges as a solution to alleviate the computational burden during inference. By representing models with a reduced bit-width, quantization minimizes the frequency of DRAM access while fully exploiting the parallelism of operations through a dense matrix format. Consequently, quantized models achieve low end-to-end latency and optimize resource utilization by addressing both memory and computing bottlenecks. In this paper, we propose a straightforward post-training quantization scheme, called Z-Fold, that fully utilizes the feature of the Transformer structure widely employed in large language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.892.pdf",
        "keywords": [
            "quantization"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Efficient inference has become crucial for hyper-scale AI models, including large language models, as their parameter count continues to increase for enhanced performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Efficient inference has become crucial for hyper-scale AI models, including large language models, as their parameter count continues to increase for enhanced performance.\""
    },
    {
        "title": "FreeAL: Towards Human-Free Active Learning in the Era of Large Language Models",
        "authors": [
            "Ruixuan Xiao",
            "Yiwen Dong",
            "Junbo Zhao",
            "Runze Wu",
            "Minmin Lin",
            "Gang Chen",
            "Haobo Wang"
        ],
        "published": "2023",
        "summary": "Collecting high-quality labeled data for model training is notoriously time-consuming and labor-intensive for various NLP tasks. While copious solutions, such as active learning for small language models (SLMs) and prevalent in-context learning in the era of large language models (LLMs), have been proposed and alleviate the labeling burden to some extent, their performances are still subject to human intervention. It is still underexplored how to reduce the annotation cost in the LLMs era. To bridge this, we revolutionize traditional active learning and propose an innovative collaborative learning framework FreeAL to interactively distill and filter the task-specific knowledge from LLMs. During collaborative training, an LLM serves as an active annotator inculcating its coarse-grained knowledge, while a downstream SLM is incurred as a student to filter out high-quality in-context samples to feedback LLM for the subsequent label refinery. Extensive experiments on eight benchmark datasets demonstrate that FreeAL largely enhances the zero-shot performances for both SLM and LLM without any human supervision.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.896.pdf",
        "keywords": [
            "active learning",
            "free active learning",
            "freeal",
            "language models",
            "large language models",
            "collaborative learning framework freeal",
            "era"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Collecting high-quality labeled data for model training is notoriously time-consuming and labor-intensive for various NLP tasks... their performances are still subject to human intervention.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Collecting high-quality labeled data for model training is notoriously time-consuming and labor-intensive for various NLP tasks... their performances are still subject to human intervention.\""
    },
    {
        "title": "API-Assisted Code Generation for Question Answering on Varied Table Structures",
        "authors": [
            "Yihan Cao",
            "Shuyi Chen",
            "Ryan Liu",
            "Zhiruo Wang",
            "Daniel Fried"
        ],
        "published": "2023",
        "summary": "A persistent challenge to table question answering (TableQA) by generating executable programs has been adapting to varied table structures, typically requiring domain-specific logical forms. In response, this paper introduces a unified TableQA framework that: (1) provides a unified representation for structured tables as multi-index Pandas data frames, (2) uses Python as a powerful querying language, and (3) uses few-shot prompting to translate NL questions into Python programs, which are executable on Pandas data frames. Furthermore, to answer complex relational questions with extended program functionality and external knowledge, our framework allows customized APIs that Python programs can call. We experiment with four TableQA datasets that involve tables of different structures — relational, multi-table, and hierarchical matrix shapes — and achieve prominent improvements over past state-of-the-art systems. In ablation studies, we (1) show benefits from our multi-index representation and APIs over baselines that use only an LLM, and (2) demonstrate that our approach is modular and can incorporate additional APIs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.897.pdf",
        "keywords": [
            "question answering",
            "table question answering",
            "code generation",
            "apis",
            "multi index representation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"in ablation studies, we (1) show benefits from our multi-index representation and APIs over baselines that use only an LLM\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"in ablation studies, we (1) show benefits from our multi-index representation and APIs over baselines that use only an LLM\""
    },
    {
        "title": "Outlier Dimensions Encode Task Specific Knowledge",
        "authors": [
            "William Rudman",
            "Catherine Chen",
            "Carsten Eickhoff"
        ],
        "published": "2023",
        "summary": "Representations from large language models (LLMs) are known to be dominated by a small subset of dimensions with exceedingly high variance. Previous works have argued that although ablating these outlier dimensions in LLM representations hurts downstream performance, outlier dimensions are detrimental to the representational quality of embeddings. In this study, we investigate how fine-tuning impacts outlier dimensions and show that 1) outlier dimensions that occur in pre-training persist in fine-tuned models and 2) a single outlier dimension can complete downstream tasks with a minimal error rate. Our results suggest that outlier dimensions can encode crucial task-specific knowledge and that the value of a representation in a single outlier dimension drives downstream model decisions.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.901.pdf",
        "keywords": [
            "dominated",
            "outlier",
            "outlier dimensions",
            "language"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Previous works have argued that although ablating these outlier dimensions in LLM representations hurts downstream performance, outlier dimensions are detrimental to the representational quality of embeddings.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Previous works have argued that although ablating these outlier dimensions in LLM representations hurts downstream performance, outlier dimensions are detrimental to the representational quality of embeddings.\""
    },
    {
        "title": "Hi-ArG: Exploring the Integration of Hierarchical Argumentation Graphs in Language Pretraining",
        "authors": [
            "Jingcong Liang",
            "Rong Ye",
            "Meng Han",
            "Qi Zhang",
            "Ruofei Lai",
            "Xinyu Zhang",
            "Zhao Cao",
            "Xuanjing Huang",
            "Zhongyu Wei"
        ],
        "published": "2023",
        "summary": "The knowledge graph is a structure to store and represent knowledge, and recent studies have discussed its capability to assist language models for various applications. Some variations of knowledge graphs aim to record arguments and their relations for computational argumentation tasks. However, many must simplify semantic types to fit specific schemas, thus losing flexibility and expression ability. In this paper, we propose the **Hi**erarchical **Ar**gumentation **G**raph (Hi-ArG), a new structure to organize arguments. We also introduce two approaches to exploit Hi-ArG, including a text-graph multi-modal model GreaseArG and a new pre-training framework augmented with graph information. Experiments on two argumentation tasks have shown that after further pre-training and fine-tuning, GreaseArG supersedes same-scale language models on these tasks, while incorporating graph information during further pre-training can also improve the performance of vanilla language models. Code for this paper is available at <https://github.com/ljcleo/Hi-ArG>.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.902.pdf",
        "keywords": [
            "knowledge graph",
            "text graph",
            "argumentation graphs",
            "argumentation",
            "language pretraining",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, many must simplify semantic types to fit specific schemas, thus losing flexibility and expression ability.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, many must simplify semantic types to fit specific schemas, thus losing flexibility and expression ability.\""
    },
    {
        "title": "Merging Experts into One: Improving Computational Efficiency of Mixture of Experts",
        "authors": [
            "Shwai He",
            "Run-Ze Fan",
            "Liang Ding",
            "Li Shen",
            "Tianyi Zhou",
            "Dacheng Tao"
        ],
        "published": "2023",
        "summary": "Scaling the size of language models usually leads to remarkable advancements in NLP tasks. But it often comes with a price of growing computational cost. Although a sparse Mixture of Experts (MoE) can reduce the cost by activating a small subset of parameters (e.g., one expert) for each input, its computation escalates significantly if increasing the number of activated experts, limiting its practical utility. Can we retain the advantages of adding more experts without substantially increasing the computational costs? In this paper, we first demonstrate the superiority of selecting multiple experts and then propose a computation-efficient approach called Merging Experts into One (MEO), which reduces the computation cost to that of a single expert. Extensive experiments show that MEO significantly improves computational efficiency, e.g., FLOPS drops from 72.0G of vanilla MoE to 28.6G (MEO). Moreover, we propose a token-level attention block that further enhances the efficiency and performance of token-level MEO, e.g., 83.3% (MEO) vs. 82.6% (vanilla MoE) average score on the GLUE benchmark. Our code will be released upon acceptance. Code will be released at: https://github.com/Shwai-He/MEO.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.907.pdf",
        "keywords": [
            "computational efficiency",
            "mixture",
            "mixture of experts",
            "merging experts"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although a sparse Mixture of Experts (MoE) can reduce the cost by activating a small subset of parameters (e.g., one expert) for each input, its computation escalates significantly if increasing the number of activated experts, limiting its practical utility.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although a sparse Mixture of Experts (MoE) can reduce the cost by activating a small subset of parameters (e.g., one expert) for each input, its computation escalates significantly if increasing the number of activated experts, limiting its practical utility.\""
    },
    {
        "title": "Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization",
        "authors": [
            "Janghwan Lee",
            "Minsoo Kim",
            "Seungcheol Baek",
            "Seok Hwang",
            "Wonyong Sung",
            "Jungwook Choi"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) are proficient in natural language processing tasks, but their deployment is often restricted by extensive parameter sizes and computational demands. This paper focuses on post-training quantization (PTQ) in LLMs, specifically 4-bit weight and 8-bit activation (W4A8) quantization, to enhance computational efficiency—a topic less explored compared to weight-only quantization. We present two innovative techniques: activation-quantization-aware scaling (AQAS) and sequence-length-aware calibration (SLAC) to enhance PTQ by considering the combined effects on weights and activations and aligning calibration sequence lengths to target tasks. Moreover, we introduce dINT, a hybrid data format combining integer and denormal representations, to address the underflow issue in W4A8 quantization, where small values are rounded to zero. Through rigorous evaluations of LLMs, including OPT and LLaMA, we demonstrate that our techniques significantly boost task accuracies to levels comparable with full-precision models. By developing arithmetic units compatible with dINT, we further confirm that our methods yield a 2× hardware efficiency improvement compared to 8-bit integer MAC unit.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.910.pdf",
        "keywords": [
            "activation quantization",
            "activation quantization aware scaling",
            "post training quantization",
            "sequence length aware calibration",
            "natural language processing",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large Language Models (LLMs) are proficient in natural language processing tasks, but their deployment is often restricted by extensive parameter sizes and computational demands.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large Language Models (LLMs) are proficient in natural language processing tasks, but their deployment is often restricted by extensive parameter sizes and computational demands.\""
    },
    {
        "title": "Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on Syllogism",
        "authors": [
            "Mengyu Ye",
            "Tatsuki Kuribayashi",
            "Jun Suzuki",
            "Goro Kobayashi",
            "Hiroaki Funayama"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) take advantage of step-by-step reasoning instructions, e.g., chain-of-thought (CoT) prompting. Building on this, their ability to perform CoT-style reasoning robustly is of interest from a probing perspective. In this study, we inspect the step-by-step reasoning ability of LLMs with a focus on negation, which is a core linguistic phenomenon that is difficult to process. In particular, we introduce several controlled settings (e.g., reasoning in case of fictional entities) to evaluate the logical reasoning abilities of the models. We observed that dozens of modern LLMs were not robust against lexical negation (e.g., plausible→implausible) when performing CoT-style reasoning, and the results highlight unique limitations in each LLM family.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.912.pdf",
        "keywords": [
            "negation",
            "lexical negation",
            "reasoning",
            "syllogism",
            "cot style reasoning",
            "logical reasoning"
        ],
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We observed that dozens of modern LLMs were not robust against lexical negation (e.g., plausible→implausible) when performing CoT-style reasoning, and the results highlight unique limitations in each LLM family.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"We observed that dozens of modern LLMs were not robust against lexical negation (e.g., plausible→implausible) when performing CoT-style reasoning, and the results highlight unique limitations in each LLM family.\""
    },
    {
        "title": "Chain-of-Thought Tuning: Masked Language Models can also Think Step By Step in Natural Language Understanding",
        "authors": [
            "Caoyun Fan",
            "Jidong Tian",
            "Yitian Li",
            "Wenqing Chen",
            "Hao He",
            "Yaohui Jin"
        ],
        "published": "2023",
        "summary": "Chain-of-Thought (CoT) is a technique that guides Large Language Models (LLMs) to decompose complex tasks into multi-step reasoning through intermediate steps in natural language form. Briefly, CoT enables LLMs to think step by step. However, although many Natural Language Understanding (NLU) tasks also require thinking step by step, LLMs perform less well than small-scale Masked Language Models (MLMs). To migrate CoT from LLMs to MLMs, we propose Chain-of-Thought Tuning (CoTT), a two-step reasoning framework based on prompt tuning, to implement step-by-step thinking for MLMs on NLU tasks. From the perspective of CoT, CoTT’s two-step framework enables MLMs to implement task decomposition; CoTT’s prompt tuning allows intermediate steps to be used in natural language form. Thereby, the success of CoT can be extended to NLU tasks through MLMs. To verify the effectiveness of CoTT, we conduct experiments on two NLU tasks: hierarchical classification and relation extraction, and the results show that CoTT outperforms baselines and achieves state-of-the-art performance.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.913.pdf",
        "keywords": [
            "thought tuning",
            "natural language understanding",
            "chain of thought tuning",
            "chain of thought",
            "masked",
            "relation extraction"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, although many Natural Language Understanding (NLU) tasks also require thinking step by step, LLMs perform less well than small-scale Masked Language Models (MLMs).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, although many Natural Language Understanding (NLU) tasks also require thinking step by step, LLMs perform less well than small-scale Masked Language Models (MLMs).\""
    },
    {
        "title": "Large Language Models are Complex Table Parsers",
        "authors": [
            "Bowen Zhao",
            "Changkai Ji",
            "Yuejie Zhang",
            "Wen He",
            "Yingwen Wang",
            "Qing Wang",
            "Rui Feng",
            "Xiaobo Zhang"
        ],
        "published": "2023",
        "summary": "With the Generative Pre-trained Transformer 3.5 (GPT-3.5) exhibiting remarkable reasoning and comprehension abilities in Natural Language Processing (NLP), most Question Answering (QA) research has primarily centered around general QA tasks based on GPT, neglecting the specific challenges posed by Complex Table QA. In this paper, we propose to incorporate GPT-3.5 to address such challenges, in which complex tables are reconstructed into tuples and specific prompt designs are employed for dialogues. Specifically, we encode each cell’s hierarchical structure, position information, and content as a tuple. By enhancing the prompt template with an explanatory description of the meaning of each tuple and the logical reasoning process of the task, we effectively improve the hierarchical structure awareness capability of GPT-3.5 to better parse the complex tables. Extensive experiments and results on Complex Table QA datasets, i.e., the open-domain dataset HiTAB and the aviation domain dataset AIT-QA show that our approach significantly outperforms previous work on both datasets, leading to state-of-the-art (SOTA) performance.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.914.pdf",
        "keywords": [
            "table parsers",
            "large language models",
            "generative"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the abstract mentions \"specific challenges posed by Complex Table QA\" which might imply some limitations, however it is not explicitly stated.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the abstract mentions \"specific challenges posed by Complex Table QA\" which might imply some limitations, however it is not explicitly stated."
    },
    {
        "title": "R2H: Building Multimodal Navigation Helpers that Respond to Help Requests",
        "authors": [
            "Yue Fan",
            "Jing Gu",
            "Kaizhi Zheng",
            "Xin Wang"
        ],
        "published": "2023",
        "summary": "Intelligent navigation-helper agents are critical as they can navigate users in unknown areas through environmental awareness and conversational ability, serving as potential accessibility tools for individuals with disabilities. In this work, we first introduce a novel benchmark, Respond to Help Requests (R2H), to promote the development of multi-modal navigation helpers capable of responding to requests for help, utilizing existing dialog-based embodied datasets. R2H mainly includes two tasks: (1) Respond to Dialog History (RDH), which assesses the helper agent’s ability to generate informative responses based on a given dialog history, and (2) Respond during Interaction (RdI), which evaluates the effectiveness and efficiency of the response during consistent cooperation with a task performer. Furthermore, we explore two approaches to construct the navigation-helper agent, including fine-tuning a novel task-oriented multi-modal response generation model that can see and respond, named SeeRee, and employing a multi-modal large language model in a zero-shot manner. Analysis of the task and method was conducted based on both automatic benchmarking and human evaluations.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.915.pdf",
        "keywords": [
            "helper agent",
            "helpers",
            "navigation helper agents",
            "multimodal navigation helpers",
            "automatic benchmarking"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"employing a multi-modal large language model in a zero-shot manner.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"employing a multi-modal large language model in a zero-shot manner.\""
    },
    {
        "title": "Speech-enriched Memory for Inference-time Adaptation of ASR Models to Word Dictionaries",
        "authors": [
            "Ashish Mittal",
            "Sunita Sarawagi",
            "Preethi Jyothi",
            "George Saon",
            "Gakuto Kurata"
        ],
        "published": "2023",
        "summary": "Despite the impressive performance of ASR models on mainstream benchmarks, their performance on rare words is unsatisfactory. In enterprise settings, often a focused list of entities (such as locations, names, etc) are available which can be used to adapt the model to the terminology of specific domains. In this paper, we present a novel inference algorithm that improves the prediction of state-of-the-art ASR models using nearest-neighbor-based matching on an inference-time word list. We consider both the Transducer architecture that is useful in the streaming setting, and state-of-the-art encoder-decoder models such as Whisper. In our approach, a list of rare entities is indexed in a memory by synthesizing speech for each entry, and then storing the internal acoustic and language model states obtained from the best possible alignment on the ASR model. The memory is organized as a trie which we harness to perform a stateful lookup during inference. A key property of our extension is that we prevent spurious matches by restricting to only word-level matches. In our experiments on publicly available datasets and private benchmarks, we show that our method is effective in significantly improving rare word recognition.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.916.pdf",
        "keywords": [
            "asr",
            "speech",
            "inference algorithm",
            "inference time",
            "inference time adaptation",
            "lookup during inference",
            "word recognition"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Learning to Describe for Predicting Zero-shot Drug-Drug Interactions",
        "authors": [
            "Fangqi Zhu",
            "Yongqi Zhang",
            "Lei Chen",
            "Bing Qin",
            "Ruifeng Xu"
        ],
        "published": "2023",
        "summary": "Adverse drug-drug interactions (DDIs) can compromise the effectiveness of concurrent drug administration, posing a significant challenge in healthcare. As the development of new drugs continues, the potential for unknown adverse effects resulting from DDIs becomes a growing concern. Traditional computational methods for DDI prediction may fail to capture interactions for new drugs due to the lack of knowledge. In this paper, we introduce a new problem setup as zero-shot DDI prediction that deals with the case of new drugs. Leveraging textual information from online databases like DrugBank and PubChem, we propose an innovative approach TextDDI with a language model-based DDI predictor and a reinforcement learning (RL)-based information selector, enabling the selection of concise and pertinent text for accurate DDI prediction on new drugs. Empirical results show the benefits of the proposed approach on several settings including zero-shot and few-shot DDI prediction, and the selected texts are semantically relevant. Our code and data are available at https://github.com/zhufq00/DDIs-Prediction.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.918.pdf",
        "keywords": [
            "reinforcement learning",
            "ddi",
            "zero shot ddi prediction"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Leveraging textual information from online databases like DrugBank and PubChem, we propose an innovative approach TextDDI with a language model-based DDI predictor\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Leveraging textual information from online databases like DrugBank and PubChem, we propose an innovative approach TextDDI with a language model-based DDI predictor\""
    },
    {
        "title": "A Simple Baseline for Knowledge-Based Visual Question Answering",
        "authors": [
            "Alexandros Xenos",
            "Themos Stafylakis",
            "Ioannis Patras",
            "Georgios Tzimiropoulos"
        ],
        "published": "2023",
        "summary": "This paper is on the problem of Knowledge-Based Visual Question Answering (KB-VQA). Recent works have emphasized the significance of incorporating both explicit (through external databases) and implicit (through LLMs) knowledge to answer questions requiring external knowledge effectively. A common limitation of such approaches is that they consist of relatively complicated pipelines and often heavily rely on accessing GPT-3 API. Our main contribution in this paper is to propose a much simpler and readily reproducible pipeline which, in a nutshell, is based on efficient in-context learning by prompting LLaMA (1 and 2) using question-informative captions as contextual information. Contrary to recent approaches, our method is training-free, does not require access to external databases or APIs, and yet achieves state-of-the-art accuracy on the OK-VQA and A-OK-VQA datasets. Finally, we perform several ablation studies to understand important aspects of our method. Our code is publicly available at https://github.com/alexandrosXe/ASimple-Baseline-For-Knowledge-Based-VQA",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.919.pdf",
        "keywords": [
            "knowledge",
            "knowledge based"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A common limitation of such approaches is that they consist of relatively complicated pipelines and often heavily rely on accessing GPT-3 API.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"A common limitation of such approaches is that they consist of relatively complicated pipelines and often heavily rely on accessing GPT-3 API.\""
    },
    {
        "title": "Unveiling the Essence of Poetry: Introducing a Comprehensive Dataset and Benchmark for Poem Summarization",
        "authors": [
            "Ridwan Mahbub",
            "Ifrad Khan",
            "Samiha Anuva",
            "Md Shihab Shahriar",
            "Md Tahmid Rahman Laskar",
            "Sabbir Ahmed"
        ],
        "published": "2023",
        "summary": "While research in natural language processing has progressed significantly in creative language generation, the question of whether language models can interpret the intended meaning of creative language largely remains unanswered. Poetry as a creative art form has existed for generations, and summarization of such content requires deciphering the figurative patterns to find out the actual intent and message of the poet. This task can provide the researchers an opportunity to evaluate the creative language interpretation capacity of the language models. Unlike typical text, summarization of poems is a challenging task as poems carry a deeper meaning, which can be easily lost if only the literal meaning is considered. That being said, we propose a new task in the field of natural language understanding called ‘Poem Summarization’. As a starting, we propose the first-ever dataset for this task, named ‘PoemSum’, consisting of 3011 samples of poetry and its corresponding summarized interpretation in the English language. We have benchmarked the performance of different state-of-the-art summarization models and provided observations on their limitations. The dataset and all relevant code used in this work have been made publicly available.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.920.pdf",
        "keywords": [
            "language",
            "poem summarization",
            "summarization",
            "poetry",
            "creative language",
            "natural language understanding",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We have benchmarked the performance of different state-of-the-art summarization models and provided observations on their limitations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We have benchmarked the performance of different state-of-the-art summarization models and provided observations on their limitations.\""
    },
    {
        "title": "Privacy Implications of Retrieval-Based Language Models",
        "authors": [
            "Yangsibo Huang",
            "Samyak Gupta",
            "Zexuan Zhong",
            "Kai Li",
            "Danqi Chen"
        ],
        "published": "2023",
        "summary": "Retrieval-based language models (LMs) have demonstrated improved interpretability, factuality, and adaptability compared to their parametric counterparts by incorporating retrieved text from external datastores. While it is well known that parametric models are prone to leaking private data, it remains unclear how the addition of a retrieval datastore impacts model privacy. In this work, we present the first study of privacy risks in retrieval-based LMs, particularly kNN-LMs. Our goal is to explore the optimal design and training procedure in domains where privacy is of concern, aiming to strike a balance between utility and privacy. Crucially, we find that kNN-LMs are more susceptible to leaking private information from their private datastore than parametric models. We further explore mitigations of privacy risks: When privacy information is targeted and readily detected in the text, we find that a simple sanitization step would eliminate the risks while decoupling query and key encoders achieves an even better utility-privacy trade-off. Otherwise, we consider strategies of mixing public and private data in both datastore and encoder training. While these methods offer modest improvements, they leave considerable room for future work. Together, our findings provide insights for practitioners to better understand and mitigate privacy risks in retrieval-based LMs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.921.pdf",
        "keywords": [
            "language models",
            "retrieval",
            "privacy",
            "retrieval based language models",
            "privacy risks",
            "utility privacy",
            "parametric"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Crucially, we find that kNN-LMs are more susceptible to leaking private information from their private datastore than parametric models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Crucially, we find that kNN-LMs are more susceptible to leaking private information from their private datastore than parametric models.\""
    },
    {
        "title": "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents",
        "authors": [
            "Weiwei Sun",
            "Lingyong Yan",
            "Xinyu Ma",
            "Shuaiqiang Wang",
            "Pengjie Ren",
            "Zhumin Chen",
            "Dawei Yin",
            "Zhaochun Ren"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable zero-shot generalization across various language-related tasks, including search engines. However, existing work utilizes the generative ability of LLMs for Information Retrieval (IR) rather than direct passage ranking. The discrepancy between the pre-training objectives of LLMs and the ranking objective poses another challenge. In this paper, we first investigate generative LLMs such as ChatGPT and GPT-4 for relevance ranking in IR. Surprisingly, our experiments reveal that properly instructed LLMs can deliver competitive, even superior results to state-of-the-art supervised methods on popular IR benchmarks. Furthermore, to address concerns about data contamination of LLMs, we collect a new test set called NovelEval, based on the latest knowledge and aiming to verify the model’s ability to rank unknown knowledge. Finally, to improve efficiency in real-world applications, we delve into the potential for distilling the ranking capabilities of ChatGPT into small specialized models using a permutation distillation scheme. Our evaluation results turn out that a distilled 440M model outperforms a 3B supervised model on the BEIR benchmark. The code to reproduce our results is available at www.github.com/sunnweiwei/RankGPT.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.923.pdf",
        "keywords": [
            "language models",
            "large language models",
            "search",
            "ranking",
            "permutation distillation",
            "chatgpt good",
            "re ranking agents"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The discrepancy between the pre-training objectives of LLMs and the ranking objective poses another challenge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The discrepancy between the pre-training objectives of LLMs and the ranking objective poses another challenge.\""
    },
    {
        "title": "DiNeR: A Large Realistic Dataset for Evaluating Compositional Generalization",
        "authors": [
            "Chengang Hu",
            "Xiao Liu",
            "Yansong Feng"
        ],
        "published": "2023",
        "summary": "Most of the existing compositional generalization datasets are synthetically-generated, resulting in a lack of natural language variation. While there have been recent attempts to introduce non-synthetic datasets for compositional generalization, they suffer from either limited data scale or a lack of diversity in the forms of combinations. To better investigate compositional generalization with more linguistic phenomena and compositional diversity, we propose the DIsh NamE Recognition (DiNeR) task and create a large realistic Chinese dataset. Given a recipe instruction, models are required to recognize the dish name composed of diverse combinations of food, actions, and flavors. Our dataset consists of 3,811 dishes and 228,114 recipes, and involves plenty of linguistic phenomena such as anaphora, omission and ambiguity. We provide two strong baselines based on T5 and large language models (LLMs). This work contributes a challenging task, baseline methods to tackle the task, and insights into compositional generalization in the context of dish name recognition.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.924.pdf",
        "keywords": [
            "diner",
            "compositional diversity",
            "compositional generalization"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We provide two strong baselines based on T5 and large language models (LLMs).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"We provide two strong baselines based on T5 and large language models (LLMs).\""
    },
    {
        "title": "Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?",
        "authors": [
            "Yang Chen",
            "Hexiang Hu",
            "Yi Luan",
            "Haitian Sun",
            "Soravit Changpinyo",
            "Alan Ritter",
            "Ming-Wei Chang"
        ],
        "published": "2023",
        "summary": "Pre-trained vision and language models have demonstrated state-of-the-art capabilities over existing tasks involving images and texts, including visual question answering. However, it remains unclear whether these models possess the capability to answer questions that are not only querying visual content but knowledge-intensive and information-seeking. In this study, we introduce InfoSeek, a visual question answering dataset tailored for information-seeking questions that cannot be answered with only common sense knowledge. Using InfoSeek, we analyze various pre-trained visual question answering models and gain insights into their characteristics. Our findings reveal that state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, InstructBLIP) face challenges in answering visual information-seeking questions, but fine-tuning on the InfoSeek dataset elicits models to use fine-grained knowledge that was learned during pre-training. Furthermore, we show that accurate visual entity recognition can be used to improve performance on InfoSeek by retrieving relevant documents, showing a significant space for improvement.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.925.pdf",
        "keywords": [
            "language models",
            "infoseek"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our findings reveal that state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, InstructBLIP) face challenges in answering visual information-seeking questions,\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Our findings reveal that state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, InstructBLIP) face challenges in answering visual information-seeking questions,\""
    },
    {
        "title": "It Ain’t Over: A Multi-aspect Diverse Math Word Problem Dataset",
        "authors": [
            "Jiwoo Kim",
            "Youngbin Kim",
            "Ilwoong Baek",
            "JinYeong Bak",
            "Jongwuk Lee"
        ],
        "published": "2023",
        "summary": "The math word problem (MWP) is a complex task that requires natural language understanding and logical reasoning to extract key knowledge from natural language narratives. Previous studies have provided various MWP datasets but lack diversity in problem types, lexical usage patterns, languages, and annotations for intermediate solutions. To address these limitations, we introduce a new MWP dataset, named DMath (Diverse Math Word Problems), offering a wide range of diversity in problem types, lexical usage patterns, languages, and intermediate solutions. The problems are available in English and Korean and include an expression tree and Python code as intermediate solutions. Through extensive experiments, we demonstrate that the DMath dataset provides a new opportunity to evaluate the capability of large language models, i.e., GPT-4 only achieves about 75% accuracy on the DMath dataset.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.927.pdf",
        "keywords": [
            "math word problem",
            "mwp",
            "diverse math word"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Through extensive experiments, we demonstrate that the DMath dataset provides a new opportunity to evaluate the capability of large language models, i.e., GPT-4 only achieves about 75% accuracy on the DMath dataset.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Through extensive experiments, we demonstrate that the DMath dataset provides a new opportunity to evaluate the capability of large language models, i.e., GPT-4 only achieves about 75% accuracy on the DMath dataset.\""
    },
    {
        "title": "Dr ChatGPT tell me what I want to hear: How different prompts impact health answer correctness",
        "authors": [
            "Bevan Koopman",
            "Guido Zuccon"
        ],
        "published": "2023",
        "summary": "This paper investigates the significant impact different prompts have on the behaviour of ChatGPT when used for health information seeking. As people more and more depend on generative large language models (LLMs) like ChatGPT, it is critical to understand model behaviour under different conditions, especially for domains where incorrect answers can have serious consequences such as health. Using the TREC Misinformation dataset, we empirically evaluate ChatGPT to show not just its effectiveness but reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness. We show this occurs both for retrieve-then-generate pipelines and based on how a user phrases their question as well as the question type. This work has important implications for the development of more robust and transparent question-answering systems based on generative large language models. Prompts, raw result files and manual analysis are made publicly available at https://github.com/ielab/drchatgpt-health_prompting.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.928.pdf",
        "keywords": [
            "answer correctness",
            "prompts"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness.\""
    },
    {
        "title": "kNN-LM Does Not Improve Open-ended Text Generation",
        "authors": [
            "Shufan Wang",
            "Yixiao Song",
            "Andrew Drozdov",
            "Aparna Garimella",
            "Varun Manjunatha",
            "Mohit Iyyer"
        ],
        "published": "2023",
        "summary": "In this paper, we study the generation quality of interpolation-based retrieval-augmented language models (LMs). These methods, best exemplified by the kNN-LM, interpolate the LM’s predicted distribution of the next word with a distribution formed from the most relevant retrievals for a given prefix. While the kNN-LM and related methods yield impressive decreases in perplexity, we discover that they do not exhibit corresponding improvements in open-ended generation quality, as measured by both automatic evaluation metrics (e.g., MAUVE) and human evaluations. Digging deeper, we find that interpolating with a retrieval distribution actually increases perplexity compared to a baseline LM for the majority of tokens in the WikiText-103 test set, even though the overall perplexity is lower due to a smaller number of tokens for which perplexity dramatically decreases after interpolation. However, when decoding a long sequence at inference time, significant improvements on this smaller subset of tokens are washed out by slightly worse predictions on most tokens. Furthermore, we discover that the entropy of the retrieval distribution increases faster than that of the base LM as the generated sequence becomes longer, which indicates that retrieval is less reliable when using model-generated text as queries (i.e., is subject to exposure bias). We hope that our analysis spurs future work on improved decoding algorithms and interpolation strategies for retrieval-augmented language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.929.pdf",
        "keywords": [
            "augmented language models",
            "interpolating",
            "model generated text",
            "text generation",
            "perplexity",
            "generation quality",
            "knn lm",
            "sequence"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While the kNN-LM and related methods yield impressive decreases in perplexity, we discover that they do not exhibit corresponding improvements in open-ended generation quality, as measured by both automatic evaluation metrics (e.g., MAUVE) and human evaluations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"While the kNN-LM and related methods yield impressive decreases in perplexity, we discover that they do not exhibit corresponding improvements in open-ended generation quality, as measured by both automatic evaluation metrics (e.g., MAUVE) and human evaluations.\""
    },
    {
        "title": "Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model",
        "authors": [
            "Zeyu Liu",
            "Tim Dettmers",
            "Xi Lin",
            "Veselin Stoyanov",
            "Xian Li"
        ],
        "published": "2023",
        "summary": "Large and sparse feed-forward layers (S-FFN) such as Mixture-of-Experts (MoE) have proven effective in scaling up Transformers model size for pretraining large language models. By only activating part of the FFN parameters conditioning on input, S-FFN improves generalization performance while keeping training and inference costs (in FLOPs) fixed. In this work, we analyzed two major design choices of S-FFN: the memory block (a.k.a. expert) size and the memory block selection method under a general conceptual framework of sparse neural memory. Using this unified framework, we compare several S-FFN architectures for language modeling and provide insights into their relative efficacy and efficiency. We found a simpler selection method — Avg-K that selects blocks through their mean aggregated hidden states, achieving lower perplexity in language model pretraining compared to existing MoE architectures including Switch Transformer (Fedus et al., 2021) and HashLayer (Roller et al., 2021).",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.930.pdf",
        "keywords": [
            "language model pretraining",
            "language modeling",
            "perplexity",
            "pretraining large language models",
            "sparse neural memory",
            "transformers model"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper discusses the effectiveness of S-FFN in scaling up Transformers for pretraining large language models, but does not mention any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper discusses the effectiveness of S-FFN in scaling up Transformers for pretraining large language models, but does not mention any limitations of LLMs."
    },
    {
        "title": "Exploring the Impact of Model Scaling on Parameter-Efficient Tuning",
        "authors": [
            "Yusheng Su",
            "Chi-Min Chan",
            "Jiali Cheng",
            "Yujia Qin",
            "Yankai Lin",
            "Shengding Hu",
            "Zonghan Yang",
            "Ning Ding",
            "Xingzhi Sun",
            "Guotong Xie",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2023",
        "summary": "Parameter-efficient tuning (PET) methods can effectively drive extremely large pre-trained language models (PLMs) by training only minimal parameters. Different PET methods utilize different manually designed tunable modules. In small PLMs, there are usually noticeable performance differences among PET methods. Nevertheless, as the model scale increases, the performance differences become marginal. Hence, we hypothesize that model scaling mitigates the impact of design differences on PET methods. To investigate this hypothesis, we introduce a more flexible PET method called Arbitrary PET (APET) method. The APET method is compatible with a tunable module, which consists of any number of parameters distributed in arbitrary positions. Then, we utilize it and conduct experiments on 11 NLP tasks across 3 representative PLMs. Our investigations reveal that model scaling (1) mitigates the effects of the positions of tunable parameters on performance, and (2) enables tuning methods to achieve performance comparable to full-parameter fine-tuning by optimizing fewer tunable parameters. Intriguingly, we also observe that tuning methods optimize the similar number of tunable parameters to exceed random guess performance on different tasks. We collectively discuss this phenomenon and the two aforementioned findings from an optimization perspective to understand the underlying mechanisms. These conclusions enhance our understanding of the impact of model scaling on PET and assist in designing more effective and efficient PET methods for PLMs of different scales. The source code can be obtained from this GitHub repository: https://github.com/yushengsu-thu/PET_Scaling.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.931.pdf",
        "keywords": [
            "scales",
            "model scaling",
            "tuning",
            "parameter efficient tuning",
            "parameter fine tuning",
            "tunable parameters",
            "language models",
            "tunable"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations are discussed, but the paper mentions that \"the performance differences become marginal\" as the model scale increases, which could be interpreted as a limitation of model scaling.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: No explicit limitations are discussed, but the paper mentions that \"the performance differences become marginal\" as the model scale increases, which could be interpreted as a limitation of model scaling."
    },
    {
        "title": "Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication",
        "authors": [
            "Zhangyue Yin",
            "Qiushi Sun",
            "Cheng Chang",
            "Qipeng Guo",
            "Junqi Dai",
            "Xuanjing Huang",
            "Xipeng Qiu"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have recently made significant strides in complex reasoning tasks through the Chain-of-Thought technique. Despite this progress, their reasoning is often constrained by their intrinsic understanding, lacking external insights. To address this, we propose Exchange-of-Thought (EoT), a novel framework that enables cross-model communication during problem-solving. Drawing inspiration from network topology, EoT integrates four unique communication paradigms: Memory, Report, Relay, and Debate. This paper delves into the communication dynamics and volume associated with each paradigm. To counterbalance the risks of incorrect reasoning chains, we implement a robust confidence evaluation mechanism within these communications. Our experiments across diverse complex reasoning tasks demonstrate that EoT significantly surpasses established baselines, underscoring the value of external insights in enhancing LLM performance. Furthermore, we show that EoT achieves these superior results in a cost-effective manner, marking a promising advancement for efficient and collaborative AI problem-solving.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.936.pdf",
        "keywords": [
            "exchange of thought",
            "thought",
            "large language model",
            "cross model communication",
            "reasoning",
            "complex reasoning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"their reasoning is often constrained by their intrinsic understanding, lacking external insights.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"their reasoning is often constrained by their intrinsic understanding, lacking external insights.\""
    },
    {
        "title": "Connecting degree and polarity: An artificial language learning study",
        "authors": [
            "Lisa Bylinina",
            "Alexey Tikhonov",
            "Ekaterina Garmash"
        ],
        "published": "2023",
        "summary": "We investigate a new linguistic generalisation in pre-trained language models (taking BERT Devlin et al. 2019 as a case study). We focus on degree modifiers (expressions like slightly, very, rather, extremely) and test the hypothesis that the degree expressed by a modifier (low, medium or high degree) is related to the modifier’s sensitivity to sentence polarity (whether it shows preference for affirmative or negative sentences or neither). To probe this connection, we apply the Artificial Language Learning experimental paradigm from psycholinguistics to a neural language model. Our experimental results suggest that BERT generalizes in line with existing linguistic observations that relate de- gree semantics to polarity sensitivity, including the main one: low degree semantics is associated with preference towards positive polarity.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.938.pdf",
        "keywords": [
            "psycholinguistics",
            "artificial language learning",
            "degree modifiers",
            "connecting degree",
            "neural language model",
            "polarity",
            "linguistic generalisation"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "Prompting with Pseudo-Code Instructions",
        "authors": [
            "Mayank Mishra",
            "Prince Kumar",
            "Riyaz Bhat",
            "Rudra Murthy",
            "Danish Contractor",
            "Srikanth Tamilselvam"
        ],
        "published": "2023",
        "summary": "Prompting with natural language instructions has recently emerged as a popular method of harnessing the capabilities of large language models (LLM). Given the inherent ambiguity present in natural language, it is intuitive to consider the possible advantages of prompting with less ambiguous prompt styles, like pseudo-code. In this paper, we explore if prompting via pseudo-code instructions helps improve the performance of pre-trained language models. We manually create a dataset of pseudo-code prompts for 132 different tasks spanning classification, QA, and generative language tasks, sourced from the Super-NaturalInstructions dataset. Using these prompts along with their counterparts in natural language, we study their performance on two LLM families - BLOOM, CodeGen. Our experiments show that using pseudo-code instructions leads to better results, with an average increase (absolute) of 7-16 points in F1 scores for classification tasks and an improvement (relative) of 12-38% in aggregate ROUGE-L scores across all tasks. We include detailed ablation studies which indicate that code comments, docstrings, and the structural clues encoded in pseudo-code all contribute towards the improvement in performance. To the best of our knowledge, our work is the first to demonstrate how pseudo-code prompts can be helpful in improving the performance of pre-trained LMs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.939.pdf",
        "keywords": [
            "pseudo code",
            "pseudo code prompts",
            "prompting",
            "pseudo code instructions",
            "language",
            "language models",
            "natural language"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Given the inherent ambiguity present in natural language...\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Given the inherent ambiguity present in natural language...\""
    },
    {
        "title": "CRAB: Assessing the Strength of Causal Relationships Between Real-world Events",
        "authors": [
            "Angelika Romanou",
            "Syrielle Montariol",
            "Debjit Paul",
            "Leo Laugier",
            "Karl Aberer",
            "Antoine Bosselut"
        ],
        "published": "2023",
        "summary": "Understanding narratives requires reasoning about the cause-and-effect relationships between events mentioned in the text. While existing foundation models yield impressive results in many NLP tasks requiring reasoning, it is unclear whether they understand the complexity of the underlying network of causal relationships of events in narratives. In this work, we present CRAB, a new Causal Reasoning Assessment Benchmark designed to evaluate causal understanding of events in real-world narratives. CRAB contains fine-grained, contextual causality annotations for ~2.7K pairs of real-world events that describe various newsworthy event timelines (e.g., the acquisition of Twitter by Elon Musk). Using CRAB, we measure the performance of several large language models, demonstrating that most systems achieve poor performance on the task. Motivated by classical causal principles, we also analyze the causal structures of groups of events in CRAB, and find that models perform worse on causal reasoning when events are derived from complex causal structures compared to simple linear causal chains. We make our dataset and code available to the research community.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.940.pdf",
        "keywords": [
            "narratives",
            "events",
            "real world narratives",
            "real world events",
            "causal chains",
            "causal reasoning",
            "causal relationships",
            "complex causal structures",
            "twitter"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Using CRAB, we measure the performance of several large language models, demonstrating that most systems achieve poor performance on the task.\"; \"We... find that models perform worse on causal reasoning when events are derived from complex causal structures compared to simple linear causal chains.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Using CRAB, we measure the performance of several large language models, demonstrating that most systems achieve poor performance on the task.\"; \"We... find that models perform worse on causal reasoning when events are derived from complex causal structures compared to simple linear causal chains.\""
    },
    {
        "title": "NORMSAGE: Multi-Lingual Multi-Cultural Norm Discovery from Conversations On-the-Fly",
        "authors": [
            "Yi Fung",
            "Tuhin Chakrabarty",
            "Hao Guo",
            "Owen Rambow",
            "Smaranda Muresan",
            "Heng Ji"
        ],
        "published": "2023",
        "summary": "Knowledge of norms is needed to understand and reason about acceptable behavior in human communication and interactions across sociocultural scenarios. Most computational research on norms has focused on a single culture, and manually built datasets, from non-conversational settings. We address these limitations by proposing a new framework, NormSage, to automatically extract culture-specific norms from multi-lingual conversations. NormSage uses GPT-3 prompting to 1) extract candidate norms directly from conversations and 2) provide explainable self-verification to ensure correctness and relevance. Comprehensive empirical results show the promise of our approach to extract high-quality culture-aware norms from multi-lingual conversations (English and Chinese), across several quality metrics. Further, our relevance verification can be extended to assess the adherence and violation of any norm with respect to a conversation on-the-fly, along with textual explanation. NormSage achieves an AUC of 94.6% in this grounding setup, with generated explanations matching human-written quality.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.941.pdf",
        "keywords": [
            "norms",
            "normsage",
            "verification",
            "relevance verification",
            "multi cultural norm discovery"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions using GPT-3 prompting, which implies the use of a Large Language Model, but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions using GPT-3 prompting, which implies the use of a Large Language Model, but does not discuss any limitations of LLMs."
    },
    {
        "title": "Task-Adaptive Tokenization: Enhancing Long-Form Text Generation Efficacy in Mental Health and Beyond",
        "authors": [
            "Siyang Liu",
            "Naihao Deng",
            "Sahand Sabour",
            "Yilin Jia",
            "Minlie Huang",
            "Rada Mihalcea"
        ],
        "published": "2023",
        "summary": "We propose task-adaptive tokenization as a way to adapt the generation pipeline to the specifics of a downstream task and enhance long-form generation in mental health. Inspired by insights from cognitive science, our task-adaptive tokenizer samples variable segmentations from multiple outcomes, with sampling probabilities optimized based on task-specific data. We introduce a strategy for building a specialized vocabulary and introduce a vocabulary merging protocol that allows for the integration of task-specific tokens into the pre-trained model’s tokenization step. Through extensive experiments on psychological question-answering tasks in both Chinese and English, we find that our task-adaptive tokenization approach brings a significant improvement in generation performance while using up to 60% fewer tokens. Preliminary experiments point to promising results when using our tokenization approach with very large language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.944.pdf",
        "keywords": [
            "tokenization",
            "s tokenization",
            "adaptive tokenization",
            "task adaptive tokenization",
            "mental health"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Preliminary experiments point to promising results when using our tokenization approach with very large language models.\"\n\nThis paper mentions large language models but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Preliminary experiments point to promising results when using our tokenization approach with very large language models.\"\n\nThis paper mentions large language models but does not discuss any limitations of LLMs."
    },
    {
        "title": "Specialist or Generalist? Instruction Tuning for Specific NLP Tasks",
        "authors": [
            "Chufan Shi",
            "Yixuan Su",
            "Cheng Yang",
            "Yujiu Yang",
            "Deng Cai"
        ],
        "published": "2023",
        "summary": "The potential of large language models (LLMs) to simultaneously perform a wide range of natural language processing (NLP) tasks has been the subject of extensive research. Although instruction tuning has proven to be a data-efficient method for transforming LLMs into such generalist models, their performance still lags behind specialist models trained exclusively for specific tasks. In this paper, we investigate whether incorporating broadcoverage generalist instruction tuning can contribute to building a specialist model. We hypothesize that its efficacy depends on task specificity and skill requirements. Our experiments assess four target tasks with distinct coverage levels, revealing that integrating generalist instruction tuning consistently enhances model performance when the task coverage is broad. The effect is particularly pronounced when the amount of task-specific training data is limited. Further investigation into three target tasks focusing on different capabilities demonstrates that generalist instruction tuning improves understanding and reasoning abilities. However, for tasks requiring factual knowledge, generalist data containing hallucinatory information may negatively affect the model’s performance. Overall, our work provides a systematic guide for developing specialist models with general instruction tuning.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.947.pdf",
        "keywords": [
            "tuning",
            "instruction tuning",
            "generalist instruction tuning",
            "specialist",
            "specialist model",
            "natural language processing",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although instruction tuning has proven to be a data-efficient method for transforming LLMs into such generalist models, their performance still lags behind specialist models trained exclusively for specific tasks.\" and \"However, for tasks requiring factual knowledge, generalist data containing hallucinatory information may negatively affect the model’s performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Although instruction tuning has proven to be a data-efficient method for transforming LLMs into such generalist models, their performance still lags behind specialist models trained exclusively for specific tasks.\" and \"However, for tasks requiring factual knowledge, generalist data containing hallucinatory information may negatively affect the model’s performance.\""
    },
    {
        "title": "Making Large Language Models Better Data Creators",
        "authors": [
            "Dong-Ho Lee",
            "Jay Pujara",
            "Mohit Sewak",
            "Ryen White",
            "Sujay Jauhar"
        ],
        "published": "2023",
        "summary": "Although large language models (LLMs) have advanced the state-of-the-art in NLP significantly, deploying them for downstream applications is still challenging due to cost, responsiveness, control, or concerns around privacy and security. As such, trainable models are still the preferred option in some cases. However, these models still require human-labeled data for optimal performance, which is expensive and time-consuming to obtain. In order to address this issue, several techniques to reduce human effort involve labeling or generating data using LLMs. Although these methods are effective for certain applications, in practice they encounter difficulties in real-world scenarios. Labeling data requires careful data selection, while generating data necessitates task-specific prompt engineering. In this paper, we propose a unified data creation pipeline that requires only a single formatting example, and which is applicable to a broad range of tasks, including traditionally problematic ones with semantically devoid label spaces. In our experiments we demonstrate that instruction-following LLMs are highly cost-effective data creators, and that models trained with these data exhibit performance better than those trained with human-labeled data (by up to 17.5%) on out-of-distribution evaluation, while maintaining comparable performance on in-distribution tasks. These results have important implications for the robustness of NLP systems deployed in the real-world.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.948.pdf",
        "keywords": [
            "performance",
            "language models",
            "distribution evaluation",
            "large language models",
            "security",
            "label",
            "data creation pipeline",
            "art"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"deploying them for downstream applications is still challenging due to cost, responsiveness, control, or concerns around privacy and security.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"deploying them for downstream applications is still challenging due to cost, responsiveness, control, or concerns around privacy and security.\""
    },
    {
        "title": "Hallucination Detection for Generative Large Language Models by Bayesian Sequential Estimation",
        "authors": [
            "Xiaohua Wang",
            "Yuliang Yan",
            "Longtao Huang",
            "Xiaoqing Zheng",
            "Xuanjing Huang"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have made remarkable advancements in the field of natural language generation. However, the propensity of LLMs to generate inaccurate or non-factual content, termed “hallucinations”, remains a significant challenge. Current hallucination detection methods often necessitate the retrieval of great numbers of relevant evidence, thereby increasing response times. We introduce a unique framework that leverages statistical decision theory and Bayesian sequential analysis to optimize the trade-off between costs and benefits during the hallucination detection process. This approach does not require a predetermined number of observations. Instead, the analysis proceeds in a sequential manner, enabling an expeditious decision towards “belief” or “disbelief” through a stop-or-continue strategy. Extensive experiments reveal that this novel framework surpasses existing methods in both efficiency and precision of hallucination detection. Furthermore, it requires fewer retrieval steps on average, thus decreasing response times.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.949.pdf",
        "keywords": [
            "hallucination detection",
            "bayesian sequential estimation",
            "reveal",
            "large language models",
            "propensity"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the propensity of LLMs to generate inaccurate or non-factual content, termed “hallucinations”, remains a significant challenge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, the propensity of LLMs to generate inaccurate or non-factual content, termed “hallucinations”, remains a significant challenge.\""
    },
    {
        "title": "Guideline Learning for In-Context Information Extraction",
        "authors": [
            "Chaoxu Pang",
            "Yixuan Cao",
            "Qiang Ding",
            "Ping Luo"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) can perform a new task by merely conditioning on task instructions and a few input-output examples, without optimizing any parameters. This is called In-Context Learning (ICL). In-context Information Extraction (IE) has recently garnered attention in the research community. However, the performance of In-context IE generally lags behind the state-of-the-art supervised expert models. We highlight a key reason for this shortfall: underspecified task description. The limited-length context struggles to thoroughly express the intricate IE task instructions and various edge cases, leading to misalignment in task comprehension with humans. In this paper, we propose a Guideline Learning (GL) framework for In-context IE which reflectively learns and follows guidelines. During the learning phrase, GL automatically synthesizes a set of guidelines based on a few error cases, and during inference, GL retrieves helpful guidelines for better ICL. Moreover, we propose a self-consistency-based active learning method to enhance the efficiency of GL. Experiments on event extraction and relation extraction show that GL can significantly improve the performance of in-context IE.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.950.pdf",
        "keywords": [
            "relation extraction",
            "context information extraction",
            "context learning",
            "guideline learning",
            "event extraction",
            "active learning",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the performance of In-context IE generally lags behind the state-of-the-art supervised expert models... The limited-length context struggles to thoroughly express the intricate IE task instructions and various edge cases, leading to misalignment in task comprehension with humans.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (underspecified task description, limited-length context,",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the performance of In-context IE generally lags behind the state-of-the-art supervised expert models... The limited-length context struggles to thoroughly express the intricate IE task instructions and various edge cases, leading to misalignment in task comprehension with humans.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (underspecified task description, limited-length context,"
    },
    {
        "title": "CorefPrompt: Prompt-based Event Coreference Resolution by Measuring Event Type and Argument Compatibilities",
        "authors": [
            "Sheng Xu",
            "Peifeng Li",
            "Qiaoming Zhu"
        ],
        "published": "2023",
        "summary": "Event coreference resolution (ECR) aims to group event mentions referring to the same real-world event into clusters. Most previous studies adopt the “encoding first, then scoring” framework, making the coreference judgment rely on event encoding. Furthermore, current methods struggle to leverage human-summarized ECR rules, e.g., coreferential events should have the same event type, to guide the model. To address these two issues, we propose a prompt-based approach, CorefPrompt, to transform ECR into a cloze-style MLM (masked language model) task. This allows for simultaneous event modeling and coreference discrimination within a single template, with a fully shared context. In addition, we introduce two auxiliary prompt tasks, event-type compatibility and argument compatibility, to explicitly demonstrate the reasoning process of ECR, which helps the model make final predictions. Experimental results show that our method CorefPrompt performs well in a state-of-the-art (SOTA) benchmark.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.954.pdf",
        "keywords": [
            "event coreference resolution",
            "argument compatibilities",
            "corefprompt",
            "event type",
            "coreference discrimination",
            "masked language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"current methods struggle to leverage human-summarized ECR rules\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"current methods struggle to leverage human-summarized ECR rules\""
    },
    {
        "title": "Rationale-Enhanced Language Models are Better Continual Relation Learners",
        "authors": [
            "Weimin Xiong",
            "Yifan Song",
            "Peiyi Wang",
            "Sujian Li"
        ],
        "published": "2023",
        "summary": "Continual relation extraction (CRE) aims to solve the problem of catastrophic forgetting when learning a sequence of newly emerging relations. Recent CRE studies have found that catastrophic forgetting arises from the model’s lack of robustness against future analogous relations. To address the issue, we introduce rationale, i.e., the explanations of relation classification results generated by Large Language Models (LLM), into CRE task. Specifically, we design the multi-task rationale tuning strategy to help the model learn current relations robustly. We also conduct contrastive rationale replay to further distinguish analogous relations. Experimental results on two standard benchmarks demonstrate that our method outperforms the state-of-the-art CRE models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.958.pdf",
        "keywords": [
            "rationale",
            "continual relation extraction",
            "relation classification",
            "language models",
            "multi task rationale"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but mentions \"catastrophic forgetting\" which is a limitation, however, it does not directly relate to LLMs, but rather to CRE task.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but mentions \"catastrophic forgetting\" which is a limitation, however, it does not directly relate to LLMs, but rather to CRE task."
    },
    {
        "title": "ScanDL: A Diffusion Model for Generating Synthetic Scanpaths on Texts",
        "authors": [
            "Lena Bolliger",
            "David Reich",
            "Patrick Haller",
            "Deborah Jakobi",
            "Paul Prasse",
            "Lena Jäger"
        ],
        "published": "2023",
        "summary": "Eye movements in reading play a crucial role in psycholinguistic research studying the cognitive mechanisms underlying human language processing. More recently, the tight coupling between eye movements and cognition has also been leveraged for language-related machine learning tasks such as the interpretability, enhancement, and pre-training of language models, as well as the inference of reader- and text-specific properties. However, scarcity of eye movement data and its unavailability at application time poses a major challenge for this line of research. Initially, this problem was tackled by resorting to cognitive models for synthesizing eye movement data. However, for the sole purpose of generating human-like scanpaths, purely data-driven machine-learning-based methods have proven to be more suitable. Following recent advances in adapting diffusion processes to discrete data, we propose ScanDL, a novel discrete sequence-to-sequence diffusion model that generates synthetic scanpaths on texts. By leveraging pre-trained word representations and jointly embedding both the stimulus text and the fixation sequence, our model captures multi-modal interactions between the two inputs. We evaluate ScanDL within- and across-dataset and demonstrate that it significantly outperforms state-of-the-art scanpath generation methods. Finally, we provide an extensive psycholinguistic analysis that underlines the model’s ability to exhibit human-like reading behavior. Our implementation is made available at https://github.com/DiLi-Lab/ScanDL.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.960.pdf",
        "keywords": [
            "scanpaths",
            "synthetic scanpaths",
            "sequence diffusion model",
            "cognitive models"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs or their limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs or their limitations."
    },
    {
        "title": "From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models",
        "authors": [
            "Dongjun Kang",
            "Joonsuk Park",
            "Yohan Jo",
            "JinYeong Bak"
        ],
        "published": "2023",
        "summary": "Being able to predict people’s opinions on issues and behaviors in realistic scenarios can be helpful in various domains, such as politics and marketing. However, conducting large-scale surveys like the European Social Survey to solicit people’s opinions on individual issues can incur prohibitive costs. Leveraging prior research showing influence of core human values on individual decisions and actions, we propose to use value-injected large language models (LLM) to predict opinions and behaviors. To this end, we present Value Injection Method (VIM), a collection of two methods—argument generation and question answering—designed to inject targeted value distributions into LLMs via fine-tuning. We then conduct a series of experiments on four tasks to test the effectiveness of VIM and the possibility of using value-injected LLMs to predict opinions and behaviors of people. We find that LLMs value-injected with variations of VIM substantially outperform the baselines. Also, the results suggest that opinions and behaviors can be better predicted using value-injected LLMs than the baseline approaches.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.961.pdf",
        "keywords": [
            "behaviors",
            "value injection"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "Inverse Scaling Can Become U-Shaped",
        "authors": [
            "Jason Wei",
            "Najoung Kim",
            "Yi Tay",
            "Quoc Le"
        ],
        "published": "2023",
        "summary": "Scaling up language models has been empirically shown to improve performance on a wide range of downstream tasks. However, if we were to observe worse performance as a function of scale (inverse scaling) on certain tasks, this would indicate that scaling can also encourage behaviors that are misaligned with human preferences. The Inverse Scaling Prize (McKenzie et al. 2023) identified eleven such inverse scaling tasks, evaluated on models of up to 280B parameters and up to 500 zettaFLOPs of training compute. This paper takes a closer look at these inverse scaling tasks. In this paper, we evaluate models of up to 540B parameters, trained on five times more compute than those evaluated in the Inverse Scaling Prize. With this increased range of model sizes and compute, only four out of the eleven tasks remain inverse scaling. Six tasks exhibit U-shaped scaling, where performance decreases up to a certain size, and then increases again up to the largest model evaluated (the one remaining task displays positive scaling). In addition, 1-shot examples and chain-of-thought can help mitigate undesirable scaling patterns even further. U-shaped scaling suggests that the inverse scaling trend observed in McKenzie et al. (2023) may not continue to hold for larger models, which we attribute to the presence of distractor tasks that only sufficiently large models can avoid.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.963.pdf",
        "keywords": [
            "inverse scaling",
            "scale",
            "scaling up language models"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, if we were to observe worse performance as a function of scale (inverse scaling) on certain tasks, this would indicate that scaling can also encourage behaviors that are misaligned with human preferences.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, if we were to observe worse performance as a function of scale (inverse scaling) on certain tasks, this would indicate that scaling can also encourage behaviors that are misaligned with human preferences.\""
    },
    {
        "title": "MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter",
        "authors": [
            "Zhiyuan Liu",
            "Sihang Li",
            "Yanchen Luo",
            "Hao Fei",
            "Yixin Cao",
            "Kenji Kawaguchi",
            "Xiang Wang",
            "Tat-Seng Chua"
        ],
        "published": "2023",
        "summary": "Language Models (LMs) have demonstrated impressive molecule understanding ability on various 1D text-related tasks. However, they inherently lack 2D graph perception — a critical ability of human professionals in comprehending molecules’ topological structures. To bridge this gap, we propose MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter. MolCA enables an LM (i.e., Galactica) to understand both text- and graph-based molecular contents via the cross-modal projector. Specifically, the cross-modal projector is implemented as a Q-Former to connect a graph encoder’s representation space and an LM’s text space. Further, MolCA employs a uni-modal adapter (i.e., LoRA) for the LM’s efficient adaptation to downstream tasks. Unlike previous studies that couple an LM with a graph encoder via cross-modal contrastive learning, MolCA retains the LM’s ability of open-ended text generation and augments it with 2D graph information. To showcase its effectiveness, we extensively benchmark MolCA on tasks of molecule captioning, IUPAC name prediction, and molecule-text retrieval, on which MolCA significantly outperforms the baselines.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.966.pdf",
        "keywords": [
            "uni modal adapter",
            "s ability",
            "molecular graph language",
            "molca",
            "molecular graph language modeling",
            "cross modal projector",
            "projector"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they inherently lack 2D graph perception — a critical ability of human professionals in comprehending molecules’ topological structures.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, they inherently lack 2D graph perception — a critical ability of human professionals in comprehending molecules’ topological structures.\""
    },
    {
        "title": "Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations",
        "authors": [
            "Wei-Lin Chen",
            "Cheng-Kuang Wu",
            "Yun-Nung Chen",
            "Hsin-Hsi Chen"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have exhibited striking in-context learning (ICL) ability to adapt to target tasks with a few input-output demonstrations. For better ICL, different methods are proposed to select representative demonstrations from existing training corpora. However, such settings are not aligned with real-world practices, as end-users usually query LMs without access to demonstration pools. In this work, we introduce Self-ICL—a simple framework which bootstraps LMs’ intrinsic capabilities to perform zero-shot ICL. Given a test input, Self-ICL first prompts the model to generate pseudo-inputs. Next, the model predicts pseudo-labels for the pseudo-inputs via zero-shot prompting. Finally, we perform ICL for the test input with the pseudo-input-label pairs as demonstrations. Evaluation on 23 BIG-Bench Hard tasks shows Self-ICL outperforms zero-shot baselines on both average accuracy and head-to-head comparison. Moreover, with zero-shot chain-of-thought, Self-ICL achieves results comparable to using real demonstrations. Additionally, we conduct a range of analyses to validate Self-ICL’s effectiveness and provide insights for its behaviors under different settings.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.968.pdf",
        "keywords": [
            "demonstrations",
            "self"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the abstract implies that the authors are addressing the limitation of traditional in-context learning methods that rely on existing demonstration pools, by proposing a self-generated demonstration approach.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the abstract implies that the authors are addressing the limitation of traditional in-context learning methods that rely on existing demonstration pools, by proposing a self-generated demonstration approach."
    },
    {
        "title": "Learning Knowledge-Enhanced Contextual Language Representations for Domain Natural Language Understanding",
        "authors": [
            "Taolin Zhang",
            "Ruyao Xu",
            "Chengyu Wang",
            "Zhongjie Duan",
            "Cen Chen",
            "Minghui Qiu",
            "Dawei Cheng",
            "Xiaofeng He",
            "Weining Qian"
        ],
        "published": "2023",
        "summary": "Knowledge-Enhanced Pre-trained Language Models (KEPLMs) improve the performance of various downstream NLP tasks by injecting knowledge facts from large-scale Knowledge Graphs (KGs). However, existing methods for pre-training KEPLMs with relational triples are difficult to be adapted to close domains due to the lack of sufficient domain graph semantics. In this paper, we propose a Knowledge-enhanced language representation learning framework for various closed domains (KANGAROO) via capturing the implicit graph structure among the entities. Specifically, since the entity coverage rates of closed-domain KGs can be relatively low and may exhibit the global sparsity phenomenon for knowledge injection, we consider not only the shallow relational representations of triples but also the hyperbolic embeddings of deep hierarchical entity-class structures for effective knowledge fusion. Moreover, as two closed-domain entities under the same entity-class often havel locally dense neighbor subgraphs counted by max point biconnected component, we further propose a data augmentation strategy based on contrastive learning over subgraphs to construct hard negative samples of higher quality. It makes the underlying KELPMs better distinguish the semantics of these neighboring entities to further complement the global semantic sparsity. In the experiments, we evaluate KANGAROO over various knowledge-aware and general NLP tasks in both full and few-shot learning settings, outperforming various KEPLM training paradigms performance in closed-domains significantly.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.969.pdf",
        "keywords": [
            "domain natural language",
            "knowledge fusion",
            "closed domains"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing methods for pre-training KEPLMs with relational triples are difficult to be adapted to close domains due to the lack of sufficient domain graph semantics.\"\n\nThis abstract mentions a limitation of KEPLMs (a type of LLM) in adapting to closed domains, but it is not the primary focus of the paper and is used to motivate the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, existing methods for pre-training KEPLMs with relational triples are difficult to be adapted to close domains due to the lack of sufficient domain graph semantics.\"\n\nThis abstract mentions a limitation of KEPLMs (a type of LLM) in adapting to closed domains, but it is not the primary focus of the paper and is used to motivate the proposed solution."
    },
    {
        "title": "MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions",
        "authors": [
            "Zexuan Zhong",
            "Zhengxuan Wu",
            "Christopher Manning",
            "Christopher Potts",
            "Danqi Chen"
        ],
        "published": "2023",
        "summary": "The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. This has recently given rise to a range of techniques for injecting new facts through updating model weights. Current evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the model’s related beliefs. If we edit the UK Prime Minister to now be Rishi Sunak, then we should get a different answer to Who is married to the British Prime Minister? In this work, we present a benchmark MQuAKE (Multi-hop Question Answering for Knowledge Editing) comprising multi-hop questions that assess whether edited models correctly answer questions where the answer should change as an entailed consequence of edited facts. While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions. We thus propose a simple memory-based approach, MeLLo, which stores all edited facts externally while prompting the language model iteratively to generate answers that are consistent with the edited facts. While MQuAKE remains challenging, we show that MeLLo scales well with LLMs (up to 175B) and outperforms previous model editors by a large margin.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.971.pdf",
        "keywords": [
            "knowledge editing",
            "language models",
            "multi",
            "multi hop questions"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions.\""
    },
    {
        "title": "NormDial: A Comparable Bilingual Synthetic Dialog Dataset for Modeling Social Norm Adherence and Violation",
        "authors": [
            "Oliver Li",
            "Mallika Subramanian",
            "Arkadiy Saakyan",
            "Sky CH-Wang",
            "Smaranda Muresan"
        ],
        "published": "2023",
        "summary": "Social norms fundamentally shape interpersonal communication. We present NormDial, a high-quality dyadic dialogue dataset with turn-by-turn annotations of social norm adherences and violations for Chinese and American cultures. Introducing the task of social norm observance detection, our dataset is synthetically generated in both Chinese and English using a human-in-the-loop pipeline by prompting large language models with a small collection of expert-annotated social norms. We show that our generated dialogues are of high quality through human evaluation and further evaluate the performance of existing large language models on this task. Our findings point towards new directions for understanding the nuances of social norms as they manifest in conversational contexts that span across languages and cultures.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.974.pdf",
        "keywords": [
            "social norm",
            "violation",
            "bilingual synthetic dialog",
            "dialogue",
            "language models",
            "expert annotated social norms",
            "synthetically generated"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We show that our generated dialogues are of high quality through human evaluation and further evaluate the performance of existing large language models on this task.\"\n\nThis abstract mentions that the performance of existing large language models is evaluated on the task of social norm observance detection, implying that there may be limitations in their performance, but it does not elaborate on these limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We show that our generated dialogues are of high quality through human evaluation and further evaluate the performance of existing large language models on this task.\"\n\nThis abstract mentions that the performance of existing large language models is evaluated on the task of social norm observance detection, implying that there may be limitations in their performance, but it does not elaborate on these limitations."
    },
    {
        "title": "Global Voices, Local Biases: Socio-Cultural Prejudices across Languages",
        "authors": [
            "Anjishnu Mukherjee",
            "Chahat Raj",
            "Ziwei Zhu",
            "Antonios Anastasopoulos"
        ],
        "published": "2023",
        "summary": "Human biases are ubiquitous but not uniform: disparities exist across linguistic, cultural, and societal borders. As large amounts of recent literature suggest, language models (LMs) trained on human data can reflect and often amplify the effects of these social biases. However, the vast majority of existing studies on bias are heavily skewed towards Western and European languages. In this work, we scale the Word Embedding Association Test (WEAT) to 24 languages, enabling broader studies and yielding interesting findings about LM bias. We additionally enhance this data with culturally relevant information for each language, capturing local contexts on a global scale. Further, to encompass more widely prevalent societal biases, we examine new bias dimensions across toxicity, ableism, and more. Moreover, we delve deeper into the Indian linguistic landscape, conducting a comprehensive regional bias analysis across six prevalent Indian languages. Finally, we highlight the significance of these social biases and the new dimensions through an extensive comparison of embedding methods, reinforcing the need to address them in pursuit of more equitable language models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.981.pdf",
        "keywords": [
            "social biases",
            "word embedding association test"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As large amounts of recent literature suggest, language models (LMs) trained on human data can reflect and often amplify the effects of these social biases.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"As large amounts of recent literature suggest, language models (LMs) trained on human data can reflect and often amplify the effects of these social biases.\""
    },
    {
        "title": "Are Compressed Language Models Less Subgroup Robust?",
        "authors": [
            "Leonidas Gee",
            "Andrea Zugarini",
            "Novi Quadrianto"
        ],
        "published": "2023",
        "summary": "To reduce the inference cost of large language models, model compression is increasingly used to create smaller scalable models. However, little is known about their robustness to minority subgroups defined by the labels and attributes of a dataset. In this paper, we investigate the effects of 18 different compression methods and settings on the subgroup robustness of BERT language models. We show that worst-group performance does not depend on model size alone, but also on the compression method used. Additionally, we find that model compression does not always worsen the performance on minority subgroups. Altogether, our analysis serves to further research into the subgroup robustness of model compression.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.983.pdf",
        "keywords": [
            "subgroup robust",
            "compressed language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, little is known about their robustness to minority subgroups defined by the labels and attributes of a dataset.\"; \"We show that worst-group performance does not depend on model size alone, but also on the compression method used.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, little is known about their robustness to minority subgroups defined by the labels and attributes of a dataset.\"; \"We show that worst-group performance does not depend on model size alone, but also on the compression method used.\""
    },
    {
        "title": "NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models",
        "authors": [
            "Yongchao Chen",
            "Rujul Gandhi",
            "Yang Zhang",
            "Chuchu Fan"
        ],
        "published": "2023",
        "summary": "Temporal Logic (TL) can be used to rigorously specify complex high-level specification for systems in many engineering applications. The translation between natural language (NL) and TL has been under-explored due to the lack of dataset and generalizable model across different application domains. In this paper, we propose an accurate and generalizable transformation framework of English instructions from NL to TL, exploring the use of Large Language Models (LLMs) at multiple stages. Our contributions are twofold. First, we develop a framework to create a dataset of NL-TL pairs combining LLMs and human annotation. We publish a dataset with 23K NL-TL pairs. Then, we finetune T5 models on the lifted versions (i.e., the specific Atomic Propositions (AP) are hidden) of the NL and TL. The enhanced generalizability originates from two aspects: 1) Usage of lifted NL-TL characterizes common logical structures, without constraints of specific domains. 2) Application of LLMs in dataset creation largely enhances corpus richness. We test the generalization of trained models on five varied domains. To achieve full NL-TL transformation, we either combine the lifted model with AP recognition task or do the further finetuning on each specific domain. During the further finetuning, our model achieves higher accuracy (> 95%) using only <10% training data, compared with the baseline sequence to sequence (Seq2Seq) model.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.985.pdf",
        "keywords": [
            "temporal logic",
            "high level specification",
            "temporal",
            "natural language",
            "transforming natural languages",
            "specific atomic propositions",
            "corpus richness",
            "large language"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but the paper aims to address the lack of a generalizable model across different application domains, which implies a limitation of existing LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None explicitly mentioned, but the paper aims to address the lack of a generalizable model across different application domains, which implies a limitation of existing LLMs."
    },
    {
        "title": "Reformulating NLP tasks to Capture Longitudinal Manifestation of Language Disorders in People with Dementia.",
        "authors": [
            "Dimitris Gkoumas",
            "Matthew Purver",
            "Maria Liakata"
        ],
        "published": "2023",
        "summary": "Dementia is associated with language disorders which impede communication. Here, we automatically learn linguistic disorder patterns by making use of a moderately-sized pre-trained language model and forcing it to focus on reformulated natural language processing (NLP) tasks and associated linguistic patterns. Our experiments show that NLP tasks that encapsulate contextual information and enhance the gradient signal with linguistic patterns benefit performance. We then use the probability estimates from the best model to construct digital linguistic markers measuring the overall quality in communication and the intensity of a variety of language disorders. We investigate how the digital markers characterize dementia speech from a longitudinal perspective. We find that our proposed communication marker is able to robustly and reliably characterize the language of people with dementia, outperforming existing linguistic approaches; and shows external validity via significant correlation with clinical markers of behaviour. Finally, our proposed linguistic disorder markers provide useful insights into gradual language impairment associated with disease progression.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.986.pdf",
        "keywords": [
            "longitudinal manifestation",
            "language disorders",
            "language impairment",
            "language model",
            "linguistic disorder",
            "digital linguistic markers",
            "reformulated natural language processing",
            "longitudinal perspective",
            "patterns"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"making use of a moderately-sized pre-trained language model\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"making use of a moderately-sized pre-trained language model\""
    },
    {
        "title": "Elevating Code-mixed Text Handling through Auditory Information of Words",
        "authors": [
            "Mamta Mamta",
            "Zishan Ahmad",
            "Asif Ekbal"
        ],
        "published": "2023",
        "summary": "With the growing popularity of code-mixed data, there is an increasing need for better handling of this type of data, which poses a number of challenges, such as dealing with spelling variations, multiple languages, different scripts, and a lack of resources. Current language models face difficulty in effectively handling code-mixed data as they primarily focus on the semantic representation of words and ignore the auditory phonetic features. This leads to difficulties in handling spelling variations in code-mixed text. In this paper, we propose an effective approach for creating language models for handling code-mixed textual data using auditory information of words from SOUNDEX. Our approach includes a pre-training step based on masked-language-modelling, which includes SOUNDEX representations (SAMLM) and a new method of providing input data to the pre-trained model. Through experimentation on various code-mixed datasets (of different languages) for sentiment, offensive and aggression classification tasks, we establish that our novel language modeling approach (SAMLM) results in improved robustness towards adversarial attacks on code-mixed classification tasks. Additionally, our SAMLM based approach also results in better classification results over the popular baselines for code-mixed tasks. We use the explainability technique, SHAP (SHapley Additive exPlanations) to explain how the auditory features incorporated through SAMLM assist the model to handle the code-mixed text effectively and increase robustness against adversarial attacks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.987.pdf",
        "keywords": [
            "soundex representations",
            "code mixed text handling",
            "language modeling",
            "language",
            "auditory information"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Current language models face difficulty in effectively handling code-mixed data as they primarily focus on the semantic representation of words and ignore the auditory phonetic features.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Current language models face difficulty in effectively handling code-mixed data as they primarily focus on the semantic representation of words and ignore the auditory phonetic features.\""
    },
    {
        "title": "Consistency Analysis of ChatGPT",
        "authors": [
            "Myeongjun Jang",
            "Thomas Lukasiewicz"
        ],
        "published": "2023",
        "summary": "ChatGPT has gained a huge popularity since its introduction. Its positive aspects have been reported through many media platforms, and some analyses even showed that ChatGPT achieved a decent grade in professional exams, adding extra support to the claim that AI can now assist and even replace humans in industrial fields. Others, however, doubt its reliability and trustworthiness. This paper investigates the trustworthiness of ChatGPT and GPT-4 regarding logically consistent behaviour, focusing specifically on semantic consistency and the properties of negation, symmetric, and transitive consistency. Our findings suggest that while both models appear to show an enhanced language understanding and reasoning ability, they still frequently fall short of generating logically consistent predictions. We also ascertain via experiments that prompt designing, few-shot learning and employing larger large language models (LLMs) are unlikely to be the ultimate solution to resolve the inconsistency issue of LLMs.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.991.pdf",
        "keywords": [
            "chatgpt",
            "consistency analysis",
            "logically consistent behaviour",
            "transitive consistency",
            "semantic consistency"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Others, however, doubt its reliability and trustworthiness... Our findings suggest that while both models appear to show an enhanced language understanding and reasoning ability, they still frequently fall short of generating logically consistent predictions... prompt designing, few-shot learning and employing larger large language models (LLMs) are unlikely to be the ultimate solution to resolve the inconsistency issue of LLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Others, however, doubt its reliability and trustworthiness... Our findings suggest that while both models appear to show an enhanced language understanding and reasoning ability, they still frequently fall short of generating logically consistent predictions... prompt designing, few-shot learning and employing larger large language models (LLMs) are unlikely to be the ultimate solution to resolve the inconsistency issue of LLMs.\""
    },
    {
        "title": "Assessing the influence of attractor-verb distance on grammatical agreement in humans and language models",
        "authors": [
            "Christos Zacharopoulos",
            "Théo Desbordes",
            "Mathias Sablé-Meyer"
        ],
        "published": "2023",
        "summary": "Subject-verb agreement in the presence of an attractor noun located between the main noun and the verb elicits complex behavior: judgments of grammaticality are modulated by the grammatical features of the attractor. For example, in the sentence ``The girl near the boys likes climbing'', the attractor (boys) disagrees in grammatical number with the verb (likes), creating a locally implausible transition probability. Here, we parametrically modulate the distance between the attractor and the verb while keeping the length of the sentence equal. We evaluate the performance of both humans and two artificial neural network models: both make more mistakes when the attractor is closer to the verb, but neural networks get close to the chance level while humans are mostly able to overcome the attractor interference. Additionally, we report a linear effect of attractor distance on reaction times. We hypothesize that a possible reason for the proximity effect is the calculation of transition probabilities between adjacent words. Nevertheless, classical models of attraction such as the cue-based model might suffice to explain this phenomenon, thus paving the way for new research. Data and analyses available at https://osf.io/d4g6k",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.998.pdf",
        "keywords": [
            "attractor distance",
            "attractor",
            "attractor verb distance",
            "grammatical agreement",
            "neural networks",
            "artificial neural network",
            "subject verb agreement",
            "grammatical number",
            "attractor interference",
            "grammaticality"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"neural networks get close to the chance level while humans are mostly able to overcome the attractor interference.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"neural networks get close to the chance level while humans are mostly able to overcome the attractor interference.\""
    },
    {
        "title": "ToolWriter: Question Specific Tool Synthesis for Tabular Data",
        "authors": [
            "Carlos Gemmell",
            "Jeff Dalton"
        ],
        "published": "2023",
        "summary": "Tabular question answering (TQA) presents a challenging setting for neural systems by requiring joint reasoning of natural language with large amounts of semi-structured data. Unlike humans who use programmatic tools like filters to transform data before processing, language models in TQA process tables directly, resulting in information loss as table size increases. In this paper we propose ToolWriter to generate query specific programs and detect when to apply them to transform tables and align them with the TQA model’s capabilities. Focusing Toolwriter to generate row-filtering tools improves the state-of-the-art for WikiTableQuestions and WikiSQL with the most performance gained on long tables. By investigating headroom, our work highlights the broader potential for programmatic tools combined with neural components to manipulate large amounts of structured data.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1003.pdf",
        "keywords": [
            "toolwriter",
            "tabular question answering",
            "headroom",
            "tabular data",
            "question specific tool synthesis",
            "language"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unlike humans who use programmatic tools like filters to transform data before processing, language models in TQA process tables directly, resulting in information loss as table size increases.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Unlike humans who use programmatic tools like filters to transform data before processing, language models in TQA process tables directly, resulting in information loss as table size increases.\""
    },
    {
        "title": "CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Low Resource With Contrastive Learning",
        "authors": [
            "Xiaoming Liu",
            "Zhaohan Zhang",
            "Yichen Wang",
            "Hang Pu",
            "Yu Lan",
            "Chao Shen"
        ],
        "published": "2023",
        "summary": "Machine-Generated Text (MGT) detection, a task that discriminates MGT from Human-Written Text (HWT), plays a crucial role in preventing misuse of text generative models, which excel in mimicking human writing style recently. Latest proposed detectors usually take coarse text sequences as input and fine-tune pretrained models with standard cross-entropy loss. However, these methods fail to consider the linguistic structure of texts. Moreover, they lack the ability to handle the low-resource problem which could often happen in practice considering the enormous amount of textual data online. In this paper, we present a coherence-based contrastive learning model named CoCo to detect the possible MGT under low-resource scenario. To exploit the linguistic feature, we encode coherence information in form of graph into text representation. To tackle the challenges of low data resource, we employ a contrastive learning framework and propose an improved contrastive loss for preventing performance degradation brought by simple samples. The experiment results on two public datasets and two self-constructed datasets prove our approach outperforms the state-of-art methods significantly. Also, we surprisingly find that MGTs originated from up-to-date language models could be easier to detect than these from previous models, in our experiments. And we propose some preliminary explanations for this counter-intuitive phenomena. All the codes and datasets are open-sourced.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1005.pdf",
        "keywords": [
            "contrastive learning",
            "coherence",
            "low resource",
            "low data resource",
            "machine generated text"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these methods fail to consider the linguistic structure of texts.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these methods fail to consider the linguistic structure of texts.\""
    },
    {
        "title": "AnyTOD: A Programmable Task-Oriented Dialog System",
        "authors": [
            "Jeffrey Zhao",
            "Yuan Cao",
            "Raghav Gupta",
            "Harrison Lee",
            "Abhinav Rastogi",
            "Mingqiu Wang",
            "Hagen Soltau",
            "Izhak Shafran",
            "Yonghui Wu"
        ],
        "published": "2023",
        "summary": "We propose AnyTOD, an end-to-end, zero-shot task-oriented dialog (TOD) system capable of zero-shot adaptation onto unseen tasks or domains. We view TOD as a program executed by a language model (LM), where program logic and ontology is provided by a designer as a schema. To enable generalization to unseen schemas and programs without prior training, AnyTOD adopts a neuro-symbolic approach. A neural LM keeps track of events that occur during a conversation, and a symbolic program implementing dialog policy is executed to recommend actions AnyTOD should take. This approach drastically reduces data annotation and model training requirements, addressing the enduring challenge of rapidly adapting a TOD system to unseen tasks and domains. We demonstrate state-of-the-art results on STAR, ABCD and SGD benchmarks. We also demonstrate strong zero-shot transfer ability in low-resource settings, such as zero-shot transfer onto MultiWOZ. In addition, we release STARv2, an updated version of the STAR dataset with richer annotations, for benchmarking zero-shot task transfer for end-to-end TOD models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1006.pdf",
        "keywords": [
            "dialog",
            "language model",
            "shot task oriented dialog",
            "shot task",
            "ontology"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"addressing the enduring challenge of rapidly adapting a TOD system to unseen tasks and domains.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (rapid adaptation to unseen tasks and domains) in passing, but it is not the primary focus of the paper. The paper focuses more on the proposed solution (AnyTOD) and its results, rather than",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"addressing the enduring challenge of rapidly adapting a TOD system to unseen tasks and domains.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (rapid adaptation to unseen tasks and domains) in passing, but it is not the primary focus of the paper. The paper focuses more on the proposed solution (AnyTOD) and its results, rather than"
    },
    {
        "title": "Can LMs Generalize to Future Data? An Empirical Analysis on Text Summarization",
        "authors": [
            "Chi Cheang",
            "Hou Chan",
            "Derek Wong",
            "Xuebo Liu",
            "Zhaocong Li",
            "Yanming Sun",
            "Shudong Liu",
            "Lidia Chao"
        ],
        "published": "2023",
        "summary": "Recent pre-trained language models (PLMs) achieve promising results in existing abstractive summarization datasets. However, existing summarization benchmarks overlap in time with the standard pre-training corpora and finetuning datasets. Hence, the strong performance of PLMs may rely on the parametric knowledge that is memorized during pre-training and fine-tuning. Moreover, the knowledge memorized by PLMs may quickly become outdated, which affects the generalization performance of PLMs on future data. In this work, we propose TempoSum, a novel benchmark that contains data samples from 2010 to 2022, to understand the temporal generalization ability of abstractive summarization models. Through extensive human evaluation, we show that parametric knowledge stored in summarization models significantly affects the faithfulness of the generated summaries on future data. Moreover, existing faithfulness enhancement methods cannot reliably improve the faithfulness of summarization models on future data. Finally, we discuss several recommendations to the research community on how to evaluate and improve the temporal generalization capability of text summarization models.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1007.pdf",
        "keywords": [
            "text summarization",
            "temporal generalization",
            "lms generalize"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the knowledge memorized by PLMs may quickly become outdated, which affects the generalization performance of PLMs on future data.\"; \"Through extensive human evaluation, we show that parametric knowledge stored in summarization models significantly affects the faithfulness of the generated summaries on future data.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, the knowledge memorized by PLMs may quickly become outdated, which affects the generalization performance of PLMs on future data.\"; \"Through extensive human evaluation, we show that parametric knowledge stored in summarization models significantly affects the faithfulness of the generated summaries on future data.\""
    },
    {
        "title": "Zero-Shot Multi-Label Topic Inference with Sentence Encoders and LLMs",
        "authors": [
            "Souvika Sarkar",
            "Dongji Feng",
            "Shubhra Kanti Karmaker Santu"
        ],
        "published": "2023",
        "summary": "In this paper, we conducted a comprehensive study with the latest Sentence Encoders and Large Language Models (LLMs) on the challenging task of “definition-wild zero-shot topic inference”, where users define or provide the topics of interest in real-time. Through extensive experimentation on seven diverse data sets, we observed that LLMs, such as ChatGPT-3.5 and PaLM, demonstrated superior generality compared to other LLMs, e.g., BLOOM and GPT-NeoX. Furthermore, Sentence-BERT, a BERT-based classical sentence encoder, outperformed PaLM and achieved performance comparable to ChatGPT-3.5.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1008.pdf",
        "keywords": [
            "encoders",
            "sentence encoders",
            "topic inference"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we observed that LLMs, such as ChatGPT-3.5 and PaLM, demonstrated superior generality compared to other LLMs, e.g., BLOOM and GPT-NeoX.\"\n\nThis rating is given because the abstract mentions a limitation of some LLMs (BLOOM and GPT-NeoX) in comparison to others (Chat",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"we observed that LLMs, such as ChatGPT-3.5 and PaLM, demonstrated superior generality compared to other LLMs, e.g., BLOOM and GPT-NeoX.\"\n\nThis rating is given because the abstract mentions a limitation of some LLMs (BLOOM and GPT-NeoX) in comparison to others (Chat"
    },
    {
        "title": "TaskDiff: A Similarity Metric for Task-Oriented Conversations",
        "authors": [
            "Ankita Bhaumik",
            "Praveen Venkateswaran",
            "Yara Rizk",
            "Vatche Isahagian"
        ],
        "published": "2023",
        "summary": "The popularity of conversational digital assistants has resulted in the availability of large amounts of conversational data which can be utilized for improved user experience and personalized response generation. Building these assistants using popular large language models like ChatGPT also require additional emphasis on prompt engineering and evaluation methods. Textual similarity metrics are a key ingredient for such analysis and evaluations. While many similarity metrics have been proposed in the literature, they have not proven effective for task-oriented conversations as they do not take advantage of unique conversational features. To address this gap, we present TaskDiff, a novel conversational similarity metric that utilizes different dialogue components (utterances, intents, and slots) and their distributions to compute similarity. Extensive experimental evaluation of TaskDiff on a benchmark dataset demonstrates its superior performance and improved robustness over other related approaches.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1009.pdf",
        "keywords": [
            "conversations",
            "conversational similarity metric",
            "similarity metric",
            "conversational digital assistants",
            "task oriented conversations"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Building these assistants using popular large language models like ChatGPT also require additional emphasis on prompt engineering and evaluation methods.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Building these assistants using popular large language models like ChatGPT also require additional emphasis on prompt engineering and evaluation methods.\""
    },
    {
        "title": "Euphemistic Abuse – A New Dataset and Classification Experiments for Implicitly Abusive Language",
        "authors": [
            "Michael Wiegand",
            "Jana Kampfmeier",
            "Elisabeth Eder",
            "Josef Ruppenhofer"
        ],
        "published": "2023",
        "summary": "We address the task of identifying euphemistic abuse (e.g. “You inspire me to fall asleep”) paraphrasing simple explicitly abusive utterances (e.g. “You are boring”). For this task, we introduce a novel dataset that has been created via crowdsourcing. Special attention has been paid to the generation of appropriate negative (non-abusive) data. We report on classification experiments showing that classifiers trained on previous datasets are less capable of detecting such abuse. Best automatic results are obtained by a classifier that augments training data from our new dataset with automatically-generated GPT-3 completions. We also present a classifier that combines a few manually extracted features that exemplify the major linguistic phenomena constituting euphemistic abuse.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1012.pdf",
        "keywords": [
            "implicitly abusive language",
            "crowdsourcing",
            "classification",
            "abuse"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Best automatic results are obtained by a classifier that augments training data from our new dataset with automatically-generated GPT-3 completions.\"\n\nThis evidence is brief and mentions a limitation of the previous datasets, which is indirectly related to the performance of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Best automatic results are obtained by a classifier that augments training data from our new dataset with automatically-generated GPT-3 completions.\"\n\nThis evidence is brief and mentions a limitation of the previous datasets, which is indirectly related to the performance of LLMs."
    },
    {
        "title": "Exploring Distributional Shifts in Large Language Models for Code Analysis",
        "authors": [
            "Shushan Arakelyan",
            "Rocktim Das",
            "Yi Mao",
            "Xiang Ren"
        ],
        "published": "2023",
        "summary": "We systematically study how three large language models with code capabilities - CodeT5, Codex, and ChatGPT - generalize to out-of-domain data. We consider two fundamental applications - code summarization, and code generation. We split data into domains following its natural boundaries - by an organization, by a project, and by a module within the software project. We establish that samples from each new domain present all the models with a significant challenge of distribution shift. We study how established methods adapt models to better generalize to new domains. Our experiments show that while multitask learning alone is a reasonable baseline, combining it with few-shot finetuning on examples retrieved from training data can achieve very strong performance. Moreover, this solution can outperform direct finetuning for very low-data scenarios. Finally, we consider variations of this approach to create a more broadly applicable method to adapt to multiple domains at once. We find that for code generation, a model adapted to multiple domains simultaneously performs on par with those adapted to a single domain.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1013.pdf",
        "keywords": [
            "code analysis",
            "code",
            "code generation",
            "language models",
            "large language models",
            "finetuning",
            "code summarization",
            "distributional shifts",
            "codex",
            "multitask learning"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We establish that samples from each new domain present all the models with a significant challenge of distribution shift.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We establish that samples from each new domain present all the models with a significant challenge of distribution shift.\""
    },
    {
        "title": "ATHENA: Mathematical Reasoning with Thought Expansion",
        "authors": [
            "Jb. Kim",
            "Hazel Kim",
            "Joonghyuk Hahn",
            "Yo-Sub Han"
        ],
        "published": "2023",
        "summary": "Solving math word problems depends on how to articulate the problems, the lens through which models view human linguistic expressions. Real-world settings count on such a method even more due to the diverse practices of the same mathematical operations. Earlier works constrain available thinking processes by limited prediction strategies without considering their significance in acquiring mathematical knowledge. We introduce Attention-based THought Expansion Network Architecture (ATHENA) to tackle the challenges of real-world practices by mimicking human thought expansion mechanisms in the form of neural network propagation. A thought expansion recurrently generates the candidates carrying the thoughts of possible math expressions driven from the previous step and yields reasonable thoughts by selecting the valid pathways to the goal. Our experiments show that ATHENA achieves a new state-of-the-art stage toward the ideal model that is compelling in variant questions even when the informativeness in training examples is restricted.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1014.pdf",
        "keywords": [
            "thought expansion"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "A Benchmark for Reasoning with Spatial Prepositions",
        "authors": [
            "Iulia Comsa",
            "Srini Narayanan"
        ],
        "published": "2023",
        "summary": "Spatial reasoning is a fundamental building block of human cognition, used in representing, grounding, and reasoning about physical and abstract concepts. We propose a novel benchmark focused on assessing inferential properties of statements with spatial prepositions. The benchmark includes original datasets in English and Romanian and aims to probe the limits of reasoning about spatial relations in large language models. We use prompt engineering to study the performance of two families of large language models, PaLM and GPT-3, on our benchmark. Our results show considerable variability in the performance of smaller and larger models, as well as across prompts and languages. However, none of the models reaches human performance.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1015.pdf",
        "keywords": [
            "spatial prepositions",
            "spatial reasoning"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"probe the limits of reasoning about spatial relations in large language models\"; \"However, none of the models reaches human performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"probe the limits of reasoning about spatial relations in large language models\"; \"However, none of the models reaches human performance.\""
    },
    {
        "title": "Can language models learn analogical reasoning? Investigating training objectives and comparisons to human performance",
        "authors": [
            "Molly Petersen",
            "Lonneke van der Plas"
        ],
        "published": "2023",
        "summary": "While analogies are a common way to evaluate word embeddings in NLP, it is also of interest to investigate whether or not analogical reasoning is a task in itself that can be learned. In this paper, we test several ways to learn basic analogical reasoning, specifically focusing on analogies that are more typical of what is used to evaluate analogical reasoning in humans than those in commonly used NLP benchmarks. Our experiments find that models are able to learn analogical reasoning, even with a small amount of data. We additionally compare our models to a dataset with a human baseline, and find that after training models approach human performance.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1022.pdf",
        "keywords": [
            "analogical reasoning",
            "analogies",
            "training objectives"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract does not mention any limitations of LLMs, but rather discusses their ability to learn analogical reasoning.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract does not mention any limitations of LLMs, but rather discusses their ability to learn analogical reasoning."
    },
    {
        "title": "A Picture is Worth a Thousand Words: Language Models Plan from Pixels",
        "authors": [
            "Anthony Liu",
            "Lajanugen Logeswaran",
            "Sungryull Sohn",
            "Honglak Lee"
        ],
        "published": "2023",
        "summary": "Planning is an important capability of artificial agents that perform long-horizon tasks in real-world environments. In this work, we explore the use of pre-trained language models (PLMs) to reason about plan sequences from text instructions in embodied visual environments. Prior PLM based approaches for planning either assume observations are available in the form of text by a captioning model, reason about plans from the instruction alone, or incorporate information about the visual environment in limited ways (such as a pre-trained affordance function). In contrast, we show that the PLM can accurately plan even when observations are directly encoded as input prompts for the PLM. We show this simple approach outperforms prior approaches in experiments on the ALFWorld and VirtualHome benchmarks.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1025.pdf",
        "keywords": [
            "pre trained language models",
            "language models plan"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Prior PLM based approaches for planning either assume observations are available in the form of text by a captioning model, reason about plans from the instruction alone, or incorporate information about the visual environment in limited ways (such as a pre-trained affordance function).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Prior PLM based approaches for planning either assume observations are available in the form of text by a captioning model, reason about plans from the instruction alone, or incorporate information about the visual environment in limited ways (such as a pre-trained affordance function).\""
    },
    {
        "title": "Do Transformers Parse while Predicting the Masked Word?",
        "authors": [
            "Haoyu Zhao",
            "Abhishek Panigrahi",
            "Rong Ge",
            "Sanjeev Arora"
        ],
        "published": "2023",
        "summary": "Pre-trained language models have been shown to encode linguistic structures like parse trees in their embeddings while being trained unsupervised. Some doubts have been raised whether the models are doing parsing or only some computation weakly correlated with it. Concretely: (a) Is it possible to explicitly describe transformers with realistic embedding dimensions, number of heads, etc. that are capable of doing parsing — or even approximate parsing? (b) Why do pre-trained models capture parsing structure? This paper takes a step toward answering these questions in the context of generative modeling with PCFGs. We show that masked language models like BERT or RoBERTa of moderate sizes can approximately execute the Inside-Outside algorithm for the English PCFG (Marcus et al., 1993). We also show that the Inside-Outside algorithm is optimal for masked language modeling loss on the PCFG-generated data. We conduct probing experiments on models pre-trained on PCFG-generated data to show that this not only allows recovery of approximate parse tree, but also recovers marginal span probabilities computed by the Inside-Outside algorithm, which suggests an implicit bias of masked language modeling towards this algorithm.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1029.pdf",
        "keywords": [
            "masked language models",
            "transformers",
            "transformers parse",
            "pcfgs",
            "masked word",
            "algorithm",
            "generative",
            "marginal"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Some doubts have been raised whether the models are doing parsing or only some computation weakly correlated with it.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Some doubts have been raised whether the models are doing parsing or only some computation weakly correlated with it.\""
    },
    {
        "title": "Composable Text Controls in Latent Space with ODEs",
        "authors": [
            "Guangyi Liu",
            "Zeyu Feng",
            "Yuan Gao",
            "Zichao Yang",
            "Xiaodan Liang",
            "Junwei Bao",
            "Xiaodong He",
            "Shuguang Cui",
            "Zhen Li",
            "Zhiting Hu"
        ],
        "published": "2023",
        "summary": "Real-world text applications often involve composing a wide range of text control operations, such as editing the text w.r.t. an attribute, manipulating keywords and structure, and generating new text of desired properties. Prior work typically learns/finetunes a language model (LM) to perform individual or specific subsets of operations. Recent research has studied combining operations in a plug-and-play manner, often with costly search or optimization in the complex sequence space. This paper proposes a new efficient approach for composable text operations in the compact latent space of text. The low-dimensionality and differentiability of the text latent vector allow us to develop an efficient sampler based on ordinary differential equations (ODEs) given arbitrary plug-in operators (e.g., attribute classifiers). By connecting pretrained LMs (e.g., GPT2) to the latent space through efficient adaption, we then decode the sampled vectors into desired text sequences. The flexible approach permits diverse control operators (sentiment, tense, formality, keywords, etc.) acquired using any relevant data from different domains. Experiments show that composing those operators within our approach manages to generate or edit high-quality text, substantially improving over previous methods in terms of generation quality and efficiency.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1030.pdf",
        "keywords": [
            "text",
            "latent space",
            "text control",
            "composable text",
            "composable text controls",
            "language model",
            "text latent vector"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Prior work typically learns/finetunes a language model (LM) to perform individual or specific subsets of operations.\"\n\nThis paper talks about LLMs but does not mention any explicit limitation of the models in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Prior work typically learns/finetunes a language model (LM) to perform individual or specific subsets of operations.\"\n\nThis paper talks about LLMs but does not mention any explicit limitation of the models in the abstract."
    },
    {
        "title": "P5: Plug-and-Play Persona Prompting for Personalized Response Selection",
        "authors": [
            "Joosung Lee",
            "Minsik Oh",
            "Donghun Lee"
        ],
        "published": "2023",
        "summary": "The use of persona-grounded retrieval-based chatbots is crucial for personalized conversations, but there are several challenges that need to be addressed. 1) In general, collecting persona-grounded corpus is very expensive. 2) The chatbot system does not always respond in consideration of persona at real applications. To address these challenges, we propose a plug-and-play persona prompting method. Our system can function as a standard open-domain chatbot if persona information is not available. We demonstrate that this approach performs well in the zero-shot setting, which reduces the dependence on persona-ground training data. This makes it easier to expand the system to other languages without the need to build a persona-grounded corpus. Additionally, our model can be fine-tuned for even better performance. In our experiments, the zero-shot model improved the standard model by 7.71 and 1.04 points in the original persona and revised persona, respectively. The fine-tuned model improved the previous state-of-the-art system by 1.95 and 3.39 points in the original persona and revised persona, respectively. To the best of our knowledge, this is the first attempt to solve the problem of personalized response selection using prompt sequences. Our code is available on github.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1031.pdf",
        "keywords": [
            "personalized",
            "persona",
            "chatbots",
            "plug and play",
            "personalized response",
            "prompting",
            "personalized response selection",
            "grounded corpus",
            "github"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"2) The chatbot system does not always respond in consideration of persona at real applications.\"\n\nThis abstract mentions a limitation of chatbot systems, which are often powered by LLMs, but it does not explore this limitation in depth and focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"2) The chatbot system does not always respond in consideration of persona at real applications.\"\n\nThis abstract mentions a limitation of chatbot systems, which are often powered by LLMs, but it does not explore this limitation in depth and focuses on the proposed solution."
    },
    {
        "title": "Reader: Model-based language-instructed reinforcement learning",
        "authors": [
            "Nicola Dainese",
            "Pekka Marttinen",
            "Alexander Ilin"
        ],
        "published": "2023",
        "summary": "We explore how we can build accurate world models, which are partially specified by language, and how we can plan with them in the face of novelty and uncertainty. We propose the first model-based reinforcement learning approach to tackle the environment Read To Fight Monsters (Zhong et al., 2019), a grounded policy learning problem. In RTFM an agent has to reason over a set of rules and a goal, both described in a language manual, and the observations, while taking into account the uncertainty arising from the stochasticity of the environment, in order to generalize successfully its policy to test episodes. We demonstrate the superior performance and sample efficiency of our model-based approach to the existing model-free SOTA agents in eight variants of RTFM. Furthermore, we show how the agent’s plans can be inspected, which represents progress towards more interpretable agents.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1032.pdf",
        "keywords": [
            "reinforcement learning",
            "policy learning",
            "world models",
            "reader"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Document-Level Machine Translation with Large Language Models",
        "authors": [
            "Longyue Wang",
            "Chenyang Lyu",
            "Tianbo Ji",
            "Zhirui Zhang",
            "Dian Yu",
            "Shuming Shi",
            "Zhaopeng Tu"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) such as ChatGPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks. Taking document-level machine translation (MT) as a testbed, this paper provides an in-depth evaluation of LLMs’ ability on discourse modeling. The study focuses on three aspects: 1) Effects of Context-Aware Prompts, where we investigate the impact of different prompts on document-level translation quality and discourse phenomena; 2) Comparison of Translation Models, where we compare the translation performance of ChatGPT with commercial MT systems and advanced document-level MT methods; 3) Analysis of Discourse Modelling Abilities, where we further probe discourse knowledge encoded in LLMs and shed light on impacts of training techniques on discourse modeling. By evaluating on a number of benchmarks, we surprisingly find that LLMs have demonstrated superior performance and show potential to become a new paradigm for document-level translation: 1) leveraging their powerful long-text modeling capabilities, GPT-3.5 and GPT-4 outperform commercial MT systems in terms of human evaluation; 2) GPT-4 demonstrates a stronger ability for probing linguistic knowledge than GPT-3.5. This work highlights the challenges and opportunities of LLMs for MT, which we hope can inspire the future design and evaluation of LLMs (We release our data and annotations at https://github.com/longyuewangdcu/Document-MT-LLM).",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1036.pdf",
        "keywords": [
            "language models",
            "translation",
            "knowledge",
            "document level machine translation",
            "discourse modeling",
            "evaluation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This work highlights the challenges and opportunities of LLMs for MT\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"This work highlights the challenges and opportunities of LLMs for MT\""
    },
    {
        "title": "Argue with Me Tersely: Towards Sentence-Level Counter-Argument Generation",
        "authors": [
            "Jiayu Lin",
            "Rong Ye",
            "Meng Han",
            "Qi Zhang",
            "Ruofei Lai",
            "Xinyu Zhang",
            "Zhao Cao",
            "Xuanjing Huang",
            "Zhongyu Wei"
        ],
        "published": "2023",
        "summary": "Counter-argument generation—a captivating area in computational linguistics—seeks to craft statements that offer opposing views. While most research has ventured into paragraph-level generation, sentence-level counter-argument generation beckons with its unique constraints and brevity-focused challenges. Furthermore, the diverse nature of counter-arguments poses challenges for evaluating model performance solely based on n-gram-based metrics. In this paper, we present the ArgTersely benchmark for sentence-level counter-argument generation, drawing from a manually annotated dataset from the ChangeMyView debate forum. We also propose Arg-LlaMA for generating high-quality counter-argument. For better evaluation, we trained a BERT-based evaluator Arg-Judge with human preference data. We conducted comparative experiments involving various baselines such as LlaMA, Alpaca, GPT-3, and others. The results show the competitiveness of our proposed framework and evaluator in counter-argument generation tasks. Code and data are available at https://github.com/amazingljy1206/ArgTersely.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1039.pdf",
        "keywords": [
            "argument generation",
            "counter argument generation",
            "sentence level"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The diverse nature of counter-arguments poses challenges for evaluating model performance solely based on n-gram-based metrics.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The diverse nature of counter-arguments poses challenges for evaluating model performance solely based on n-gram-based metrics.\""
    },
    {
        "title": "JASMINE: Arabic GPT Models for Few-Shot Learning",
        "authors": [
            "El Moatez Billah Nagoudi",
            "Muhammad Abdul-Mageed",
            "AbdelRahim Elmadany",
            "Alcides Inciarte",
            "Md Tawkat Islam Khondaker"
        ],
        "published": "2023",
        "summary": "Scholarship on generative pretraining (GPT) remains acutely Anglocentric, leaving serious gaps in our understanding of the whole class of autoregressive models. For example, we have little knowledge about the potential of these models and their societal impacts in diverse linguistic and cultural settings. We alleviate this issue for Arabic, a wide collection of languages and dialectal varieties with more than 400 million population, by introducing JASMINE. JASMINE is a suite of powerful Arabic autoregressive Transformer language models ranging in size between 300 million-6.7 billion parameters pretrained on a large and diverse dataset ( 235 GB of text). We also carefully design and release a comprehensive benchmark for both automated and human evaluation of Arabic autoregressive models, with coverage of potential social biases, harms, and toxicity. Using our novel benchmark, we evaluate JASMINE extensively showing powerful performance intrinsically as well as in few-shot learning on a wide range of NLP tasks. We aim to responsibly release our models and evaluation benchmark with interested researchers, along with code for experimenting with them.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1040.pdf",
        "keywords": [
            "generative pretraining",
            "arabic gpt models",
            "arabic autoregressive models",
            "few shot learning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"coverage of potential social biases, harms, and toxicity.\"\n\nThis rating is chosen because the paper mentions potential limitations of LLMs (social biases, harms, and toxicity) but only briefly and does not explore these limitations in detail. The primary focus of the paper is on introducing a new Arabic autoregressive Transformer language model and evaluating its performance.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"coverage of potential social biases, harms, and toxicity.\"\n\nThis rating is chosen because the paper mentions potential limitations of LLMs (social biases, harms, and toxicity) but only briefly and does not explore these limitations in detail. The primary focus of the paper is on introducing a new Arabic autoregressive Transformer language model and evaluating its performance."
    },
    {
        "title": "Multi Document Summarization Evaluation in the Presence of Damaging Content",
        "authors": [
            "Avshalom Manevich",
            "David Carmel",
            "Nachshon Cohen",
            "Elad Kravi",
            "Ori Shapira"
        ],
        "published": "2023",
        "summary": "In the Multi-document summarization (MDS) task, a summary is produced for a given set of documents. A recent line of research introduced the concept of damaging documents, denoting documents that should not be exposed to readers due to various reasons. In the presence of damaging documents, a summarizer is ideally expected to exclude damaging content in its output. Existing metrics evaluate a summary based on aspects such as relevance and consistency with the source documents. We propose to additionally measure the ability of MDS systems to properly handle damaging documents in their input set. To that end, we offer two novel metrics based on lexical similarity and language model likelihood. A set of experiments demonstrates the effectiveness of our metrics in measuring the ability of MDS systems to summarize a set of documents while eliminating damaging content from their summaries.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"two novel metrics... based on... language model likelihood.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"two novel metrics... based on... language model likelihood.\""
    },
    {
        "title": "Time-Aware Representation Learning for Time-Sensitive Question Answering",
        "authors": [
            "Jungbin Son",
            "Alice Oh"
        ],
        "published": "2023",
        "summary": "Time is one of the crucial factors in real-world question answering (QA) problems. However, language models have difficulty understanding the relationships between time specifiers, such as ‘after’ and ‘before’, and numbers, since existing QA datasets do not include sufficient time expressions. To address this issue, we propose a Time-Context aware Question Answering (TCQA) framework. We suggest a Time-Context dependent Span Extraction (TCSE) task, and build a time-context dependent data generation framework for model training. Moreover, we present a metric to evaluate the time awareness of the QA model using TCSE. The TCSE task consists of a question and four sentence candidates classified as correct or incorrect based on time and context. The model is trained to extract the answer span from the sentence that is both correct in time and context. The model trained with TCQA outperforms baseline models up to 8.5 of the F1-score in the TimeQA dataset. Our dataset and code are available at https://github.com/sonjbin/TCQA",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.6.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, language models have difficulty understanding the relationships between time specifiers, such as ‘after’ and ‘before’, and numbers, since existing QA datasets do not include sufficient time expressions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, language models have difficulty understanding the relationships between time specifiers, such as ‘after’ and ‘before’, and numbers, since existing QA datasets do not include sufficient time expressions.\""
    },
    {
        "title": "EffEval: A Comprehensive Evaluation of Efficiency for MT Evaluation Metrics",
        "authors": [
            "Daniil Larionov",
            "Jens Grünwald",
            "Christoph Leiter",
            "Steffen Eger"
        ],
        "published": "2023",
        "summary": "Efficiency is a key property to foster inclusiveness and reduce environmental costs, especially in an era of LLMs. In this work, we provide a comprehensive evaluation of efficiency for MT evaluation metrics. Our approach involves replacing computation-intensive transformers with lighter alternatives and employing linear and quadratic approximations for alignment algorithms on top of LLM representations. We evaluate six (reference-free and reference-based) metrics across three MT datasets and examine 16 lightweight transformers. In addition, we look into the training efficiency of metrics like COMET by utilizing adapters. Our results indicate that (a) TinyBERT provides the optimal balance between quality and efficiency, (b) CPU speed-ups are more substantial than those on GPU; (c) WMD approximations yield no efficiency gains while reducing quality and (d) adapters enhance training efficiency (regarding backward pass speed and memory requirements) as well as, in some cases, metric quality. These findings can help to strike a balance between evaluation speed and quality, which is essential for effective NLG systems. Furthermore, our research contributes to the ongoing efforts to optimize NLG evaluation metrics with minimal impact on performance. To our knowledge, ours is the most comprehensive analysis of different aspects of efficiency for MT metrics conducted so far.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.7.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Efficiency is a key property to foster inclusiveness and reduce environmental costs, especially in an era of LLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Efficiency is a key property to foster inclusiveness and reduce environmental costs, especially in an era of LLMs.\""
    },
    {
        "title": "On the Relation between Sensitivity and Accuracy in In-Context Learning",
        "authors": [
            "Yanda Chen",
            "Chen Zhao",
            "Zhou Yu",
            "Kathleen McKeown",
            "He He"
        ],
        "published": "2023",
        "summary": "In-context learning (ICL) suffers from oversensitivity to the prompt, making it unreliable in real-world scenarios. We study the sensitivity of ICL with respect to multiple perturbation types. First, we find that label bias obscures the true sensitivity, and therefore prior work may have significantly underestimated ICL sensitivity. Second, we observe a strong negative correlation between ICL sensitivity and accuracy: predictions sensitive to perturbations are less likely to be correct. Motivated by these findings, we propose SenSel, a few-shot selective prediction method that abstains from sensitive predictions. Experiments on ten classification datasets show that SenSel consistently outperforms two commonly used confidence-based and entropy-based baselines on abstention decisions.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.12.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Reference Free Domain Adaptation for Translation of Noisy Questions with Question Specific Rewards",
        "authors": [
            "Baban Gain",
            "Ramakrishna Appicharla",
            "Soumya Chennabasavaraj",
            "Nikesh Garera",
            "Asif Ekbal",
            "Muthusamy Chelliah"
        ],
        "published": "2023",
        "summary": "Community Question-Answering (CQA) portals serve as a valuable tool for helping users within an organization. However, making them accessible to non-English-speaking users continues to be a challenge. Translating questions can broaden the community’s reach, benefiting individuals with similar inquiries in various languages. Translating questions using Neural Machine Translation (NMT) poses more challenges, especially in noisy environments, where the grammatical correctness of the questions is not monitored. These questions may be phrased as statements by non-native speakers, with incorrect subject-verb order and sometimes even missing question marks. Creating a synthetic parallel corpus from such data is also difficult due to its noisy nature. To address this issue, we propose a training methodology that fine-tunes the NMT system only using source-side data. Our approach balances adequacy and fluency by utilizing a loss function that combines BERTScore and Masked Language Model (MLM) Score. Our method surpasses the conventional Maximum Likelihood Estimation (MLE) based fine-tuning approach, which relies on synthetic target data, by achieving a 1.9 BLEU score improvement. Our model exhibits robustness while we add noise to our baseline, and still achieve 1.1 BLEU improvement and large improvements on TER and BLEURT metrics. Our proposed methodology is model-agnostic and is only necessary during the training phase. We make the codes and datasets publicly available at https://www.iitp.ac.in/~ai-nlp-ml/resources.html#DomainAdapt for facilitating further research.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.16.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Translating questions using Neural Machine Translation (NMT) poses more challenges, especially in noisy environments, where the grammatical correctness of the questions is not monitored.\"\n\nNote that the paper does not explicitly discuss Large Language Models (LLMs), but it mentions Masked Language Model (MLM) which is a type of language model. However, the limitation mentioned is related to Neural",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Translating questions using Neural Machine Translation (NMT) poses more challenges, especially in noisy environments, where the grammatical correctness of the questions is not monitored.\"\n\nNote that the paper does not explicitly discuss Large Language Models (LLMs), but it mentions Masked Language Model (MLM) which is a type of language model. However, the limitation mentioned is related to Neural"
    },
    {
        "title": "Data Pruning for Efficient Model Pruning in Neural Machine Translation",
        "authors": [
            "Abdul Hameed Azeemi",
            "Ihsan Qazi",
            "Agha Raza"
        ],
        "published": "2023",
        "summary": "Model pruning methods reduce memory requirements and inference time of large-scale pre-trained language models after deployment. However, the actual pruning procedure is computationally intensive, involving repeated training and pruning until the required sparsity is achieved. This paper combines data pruning with movement pruning for Neural Machine Translation (NMT) to enable efficient fine-pruning. We design a dataset pruning strategy by leveraging cross-entropy scores of individual training instances. We conduct pruning experiments on the task of machine translation from Romanian-to-English and Turkish-to-English, and demonstrate that selecting hard-to-learn examples (top-k) based on training cross-entropy scores outperforms other dataset pruning methods. We empirically demonstrate that data pruning reduces the overall steps required for convergence and the training time of movement pruning. Finally, we perform a series of experiments to tease apart the role of training data during movement pruning and uncover new insights to understand the interplay between data and model pruning in the context of NMT.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.18.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the actual pruning procedure is computationally intensive, involving repeated training and pruning until the required sparsity is achieved.\"\n\nThis paper mentions a limitation of LLMs in passing, specifically the computational intensity of the pruning procedure, but does not explore it in depth and focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the actual pruning procedure is computationally intensive, involving repeated training and pruning until the required sparsity is achieved.\"\n\nThis paper mentions a limitation of LLMs in passing, specifically the computational intensity of the pruning procedure, but does not explore it in depth and focuses on the proposed solution."
    },
    {
        "title": "Long-Form Speech Translation through Segmentation with Finite-State Decoding Constraints on Large Language Models",
        "authors": [
            "Arya McCarthy",
            "Hao Zhang",
            "Shankar Kumar",
            "Felix Stahlberg",
            "Ke Wu"
        ],
        "published": "2023",
        "summary": "One challenge in speech translation is that plenty of spoken content is long-form, but short units are necessary for obtaining high-quality translations. To address this mismatch, we adapt large language models (LLMs) to split long ASR transcripts into segments that can be independently translated so as to maximize the overall translation quality. We overcome the tendency of hallucination in LLMs by incorporating finite-state constraints during decoding; these eliminate invalid outputs without requiring additional training. We discover that LLMs are adaptable to transcripts containing ASR errors through prompt-tuning or fine-tuning. Relative to a state-of-the-art automatic punctuation baseline, our best LLM improves the average BLEU by 2.9 points for English–German, English–Spanish, and English–Arabic TED talk translation in 9 test sets, just by improving segmentation.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.19.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We overcome the tendency of hallucination in LLMs by incorporating finite-state constraints during decoding; these eliminate invalid outputs without requiring additional training.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We overcome the tendency of hallucination in LLMs by incorporating finite-state constraints during decoding; these eliminate invalid outputs without requiring additional training.\""
    },
    {
        "title": "AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies",
        "authors": [
            "Weiyan Shi",
            "Emily Dinan",
            "Adi Renduchintala",
            "Daniel Fried",
            "Athul Jacob",
            "Zhou Yu",
            "Mike Lewis"
        ],
        "published": "2023",
        "summary": "We show that dialogue models can detect errors in their own messages, by calculating the likelihood of replies that are indicative of poor messages. For example, if an agent believes its partner is likely to respond “I don’t understand” to a candidate message, that message may not make sense, so an alternative message should be chosen. We evaluate our approach on a dataset from the game Diplomacy, which contains long dialogues richly grounded in the game state, on which existing models make many errors. We first show that hand-crafted replies can be effective for the task of detecting nonsense in applications as complex as Diplomacy. We then design AutoReply, an algorithm to search for such discriminative replies automatically, given a small number of annotated dialogue examples. We find that AutoReply-generated replies outperform handcrafted replies and perform on par with supervised learning approaches.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.23.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Plausibility Processing in Transformer Language Models: Focusing on the Role of Attention Heads in GPT",
        "authors": [
            "Soo Ryu"
        ],
        "published": "2023",
        "summary": "The goal of this paper is to explore how Transformer language models process semantic knowledge, especially regarding the plausibility of noun-verb relations. First, I demonstrate GPT2 exhibits a higher degree of similarity with humans in plausibility processing compared to other Transformer language models. Next, I delve into how knowledge of plausibility is contained within attention heads of GPT2 and how these heads causally contribute to GPT2’s plausibility processing ability. Through several experiments, it was found that: i) GPT2 has a number of attention heads that detect plausible noun-verb relationships; ii) these heads collectively contribute to the Transformer’s ability to process plausibility, albeit to varying degrees; and iii) attention heads’ individual performance in detecting plausibility does not necessarily correlate with how much they contribute to GPT2’s plausibility processing ability.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.27.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations are mentioned, but the paper's focus on understanding how a specific model (GPT2) processes plausibility and the varying performance of attention heads in detecting plausibility implies some limitations in the model's ability to process semantic knowledge.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: No explicit limitations are mentioned, but the paper's focus on understanding how a specific model (GPT2) processes plausibility and the varying performance of attention heads in detecting plausibility implies some limitations in the model's ability to process semantic knowledge."
    },
    {
        "title": "Automatic Unit Test Data Generation and Actor-Critic Reinforcement Learning for Code Synthesis",
        "authors": [
            "Philip Gorinski",
            "Matthieu Zimmer",
            "Gerasimos Lampouras",
            "Derrick Goh Xin Deik",
            "Ignacio Iacobacci"
        ],
        "published": "2023",
        "summary": "The advent of large pre-trained language models in the domain of Code Synthesis has shown remarkable performance on various benchmarks, treating the problem of Code Generation in a fashion similar to Natural Language Generation, trained with a Language Modelling (LM) objective. In addition, the property of programming language code being precisely evaluable with respect to its semantics – through the use of Unit Tests to check its functional correctness – lends itself to using Reinforcement Learning (RL) as a further training paradigm. Previous work has shown that RL can be applied as such to improve models’ coding capabilities; however, such RL-based methods rely on a reward signal based on defined Unit Tests, which are much harder to obtain compared to the huge crawled code datasets used in LM objectives. In this work, we present a novel approach to automatically obtain data consisting of function signatures and associated Unit Tests, suitable for RL training of Code Synthesis models. We also introduce a straightforward, simple yet effective Actor-Critic RL training scheme and show that it, in conjunction with automatically generated training data, leads to improvement of a pre-trained code language model’s performance by up to 9.9% improvement over the original underlying code synthesis LM, and up to 4.3% over RL-based models trained with standard PPO or CodeRL.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.28.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Previous work has shown that RL can be applied as such to improve models’ coding capabilities; however, such RL-based methods rely on a reward signal based on defined Unit Tests, which are much harder to obtain compared to the huge crawled code datasets used in LM objectives.\"\n\nThis rating is given because the paper mentions one limitation of LLMs (the difficulty in obtaining Unit Tests for RL",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Previous work has shown that RL can be applied as such to improve models’ coding capabilities; however, such RL-based methods rely on a reward signal based on defined Unit Tests, which are much harder to obtain compared to the huge crawled code datasets used in LM objectives.\"\n\nThis rating is given because the paper mentions one limitation of LLMs (the difficulty in obtaining Unit Tests for RL"
    },
    {
        "title": "Towards Agile Text Classifiers for Everyone",
        "authors": [
            "Maximilian Mozes",
            "Jessica Hoffmann",
            "Katrin Tomanek",
            "Muhamed Kouate",
            "Nithum Thain",
            "Ann Yuan",
            "Tolga Bolukbasi",
            "Lucas Dixon"
        ],
        "published": "2023",
        "summary": "Text-based safety classifiers are widely used for content moderation and increasingly to tune generative language model behavior - a topic of growing concern for the safety of digital assistants and chatbots. However, different policies require different classifiers, and safety policies themselves improve from iteration and adaptation. This paper introduces and evaluates methods for agile text classification, whereby classifiers are trained using small, targeted datasets that can be quickly developed for a particular policy. Experimenting with 7 datasets from three safety-related domains, comprising 15 annotation schemes, led to our key finding: prompt-tuning large language models, like PaLM 62B, with a labeled dataset of as few as 80 examples can achieve state-of-the-art performance. We argue that this enables a paradigm shift for text classification, especially for models supporting safer online discourse. Instead of collecting millions of examples to attempt to create universal safety classifiers over months or years, classifiers could be tuned using small datasets, created by individuals or small organizations, tailored for specific use cases, and iterated on and adapted in the time-span of a day.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.30.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, different policies require different classifiers, and safety policies themselves improve from iteration and adaptation.\"\n\nThis rating is chosen because the abstract mentions the need for adaptation and iteration of safety policies, but it is not a direct limitation of LLMs. The focus is on the potential of LLMs to be fine-tuned for specific use cases with small datasets, rather than on the limitations",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, different policies require different classifiers, and safety policies themselves improve from iteration and adaptation.\"\n\nThis rating is chosen because the abstract mentions the need for adaptation and iteration of safety policies, but it is not a direct limitation of LLMs. The focus is on the potential of LLMs to be fine-tuned for specific use cases with small datasets, rather than on the limitations"
    },
    {
        "title": "BERT Goes Off-Topic: Investigating the Domain Transfer Challenge using Genre Classification",
        "authors": [
            "Dmitri Roussinov",
            "Serge Sharoff"
        ],
        "published": "2023",
        "summary": "While performance of many text classification tasks has been recently improved due to Pretrained Language Models (PLMs), in this paper we show that they still suffer from a performance gap when the underlying distribution of topics changes. For example, a genre classifier trained on political topics often fails when tested on documents in the same genre, but about sport or medicine. In this work, we quantify this phenomenon empirically with a large corpus and a large set of topics. Thus, we verify that domain transfer remains challenging both for classic PLMs, such as BERT, and for modern large models (LLMs), such as GPT. We develop a data augmentation approach by generating texts in any desired genre and on any desired topic, even when there are no documents in the training corpus that are both in that particular genre and on that particular topic. When we augment the training dataset with the topically-controlled synthetic texts, F1 improves up to 50% for some topics, approaching on-topic training, while showing no or next to no improvement for other topics. While our empirical results focus on genre classification, our methodology is applicable to other classification tasks such as gender, authorship, or sentiment classification.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.34.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While performance of many text classification tasks has been recently improved due to Pretrained Language Models (PLMs), in this paper we show that they still suffer from a performance gap when the underlying distribution of topics changes... Thus, we verify that domain transfer remains challenging both for classic PLMs, such as BERT, and for modern large models (LLMs), such as GPT.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"While performance of many text classification tasks has been recently improved due to Pretrained Language Models (PLMs), in this paper we show that they still suffer from a performance gap when the underlying distribution of topics changes... Thus, we verify that domain transfer remains challenging both for classic PLMs, such as BERT, and for modern large models (LLMs), such as GPT.\""
    },
    {
        "title": "MEAL: Stable and Active Learning for Few-Shot Prompting",
        "authors": [
            "Abdullatif Köksal",
            "Timo Schick",
            "Hinrich Schuetze"
        ],
        "published": "2023",
        "summary": "Few-shot classification has made great strides due to foundation models that, through priming and prompting, are highly effective few-shot learners. However, this approach has high variance both across different sets of few shots (*data selection*) and across different finetuning runs (*run variability*). This is problematic not only because it impedes the fair comparison of different approaches, but especially because it makes few-shot learning too unreliable for many real-world applications. To alleviate these issues, we make two contributions for more stable and effective few-shot learning: First, we propose novel ensembling methods and show that they substantially reduce *run variability*. Second, we introduce a new active learning (AL) criterion for *data selection* and present the first AL-based approach specifically tailored towards prompt-based learning. In our experiments, we show that our combined method, MEAL (**M**ultiprompt finetuning and prediction **E**nsembling with **A**ctive **L**earning), improves overall performance of prompt-based finetuning by 2.3 points on five diverse tasks. We publicly share our code and data splits in https://github.com/akoksal/MEAL.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.36.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, this approach has high variance both across different sets of few shots (*data selection*) and across different finetuning runs (*run variability*). This is problematic not only because it impedes the fair comparison of different approaches, but especially because it makes few-shot learning too unreliable for many real-world applications.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, this approach has high variance both across different sets of few shots (*data selection*) and across different finetuning runs (*run variability*). This is problematic not only because it impedes the fair comparison of different approaches, but especially because it makes few-shot learning too unreliable for many real-world applications.\""
    },
    {
        "title": "Weakly-supervised Deep Cognate Detection Framework for Low-Resourced Languages Using Morphological Knowledge of Closely-Related Languages",
        "authors": [
            "Koustava Goswami",
            "Priya Rani",
            "Theodorus Fransen",
            "John McCrae"
        ],
        "published": "2023",
        "summary": "Exploiting cognates for transfer learning in under-resourced languages is an exciting opportunity for language understanding tasks, including unsupervised machine translation, named entity recognition and information retrieval. Previous approaches mainly focused on supervised cognate detection tasks based on orthographic, phonetic or state-of-the-art contextual language models, which under-perform for most under-resourced languages. This paper proposes a novel language-agnostic weakly-supervised deep cognate detection framework for under-resourced languages using morphological knowledge from closely related languages. We train an encoder to gain morphological knowledge of a language and transfer the knowledge to perform unsupervised and weakly-supervised cognate detection tasks with and without the pivot language for the closely-related languages. While unsupervised, it overcomes the need for hand-crafted annotation of cognates. We performed experiments on different published cognate detection datasets across language families and observed not only significant improvement over the state-of-the-art but also our method outperformed the state-of-the-art supervised and unsupervised methods. Our model can be extended to a wide range of languages from any language family as it overcomes the requirement of the annotation of the cognate pairs for training.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.38.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data",
        "authors": [
            "Ruoxi Sun",
            "Sercan Arik",
            "Rajarishi Sinha",
            "Hootan Nakhost",
            "Hanjun Dai",
            "Pengcheng Yin",
            "Tomas Pfister"
        ],
        "published": "2023",
        "summary": "Text-to-SQL aims to automate the process of generating SQL queries on a database from natural language text. In this work, we propose “SQLPrompt”, tailored to improve the few-shot prompting capabilities of Text-to-SQL for Large Language Models (LLMs). Our methods include innovative prompt design, execution-based consistency decoding strategy which selects the SQL with the most consistent execution outcome among other SQL proposals, and a method that aims to improve performance by diversifying the SQL proposals during consistency selection with different prompt designs (“MixPrompt”) and foundation models (“MixLLMs”). We show that SQLPrompt outperforms previous approaches for in-context learning with zero labeled data by a large margin, closing the gap with finetuning state-of-the-art with thousands of labeled data.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.39.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"tailored to improve the few-shot prompting capabilities of Text-to-SQL for Large Language Models (LLMs)\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"tailored to improve the few-shot prompting capabilities of Text-to-SQL for Large Language Models (LLMs)\""
    },
    {
        "title": "A Table-to-Text Framework with Heterogeneous Multidominance Attention and Self-Evaluated Multi-Pass Deliberation",
        "authors": [
            "Xi Chen",
            "Xinjiang Lu",
            "Haoran Xin",
            "Wenjun Peng",
            "Haoyang Duan",
            "Feihu Jiang",
            "Jingbo Zhou",
            "Hui Xiong"
        ],
        "published": "2023",
        "summary": "Though big progress in table-to-text works, effectively leveraging table structure signals, e.g., hierarchical structure, remains challenging. Besides, deliberating generated descriptions proves to be effective for table-to-text. However, determining the appropriate outcome when encountering multi-pass candidates is another challenge. To this end, we propose a novel table-to-text approach on top of Self-evaluated multi-pass Generation and Heterogenous Multidominance Attention, namely SG-HMA. Specifically, we formulate the table structure into a multidominance (MD) structure and devise a heterogenous multidominance attention (HMA) to comprehensively explore the complex interactions encoded in the hierarchical structure, which can further deliver rich signals for text generation with the help of pre-trained language models (PLMs). Afterward, a contrastive loss is introduced to align the generation objective with evaluation metrics, so the more faithful generated descriptions can be guaranteed. We conduct extensive experiments on three public datasets, demonstrating that SG-HMA outperforms several SOTA methods quantitatively and qualitatively.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.44.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Though big progress in table-to-text works, effectively leveraging table structure signals, e.g., hierarchical structure, remains challenging.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Though big progress in table-to-text works, effectively leveraging table structure signals, e.g., hierarchical structure, remains challenging.\""
    },
    {
        "title": "Crossing the Aisle: Unveiling Partisan and Counter-Partisan Events in News Reporting",
        "authors": [
            "Kaijian Zou",
            "Xinliang Zhang",
            "Winston Wu",
            "Nicholas Beauchamp",
            "Lu Wang"
        ],
        "published": "2023",
        "summary": "News media is expected to uphold unbiased reporting. Yet they may still affect public opinion by selectively including or omitting events that support or contradict their ideological positions. Prior work in NLP has only studied media bias via linguistic style and word usage. In this paper, we study to which degree media balances news reporting and affects consumers through event inclusion or omission. We first introduce the task of detecting both partisan and counter-partisan events: events that support or oppose the author’s political ideology. To conduct our study, we annotate a high-quality dataset, PAC, containing 8,511 (counter-)partisan event annotations in 304 news articles from ideologically diverse media outlets. We benchmark PAC to highlight the challenges of this task. Our findings highlight both the ways in which the news subtly shapes opinion and the need for large language models that better understand events within a broader context. Our dataset can be found at https://github.com/launchnlp/Partisan-Event-Dataset.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.45.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the need for large language models that better understand events within a broader context.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the need for large language models that better understand events within a broader context.\""
    },
    {
        "title": "Zero-Shot-BERT-Adapters: a Zero-Shot Pipeline for Unknown Intent Detection",
        "authors": [
            "Daniele Comi",
            "Dimitrios Christofidellis",
            "Pier Piazza",
            "Matteo Manica"
        ],
        "published": "2023",
        "summary": "Intent discovery is a crucial task in natural language processing, and it is increasingly relevant for various of industrial applications. Identifying novel, unseen intents from user inputs remains one of the biggest challenges in this field. Herein, we propose Zero-Shot-BERT-Adapters, a two-stage method for multilingual intent discovery relying on a Transformer architecture, fine-tuned with Adapters. We train the model for Natural Language Inference (NLI) and later perform unknown intent classification in a zero-shot setting for multiple languages. In our evaluation, we first analyze the quality of the model after adaptive fine-tuning on known classes. Secondly, we evaluate its performance in casting intent classification as an NLI task. Lastly, we test the zero-shot performance of the model on unseen classes, showing how Zero-Shot-BERT-Adapters can effectively perform intent discovery by generating semantically similar intents, if not equal, to the ground-truth ones. Our experiments show how Zero-Shot-BERT-Adapters outperforms various baselines in two zero-shot settings: known intent classification and unseen intent discovery. The proposed pipeline holds the potential for broad application in customer care. It enables automated dynamic triage using a lightweight model that can be easily deployed and scaled in various business scenarios, unlike large language models. Zero-Shot-BERT-Adapters represents an innovative multi-language approach for intent discovery, enabling the online generation of novel intents. A Python package implementing the pipeline and the new datasets we compiled are available at the following link: https://github.com/GT4SD/zero-shot-bert-adapters.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.47.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"unlike large language models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"unlike large language models\""
    },
    {
        "title": "Approximating Two-Layer Feedforward Networks for Efficient Transformers",
        "authors": [
            "Róbert Csordás",
            "Kazuki Irie",
            "Jürgen Schmidhuber"
        ],
        "published": "2023",
        "summary": "How to reduce compute and memory requirements of neural networks (NNs) without sacrificing performance? Many recent works use sparse Mixtures of Experts (MoEs) to build resource-efficient large language models (LMs). Here we introduce several novel perspectives on MoEs, presenting a general framework that *unifies* various methods to *approximate two-layer NNs* (e.g., feedforward blocks of Transformers), including product-key memories (PKMs). Leveraging insights from this framework, we propose methods to improve both MoEs and PKMs. Unlike prior work that compares MoEs with dense baselines under the *compute-equal* condition, our evaluation condition is *parameter-equal*, which is crucial to properly evaluate LMs. We show that our MoEs are competitive with the *dense* Transformer-XL on both the WikiText-103 and enwiki8 datasets at two different scales, while being much more resource efficient. This demonstrates that MoEs are relevant not only to extremely large LMs but also to any-scale resource-efficient LMs. Our code is public.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.49.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"How to reduce compute and memory requirements of neural networks (NNs) without sacrificing performance?\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"How to reduce compute and memory requirements of neural networks (NNs) without sacrificing performance?\""
    },
    {
        "title": "Adapter-TST: A Parameter Efficient Method for Multiple-Attribute Text Style Transfer",
        "authors": [
            "Zhiqiang Hu",
            "Nancy Chen",
            "Roy Lee"
        ],
        "published": "2023",
        "summary": "Adapting a large language model for multiple-attribute text style transfer via fine-tuning can be challenging due to the substantial amount of computational resources and labeled data required for the specific downstream task. In this paper, we address this challenge by introducing Adapter-TST, a framework that freezes the pre-trained model’s original parameters and enables the development of a multiple-attribute text style transfer model. Using BART as the backbone model, Adapter-TST utilizes different neural adapters to model different types of attribute information, similar to a plug-in connected to BART. Our method allows control over multiple attributes (e.g. sentiment, tense, active or passive voice) and configures the adapters’ architecture to generate multiple outputs in respect to attributes or compositional editing on the same sentence. We evaluate the proposed model on both traditional sentiment transfer and multiple-attribute transfer tasks. The experiment results demonstrate that Adapter-TST outperforms all the state-of-the-art baselines with significantly less computational resources. We have also empirically shown that each adapter is able to characterize specific stylistic attributes effectively and can be configured to perform compositional editing.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.50.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Adapting a large language model for multiple-attribute text style transfer via fine-tuning can be challenging due to the substantial amount of computational resources and labeled data required for the specific downstream task.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Adapting a large language model for multiple-attribute text style transfer via fine-tuning can be challenging due to the substantial amount of computational resources and labeled data required for the specific downstream task.\""
    },
    {
        "title": "Solving the Right Problem is Key for Translational NLP: A Case Study in UMLS Vocabulary Insertion",
        "authors": [
            "Bernal Gutierrez",
            "Yuqing Mao",
            "Vinh Nguyen",
            "Kin Fung",
            "Yu Su",
            "Olivier Bodenreider"
        ],
        "published": "2023",
        "summary": "As the immense opportunities enabled by large language models become more apparent, NLP systems will be increasingly expected to excel in real-world settings. However, in many instances, powerful models alone will not yield translational NLP solutions, especially if the formulated problem is not well aligned with the real-world task. In this work, we study the case of UMLS vocabulary insertion, an important real-world task in which hundreds of thousands of new terms, referred to as atoms, are added to the UMLS, one of the most comprehensive open-source biomedical knowledge bases. Previous work aimed to develop an automated NLP system to make this time-consuming, costly, and error-prone task more efficient. Nevertheless, practical progress in this direction has been difficult to achieve due to a problem formulation and evaluation gap between research output and the real-world task. In order to address this gap, we introduce a new formulation for UMLS vocabulary insertion which mirrors the real-world task, datasets which faithfully represent it and several strong baselines we developed through re-purposing existing solutions. Additionally, we propose an effective rule-enhanced biomedical language model which enables important new model behavior, outperforms all strong baselines and provides measurable qualitative improvements to editors who carry out the UVI task. We hope this case study provides insight into the considerable importance of problem formulation for the success of translational NLP solutions.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.51.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, in many instances, powerful models alone will not yield translational NLP solutions, especially if the formulated problem is not well aligned with the real-world task.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, in many instances, powerful models alone will not yield translational NLP solutions, especially if the formulated problem is not well aligned with the real-world task.\""
    },
    {
        "title": "Improving Cross-lingual Transfer through Subtree-aware Word Reordering",
        "authors": [
            "Ofir Arviv",
            "Dmitry Nikolaev",
            "Taelin Karidi",
            "Omri Abend"
        ],
        "published": "2023",
        "summary": "Despite the impressive growth of the abilities of multilingual language models, such as XLM-R and mT5, it has been shown that they still face difficulties when tackling typologically-distant languages, particularly in the low-resource setting. One obstacle for effective cross-lingual transfer is variability in word-order patterns. It can be potentially mitigated via source- or target-side word reordering, and numerous approaches to reordering have been proposed. However, they rely on language-specific rules, work on the level of POS tags, or only target the main clause, leaving subordinate clauses intact. To address these limitations, we present a new powerful reordering method, defined in terms of Universal Dependencies, that is able to learn fine-grained word-order patterns conditioned on the syntactic context from a small amount of annotated data and can be applied at all levels of the syntactic tree. We conduct experiments on a diverse set of tasks and show that our method consistently outperforms strong baselines over different language pairs and model architectures. This performance advantage holds true in both zero-shot and few-shot scenarios.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.52.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the impressive growth of the abilities of multilingual language models, such as XLM-R and mT5, it has been shown that they still face difficulties when tackling typologically-distant languages, particularly in the low-resource setting.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite the impressive growth of the abilities of multilingual language models, such as XLM-R and mT5, it has been shown that they still face difficulties when tackling typologically-distant languages, particularly in the low-resource setting.\""
    },
    {
        "title": "Automatic Model Selection with Large Language Models for Reasoning",
        "authors": [
            "James Zhao",
            "Yuxi Xie",
            "Kenji Kawaguchi",
            "Junxian He",
            "Michael Xie"
        ],
        "published": "2023",
        "summary": "Chain-of-Thought (CoT) and Program-Aided Language Models (PAL) represent two distinct reasoning methods, each with its own strengths. CoT employs natural language, offering flexibility and interpretability, while PAL utilizes programming language, yielding more structured and rigorous logic. We introduce a model selection method to combine the best of both worlds by employing a large language model (LLM) to dynamically select between them. Our theoretical analysis underscores the feasibility of this method, which is further corroborated by empirical results. Our proposed method demonstrates significant performance improvements across eight reasoning datasets with Codex, ChatGPT, and GPT-4. Additionally, our method is complementary to self-consistency; when integrated, it can further enhance performance while significantly reducing computation costs. Moreover, we achieve new state-of-the-art results on GSM8K and SVAMP, with respective accuracies of 96.8% and 93.7%.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.55.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "Improving Question Generation with Multi-level Content Planning",
        "authors": [
            "Zehua Xia",
            "Qi Gou",
            "Bowen Yu",
            "Haiyang Yu",
            "Fei Huang",
            "Yongbin Li",
            "Nguyen Cam-Tu"
        ],
        "published": "2023",
        "summary": "This paper addresses the problem of generating questions from a given context and an answer, specifically focusing on questions that require multi-hop reasoning across an extended context. Previous studies have suggested that key phrase selection is essential for question generation (QG), yet it is still challenging to connect such disjointed phrases into meaningful questions, particularly for long context. To mitigate this issue, we propose MultiFactor, a novel QG framework based on multi-level content planning. Specifically, MultiFactor includes two components: FA-Model, which simultaneously selects key phrases and generates full answers, and Q-Model which takes the generated full answer as an additional input to generate questions. Here, full answer generation is introduced to connect the short answer with the selected key phrases, thus forming an answer-aware summary to facilitate QG. Both FA-Model and Q-Model are formalized as simple-yet-effective Phrase-Enhanced Transformers, our joint model for phrase selection and text generation. Experimental results show that our method outperforms strong baselines on two popular QG datasets. Our code is available at https://github.com/zeaver/MultiFactor.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.57.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Is ChatGPT a Financial Expert? Evaluating Language Models on Financial Natural Language Processing",
        "authors": [
            "Yue Guo",
            "Zian Xu",
            "Yi Yang"
        ],
        "published": "2023",
        "summary": "The emergence of Large Language Models (LLMs), such as ChatGPT, has revolutionized general natural language preprocessing (NLP) tasks. However, their expertise in the financial domain lacks a comprehensive evaluation. To assess the ability of LLMs to solve financial NLP tasks, we present FinLMEval, a framework for Financial Language Model Evaluation, comprising nine datasets designed to evaluate the performance of language models. This study compares the performance of fine-tuned auto-encoding language models (BERT, RoBERTa, FinBERT) and the LLM ChatGPT. Our findings reveal that while ChatGPT demonstrates notable performance across most financial tasks, it generally lags behind the fine-tuned expert models, especially when dealing with proprietary datasets. We hope this study builds foundation evaluation benchmarks for continuing efforts to build more advanced LLMs in the financial domain.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.58.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our findings reveal that while ChatGPT demonstrates notable performance across most financial tasks, it generally lags behind the fine-tuned expert models, especially when dealing with proprietary datasets.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Our findings reveal that while ChatGPT demonstrates notable performance across most financial tasks, it generally lags behind the fine-tuned expert models, especially when dealing with proprietary datasets.\""
    },
    {
        "title": "DelucionQA: Detecting Hallucinations in Domain-specific Question Answering",
        "authors": [
            "Mobashir Sadat",
            "Zhengyu Zhou",
            "Lukas Lange",
            "Jun Araki",
            "Arsalan Gundroo",
            "Bingqing Wang",
            "Rakesh Menon",
            "Md Parvez",
            "Zhe Feng"
        ],
        "published": "2023",
        "summary": "Hallucination is a well-known phenomenon in text generated by large language models (LLMs). The existence of hallucinatory responses is found in almost all application scenarios e.g., summarization, question-answering (QA) etc. For applications requiring high reliability (e.g., customer-facing assistants), the potential existence of hallucination in LLM-generated text is a critical problem. The amount of hallucination can be reduced by leveraging information retrieval to provide relevant background information to the LLM. However, LLMs can still generate hallucinatory content for various reasons (e.g., prioritizing its parametric knowledge over the context, failure to capture the relevant information from the context, etc.). Detecting hallucinations through automated methods is thus paramount. To facilitate research in this direction, we introduce a sophisticated dataset, DelucionQA, that captures hallucinations made by retrieval-augmented LLMs for a domain-specific QA task. Furthermore, we propose a set of hallucination detection methods to serve as baselines for future works from the research community. Analysis and case study are also provided to share valuable insights on hallucination phenomena in the target scenario.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.59.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The existence of hallucinatory responses is found in almost all application scenarios e.g., summarization, question-answering (QA) etc. For applications requiring high reliability (e.g., customer-facing assistants), the potential existence of hallucination in LLM-generated text is a critical problem.... LLMs can still generate hallucinatory content for various reasons (e.g",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"The existence of hallucinatory responses is found in almost all application scenarios e.g., summarization, question-answering (QA) etc. For applications requiring high reliability (e.g., customer-facing assistants), the potential existence of hallucination in LLM-generated text is a critical problem.... LLMs can still generate hallucinatory content for various reasons (e.g"
    },
    {
        "title": "Dissecting In-Context Learning of Translations in GPT-3",
        "authors": [
            "Vikas Raunak",
            "Arul Menezes",
            "Hany Awadalla"
        ],
        "published": "2023",
        "summary": "Most of the recent work in leveraging Large Language Models (LLMs) such as GPT-3 for Machine Translation (MT) has focused on selecting the few-shot samples for prompting. In this work, we try to better understand the role of demonstration attributes for the in-context learning of translations through perturbations of high-quality, in-domain demonstrations. We find that asymmetric perturbation of the source-target mappings yield vastly different results. We show that the perturbation of the source side has surprisingly little impact, while target perturbation can drastically reduce translation quality, suggesting that it is the output text distribution that provides the most important learning signal during in-context learning of translations. We propose a method named Zero-Shot-Context to add this signal automatically in Zero-Shot prompting. We demonstrate that it improves upon the zero-shot translation performance of GPT-3, even making it competitive with few-shot prompted translations.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.61.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"target perturbation can drastically reduce translation quality\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"target perturbation can drastically reduce translation quality\""
    },
    {
        "title": "TESTA: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding",
        "authors": [
            "Shuhuai Ren",
            "Sishuo Chen",
            "Shicheng Li",
            "Xu Sun",
            "Lu Hou"
        ],
        "published": "2023",
        "summary": "Large-scale video-language pre-training has made remarkable strides in advancing video-language understanding tasks. However, the heavy computational burden of video encoding remains a formidable efficiency bottleneck, particularly for long-form videos. These videos contain massive visual tokens due to their inherent 3D properties and spatiotemporal redundancy, making it challenging to capture complex temporal and spatial relationships. To tackle this issue, we propose an efficient method called TEmporal-Spatial Token Aggregation (TESTA). TESTA condenses video semantics by adaptively aggregating similar frames, as well as similar patches within each frame. TESTA can reduce the number of visual tokens by 75% and thus accelerate video encoding. Building upon TESTA, we introduce a pre-trained video-language model equipped with a divided space-time token aggregation module in each video encoder block. We evaluate our model on five datasets for paragraph-to-video retrieval and long-form VideoQA tasks. Experimental results show that TESTA improves computing efficiency by 1.7 times, and achieves significant performance gains from its scalability in processing longer input frames, e.g., +13.7 R@1 on QuerYD and +6.5 R@1 on Condensed Movie.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.66.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the heavy computational burden of video encoding remains a formidable efficiency bottleneck, particularly for long-form videos.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the heavy computational burden of video encoding remains a formidable efficiency bottleneck, particularly for long-form videos.\""
    },
    {
        "title": "Fusing Temporal Graphs into Transformers for Time-Sensitive Question Answering",
        "authors": [
            "Xin Su",
            "Phillip Howard",
            "Nagib Hakim",
            "Steven Bethard"
        ],
        "published": "2023",
        "summary": "Answering time-sensitive questions from long documents requires temporal reasoning over the times in questions and documents. An important open question is whether large language models can perform such reasoning solely using a provided text document, or whether they can benefit from additional temporal information extracted using other systems. We address this research question by applying existing temporal information extraction systems to construct temporal graphs of events, times, and temporal relations in questions and documents. We then investigate different approaches for fusing these graphs into Transformer models. Experimental results show that our proposed approach for fusing temporal graphs into input text substantially enhances the temporal reasoning capabilities of Transformer models with or without fine-tuning. Additionally, our proposed method outperforms various graph convolution-based approaches and establishes a new state-of-the-art performance on SituatedQA and three splits of TimeQA.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.67.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"An important open question is whether large language models can perform such reasoning solely using a provided text document, or whether they can benefit from additional temporal information extracted using other systems.\"\n\nThis rating is given because the abstract mentions a limitation of large language models (their ability to perform temporal reasoning solely using a provided text document) in passing, but does not elaborate on it further. The primary focus",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"An important open question is whether large language models can perform such reasoning solely using a provided text document, or whether they can benefit from additional temporal information extracted using other systems.\"\n\nThis rating is given because the abstract mentions a limitation of large language models (their ability to perform temporal reasoning solely using a provided text document) in passing, but does not elaborate on it further. The primary focus"
    },
    {
        "title": "The Internal State of an LLM Knows When It’s Lying",
        "authors": [
            "Amos Azaria",
            "Tom Mitchell"
        ],
        "published": "2023",
        "summary": "While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM’s internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71% to 83% accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier’s performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.68.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"one of their most prominent drawbacks is generating inaccurate or false information with a confident tone.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"one of their most prominent drawbacks is generating inaccurate or false information with a confident tone.\""
    },
    {
        "title": "Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models",
        "authors": [
            "Ziqiao Ma",
            "Jacob Sansom",
            "Run Peng",
            "Joyce Chai"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have generated considerable interest and debate regarding their potential emergence of Theory of Mind (ToM). Several recent inquiries reveal a lack of robust ToM in these models and pose a pressing demand to develop new benchmarks, as current ones primarily focus on different aspects of ToM and are prone to shortcuts and data leakage. In this position paper, we seek to answer two road-blocking questions: (1) How can we taxonomize a holistic landscape of machine ToM? (2) What is a more effective evaluation protocol for machine ToM? Following psychological studies, we taxonomize machine ToM into 7 mental state categories and delineate existing benchmarks to identify under-explored aspects of ToM. We argue for a holistic and situated evaluation of ToM to break ToM into individual components and treat LLMs as an agent who is physically situated in environments and socially situated in interactions with humans. Such situated evaluation provides a more comprehensive assessment of mental states and potentially mitigates the risk of shortcuts and data leakage. We further present a pilot study in a grid world setup as a proof of concept. We hope this position paper can facilitate future research to integrate ToM with LLMs and offer an intuitive means for researchers to better position their work in the landscape of ToM.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.72.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Several recent inquiries reveal a lack of robust ToM in these models and pose a pressing demand to develop new benchmarks, as current ones primarily focus on different aspects of ToM and are prone to shortcuts and data leakage.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Several recent inquiries reveal a lack of robust ToM in these models and pose a pressing demand to develop new benchmarks, as current ones primarily focus on different aspects of ToM and are prone to shortcuts and data leakage.\""
    },
    {
        "title": "IRFL: Image Recognition of Figurative Language",
        "authors": [
            "Ron Yosef",
            "Yonatan Bitton",
            "Dafna Shahaf"
        ],
        "published": "2023",
        "summary": "Figures of speech such as metaphors, similes, and idioms are integral parts of human communication. They are ubiquitous in many forms of discourse, allowing people to convey complex, abstract ideas and evoke emotion. As figurative forms are often conveyed through multiple modalities (e.g., both text and images), understanding multimodal figurative language is an important AI challenge, weaving together profound vision, language, commonsense and cultural knowledge. In this work, we develop the Image Recognition of Figurative Language (IRFL) dataset. We leverage human annotation and an automatic pipeline we created to generate a multimodal dataset, and introduce two novel tasks as a benchmark for multimodal figurative language understanding. We experimented with state-of-the-art vision and language models and found that the best (22%) performed substantially worse than humans (97%). We release our dataset, benchmark, and code in hopes of driving the development of models that can better understand figurative language.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.74.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We experimented with state-of-the-art vision and language models and found that the best (22%) performed substantially worse than humans (97%).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We experimented with state-of-the-art vision and language models and found that the best (22%) performed substantially worse than humans (97%).\""
    },
    {
        "title": "Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization",
        "authors": [
            "Kaihang Pan",
            "Juncheng Li",
            "Hongye Song",
            "Jun Lin",
            "Xiaozhong Liu",
            "Siliang Tang"
        ],
        "published": "2023",
        "summary": "Prompt tuning is a parameter-efficient method, which learns soft prompts and conditions frozen language models to perform specific downstream tasks. Though effective, prompt tuning under few-shot settings on the one hand heavily relies on a good initialization of soft prompts. On the other hand, it can easily overfit to few-shot training samples, thereby undermining generalizability. Existing works leverage pre-training or supervised meta-learning to initialize soft prompts but they fail to data-efficiently generalize to unseen downstream tasks. To address the above problems, this paper proposes a novel Self-sUpervised meta-Prompt learning framework with MEta-gradient Regularization for few-shot generalization (SUPMER). SUPMER leverages self-supervised meta-learning with a diverse set of well-designed meta-tasks to learn a universal prompt initialization for efficient adaptation using only unlabeled data. Additionally, it jointly meta-learns a gradient regularization function to transform raw gradients into a domain-generalizable direction, thus alleviating the problem of overfitting. Extensive experiments show that SUPMER achieves better performance for different few-shot downstream tasks, and also exhibits a stronger domain generalization ability. The code for SUPMER will be available at https://github.com/beepkh/SUPMER.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.75.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"On the other hand, it can easily overfit to few-shot training samples, thereby undermining generalizability.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"On the other hand, it can easily overfit to few-shot training samples, thereby undermining generalizability.\""
    },
    {
        "title": "An Adaptive Prompt Generation Framework for Task-oriented Dialogue System",
        "authors": [
            "Jun Gao",
            "Liuyu Xiang",
            "Huijia Wu",
            "Han Zhao",
            "Yiqi Tong",
            "Zhaofeng He"
        ],
        "published": "2023",
        "summary": "The de facto way of utilizing black-box large language models (LLMs) to perform various downstream tasks is prompting. However, obtaining suitable prompts for specific tasks is still a challenging problem. While existing LLM-based methods demonstrate promising performance in task-oriented dialogue (TOD) task, they often require manual adjustment in prompt selection, or focus solely on dialogue understanding or generation. To address these issues, we propose an adaptive prompt generation framework to fully unleash the potential of LLMs for the comprehensive TOD system. Firstly, we design a trainable slot generator (TSG) that can generate domain and slot information in the belief state, which serves as prior knowledge for subsequent prompt generation. Next, we propose an adaptive prompt generator (APG) that utilizes the prior knowledge to generate prompts for the LLM, deriving the belief state and system response of the dialogue for evaluation. Finally, we evaluate our framework on the MultiWOZ 2.0 dataset. Extensive experiments demonstrate that our method outperforms existing methods. Our code and data will be released.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.76.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, obtaining suitable prompts for specific tasks is still a challenging problem.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, obtaining suitable prompts for specific tasks is still a challenging problem.\""
    },
    {
        "title": "Adaptive Structure Induction for Aspect-based Sentiment Analysis with Spectral Perspective",
        "authors": [
            "Hao Niu",
            "Yun Xiong",
            "Xiaosu Wang",
            "Wenjing Yu",
            "Yao Zhang",
            "Zhonglei Guo"
        ],
        "published": "2023",
        "summary": "Recently, incorporating structure information (e.g. dependency syntactic tree) can enhance the performance of aspect-based sentiment analysis (ABSA). However, this structure information is obtained from off-the-shelf parsers, which is often sub-optimal and cumbersome. Thus, automatically learning adaptive structures is conducive to solving this problem. In this work, we concentrate on structure induction from pre-trained language models (PLMs) and throw the structure induction into a spectrum perspective to explore the impact of scale information in language representation on structure induction ability. Concretely, the main architecture of our model is composed of commonly used PLMs (e.g. RoBERTa, etc), and a simple yet effective graph structure learning (GSL) module (graph learner + GNNs). Subsequently, we plug in spectral filters with different bands respectively after the PLMs to produce filtered language representations and feed them into the GSL module to induce latent structures. We conduct extensive experiments on three public benchmarks for ABSA. The results and further analyses demonstrate that introducing this spectral approach can shorten Aspects-sentiment Distance (AsD) and be beneficial to structure induction. Even based on such a simple framework, the effects on three datasets can reach SOTA (state of the art) or near SOTA performance. Additionally, our exploration also has the potential to be generalized to other tasks or to bring inspiration to other similar domains.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.79.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit evidence of limitations of LLMs, but mentions \"structure information is obtained from off-the-shelf parsers, which is often sub-optimal and cumbersome\" which is related to the general task of using pre-trained language models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit evidence of limitations of LLMs, but mentions \"structure information is obtained from off-the-shelf parsers, which is often sub-optimal and cumbersome\" which is related to the general task of using pre-trained language models."
    },
    {
        "title": "NovaCOMET: Open Commonsense Foundation Models with Symbolic Knowledge Distillation",
        "authors": [
            "Peter West",
            "Ronan Bras",
            "Taylor Sorensen",
            "Bill Lin",
            "Liwei Jiang",
            "Ximing Lu",
            "Khyathi Chandu",
            "Jack Hessel",
            "Ashutosh Baheti",
            "Chandra Bhagavatula",
            "Yejin Choi"
        ],
        "published": "2023",
        "summary": "We present NovaCOMET, an open commonsense knowledge model, that combines the best aspects of knowledge and general task models. Compared to previous knowledge models, NovaCOMET allows open-format relations enabling direct application to reasoning tasks; compared to general task models like Flan-T5, it explicitly centers knowledge, enabling superior performance for commonsense reasoning. NovaCOMET leverages the knowledge of opaque proprietary models to create an open knowledge pipeline. First, knowledge is symbolically distilled into NovATOMIC, a publicly-releaseddiscrete knowledge graph which can be audited, critiqued, and filtered. Next, we train NovaCOMET on NovATOMIC by fine-tuning an open-source pretrained model. NovaCOMET uses an open-format training objective, replacing the fixed relation sets of past knowledge models, enabling arbitrary structures within the data to serve as inputs or outputs. The resulting generation model, optionally augmented with human annotation, matches or exceeds comparable open task models like Flan-T5 on a range of commonsense generation tasks. NovaCOMET serves as a counterexample to the contemporary focus on instruction tuning only, demonstrating a distinct advantage to explicitly modeling commonsense knowledge as well.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.80.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but the title and abstract imply that previous knowledge models and general task models like Flan-T5 have limitations, such as fixed relation sets and lack of explicit knowledge modeling.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations, but the title and abstract imply that previous knowledge models and general task models like Flan-T5 have limitations, such as fixed relation sets and lack of explicit knowledge modeling."
    },
    {
        "title": "In-Context Demonstration Selection with Cross Entropy Difference",
        "authors": [
            "Dan Iter",
            "Reid Pryzant",
            "Ruochen Xu",
            "Shuohang Wang",
            "Yang Liu",
            "Yichong Xu",
            "Chenguang Zhu"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) can use in-context demonstrations to improve performance on zero-shot tasks. However, selecting the best in-context examples is challenging because model performance can vary widely depending on the selected examples. We present a cross-entropy difference (CED) method for selecting in-context demonstrations. Our method is based on the observation that the effectiveness of in-context demonstrations negatively correlates with the perplexity of the test example by a language model that was finetuned on that demonstration. We utilize parameter efficient finetuning to train small models on training data that are used for computing the cross-entropy difference between a test example and every candidate in-context demonstration. This metric is used to rank and select in-context demonstrations independently for each test input. We evaluate our method on a mix-domain dataset that combines 8 benchmarks, representing 4 text generation tasks, showing that CED for in-context demonstration selection can improve performance for a variety of LLMs over baseline selection methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.81.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, selecting the best in-context examples is challenging because model performance can vary widely depending on the selected examples.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, selecting the best in-context examples is challenging because model performance can vary widely depending on the selected examples.\""
    },
    {
        "title": "SoulChat: Improving LLMs’ Empathy, Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations",
        "authors": [
            "Yirong Chen",
            "Xiaofen Xing",
            "Jingkai Lin",
            "Huimin Zheng",
            "Zhenyu Wang",
            "Qi Liu",
            "Xiangmin Xu"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have been widely applied in various fields due to their excellent capability for memorizing knowledge and chain of thought (CoT). When these language models are applied in the field of psychological counseling, they often rush to provide universal advice. However, when users seek psychological support, they need to gain empathy, trust, understanding and comfort, rather than just reasonable advice. To this end, we constructed a multi-turn empathetic conversation dataset of more than 2 million samples, in which the input is the multi-turn conversation context, and the target is empathetic responses that cover expressions such as questioning, comfort, recognition, listening, trust, emotional support, etc. Experiments have shown that the empathy ability of LLMs can be significantly enhanced when finetuning by using multi-turn dialogue history and responses that are closer to the expression of a psychological consultant.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.83.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, when users seek psychological support, they need to gain empathy, trust, understanding and comfort, rather than just reasonable advice.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, when users seek psychological support, they need to gain empathy, trust, understanding and comfort, rather than just reasonable advice.\""
    },
    {
        "title": "Can ChatGPT Assess Human Personalities? A General Evaluation Framework",
        "authors": [
            "Haocong Rao",
            "Cyril Leung",
            "Chunyan Miao"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) especially ChatGPT have produced impressive results in various areas, but their potential human-like psychology is still largely unexplored. Existing works study the virtual personalities of LLMs but rarely explore the possibility of analyzing human personalities via LLMs. This paper presents a generic evaluation framework for LLMs to assess human personalities based on Myers–Briggs Type Indicator (MBTI) tests. Specifically, we first devise unbiased prompts by randomly permuting options in MBTI questions and adopt the average testing result to encourage more impartial answer generation. Then, we propose to replace the subject in question statements to enable flexible queries and assessments on different subjects from LLMs. Finally, we re-formulate the question instructions in a manner of correctness evaluation to facilitate LLMs to generate clearer responses. The proposed framework enables LLMs to flexibly assess personalities of different groups of people. We further propose three evaluation metrics to measure the consistency, robustness, and fairness of assessment results from state-of-the-art LLMs including ChatGPT and GPT-4. Our experiments reveal ChatGPT’s ability to assess human personalities, and the average results demonstrate that it can achieve more consistent and fairer assessments in spite of lower robustness against prompt biases compared with InstructGPT.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.84.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our experiments... demonstrate that it can achieve more consistent and fairer assessments in spite of lower robustness against prompt biases compared with InstructGPT.\"\n\nThis rating is based on the fact that the paper briefly mentions a limitation of LLMs (lower robustness against prompt biases), but it is not the primary focus of the paper. The main focus is on the proposed framework and",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our experiments... demonstrate that it can achieve more consistent and fairer assessments in spite of lower robustness against prompt biases compared with InstructGPT.\"\n\nThis rating is based on the fact that the paper briefly mentions a limitation of LLMs (lower robustness against prompt biases), but it is not the primary focus of the paper. The main focus is on the proposed framework and"
    },
    {
        "title": "MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model",
        "authors": [
            "Le Zhang",
            "Yihong Wu",
            "Fengran Mo",
            "Jian-Yun Nie",
            "Aishwarya Agrawal"
        ],
        "published": "2023",
        "summary": "Multi-modal open-domain question answering typically requires evidence retrieval from databases across diverse modalities, such as images, tables, passages, etc. Even Large Language Models (LLMs) like GPT-4 fall short in this task. To enable LLMs to tackle the task in a zero-shot manner, we introduce MoqaGPT, a straightforward and flexible framework. Using a divide-and-conquer strategy that bypasses intricate multi-modality ranking, our framework can accommodate new modalities and seamlessly transition to new models for the task. Built upon LLMs, MoqaGPT retrieves and extracts answers from each modality separately, then fuses this multi-modal information using LLMs to produce a final answer. Our methodology boosts performance on the MMCoQA dataset, improving F1 by +37.91 points and EM by +34.07 points over the supervised baseline. On the MultiModalQA dataset, MoqaGPT surpasses the zero-shot baseline, improving F1 by 9.5 points and EM by 10.1 points, and significantly closes the gap with supervised methods. Our codebase is available at https://github.com/lezhang7/MOQAGPT.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.85.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Even Large Language Models (LLMs) like GPT-4 fall short in this task.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Even Large Language Models (LLMs) like GPT-4 fall short in this task.\""
    },
    {
        "title": "Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search",
        "authors": [
            "Kelong Mao",
            "Zhicheng Dou",
            "Fengran Mo",
            "Jiewen Hou",
            "Haonan Chen",
            "Hongjin Qian"
        ],
        "published": "2023",
        "summary": "Precisely understanding users’ contextual search intent has been an important challenge for conversational search. As conversational search sessions are much more diverse and long-tailed, existing methods trained on limited data still show unsatisfactory effectiveness and robustness to handle real conversational search scenarios. Recently, large language models (LLMs) have demonstrated amazing capabilities for text generation and conversation understanding. In this work, we present a simple yet effective prompting framework, called LLM4CS, to leverage LLMs as a text-based search intent interpreter to help conversational search. Under this framework, we explore three prompting methods to generate multiple query rewrites and hypothetical responses, and propose to aggregate them into an integrated representation that can robustly represent the user’s real contextual search intent. Extensive automatic evaluations and human evaluations on three widely used conversational search benchmarks, including CAsT-19, CAsT-20, and CAsT-21, demonstrate the remarkable performance of our simple LLM4CS framework compared with existing methods and even using human rewrites. Our findings provide important evidence to better understand and leverage LLMs for conversational search.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.86.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract discusses the capabilities of LLMs and presents a framework to leverage them, but does not mention any limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract discusses the capabilities of LLMs and presents a framework to leverage them, but does not mention any limitations."
    },
    {
        "title": "DocAsRef: An Empirical Study on Repurposing Reference-based Summary Quality Metrics as Reference-free Metrics",
        "authors": [
            "Forrest Bao",
            "Ruixuan Tu",
            "Ge Luo",
            "Yinfei Yang",
            "Hebi Li",
            "Minghui Qiu",
            "Youbiao He",
            "Cen Chen"
        ],
        "published": "2023",
        "summary": "Automated summary quality assessment falls into two categories: reference-based and reference-free. Reference-based metrics, historically deemed more accurate due to the additional information provided by human-written references, are limited by their reliance on human input. In this paper, we hypothesize that the comparison methodologies used by some reference-based metrics to evaluate a system summary against its corresponding reference can be effectively adapted to assess it against its source document, thereby transforming these metrics into reference-free ones. Experimental results support this hypothesis. After being repurposed reference-freely, the zero-shot BERTScore using the pretrained DeBERTa-large-MNLI model of <0.5B parameters consistently outperforms its original reference-based version across various aspects on the SummEval and Newsroom datasets. It also excels in comparison to most existing reference-free metrics and closely competes with zero-shot summary evaluators based on GPT-3.5.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.87.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"zero-shot summary evaluators based on GPT-3.5.\"\n\nThis paper discusses LLMs, but only mentions them in passing as a basis for comparison, without discussing any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"zero-shot summary evaluators based on GPT-3.5.\"\n\nThis paper discusses LLMs, but only mentions them in passing as a basis for comparison, without discussing any limitations of LLMs."
    },
    {
        "title": "Toxicity in chatgpt: Analyzing persona-assigned language models",
        "authors": [
            "Ameet Deshpande",
            "Vishvak Murahari",
            "Tanmay Rajpurohit",
            "Ashwin Kalyan",
            "Karthik Narasimhan"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Legislation has recognized its significance and recently drafted a “Blueprint For An AI Bill Of Rights” which calls for domain experts to identify risks and potential impact of AI systems. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to 6×, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. Furthermore, we find concerning patterns where specific entities (e.g., certain races) are targeted more than others (3× more) irrespective of the assigned persona, reflecting discriminatory biases in the model. Our findings show that multiple provisions in the legislative blueprint are being violated, and we hope that the broader AI community rethinks the efficacy of current safety guardrails and develops better techniques that lead to robust, safe, and trustworthy AI.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.88.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to 6×, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. Furthermore, we find concerning patterns where specific entities",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to 6×, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. Furthermore, we find concerning patterns where specific entities"
    },
    {
        "title": "Execution-Based Evaluation for Open-Domain Code Generation",
        "authors": [
            "Zhiruo Wang",
            "Shuyan Zhou",
            "Daniel Fried",
            "Graham Neubig"
        ],
        "published": "2023",
        "summary": "To extend the scope of coding queries to more realistic settings, we propose ODEX, the first Open-Domain EXecution-based natural language (NL) to Python code generation dataset. ODEX has 945 NL-Code pairs spanning 79 diverse libraries, along with 1,707 human-written test cases for execution. Our NL-Code pairs are harvested from StackOverflow forums to encourage natural and practical coding queries. Moreover, ODEX supports four natural languages as intents, in English, Spanish, Japanese, and Russian. ODEX unveils intriguing behavioral differences among top-performing code language models (LM). While CODEX achieves better overall results, CODEGEN improves effectively via scaling – CODEGEN 6.1B performs comparably with CODEX 12B. Both models show substantial gaps between open and closed domains, but CODEGEN gaps tend to decrease with model size while CODEX gaps increase. We release ODEX to facilitate research into open-domain problems for the code generation community.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.89.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While CODEX achieves better overall results, CODEGEN improves effectively via scaling – CODEGEN 6.1B performs comparably with CODEX 12B. Both models show substantial gaps between open and closed domains, but CODEGEN gaps tend to decrease with model size while CODEX gaps increase.\"\n\nThis rating is given because the abstract mentions a limitation of the models (gaps between",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While CODEX achieves better overall results, CODEGEN improves effectively via scaling – CODEGEN 6.1B performs comparably with CODEX 12B. Both models show substantial gaps between open and closed domains, but CODEGEN gaps tend to decrease with model size while CODEX gaps increase.\"\n\nThis rating is given because the abstract mentions a limitation of the models (gaps between"
    },
    {
        "title": "Is Explanation the Cure? Misinformation Mitigation in the Short Term and Long Term",
        "authors": [
            "Yi-Li Hsu",
            "Shih-Chieh Dai",
            "Aiping Xiong",
            "Lun-Wei Ku"
        ],
        "published": "2023",
        "summary": "With advancements in natural language processing (NLP) models, automatic explanation generation has been proposed to mitigate misinformation on social media platforms in addition to adding warning labels to identified fake news. While many researchers have focused on generating good explanations, how these explanations can really help humans combat fake news is under-explored. In this study, we compare the effectiveness of a warning label and the state-of- the-art counterfactual explanations generated by GPT-4 in debunking misinformation. In a two-wave, online human-subject study, participants (N = 215) were randomly assigned to a control group in which false contents are shown without any intervention, a warning tag group in which the false claims were labeled, or an explanation group in which the false contents were accompanied by GPT-4 generated explanations. Our results show that both interventions significantly decrease participants’ self-reported belief in fake claims in an equivalent manner for the short-term and long-term. We discuss the implications of our findings and directions for future NLP-based misinformation debunking strategies.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.92.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but mentions the use of GPT-4, a type of LLM, in generating counterfactual explanations, without discussing its limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but mentions the use of GPT-4, a type of LLM, in generating counterfactual explanations, without discussing its limitations."
    },
    {
        "title": "How Reliable Are AI-Generated-Text Detectors? An Assessment Framework Using Evasive Soft Prompts",
        "authors": [
            "Tharindu Kumarage",
            "Paras Sheth",
            "Raha Moraffah",
            "Joshua Garland",
            "Huan Liu"
        ],
        "published": "2023",
        "summary": "In recent years, there has been a rapid proliferation of AI-generated text, primarily driven by the release of powerful pre-trained language models (PLMs). To address the issue of misuse associated with AI-generated text, various high-performing detectors have been developed, including the OpenAI detector and the Stanford DetectGPT. In our study, we ask how reliable these detectors are. We answer the question by designing a novel approach that can prompt any PLM to generate text that evades these high-performing detectors. The proposed approach suggests a universal evasive prompt, a novel type of soft prompt, which guides PLMs in producing “human-like” text that can mislead the detectors. The novel universal evasive prompt is achieved in two steps: First, we create an evasive soft prompt tailored to a specific PLM through prompt tuning; and then, we leverage the transferability of soft prompts to transfer the learned evasive soft prompt from one PLM to another. Employing multiple PLMs in various writing tasks, we conduct extensive experiments to evaluate the efficacy of the evasive soft prompts in their evasion of state-of-the-art detectors.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.94.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"To address the issue of misuse associated with AI-generated text, various high-performing detectors have been developed, including the OpenAI detector and the Stanford DetectGPT.... Employing multiple PLMs in various writing tasks, we conduct extensive experiments to evaluate the efficacy of the evasive soft prompts in their evasion of state-of-the-art detectors.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"To address the issue of misuse associated with AI-generated text, various high-performing detectors have been developed, including the OpenAI detector and the Stanford DetectGPT.... Employing multiple PLMs in various writing tasks, we conduct extensive experiments to evaluate the efficacy of the evasive soft prompts in their evasion of state-of-the-art detectors.\""
    },
    {
        "title": "Knowledge is a Region in Weight Space for Fine-tuned Language Models",
        "authors": [
            "Almog Gueta",
            "Elad Venezian",
            "Colin Raffel",
            "Noam Slonim",
            "Yoav Katz",
            "Leshem Choshen"
        ],
        "published": "2023",
        "summary": "Research on neural networks has focused on understanding a single model trained on a single dataset. However, relatively little is known about the relationships between different models, particularly those trained or tested on different datasets. We address this by studying how the weight space and the underlying loss landscape of different models are interconnected. Specifically, we demonstrate that finetuned models that were optimized for high performance, reside in well-defined regions in weight space, and vice versa – that any model that resides anywhere in those regions also exhibits high performance. Notably, we show that language models that have been finetuned on the same dataset form a tight cluster in the weight space, while models finetuned on different datasets from the same underlying task form a looser cluster. Moreover, traversing around the region between the models leads to new models that perform comparably or even better than models obtained via finetuning, even on tasks that the original models were not finetuned on. Our findings provide insight into the relationships between models, demonstrating that a model positioned between two similar models can acquire the knowledge of both. We leverage this and design a method for selecting a better model for efficient finetuning. Specifically, we show that starting from the center of the region is as effective, if not more, than using the pretrained model in 11 out of 12 datasets, resulting in an average accuracy improvement of 3.06.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.95.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation of LLMs is mentioned in the abstract, but it does mention that \"models finetuned on different datasets from the same underlying task form a looser cluster\", which could be seen as a limitation in terms of adaptability of LLMs to different datasets.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation of LLMs is mentioned in the abstract, but it does mention that \"models finetuned on different datasets from the same underlying task form a looser cluster\", which could be seen as a limitation in terms of adaptability of LLMs to different datasets."
    },
    {
        "title": "On the Risk of Misinformation Pollution with Large Language Models",
        "authors": [
            "Yikang Pan",
            "Liangming Pan",
            "Wenhu Chen",
            "Preslav Nakov",
            "Min-Yen Kan",
            "William Wang"
        ],
        "published": "2023",
        "summary": "We investigate the potential misuse of modern Large Language Models (LLMs) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly Open-Domain Question Answering (ODQA) systems. We establish a threat model and simulate potential misuse scenarios, both unintentional and intentional, to assess the extent to which LLMs can be utilized to produce misinformation. Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation (up to 87%) in the performance of ODQA systems. Moreover, we uncover disparities in the attributes associated with persuading humans and machines, presenting an obstacle to current human-centric approaches to combat misinformation. To mitigate the harm caused by LLM-generated misinformation, we propose three defense strategies: misinformation detection, vigilant prompting, and reader ensemble. These approaches have demonstrated promising results, albeit with certain associated costs. Lastly, we discuss the practicality of utilizing LLMs as automatic misinformation generators and provide relevant resources and code to facilitate future research in this area.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.97.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We investigate the potential misuse of modern Large Language Models (LLMs) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly Open-Domain Question Answering (ODQA) systems... Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation (up to 87%) in the performance of ODQA systems.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We investigate the potential misuse of modern Large Language Models (LLMs) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly Open-Domain Question Answering (ODQA) systems... Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation (up to 87%) in the performance of ODQA systems.\""
    },
    {
        "title": "MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models",
        "authors": [
            "Yifan Wei",
            "Yisong Su",
            "Huanhuan Ma",
            "Xiaoyan Yu",
            "Fangyu Lei",
            "Yuanzhe Zhang",
            "Jun Zhao",
            "Kang Liu"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have shown nearly saturated performance on many natural language processing (NLP) tasks. As a result, it is natural for people to believe that LLMs have also mastered abilities such as time understanding and reasoning. However, research on the temporal sensitivity of LLMs has been insufficiently emphasized. To fill this gap, this paper constructs Multiple Sensitive Factors Time QA (MenatQA), which encompasses three temporal factors (scope factor, order factor, counterfactual factor) with total 2,853 samples for evaluating the time comprehension and reasoning abilities of LLMs. This paper tests current mainstream LLMs with different parameter sizes, ranging from billions to hundreds of billions. The results show most LLMs fall behind smaller temporal reasoning models with different degree on these factors. In specific, LLMs show a significant vulnerability to temporal biases and depend heavily on the temporal information provided in questions. Furthermore, this paper undertakes a preliminary investigation into potential improvement strategies by devising specific prompts and leveraging external tools. These approaches serve as valuable baselines or references for future research endeavors.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.100.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, research on the temporal sensitivity of LLMs has been insufficiently emphasized.\"; \"The results show most LLMs fall behind smaller temporal reasoning models with different degree on these factors.\"; \"In specific, LLMs show a significant vulnerability to temporal biases and depend heavily on the temporal information provided in questions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, research on the temporal sensitivity of LLMs has been insufficiently emphasized.\"; \"The results show most LLMs fall behind smaller temporal reasoning models with different degree on these factors.\"; \"In specific, LLMs show a significant vulnerability to temporal biases and depend heavily on the temporal information provided in questions.\""
    },
    {
        "title": "What Makes Chain-of-Thought Prompting Effective? A Counterfactual Study",
        "authors": [
            "Aman Madaan",
            "Katherine Hermann",
            "Amir Yazdanbakhsh"
        ],
        "published": "2023",
        "summary": "The effectiveness of Chain-of-thought prompting (CoT) has been widely recognized, but the underlying mechanisms behind its success, the reason why it just works for a wide range of tasks, remains an open question. To investigate this, we employ a counterfactual prompting approach, systematically manipulating elements of examples used in a few-shot prompt, and testing the consequences on model behavior. This allows us to understand the relative contributions of prompt elements such as symbols (digits, entities) and patterns (equations, sentence structure) on in-context learning. Our experiments with three different large language models (LLMs) reveal several key findings. First, the specific symbols used in the prompt do not significantly impact the model’s performance. However, consistent patterns in examples and specifying text in style frequently found on the web are crucial. Second, our findings suggest that the necessity of accurate few-shot examples depends on their role in communicating task understanding. We identify tasks where inaccurate few-shot examples hurt and, surprisingly, tasks where they improve performance. Additionally, we find that the intermediate steps in CoT may not necessarily facilitate learning how to solve a task, but instead efficiently convey task understanding (what) to the model. Furthermore, CoT leverages LLMs to fill in missing commonsense information, particularly helping difficult reasoning problems and long-tail questions.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.101.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, consistent patterns in examples and specifying text in style frequently found on the web are crucial.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, consistent patterns in examples and specifying text in style frequently found on the web are crucial.\""
    },
    {
        "title": "Perceptual Structure in the absence of grounding: the impact of abstractedness and subjectivity in color language for LLMs",
        "authors": [
            "Pablo Loyola",
            "Edison Marrese-Taylor",
            "Andres Hoyos-Idrobo"
        ],
        "published": "2023",
        "summary": "The need for grounding in language understanding is an active research topic. Previous work has suggested that color perception and color language appear as a suitable test bed to empirically study the problem, given its cognitive significance and showing that there is considerable alignment between a defined color space and the feature space defined by a language model. To further study this issue, we collect a large scale source of colors and their descriptions, containing almost a 1 million examples , and perform an empirical analysis to compare two kinds of alignments: (i) inter-space, by learning a mapping between embedding space and color space, and (ii) intra-space, by means of prompting comparatives between color descriptions. Our results show that while color space alignment holds for monolexemic, highly pragmatic color descriptions, this alignment drops considerably in the presence of examples that exhibit elements of real linguistic usage such as subjectivity and abstractedness, suggesting that grounding may be required in such cases.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.102.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"this alignment drops considerably in the presence of examples that exhibit elements of real linguistic usage such as subjectivity and abstractedness, suggesting that grounding may be required in such cases.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"this alignment drops considerably in the presence of examples that exhibit elements of real linguistic usage such as subjectivity and abstractedness, suggesting that grounding may be required in such cases.\""
    },
    {
        "title": "Remember what you did so you know what to do next",
        "authors": [
            "Manuel Ciosici",
            "Alex Hedges",
            "Yash Kankanampati",
            "Justin Martin",
            "Marjorie Freedman",
            "Ralph Weischedel"
        ],
        "published": "2023",
        "summary": "We explore using the 6B parameter GPT-J language model to create a plan for a simulated robot to achieve 30 classes of goals in ScienceWorld, a text game simulator for elementary science experiments and for which previously published empirical work has shown large language models (LLM)s to be a poor fit (Wang et al., 2022). Using the Markov assumption, the LLM outperforms the state-of-the-art based on reinforcement learning by a factor of 1.4. When we fill the LLM’s input buffer with as many prior steps as will fit, improvement rises to 3.3x. Even when training on only 6.5% of the training data, we observe a 2.3x improvement over the state-of-the-art. Our experiments show that performance varies widely across the 30 classes of actions, indicating that averaging over tasks can hide significant performance issues.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.104.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"for which previously published empirical work has shown large language models (LLM)s to be a poor fit (Wang et al., 2022)\"; \"Our experiments show that performance varies widely across the 30 classes of actions, indicating that averaging over tasks can hide significant performance issues.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"for which previously published empirical work has shown large language models (LLM)s to be a poor fit (Wang et al., 2022)\"; \"Our experiments show that performance varies widely across the 30 classes of actions, indicating that averaging over tasks can hide significant performance issues.\""
    },
    {
        "title": "Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach",
        "authors": [
            "Zheyuan Zhang",
            "Jifan Yu",
            "Juanzi Li",
            "Lei Hou"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have not only exhibited exceptional performance across various tasks, but also demonstrated sparks of intelligence. Recent studies have focused on assessing their capabilities on human exams and revealed their impressive competence in different domains. However, cognitive research on the overall knowledge structure of LLMs is still lacking. In this paper, based on educational diagnostic assessment method, we conduct an evaluation using MoocRadar, a meticulously annotated human test dataset based on Bloom Taxonomy. We aim to reveal the knowledge structures of LLMs and gain insights of their cognitive capabilities. This research emphasizes the significance of investigating LLMs’ knowledge and understanding the disparate cognitive patterns of LLMs. By shedding light on models’ knowledge, researchers can advance development and utilization of LLMs in a more informed and effective manner.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.111.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, cognitive research on the overall knowledge structure of LLMs is still lacking.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, cognitive research on the overall knowledge structure of LLMs is still lacking.\""
    },
    {
        "title": "Simple Hardware-Efficient PCFGs with Independent Left and Right Productions",
        "authors": [
            "Wei Liu",
            "Songlin Yang",
            "Yoon Kim",
            "Kewei Tu"
        ],
        "published": "2023",
        "summary": "Scaling dense PCFGs to thousands of nonterminals via low-rank parameterizations of the rule probability tensor has been shown to be beneficial for unsupervised parsing. However, PCFGs scaled this way still perform poorly as a language model, and even underperform similarly-sized HMMs. This work introduces SimplePCFG, a simple PCFG formalism with independent left and right productions. Despite imposing a stronger independence assumption than the low-rank approach, we find that this formalism scales more effectively both as a language model and as an unsupervised parser. We further introduce FlashInside, a hardware IO-aware implementation of the inside algorithm for efficiently scaling simple PCFGs. Through extensive experiments on multiple grammar induction benchmarks, we validate the effectiveness of simple PCFGs over low-rank baselines.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.113.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "R3 Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context",
        "authors": [
            "Qingyuan Tian",
            "Hanlun Zhu",
            "Lei Wang",
            "Yang Li",
            "Yunshi Lan"
        ],
        "published": "2023",
        "summary": "With the help of Chain-of-Thought (CoT) prompting, Large Language Models (LLMs) have achieved remarkable performance on various reasoning tasks. However, most of them have been evaluated under noise-free context and the dilemma for LLMs to produce inaccurate results under the noisy context has not been fully investigated. Existing studies utilize trigger sentences to encourage LLMs to concentrate on the relevant information but the trigger has limited effect on final answer prediction. Inspired by interactive CoT method, where intermediate reasoning steps are promoted by multiple rounds of interaction between users and LLMs, we propose a novel prompting method, namely R3 prompting, for CoT reasoning under noisy context. Specifically, R3 prompting interacts with LLMs to perform key sentence extraction, variable declaration and answer prediction, which corresponds to a thought process of reviewing, rephrasing and resolving. The responses generated at the last interaction will perform as hints to guide toward the responses of the next interaction. Our experiments show that R3 prompting significantly outperforms existing CoT prompting methods on five reasoning tasks under noisy context. With GPT-3.5-turbo, we observe 3.7% accuracy improvement on average on the reasoning tasks under noisy context compared to the most competitive prompting baseline. More analyses and ablation studies show the robustness and generalization of R3 prompting method in solving reasoning tasks in LLMs under noisy context.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.114.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, most of them have been evaluated under noise-free context and the dilemma for LLMs to produce inaccurate results under the noisy context has not been fully investigated.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, most of them have been evaluated under noise-free context and the dilemma for LLMs to produce inaccurate results under the noisy context has not been fully investigated.\""
    },
    {
        "title": "Adapter Pruning using Tropical Characterization",
        "authors": [
            "Rishabh Bhardwaj",
            "Tushar Vaidya",
            "Soujanya Poria"
        ],
        "published": "2023",
        "summary": "Adapters are widely popular parameter-efficient transfer learning approaches in natural language processing that insert trainable modules in between layers of a pre-trained language model. Apart from several heuristics, however, there has been a lack of studies analyzing the optimal number of adapter parameters needed for downstream applications. Thus, we propose an adapter pruning approach by studying the tropical characteristics of trainable modules. We cast it as an optimization problem that aims to prune parameters from the adapter layers without changing the orientation of underlying tropical hypersurfaces. Our experiments on five NLP datasets show that tropical geometry tends to identify more relevant parameters to prune when compared with the magnitude-based baseline, while a combined approach works best across the tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.116.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Apart from several heuristics, however, there has been a lack of studies analyzing the optimal number of adapter parameters needed for downstream applications.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Apart from several heuristics, however, there has been a lack of studies analyzing the optimal number of adapter parameters needed for downstream applications.\""
    },
    {
        "title": "Explain-then-translate: an analysis on improving program translation with self-generated explanations",
        "authors": [
            "Zilu Tang",
            "Mayank Agarwal",
            "Alexander Shypula",
            "Bailin Wang",
            "Derry Wijaya",
            "Jie Chen",
            "Yoon Kim"
        ],
        "published": "2023",
        "summary": "This work explores the use of self-generated natural language explanations as an intermediate step for code-to-code translation with language models. Across three types of explanations and 19 programming languages constructed from the MultiPL-E dataset, we find the explanations to be particularly effective in the zero-shot case, improving performance by 12% on average. Improvements with natural language explanations are particularly pronounced on difficult programs. We release our dataset, code, and canonical solutions in all 19 languages.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.119.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Improvements with natural language explanations are particularly pronounced on difficult programs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Improvements with natural language explanations are particularly pronounced on difficult programs.\""
    },
    {
        "title": "GLGR: Question-aware Global-to-Local Graph Reasoning for Multi-party Dialogue Reading Comprehension",
        "authors": [
            "Yanling Li",
            "Bowei Zou",
            "Yifan Fan",
            "Xibo Li",
            "Ai Ti Aw",
            "Yu Hong"
        ],
        "published": "2023",
        "summary": "Graph reasoning contributes to the integration of discretely-distributed attentive information (clues) for Multi-party Dialogue Reading Comprehension (MDRC). This is attributed primarily to multi-hop reasoning over global conversational structures. However, existing approaches barely apply questions for anti-noise graph reasoning. More seriously, the local semantic structures in utterances are neglected, although they are beneficial for bridging across semantically-related clues. In this paper, we propose a question-aware global-to-local graph reasoning approach. It expands the canonical Interlocutor-Utterance graph by introducing a question node, enabling comprehensive global graph reasoning. More importantly, it constructs a semantic-role graph for each utterance, and accordingly performs local graph reasoning conditioned on the semantic relations. We design a two-stage encoder network to implement the progressive reasoning from the global graph to local. The experiments on the benchmark datasets Molweni and FriendsQA show that our approach yields significant improvements, compared to BERT and ELECTRA baselines. It achieves 73.6% and 77.2% F1-scores on Molweni and FriendsQA, respectively, outperforming state-of-the-art methods that employ different pretrained language models as backbones.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.122.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Towards Mitigating LLM Hallucination via Self Reflection",
        "authors": [
            "Ziwei Ji",
            "Tiezheng Yu",
            "Yan Xu",
            "Nayeon Lee",
            "Etsuko Ishii",
            "Pascale Fung"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of “hallucination”, where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Consequently, we harness the interactivity and multitasking ability of LLMs and produce progressively more precise and accurate answers. Experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.123.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the practical deployment still faces challenges, notably the issue of “hallucination”, where models generate plausible-sounding but unfaithful or nonsensical information.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, the practical deployment still faces challenges, notably the issue of “hallucination”, where models generate plausible-sounding but unfaithful or nonsensical information.\""
    },
    {
        "title": "XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages",
        "authors": [
            "Sebastian Ruder",
            "Jonathan H. Clark",
            "Alexander Gutkin",
            "Mihir Kale",
            "Min Ma",
            "Massimo Nicosia",
            "Shruti Rijhwani",
            "Parker Riley",
            "Jean-Michel A- Sarr",
            "Xinyi Wang",
            "John Wieting",
            "Nitish Gupta",
            "Anna Katanova",
            "Christo Kirov",
            "Dana L. Dickinson",
            "Brian Roark",
            "Bidisha Samanta",
            "Connie Tao",
            "David I. Adelani",
            "Vera Axelrod",
            "Isaac Caswell",
            "Colin Cherry",
            "Dan Garrette",
            "Reeve Ingle",
            "Melvin Johnson",
            "Dmitry Panteleev",
            "Partha Talukdar"
        ],
        "published": "2023",
        "summary": "Data scarcity is a crucial issue for the development of highly multilingual NLP systems. Yet for many under-represented languages (ULs) — languages for which NLP research is particularly far behind in meeting user needs — it is feasible to annotate small amounts of data. Motivated by this, we propose XTREME-UP, a benchmark defined by: its focus on the scarce-data scenario rather than zero-shot; its focus on user-centric tasks — tasks with broad adoption by speakers of high-resource languages; and its focus on under-represented languages where this scarce-data scenario tends to be most realistic. XTREME-UP evaluates the capabilities of language models across 88 under-represented languages over 9 key user-centric technologies including ASR, OCR, MT, and information access tasks that are of general utility. We create new datasets for OCR, autocomplete, semantic parsing, and transliteration, and build on and refine existing datasets for other tasks. XTREME-UP provides methodology for evaluating many modeling scenarios including text only, multi-modal (vision, audio, and text), supervised parameter tuning, and in-context learning. We evaluate commonly used models on the benchmark. We release all code and scripts to train and evaluate models.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.125.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Data scarcity is a crucial issue for the development of highly multilingual NLP systems.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Data scarcity is a crucial issue for the development of highly multilingual NLP systems.\""
    },
    {
        "title": "DiFair: A Benchmark for Disentangled Assessment of Gender Knowledge and Bias",
        "authors": [
            "Mahdi Zakizadeh",
            "Kaveh Miandoab",
            "Mohammad Pilehvar"
        ],
        "published": "2023",
        "summary": "Numerous debiasing techniques have been proposed to mitigate the gender bias that is prevalent in pretrained language models. These are often evaluated on datasets that check the extent to which the model is gender-neutral in its predictions. Importantly, this evaluation protocol overlooks the possible adverse impact of bias mitigation on useful gender knowledge. To fill this gap, we propose **DiFair**, a manually curated dataset based on masked language modeling objectives. **DiFair** allows us to introduce a unified metric, *gender invariance score*, that not only quantifies a model’s biased behavior, but also checks if useful gender knowledge is preserved. We use **DiFair** as a benchmark for a number of widely-used pretained language models and debiasing techniques. Experimental results corroborate previous findings on the existing gender biases, while also demonstrating that although debiasing techniques ameliorate the issue of gender bias, this improvement usually comes at the price of lowering useful gender knowledge of the model.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.127.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"although debiasing techniques ameliorate the issue of gender bias, this improvement usually comes at the price of lowering useful gender knowledge of the model.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"although debiasing techniques ameliorate the issue of gender bias, this improvement usually comes at the price of lowering useful gender knowledge of the model.\""
    },
    {
        "title": "Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens",
        "authors": [
            "Byung-Doh Oh",
            "William Schuler"
        ],
        "published": "2023",
        "summary": "Recent psycholinguistic studies have drawn conflicting conclusions about the relationship between the quality of a language model and the ability of its surprisal estimates to predict human reading times, which has been speculated to be due to the large gap in both the amount of training data and model capacity across studies. The current work aims to consolidate these findings by evaluating surprisal estimates from Transformer-based language model variants that vary systematically in the amount of training data and model capacity on their ability to predict human reading times. The results show that surprisal estimates from most variants with contemporary model capacities provide the best fit after seeing about two billion training tokens, after which they begin to diverge from humanlike expectations. Additionally, newly-trained smaller model variants reveal a ‘tipping point’ at convergence, after which the decrease in language model perplexity begins to result in poorer fits to human reading times. These results suggest that the massive amount of training data is mainly responsible for the poorer fit achieved by surprisal from larger pre-trained language models, and that a certain degree of model capacity is necessary for Transformer-based language models to capture humanlike expectations.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.128.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"after which they begin to diverge from humanlike expectations\"; \"the decrease in language model perplexity begins to result in poorer fits to human reading times.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"after which they begin to diverge from humanlike expectations\"; \"the decrease in language model perplexity begins to result in poorer fits to human reading times.\""
    },
    {
        "title": "ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination",
        "authors": [
            "Dongfang Li",
            "Jindi Yu",
            "Baotian Hu",
            "Zhenran Xu",
            "Min Zhang"
        ],
        "published": "2023",
        "summary": "In the field of Large Language Models (LLMs), researchers are increasingly exploring their effectiveness across a wide range of tasks. However, a critical area that requires further investigation is the interpretability of these models, particularly the ability to generate rational explanations for their decisions. Most existing explanation datasets are limited to the English language and the general domain, which leads to a scarcity of linguistic diversity and a lack of resources in specialized domains, such as medical. To mitigate this, we propose ExplainCPE, a challenging medical dataset consisting of over 7K problems from Chinese Pharmacist Examination, specifically tailored to assess the model-generated explanations. From the overall results, only GPT-4 passes the pharmacist examination with a 75.7% accuracy, while other models like ChatGPT fail. Further detailed analysis of LLM-generated explanations reveals the limitations of LLMs in understanding medical text and executing computational reasoning. With the increasing importance of AI safety and trustworthiness, ExplainCPE takes a step towards improving and evaluating the interpretability of LLMs in the medical domain.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.129.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Further detailed analysis of LLM-generated explanations reveals the limitations of LLMs in understanding medical text and executing computational reasoning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Further detailed analysis of LLM-generated explanations reveals the limitations of LLMs in understanding medical text and executing computational reasoning.\""
    },
    {
        "title": "CLASS: A Design Framework for Building Intelligent Tutoring Systems Based on Learning Science principles",
        "authors": [
            "Shashank Sonkar",
            "Naiming Liu",
            "Debshila Mallick",
            "Richard Baraniuk"
        ],
        "published": "2023",
        "summary": "We present a design framework called Conversational Learning with Analytical Step-by-Step Strategies (CLASS) for building advanced Intelligent Tutoring Systems (ITS) powered by high-performance Large Language Models (LLMs). The CLASS framework empowers ITS with two key capabilities. First, through a carefully curated scaffolding dataset, CLASS equips ITS with essential problem-solving strategies, enabling it to provide tutor-like, step-by-step guidance to students. Second, by using a dynamic conversational dataset, CLASS assists ITS in facilitating natural language interactions, fostering engaging student-tutor conversations. The CLASS framework also provides valuable insights into ITS’s internal decision-making process which allows seamless integration of user feedback, thus enabling continuous refinement and improvement. We also present a proof-of-concept ITS, referred to as SPOCK, which is trained using the CLASS framework with a focus on introductory college level biology content. A carefully constructed protocol was developed for SPOCK’s preliminary evaluation, examining aspects such as the factual accuracy and relevance of its responses. Experts in the field of biology offered favorable remarks, particularly highlighting SPOCK’s capability to break down questions into manageable subproblems and provide encouraging responses to students.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.130.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions \"factual accuracy\" which could imply potential limitations, however, it is not explicitly stated as a limitation of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions \"factual accuracy\" which could imply potential limitations, however, it is not explicitly stated as a limitation of LLMs."
    },
    {
        "title": "mmT5: Modular Multilingual Pre-Training Solves Source Language Hallucinations",
        "authors": [
            "Jonas Pfeiffer",
            "Francesco Piccinno",
            "Massimo Nicosia",
            "Xinyi Wang",
            "Machel Reid",
            "Sebastian Ruder"
        ],
        "published": "2023",
        "summary": "Multilingual sequence-to-sequence models perform poorly with increased language coverage and fail to consistently generate text in the correct target language in few-shot settings. To address these challenges, we propose mmT5, a modular multilingual sequence-to-sequence model. mmT5 utilizes language-specific modules during pre-training, which disentangle language-specific information from language-agnostic information. We identify representation drift during fine-tuning as a key limitation of modular generative models and develop strategies that enable effective zero-shot transfer. Our model outperforms mT5 at the same parameter sizes by a large margin on representative natural language understanding and generation tasks in 40+ languages. Compared to mT5, mmT5 raises the rate of generating text in the correct language under zero-shot settings from 7% to 99%, thereby greatly alleviating the source language hallucination problem.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.132.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Multilingual sequence-to-sequence models perform poorly with increased language coverage and fail to consistently generate text in the correct target language in few-shot settings.\"; \"We identify representation drift during fine-tuning as a key limitation of modular generative models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Multilingual sequence-to-sequence models perform poorly with increased language coverage and fail to consistently generate text in the correct target language in few-shot settings.\"; \"We identify representation drift during fine-tuning as a key limitation of modular generative models\""
    },
    {
        "title": "ImageNetVC: Zero- and Few-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories",
        "authors": [
            "Heming Xia",
            "Qingxiu Dong",
            "Lei Li",
            "Jingjing Xu",
            "Tianyu Liu",
            "Ziwei Qin",
            "Zhifang Sui"
        ],
        "published": "2023",
        "summary": "Recently, Large Language Models (LLMs) have been serving as general-purpose interfaces, posing a significant demand for comprehensive visual knowledge. However, it remains unclear how well current LLMs and their visually augmented counterparts (VaLMs) can master visual commonsense knowledge. To investigate this, we propose ImageNetVC, a human-annotated dataset specifically designed for zero- and few-shot visual commonsense evaluation across 1,000 ImageNet categories. Utilizing ImageNetVC, we benchmark the fundamental visual commonsense knowledge of both unimodal LLMs and VaLMs. Furthermore, we analyze the factors affecting the visual commonsense knowledge of large-scale models, providing insights into the development of language models enriched with visual commonsense knowledge. Our code and dataset are available at https://github.com/hemingkx/ImageNetVC.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.133.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it remains unclear how well current LLMs and their visually augmented counterparts (VaLMs) can master visual commonsense knowledge.\"\n\nThis abstract mentions a limitation of LLMs in passing, specifically their unclear ability to master visual commonsense knowledge, but does not elaborate on this limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, it remains unclear how well current LLMs and their visually augmented counterparts (VaLMs) can master visual commonsense knowledge.\"\n\nThis abstract mentions a limitation of LLMs in passing, specifically their unclear ability to master visual commonsense knowledge, but does not elaborate on this limitation."
    },
    {
        "title": "MEEP: Is this Engaging? Prompting Large Language Models for Dialogue Evaluation in Multilingual Settings",
        "authors": [
            "Amila Ferron",
            "Amber Shore",
            "Ekata Mitra",
            "Ameeta Agrawal"
        ],
        "published": "2023",
        "summary": "As dialogue systems become more popular, evaluation of their response quality gains importance. Engagingness highly correlates with overall quality and creates a sense of connection that gives human participants a more fulfilling experience. Although qualities like coherence and fluency are readily measured with well-worn automatic metrics, evaluating engagingness often relies on human assessment, which is a costly and time-consuming process. Existing automatic engagingness metrics evaluate the response without the conversation history, are designed for one dataset, or have limited correlation with human annotations. Furthermore, they have been tested exclusively on English conversations. Given that dialogue systems are increasingly available in languages beyond English, multilingual evaluation capabilities are essential. We propose that large language models (LLMs) may be used for evaluation of engagingness in dialogue through prompting, and ask how prompt constructs and translated prompts compare in a multilingual setting. We provide a prompt-design taxonomy for engagingness and find that using selected prompt elements with LLMs, including our comprehensive definition of engagingness, outperforms state-of-the-art methods on evaluation of engagingness in dialogue across multiple languages.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.137.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "not extracted",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs:  \nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models",
        "authors": [
            "Jaeyoung Choe",
            "Keonwoong Noh",
            "Nayeon Kim",
            "Seyun Ahn",
            "Woohwan Jung"
        ],
        "published": "2023",
        "summary": "Over the past few years, various domain-specific pretrained language models (PLMs) have been proposed and have outperformed general-domain PLMs in specialized areas such as biomedical, scientific, and clinical domains. In addition, financial PLMs have been studied because of the high economic impact of financial data analysis. However, we found that financial PLMs were not pretrained on sufficiently diverse financial data. This lack of diverse training data leads to a subpar generalization performance, resulting in general-purpose PLMs, including BERT, often outperforming financial PLMs on many downstream tasks. To address this issue, we collected a broad range of financial corpus and trained the Financial Language Model (FiLM) on these diverse datasets. Our experimental results confirm that FiLM outperforms not only existing financial PLMs but also general domain PLMs. Furthermore, we provide empirical evidence that this improvement can be achieved even for unseen corpus groups.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.138.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, we found that financial PLMs were not pretrained on sufficiently diverse financial data. This lack of diverse training data leads to a subpar generalization performance, resulting in general-purpose PLMs, including BERT, often outperforming financial PLMs on many downstream tasks.\"\n\nThis abstract mentions a limitation of LLMs (lack of diverse training data leading to subpar generalization",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, we found that financial PLMs were not pretrained on sufficiently diverse financial data. This lack of diverse training data leads to a subpar generalization performance, resulting in general-purpose PLMs, including BERT, often outperforming financial PLMs on many downstream tasks.\"\n\nThis abstract mentions a limitation of LLMs (lack of diverse training data leading to subpar generalization"
    },
    {
        "title": "LLMDet: A Third Party Large Language Models Generated Text Detection Tool",
        "authors": [
            "Kangxi Wu",
            "Liang Pang",
            "Huawei Shen",
            "Xueqi Cheng",
            "Tat-Seng Chua"
        ],
        "published": "2023",
        "summary": "Generated texts from large language models (LLMs) are remarkably close to high-quality human-authored text, raising concerns about their potential misuse in spreading false information and academic misconduct. Consequently, there is an urgent need for a highly practical detection tool capable of accurately identifying the source of a given text. However, existing detection tools typically rely on access to LLMs and can only differentiate between machine-generated and human-authored text, failing to meet the requirements of fine-grained tracing, intermediary judgment, and rapid detection. Therefore, we propose LLMDet, a model-specific, secure, efficient, and extendable detection tool, that can source text from specific LLMs, such as GPT-2, OPT, LLaMA, and others. In LLMDet, we record the next-token probabilities of salient n-grams as features to calculate proxy perplexity for each LLM. By jointly analyzing the proxy perplexities of LLMs, we can determine the source of the generated text. Experimental results show that LLMDet yields impressive detection performance while ensuring speed and security, achieving 98.54% precision and about × 5.0 faster for recognizing human-authored text. Additionally, LLMDet can effortlessly extend its detection capabilities to a new open-source model. We will provide an open-source tool at https://github.com/TrustedLLM/LLMDet.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.139.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing detection tools typically rely on access to LLMs and can only differentiate between machine-generated and human-authored text, failing to meet the requirements of fine-grained tracing, intermediary judgment, and rapid detection.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, existing detection tools typically rely on access to LLMs and can only differentiate between machine-generated and human-authored text, failing to meet the requirements of fine-grained tracing, intermediary judgment, and rapid detection.\""
    },
    {
        "title": "G-SPEED: General SParse Efficient Editing MoDel",
        "authors": [
            "Haoke Zhang",
            "Yue Wang",
            "Juntao Li",
            "Xiabing Zhou",
            "Min Zhang"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have demonstrated incredible capabilities in understanding, generating, and manipulating languages. Through human-model interactions, LLMs can automatically understand human-issued instructions and output the expected contents, which can significantly increase working efficiency. In various types of real-world demands, editing-oriented tasks account for a considerable proportion, which involves an interactive process that entails the continuous refinement of existing texts to meet specific criteria. Due to the need for multi-round human-model interaction and the generation of complicated editing tasks, there is an emergent need for efficient general editing models. In this paper, we propose General SParse Efficient Editing MoDel (G-SPEED), which can fulfill diverse editing requirements through a single model while maintaining low computational costs. Specifically, we first propose a novel unsupervised text editing data clustering algorithm to deal with the data scarcity problem. Subsequently, we introduce a sparse editing model architecture to mitigate the inherently limited learning capabilities of small language models. The experimental outcomes indicate that G-SPEED, with its 508M parameters, can surpass LLMs equipped with 175B parameters. Our code and model checkpoints are available at https://github.com/Banner-Z/G-SPEED.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.142.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"mitigate the inherently limited learning capabilities of small language models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"mitigate the inherently limited learning capabilities of small language models.\""
    },
    {
        "title": "Attack Prompt Generation for Red Teaming and Defending Large Language Models",
        "authors": [
            "Boyi Deng",
            "Wenjie Wang",
            "Fuli Feng",
            "Yang Deng",
            "Qifan Wang",
            "Xiangnan He"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) are susceptible to red teaming attacks, which can induce LLMs to generate harmful content. Previous research constructs attack prompts via manual or automatic methods, which have their own limitations on construction cost and quality. To address these issues, we propose an integrated approach that combines manual and automatic methods to economically generate high-quality attack prompts. Specifically, considering the impressive capabilities of newly emerged LLMs, we propose an attack framework to instruct LLMs to mimic human-generated prompts through in-context learning. Furthermore, we propose a defense framework that fine-tunes victim LLMs through iterative interactions with the attack framework to enhance their safety against red teaming attacks. Extensive experiments on different LLMs validate the effectiveness of our proposed attack and defense frameworks. Additionally, we release a series of attack prompts datasets named SAP with varying sizes, facilitating the safety evaluation and enhancement of more LLMs.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.143.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models (LLMs) are susceptible to red teaming attacks, which can induce LLMs to generate harmful content.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large language models (LLMs) are susceptible to red teaming attacks, which can induce LLMs to generate harmful content.\""
    },
    {
        "title": "Smart “Chef”: Verifying the Effect of Role-based Paraphrasing for Aspect Term Extraction",
        "authors": [
            "Jiaxiang Chen",
            "Yu Hong",
            "Qingting Xu",
            "Jianmin Yao"
        ],
        "published": "2023",
        "summary": "We tackle Aspect Term Extraction (ATE), a task of automatically extracting aspect terms from sentences. The current Pretrained Language Model (PLM) based extractors have achieved significant improvements. They primarily benefit from context-aware encoding. However, a considerable number of sentences in ATE corpora contain uninformative or low-quality contexts. Such sentences frequently act as “troublemakers” during test. In this study, we explore the context-oriented quality improvement method. Specifically, we propose to automatically rewrite the sentences from the perspectives of virtual experts with different roles, such as a “chef” in the restaurant domain. On this basis, we perform ATE over the paraphrased sentences during test, using the well-trained extractors without any change. In the experiments, we leverage ChatGPT to determine virtual experts in the considered domains, and induce ChatGPT to generate paraphrases conditioned on the roles of virtual experts. We experiment on the benchmark SemEval datasets, including Laptop-domain L14 and Restaurant-domain R14-16. The experimental results show that our approach effectively recalls the inconspicuous aspect terms like “al di la”, although it reduces the precision. In addition, it is proven that our approach can be substantially improved by redundancy elimination and multi-role voting. More importantly, our approach can be used to expand the predictions obtained on the original sentences. This yields state-of-the-art performance (i.e., F1-scores of 86.2%, 89.3%, 77.7%, 82.7% on L14 and R14-16) without retraining or fine-tuning the baseline extractors.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.144.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, a considerable number of sentences in ATE corpora contain uninformative or low-quality contexts. Such sentences frequently act as “troublemakers” during test.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs in passing, specifically that they can be negatively affected by uninformative or low-quality contexts, but does not elaborate on this",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, a considerable number of sentences in ATE corpora contain uninformative or low-quality contexts. Such sentences frequently act as “troublemakers” during test.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs in passing, specifically that they can be negatively affected by uninformative or low-quality contexts, but does not elaborate on this"
    },
    {
        "title": "DRAFT: Dense Retrieval Augmented Few-shot Topic classifier Framework",
        "authors": [
            "Keonwoo Kim",
            "Younggun Lee"
        ],
        "published": "2023",
        "summary": "With the growing volume of diverse information, the demand for classifying arbitrary topics has become increasingly critical. To address this challenge, we introduce DRAFT, a simple framework designed to train a classifier for few-shot topic classification. DRAFT uses a few examples of a specific topic as queries to construct Customized dataset with a dense retriever model. Multi-query retrieval (MQR) algorithm, which effectively handles multiple queries related to a specific topic, is applied to construct the Customized dataset. Subsequently, we fine-tune a classifier using the Customized dataset to identify the topic. To demonstrate the efficacy of our proposed approach, we conduct evaluations on both widely used classification benchmark datasets and manually constructed datasets with 291 diverse topics, which simulate diverse contents encountered in real-world applications. DRAFT shows competitive or superior performance compared to baselines that use in-context learning, such as GPT-3 175B and InstructGPT 175B, on few-shot topic classification tasks despite having 177 times fewer parameters, demonstrating its effectiveness.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.150.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"despite having 177 times fewer parameters\"\n\n(Note: Although this paper mentions LLMs, specifically GPT-3 175B and InstructGPT-175B, it does not discuss any limitations of LLMs explicitly. The mention of \"177 times fewer parameters\" is a comparison to highlight the efficiency of the proposed approach, but it does not elaborate on any",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"despite having 177 times fewer parameters\"\n\n(Note: Although this paper mentions LLMs, specifically GPT-3 175B and InstructGPT-175B, it does not discuss any limitations of LLMs explicitly. The mention of \"177 times fewer parameters\" is a comparison to highlight the efficiency of the proposed approach, but it does not elaborate on any"
    },
    {
        "title": "A Framework for Exploring Player Perceptions of LLM-Generated Dialogue in Commercial Video Games",
        "authors": [
            "Nader Akoury",
            "Qian Yang",
            "Mohit Iyyer"
        ],
        "published": "2023",
        "summary": "The growing capabilities of large language models (LLMs) have inspired recent efforts to integrate LLM-generated dialogue into video games. However, evaluation remains a major challenge: how do we assess the player experience in a commercial game augmented with LLM-generated dialogue? To explore this question, we introduce a dynamic evaluation framework for the dialogue management systems that govern the task-oriented dialogue often found in roleplaying video games. We first extract dialogue from the widely-acclaimed role-playing game *Disco Elysium: The Final Cut*, which contains 1.1M words of dialogue spread across a complex graph of utterances where node reachability depends on game state (e.g., whether a certain item is held). Using this dataset, we have GPT-4 perform *dialogue infilling* to generate grounded utterances based on game state represented via code. In a statistically robust study of 28 players recruited from the r/DiscoyElysium subreddit, the LLM outputs are evaluated against the game designers’ writing via both preference judgments and free-form feedback using a web interface that recreates the game’s core conversation functionality. Overall, the game designers’ prose is significantly preferred to GPT-4 generations, with participants citing reasons such as improved logical flow and grounding with the game state. To spur more principled future research in this area, we release our web interface and tools to enable researchers to build upon our work. https://pl.aiwright.dev",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.151.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Overall, the game designers’ prose is significantly preferred to GPT-4 generations, with participants citing reasons such as improved logical flow and grounding with the game state.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Overall, the game designers’ prose is significantly preferred to GPT-4 generations, with participants citing reasons such as improved logical flow and grounding with the game state.\""
    },
    {
        "title": "Generative Calibration for In-context Learning",
        "authors": [
            "Zhongtao Jiang",
            "Yuanzhe Zhang",
            "Cao Liu",
            "Jun Zhao",
            "Kang Liu"
        ],
        "published": "2023",
        "summary": "As one of the most exciting features of large language models (LLMs), in-context learning is a mixed blessing. While it allows users to fast-prototype a task solver with only a few training examples, the performance is generally sensitive to various configurations of the prompt such as the choice or order of the training examples. In this paper, we for the first time theoretically and empirically identify that such a paradox is mainly due to the label shift of the in-context model to the data distribution, in which LLMs shift the label marginal p(y) while having a good label conditional p(x|y). With this understanding, we can simply calibrate the in-context predictive distribution by adjusting the label marginal, which is estimated via Monte-Carlo sampling over the in-context model, i.e., generation of LLMs. We call our approach as generative calibration. We conduct exhaustive experiments with 12 text classification tasks and 12 LLMs scaling from 774M to 33B, generally find that the proposed method greatly and consistently outperforms the ICL as well as state-of-the-art calibration methods, by up to 27% absolute in macro-F1. Meanwhile, the proposed method is also stable under different prompt configurations.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.152.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While it allows users to fast-prototype a task solver with only a few training examples, the performance is generally sensitive to various configurations of the prompt such as the choice or order of the training examples.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"While it allows users to fast-prototype a task solver with only a few training examples, the performance is generally sensitive to various configurations of the prompt such as the choice or order of the training examples.\""
    },
    {
        "title": "Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation Extraction",
        "authors": [
            "Xilai Ma",
            "Jing Li",
            "Min Zhang"
        ],
        "published": "2023",
        "summary": "Few-shot relation extraction involves identifying the type of relationship between two specific entities within a text, using a limited number of annotated samples. A variety of solutions to this problem have emerged by applying meta-learning and neural graph techniques which typically necessitate a training process for adaptation. Recently, the strategy of in-context learning has been demonstrating notable results without the need of training. Few studies have already utilized in-context learning for zero-shot information extraction. Unfortunately, the evidence for inference is either not considered or implicitly modeled during the construction of chain-of-thought prompts. In this paper, we propose a novel approach for few-shot relation extraction using large language models, named CoT-ER, chain-of-thought with explicit evidence reasoning. In particular, CoT-ER first induces large language models to generate evidences using task-specific and concept-level knowledge. Then these evidences are explicitly incorporated into chain-of-thought prompting for relation extraction. Experimental results demonstrate that our CoT-ER approach (with 0% training data) achieves competitive performance compared to the fully-supervised (with 100% training data) state-of-the-art approach on the FewRel1.0 and FewRel2.0 datasets.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.153.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unfortunately, the evidence for inference is either not considered or implicitly modeled during the construction of chain-of-thought prompts.\"\n\nThis paper talks about LLMs but does not explicitly discuss their limitations. The mention of \"Unfortunately\" could imply a limitation, but it is more related to the previous approaches rather than LLMs themselves.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Unfortunately, the evidence for inference is either not considered or implicitly modeled during the construction of chain-of-thought prompts.\"\n\nThis paper talks about LLMs but does not explicitly discuss their limitations. The mention of \"Unfortunately\" could imply a limitation, but it is more related to the previous approaches rather than LLMs themselves."
    },
    {
        "title": "Manipulating the Perceived Personality Traits of Language Models",
        "authors": [
            "Graham Caron",
            "Shashank Srivastava"
        ],
        "published": "2023",
        "summary": "Psychology research has long explored aspects of human personality like extroversion, agreeableness and emotional stability, three of the personality traits that make up the ‘Big Five’. Categorizations like the ‘Big Five’ are commonly used to assess and diagnose personality types. In this work, we explore whether text generated from large language models exhibits consistency in it’s perceived ‘Big Five’ personality traits. For example, is a language model such as GPT2 likely to respond in a consistent way if asked to go out to a party? We also show that when exposed to different types of contexts (such as personality descriptions, or answers to diagnostic questions about personality traits), language models such as BERT and GPT2 consistently identify and mirror personality markers in those contexts. This behavior illustrates an ability to be manipulated in a predictable way (with correlations up to 0.84 between intended and realized changes in personality traits), and frames them as tools for controlling personas in applications such as dialog systems. We contribute two data-sets of personality descriptions of humans subjects.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.156.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This behavior illustrates an ability to be manipulated in a predictable way\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"This behavior illustrates an ability to be manipulated in a predictable way\""
    },
    {
        "title": "WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia",
        "authors": [
            "Sina Semnani",
            "Violet Yao",
            "Heidi Zhang",
            "Monica Lam"
        ],
        "published": "2023",
        "summary": "This paper presents the first few-shot LLM-based chatbot that almost never hallucinates and has high conversationality and low latency. WikiChat is grounded on the English Wikipedia, the largest curated free-text corpus. WikiChat generates a response from an LLM, retains only the grounded facts, and combines them with additional information it retrieves from the corpus to form factual and engaging responses. We distill WikiChat based on GPT-4 into a 7B-parameter LLaMA model with minimal loss of quality, to significantly improve its latency, cost and privacy, and facilitate research and deployment. Using a novel hybrid human-and-LLM evaluation methodology, we show that our best system achieves 97.3% factual accuracy in simulated conversations. It significantly outperforms all retrieval-based and LLM-based baselines, and by 3.9%, 38.6% and 51.0% on head, tail and recent knowledge compared to GPT-4. Compared to previous state-of-the-art retrieval-based chatbots, WikiChat is also significantly more informative and engaging, just like an LLM. WikiChat achieves 97.9% factual accuracy in conversations with human users about recent topics, 55.0% better than GPT-4, while receiving significantly higher user ratings and more favorable comments.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.157.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Stopping the Hallucination of Large Language Model Chatbots\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Stopping the Hallucination of Large Language Model Chatbots\""
    },
    {
        "title": "Automated Few-Shot Classification with Instruction-Finetuned Language Models",
        "authors": [
            "Rami Aly",
            "Xingjian Shi",
            "Kaixiang Lin",
            "Aston Zhang",
            "Andrew Wilson"
        ],
        "published": "2023",
        "summary": "A particularly successful class of approaches for few-shot learning combines language models with prompts - hand-crafted task descriptions that complement data samples. However, designing prompts by hand for each task commonly requires domain knowledge and substantial guesswork. We observe, in the context of classification tasks, that instruction finetuned language models are remarkably robust towards some dimensions of a prompt’s design. We subsequently propose a simple method to eliminate the need for handcrafted prompts, named AuT-Few. This approach consists of (i) a prompt retrieval module that selects suitable task instructions from the instruction-tuning knowledge base, and (ii) the generation of two distinct, semantically meaningful, class descriptions and a selection mechanism via cross-validation. Over 12 datasets, spanning 8 classification tasks, we show that AuT-Few outperforms current state-of-the-art few-shot learning methods. Moreover, AuT-Few is the best ranking method across datasets on the RAFT few-shot benchmark. Notably, these results are achieved without task-specific handcrafted prompts on unseen tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.158.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions the need to eliminate the need for handcrafted prompts, which might be considered a minor limitation of LLMs, however, it is not explicitly stated as a limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions the need to eliminate the need for handcrafted prompts, which might be considered a minor limitation of LLMs, however, it is not explicitly stated as a limitation."
    },
    {
        "title": "Meta-Learning of Prompt Generation for Lightweight Prompt Engineering on Language-Model-as-a-Service",
        "authors": [
            "Hyeonmin Ha",
            "Jihye Lee",
            "Wookje Han",
            "Byung-Gon Chun"
        ],
        "published": "2023",
        "summary": "Recently, many companies have been providing the capabilities of large language models as services. These Language-Model-as-a-Service (LMaaS) offerings support a variety of user tasks through in-context learning from prompts, which include instructions and demonstrations of the task. However, for users, manually crafting prompts or running automatic prompt tuning methods themselves can be demanding. Despite these challenges, LMaaS providers do not offer automatic prompt engineering methods as part of their services. One of the major obstacles to deploying them on an LMaaS is the heavy computational costs associated with automatic prompt engineering methods. These methods are typically designed to iterate through tens of thousands of examples, which impose unaffordable overheads for LMaaS providers. In this paper, we introduce MetaL-Prompt, a novel lightweight automatic prompt generation method for LMaaS. MetaL-Prompt meta-trains a prompt generation model (PGM) to enable robust learning by the language model from the contexts created by the generated prompts (i.e., in-context learning). Thanks to our meta-learning approach, a PGM can generate prompts for unseen tasks without requiring additional training for those specific tasks. Furthermore, the PGM can generate prompts with a single forward pass, significantly reducing computational costs compared to previous methods. We evaluate MetaL-Prompt on a range of unseen tasks and find that it improves performance by up to 19.4% in terms of mean F1 score on QA datasets compared to the state-of-the-art baseline P-tuning, with limited computational cost.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.159.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, for users, manually crafting prompts or running automatic prompt tuning methods themselves can be demanding.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, for users, manually crafting prompts or running automatic prompt tuning methods themselves can be demanding.\""
    },
    {
        "title": "Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction",
        "authors": [
            "Siyu Yuan",
            "Jiangjie Chen",
            "Xuyang Ge",
            "Yanghua Xiao",
            "Deqing Yang"
        ],
        "published": "2023",
        "summary": "The vital role of analogical reasoning in human cognition allows us to grasp novel concepts by linking them with familiar ones through shared relational structures. Despite the attention previous research has given to word analogies, this work suggests that Large Language Models (LLMs) often overlook the structures that underpin these analogies, raising questions about the efficacy of word analogies as a measure of analogical reasoning skills akin to human cognition. In response to this, our paper introduces a task of analogical structure abduction, grounded in cognitive psychology, designed to abduce structures that form an analogy between two systems. In support of this task, we establish a benchmark called SCAR, containing 400 scientific analogies from 13 distinct fields, tailored for evaluating analogical reasoning with structure abduction. The empirical evidence underlines the continued challenges faced by LLMs, including ChatGPT and GPT-4, in mastering this task, signifying the need for future exploration to enhance their abilities.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.160.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the attention previous research has given to word analogies, this work suggests that Large Language Models (LLMs) often overlook the structures that underpin these analogies, raising questions about the efficacy of word analogies as a measure of analogical reasoning skills akin to human cognition.\"; \"The empirical evidence underlines the continued challenges faced by LLMs, including ChatGPT and",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Despite the attention previous research has given to word analogies, this work suggests that Large Language Models (LLMs) often overlook the structures that underpin these analogies, raising questions about the efficacy of word analogies as a measure of analogical reasoning skills akin to human cognition.\"; \"The empirical evidence underlines the continued challenges faced by LLMs, including ChatGPT and"
    },
    {
        "title": "RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder for Language Modeling",
        "authors": [
            "Jingcheng Deng",
            "Liang Pang",
            "Huawei Shen",
            "Xueqi Cheng"
        ],
        "published": "2023",
        "summary": "Retrieval-augmented language models show promise in addressing issues like outdated information and hallucinations in language models (LMs). However, current research faces two main problems: 1) determining what information to retrieve, and 2) effectively combining retrieved information during generation. We argue that valuable retrieved information should not only be related to the current source text but also consider the future target text, given the nature of LMs that model future tokens. Moreover, we propose that aggregation using latent variables derived from a compact latent space is more efficient than utilizing explicit raw text, which is limited by context length and susceptible to noise. Therefore, we introduce RegaVAE, a retrieval-augmented language model built upon the variational auto-encoder (VAE). It encodes the text corpus into a latent space, capturing current and future information from both source and target text. Additionally, we leverage the VAE to initialize the latent space and adopt the probabilistic form of the retrieval generation paradigm by expanding the Gaussian prior distribution into a Gaussian mixture distribution. Theoretical analysis provides an optimizable upper bound for RegaVAE. Experimental results on various datasets demonstrate significant improvements in text generation quality and hallucination removal.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.164.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Retrieval-augmented language models show promise in addressing issues like outdated information and hallucinations in language models (LMs).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Retrieval-augmented language models show promise in addressing issues like outdated information and hallucinations in language models (LMs).\""
    },
    {
        "title": "RefGPT: Dialogue Generation of GPT, by GPT, and for GPT",
        "authors": [
            "Dongjie Yang",
            "Ruifeng Yuan",
            "Yuantao Fan",
            "Yifei Yang",
            "Zili Wang",
            "Shusen Wang",
            "Hai Zhao"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have attained the impressive capability to resolve a wide range of NLP tasks by fine-tuning high-quality instruction data. However, collecting human-written data of high quality, especially multi-turn dialogues, is expensive and unattainable for most people. Though previous studies have used powerful LLMs to generate the dialogues automatically, they all suffer from generating untruthful dialogues because of the model hallucination. Therefore, we propose a method called RefGPT to generate enormous truthful and customized dialogues without worrying about factual errors caused by the model hallucination. RefGPT solves the model hallucination in dialogue generation by restricting the LLMs to leverage the given reference instead of reciting their own knowledge to generate dialogues. Additionally, RefGPT adds detailed controls on every utterance to enable high customization capability, which previous studies have ignored. On the basis of RefGPT, we also propose two high-quality dialogue datasets generated by GPT-4, namely **RefGPT-Fact** and **RefGPT-Code**. RefGPT-Fact is a dataset with 100k multi-turn dialogues based on factual knowledge and RefGPT-Code has 76k multi-turn dialogues covering a wide range of coding scenarios. Our code and datasets are released in https://github.com/mutonix/RefGPT.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.165.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Though previous studies have used powerful LLMs to generate the dialogues automatically, they all suffer from generating untruthful dialogues because of the model hallucination.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Though previous studies have used powerful LLMs to generate the dialogues automatically, they all suffer from generating untruthful dialogues because of the model hallucination.\""
    },
    {
        "title": "INA: An Integrative Approach for Enhancing Negotiation Strategies with Reward-Based Dialogue Agent",
        "authors": [
            "Zishan Ahmad",
            "Suman Saurabh",
            "Vaishakh Menon",
            "Asif Ekbal",
            "Roshni Ramnani",
            "Anutosh Maitra"
        ],
        "published": "2023",
        "summary": "In this paper, we propose a novel negotiation agent designed for the online marketplace. Our dialogue agent is integrative in nature i.e, it possesses the capability to negotiate on price as well as other factors, such as the addition or removal of items from a deal bundle, thereby offering a more flexible and comprehensive negotiation experience. To enable this functionality, we create a new dataset called Integrative Negotiation Dataset (IND). For this dataset creation, we introduce a new semi-automated data creation method, which combines defining negotiation intents, actions, and intent-action simulation between users and the agent to generate potential dialogue flows. Finally, the prompting of GPT-J, a state-of-the-art language model, is done to generate dialogues for a given intent, with a human-in-the-loop process for post-editing and refining minor errors to ensure high data quality. We first train a maximum likelihood loss based model on IND, and then employ a set of novel rewards specifically tailored for the negotiation task to train our Integrative Negotiation Agent (INA). These rewards incentivize the agent to learn effective negotiation strategies that can adapt to various contextual requirements and price proposals. We train our model and conduct experiments to evaluate the effectiveness of our reward-based dialogue agent for negotiation. Our results demonstrate that the proposed approach and reward functions significantly enhance the negotiation capabilities of the dialogue agent. The INA successfully engages in integrative negotiations, displaying the ability to dynamically adjust prices and negotiate the inclusion or exclusion of items in a deal bundle.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.166.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the prompting of GPT-J, a state-of-the-art language model, is done to generate dialogues for a given intent, with a human-in-the-loop process for post-editing and refining minor errors to ensure high data quality.\"\n\nThis rating is given because the abstract mentions LLMs but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"the prompting of GPT-J, a state-of-the-art language model, is done to generate dialogues for a given intent, with a human-in-the-loop process for post-editing and refining minor errors to ensure high data quality.\"\n\nThis rating is given because the abstract mentions LLMs but does not discuss any limitations of LLMs."
    },
    {
        "title": "Large Language Models are Better Reasoners with Self-Verification",
        "authors": [
            "Yixuan Weng",
            "Minjun Zhu",
            "Fei Xia",
            "Bin Li",
            "Shizhu He",
            "Shengping Liu",
            "Bin Sun",
            "Kang Liu",
            "Jun Zhao"
        ],
        "published": "2023",
        "summary": "Recently, with the chain of thought (CoT) prompting, large language models (LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural language processing tasks such as arithmetic, commonsense, and logical reasoning. However, LLMs with CoT require multi-step prompting and multi-token prediction, which is highly sensitive to individual mistakes and vulnerable to error accumulation. The above issues make the LLMs need the ability to verify the answers. In fact, after inferring conclusions in some thinking decision tasks, people often check them by re-verifying steps to avoid some mistakes. In this paper, we propose and prove that LLMs also have similar self-verification abilities. We take the conclusion obtained by CoT as one of the conditions for solving the original problem. By performing a backward verification of the answers that LLM deduced for itself, we can obtain interpretable answer validation scores to select the candidate answer with the highest score. Experimental results demonstrate that the proposed method can improve the reasoning performance on various arithmetic, commonsense, and logical reasoning datasets. Our code is publicly available at: https://github.com/WENGSYX/Self-Verification.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.167.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, LLMs with CoT require multi-step prompting and multi-token prediction, which is highly sensitive to individual mistakes and vulnerable to error accumulation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, LLMs with CoT require multi-step prompting and multi-token prediction, which is highly sensitive to individual mistakes and vulnerable to error accumulation.\""
    },
    {
        "title": "For Generated Text, Is NLI-Neutral Text the Best Text?",
        "authors": [
            "Michail Mersinias",
            "Kyle Mahowald"
        ],
        "published": "2023",
        "summary": "We explore incorporating natural language inference (NLI) into the text generative pipeline by using a pre-trained NLI model to assess whether a generated sentence entails, contradicts, or is neutral to the prompt and preceding text. First, we show that the NLI task is predictive of generation errors made by GPT-3. We use these results to develop an NLI-informed generation procedure for GPT-J. Then, we evaluate these generations by obtaining human annotations on error types and overall quality. We find that an NLI strategy of maximizing entailment improves text generation when the nucleus sampling randomness parameter value is high, while one which maximizes contradiction is in fact productive when the parameter value is low. Overall, though, we demonstrate that an NLI strategy of maximizing the neutral class provides the highest quality of generated text (significantly better than the vanilla generations), regardless of parameter value.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.170.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"First, we show that the NLI task is predictive of generation errors made by GPT-3.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"First, we show that the NLI task is predictive of generation errors made by GPT-3.\""
    },
    {
        "title": "Multi-label and Multi-target Sampling of Machine Annotation for Computational Stance Detection",
        "authors": [
            "Zhengyuan Liu",
            "Hai Leong Chieu",
            "Nancy Chen"
        ],
        "published": "2023",
        "summary": "Data collection from manual labeling provides domain-specific and task-aligned supervision for data-driven approaches, and a critical mass of well-annotated resources is required to achieve reasonable performance in natural language processing tasks. However, manual annotations are often challenging to scale up in terms of time and budget, especially when domain knowledge, capturing subtle semantic features, and reasoning steps are needed. In this paper, we investigate the efficacy of leveraging large language models on automated labeling for computational stance detection. We empirically observe that while large language models show strong potential as an alternative to human annotators, their sensitivity to task-specific instructions and their intrinsic biases pose intriguing yet unique challenges in machine annotation. We introduce a multi-label and multi-target sampling strategy to optimize the annotation quality. Experimental results on the benchmark stance detection corpora show that our method can significantly improve performance and learning efficacy.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.174.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"their sensitivity to task-specific instructions and their intrinsic biases pose intriguing yet unique challenges in machine annotation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"their sensitivity to task-specific instructions and their intrinsic biases pose intriguing yet unique challenges in machine annotation.\""
    },
    {
        "title": "In What Languages are Generative Language Models the Most Formal? Analyzing Formality Distribution across Languages",
        "authors": [
            "Asım Ersoy",
            "Gerson Vizcarra",
            "Tahsin Mayeesha",
            "Benjamin Muller"
        ],
        "published": "2023",
        "summary": "Multilingual generative language models (LMs) are increasingly fluent in a large variety of languages. Trained on the concatenation of corpora in multiple languages, they enable powerful transfer from high-resource languages to low-resource ones. However, it is still unknown what cultural biases are induced in the predictions of these models. In this work, we focus on one language property highly influenced by culture: formality. We analyze the formality distributions of XGLM and BLOOM’s predictions, two popular generative multilingual language models, in 5 languages. We classify 1,200 generations per language as formal, informal, or incohesive and measure the impact of the prompt formality on the predictions. Overall, we observe a diversity of behaviors across the models and languages. For instance, XGLM generates informal text in Arabic and Bengali when conditioned with informal prompts, much more than BLOOM. In addition, even though both models are highly biased toward the formal style when prompted neutrally, we find that the models generate a significant amount of informal predictions even when prompted with formal text. We release with this work 6,000 annotated samples, paving the way for future work on the formality of generative multilingual LMs.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.175.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We observe a diversity of behaviors across the models and languages. For instance, XGLM generates informal text in Arabic and Bengali when conditioned with informal prompts, much more than BLOOM. In addition, even though both models are highly biased toward the formal style when prompted neutrally, we find that the models generate a significant amount of informal predictions even when prompted with formal text",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We observe a diversity of behaviors across the models and languages. For instance, XGLM generates informal text in Arabic and Bengali when conditioned with informal prompts, much more than BLOOM. In addition, even though both models are highly biased toward the formal style when prompted neutrally, we find that the models generate a significant amount of informal predictions even when prompted with formal text"
    },
    {
        "title": "Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning",
        "authors": [
            "Jinyuan Wang",
            "Junlong Li",
            "Hai Zhao"
        ],
        "published": "2023",
        "summary": "In open-domain question-answering (ODQA), most existing questions require single-hop reasoning on commonsense. To further extend this task, we officially introduce open-domain multi-hop reasoning (ODMR) by answering multi-hop questions with explicit reasoning steps in open-domain setting. Recently, large language models (LLMs) have found significant utility in facilitating ODQA without external corpus. Furthermore, chain-of-thought (CoT) prompting boosts the reasoning capability of LLMs to a greater extent with manual or automated paradigms. However, existing automated methods lack of quality assurance, while manual approaches suffer from limited scalability and poor diversity, hindering the capabilities of LLMs. In this paper, we propose Self-prompted Chain-of-Thought (SP-CoT), an automated framework to mass-produce high quality CoTs of LLMs, by LLMs and for LLMs. SP-CoT introduces an automated generation pipeline of high quality ODMR datasets, an adaptive sampler for in-context CoT selection and self-prompted inference via in-context learning. Extensive experiments on four multi-hop question-answering benchmarks show that our proposed SP-CoT not only significantly surpasses the previous SOTA methods on large-scale (175B) LLMs, but also nearly doubles the zero-shot performance of small-scale (13B) LLMs. Further analysis reveals the remarkable capability of SP-CoT to elicit direct and concise intermediate reasoning steps by recalling ~50% of intermediate answers on MuSiQue-Ans dataset.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.179.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing automated methods lack of quality assurance, while manual approaches suffer from limited scalability and poor diversity, hindering the capabilities of LLMs.\"\n\nPlease note that the limitation mentioned in the abstract is brief and does not receive significant attention in the paper. The primary focus is on the proposed solution and its results.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, existing automated methods lack of quality assurance, while manual approaches suffer from limited scalability and poor diversity, hindering the capabilities of LLMs.\"\n\nPlease note that the limitation mentioned in the abstract is brief and does not receive significant attention in the paper. The primary focus is on the proposed solution and its results."
    },
    {
        "title": "CASE: Commonsense-Augmented Score with an Expanded Answer Space",
        "authors": [
            "Wenkai Chen",
            "Sahithya Ravi",
            "Vered Shwartz"
        ],
        "published": "2023",
        "summary": "LLMs have demonstrated impressive zero-shot performance on NLP tasks thanks to the knowledge they acquired in their training. In multiple-choice QA tasks, the LM probabilities are used as an imperfect measure of the plausibility of each answer choice. One of the major limitations of the basic score is that it treats all words as equally important. We propose CASE, a Commonsense-Augmented Score with an Expanded Answer Space. CASE addresses this limitation by assigning importance weights for individual words based on their semantic relations to other words in the input. The dynamic weighting approach outperforms basic LM scores, not only because it reduces noise from unimportant words, but also because it informs the model of implicit commonsense knowledge that may be useful for answering the question. We then also follow prior work in expanding the answer space by generating lexically-divergent answers that are conceptually-similar to the choices. When combined with answer space expansion, our method outperforms strong baselines on 5 commonsense benchmarks. We further show these two approaches are complementary and may be especially beneficial when using smaller LMs.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.180.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"One of the major limitations of the basic score is that it treats all words as equally important.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"One of the major limitations of the basic score is that it treats all words as equally important.\""
    },
    {
        "title": "GRENADE: Graph-Centric Language Model for Self-Supervised Representation Learning on Text-Attributed Graphs",
        "authors": [
            "Yichuan Li",
            "Kaize Ding",
            "Kyumin Lee"
        ],
        "published": "2023",
        "summary": "Self-supervised representation learning on text-attributed graphs, which aims to create expressive and generalizable representations for various downstream tasks, has received increasing research attention lately. However, existing methods either struggle to capture the full extent of structural context information or rely on task-specific training labels, which largely hampers their effectiveness and generalizability in practice. To solve the problem of self-supervised representation learning on text-attributed graphs, we develop a novel Graph-Centric Language model – GRENADE. Specifically, GRENADE harnesses the synergy of both pre-trained language model and graph neural network by optimizing with two specialized self-supervised learning algorithms: graph-centric contrastive learning and graph-centric knowledge alignment. The proposed graph-centric self-supervised learning algorithms effectively help GRENADE to capture informative textual semantics as well as structural context information on text-attributed graphs. Through extensive experiments, GRENADE shows its superiority over state-of-the-art methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.181.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing methods either struggle to capture the full extent of structural context information or rely on task-specific training labels, which largely hampers their effectiveness and generalizability in practice.\"\n\n(Note: Although the abstract mentions a limitation, it is not specific to LLMs, but rather to existing methods in general. However, since the paper is about developing a new model that utilizes",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, existing methods either struggle to capture the full extent of structural context information or rely on task-specific training labels, which largely hampers their effectiveness and generalizability in practice.\"\n\n(Note: Although the abstract mentions a limitation, it is not specific to LLMs, but rather to existing methods in general. However, since the paper is about developing a new model that utilizes"
    },
    {
        "title": "Sources of Hallucination by Large Language Models on Inference Tasks",
        "authors": [
            "Nick McKenna",
            "Tianyi Li",
            "Liang Cheng",
            "Mohammad Hosseini",
            "Mark Johnson",
            "Mark Steedman"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) are claimed to be capable of Natural Language Inference (NLI), necessary for applied tasks like question answering and summarization. We present a series of behavioral studies on several LLM families (LLaMA, GPT-3.5, and PaLM) which probe their behavior using controlled experiments. We establish two biases originating from pretraining which predict much of their behavior, and show that these are major sources of hallucination in generative LLMs. First, memorization at the level of sentences: we show that, regardless of the premise, models falsely label NLI test samples as entailing when the hypothesis is attested in training data, and that entities are used as “indices’ to access the memorized data. Second, statistical patterns of usage learned at the level of corpora: we further show a similar effect when the premise predicate is less frequent than that of the hypothesis in the training data, a bias following from previous studies. We demonstrate that LLMs perform significantly worse on NLI test samples which do not conform to these biases than those which do, and we offer these as valuable controls for future LLM evaluation.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.182.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We establish two biases originating from pretraining which predict much of their behavior, and show that these are major sources of hallucination in generative LLMs. First, memorization at the level of sentences... Second, statistical patterns of usage learned at the level of corpora...\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"We establish two biases originating from pretraining which predict much of their behavior, and show that these are major sources of hallucination in generative LLMs. First, memorization at the level of sentences... Second, statistical patterns of usage learned at the level of corpora...\""
    },
    {
        "title": "Prompting ChatGPT in MNER: Enhanced Multimodal Named Entity Recognition with Auxiliary Refined Knowledge",
        "authors": [
            "Jinyuan Li",
            "Han Li",
            "Zhuo Pan",
            "Di Sun",
            "Jiahao Wang",
            "Wenkun Zhang",
            "Gang Pan"
        ],
        "published": "2023",
        "summary": "Multimodal Named Entity Recognition (MNER) on social media aims to enhance textual entity prediction by incorporating image-based clues. Existing studies mainly focus on maximizing the utilization of pertinent image information or incorporating external knowledge from explicit knowledge bases. However, these methods either neglect the necessity of providing the model with external knowledge, or encounter issues of high redundancy in the retrieved knowledge. In this paper, we present PGIM — a two-stage framework that aims to leverage ChatGPT as an implicit knowledge base and enable it to heuristically generate auxiliary knowledge for more efficient entity prediction. Specifically, PGIM contains a Multimodal Similar Example Awareness module that selects suitable examples from a small number of predefined artificial samples. These examples are then integrated into a formatted prompt template tailored to the MNER and guide ChatGPT to generate auxiliary refined knowledge. Finally, the acquired knowledge is integrated with the original text and fed into a downstream model for further processing. Extensive experiments show that PGIM outperforms state-of-the-art methods on two classic MNER datasets and exhibits a stronger robustness and generalization capability.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.184.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing studies mainly focus on maximizing the utilization of pertinent image information or incorporating external knowledge from explicit knowledge bases. However, these methods either neglect the necessity of providing the model with external knowledge, or encounter issues of high redundancy in the retrieved knowledge.\"\n\nThis paper discusses the limitations of existing methods in utilizing external knowledge, but does not explicitly discuss the limitations of Large Language Models (LLMs)",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Existing studies mainly focus on maximizing the utilization of pertinent image information or incorporating external knowledge from explicit knowledge bases. However, these methods either neglect the necessity of providing the model with external knowledge, or encounter issues of high redundancy in the retrieved knowledge.\"\n\nThis paper discusses the limitations of existing methods in utilizing external knowledge, but does not explicitly discuss the limitations of Large Language Models (LLMs)"
    },
    {
        "title": "Understanding HTML with Large Language Models",
        "authors": [
            "Izzeddin Gur",
            "Ofir Nachum",
            "Yingjie Miao",
            "Mustafa Safdari",
            "Austin Huang",
            "Aakanksha Chowdhery",
            "Sharan Narang",
            "Noah Fiedel",
            "Aleksandra Faust"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have shown exceptional performance on a variety of natural language tasks. Yet, their capabilities for HTML understanding – i.e., parsing the raw HTML of a webpage, with applications to automation of web-based tasks, crawling, and browser-assisted retrieval – have not been fully explored. We contribute HTML understanding models (fine-tuned LLMs) and an in-depth analysis of their capabilities under three tasks: (i) Semantic Classification of HTML elements, (ii) Description Generation for HTML inputs, and (iii) Autonomous Web Navigation of HTML pages. While previous work has developed dedicated architectures and training procedures for HTML understanding, we show that LLMs pretrained on standard natural language corpora transfer remarkably well to HTML understanding tasks. For instance, when fine-tuned on data from the MiniWoB benchmark, LLMs successfully complete 50% more tasks using 192x less data compared to the previous best supervised model. We create and open-source a large-scale HTML dataset distilled and auto-labeled from CommonCrawl",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.185.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, the paper only mentions the lack of exploration of their capabilities for HTML understanding.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, the paper only mentions the lack of exploration of their capabilities for HTML understanding."
    },
    {
        "title": "UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model",
        "authors": [
            "Jiabo Ye",
            "Anwen Hu",
            "Haiyang Xu",
            "Qinghao Ye",
            "Ming Yan",
            "Guohai Xu",
            "Chenliang Li",
            "Junfeng Tian",
            "Qi Qian",
            "Ji Zhang",
            "Qin Jin",
            "Liang He",
            "Xin Lin",
            "Fei Huang"
        ],
        "published": "2023",
        "summary": "Text is ubiquitous in our visual world, conveying crucial information, such as in documents, websites, and everyday photographs. In this work, we propose UReader, a first exploration of universal OCR-free visually-situated language understanding based on the Multimodal Large Language Model (MLLM). By leveraging the shallow text recognition ability of the MLLM, we only finetuned 1.2% parameters and the training cost is much lower than previous work following domain-specific pretraining and finetuning paradigms. Concretely, UReader is jointly finetuned on a wide range of Visually-situated Language Understanding tasks via a unified instruction format. To enhance the visual text and semantic understanding, we further apply two auxiliary tasks with the same format, namely text reading and key points generation tasks. We design a shape-adaptive cropping module before the encoder-decoder architecture of MLLM to leverage the frozen low-resolution vision encoder for processing high-resolution images. Without downstream finetuning, our single model achieves state-of-the-art ocr-free performance in 8 out of 10 visually-situated language understanding tasks, across 5 domains: documents, tables, charts, natural images, and webpage screenshots. Codes and instruction-tuning datasets will be released.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.187.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned in the abstract."
    },
    {
        "title": "Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback",
        "authors": [
            "Wei Shen",
            "Rui Zheng",
            "Wenyu Zhan",
            "Jun Zhao",
            "Shihan Dou",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2023",
        "summary": "Reinforcement learning from human feedback serves as a crucial bridge, aligning large language models with human and societal values. This alignment requires a vast corpus of human feedback to learn a reward model, which is subsequently used to finetune language models. However, we have identified that the reward model often finds shortcuts to bypass its intended objectives, misleadingly assuming that humans prefer longer responses. The emergence of length bias often induces the model to favor longer outputs, yet it doesn’t equate to an increase in helpful information within these outputs. In this paper, we propose an innovative solution, applying the Product-of-Experts (PoE) technique to separate reward modeling from the influence of sequence length. In our framework, the main expert concentrates on understanding human intents, while the biased expert targets the identification and capture of length bias. To further enhance the learning of bias, we introduce perturbations into the bias-focused expert, disrupting the flow of semantic information. Experimental results validate the effectiveness of our approach, indicating that language model performance is improved, irrespective of sequence length.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.188.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, we have identified that the reward model often finds shortcuts to bypass its intended objectives, misleadingly assuming that humans prefer longer responses. The emergence of length bias often induces the model to favor longer outputs, yet it doesn’t equate to an increase in helpful information within these outputs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, we have identified that the reward model often finds shortcuts to bypass its intended objectives, misleadingly assuming that humans prefer longer responses. The emergence of length bias often induces the model to favor longer outputs, yet it doesn’t equate to an increase in helpful information within these outputs.\""
    },
    {
        "title": "Filling the Image Information Gap for VQA: Prompting Large Language Models to Proactively Ask Questions",
        "authors": [
            "Ziyue Wang",
            "Chi Chen",
            "Peng Li",
            "Yang Liu"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) demonstrate impressive reasoning ability and the maintenance of world knowledge not only in natural language tasks, but also in some vision-language tasks such as open-domain knowledge-based visual question answering (OK-VQA). As images are invisible to LLMs, researchers convert images to text to engage LLMs into the visual question reasoning procedure. This leads to discrepancies between images and their textual representations presented to LLMs, which consequently impedes final reasoning performance. To fill the information gap and better leverage the reasoning capability, we design a framework that enables LLMs to proactively ask relevant questions to unveil more details in the image, along with filters for refining the generated information. We validate our idea on OK-VQA and A-OKVQA. Our method continuously boosts the performance of baselines methods by an average gain of 2.15% on OK-VQA, and achieves consistent improvements across different LLMs.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.189.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As images are invisible to LLMs, researchers convert images to text to engage LLMs into the visual question reasoning procedure. This leads to discrepancies between images and their textual representations presented to LLMs, which consequently impedes final reasoning performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"As images are invisible to LLMs, researchers convert images to text to engage LLMs into the visual question reasoning procedure. This leads to discrepancies between images and their textual representations presented to LLMs, which consequently impedes final reasoning performance.\""
    },
    {
        "title": "Take a Closer Look at Multilinguality! Improve Multilingual Pre-Training Using Monolingual Corpora Only",
        "authors": [
            "Jinliang Lu",
            "Yu Lu",
            "Jiajun Zhang"
        ],
        "published": "2023",
        "summary": "Recent studies have revealed the remarkable cross-lingual capability of multilingual pre-trained language models (mPLMs), even when pre-trained without parallel corpora (mono-mPLMs). Intuitively, semantic alignments may be the reason behind such capability but remain under-explored. In this work, we investigate the alignment properties from the token perspective in mono-mPLMs and find that the alignments correspond to the geometric similarity of embedding space across different languages. Nevertheless, mono-mPLMs tend to damage this geometric similarity at the higher layers due to the lack of cross-lingual interactions, thus limiting their cross-lingual transfer capabilities. To address this issue, we introduce token-level and semantic-level code-switched masked language modeling, employing the self-induced token alignments to explicitly improve cross-lingual interactions over layers of mono-mPLMs without relying on parallel sentences. We evaluate our method on various natural language understanding tasks and unsupervised machine translation tasks. The results demonstrate that our methods outperform the strong baselines and achieve comparable performance with mPLMs trained with parallel corpora.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.190.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Nevertheless, mono-mPLMs tend to damage this geometric similarity at the higher layers due to the lack of cross-lingual interactions, thus limiting their cross-lingual transfer capabilities.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Nevertheless, mono-mPLMs tend to damage this geometric similarity at the higher layers due to the lack of cross-lingual interactions, thus limiting their cross-lingual transfer capabilities.\""
    },
    {
        "title": "LogiCoT: Logical Chain-of-Thought Instruction Tuning",
        "authors": [
            "Hanmeng Liu",
            "Zhiyang Teng",
            "Leyang Cui",
            "Chaoli Zhang",
            "Qiji Zhou",
            "Yue Zhang"
        ],
        "published": "2023",
        "summary": "Generative Pre-trained Transformer 4 (GPT-4) demonstrates impressive chain-of-thought reasoning ability. Recent work on self-instruction tuning, such as Alpaca, has focused on enhancing the general proficiency of models. These instructions enable the model to achieve performance comparable to GPT-3.5 on general tasks like open-domain text generation and paraphrasing. However, they fall short of helping the model handle complex reasoning tasks. To bridge the gap, this paper presents LogiCoT, a new instruction-tuning dataset for Logical Chain-of-Thought reasoning with GPT-4. We elaborate on the process of harvesting instructions for prompting GPT-4 to generate chain-of-thought rationales. LogiCoT serves as an instruction set for teaching models of logical reasoning and elicits general reasoning skills.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.191.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they fall short of helping the model handle complex reasoning tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, they fall short of helping the model handle complex reasoning tasks.\""
    },
    {
        "title": "Reducing Spurious Correlations in Aspect-based Sentiment Analysis with Explanation from Large Language Models",
        "authors": [
            "Qianlong Wang",
            "Keyang Ding",
            "Bin Liang",
            "Min Yang",
            "Ruifeng Xu"
        ],
        "published": "2023",
        "summary": "Recently, aspect-based sentiment analysis (ABSA) models have yielded promising results. However, they are susceptible to learning spurious correlations between certain words of the input text and output labels while modeling the sentiment feature of the aspect. This spurious correlation will potentially undermine the performance of ABSA models. One direct solution for this problem is to make the model see and learn an explanation of sentiment expression rather than certain words. Motivated by this, we exploit explanations for the sentiment polarity of each aspect from large language models (LLMs) to reduce spurious correlations in ABSA. First, we formulate a prompt template that wraps the sentence, an aspect, and the sentiment label. This template is utilized to prompt LLMs to generate an appropriate explanation that states the sentiment cause. Then, we propose two straightforward yet effective methods to leverage the explanation for preventing the learning of spurious correlations. We conducted extensive comparative experiments on five datasets by integrating them with some representative ABSA models. Results show that our methods can achieve performance gains and enhance the performance and generalization ability of ABSA models.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.193.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "not extracted",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.\n\nThis abstract does not mention any limitations of Large Language Models, but instead uses LLMs as a tool to solve a problem in aspect-based sentiment analysis.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs:  \nEvidence: No evidence of discussion of limitations of LLMs.\n\nThis abstract does not mention any limitations of Large Language Models, but instead uses LLMs as a tool to solve a problem in aspect-based sentiment analysis."
    },
    {
        "title": "High-quality argumentative information in low resources approaches improve counter-narrative generation",
        "authors": [
            "Damián Furman",
            "Pablo Torres",
            "José Rodríguez",
            "Diego Letzen",
            "Maria Martinez",
            "Laura Alemany"
        ],
        "published": "2023",
        "summary": "It has been shown that high quality fine-tuning boosts the performance of language models, even if the size of the fine-tuning is small. In this work we show how highly targeted fine-tuning improves the task of hate speech counter-narrative generation in user-generated text, even for very small sizes of training (1722 counter-narratives for English and 355 for Spanish). Providing a small subset of examples focusing on single argumentative strategies, together with the argumentative analysis relevant to that strategy, yields counter-narratives that are as satisfactory as providing the whole set of counter-narratives. We also show that a good base model is required for the fine-tuning to have a positive impact. Indeed, for Spanish, the counter-narratives obtained without fine-tuning are mostly unacceptable, and, while fine-tuning improves their overall quality, the performance still remains quite unsatisfactory.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.194.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Indeed, for Spanish, the counter-narratives obtained without fine-tuning are mostly unacceptable, and, while fine-tuning improves their overall quality, the performance still remains quite unsatisfactory.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Indeed, for Spanish, the counter-narratives obtained without fine-tuning are mostly unacceptable, and, while fine-tuning improves their overall quality, the performance still remains quite unsatisfactory.\""
    },
    {
        "title": "In-context Learning for Few-shot Multimodal Named Entity Recognition",
        "authors": [
            "Chenran Cai",
            "Qianlong Wang",
            "Bin Liang",
            "Bing Qin",
            "Min Yang",
            "Kam-Fai Wong",
            "Ruifeng Xu"
        ],
        "published": "2023",
        "summary": "Thanks in part to the availability of copious annotated resources for some entity categories, existing studies have achieved superior performance in multimodal named entity recognition (MNER). However, in the real-world scenario, it is infeasible to enumerate all entity categories in advance. Therefore, in this paper, we formulate a new few-shot multimodal named entity recognition (FewMNER) task, which aims to effectively locate and identify named entities for a text-image pair only using a small number of labeled examples. Further, we explore the merit of in-context learning (ICL) and propose a novel framework to deal with FewMNER, where three points are taken into account: i.e., converting visual modality, selecting useful examples, and designing an effective task demonstration. Specifically, we first employ an image caption model to convert images into textual descriptions, enabling large language models to absorb information from visual modality. Then, we use the ranking of the sum of similarity rankings from both text and image modalities to select k-nearest examples, which form a demonstration context. Finally, we utilize the MNER definition and the meaning of each entity category as effective instruction. Extensive experimental results demonstrate that our framework outperforms baselines under several few-shot settings.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.196.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"enabling large language models to absorb information from visual modality.\"\n\nThis paper mentions LLMs but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"enabling large language models to absorb information from visual modality.\"\n\nThis paper mentions LLMs but does not discuss any limitations of LLMs."
    },
    {
        "title": "Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning",
        "authors": [
            "An-Zi Yen",
            "Wei-Ling Hsu"
        ],
        "published": "2023",
        "summary": "Due to the remarkable language understanding and generation abilities of large language models (LLMs), their use in educational applications has been explored. However, little work has been done on investigating the pedagogical ability of LLMs in helping students to learn mathematics. In this position paper, we discuss the challenges associated with employing LLMs to enhance students’ mathematical problem-solving skills by providing adaptive feedback. Apart from generating the wrong reasoning processes, LLMs can misinterpret the meaning of the question, and also exhibit difficulty in understanding the given questions’ rationales when attempting to correct students’ answers. Three research questions are formulated.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.201.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Apart from generating the wrong reasoning processes, LLMs can misinterpret the meaning of the question, and also exhibit difficulty in understanding the given questions’ rationales when attempting to correct students’ answers.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Apart from generating the wrong reasoning processes, LLMs can misinterpret the meaning of the question, and also exhibit difficulty in understanding the given questions’ rationales when attempting to correct students’ answers.\""
    },
    {
        "title": "Dynamic Voting for Efficient Reasoning in Large Language Models",
        "authors": [
            "Mingfeng Xue",
            "Dayiheng Liu",
            "Wenqiang Lei",
            "Xingzhang Ren",
            "Baosong Yang",
            "Jun Xie",
            "Yidan Zhang",
            "Dezhong Peng",
            "Jiancheng Lv"
        ],
        "published": "2023",
        "summary": "Multi-path voting methods like Self-consistency have been used to mitigate reasoning errors in large language models caused by factual errors and illusion generation. However, these methods require excessive computing resources as they generate numerous reasoning paths for each problem. And our experiments show that on the arithmetic reasoning task, SVAMP, half of the problems fail to obtain noticeable accuracy gains when voting with more than three paths. In this paper, we propose a novel multi-path voting technique called Dynamic Voting, which effectively reduces the number of reasoning paths during multi-path voting while preserving accuracies by applying early exiting for problems that large language models can confidently solve. Experimental evaluations on arithmetic, commonsense, and symbolic reasoning tasks under few-shot and zero-shot settings demonstrate that Dynamic Voting achieves comparable accuracies employing significantly fewer reasoning paths. Notably, one of our Dynamic Voting strategies outperforms Self-consistency using only 24.7% of the number of paths on the LetterConcat task in the few-shot setting. Furthermore, Dynamic Voting showcases strong robustness in threshold selection. It also demonstrates excellent generalizability when combined with other voting techniques, different models, and diverse prompts.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.203.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Multi-path voting methods like Self-consistency have been used to mitigate reasoning errors in large language models caused by factual errors and illusion generation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Multi-path voting methods like Self-consistency have been used to mitigate reasoning errors in large language models caused by factual errors and illusion generation.\""
    },
    {
        "title": "AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models",
        "authors": [
            "Siqi Ouyang",
            "Lei Li"
        ],
        "published": "2023",
        "summary": "Recent large language models (LLMs) are promising for making decisions in grounded environments. However, LLMs frequently fail in complex decision-making tasks due to the misalignment between the pre-trained knowledge in LLMs and the actual rules in the environment. Existing methods require either costly gradient computation or lengthy in-context demonstrations. In this paper, we propose AutoPlan, an approach to guide LLM-based agents to accomplish interactive decision-making tasks. AutoPlan augments the LLM prompt with a task-solving plan and optimizes it through iterative experience collection and reflection. Our experiments show that AutoPlan, though using no in-context demonstrations, achieves success rates on par with the baselines using human-written demonstrations on ALFWorld and even outperforms them by 8% on HotpotQA. The code is available at https://github.com/owaski/AutoPlan.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.205.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, LLMs frequently fail in complex decision-making tasks due to the misalignment between the pre-trained knowledge in LLMs and the actual rules in the environment.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, LLMs frequently fail in complex decision-making tasks due to the misalignment between the pre-trained knowledge in LLMs and the actual rules in the environment.\""
    },
    {
        "title": "Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering",
        "authors": [
            "Sukmin Cho",
            "Jeongyeon Seo",
            "Soyeong Jeong",
            "Jong Park"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) enable zero-shot approaches in open-domain question answering (ODQA), yet with limited advancements as the reader is compared to the retriever. This study aims at the feasibility of a zero-shot reader that addresses the challenges of computational cost and the need for labeled data. We find that LLMs are distracted due to irrelevant documents in the retrieved set and the overconfidence of the generated answers when they are exploited as zero-shot readers. To tackle these problems, we mitigate the impact of such documents via Distraction-aware Answer Selection (DAS) with a negation-based instruction and score adjustment for proper answer selection. Experimental results show that our approach successfully handles distraction across diverse scenarios, enhancing the performance of zero-shot readers. Furthermore, unlike supervised readers struggling with unseen data, zero-shot readers demonstrate outstanding transferability without any training.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.207.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that LLMs are distracted due to irrelevant documents in the retrieved set and the overconfidence of the generated answers when they are exploited as zero-shot readers.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We find that LLMs are distracted due to irrelevant documents in the retrieved set and the overconfidence of the generated answers when they are exploited as zero-shot readers.\""
    },
    {
        "title": "Can you Summarize my learnings? Towards Perspective-based Educational Dialogue Summarization",
        "authors": [
            "Raghav Jain",
            "Tulika Saha",
            "Jhagrut Lalwani",
            "Sriparna Saha"
        ],
        "published": "2023",
        "summary": "The steady increase in the utilization of Virtual Tutors (VT) over recent years has allowed for a more efficient, personalized, and interactive AI-based learning experiences. A vital aspect in these educational chatbots is summarizing the conversations between the VT and the students, as it is critical in consolidating learning points and monitoring progress. However, the approach to summarization should be tailored according to the perspective. Summarization from the VTs perspective should emphasize on its teaching efficiency and potential improvements. Conversely, student-oriented summaries should distill learning points, track progress, and suggest scope for improvements. Based on this hypothesis, in this work, we propose a new task of Multi-modal Perspective based Dialogue Summarization (MM-PerSumm), demonstrated in an educational setting. Towards this aim, we introduce a novel dataset, CIMA-Summ that summarizes educational dialogues from three unique perspectives: the Student, the Tutor, and a Generic viewpoint. In addition, we propose an Image and Perspective-guided Dialogue Summarization (IP-Summ) model which is a Seq2Seq language model incorporating (i) multi-modal learning from images and (ii) a perspective-based encoder that constructs a dialogue graph capturing the intentions and actions of both the VT and the student, enabling the summarization of a dialogue from diverse perspectives. Lastly, we conduct detailed analyses of our model’s performance, highlighting the aspects that could lead to optimal modeling of IP-Summ.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.208.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"a Seq2Seq language model\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"a Seq2Seq language model\""
    },
    {
        "title": "Towards Informative Open-ended Text Generation with Dynamic Knowledge Triples",
        "authors": [
            "Zixuan Ren",
            "Yang Zhao",
            "Chengqing Zong"
        ],
        "published": "2023",
        "summary": "Pretrained language models (PLMs), especially large language models (LLMs) demonstrate impressive capabilities in open-ended text generation. While our statistical results show that LLMs often suffer from over-concentrated information, where the generated texts overly focus on the given prompt and fail to provide sufficient background and detailed information as humans do. To address this issue, we propose a dynamic knowledge-guided informative open-ended text generation approach, that utilizes a knowledge graph to help the model generate more contextually related entities and detailed facts. Specifically, we first employ a local knowledge filter to extract relevant knowledge from the comprehensive knowledge graph for a given topic sentence. Then we introduce a dynamic knowledge selector to predict the entity to be mentioned in the subsequent sentence. Finally, we utilize a knowledge-enhanced text generator to produce a more informative output. To evaluate the effectiveness of our approach, we evaluate the proposed approach in two scenarios: fine-tuning for small PLMs and prompt tuning for LLMs. Experimental results show that our approach could generate more informative texts than baselines.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.210.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"while our statistical results show that LLMs often suffer from over-concentrated information, where the generated texts overly focus on the given prompt and fail to provide sufficient background and detailed information as humans do.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"while our statistical results show that LLMs often suffer from over-concentrated information, where the generated texts overly focus on the given prompt and fail to provide sufficient background and detailed information as humans do.\""
    },
    {
        "title": "Ask Language Model to Clean Your Noisy Translation Data",
        "authors": [
            "Quinten Bolding",
            "Baohao Liao",
            "Brandon Denis",
            "Jun Luo",
            "Christof Monz"
        ],
        "published": "2023",
        "summary": "TTransformer models have demonstrated remarkable performance in neural machine translation (NMT). However, their vulnerability to noisy input poses a significant challenge in practical implementation, where generating clean output from noisy input is crucial. The MTNT dataset is widely used as a benchmark for evaluating the robustness of NMT models against noisy input. Nevertheless, its utility is limited due to the presence of noise in both the source and target sentences. To address this limitation, we focus on cleaning the noise from the target sentences in MTNT, making it more suitable as a benchmark for noise evaluation. Leveraging the capabilities of large language models (LLMs), we observe their impressive abilities in noise removal. For example, they can remove emojis while considering their semantic meaning. Additionally, we show that LLM can effectively rephrase slang, jargon, and profanities. The resulting datasets, called C-MTNT, exhibit significantly less noise in the target sentences while preserving the semantic integrity of the original sentences. Our human and GPT-4 evaluations also lead to a consistent conclusion that LLM performs well on this task. Lastly, experiments on C-MTNT showcased its effectiveness in evaluating the robustness of NMT models, highlighting the potential of advanced language models for data cleaning and emphasizing C-MTNT as a valuable resource.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.212.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their vulnerability to noisy input poses a significant challenge in practical implementation, where generating clean output from noisy input is crucial.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, their vulnerability to noisy input poses a significant challenge in practical implementation, where generating clean output from noisy input is crucial.\""
    },
    {
        "title": "Multi-User MultiWOZ: Task-Oriented Dialogues among Multiple Users",
        "authors": [
            "Yohan Jo",
            "Xinyan Zhao",
            "Arijit Biswas",
            "Nikoletta Basiou",
            "Vincent Auvray",
            "Nikolaos Malandrakis",
            "Angeliki Metallinou",
            "Alexandros Potamianos"
        ],
        "published": "2023",
        "summary": "While most task-oriented dialogues assume conversations between the agent and one user at a time, dialogue systems are increasingly expected to communicate with multiple users simultaneously who make decisions collaboratively. To facilitate development of such systems, we release the Multi-User MultiWOZ dataset: task-oriented dialogues among two users and one agent. To collect this dataset, each user utterance from MultiWOZ 2.2 was replaced with a small chat between two users that is semantically and pragmatically consistent with the original user utterance, thus resulting in the same dialogue state and system response. These dialogues reflect interesting dynamics of collaborative decision-making in task-oriented scenarios, e.g., social chatter and deliberation. Supported by this data, we propose the novel task of multi-user contextual query rewriting: to rewrite a task-oriented chat between two users as a concise task-oriented query that retains only task-relevant information and that is directly consumable by the dialogue system. We demonstrate that in multi-user dialogues, using predicted rewrites substantially improves dialogue state tracking without modifying existing dialogue systems that are trained for single-user dialogues. Further, this method surpasses training a medium-sized model directly on multi-user dialogues and generalizes to unseen domains.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.213.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs or language models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs or language models."
    },
    {
        "title": "Extractive Summarization via ChatGPT for Faithful Summary Generation",
        "authors": [
            "Haopeng Zhang",
            "Xiao Liu",
            "Jiawei Zhang"
        ],
        "published": "2023",
        "summary": "Extractive summarization is a crucial task in natural language processing that aims to condense long documents into shorter versions by directly extracting sentences. The recent introduction of large language models has attracted significant interest in the NLP community due to its remarkable performance on a wide range of downstream tasks. This paper first presents a thorough evaluation of ChatGPT’s performance on extractive summarization and compares it with traditional fine-tuning methods on various benchmark datasets. Our experimental analysis reveals that ChatGPT exhibits inferior extractive summarization performance in terms of ROUGE scores compared to existing supervised systems, while achieving higher performance based on LLM-based evaluation metrics. In addition, we explore the effectiveness of in-context learning and chain-of-thought reasoning for enhancing its performance. Furthermore, we find that applying an extract-then-generate pipeline with ChatGPT yields significant performance improvements over abstractive baselines in terms of summary faithfulness. These observations highlight potential directions for enhancing ChatGPT’s capabilities in faithful summarization using two-stage approaches.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.214.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our experimental analysis reveals that ChatGPT exhibits inferior extractive summarization performance in terms of ROUGE scores compared to existing supervised systems, while achieving higher performance based on LLM-based evaluation metrics.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Our experimental analysis reveals that ChatGPT exhibits inferior extractive summarization performance in terms of ROUGE scores compared to existing supervised systems, while achieving higher performance based on LLM-based evaluation metrics.\""
    },
    {
        "title": "MAPO: Boosting Large Language Model Performance with Model-Adaptive Prompt Optimization",
        "authors": [
            "Yuyan Chen",
            "Zhihao Wen",
            "Ge Fan",
            "Zhengyu Chen",
            "Wei Wu",
            "Dayiheng Liu",
            "Zhixu Li",
            "Bang Liu",
            "Yanghua Xiao"
        ],
        "published": "2023",
        "summary": "Prompt engineering, as an efficient and effective way to leverage Large Language Models (LLM), has drawn a lot of attention from the research community. The existing research primarily emphasizes the importance of adapting prompts to specific tasks, rather than specific LLMs. However, a good prompt is not solely defined by its wording, but also binds to the nature of the LLM in question. In this work, we first quantitatively demonstrate that different prompts should be adapted to different LLMs to enhance their capabilities across various downstream tasks in NLP. Then we novelly propose a model-adaptive prompt optimizer (MAPO) method that optimizes the original prompts for each specific LLM in downstream tasks. Extensive experiments indicate that the proposed method can effectively refine prompts for an LLM, leading to significant improvements over various downstream tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.215.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, a good prompt is not solely defined by its wording, but also binds to the nature of the LLM in question.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs, specifically that they require prompts to be adapted to their specific nature, but this limitation is not explored in depth and is primarily used to justify the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, a good prompt is not solely defined by its wording, but also binds to the nature of the LLM in question.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs, specifically that they require prompts to be adapted to their specific nature, but this limitation is not explored in depth and is primarily used to justify the proposed solution."
    },
    {
        "title": "PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection",
        "authors": [
            "Tao Yang",
            "Tianyuan Shi",
            "Fanqi Wan",
            "Xiaojun Quan",
            "Qifan Wang",
            "Bingzhe Wu",
            "Jiaxiang Wu"
        ],
        "published": "2023",
        "summary": "Recent advances in large language models (LLMs), such as ChatGPT, have showcased remarkable zero-shot performance across various NLP tasks. However, the potential of LLMs in personality detection, which involves identifying an individual’s personality from their written texts, remains largely unexplored. Drawing inspiration from Psychological Questionnaires, which are carefully designed by psychologists to evaluate individual personality traits through a series of targeted items, we argue that these items can be regarded as a collection of well-structured chain-of-thought (CoT) processes. By incorporating these processes, LLMs can enhance their capabilities to make more reasonable inferences on personality from textual input. In light of this, we propose a novel personality detection method, called PsyCoT, which mimics the way individuals complete psychological questionnaires in a multi-turn dialogue manner. In particular, we employ a LLM as an AI assistant with a specialization in text analysis. We prompt the assistant to rate individual items at each turn and leverage the historical rating results to derive a conclusive personality preference. Our experiments demonstrate that PsyCoT significantly improves the performance and robustness of GPT-3.5 in personality detection, achieving an average F1 score improvement of 4.23/10.63 points on two benchmark datasets compared to the standard prompting method. Our code is available at https://github.com/TaoYang225/PsyCoT.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.216.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations mentioned, but the abstract implies that LLMs' potential in personality detection remains \"largely unexplored\", which suggests that there may be limitations or challenges in this area.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations mentioned, but the abstract implies that LLMs' potential in personality detection remains \"largely unexplored\", which suggests that there may be limitations or challenges in this area."
    },
    {
        "title": "Harnessing the power of LLMs: Evaluating human-AI text co-creation through the lens of news headline generation",
        "authors": [
            "Zijian Ding",
            "Alison Smith-Renner",
            "Wenjuan Zhang",
            "Joel Tetreault",
            "Alejandro Jaimes"
        ],
        "published": "2023",
        "summary": "To explore how humans can best leverage LLMs for writing and how interacting with these models affects feelings of ownership and trust in the writing process, we compared common human-AI interaction types (e.g., guiding system, selecting from system outputs, post-editing outputs) in the context of LLM-assisted news headline generation. While LLMs alone can generate satisfactory news headlines, on average, human control is needed to fix undesirable model outputs. Of the interaction methods, guiding and selecting model output added the most benefit with the lowest cost (in time and effort). Further, AI assistance did not harm participants’ perception of control compared to freeform editing.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.217.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While LLMs alone can generate satisfactory news headlines, on average, human control is needed to fix undesirable model outputs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While LLMs alone can generate satisfactory news headlines, on average, human control is needed to fix undesirable model outputs.\""
    },
    {
        "title": "NERetrieve: Dataset for Next Generation Named Entity Recognition and Retrieval",
        "authors": [
            "Uri Katz",
            "Matan Vetzler",
            "Amir Cohen",
            "Yoav Goldberg"
        ],
        "published": "2023",
        "summary": "Recognizing entities in texts is a central need in many information-seeking scenarios, and indeed, Named Entity Recognition (NER) is arguably one of the most successful examples of a widely adopted NLP task and corresponding NLP technology. Recent advances in large language models (LLMs) appear to provide effective solutions (also) for NER tasks that were traditionally handled with dedicated models, often matching or surpassing the abilities of the dedicated models. Should NER be considered a solved problem? We argue to the contrary: the capabilities provided by LLMs are not the end of NER research, but rather an exciting beginning. They allow taking NER to the next level, tackling increasingly more useful, and increasingly more challenging, variants. We present three variants of the NER task, together with a dataset to support them. The first is a move towards more fine-grained—and intersectional—entity types. The second is a move towards zero-shot recognition and extraction of these fine-grained types based on entity-type labels. The third, and most challenging, is the move from the recognition setup to a novel retrieval setup, where the query is a zero-shot entity type, and the expected result is all the sentences from a large, pre-indexed corpus that contain entities of these types, and their corresponding spans. We show that all of these are far from being solved. We provide a large, silver-annotated corpus of 4 million paragraphs covering 500 entity types, to facilitate research towards all of these three goals.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.218.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We argue to the contrary: the capabilities provided by LLMs are not the end of NER research, but rather an exciting beginning... We show that all of these are far from being solved.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We argue to the contrary: the capabilities provided by LLMs are not the end of NER research, but rather an exciting beginning... We show that all of these are far from being solved.\""
    },
    {
        "title": "SWEET - Weakly Supervised Person Name Extraction for Fighting Human Trafficking",
        "authors": [
            "Javin Liu",
            "Hao Yu",
            "Vidya Sujaya",
            "Pratheeksha Nair",
            "Kellin Pelrine",
            "Reihaneh Rabbany"
        ],
        "published": "2023",
        "summary": "In this work, we propose a weak supervision pipeline SWEET: Supervise Weakly for Entity Extraction to fight Trafficking for extracting person names from noisy escort advertisements. Our method combines the simplicity of rule-matching (through antirules, i.e., negated rules) and the generalizability of large language models fine-tuned on benchmark, domain-specific and synthetic datasets, treating them as weak labels. One of the major challenges in this domain is limited labeled data. SWEET addresses this by obtaining multiple weak labels through labeling functions and effectively aggregating them. SWEET outperforms the previous supervised SOTA method for this task by 9% F1 score on domain data and better generalizes to common benchmark datasets. Furthermore, we also release HTGEN, a synthetically generated dataset of escort advertisements (built using ChatGPT) to facilitate further research within the community.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.219.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"One of the major challenges in this domain is limited labeled data.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"One of the major challenges in this domain is limited labeled data.\""
    },
    {
        "title": "Watermarking LLMs with Weight Quantization",
        "authors": [
            "Linyang Li",
            "Botian Jiang",
            "Pengyu Wang",
            "Ke Ren",
            "Hang Yan",
            "Xipeng Qiu"
        ],
        "published": "2023",
        "summary": "Abuse of large language models reveals high risks as large language models are being deployed at an astonishing speed. It is important to protect the model weights to avoid malicious usage that violates licenses of open-source large language models. This paper proposes a novel watermarking strategy that plants watermarks in the quantization process of large language models without pre-defined triggers during inference. The watermark works when the model is used in the fp32 mode and remains hidden when the model is quantized to int8, in this way, the users can only inference the model without further supervised fine-tuning of the model. We successfully plant the watermark into open-source large language model weights including GPT-Neo and LLaMA. We hope our proposed method can provide a potential direction for protecting model weights in the era of large language model applications.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.220.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Abuse of large language models reveals high risks as large language models are being deployed at an astonishing speed. It is important to protect the model weights to avoid malicious usage that violates licenses of open-source large language models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Abuse of large language models reveals high risks as large language models are being deployed at an astonishing speed. It is important to protect the model weights to avoid malicious usage that violates licenses of open-source large language models.\""
    },
    {
        "title": "Disentangling Extraction and Reasoning in Multi-hop Spatial Reasoning",
        "authors": [
            "Roshanak Mirzaee",
            "Parisa Kordjamshidi"
        ],
        "published": "2023",
        "summary": "Spatial reasoning over text is challenging as the models not only need to extract the direct spatial information from the text but also reason over those and infer implicit spatial relations. Recent studies highlight the struggles even large language models encounter when it comes to performing spatial reasoning over text. In this paper, we explore the potential benefits of disentangling the processes of information extraction and reasoning in models to address this challenge. To explore this, we design various models that disentangle extraction and reasoning(either symbolic or neural) and compare them with state-of-the-art(SOTA) baselines with no explicit design for these parts. Our experimental results consistently demonstrate the efficacy of disentangling, showcasing its ability to enhance models’ generalizability within realistic data domains.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.221.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent studies highlight the struggles even large language models encounter when it comes to performing spatial reasoning over text.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Recent studies highlight the struggles even large language models encounter when it comes to performing spatial reasoning over text.\""
    },
    {
        "title": "RoAST: Robustifying Language Models via Adversarial Perturbation with Selective Training",
        "authors": [
            "Jaehyung Kim",
            "Yuning Mao",
            "Rui Hou",
            "Hanchao Yu",
            "Davis Liang",
            "Pascale Fung",
            "Qifan Wang",
            "Fuli Feng",
            "Lifu Huang",
            "Madian Khabsa"
        ],
        "published": "2023",
        "summary": "Fine-tuning pre-trained language models (LMs) has become the de facto standard in many NLP tasks. Nevertheless, fine-tuned LMs are still prone to robustness issues, such as adversarial robustness and model calibration. Several perspectives of robustness for LMs have been studied independently, but lacking a unified consideration in multiple perspectives. In this paper, we propose Robustifying LMs via Adversarial perturbation with Selective Training (RoAST), a simple yet effective fine-tuning technique to enhance the multi-perspective robustness of LMs in a unified way. RoAST effectively incorporates two important sources for the model robustness, robustness on the perturbed inputs and generalizable knowledge in pre-trained LMs. To be specific, RoAST introduces adversarial perturbation during fine-tuning while the model parameters are selectively updated upon their relative importance to minimize unnecessary deviation. Under a unified evaluation of fine-tuned LMs by incorporating four representative perspectives of model robustness, we demonstrate the effectiveness of RoAST compared to state-of-the-art fine-tuning methods on six different types of LMs, which indicates its usefulness in practice.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.223.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"fine-tuned LMs are still prone to robustness issues, such as adversarial robustness and model calibration.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"fine-tuned LMs are still prone to robustness issues, such as adversarial robustness and model calibration.\""
    },
    {
        "title": "Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models",
        "authors": [
            "Gabriel Sarch",
            "Yue Wu",
            "Michael Tarr",
            "Katerina Fragkiadaki"
        ],
        "published": "2023",
        "summary": "Pre-trained and frozen LLMs can effectively map simple scene re-arrangement instructions to programs over a robot’s visuomotor functions through appropriate few-shot example prompting. To parse open-domain natural language and adapt to a user’s idiosyncratic procedures, not known during prompt engineering time, fixed prompts fall short. In this paper, we introduce HELPER, an embodied agent equipped with an external memory of language-program pairs that parses free-form human-robot dialogue into action programs through retrieval-augmented LLM prompting: relevant memories are retrieved based on the current dialogue, instruction, correction or VLM description, and used as in-context prompt examples for LLM querying. The memory is expanded during deployment to include pairs of user’s language and action plans, to assist future inferences and personalize them to the user’s language and routines. HELPER sets a new state-of-the-art in the TEACh benchmark in both Execution from Dialog History (EDH) and Trajectory from Dialogue (TfD), with 1.7x improvement over the previous SOTA for TfD. Our models, code and video results can be found in our project’s website: https://helper-agent-llm.github.io.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.226.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"To parse open-domain natural language and adapt to a user’s idiosyncratic procedures, not known during prompt engineering time, fixed prompts fall short.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"To parse open-domain natural language and adapt to a user’s idiosyncratic procedures, not known during prompt engineering time, fixed prompts fall short.\""
    },
    {
        "title": "ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought",
        "authors": [
            "Hanchong Zhang",
            "Ruisheng Cao",
            "Lu Chen",
            "Hongshen Xu",
            "Kai Yu"
        ],
        "published": "2023",
        "summary": "Recently Large Language Models (LLMs) have been proven to have strong abilities in various domains and tasks. We study the problem of prompt designing in the text-to-SQL task and attempt to improve the LLMs’ reasoning ability when generating SQL queries. Besides the trivial few-shot in-context learning setting, we design our chain-of-thought (CoT) prompt with a similar method to schema linking. We provide a method named ACT-SQL to automatically generate auto-CoT exemplars and thus the whole process doesn’t need manual labeling. Our approach is cost-saving since we only use the LLMs’ API call once when generating one SQL query. Furthermore, we extend our in-context learning method to the multi-turn text-to-SQL task. The experiment results show that the LLMs’ performance can benefit from our ACT-SQL approach. Our approach achieves SOTA performance on the Spider dev set among existing in-context learning approaches.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.227.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract discusses the capabilities and improvements of LLMs but does not mention any limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract discusses the capabilities and improvements of LLMs but does not mention any limitations."
    },
    {
        "title": "ASPIRO: Any-shot Structured Parsing-error-Induced ReprOmpting for Consistent Data-to-Text Generation",
        "authors": [
            "Martin Vejvar",
            "Yasutaka Fujimoto"
        ],
        "published": "2023",
        "summary": "We present ASPIRO, an approach for structured data verbalisation into short template sentences in zero to few-shot settings. Unlike previous methods, our approach prompts Large Language Models (LLMs) to directly produce entity-agnostic templates, rather than relying on LLMs to faithfully copy the given example entities, or validating/crafting the templates manually. We incorporate LLM re-prompting, triggered by algorithmic parsing checks, as well as the PARENT metric induced consistency validation to identify and rectify template generation problems in real-time. ASPIRO, compared to direct LLM output, averages 66% parsing error rate reduction in generated verbalisations of RDF triples on the DART dataset. Our best 5-shot text-davinci-003 setup, scoring BLEU of 50.62, METEOR of 45.16, BLEURT of 0.82, NUBIA of 0.87, and PARENT of 0.8962 on the Rel2Text dataset, competes effectively with recent fine-tuned pretrained language models.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.229.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unlike previous methods, our approach prompts Large Language Models (LLMs) to directly produce entity-agnostic templates, rather than relying on LLMs to faithfully copy the given example entities, or validating/crafting the templates manually.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Unlike previous methods, our approach prompts Large Language Models (LLMs) to directly produce entity-agnostic templates, rather than relying on LLMs to faithfully copy the given example entities, or validating/crafting the templates manually.\""
    },
    {
        "title": "Detecting Syntactic Change with Pre-trained Transformer Models",
        "authors": [
            "Liwen Hou",
            "David Smith"
        ],
        "published": "2023",
        "summary": "We investigate the ability of Transformer-based language models to find syntactic differences between the English of the early 1800s and that of the late 1900s. First, we show that a fine-tuned BERT model can distinguish between text from these two periods using syntactic information only; to show this, we employ a strategy to hide semantic information from the text. Second, we make further use of fine-tuned BERT models to identify specific instances of syntactic change and specific words for which a new part of speech was introduced. To do this, we employ an automatic part-of-speech (POS) tagger and use it to train corpora-specific taggers based only on BERT representations pretrained on different corpora. Notably, our methods of identifying specific candidates for syntactic change avoid using any automatic POS tagger on old text, where its performance may be unreliable; instead, our methods only use untagged old text together with tagged modern text. We examine samples and distributional properties of the model output to validate automatically identified cases of syntactic change. Finally, we use our techniques to confirm the historical rise of the progressive construction, a known example of syntactic change.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.230.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract does not mention any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract does not mention any limitations of LLMs."
    },
    {
        "title": "Gold: A Global and Local-aware Denoising Framework for Commonsense Knowledge Graph Noise Detection",
        "authors": [
            "Zheye Deng",
            "Weiqi Wang",
            "Zhaowei Wang",
            "Xin Liu",
            "Yangqiu Song"
        ],
        "published": "2023",
        "summary": "Commonsense Knowledge Graphs (CSKGs) are crucial for commonsense reasoning, yet constructing them through human annotations can be costly. As a result, various automatic methods have been proposed to construct CSKG with larger semantic coverage. However, these unsupervised approaches introduce spurious noise that can lower the quality of the resulting CSKG, which cannot be tackled easily by existing denoising algorithms due to the unique characteristics of nodes and structures in CSKGs. To address this issue, we propose Gold (Global and Local-aware Denoising), a denoising framework for CSKGs that incorporates entity semantic information, global rules, and local structural information from the CSKG. Experiment results demonstrate that Gold outperforms all baseline methods in noise detection tasks on synthetic noisy CSKG benchmarks. Furthermore, we show that denoising a real-world CSKG is effective and even benefits the downstream zero-shot commonsense question-answering task. Our code and data are publicly available at https://github.com/HKUST-KnowComp/GOLD.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.232.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Improving Conversational Recommendation Systems via Bias Analysis and Language-Model-Enhanced Data Augmentation",
        "authors": [
            "Xi Wang",
            "Hossein Rahmani",
            "Jiqun Liu",
            "Emine Yilmaz"
        ],
        "published": "2023",
        "summary": "Conversational Recommendation System (CRS) is a rapidly growing research area that has gained significant attention alongside advancements in language modelling techniques. However, the current state of conversational recommendation faces numerous challenges due to its relative novelty and limited existing contributions. In this study, we delve into benchmark datasets for developing CRS models and address potential biases arising from the feedback loop inherent in multi-turn interactions, including selection bias and multiple popularity bias variants. Drawing inspiration from the success of generative data via using language models and data augmentation techniques, we present two novel strategies, ‘Once-Aug’ and ‘PopNudge’, to enhance model performance while mitigating biases. Through extensive experiments on ReDial and TG-ReDial benchmark datasets, we show a consistent improvement of CRS techniques with our data augmentation approaches and offer additional insights on addressing multiple newly formulated biases.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.233.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the current state of conversational recommendation faces numerous challenges due to its relative novelty and limited existing contributions.\"\n\n(Note: Although the abstract mentions challenges, it does not explicitly discuss limitations of LLMs, but rather the challenges of the conversational recommendation system. The mention of LLMs is brief and does not imply any limitations.)",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, the current state of conversational recommendation faces numerous challenges due to its relative novelty and limited existing contributions.\"\n\n(Note: Although the abstract mentions challenges, it does not explicitly discuss limitations of LLMs, but rather the challenges of the conversational recommendation system. The mention of LLMs is brief and does not imply any limitations.)"
    },
    {
        "title": "Test-time Augmentation for Factual Probing",
        "authors": [
            "Go Kamoda",
            "Benjamin Heinzerling",
            "Keisuke Sakaguchi",
            "Kentaro Inui"
        ],
        "published": "2023",
        "summary": "Factual probing is a method that uses prompts to test if a language model “knows” certain world knowledge facts. A problem in factual probing is that small changes to the prompt can lead to large changes in model output. Previous work aimed to alleviate this problem by optimizing prompts via text mining or fine-tuning. However, such approaches are relation-specific and do not generalize to unseen relation types. Here, we propose to use test-time augmentation (TTA) as a relation-agnostic method for reducing sensitivity to prompt variations by automatically augmenting and ensembling prompts at test time. Experiments show improved model calibration, i.e., with TTA, model confidence better reflects prediction accuracy. Improvements in prediction accuracy are observed for some models, but for other models, TTA leads to degradation. Error analysis identifies the difficulty of producing high-quality prompt variations as the main challenge for TTA.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.236.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A problem in factual probing is that small changes to the prompt can lead to large changes in model output\"; \"Error analysis identifies the difficulty of producing high-quality prompt variations as the main challenge for TTA.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"A problem in factual probing is that small changes to the prompt can lead to large changes in model output\"; \"Error analysis identifies the difficulty of producing high-quality prompt variations as the main challenge for TTA.\""
    },
    {
        "title": "Methodological Insights in Detecting Subtle Semantic Shifts with Contextualized and Static Language Models",
        "authors": [
            "Sanne Hoeken",
            "Özge Alacam",
            "Antske Fokkens",
            "Pia Sommerauer"
        ],
        "published": "2023",
        "summary": "In this paper, we investigate automatic detection of subtle semantic shifts between social communities of different political convictions in Dutch and English. We perform a methodological study comparing methods using static and contextualized language models. We investigate the impact of specializing contextualized models through fine-tuning on target corpora, word sense disambiguation and sentiment. We furthermore propose a new approach using masked token prediction, that relies on behavioral information, specifically the most probable substitutions, instead of geometrical comparison of representations. Our results show that methods using static models and our masked token prediction method can detect differences in connotation of politically loaded terms, whereas methods that rely on measuring the distance between contextualized representations are not providing clear signals, even in synthetic scenarios of extreme shifts.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.237.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"methods that rely on measuring the distance between contextualized representations are not providing clear signals, even in synthetic scenarios of extreme shifts.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"methods that rely on measuring the distance between contextualized representations are not providing clear signals, even in synthetic scenarios of extreme shifts.\""
    },
    {
        "title": "Disfluent Cues for Enhanced Speech Understanding in Large Language Models",
        "authors": [
            "Morteza Rohanian",
            "Farhad Nooralahzadeh",
            "Omid Rohanian",
            "David Clifton",
            "Michael Krauthammer"
        ],
        "published": "2023",
        "summary": "In computational linguistics, the common practice is to “clean” disfluent content from spontaneous speech. However, we hypothesize that these disfluencies might serve as more than mere noise, potentially acting as informative cues. We use a range of pre-trained models for a reading comprehension task involving disfluent queries, specifically featuring different types of speech repairs. The findings indicate that certain disfluencies can indeed improve model performance, particularly those stemming from context-based adjustments. However, large-scale language models struggle to handle repairs involving decision-making or the correction of lexical or syntactic errors, suggesting a crucial area for potential improvement. This paper thus highlights the importance of a nuanced approach to disfluencies, advocating for their potential utility in enhancing model performance rather than their removal.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.238.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, large-scale language models struggle to handle repairs involving decision-making or the correction of lexical or syntactic errors, suggesting a crucial area for potential improvement.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, large-scale language models struggle to handle repairs involving decision-making or the correction of lexical or syntactic errors, suggesting a crucial area for potential improvement.\""
    },
    {
        "title": "Watermarking PLMs on Classification Tasks by Combining Contrastive Learning with Weight Perturbation",
        "authors": [
            "Chenxi Gu",
            "Xiaoqing Zheng",
            "Jianhan Xu",
            "Muling Wu",
            "Cenyuan Zhang",
            "Chengsong Huang",
            "Hua Cai",
            "Xuanjing Huang"
        ],
        "published": "2023",
        "summary": "Large pre-trained language models (PLMs) have achieved remarkable success, making them highly valuable intellectual property due to their expensive training costs. Consequently, model watermarking, a method developed to protect the intellectual property of neural models, has emerged as a crucial yet underexplored technique. The problem of watermarking PLMs has remained unsolved since the parameters of PLMs will be updated when fine-tuned on downstream datasets, and then embedded watermarks could be removed easily due to the catastrophic forgetting phenomenon. This study investigates the feasibility of watermarking PLMs by embedding backdoors that can be triggered by specific inputs. We employ contrastive learning during the watermarking phase, allowing the representations of specific inputs to be isolated from others and mapped to a particular label after fine-tuning. Moreover, we demonstrate that by combining weight perturbation with the proposed method, watermarks can be embedded in a flatter region of the loss landscape, thereby increasing their robustness to watermark removal. Extensive experiments on multiple datasets demonstrate that the embedded watermarks can be robustly extracted without any knowledge about downstream tasks, and with a high success rate.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.239.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the parameters of PLMs will be updated when fine-tuned on downstream datasets, and then embedded watermarks could be removed easily due to the catastrophic forgetting phenomenon.\"\n\nThis rating is chosen because the abstract mentions a limitation of PLMs (catastrophic forgetting phenomenon) but only briefly and as a minor detail to justify the paper's goals and methods. The primary focus is on the",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the parameters of PLMs will be updated when fine-tuned on downstream datasets, and then embedded watermarks could be removed easily due to the catastrophic forgetting phenomenon.\"\n\nThis rating is chosen because the abstract mentions a limitation of PLMs (catastrophic forgetting phenomenon) but only briefly and as a minor detail to justify the paper's goals and methods. The primary focus is on the"
    },
    {
        "title": "Exploring the Sensitivity of LLMs’ Decision-Making Capabilities: Insights from Prompt Variations and Hyperparameters",
        "authors": [
            "Manikanta Loya",
            "Divya Sinha",
            "Richard Futrell"
        ],
        "published": "2023",
        "summary": "The advancement of Large Language Models (LLMs) has led to their widespread use across a broad spectrum of tasks, including decision-making. Prior studies have compared the decision-making abilities of LLMs with those of humans from a psychological perspective. However, these studies have not always properly accounted for the sensitivity of LLMs’ behavior to hyperparameters and variations in the prompt. In this study, we examine LLMs’ performance on the Horizon decision-making task studied by Binz and Schulz (2023), analyzing how LLMs respond to variations in prompts and hyperparameters. By experimenting on three OpenAI language models possessing different capabilities, we observe that the decision-making abilities fluctuate based on the input prompts and temperature settings. Contrary to previous findings, language models display a human-like exploration–exploitation tradeoff after simple adjustments to the prompt.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.241.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these studies have not always properly accounted for the sensitivity of LLMs’ behavior to hyperparameters and variations in the prompt.\"; \"By experimenting on three OpenAI language models possessing different capabilities, we observe that the decision-making abilities fluctuate based on the input prompts and temperature settings.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, these studies have not always properly accounted for the sensitivity of LLMs’ behavior to hyperparameters and variations in the prompt.\"; \"By experimenting on three OpenAI language models possessing different capabilities, we observe that the decision-making abilities fluctuate based on the input prompts and temperature settings.\""
    },
    {
        "title": "Search Augmented Instruction Learning",
        "authors": [
            "Hongyin Luo",
            "Tianhua Zhang",
            "Yung-Sung Chuang",
            "Yuan Gong",
            "Yoon Kim",
            "Xixin Wu",
            "Helen Meng",
            "James Glass"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information. In this work, we propose search-augmented instruction learning (SAIL), which grounds the language generation and instruction following abilities on complex search results generated by in-house and external search engines. With an instruction tuning corpus, we collect search results for each training case from different search APIs and domains, and construct a new search-grounded training set containing (instruction, grounding information, response) triplets. We then fine-tune the LLaMA-7B model on the constructed training set. Since the collected results contain unrelated and disputing languages, the model needs to learn to ground on trustworthy search results, filter out distracting passages, and generate the target response. The search result-denoising process entails explicit trustworthy information selection and multi-hop reasoning, since the retrieved passages might be informative but not contain the instruction-following answer. Experiments show that the fine-tuned SAIL-7B model has a strong instruction-following ability, and it performs significantly better on transparency-sensitive tasks, including open-ended question answering and fact checking.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.242.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information.\""
    },
    {
        "title": "“Kelly is a Warm Person, Joseph is a Role Model”: Gender Biases in LLM-Generated Reference Letters",
        "authors": [
            "Yixin Wan",
            "George Pu",
            "Jiao Sun",
            "Aparna Garimella",
            "Kai-Wei Chang",
            "Nanyun Peng"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have recently emerged as an effective tool to assist individuals in writing various types of content, including professional documents such as recommendation letters. Though bringing convenience, this application also introduces unprecedented fairness concerns. Model-generated reference letters might be directly used by users in professional scenarios. If underlying biases exist in these model-constructed letters, using them without scrutinization could lead to direct societal harms, such as sabotaging application success rates for female applicants. In light of this pressing issue, it is imminent and necessary to comprehensively study fairness issues and associated harms in this real-world use case. In this paper, we critically examine gender biases in LLM-generated reference letters. Drawing inspiration from social science findings, we design evaluation methods to manifest biases through 2 dimensions: (1) biases in language style and (2) biases in lexical content. We further investigate the extent of bias propagation by analyzing the hallucination bias of models, a term that we define to be bias exacerbation in model-hallucinated contents. Through benchmarking evaluation on 2 popular LLMs- ChatGPT and Alpaca, we reveal significant gender biases in LLM-generated recommendation letters. Our findings not only warn against using LLMs for this application without scrutinization, but also illuminate the importance of thoroughly studying hidden biases and harms in LLM-generated professional documents.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.243.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Though bringing convenience, this application also introduces unprecedented fairness concerns. Model-generated reference letters might be directly used by users in professional scenarios. If underlying biases exist in these model-constructed letters, using them without scrutinization could lead to direct societal harms, such as sabotaging application success rates for female applicants.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Though bringing convenience, this application also introduces unprecedented fairness concerns. Model-generated reference letters might be directly used by users in professional scenarios. If underlying biases exist in these model-constructed letters, using them without scrutinization could lead to direct societal harms, such as sabotaging application success rates for female applicants.\""
    },
    {
        "title": "TextMixer: Mixing Multiple Inputs for Privacy-Preserving Inference",
        "authors": [
            "Xin Zhou",
            "Yi Lu",
            "Ruotian Ma",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2023",
        "summary": "Pre-trained language models (PLMs) are often deployed as cloud services, enabling users to upload textual data and perform inference remotely. However, users’ personal text often contains sensitive information, and sharing such data directly with the service providers can lead to serious privacy leakage. To address this problem, we introduce a novel privacy-preserving inference framework called MixPi , which prevents plaintext leakage during the inference phase. Inspired by k-anonymity, MixPi aims to obfuscate a user’s private input by mixing it with multiple other inputs, thereby confounding potential privacy attackers. To achieve this, our approach involves: (1) proposing a novel encryption module, Privacy Mixer, which encrypts input from three distinct dimensions: mixing, representation, and position. (2) adopting a pre-trained Multi-input Multi-output network to handle mixed representations and obtain multiple predictions. (3) employing a Privacy Demixer to ensure only the user can decrypt the real output among the multiple predictions. Furthermore, we explore different ways to automatically generate synthetic inputs required for mixing. Experimental results on token and sentence classification tasks demonstrate that MixPi greatly surpasses existing privacy-preserving methods in both performance and privacy.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.244.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"users’ personal text often contains sensitive information, and sharing such data directly with the service providers can lead to serious privacy leakage.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"users’ personal text often contains sensitive information, and sharing such data directly with the service providers can lead to serious privacy leakage.\""
    },
    {
        "title": "FinePrompt: Unveiling the Role of Finetuned Inductive Bias on Compositional Reasoning in GPT-4",
        "authors": [
            "Jeonghwan Kim",
            "Giwon Hong",
            "Sung-Hyon Myaeng",
            "Joyce Whang"
        ],
        "published": "2023",
        "summary": "Compositional reasoning across texts has been a long-standing challenge in natural language processing. With large language models like GPT-4 taking over the field, prompting techniques such as chain-of-thought (CoT) were proposed to unlock compositional, multi-step reasoning capabilities of LLMs. Despite their success, the prompts demand significant human effort to discover and validate them. Our work draws attention to the idea of transferring task-specific inductive biases from finetuned models to prompts, as a way of improving GPT-4’s compositional reasoning capabilities. To leverage these inductive biases, we formulate prompt templates to ease the transfer of inductive biases. The experimental results on multi-hop question answering and numerical reasoning over text show that our proposed prompt scheme shows competitive zero-shot and few-shot performances compared to existing prompts on complicated reasoning tasks, highlighting the importance of adopting the validated biases of the previous paradigm.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.245.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite their success, the prompts demand significant human effort to discover and validate them.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite their success, the prompts demand significant human effort to discover and validate them.\""
    },
    {
        "title": "Allies: Prompting Large Language Model with Beam Search",
        "authors": [
            "Hao Sun",
            "Xiao Liu",
            "Yeyun Gong",
            "Yan Zhang",
            "Daxin Jiang",
            "Linjun Yang",
            "Nan Duan"
        ],
        "published": "2023",
        "summary": "With the advance of large language models (LLMs), the research field of LLM applications becomes more and more popular and the idea of constructing pipelines to accomplish complex tasks by stacking LLM API calls come true. However, this kind of methods face two limitations: narrow information coverage and low fault tolerance. In this work, we propose a novel method called ALLIES. Given an input query, ALLIES leverages LLMs to iteratively generate new queries related to the original query, enabling an iterative reasoning process. By iteratively refining and expanding the scope of the original query, ALLIES captures and utilizes hidden knowledge that may not be directly obtainable through retrieval. We take zero-shot open-domain question answering (ODQA) as an application scene and evaluate ALLIES on the widely-used benchmarks, such as NQ, WebQ and TriviaQA. The experimental results demonstrate that ALLIES significantly outperforms other zero-shot baselines, indicating its effectiveness in tackling those challenges. Our code is available in https://github.com/microsoft/SimXNS/tree/main/ALLIES.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.247.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, this kind of methods face two limitations: narrow information coverage and low fault tolerance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, this kind of methods face two limitations: narrow information coverage and low fault tolerance.\""
    },
    {
        "title": "Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning",
        "authors": [
            "Liangming Pan",
            "Alon Albalak",
            "Xinyi Wang",
            "William Wang"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic symbolic solver performs inference on the formulated problem. We also introduce a self-refinement module, which utilizes the symbolic solver’s error messages to revise symbolic formalizations. We demonstrate Logic-LM’s effectiveness on five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO, LogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant performance boost of 39.2% over using LLM alone with standard prompting and 18.4% over LLM with chain-of-thought prompting. Our findings suggest that Logic-LM, by combining LLMs with symbolic logic, offers a promising avenue for faithful logical reasoning.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.248.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems.\"\n\nThis abstract mentions a limitation of LLMs (struggling with complex logical problems) but does not explore it in depth, instead focusing on the proposed solution, Logic-LM.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems.\"\n\nThis abstract mentions a limitation of LLMs (struggling with complex logical problems) but does not explore it in depth, instead focusing on the proposed solution, Logic-LM."
    },
    {
        "title": "Answer-state Recurrent Relational Network (AsRRN) for Constructed Response Assessment and Feedback Grouping",
        "authors": [
            "Zhaohui Li",
            "Susan Lloyd",
            "Matthew Beckman",
            "Rebecca Passonneau"
        ],
        "published": "2023",
        "summary": "STEM educators must trade off the ease of assessing selected response (SR) questions, like multiple choice, with constructed response (CR) questions, where students articulate their own reasoning. Our work addresses a CR type new to NLP but common in college STEM, consisting of multiple questions per context. To relate the context, the questions, the reference responses, and students’ answers, we developed an Answer-state Recurrent Relational Network (AsRRN). In recurrent time-steps, relation vectors are learned for specific dependencies in a computational graph, where the nodes encode the distinct types of text input. AsRRN incorporates contrastive loss for better representation learning, which improves performance and supports student feedback. AsRRN was developed on a new dataset of 6,532 student responses to three, two-part CR questions. AsRRN outperforms classifiers based on LLMs, a previous relational network for CR questions, and few-shot learning with GPT-3.5. Ablation studies show the distinct contributions of AsRRN’s dependency structure, the number of time steps in the recurrence, and the contrastive loss.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.254.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"AsRRN outperforms classifiers based on LLMs\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"AsRRN outperforms classifiers based on LLMs\""
    },
    {
        "title": "Low-Resource Comparative Opinion Quintuple Extraction by Data Augmentation with Prompting",
        "authors": [
            "Qingting Xu",
            "Yu Hong",
            "Fubang Zhao",
            "Kaisong Song",
            "Yangyang Kang",
            "Jiaxiang Chen",
            "Guodong Zhou"
        ],
        "published": "2023",
        "summary": "Comparative Opinion Quintuple Extraction (COQE) aims to predict comparative opinion quintuples from comparative sentences. These quintuples include subject, object, shareable aspect, comparative opinion, and preference. The existing pipeline-based COQE method fails in error propagation. In addition, the complexity and insufficient amounts of annotated data hinder the performance of COQE models. In this paper, we introduce a novel approach called low-resource comparative opinion quintuple extraction by Data Augmentation with Prompting (DAP). Firstly, we present an end-to-end model architecture better suited to the data augmentation method from triplets to quintuples and can effectively avoid error propagation. Additionally, we introduce a data-centric augmentation approach that leverages the robust generative abilities of ChatGPT and integrates transfer learning techniques. Experimental results over three datasets (Camera, Car, Ele) demonstrate that our approach yields substantial improvements and achieves state-of-the-art results. The source code and data are publicly released at: https://github.com/qtxu-nlp/COQE-DAP.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.255.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The existing pipeline-based COQE method fails in error propagation. In addition, the complexity and insufficient amounts of annotated data hinder the performance of COQE models.\"\n\n(Note: Although LLMs are not explicitly mentioned, the mention of \"robust generative abilities of ChatGPT\" implies the use of a large language model, and the limitations mentioned are related to the task of comparative",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The existing pipeline-based COQE method fails in error propagation. In addition, the complexity and insufficient amounts of annotated data hinder the performance of COQE models.\"\n\n(Note: Although LLMs are not explicitly mentioned, the mention of \"robust generative abilities of ChatGPT\" implies the use of a large language model, and the limitations mentioned are related to the task of comparative"
    },
    {
        "title": "A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection",
        "authors": [
            "Shiping Yang",
            "Renliang Sun",
            "Xiaojun Wan"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have shown their ability to collaborate effectively with humans in real-world scenarios. However, LLMs are apt to generate hallucinations, i.e., makeup incorrect text and unverified information, which can cause significant damage when deployed for mission-critical tasks. In this paper, we propose a self-check approach based on reverse validation to detect factual errors automatically in a zero-resource fashion. To facilitate future studies and assess different methods, we construct a hallucination detection benchmark named PHD, which is generated by ChatGPT and annotated by human annotators. Contrasting previous studies of zero-resource hallucination detection, our method and benchmark concentrate on passage-level detection instead of sentence-level. We empirically evaluate our method and existing zero-resource detection methods on two datasets. The experimental results demonstrate that the proposed method considerably outperforms the baselines while costing fewer tokens and less time. Furthermore, we manually analyze some hallucination cases that LLM failed to capture, revealing the shared limitation of zero-resource methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.256.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, LLMs are apt to generate hallucinations, i.e., makeup incorrect text and unverified information, which can cause significant damage when deployed for mission-critical tasks.\"; \"revealing the shared limitation of zero-resource methods.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, LLMs are apt to generate hallucinations, i.e., makeup incorrect text and unverified information, which can cause significant damage when deployed for mission-critical tasks.\"; \"revealing the shared limitation of zero-resource methods.\""
    },
    {
        "title": "Speculative Decoding: Exploiting Speculative Execution for Accelerating Seq2seq Generation",
        "authors": [
            "Heming Xia",
            "Tao Ge",
            "Peiyi Wang",
            "Si-Qing Chen",
            "Furu Wei",
            "Zhifang Sui"
        ],
        "published": "2023",
        "summary": "We propose Speculative Decoding (SpecDec), for the first time ever, to formally study exploiting the idea of speculative execution to accelerate autoregressive (AR) decoding. Speculative Decoding has two innovations: Spec-Drafter – an independent model specially optimized for efficient and accurate drafting – and Spec-Verification – a reliable method for verifying the drafted tokens efficiently in the decoding paradigm. Experimental results on various seq2seq tasks including machine translation and abstractive summarization show our approach can achieve around 5x speedup for the popular Transformer architectures with comparable generation quality to beam search decoding, refreshing the impression that the draft-then-verify paradigm introduces only 1.4x~2x speedup. In addition to the remarkable speedup, we also demonstrate 3 additional advantages of SpecDec, revealing its practical value for accelerating generative models in real-world applications. Our models and codes are available at https://github.com/hemingkx/SpecDec.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.257.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "2INER: Instructive and In-Context Learning on Few-Shot Named Entity Recognition",
        "authors": [
            "Jiasheng Zhang",
            "Xikai Liu",
            "Xinyi Lai",
            "Yan Gao",
            "Shusen Wang",
            "Yao Hu",
            "Yiqing Lin"
        ],
        "published": "2023",
        "summary": "Prompt-based learning has emerged as a powerful technique in natural language processing (NLP) due to its ability to leverage pre-training knowledge for downstream few-shot tasks. In this paper, we propose 2INER, a novel text-to-text framework for Few-Shot Named Entity Recognition (NER) tasks. Our approach employs instruction finetuning based on InstructionNER to enable the model to effectively comprehend and process task-specific instructions, including both main and auxiliary tasks. We also introduce a new auxiliary task, called Type Extracting, to enhance the model’s understanding of entity types in the overall semantic context of a sentence. To facilitate in-context learning, we concatenate examples to the input, enabling the model to learn from additional contextual information. Experimental results on four datasets demonstrate that our approach outperforms existing Few-Shot NER methods and remains competitive with state-of-the-art standard NER algorithms.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.259.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but the paper aims to improve few-shot learning, which might be related to limitations of LLMs in low-resource settings.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None explicitly mentioned, but the paper aims to improve few-shot learning, which might be related to limitations of LLMs in low-resource settings."
    },
    {
        "title": "Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models",
        "authors": [
            "Sean Xie",
            "Soroush Vosoughi",
            "Saeed Hassanpour"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP), but their lack of interpretability has been a major concern. Current methods for interpreting LLMs are post hoc, applied after inference time, and have limitations such as their focus on low-level features and lack of explainability at higher-level text units. In this work, we introduce proto-lm, a prototypical network-based white-box framework that allows LLMs to learn immediately interpretable embeddings during the fine-tuning stage while maintaining competitive performance. Our method’s applicability and interpretability are demonstrated through experiments on a wide range of NLP tasks, and our results indicate a new possibility of creating interpretable models without sacrificing performance. This novel approach to interpretability in LLMs can pave the way for more interpretable models without the need to sacrifice performance. We release our code at https://github.com/yx131/proto-lm.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.261.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP), but their lack of interpretability has been a major concern. Current methods for interpreting LLMs are post hoc, applied after inference time, and have limitations such as their focus on low-level features and lack of explainability at higher-level text units.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP), but their lack of interpretability has been a major concern. Current methods for interpreting LLMs are post hoc, applied after inference time, and have limitations such as their focus on low-level features and lack of explainability at higher-level text units.\""
    },
    {
        "title": "GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence",
        "authors": [
            "Zhihua Wen",
            "Zhiliang Tian",
            "Wei Wu",
            "Yuxin Yang",
            "Yanqi Shi",
            "Zhen Huang",
            "Dongsheng Li"
        ],
        "published": "2023",
        "summary": "Conditional story generation is significant in human-machine interaction, particularly in producing stories with complex plots. While Large language models (LLMs) perform well on multiple NLP tasks, including story generation, it is challenging to generate stories with both complex and creative plots. Existing methods often rely on detailed prompts to guide LLMs to meet target conditions, which inadvertently restrict the creative potential of the generated stories. We argue that leveraging information from exemplary human-written stories facilitates generating more diverse plotlines. Delving deeper into story details helps build complex and credible plots. In this paper, we propose a retrieval-auGmented stoRy generation framework with a fOrest of eVidEnce (GROVE) to enhance stories’ complexity. We build a retrieval repository for target conditions to produce few-shot examples to prompt LLMs. Additionally, we design an “asking-why” prompting scheme that extracts a forest of evidence, providing compensation for the ambiguities that may occur in the generated story. This iterative process uncovers underlying story backgrounds. Finally, we select the most fitting chains of evidence from the evidence forest and integrate them into the generated story, thereby enhancing the narrative’s complexity and credibility. Experimental results and numerous examples verify the effectiveness of our method.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.262.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"it is challenging to generate stories with both complex and creative plots.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"it is challenging to generate stories with both complex and creative plots.\""
    },
    {
        "title": "KAPALM: Knowledge grAPh enhAnced Language Models for Fake News Detection",
        "authors": [
            "Jing Ma",
            "Chen Chen",
            "Chunyan Hou",
            "Xiaojie Yuan"
        ],
        "published": "2023",
        "summary": "Social media has not only facilitated news consumption, but also led to the wide spread of fake news. Because news articles in social media is usually condensed and full of knowledge entities, existing methods of fake news detection use external entity knowledge. However, majority of these methods focus on news entity information and ignore the structured knowledge among news entities. To address this issue, in this work, we propose a Knowledge grAPh enhAnced Language Model (KAPALM) which is a novel model that fuses coarse- and fine-grained representations of entity knowledge from Knowledge Graphs (KGs). Firstly, we identify entities in news content and link them to entities in KGs. Then, a subgraph of KGs is extracted to provide structured knowledge of entities in KGs and fed into a graph neural network to obtain the coarse-grained knowledge representation. This subgraph is pruned to provide fine-grained knowledge and fed into the attentive graph and graph pooling layer. Finally, we integrate the coarse- and fine-grained entity knowledge representations with the textual representation for fake news detection. The experimental results on two benchmark datasets show that our method is superior to state-of-the-art baselines. In addition, it is competitive in the few-shot scenario.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.263.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the title mentions \"Language Models\".",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the title mentions \"Language Models\"."
    },
    {
        "title": "Comparing the Evaluation and Production of Loophole Behavior in Humans and Large Language Models",
        "authors": [
            "Sonia Murthy",
            "Kiera Parece",
            "Sophie Bridgers",
            "Peng Qian",
            "Tomer Ullman"
        ],
        "published": "2023",
        "summary": "In law, lore, and everyday life, loopholes are commonplace. When people exploit a loophole, they understand the intended meaning or goal of another person, but choose to go with a different interpretation. Past and current AI research has shown that artificial intelligence engages in what seems superficially like the exploitation of loopholes, but this is likely anthropomorphization. It remains unclear to what extent current models, especially Large Language Models (LLMs), capture the pragmatic understanding required for engaging in loopholes. We examined the performance of LLMs on two metrics developed for studying loophole behavior in humans: evaluation (ratings of trouble, upset, and humor), and generation (coming up with new loopholes in a given context). We conducted a fine-grained comparison of state-of-the-art LLMs to humans, and find that while many of the models rate loophole behaviors as resulting in less trouble and upset than outright non-compliance (in line with adults), they struggle to recognize the humor in the creative exploitation of loopholes in the way that humans do. Furthermore, only two of the models, GPT 3 and 3.5, are capable of generating loopholes of their own, with GPT3.5 performing closest to the human baseline.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.264.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"they struggle to recognize the humor in the creative exploitation of loopholes in the way that humans do. Furthermore, only two of the models, GPT3 and 3.5, are capable of generating loopholes of their own, with GPT3.5 performing closest to the human baseline.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"they struggle to recognize the humor in the creative exploitation of loopholes in the way that humans do. Furthermore, only two of the models, GPT3 and 3.5, are capable of generating loopholes of their own, with GPT3.5 performing closest to the human baseline.\""
    },
    {
        "title": "InstructExcel: A Benchmark for Natural Language Instruction in Excel",
        "authors": [
            "Justin Payan",
            "Swaroop Mishra",
            "Mukul Singh",
            "Carina Negreanu",
            "Christian Poelitz",
            "Chitta Baral",
            "Subhro Roy",
            "Rasika Chakravarthy",
            "Benjamin Van Durme",
            "Elnaz Nouri"
        ],
        "published": "2023",
        "summary": "With the evolution of Large Language Models (LLMs) we can solve increasingly more complex NLP tasks across various domains, including spreadsheets. This work investigates whether LLMs can generate code (Excel OfficeScripts, a TypeScript API for executing many tasks in Excel) that solves Excel specific tasks provided via natural language user instructions. To do so we introduce a new large-scale benchmark, InstructExcel, created by leveraging the ‘Automate’ feature in Excel to automatically generate OfficeScripts from users’ actions. Our benchmark includes over 10k samples covering 170+ Excel operations across 2,000 publicly available Excel spreadsheets. Experiments across various zero-shot and few-shot settings show that InstructExcel is a hard benchmark for state of the art models like GPT-4. We observe that (1) using GPT-4 over GPT-3.5, (2) providing more in-context examples, and (3) dynamic prompting can help improve performance on this benchmark.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.265.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experiments across various zero-shot and few-shot settings show that InstructExcel is a hard benchmark for state of the art models like GPT-4.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Experiments across various zero-shot and few-shot settings show that InstructExcel is a hard benchmark for state of the art models like GPT-4.\""
    },
    {
        "title": "Hallucination Detection for Grounded Instruction Generation",
        "authors": [
            "Lingjun Zhao",
            "Khanh Nguyen",
            "Hal Daumé III"
        ],
        "published": "2023",
        "summary": "We investigate the problem of generating instructions to guide humans to navigate in simulated residential environments. A major issue with current models is hallucination: they generate references to actions or objects that are inconsistent with what a human follower would perform or encounter along the described path. We develop a model that detects these hallucinated references by adopting a model pre-trained on a large corpus of image-text pairs, and fine-tuning it with a contrastive loss that separates correct instructions from instructions containing synthesized hallucinations. Our final model outperforms several baselines, including using word probability estimated by the instruction-generation model, and supervised models based on LSTM and Transformer.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.266.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Definitions Matter: Guiding GPT for Multi-label Classification",
        "authors": [
            "Youri Peskine",
            "Damir Korenčić",
            "Ivan Grubisic",
            "Paolo Papotti",
            "Raphael Troncy",
            "Paolo Rosso"
        ],
        "published": "2023",
        "summary": "Large language models have recently risen in popularity due to their ability to perform many natural language tasks without requiring any fine-tuning. In this work, we focus on two novel ideas: (1) generating definitions from examples and using them for zero-shot classification, and (2) investigating how an LLM makes use of the definitions. We thoroughly analyze the performance of GPT-3 model for fine-grained multi-label conspiracy theory classification of tweets using zero-shot labeling. In doing so, we asses how to improve the labeling by providing minimal but meaningful context in the form of the definitions of the labels. We compare descriptive noun phrases, human-crafted definitions, introduce a new method to help the model generate definitions from examples, and propose a method to evaluate GPT-3’s understanding of the definitions. We demonstrate that improving definitions of class labels has a direct consequence on the downstream classification results.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.267.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned in the abstract, but it can be inferred that the model may require improved definitions for better performance, as the paper aims to \"improve the labeling by providing minimal but meaningful context in the form of the definitions of the labels.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned in the abstract, but it can be inferred that the model may require improved definitions for better performance, as the paper aims to \"improve the labeling by providing minimal but meaningful context in the form of the definitions of the labels.\""
    },
    {
        "title": "ECHo: A Visio-Linguistic Dataset for Event Causality Inference via Human-Centric Reasoning",
        "authors": [
            "Yuxi Xie",
            "Guanzhen Li",
            "Min-Yen Kan"
        ],
        "published": "2023",
        "summary": "We introduce ECHo (Event Causality Inference via Human-Centric Reasoning), a diagnostic dataset of event causality inference grounded in visio-linguistic social scenarios. ECHo employs real-world human-centric deductive information building on a television crime drama. ECHo requires the Theory-of-Mind (ToM) ability to understand and reason about social interactions based on multimodal information. Using ECHo, we propose a unified Chain-of-Thought (CoT) framework to assess the reasoning capability of current AI systems. Our ToM-enhanced CoT pipeline accommodates various large foundation models in both zero-shot and few-shot visio-linguistic reasoning. We use this framework to scrutinize recent large foundation models such as InstructGPT and MiniGPT-4 on three diagnostic human-centric tasks. Further analysis demonstrates ECHo as a challenging dataset to expose imperfections and inconsistencies in reasoning. Our data and code are publicly available at [https://github.com/YuxiXie/ECHo](https://github.com/YuxiXie/ECHo).",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.268.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Further analysis demonstrates ECHo as a challenging dataset to expose imperfections and inconsistencies in reasoning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Further analysis demonstrates ECHo as a challenging dataset to expose imperfections and inconsistencies in reasoning.\""
    },
    {
        "title": "An Empirical Study of Instruction-tuning Large Language Models in Chinese",
        "authors": [
            "Qingyi Si",
            "Tong Wang",
            "Zheng Lin",
            "Xu Zhang",
            "Yanan Cao",
            "Weiping Wang"
        ],
        "published": "2023",
        "summary": "The success of ChatGPT validates the potential of large language models (LLMs) in artificial general intelligence (AGI). Subsequently, the release of LLMs has sparked the open-source community’s interest in instruction-tuning, which is deemed to accelerate ChatGPT’s replication process. However, research on instruction-tuning LLMs in Chinese, the world’s most spoken language, is still in its early stages. Therefore, this paper makes an in-depth empirical study of instruction-tuning LLMs in Chinese, which can serve as a cookbook that provides valuable findings for effectively customizing LLMs that can better respond to Chinese instructions. Specifically, we systematically explore the impact of LLM bases, parameter-efficient methods, instruction data types, which are the three most important elements for instruction-tuning. Besides, we also conduct experiment to study the impact of other factors, e.g., chain-of-thought data and human-value alignment. We hope that this empirical study can make a modest contribution to the open Chinese version of ChatGPT. This paper will release a powerful Chinese LLM that is comparable to ChatGLM. The code and data are available at https: //github.com/PhoebusSi/Alpaca-CoT.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.269.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, research on instruction-tuning LLMs in Chinese, the world’s most spoken language, is still in its early stages.\"\n\nThis paper mentions the limitation of LLMs in the context of instruction-tuning in Chinese, but it does not elaborate on this limitation and instead focuses on the proposed study to address this limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, research on instruction-tuning LLMs in Chinese, the world’s most spoken language, is still in its early stages.\"\n\nThis paper mentions the limitation of LLMs in the context of instruction-tuning in Chinese, but it does not elaborate on this limitation and instead focuses on the proposed study to address this limitation."
    },
    {
        "title": "Multi-step Jailbreaking Privacy Attacks on ChatGPT",
        "authors": [
            "Haoran Li",
            "Dadi Guo",
            "Wei Fan",
            "Mingshi Xu",
            "Jie Huang",
            "Fanpu Meng",
            "Yangqiu Song"
        ],
        "published": "2023",
        "summary": "With the rapid progress of large language models (LLMs), many downstream NLP tasks can be well solved given appropriate prompts. Though model developers and researchers work hard on dialog safety to avoid generating harmful content from LLMs, it is still challenging to steer AI-generated content (AIGC) for the human good. As powerful LLMs are devouring existing text data from various domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether the private information is included in the training data and what privacy threats can these LLMs and their downstream applications bring. In this paper, we study the privacy threats from OpenAI’s ChatGPT and the New Bing enhanced by ChatGPT and show that application-integrated LLMs may cause new privacy threats. To this end, we conduct extensive experiments to support our claims and discuss LLMs’ privacy implications.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.272.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"it is still challenging to steer AI-generated content (AIGC) for the human good\"; \"it is natural to doubt whether the private information is included in the training data and what privacy threats can these LLMs and their downstream applications bring\"; \"we study the privacy threats from OpenAI’s ChatGPT and the New Bing enhanced by ChatGPT and show that application",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"it is still challenging to steer AI-generated content (AIGC) for the human good\"; \"it is natural to doubt whether the private information is included in the training data and what privacy threats can these LLMs and their downstream applications bring\"; \"we study the privacy threats from OpenAI’s ChatGPT and the New Bing enhanced by ChatGPT and show that application"
    },
    {
        "title": "Chain-of-Thought Embeddings for Stance Detection on Social Media",
        "authors": [
            "Joseph Gatto",
            "Omar Sharif",
            "Sarah Preum"
        ],
        "published": "2023",
        "summary": "Stance detection on social media is challenging for Large Language Models (LLMs), as emerging slang and colloquial language in online conversations often contain deeply implicit stance labels. Chain-of-Thought (COT) prompting has recently been shown to improve performance on stance detection tasks — alleviating some of these issues. However, COT prompting still struggles with implicit stance identification. This challenge arises because many samples are initially challenging to comprehend before a model becomes familiar with the slang and evolving knowledge related to different topics, all of which need to be acquired through the training data. In this study, we address this problem by introducing COT Embeddings which improve COT performance on stance detection tasks by embedding COT reasonings and integrating them into a traditional RoBERTa-based stance detection pipeline. Our analysis demonstrates that 1) text encoders can leverage COT reasonings with minor errors or hallucinations that would otherwise distort the COT output label. 2) Text encoders can overlook misleading COT reasoning when a sample’s prediction heavily depends on domain-specific patterns. Our model achieves SOTA performance on multiple stance detection datasets collected from social media.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.273.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Stance detection on social media is challenging for Large Language Models (LLMs), as emerging slang and colloquial language in online conversations often contain deeply implicit stance labels.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Stance detection on social media is challenging for Large Language Models (LLMs), as emerging slang and colloquial language in online conversations often contain deeply implicit stance labels.\""
    },
    {
        "title": "Using LLM for Improving Key Event Discovery: Temporal-Guided News Stream Clustering with Event Summaries",
        "authors": [
            "Nishanth Nakshatri",
            "Siyi Liu",
            "Sihao Chen",
            "Dan Roth",
            "Dan Goldwasser",
            "Daniel Hopkins"
        ],
        "published": "2023",
        "summary": "Understanding and characterizing the discus- sions around key events in news streams is important for analyzing political discourse. In this work, we study the problem of identification of such key events and the news articles associated with those events from news streams. We propose a generic framework for news stream clustering that analyzes the temporal trend of news articles to automatically extract the underlying key news events that draw significant media attention. We characterize such key events by generating event summaries, based on which we form document clusters in an unsupervised fashion. We evaluate our simple yet effective framework, and show that it produces more coherent event-focused clusters. To demonstrate the utility of our approach, and facilitate future research along the line, we use our framework to construct KeyEvents, a dataset of 40k articles with 611 key events from 11 topics.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.274.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract mentions using LLMs but does not discuss any limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract mentions using LLMs but does not discuss any limitations."
    },
    {
        "title": "Joint Semantic and Strategy Matching for Persuasive Dialogue",
        "authors": [
            "Chuhao Jin",
            "Yutao Zhu",
            "Lingzhen Kong",
            "Shijie Li",
            "Xiao Zhang",
            "Ruihua Song",
            "Xu Chen",
            "Huan Chen",
            "Yuchong Sun",
            "Yu Chen",
            "Jun Xu"
        ],
        "published": "2023",
        "summary": "Persuasive dialogue aims to persuade users to achieve some targets by conversations. While previous persuasion models have achieved notable successes, they mostly base themselves on utterance semantic matching, and an important aspect has been ignored, that is, the strategy of the conversations, for example, the agent can choose an emotional-appeal strategy to impress users. Compared with utterance semantics, conversation strategies are high-level concepts, which can be informative and provide complementary information to achieve effective persuasions. In this paper, we propose to build a persuasion model by jointly modeling the conversation semantics and strategies, where we design a BERT-like module and an auto-regressive predictor to match the semantics and strategies, respectively. Experimental results indicate that our proposed approach can significantly improve the state-of-the-art baseline by 5% on a small dataset and 37% on a large dataset in terms of Recall@1. Detailed analyses show that the auto-regressive predictor contributes most to the final performance.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.276.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization",
        "authors": [
            "Chenhui Shen",
            "Liying Cheng",
            "Xuan-Phi Nguyen",
            "Yang You",
            "Lidong Bing"
        ],
        "published": "2023",
        "summary": "With the recent undeniable advancement in reasoning abilities in large language models (LLMs) like ChatGPT and GPT-4, there is a growing trend for using LLMs on various tasks. One area where LLMs can be employed is as an alternative evaluation metric for complex generative tasks, which generally demands expensive human judges to complement the traditional automatic metrics for various evaluation dimensions such as fluency and consistency. In this work, we conduct extensive analysis to investigate the stability and reliability of LLMs as automatic evaluators for abstractive summarization. We found that while ChatGPT and GPT-4 outperform the commonly used automatic metrics, they are not ready as human replacements due to significant limitations. That is, LLM evaluators rate each candidate system inconsistently and are dimension-dependent. They also struggle to compare candidates with close performance and become more unreliable with higher-quality summaries by obtaining a lower correlation with humans. In other words, with better abstractive summarization systems being introduced at a fast pace, LLMs may result in misleading and unreliable evaluations.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.278.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We found that while ChatGPT and GPT-4 outperform the commonly used automatic metrics, they are not ready as human replacements due to significant limitations. That is, LLM evaluators rate each candidate system inconsistently and are dimension-dependent. They also struggle to compare candidates with close performance and become more unreliable with higher-quality summaries by obtaining a lower correlation with humans.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We found that while ChatGPT and GPT-4 outperform the commonly used automatic metrics, they are not ready as human replacements due to significant limitations. That is, LLM evaluators rate each candidate system inconsistently and are dimension-dependent. They also struggle to compare candidates with close performance and become more unreliable with higher-quality summaries by obtaining a lower correlation with humans.\""
    },
    {
        "title": "Towards Zero-shot Relation Extraction in Web Mining: A Multimodal Approach with Relative XML Path",
        "authors": [
            "Zilong Wang",
            "Jingbo Shang"
        ],
        "published": "2023",
        "summary": "The rapid growth of web pages and the increasing complexity of their structure poses a challenge for web mining models. Web mining models are required to understand semi-structured web pages, particularly when little is known about the subject or template of a new page. Current methods migrate language models to web mining by embedding the XML source code into the transformer or encoding the rendered layout with graph neural networks. However, these approaches do not take into account the relationships between text nodes within and across pages. In this paper, we propose a new approach, ReXMiner, for zero-shot relation extraction in web mining. ReXMiner encodes the shortest relative paths in the Document Object Model (DOM) tree of the web page which is a more accurate and efficient signal for key-value pair extraction within a web page. It also incorporates the popularity of each text node by counting the occurrence of the same text node across different web pages. We use contrastive learning to address the issue of sparsity in relation extraction. Extensive experiments on public benchmarks show that our method, ReXMiner, outperforms the state-of-the-art baselines in the task of zero-shot relation extraction in web mining.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.281.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Current methods migrate language models to web mining by embedding the XML source code into the transformer or encoding the rendered layout with graph neural networks. However, these approaches do not take into account the relationships between text nodes within and across pages.\"\n\nThis evidence suggests that the abstract mentions a limitation of current language models in web mining, but it is a minor detail and not the primary focus of the",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Current methods migrate language models to web mining by embedding the XML source code into the transformer or encoding the rendered layout with graph neural networks. However, these approaches do not take into account the relationships between text nodes within and across pages.\"\n\nThis evidence suggests that the abstract mentions a limitation of current language models in web mining, but it is a minor detail and not the primary focus of the"
    },
    {
        "title": "Narrative Style and the Spread of Health Misinformation on Twitter",
        "authors": [
            "Achyutarama Ganti",
            "Eslam Ali Hassan Hussein",
            "Steven Wilson",
            "Zexin Ma",
            "Xinyan Zhao"
        ],
        "published": "2023",
        "summary": "Using a narrative style is an effective way to communicate health information both on and off social media. Given the amount of misinformation being spread online and its potential negative effects, it is crucial to investigate the interplay between narrative communication style and misinformative health content on user engagement on social media platforms. To explore this in the context of Twitter, we start with previously annotated health misinformation tweets (n ≈15,000) and annotate a subset of the data (n=3,000) for the presence of narrative style. We then use these manually assigned labels to train text classifiers, experimenting with supervised fine-tuning and in-context learning for automatic narrative detection. We use our best model to label remaining portion of the dataset, then statistically analyze the relationship between narrative style, misinformation, and user-level features on engagement, finding that narrative use is connected to increased tweet engagement and can, in some cases, lead to increased engagement with misinformation. Finally, we analyze the general categories of language used in narratives and health misinformation in our dataset.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.282.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "HadSkip: Homotopic and Adaptive Layer Skipping of Pre-trained Language Models for Efficient Inference",
        "authors": [
            "Haoyu Wang",
            "Yaqing Wang",
            "Tianci Liu",
            "Tuo Zhao",
            "Jing Gao"
        ],
        "published": "2023",
        "summary": "Pre-trained language models (LMs) have brought remarkable performance on numerous NLP tasks. However, they require significant resources and entail high computational costs for inference, making them challenging to deploy in real-world and real-time systems. Existing early exiting methods aim to reduce computational complexity by selecting the layer at which to exit, but suffer from the limitation that they have to sequentially traverse through all layers prior to the selected exit layer, which lacks flexibility and degrades their performance. To solve this problem, we propose a homotopic and adaptive layer skipping fine-tuning method named HadSkip. HadSkip adaptively selects the layers to skip based on a predefined budget. Specifically, we introduce a learnable gate before each layer of the LM to determine whether the current layer should be skipped. To tackle various challenges in training such as discrete gates and the budget constraint, we propose a fine-grained initialization strategy and homotopic optimization strategy. We conduct extensive experiments on the GLUE benchmark, and experimental results demonstrate the proposed HadSkip outperforms all state-of-the-art baselines significantly.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.283.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they require significant resources and entail high computational costs for inference, making them challenging to deploy in real-world and real-time systems.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, they require significant resources and entail high computational costs for inference, making them challenging to deploy in real-world and real-time systems.\""
    },
    {
        "title": "Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting",
        "authors": [
            "Zhiyu Chen",
            "Yujie Lu",
            "William Wang"
        ],
        "published": "2023",
        "summary": "Mental illness remains one of the most critical public health issues of our time, due to the severe scarcity and accessibility limit of professionals. Psychotherapy requires high-level expertise to conduct deep, complex reasoning and analysis on the cognition modeling of the patients. In the era of Large Language Models, we believe it is the right time to develop AI assistance for computational psychotherapy. We study the task of cognitive distortion detection and propose the Diagnosis of Thought (DoT) prompting. DoT performs diagnosis on the patient’s speech via three stages: subjectivity assessment to separate the facts and the thoughts; contrastive reasoning to elicit the reasoning processes supporting and contradicting the thoughts; and schema analysis to summarize the cognition schemas. The generated diagnosis rationales through the three stages are essential for assisting the professionals. Experiments demonstrate that DoT obtains significant improvements over ChatGPT for cognitive distortion detection, while generating high-quality rationales approved by human experts.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.284.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract does not mention any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract does not mention any limitations of LLMs."
    },
    {
        "title": "Measuring the Knowledge Acquisition-Utilization Gap in Pretrained Language Models",
        "authors": [
            "Amirhossein Kazemnejad",
            "Mehdi Rezagholizadeh",
            "Prasanna Parthasarathi",
            "Sarath Chandar"
        ],
        "published": "2023",
        "summary": "While pre-trained language models (PLMs) have shown evidence of acquiring vast amounts of knowledge, it remains unclear how much of this parametric knowledge is actually usable in performing downstream tasks. We propose a systematic framework to measure parametric knowledge utilization in PLMs. Our framework first extracts knowledge from a PLM’s parameters and subsequently constructs a downstream task around this extracted knowledge. Performance on this task thus depends exclusively on utilizing the model’s possessed knowledge, avoiding confounding factors like insufficient signal. As an instantiation, we study factual knowledge of PLMs and measure utilization across 125M to 13B parameter PLMs. We observe that: (1) PLMs exhibit two gaps - in acquired vs. utilized knowledge, (2) they show limited robustness in utilizing knowledge under distribution shifts, and (3) larger models close the acquired knowledge gap but the utilized knowledge gap remains. Overall, our study provides insights into PLMs’ capabilities beyond their acquired knowledge.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.285.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"PLMs exhibit two gaps - in acquired vs. utilized knowledge, (2) they show limited robustness in utilizing knowledge under distribution shifts, and (3) larger models close the acquired knowledge gap but the utilized knowledge gap remains.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"PLMs exhibit two gaps - in acquired vs. utilized knowledge, (2) they show limited robustness in utilizing knowledge under distribution shifts, and (3) larger models close the acquired knowledge gap but the utilized knowledge gap remains.\""
    },
    {
        "title": "Non-compositional Expression Generation Based on Curriculum Learning and Continual Learning",
        "authors": [
            "Jianing Zhou",
            "Ziheng Zeng",
            "Hongyu Gong",
            "Suma Bhat"
        ],
        "published": "2023",
        "summary": "Non-compositional expressions, by virtue of their non-compositionality, are a classic ‘pain in the neck’ for NLP systems. Different from the general language modeling and generation tasks that are primarily compositional, generating non-compositional expressions is more challenging for current neural models, including large pre-trained language models. The main reasons are 1) their non-compositionality, and 2) the limited data resources. Therefore, to make the best use of available data for modeling non-compositionality, we propose a dynamic curriculum learning framework, which learns training examples from easy ones to harder ones thus optimizing the learning step by step but suffers from the forgetting problem. To alleviate the forgetting problem brought by the arrangement of training examples, we also apply a continual learning method into our curriculum learning framework. Our proposed method combined curriculum and continual learning, to gradually improve the model’s performance on the task of non-compositional expression generation. Experiments on idiomatic expression generation and metaphor generation affirm the effectiveness of our proposed curriculum learning framework and the application of continual learning. Our codes are available at https://github.com/zhjjn/CL2Gen.git.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.286.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Different from the general language modeling and generation tasks that are primarily compositional, generating non-compositional expressions is more challenging for current neural models, including large pre-trained language models. The main reasons are 1) their non-compositionality, and 2) the limited data resources.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Different from the general language modeling and generation tasks that are primarily compositional, generating non-compositional expressions is more challenging for current neural models, including large pre-trained language models. The main reasons are 1) their non-compositionality, and 2) the limited data resources.\""
    },
    {
        "title": "Information Extraction from Legal Wills: How Well Does GPT-4 Do?",
        "authors": [
            "Alice Kwak",
            "Cheonkam Jeong",
            "Gaetano Forte",
            "Derek Bambauer",
            "Clayton Morrison",
            "Mihai Surdeanu"
        ],
        "published": "2023",
        "summary": "This work presents a manually annotated dataset for Information Extraction (IE) from legal wills, and relevant in-context learning experiments on the dataset. The dataset consists of entities, binary relations between the entities (e.g., relations between testator and beneficiary), and n-ary events (e.g., bequest) extracted from 45 legal wills from two US states. This dataset can serve as a foundation for downstream tasks in the legal domain. Another use case of this dataset is evaluating the performance of large language models (LLMs) on this IE task. We evaluated GPT-4 with our dataset to investigate its ability to extract information from legal wills. Our evaluation result demonstrates that the model is capable of handling the task reasonably well. When given instructions and examples as a prompt, GPT-4 shows decent performance for both entity extraction and relation extraction tasks. Nevertheless, the evaluation result also reveals that the model is not perfect. We observed inconsistent outputs (given a prompt) as well as prompt over-generalization.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.287.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Nevertheless, the evaluation result also reveals that the model is not perfect. We observed inconsistent outputs (given a prompt) as well as prompt over-generalization.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Nevertheless, the evaluation result also reveals that the model is not perfect. We observed inconsistent outputs (given a prompt) as well as prompt over-generalization.\""
    },
    {
        "title": "Transparency at the Source: Evaluating and Interpreting Language Models With Access to the True Distribution",
        "authors": [
            "Jaap Jumelet",
            "Willem Zuidema"
        ],
        "published": "2023",
        "summary": "We present a setup for training, evaluating and interpreting neural language models, that uses artificial, language-like data. The data is generated using a massive probabilistic grammar (based on state-split PCFGs), that is itself derived from a large natural language corpus, but also provides us complete control over the generative process. We describe and release both grammar and corpus, and test for the naturalness of our generated data. This approach allows us define closed-form expressions to efficiently compute exact lower bounds on obtainable perplexity using both causal and masked language modelling. Our results show striking differences between neural language modelling architectures and training objectives in how closely they allow approximating the lower bound on perplexity. Our approach also allows us to directly compare learned representations to symbolic rules in the underlying source. We experiment with various techniques for interpreting model behaviour and learning dynamics. With access to the underlying true source, our results show striking differences and outcomes in learning dynamics between different classes of words.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.288.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our results show striking differences between neural language modelling architectures and training objectives in how closely they allow approximating the lower bound on perplexity.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our results show striking differences between neural language modelling architectures and training objectives in how closely they allow approximating the lower bound on perplexity.\""
    },
    {
        "title": "Frugal Prompting for Dialog Models",
        "authors": [
            "Bishal Santra",
            "Sakya Basak",
            "Abhinandan De",
            "Manish Gupta",
            "Pawan Goyal"
        ],
        "published": "2023",
        "summary": "The use of large language models (LLMs) in natural language processing (NLP) tasks is rapidly increasing, leading to changes in how researchers approach problems in the field. To fully utilize these models’ abilities, a better understanding of their behavior for different input protocols is required. With LLMs, users can directly interact with the models through a text-based interface to define and solve various tasks. Hence, understanding the conversational abilities of these LLMs, which may not have been specifically trained for dialog modeling, is also important. This study examines different approaches for building dialog systems using LLMs by considering various aspects of the prompt. As part of prompt tuning, we experiment with various ways of providing instructions, exemplars, current query and additional context. The research also analyzes the representations of dialog history that have the optimal usable-information density. Based on the findings, the paper suggests more compact ways of providing dialog history information while ensuring good performance and reducing model’s inference-API costs. The research contributes to a better understanding of how LLMs can be effectively used for building interactive systems.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.290.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Hence, understanding the conversational abilities of these LLMs, which may not have been specifically trained for dialog modeling, is also important.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (not being specifically trained for dialog modeling), but it is not the primary focus of the paper and is only briefly mentioned.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Hence, understanding the conversational abilities of these LLMs, which may not have been specifically trained for dialog modeling, is also important.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (not being specifically trained for dialog modeling), but it is not the primary focus of the paper and is only briefly mentioned."
    },
    {
        "title": "The Interpreter Understands Your Meaning: End-to-end Spoken Language Understanding Aided by Speech Translation",
        "authors": [
            "Mutian He",
            "Philip Garner"
        ],
        "published": "2023",
        "summary": "End-to-end spoken language understanding (SLU) remains elusive even with current large pretrained language models on text and speech, especially in multilingual cases. Machine translation has been established as a powerful pretraining objective on text as it enables the model to capture high-level semantics of the input utterance and associations between different languages, which is desired for speech models that work on lower-level acoustic frames. Motivated particularly by the task of cross-lingual SLU, we demonstrate that the task of speech translation (ST) is a good means of pretraining speech models for end-to-end SLU on both intra- and cross-lingual scenarios. By introducing ST, our models reach higher performance over baselines on monolingual and multilingual intent classification as well as spoken question answering using SLURP, MINDS-14, and NMSQA benchmarks. To verify the effectiveness of our methods, we also create new benchmark datasets from both synthetic and real sources, for speech summarization and low-resource/zero-shot transfer from English to French or Spanish. We further show the value of preserving knowledge for the ST pretraining task for better downstream performance, possibly using Bayesian transfer regularizers.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.291.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"End-to-end spoken language understanding (SLU) remains elusive even with current large pretrained language models on text and speech, especially in multilingual cases.\"\n\nThis rating is given because the abstract mentions a limitation of large pretrained language models in the context of end-to-end spoken language understanding, but it is not the primary focus of the paper and is used to motivate the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"End-to-end spoken language understanding (SLU) remains elusive even with current large pretrained language models on text and speech, especially in multilingual cases.\"\n\nThis rating is given because the abstract mentions a limitation of large pretrained language models in the context of end-to-end spoken language understanding, but it is not the primary focus of the paper and is used to motivate the proposed solution."
    },
    {
        "title": "Length-Adaptive Distillation: Customizing Small Language Model for Dynamic Token Pruning",
        "authors": [
            "Chang Liu",
            "Chongyang Tao",
            "Jianxin Liang",
            "Jiazhan Feng",
            "Tao Shen",
            "Quzhe Huang",
            "Dongyan Zhao"
        ],
        "published": "2023",
        "summary": "Pre-trained language models greatly improve the performance of various tasks but at a cost of high computation overhead. To facilitate practical applications, there are mainly two lines of research to accelerate model inference: model compression and dynamic computation (e.g., dynamic token pruning). Existing works either adopt these methods individually or simply apply dynamic computation approaches upon a compressed small language model. We argue that they are sub-optimal since the two approaches are separately designed so the compressed model may not be tailored for dynamic computation. To tackle this problem and make compressed small language models faster, we propose Length-Adaptive Distillation, a two-stage knowledge distillation framework that aims to produce a customized small language model for dynamic token pruning. In the general distillation stage, we enforce the student to mimic and reconstruct the teacher’s output based on the dynamically pruned representations. Then in the task-specific distillation stage, the student is further accustomed to token pruning while absorbing the task-specific knowledge. Experimental results on GLUE benchmark demonstrate that our method can make the small language model more customized for dynamic token pruning and achieve better speed-performance trade-off.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.294.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Pre-trained language models greatly improve the performance of various tasks but at a cost of high computation overhead.\"\n\n(Note: Although the paper discusses limitations of pre-trained language models, it does not specifically focus on Large Language Models (LLMs). However, the context suggests that LLMs are implied. The limitation mentioned is the high computation overhead, which is a common limitation of LLMs",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Pre-trained language models greatly improve the performance of various tasks but at a cost of high computation overhead.\"\n\n(Note: Although the paper discusses limitations of pre-trained language models, it does not specifically focus on Large Language Models (LLMs). However, the context suggests that LLMs are implied. The limitation mentioned is the high computation overhead, which is a common limitation of LLMs"
    },
    {
        "title": "Reasoning about Ambiguous Definite Descriptions",
        "authors": [
            "Stefan Schouten",
            "Peter Bloem",
            "Ilia Markov",
            "Piek Vossen"
        ],
        "published": "2023",
        "summary": "Natural language reasoning plays an increasingly important role in improving language models’ ability to solve complex language understanding tasks. An interesting use case for reasoning is the resolution of context-dependent ambiguity. But no resources exist to evaluate how well Large Language Models can use explicit reasoning to resolve ambiguity in language. We propose to use ambiguous definite descriptions for this purpose and create and publish the first benchmark dataset consisting of such phrases. Our method includes all information required to resolve the ambiguity in the prompt, which means a model does not require anything but reasoning to do well. We find this to be a challenging task for recent LLMs. Code and data available at: https://github.com/sfschouten/exploiting-ambiguity",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.296.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find this to be a challenging task for recent LLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We find this to be a challenging task for recent LLMs.\""
    },
    {
        "title": "MUX-PLMs: Data Multiplexing for High-throughput Language Models",
        "authors": [
            "Vishvak Murahari",
            "Ameet Deshpande",
            "Carlos Jimenez",
            "Izhak Shafran",
            "Mingqiu Wang",
            "Yuan Cao",
            "Karthik Narasimhan"
        ],
        "published": "2023",
        "summary": "The widespread adoption of large language models such as ChatGPT and Bard has led to unprecedented demand for these technologies. The burgeoning cost of inference for ever-increasing model sizes coupled with hardware shortages has limited affordable access and poses a pressing need for efficiency approaches geared towards high throughput and performance. Multi-input multi-output (MIMO) algorithms such as data multiplexing, offer a promising solution with a many-fold increase in throughput by performing inference for multiple inputs at the cost of a single input. Yet these approaches are not currently performant enough to be deployed in modern systems. We change that by developing MUX-PLMs, a class of high throughput pre-trained language models (PLMs) trained with data multiplexing, that can be fine-tuned for any downstream task to yield high-throughput high-performance. Our novel multiplexing and demultiplexing modules proficiently entangle and disentangle inputs, and enable high-performance high throughput MUX-PLMs that are competitive with vanilla PLMs while achieving 2x/5x inference speedup with only a 1-4 % drop on a broad suite of tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.301.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The burgeoning cost of inference for ever-increasing model sizes coupled with hardware shortages has limited affordable access and poses a pressing need for efficiency approaches geared towards high throughput and performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The burgeoning cost of inference for ever-increasing model sizes coupled with hardware shortages has limited affordable access and poses a pressing need for efficiency approaches geared towards high throughput and performance.\""
    },
    {
        "title": "That was the last straw, we need more: Are Translation Systems Sensitive to Disambiguating Context?",
        "authors": [
            "Jaechan Lee",
            "Alisa Liu",
            "Orevaoghene Ahia",
            "Hila Gonen",
            "Noah Smith"
        ],
        "published": "2023",
        "summary": "The translation of ambiguous text presents a challenge for translation systems, as it requires using the surrounding context to disambiguate the intended meaning as much as possible. While prior work has studied ambiguities that result from different grammatical features of the source and target language, we study semantic ambiguities that exist in the source (English in this work) itself. In particular, we focus on idioms that are open to both literal and figurative interpretations (e.g., goose egg), and collect TIDE, a dataset of 512 pairs of English sentences containing idioms with disambiguating context such that one is literal (it laid a goose egg) and another is figurative (they scored a goose egg, as in a score of zero). In experiments, we compare MT-specific models and language models for (i) their preference when given an ambiguous subsentence, (ii) their sensitivity to disambiguating context, and (iii) the performance disparity between figurative and literal source sentences. We find that current MT models consistently translate English idioms literally, even when the context suggests a figurative interpretation. On the other hand, LMs are far more context-aware, although there remain disparities across target languages. Our findings underline the potential of LMs as a strong backbone for context-aware translation.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.302.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"although there remain disparities across target languages.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs, but it is a minor detail and not the primary focus of the paper. The paper mainly discusses the potential of LMs as a strong backbone for context-aware translation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"although there remain disparities across target languages.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs, but it is a minor detail and not the primary focus of the paper. The paper mainly discusses the potential of LMs as a strong backbone for context-aware translation."
    },
    {
        "title": "MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic",
        "authors": [
            "Damien Sileo",
            "Antoine Lernould"
        ],
        "published": "2023",
        "summary": "Theory of Mind (ToM) is a critical component of intelligence but its assessment remains the subject of heated debates. Prior research applied human ToM assessments to natural language processing models using either human-created standardized tests or rule-based templates. However, these methods primarily focus on simplistic reasoning and require further validation. Here, we leverage dynamic epistemic logic to isolate a particular component of ToM and to generate controlled problems. We also introduce new verbalization techniques to express these problems in English natural language. Our findings indicate that some language model scaling (from 70M to 6B and 350M to 174B) does not consistently yield results better than random chance. While GPT-4 demonstrates superior epistemic reasoning capabilities, there is still room for improvement. Our code and datasets are publicly available.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.303.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these methods primarily focus on simplistic reasoning and require further validation\"; \"some language model scaling (from 70M to 6B and 350M to 174B) does not consistently yield results better than random chance\"; \"While GPT-4 demonstrates superior epistemic reasoning capabilities, there is still room for improvement.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, these methods primarily focus on simplistic reasoning and require further validation\"; \"some language model scaling (from 70M to 6B and 350M to 174B) does not consistently yield results better than random chance\"; \"While GPT-4 demonstrates superior epistemic reasoning capabilities, there is still room for improvement.\""
    },
    {
        "title": "RobustEmbed: Robust Sentence Embeddings Using Self-Supervised Contrastive Pre-Training",
        "authors": [
            "Javad Asl",
            "Eduardo Blanco",
            "Daniel Takabi"
        ],
        "published": "2023",
        "summary": "Pre-trained language models (PLMs) have demonstrated their exceptional performance across a wide range of natural language processing tasks. The utilization of PLM-based sentence embeddings enables the generation of contextual representations that capture rich semantic information. However, despite their success with unseen samples, current PLM-based representations suffer from poor robustness in adversarial scenarios. In this paper, we propose RobustEmbed, a self-supervised sentence embedding framework that enhances both generalization and robustness in various text representation tasks and against diverse adversarial attacks. By generating high-risk adversarial perturbations to promote higher invariance in the embedding space and leveraging the perturbation within a novel contrastive objective approach, RobustEmbed effectively learns high-quality sentence embeddings. Our extensive experiments validate the superiority of RobustEmbed over previous state-of-the-art self-supervised representations in adversarial settings, while also showcasing relative improvements in seven semantic textual similarity (STS) tasks and six transfer tasks. Specifically, our framework achieves a significant reduction in attack success rate from 75.51% to 39.62% for the BERTAttack attack technique, along with enhancements of 1.20% and 0.40% in STS tasks and transfer tasks, respectively.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.305.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, despite their success with unseen samples, current PLM-based representations suffer from poor robustness in adversarial scenarios.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, despite their success with unseen samples, current PLM-based representations suffer from poor robustness in adversarial scenarios.\""
    },
    {
        "title": "Automatic Evaluation of Attribution by Large Language Models",
        "authors": [
            "Xiang Yue",
            "Boshi Wang",
            "Ziru Chen",
            "Kai Zhang",
            "Yu Su",
            "Huan Sun"
        ],
        "published": "2023",
        "summary": "A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support its claims. However, evaluating the attribution, i.e., verifying whether the generated statement is fully supported by the cited reference, remains an open problem. Although human evaluation is common practice, it is costly and time-consuming. In this paper, we investigate automatic evaluation of attribution given by LLMs. We begin by defining different types of attribution errors, and then explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed from related tasks such as question answering, fact-checking, natural language inference, and summarization. We manually curate a set of test examples covering 12 domains from a generative search engine, New Bing. Our results on this curated test set and simulated examples from existing benchmarks highlight both promising signals and challenges. We hope our problem formulation, testbeds, and findings will help lay the foundation for future studies on this important problem.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.307.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, evaluating the attribution, i.e., verifying whether the generated statement is fully supported by the cited reference, remains an open problem.\"\n\nThis rating is given because the abstract mentions one limitation of LLMs (the difficulty in evaluating attribution) in passing, but it is not the primary focus of the paper. The paper aims to propose a solution to this problem rather than exploring the",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, evaluating the attribution, i.e., verifying whether the generated statement is fully supported by the cited reference, remains an open problem.\"\n\nThis rating is given because the abstract mentions one limitation of LLMs (the difficulty in evaluating attribution) in passing, but it is not the primary focus of the paper. The paper aims to propose a solution to this problem rather than exploring the"
    },
    {
        "title": "LDM2: A Large Decision Model Imitating Human Cognition with Dynamic Memory Enhancement",
        "authors": [
            "Xingjin Wang",
            "Linjing Li",
            "Daniel Zeng"
        ],
        "published": "2023",
        "summary": "With the rapid development of large language models (LLMs), it is highly demanded that LLMs can be adopted to make decisions to enable the artificial general intelligence. Most approaches leverage manually crafted examples to prompt the LLMs to imitate the decision process of human. However, designing optimal prompts is difficult and the patterned prompts can hardly be generalized to more complex environments. In this paper, we propose a novel model named Large Decision Model with Memory (LDM2), which leverages a dynamic memory mechanism to construct dynamic prompts, guiding the LLMs in making proper decisions according to the faced state. LDM2 consists of two stages: memory formation and memory refinement. In the former stage, human behaviors are decomposed into state-action tuples utilizing the powerful summarizing ability of LLMs. Then, these tuples are stored in the memory, whose indices are generated by the LLMs, to facilitate the retrieval of the most relevant subset of memorized tuples based on the current state. In the latter stage, our LDM2 employs tree exploration to discover more suitable decision processes and enrich the memory by adding valuable state-action tuples. The dynamic circle of exploration and memory enhancement provides LDM2 a better understanding of the global environment. Extensive experiments conducted in two interactive environments have shown that our LDM2 outperforms the baselines in terms of both score and success rate, which demonstrates its effectiveness.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.309.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, designing optimal prompts is difficult and the patterned prompts can hardly be generalized to more complex environments.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, designing optimal prompts is difficult and the patterned prompts can hardly be generalized to more complex environments.\""
    },
    {
        "title": "ZARA: Improving Few-Shot Self-Rationalization for Small Language Models",
        "authors": [
            "Wei-Lin Chen",
            "An-Zi Yen",
            "Cheng-Kuang Wu",
            "Hen-Hsen Huang",
            "Hsin-Hsi Chen"
        ],
        "published": "2023",
        "summary": "Language models (LMs) that jointly generate end-task answers as well as free-text rationales are known as self-rationalization models. Recent works demonstrate great performance gain for self-rationalization by few-shot prompting LMs with rationale-augmented exemplars. However, the ability to benefit from explanations only emerges with large-scale LMs, which have poor accessibility. In this work, we explore the less-studied setting of leveraging explanations for small LMs to improve few-shot self-rationalization. We first revisit the relationship between rationales and answers. Inspired by the implicit mental process of how human beings assess explanations, we present a novel approach, Zero-shot Augmentation of Rationale-Answer pairs (ZARA), to automatically construct pseudo-parallel data for self-training by reducing the problem of plausibility judgement to natural language inference. Experimental results show ZARA achieves SOTA performance on the FEB benchmark, for both the task accuracy and the explanation metric. In addition, we conduct human and quantitative evaluation validating ZARA’s ability to automatically identify plausible and accurate rationale-answer pairs.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.310.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the ability to benefit from explanations only emerges with large-scale LMs, which have poor accessibility.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the ability to benefit from explanations only emerges with large-scale LMs, which have poor accessibility.\""
    },
    {
        "title": "ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation",
        "authors": [
            "Zi Lin",
            "Zihan Wang",
            "Yongqi Tong",
            "Yangkun Wang",
            "Yuxin Guo",
            "Yujia Wang",
            "Jingbo Shang"
        ],
        "published": "2023",
        "summary": "Despite remarkable advances that large language models have achieved in chatbots nowadays, maintaining a non-toxic user-AI interactive environment has become increasingly critical nowadays. However, previous efforts in toxicity detection have been mostly based on benchmarks derived from social media contents, leaving the unique challenges inherent to real-world user-AI interactions insufficiently explored. In this work, we introduce ToxicChat, a novel benchmark constructed based on real user queries from an open-source chatbot. This benchmark contains the rich, nuanced phenomena that can be tricky for current toxicity detection models to identify, revealing a significant domain difference when compared to social media contents. Our systematic evaluation of models trained on existing toxicity datasets has shown their shortcomings when applied to this unique domain of ToxicChat. Our work illuminates the potentially overlooked challenges of toxicity detection in real-world user-AI conversations. In the future, ToxicChat can be a valuable resource to drive further advancements toward building a safe and healthy environment for user-AI interactions.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.311.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our systematic evaluation of models trained on existing toxicity datasets has shown their shortcomings when applied to this unique domain of ToxicChat.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Our systematic evaluation of models trained on existing toxicity datasets has shown their shortcomings when applied to this unique domain of ToxicChat.\""
    },
    {
        "title": "Dior-CVAE: Pre-trained Language Models and Diffusion Priors for Variational Dialog Generation",
        "authors": [
            "Tianyu Yang",
            "Thy Thy Tran",
            "Iryna Gurevych"
        ],
        "published": "2023",
        "summary": "Current variational dialog models have employed pre-trained language models (PLMs) to parameterize the likelihood and posterior distributions. However, the Gaussian assumption made on the prior distribution is incompatible with these distributions, thus restricting the diversity of generated responses. These models also suffer from posterior collapse, i.e., the decoder tends to ignore latent variables and directly access information captured in the encoder through the cross-attention mechanism. In this work, we propose Dior-CVAE, a hierarchical conditional variational autoencoder (CVAE) with diffusion priors to address these challenges. We employ a diffusion model to increase the complexity of the prior distribution and its compatibility with the distributions produced by a PLM. Also, we propose memory dropout to the cross-attention mechanism, which actively encourages the use of latent variables for response generation. Overall, experiments across two commonly used open-domain dialog datasets show that our method can generate more diverse responses without large-scale dialog pre-training. Code is available at https://github.com/UKPLab/dior-cvae.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.313.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the Gaussian assumption made on the prior distribution is incompatible with these distributions, thus restricting the diversity of generated responses. These models also suffer from posterior collapse, i.e., the decoder tends to ignore latent variables and directly access information captured in the encoder through the cross-attention mechanism.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, the Gaussian assumption made on the prior distribution is incompatible with these distributions, thus restricting the diversity of generated responses. These models also suffer from posterior collapse, i.e., the decoder tends to ignore latent variables and directly access information captured in the encoder through the cross-attention mechanism.\""
    },
    {
        "title": "Retrieving Multimodal Information for Augmented Generation: A Survey",
        "authors": [
            "Ruochen Zhao",
            "Hailin Chen",
            "Weishi Wang",
            "Fangkai Jiao",
            "Xuan Long Do",
            "Chengwei Qin",
            "Bosheng Ding",
            "Xiaobao Guo",
            "Minzhi Li",
            "Xingxuan Li",
            "Shafiq Joty"
        ],
        "published": "2023",
        "summary": "As Large Language Models (LLMs) become popular, there emerged an important trend of using multimodality to augment the LLMs’ generation ability, which enables LLMs to better interact with the world. However, there lacks a unified perception of at which stage and how to incorporate different modalities. In this survey, we review methods that assist and augment generative models by retrieving multimodal knowledge, whose formats range from images, codes, tables, graphs, to audio. Such methods offer a promising solution to important concerns such as factuality, reasoning, interpretability, and robustness. By providing an in-depth review, this survey is expected to provide scholars with a deeper understanding of the methods’ applications and encourage them to adapt existing techniques to the fast-growing field of LLMs.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.314.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"important concerns such as factuality, reasoning, interpretability, and robustness.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"important concerns such as factuality, reasoning, interpretability, and robustness.\""
    },
    {
        "title": "The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation",
        "authors": [
            "Dung Nguyen",
            "Le Nam",
            "Anh Dau",
            "Anh Nguyen",
            "Khanh Nghiem",
            "Jin Guo",
            "Nghi Bui"
        ],
        "published": "2023",
        "summary": "We present The Vault, an open-source dataset of high quality code-text pairs in multiple programming languages for training large language models to understand and generate code. We propose methods for thoroughly extracting samples that use both rules and deep learning to ensure that they contain high-quality pairs of code and text, resulting in a dataset of 43 million high-quality code-text pairs. We thoroughly evaluated this dataset and discovered that when used to train common code language models (such as CodeT5, CodeBERT, and CodeGen), it outperforms the same models train on other datasets such as CodeSearchNet. These evaluations included common coding tasks such as code generation, code summarization, and code search. The Vault can be used by researchers and practitioners to train a wide range of big language models that understand code. Alternatively, researchers can use our data cleaning methods and scripts to improve their own datasets. We anticipate that using The Vault to train large language models will improve their ability to understand and generate code, propelling AI research and software development forward. We are releasing our source code and a framework to make it easier for others to replicate our results.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.316.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "On the Zero-Shot Generalization of Machine-Generated Text Detectors",
        "authors": [
            "Xiao Pu",
            "Jingyu Zhang",
            "Xiaochuang Han",
            "Yulia Tsvetkov",
            "Tianxing He"
        ],
        "published": "2023",
        "summary": "The rampant proliferation of large language models, fluent enough to generate text indistinguishable from human-written language, gives unprecedented importance to the detection of machine-generated text. This work is motivated by an important research question: How will the detectors of machine-generated text perform on outputs of a new generator, that the detectors were not trained on? We begin by collecting generation data from a wide range of LLMs, and train neural detectors on data from each generator and test its performance on held-out generators. While none of the detectors can generalize to all generators, we observe a consistent and interesting pattern that the detectors trained on data from a medium-size LLM can zero-shot generalize to the larger version. As a concrete application, we demonstrate that robust detectors can be built on an ensemble of training data from medium-sized models.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.318.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While none of the detectors can generalize to all generators\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While none of the detectors can generalize to all generators\""
    },
    {
        "title": "Complex Event Schema Induction with Knowledge-Enriched Diffusion Model",
        "authors": [
            "Yupu Hao",
            "Pengfei Cao",
            "Yubo Chen",
            "Kang Liu",
            "Jiexin Xu",
            "Huaijun Li",
            "Xiaojian Jiang",
            "Jun Zhao"
        ],
        "published": "2023",
        "summary": "The concept of a complex event schema pertains to the graph structure that represents real-world knowledge of events and their multi-dimensional relationships. However, previous studies on event schema induction have been hindered by challenges such as error propagation and data quality issues. To tackle these challenges, we propose a knowledge-enriched discrete diffusion model. Specifically, we distill the abundant event scenario knowledge of Large Language Models (LLMs) through an object-oriented Python style prompt. We incorporate this knowledge into the training data, enhancing its quality. Subsequently, we employ a discrete diffusion process to generate all nodes and links simultaneously in a non-auto-regressive manner to tackle the problem of error propagation. Additionally, we devise an entity relationship prediction module to complete entity relationships between event arguments. Experimental results demonstrate that our approach achieves outstanding performance across a range of evaluation metrics.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.319.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, previous studies on event schema induction have been hindered by challenges such as error propagation and data quality issues.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, previous studies on event schema induction have been hindered by challenges such as error propagation and data quality issues.\""
    },
    {
        "title": "Long-Range Language Modeling with Selective Cache",
        "authors": [
            "Xinting Huang",
            "Nora Hollenstein"
        ],
        "published": "2023",
        "summary": "The computational cost of transformer-based language models grows quadratically with the sequence length. In this paper, we introduce the selective cache, which stores the selected key-value pairs from the previous context. By selecting important key-value pairs the model makes better use of the cache so that in limited cache size, a longer context history can be stored. We design three kinds of selection methods. The first is based on human language processing. The key-value pairs are selected if they correspond to tokens that are fixated longer, as recorded in eye-tracking-while-reading experiments. We also incorporate the cognitively-inspired selection process into the language model as a trainable process, resulting in two additional methods with improved performance. The selection task is converted into a pruning task so they can be trained with differentiable masks. We demonstrate that the proposed selective cache improves the language modeling performance across different datasets. With the same number of stored key-value pairs (cache size), our selective cache outperforms XL cache and compressive cache by considerable margins.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.321.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The computational cost of transformer-based language models grows quadratically with the sequence length.\"\n\nThis paper mentions a limitation of transformer-based language models (computational cost growing with sequence length) in passing, but does not elaborate on it further.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The computational cost of transformer-based language models grows quadratically with the sequence length.\"\n\nThis paper mentions a limitation of transformer-based language models (computational cost growing with sequence length) in passing, but does not elaborate on it further."
    },
    {
        "title": "FaLA: Fast Linear Adaptation for Replacing Backbone Models on Edge Devices",
        "authors": [
            "Shuo Huang",
            "Lizhen Qu",
            "Xingliang Yuan",
            "Chunyang Chen"
        ],
        "published": "2023",
        "summary": "In this work, we study the language model backbone replacement problem for personalized downstream tasks in a non-stationary on-device scenario. In real world, company may periodically update the knowledge and architectures of backbones to keep the competitive in the market, meanwhile, to accommodate the users’ own preference, models are personalized to fit users’ own distribution locally. Traditional full model tuning or transfer learning for such replacements often incur considerable local device training costs and necessitate extensive backpropagation within deep transformer layers. Addressing this issue, we propose a novel, lightweight tuning method for personalized NLP classification tasks post-backbone replacement. Our approach leverages a personalized matrix calculated from documents corresponding to users’ old and new backbones. This matrix facilitates top-layer parameter tuning, drastically reducing backpropagation computation. To further mitigate training costs associated with matrix linear optimization, we employ correlation clustering to curate a few examples from personalized cluster sets for individuals. Our method achieves over 1000 times computation reduction in Flops for backpropagation and brings the user-specific initialization for personal matrix yielding significant performance boost compared with popular transfer learning methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.323.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Traditional full model tuning or transfer learning for such replacements often incur considerable local device training costs and necessitate extensive backpropagation within deep transformer layers.\"\n\nThis rating is chosen because the abstract mentions a limitation of traditional full model tuning or transfer learning for LLMs (high local device training costs and extensive backpropagation within deep transformer layers) but does not explore this limitation in depth and",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Traditional full model tuning or transfer learning for such replacements often incur considerable local device training costs and necessitate extensive backpropagation within deep transformer layers.\"\n\nThis rating is chosen because the abstract mentions a limitation of traditional full model tuning or transfer learning for LLMs (high local device training costs and extensive backpropagation within deep transformer layers) but does not explore this limitation in depth and"
    },
    {
        "title": "Is ChatGPT a Good Multi-Party Conversation Solver?",
        "authors": [
            "Chao-Hong Tan",
            "Jia-Chen Gu",
            "Zhen-Hua Ling"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have emerged as influential instruments within the realm of natural language processing; nevertheless, their capacity to handle multi-party conversations (MPCs) – a scenario marked by the presence of multiple interlocutors involved in intricate information exchanges – remains uncharted. In this paper, we delve into the potential of generative LLMs such as ChatGPT and GPT-4 within the context of MPCs. An empirical analysis is conducted to assess the zero-shot learning capabilities of ChatGPT and GPT-4 by subjecting them to evaluation across three MPC datasets that encompass five representative tasks. The findings reveal that ChatGPT’s performance on a number of evaluated MPC tasks leaves much to be desired, whilst GPT-4’s results portend a promising future. Additionally, we endeavor to bolster performance through the incorporation of MPC structures, encompassing both speaker and addressee architecture. This study provides an exhaustive evaluation and analysis of applying generative LLMs to MPCs, casting a light upon the conception and creation of increasingly effective and robust MPC agents. Concurrently, this work underscores the challenges implicit in the utilization of LLMs for MPCs, such as deciphering graphical information flows and generating stylistically consistent responses.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.326.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The findings reveal that ChatGPT’s performance on a number of evaluated MPC tasks leaves much to be desired... Concurrently, this work underscores the challenges implicit in the utilization of LLMs for MPCs, such as deciphering graphical information flows and generating stylistically consistent responses.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"The findings reveal that ChatGPT’s performance on a number of evaluated MPC tasks leaves much to be desired... Concurrently, this work underscores the challenges implicit in the utilization of LLMs for MPCs, such as deciphering graphical information flows and generating stylistically consistent responses.\""
    },
    {
        "title": "Bayesian Multi-Task Transfer Learning for Soft Prompt Tuning",
        "authors": [
            "Haeju Lee",
            "Minchan Jeong",
            "Se-Young Yun",
            "Kee-Eung Kim"
        ],
        "published": "2023",
        "summary": "Prompt tuning, in which prompts are optimized to adapt large-scale pre-trained language models to downstream tasks instead of fine-tuning the full model parameters, has been shown to be particularly effective when the prompts are trained in the multi-task transfer learning setting. These methods generally involve individually training prompts for each source task and then aggregating them to provide the initialization of the prompt for the target task. However, this approach critically ignores the fact that some of the source tasks could be negatively or positively interfering with each other. We argue that when we extract knowledge from source tasks via training source prompts, we need to consider this correlation among source tasks for better transfer to target tasks. To this end, we propose a Bayesian approach where we work with the posterior distribution of prompts across source tasks. We obtain representative source prompts corresponding to the samples from the posterior utilizing Stein Variational Gradient Descent, which are then aggregated to constitute the initial target prompt. We show extensive experimental results on the standard benchmark NLP tasks, where our Bayesian multi-task transfer learning approach outperforms the state-of-the-art methods in many settings. Furthermore, our approach requires no auxiliary models other than the prompt itself, achieving high degree of parameter-efficiency.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.329.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, this approach critically ignores the fact that some of the source tasks could be negatively or positively interfering with each other.\"\n\nThis paper mentions a limitation of the existing multi-task transfer learning approach for prompt tuning in the context of LLMs, but it is a minor detail and not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, this approach critically ignores the fact that some of the source tasks could be negatively or positively interfering with each other.\"\n\nThis paper mentions a limitation of the existing multi-task transfer learning approach for prompt tuning in the context of LLMs, but it is a minor detail and not the primary focus of the paper."
    },
    {
        "title": "A Critical Analysis of Document Out-of-Distribution Detection",
        "authors": [
            "Jiuxiang Gu",
            "Yifei Ming",
            "Yi Zhou",
            "Jason Kuen",
            "Vlad Morariu",
            "Handong Zhao",
            "Ruiyi Zhang",
            "Nikolaos Barmpalios",
            "Anqi Liu",
            "Yixuan Li",
            "Tong Sun",
            "Ani Nenkova"
        ],
        "published": "2023",
        "summary": "Large-scale pre-training is widely used in recent document understanding tasks. During deployment, one may expect that models should trigger a conservative fallback policy when encountering out-of-distribution (OOD) samples, which highlights the importance of OOD detection. However, most existing OOD detection methods focus on single-modal inputs such as images or texts. While documents are multi-modal in nature, it is underexplored if and how multi-modal information in documents can be exploited for OOD detection. In this work, we first provide a systematic and in-depth analysis on OOD detection for document understanding models. We study the effects of model modality, pre-training, and fine-tuning across various types of OOD inputs. In particular, we find that spatial information is critical for document OOD detection. To better exploit spatial information, we propose a spatial-aware adapter, which serves as a parameter-efficient add-on module to adapt transformer-based language models to the document domain. Extensive experiments show that adding the spatial-aware adapter significantly improves the OOD detection performance compared to directly using the language model and achieves superior performance compared to competitive baselines.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.332.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While documents are multi-modal in nature, it is underexplored if and how multi-modal information in documents can be exploited for OOD detection.\"\n\nNote: The paper mentions a limitation of existing OOD detection methods (focusing on single-modal inputs) and implies a limitation of transformer-based language models (not being able to effectively exploit multi-modal information in documents), but it is not",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While documents are multi-modal in nature, it is underexplored if and how multi-modal information in documents can be exploited for OOD detection.\"\n\nNote: The paper mentions a limitation of existing OOD detection methods (focusing on single-modal inputs) and implies a limitation of transformer-based language models (not being able to effectively exploit multi-modal information in documents), but it is not"
    },
    {
        "title": "Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting",
        "authors": [
            "Ke Wang",
            "Jun Xie",
            "Yuqi Zhang",
            "Yu Zhao"
        ],
        "published": "2023",
        "summary": "Improving neural machine translation (NMT) systems with prompting has achieved significant progress in recent years. In this work, we focus on how to integrate multi-knowledge, multiple types of knowledge, into NMT models to enhance the performance with prompting. We propose a unified framework, which can integrate effectively multiple types of knowledge including sentences, terminologies/phrases and translation templates into NMT models. We utilize multiple types of knowledge as prefix-prompts of input for the encoder and decoder of NMT models to guide the translation process. The approach requires no changes to the model architecture and effectively adapts to domain-specific translation without retraining. The experiments on English-Chinese and English-German translation demonstrate that our approach significantly outperform strong baselines, achieving high translation quality and terminology match accuracy.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.333.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Active Learning Principles for In-Context Learning with Large Language Models",
        "authors": [
            "Katerina Margatina",
            "Timo Schick",
            "Nikolaos Aletras",
            "Jane Dwivedi-Yu"
        ],
        "published": "2023",
        "summary": "The remarkable advancements in large language models (LLMs) have significantly enhanced predictive performance in few-shot learning settings. By using only a small number of labeled examples, referred to as demonstrations, LLMs can effectively perform the task at hand through in-context learning. However, the process of selecting demonstrations for maximizing performance has received limited attention in prior work. This paper addresses the issue of identifying the most informative demonstrations for few-shot learning by approaching it as a pool-based Active Learning (AL) problem over a single iteration. We compare standard AL algorithms based on uncertainty, diversity, and similarity, and consistently observe that the latter outperforms all other methods, including random sampling. Our extensive experimentation involving a diverse range of GPT and OPT models across 24 classification and multi-choice tasks, coupled with thorough analysis, unambiguously demonstrates the importance of using demonstrations that are semantically similar to the domain of the test examples. In fact, we show higher average classification performance using “similar” demonstrations with GPT-2 (124M) than random demonstrations with GPT-Neox (20B). Notably, while diversity sampling shows promise, uncertainty sampling, despite its success in conventional supervised learning AL scenarios, performs poorly in in-context learning.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.334.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the process of selecting demonstrations for maximizing performance has received limited attention in prior work.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the process of selecting demonstrations for maximizing performance has received limited attention in prior work.\""
    },
    {
        "title": "InteMATs: Integrating Granularity-Specific Multilingual Adapters for Cross-Lingual Transfer",
        "authors": [
            "Meizhen Liu",
            "Xu Guo",
            "He Jiakai",
            "Jianye Chen",
            "Fengyu Zhou",
            "Siu Hui"
        ],
        "published": "2023",
        "summary": "Multilingual language models (MLLMs) have achieved remarkable success in various cross-lingual transfer tasks. However, they suffer poor performance in zero-shot low-resource languages, particularly when dealing with longer contexts. Existing research mainly relies on full-model fine-tuning on large parallel datasets to enhance the cross-lingual alignment of MLLMs, which is computationally expensive. In this paper, we propose InteMATs, a novel approach that integrates multilingual adapters trained on texts of different levels of granularity. To achieve this, we curate a multilingual parallel dataset comprising 42 languages to pre-train sentence-level and document-level adapters under the contrastive learning framework. Extensive experiments demonstrate the effectiveness of InteMATs in improving the cross-lingual transfer performance of MLLMs, especially on low-resource languages. Finally, our comprehensive analyses and ablation studies provide a deep understanding of the high-quality representations derived by InteMATs.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.335.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they suffer poor performance in zero-shot low-resource languages, particularly when dealing with longer contexts.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, they suffer poor performance in zero-shot low-resource languages, particularly when dealing with longer contexts.\""
    },
    {
        "title": "PlugMed: Improving Specificity in Patient-Centered Medical Dialogue Generation using In-Context Learning",
        "authors": [
            "Chengfeng Dou",
            "Zhi Jin",
            "Wenpin Jiao",
            "Haiyan Zhao",
            "Yongqiang Zhao",
            "Zhengwei Tao"
        ],
        "published": "2023",
        "summary": "The patient-centered medical dialogue systems strive to offer diagnostic interpretation services to users who are less knowledgeable about medical knowledge, through emphasizing the importance of providing responses specific to the patients. It is difficult for the large language models (LLMs) to guarantee the specificity of responses in spite of its promising performance even in some tasks in medical field. Inspired by in-context learning, we propose PlugMed, a Plug-and-Play Medical Dialogue System, for addressing this challenge. PlugMed is equipped with two modules, the prompt generation (PG) module and the response ranking (RR) module, to enhances LLMs’ dialogue strategies for improving the specificity of the dialogue. The PG module is designed to stimulate the imitative ability of LLMs by providing them with real dialogues from similar patients as prompts. The RR module incorporates fine-tuned small model as response filter to enable the selection of appropriate responses generated by LLMs. Furthermore, we introduce a new evaluation method based on matching both user’s intent and high-frequency medical term to effectively assess the specificity of the responses. We conduct experimental evaluations on three medical dialogue datasets, and the results, including both automatic and human evaluation, demonstrate the effectiveness of our approach.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.336.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"It is difficult for the large language models (LLMs) to guarantee the specificity of responses in spite of its promising performance even in some tasks in medical field.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"It is difficult for the large language models (LLMs) to guarantee the specificity of responses in spite of its promising performance even in some tasks in medical field.\""
    },
    {
        "title": "CodeTransOcean: A Comprehensive Multilingual Benchmark for Code Translation",
        "authors": [
            "Weixiang Yan",
            "Yuchen Tian",
            "Yunzhe Li",
            "Qian Chen",
            "Wen Wang"
        ],
        "published": "2023",
        "summary": "Recent code translation techniques exploit neural machine translation models to translate source code from one programming language to another to satisfy production compatibility or to improve efficiency of codebase maintenance. Most existing code translation datasets only focus on a single pair of popular programming languages. To advance research on code translation and meet diverse requirements of real-world applications, we construct **CodeTransOcean**, a large-scale comprehensive benchmark that supports the largest variety of programming languages for code translation. CodeTransOcean consists of three novel multilingual datasets, namely, **MultilingualTrans** supporting translations between multiple popular programming languages, **NicheTrans** for translating between niche programming languages and popular ones, and **LLMTrans** for evaluating executability of translated code by large language models (LLMs). CodeTransOcean also includes a novel cross-framework dataset, **DLTrans**, for translating deep learning code across different frameworks. We develop multilingual modeling approaches for code translation and demonstrate their great potential in improving the translation quality of both low-resource and high-resource language pairs and boosting the training efficiency. We also propose a novel evaluation metric **Debugging Success Rate@K** for program-level code translation. Last but not least, we evaluate LLM ChatGPT on our datasets and investigate its potential for fuzzy execution predictions. We build baselines for CodeTransOcean and analyze challenges of code translation for guiding future research. The CodeTransOcean datasets and code are publicly available at https://github.com/WeixiangYAN/CodeTransOcean.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.337.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We also propose a novel evaluation metric **Debugging Success Rate@K** for program-level code translation... We evaluate LLM ChatGPT on our datasets and investigate its potential for fuzzy execution predictions.\"\n\nThis abstract mentions LLMs but does not discuss any limitations of the models in detail.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"We also propose a novel evaluation metric **Debugging Success Rate@K** for program-level code translation... We evaluate LLM ChatGPT on our datasets and investigate its potential for fuzzy execution predictions.\"\n\nThis abstract mentions LLMs but does not discuss any limitations of the models in detail."
    },
    {
        "title": "impact of sample selection on in-context learning for entity extraction from scientific writing",
        "authors": [
            "Necva Bölücü",
            "Maciej Rybinski",
            "Stephen Wan"
        ],
        "published": "2023",
        "summary": "Prompt-based usage of Large Language Models (LLMs) is an increasingly popular way to tackle many well-known natural language problems. This trend is due, in part, to the appeal of the In-Context Learning (ICL) prompt set-up, in which a few selected training examples are provided along with the inference request. ICL, a type of few-shot learning, is especially attractive for natural language processing (NLP) tasks defined for specialised domains, such as entity extraction from scientific documents, where the annotation is very costly due to expertise requirements for the annotators. In this paper, we present a comprehensive analysis of in-context sample selection methods for entity extraction from scientific documents using GPT-3.5 and compare these results against a fully supervised transformer-based baseline. Our results indicate that the effectiveness of the in-context sample selection methods is heavily domain-dependent, but the improvements are more notable for problems with a larger number of entity types. More in-depth analysis shows that ICL is more effective for low-resource set-ups of scientific information extraction",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.338.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our results indicate that the effectiveness of the in-context sample selection methods is heavily domain-dependent\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our results indicate that the effectiveness of the in-context sample selection methods is heavily domain-dependent\""
    },
    {
        "title": "MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading Comprehension",
        "authors": [
            "Guoxin Chen",
            "Yiming Qian",
            "Bowen Wang",
            "Liangzhi Li"
        ],
        "published": "2023",
        "summary": "The large language models have achieved superior performance on various natural language tasks. One major drawback of such approaches is they are resource-intensive in fine-tuning new datasets. Soft-prompt tuning presents a resource-efficient solution to fine-tune the pre-trained language models (PLMs) while keeping their weight frozen. Existing soft prompt methods mainly focus on designing the input-independent prompts that steer the model to fit the domain of the new dataset. Those methods often ignore the fine-grained information about the task and context of the text. In this paper, we propose a multi-level prompt tuning (MPrompt) method for machine reading comprehension. It utilizes prompts at task-specific, domain-specific, and context-specific levels to enhance the comprehension of input semantics at different granularities. We also propose an independence constraint to steer each domain-specific prompt to focus on information within its domain to avoid redundancy. Moreover, we present a prompt generator that incorporates context-related knowledge in the prompt generation to enhance contextual relevancy. We conducted extensive experiments on 12 benchmarks of various QA formats and achieved an average improvement of 1.94% over the state-of-the-art methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.343.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"One major drawback of such approaches is they are resource-intensive in fine-tuning new datasets.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"One major drawback of such approaches is they are resource-intensive in fine-tuning new datasets.\""
    },
    {
        "title": "Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs",
        "authors": [
            "Jiefeng Chen",
            "Jinsung Yoon",
            "Sayna Ebrahimi",
            "Sercan Arik",
            "Tomas Pfister",
            "Somesh Jha"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have recently shown great advances in a variety of tasks, including natural language understanding and generation. However, their use in high-stakes decision-making scenarios is still limited due to the potential for errors. *Selective prediction* is a technique that can be used to improve the reliability of the LLMs by allowing them to abstain from making predictions when they are unsure of the answer. In this work, we propose a novel framework for adaptation with self-evaluation to improve the selective prediction performance of LLMs. Our framework is based on the idea of using parameter-efficient tuning to adapt the LLM to the specific task at hand while improving its ability to perform self-evaluation. We evaluate our method on a variety of question-answering (QA) datasets and show that it outperforms state-of-the-art selective prediction methods. For example, on the CoQA benchmark, our method improves the AUACC from 91.23% to 92.63% and improves the AUROC from 74.61% to 80.25%.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.345.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their use in high-stakes decision-making scenarios is still limited due to the potential for errors.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, their use in high-stakes decision-making scenarios is still limited due to the potential for errors.\""
    },
    {
        "title": "Bi-Drop: Enhancing Fine-tuning Generalization via Synchronous sub-net Estimation and Optimization",
        "authors": [
            "Shoujie Tong",
            "Heming Xia",
            "Damai Dai",
            "Runxin Xu",
            "Tianyu Liu",
            "Binghuai Lin",
            "Yunbo Cao",
            "Zhifang Sui"
        ],
        "published": "2023",
        "summary": "Pretrained language models have achieved remarkable success in natural language understanding. However, fine-tuning pretrained models on limited training data tends to overfit and thus diminish performance. This paper presents Bi-Drop, a fine-tuning strategy that selectively updates model parameters using gradients from various sub-nets dynamically generated by dropout. The sub-net estimation of Bi-Drop is performed in an in-batch manner, so it overcomes the problem of hysteresis in sub-net updating, which is possessed by previous methods that perform asynchronous sub-net estimation. Also, Bi-Drop needs only one mini-batch to estimate the sub-net so it achieves higher utility of training data. Experiments on the GLUE benchmark demonstrate that Bi-Drop consistently outperforms previous fine-tuning methods. Furthermore, empirical results also show that Bi-Drop exhibits excellent generalization ability and robustness for domain transfer, data imbalance, and low-resource scenarios.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.346.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, fine-tuning pretrained models on limited training data tends to overfit and thus diminish performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, fine-tuning pretrained models on limited training data tends to overfit and thus diminish performance.\""
    },
    {
        "title": "ClozEx: A Task toward Generation of English Cloze Explanation",
        "authors": [
            "Zizheng Zhang",
            "Masato Mita",
            "Mamoru Komachi"
        ],
        "published": "2023",
        "summary": "Providing explanations for cloze questions in language assessment (LA) has been recognized as a valuable approach to enhancing the language proficiency of learners. However, there is a noticeable absence of dedicated tasks and datasets specifically designed for generating language learner explanations. In response to this gap, this paper introduces a novel task ClozEx of generating explanations for cloze questions in LA, with a particular focus on English as a Second Language (ESL) learners. To support this task, we present a meticulously curated dataset comprising cloze questions paired with corresponding explanations. This dataset aims to assess language proficiency and facilitates language learning by offering informative and accurate explanations. To tackle the task, we fine-tuned various baseline models with our training data, including encoder-decoder and decoder-only architectures. We also explored whether large language models (LLMs) are able to generate good explanations without fine-tuning, just using pre-defined prompts. The evaluation results demonstrate that encoder-decoder models have the potential to deliver fluent and valid explanations when trained on our dataset.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.347.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We also explored whether large language models (LLMs) are able to generate good explanations without fine-tuning, just using pre-defined prompts.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (the need for fine-tuning to generate good explanations) but only briefly and in passing, without elaborating on it further.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We also explored whether large language models (LLMs) are able to generate good explanations without fine-tuning, just using pre-defined prompts.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (the need for fine-tuning to generate good explanations) but only briefly and in passing, without elaborating on it further."
    },
    {
        "title": "The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models",
        "authors": [
            "Satya Sai Srinath Namburi",
            "Makesh Sreedhar",
            "Srinath Srinivasan",
            "Frederic Sala"
        ],
        "published": "2023",
        "summary": "Compressing large language models (LLMs), often consisting of billions of parameters, provides faster inference, smaller memory footprints, and enables local deployment. The standard compression techniques are pruning and quantization, with the former eliminating redundant connections in model layers and the latter representing model parameters with as little as 4 bits. The key tradeoff is between the degree of compression and the impact on the quality of the compressed model. Existing research on LLM compression primarily focuses on performance in terms of general metrics like perplexity or downstream task accuracy. More fine-grained metrics, such as those measuring parametric knowledge, remain significantly underexplored. To help bridge this gap, we present a comprehensive analysis across multiple model families using the LAMA and LM-Harness benchmarks in order to systematically quantify the effect of commonly employed compression techniques on model performance. A particular focus is on tradeoffs involving parametric knowledge, with the goal of providing practitioners with practical insights to make informed decisions on compression.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.349.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The key tradeoff is between the degree of compression and the impact on the quality of the compressed model.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"The key tradeoff is between the degree of compression and the impact on the quality of the compressed model.\""
    },
    {
        "title": "CoEdIT: Text Editing by Task-Specific Instruction Tuning",
        "authors": [
            "Vipul Raheja",
            "Dhruv Kumar",
            "Ryan Koo",
            "Dongyeop Kang"
        ],
        "published": "2023",
        "summary": "We introduce CoEdIT, a state-of-the-art text editing system for writing assistance. CoEdIT takes instructions from the user specifying the attributes of the desired text, such as “Make the sentence simpler” or “Write it in a more neutral style,” and outputs the edited text. We present a large language model fine-tuned on a diverse collection of task-specific instructions for text editing (a total of 82K instructions). Our model (1) achieves state-of-the-art performance on various text editing benchmarks, (2) is competitive with publicly available largest-sized LLMs trained on instructions while being ~60x smaller, (3) is capable of generalizing to unseen edit instructions, and (4) exhibits abilities to generalize to composite instructions containing different combinations of edit actions. Through extensive qualitative and quantitative analysis, we show that writers prefer the edits suggested by CoEdIT relative to other state-of-the-art text editing models. Our code, data, and models are publicly available at https://github.com/vipulraheja/coedit.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.350.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper does not mention any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper does not mention any limitations of LLMs."
    },
    {
        "title": "Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection",
        "authors": [
            "Yi Dai",
            "Hao Lang",
            "Kaisheng Zeng",
            "Fei Huang",
            "Yongbin Li"
        ],
        "published": "2023",
        "summary": "Out-of-distribution (OOD) detection is essential for reliable and trustworthy machine learning. Recent multi-modal OOD detection leverages textual information from in-distribution (ID) class names for visual OOD detection, yet it currently neglects the rich contextual information of ID classes. Large language models (LLMs) encode a wealth of world knowledge and can be prompted to generate descriptive features for each class. Indiscriminately using such knowledge causes catastrophic damage to OOD detection due to LLMs’ hallucinations, as is observed by our analysis. In this paper, we propose to apply world knowledge to enhance OOD detection performance through selective generation from LLMs. Specifically, we introduce a consistency-based uncertainty calibration method to estimate the confidence score of each generation. We further extract visual objects from each image to fully capitalize on the aforementioned world knowledge. Extensive experiments demonstrate that our method consistently outperforms the state-of-the-art.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.351.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Indiscriminately using such knowledge causes catastrophic damage to OOD detection due to LLMs’ hallucinations, as is observed by our analysis.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Indiscriminately using such knowledge causes catastrophic damage to OOD detection due to LLMs’ hallucinations, as is observed by our analysis.\""
    },
    {
        "title": "Better Together: Enhancing Generative Knowledge Graph Completion with Language Models and Neighborhood Information",
        "authors": [
            "Alla Chepurova",
            "Aydar Bulatov",
            "Yuri Kuratov",
            "Mikhail Burtsev"
        ],
        "published": "2023",
        "summary": "Real-world Knowledge Graphs (KGs) often suffer from incompleteness, which limits their potential performance. Knowledge Graph Completion (KGC) techniques aim to address this issue. However, traditional KGC methods are computationally intensive and impractical for large-scale KGs, necessitating the learning of dense node embeddings and computing pairwise distances. Generative transformer-based language models (e.g., T5 and recent KGT5) offer a promising solution as they can predict the tail nodes directly. In this study, we propose to include node neighborhoods as additional information to improve KGC methods based on language models. We examine the effects of this imputation and show that, on both inductive and transductive Wikidata subsets, our method outperforms KGT5 and conventional KGC approaches. We also provide an extensive analysis of the impact of neighborhood on model prediction and show its importance. Furthermore, we point the way to significantly improve KGC through more effective neighborhood selection.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.352.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but the paper aims to improve KGC methods based on language models, implying that existing methods may have limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None explicitly mentioned, but the paper aims to improve KGC methods based on language models, implying that existing methods may have limitations."
    },
    {
        "title": "DeltaScore: Fine-Grained Story Evaluation with Perturbations",
        "authors": [
            "Zhuohan Xie",
            "Miao Li",
            "Trevor Cohn",
            "Jey Lau"
        ],
        "published": "2023",
        "summary": "Numerous evaluation metrics have been developed for natural language generation tasks, but their effectiveness in evaluating stories is limited as they are not specifically tailored to assess intricate aspects of storytelling, such as fluency and interestingness. In this paper, we introduce DeltaScore, a novel methodology that uses perturbation techniques for the evaluation of nuanced story aspects. We posit that the extent to which a story excels in a specific aspect (e.g., fluency) correlates with the magnitude of its susceptibility to particular perturbations (e.g., the introduction of typos). Given this, we measure the quality of an aspect by calculating the likelihood difference between pre- and post-perturbation states using pre-trained language models. We compare DeltaScore with existing metrics on storytelling datasets from two domains in five fine-grained story aspects: fluency, coherence, relatedness, logicality, and interestingness. DeltaScore demonstrates strong performance, revealing a surprising finding that one specific perturbation proves highly effective in capturing multiple aspects. Source code is available on our GitHub repository.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.353.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We measure the quality of an aspect by calculating the likelihood difference between pre- and post-perturbation states using pre-trained language models.\"\n\nNote that the paper mentions pre-trained language models but does not discuss any limitations of LLMs. However, it does mention that existing evaluation metrics have limitations in evaluating stories, which could be related to LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We measure the quality of an aspect by calculating the likelihood difference between pre- and post-perturbation states using pre-trained language models.\"\n\nNote that the paper mentions pre-trained language models but does not discuss any limitations of LLMs. However, it does mention that existing evaluation metrics have limitations in evaluating stories, which could be related to LLMs."
    },
    {
        "title": "Black-Box Tuning of Vision-Language Models with Effective Gradient Approximation",
        "authors": [
            "Zixian Guo",
            "Yuxiang Wei",
            "Ming Liu",
            "Zhilong Ji",
            "Jinfeng Bai",
            "Yiwen Guo",
            "Wangmeng Zuo"
        ],
        "published": "2023",
        "summary": "Parameter-efficient fine-tuning (PEFT) methods have provided an effective way for adapting large vision-language models to specific tasks or scenarios. Typically, they learn a very small scale of parameters for pre-trained models in a white-box formulation, which assumes model architectures to be known and parameters to be accessible. However, large models are often not open-source due to considerations of preventing abuse or commercial factors, hence posing a barrier to the deployment of white-box PEFT methods. To alleviate the dependence on model accessibility, we introduce collaborative black-box tuning (CBBT) for both textual prompt optimization and output feature adaptation for black-box models. Specifically, considering that the backpropagation gradients are blocked, we approximate the gradients of textual prompts by analyzing the predictions with perturbed prompts. Secondly, a lightweight adapter is deployed over the output feature of the inaccessible model, further facilitating the model adaptation process. Empowered with these designs, our CBBT is extensively evaluated on eleven downstream benchmarks and achieves remarkable improvements compared to existing black-box VL adaptation methods. Our code will be made publicly available.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.356.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, large models are often not open-source due to considerations of preventing abuse or commercial factors, hence posing a barrier to the deployment of white-box PEFT methods.\"\n\nThis rating is given because the paper mentions a limitation of large vision-language models (inaccessibility due to abuse or commercial factors) but does not explore it in depth, and instead focuses on the proposed solution to alleviate",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, large models are often not open-source due to considerations of preventing abuse or commercial factors, hence posing a barrier to the deployment of white-box PEFT methods.\"\n\nThis rating is given because the paper mentions a limitation of large vision-language models (inaccessibility due to abuse or commercial factors) but does not explore it in depth, and instead focuses on the proposed solution to alleviate"
    },
    {
        "title": "How to Determine the Most Powerful Pre-trained Language Model without Brute Force Fine-tuning? An Empirical Survey",
        "authors": [
            "Jun Bai",
            "Xiaofeng Zhang",
            "Chen Li",
            "Hanhua Hong",
            "Xi Xu",
            "Chenghua Lin",
            "Wenge Rong"
        ],
        "published": "2023",
        "summary": "Transferability estimation has been attached to great attention in the computer vision fields. Researchers try to estimate with low computational cost the performance of a model when transferred from a source task to a given target task. Considering the effectiveness of such estimations, the communities of natural language processing also began to study similar problems for the selection of pre-trained language models. However, there is a lack of a comprehensive comparison between these estimation methods yet. Also, the differences between vision and language scenarios make it doubtful whether previous conclusions can be established across fields. In this paper, we first conduct a thorough survey of existing transferability estimation methods being able to find the most suitable model, then we conduct a detailed empirical study for the surveyed methods based on the GLUE benchmark. From qualitative and quantitative analyses, we demonstrate the strengths and weaknesses of existing methods and show that H-Score generally performs well with superiorities in effectiveness and efficiency. We also outline the difficulties of consideration of training details, applicability to text generation, and consistency to certain metrics which shed light on future directions.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.357.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We also outline the difficulties of consideration of training details, applicability to text generation, and consistency to certain metrics which shed light on future directions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We also outline the difficulties of consideration of training details, applicability to text generation, and consistency to certain metrics which shed light on future directions.\""
    },
    {
        "title": "InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations",
        "authors": [
            "Nils Feldhus",
            "Qianli Wang",
            "Tatiana Anikina",
            "Sahil Chopra",
            "Cennet Oguz",
            "Sebastian Möller"
        ],
        "published": "2023",
        "summary": "While recently developed NLP explainability methods let us open the black box in various ways (Madsen et al., 2022), a missing ingredient in this endeavor is an interactive tool offering a conversational interface. Such a dialogue system can help users explore datasets and models with explanations in a contextualized manner, e.g. via clarification or follow-up questions, and through a natural language interface. We adapt the conversational explanation framework TalkToModel (Slack et al., 2022) to the NLP domain, add new NLP-specific operations such as free-text rationalization, and illustrate its generalizability on three NLP tasks (dialogue act classification, question answering, hate speech detection). To recognize user queries for explanations, we evaluate fine-tuned and few-shot prompting models and implement a novel adapter-based approach. We then conduct two user studies on (1) the perceived correctness and helpfulness of the dialogues, and (2) the simulatability, i.e. how objectively helpful dialogical explanations are for humans in figuring out the model’s predicted label when it’s not shown. We found rationalization and feature attribution were helpful in explaining the model behavior. Moreover, users could more reliably predict the model outcome based on an explanation dialogue rather than one-off explanations.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.359.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "INVITE: a Testbed of Automatically Generated Invalid Questions to Evaluate Large Language Models for Hallucinations",
        "authors": [
            "Anil Ramakrishna",
            "Rahul Gupta",
            "Jens Lehmann",
            "Morteza Ziyadi"
        ],
        "published": "2023",
        "summary": "Recent advancements in Large language models (LLMs) have enabled them to hold free form conversations over multiple turns, but they exhibit a tendency to make unfounded and incorrect statements, commonly known as hallucinations. In particular, LLMs hallucinate frequently when given invalid questions, i.e. ones with incorrect assumptions. The most common approach to evaluate LLMs on hallucinations is to test them on Question Answering (QA) test sets such as TruthfulQA. However, LLMs are increasingly pretrained on massive text corpora scraped from the Internet, which may inevitably expose these test sets to the model during training, leading eventually to an overestimation of model performances on these test sets. In this work, we present an alternative framework to address this risk and to foster further research towards making LLMs robust against invalid questions. We name our framework INVITE: a testbed of automatically generated INValId questions to evaluaTE large language models for hallucinations. In each instantiation, our framework is set up to create a fresh batch of invalid questions by distorting valid facts in which subjects or objects are replaced by similar entities. We evaluate several state of the art LLMs against a testset generated by our framework and highlight its capacity to trigger hallucinations in these models.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.360.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent advancements in Large language models (LLMs) have enabled them to hold free form conversations over multiple turns, but they exhibit a tendency to make unfounded and incorrect statements, commonly known as hallucinations.\"; \"We evaluate several state of the art LLMs against a testset generated by our framework and highlight its capacity to trigger hallucinations in these models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Recent advancements in Large language models (LLMs) have enabled them to hold free form conversations over multiple turns, but they exhibit a tendency to make unfounded and incorrect statements, commonly known as hallucinations.\"; \"We evaluate several state of the art LLMs against a testset generated by our framework and highlight its capacity to trigger hallucinations in these models.\""
    },
    {
        "title": "PROTEGE: Prompt-based Diverse Question Generation from Web Articles",
        "authors": [
            "Vinayak Puranik",
            "Anirban Majumder",
            "Vineet Chaoji"
        ],
        "published": "2023",
        "summary": "Rich and diverse knowledge bases (KB) are foundational building blocks for online knowledge sharing communities such as StackOverflow and Quora, and applications such as conversational assistants (aka chatbots). A popular format for knowledge bases is question-answer pairs (or FAQs), where questions are designed to accurately match a multitude of queries. In this paper, we address the problem of automatic creation of such Q&A-based knowledge bases from domain-specific, long-form textual content (e.g., web articles). Specifically, we consider the problem of question generation, which is the task of generating questions given a paragraph of text as input, with a goal to achieve both diversity and fidelity of the generated questions. Towards this goal we propose PROTEGE, a diverse question generation framework which consists of (1) a novel encoder-decoder based Large Language Model (LLM) architecture which can take a variety of prompts and generate a diverse set of candidate questions, and (2) a hill-climbing algorithm that maximizes a sub-modular objective function to balance diversity with fidelity. Through our experiments on three popular public Q&A datasets, we demonstrate that PROTEGE improves diversity by +16% and fidelity by +8% over diverse beam search and prompt-based baselines.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.362.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper mentions a novel LLM architecture but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper mentions a novel LLM architecture but does not discuss any limitations of LLMs."
    },
    {
        "title": "GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions",
        "authors": [
            "Ting-Yao Hsu",
            "Chieh-Yang Huang",
            "Ryan Rossi",
            "Sungchul Kim",
            "C. Giles",
            "Ting-Hao Huang"
        ],
        "published": "2023",
        "summary": "There is growing interest in systems that generate captions for scientific figures. However, assessing these systems’ output poses a significant challenge. Human evaluation requires academic expertise and is costly, while automatic evaluation depends on often low-quality author-written captions. This paper investigates using large language models (LLMs) as a cost-effective, reference-free method for evaluating figure captions. We first constructed SCICAP-EVAL, a human evaluation dataset that contains human judgments for 3,600 scientific figure captions, both original and machine-made, for 600 arXiv figures. We then prompted LLMs like GPT-4 and GPT-3 to score (1-6) each caption based on its potential to aid reader understanding, given relevant context such as figure-mentioning paragraphs. Results show that GPT-4, used as a zero-shot evaluator, outperformed all other models and even surpassed assessments made by computer science undergraduates, achieving a Kendall correlation score of 0.401 with Ph.D. students’ rankings.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.363.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, however the paper mentions that automatic evaluation depends on often low-quality author-written captions which might be indirectly related to LLMs' performance.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, however the paper mentions that automatic evaluation depends on often low-quality author-written captions which might be indirectly related to LLMs' performance."
    },
    {
        "title": "HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning",
        "authors": [
            "Yongjin Yang",
            "Joonkee Kim",
            "Yujin Kim",
            "Namgyu Ho",
            "James Thorne",
            "Se-Young Yun"
        ],
        "published": "2023",
        "summary": "With the proliferation of social media, accurate detection of hate speech has become critical to ensure safety online. To combat nuanced forms of hate speech, it is important to identify and thoroughly explain hate speech to help users understand its harmful effects. Recent benchmarks have attempted to tackle this issue by training generative models on free-text annotations of implications in hateful text. However, we find significant reasoning gaps in the existing annotations schemes, which may hinder the supervision of detection models. In this paper, we introduce a hate speech detection framework, **HARE**, which harnesses the reasoning capabilities of large language models (LLMs) to fill these gaps in explanations of hate speech, thus enabling effective supervision of detection models. Experiments on SBIC and Implicit Hate benchmarks show that our method, using model-generated data, consistently outperforms baselines, using existing free-text human annotations. Analysis demonstrates that our method enhances the explanation quality of trained models and improves generalization to unseen datasets. Our code is available at https://github.com/joonkeekim/hare-hate-speech.git.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.365.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, we find significant reasoning gaps in the existing annotations schemes, which may hinder the supervision of detection models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, we find significant reasoning gaps in the existing annotations schemes, which may hinder the supervision of detection models.\""
    },
    {
        "title": "ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction",
        "authors": [
            "Yaorui Shi",
            "An Zhang",
            "Enzhi Zhang",
            "Zhiyuan Liu",
            "Xiang Wang"
        ],
        "published": "2023",
        "summary": "Predicting chemical reactions, a fundamental challenge in chemistry, involves forecasting the resulting products from a given reaction process. Conventional techniques, notably those employing Graph Neural Networks (GNNs), are often limited by insufficient training data and their inability to utilize textual information, undermining their applicability in real-world applications. In this work, we propose **ReLM**, a novel framework that leverages the chemical knowledge encoded in language models (LMs) to assist GNNs, thereby enhancing the accuracy of real-world chemical reaction predictions. To further enhance the model’s robustness and interpretability, we incorporate the confidence score strategy, enabling the LMs to self-assess the reliability of their predictions. Our experimental results demonstrate that ReLM improves the performance of state-of-the-art GNN-based methods across various chemical reaction datasets, especially in out-of-distribution settings. Codes are available at https://github.com/syr-cn/ReLM.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.366.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, only a brief mention of leveraging \"chemical knowledge encoded in language models\" without discussing any limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, only a brief mention of leveraging \"chemical knowledge encoded in language models\" without discussing any limitations."
    },
    {
        "title": "Decomposing Complex Queries for Tip-of-the-tongue Retrieval",
        "authors": [
            "Kevin Lin",
            "Kyle Lo",
            "Joseph Gonzalez",
            "Dan Klein"
        ],
        "published": "2023",
        "summary": "When re-finding items, users who forget or are uncertain about identifying details often rely on creative strategies for expressing their information needs—complex queries that describe content elements (e.g., book characters or events), information beyond the document text (e.g., descriptions of book covers), or personal context (e.g., when they read a book). Standard retrieval models that rely on lexical or semantic overlap between query and document text are challenged in such retrieval settings, known as tip-of-the-tongue (TOT) retrieval. We introduce a simple but effective framework for handling such complex queries by decomposing the query with an LLM into individual clues routing those as subqueries to specialized retrievers, and ensembling the results. Our approach takes advantage of off-the-shelf retrievers (e.g., CLIP for retrieving images of book covers) or incorporate retriever-specific logic (e.g., date constraints). We show that our framework incorporating query decomposition into retrievers can improve gold book recall up to 6% absolute gain for Recall@5 on a new collection of 14,441 real-world query-book pairs from an online community for resolving TOT inquiries.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.367.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Standard retrieval models that rely on lexical or semantic overlap between query and document text are challenged in such retrieval settings, known as tip-of-the-tongue (TOT) retrieval.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Standard retrieval models that rely on lexical or semantic overlap between query and document text are challenged in such retrieval settings, known as tip-of-the-tongue (TOT) retrieval.\""
    },
    {
        "title": "Values, Ethics, Morals? On the Use of Moral Concepts in NLP Research",
        "authors": [
            "Karina Vida",
            "Judith Simon",
            "Anne Lauscher"
        ],
        "published": "2023",
        "summary": "With language technology increasingly affecting individuals’ lives, many recent works have investigated the ethical aspects of NLP. Among other topics, researchers focused on the notion of morality, investigating, for example, which moral judgements language models make. However, there has been little to no discussion of the terminology and the theories underpinning those efforts and their implications. This lack is highly problematic, as it hides the works’ underlying assumptions and hinders a thorough and targeted scientific debate of morality in NLP. In this work, we address this research gap by (a) providing an overview of some important ethical concepts stemming from philosophy and (b) systematically surveying the existing literature on moral NLP w.r.t. their philosophical foundation, terminology, and data basis. For instance, we analyse what ethical theory an approach is based on, how this decision is justified, and what implications it entails. Our findings surveying 92 papers show that, for instance, most papers neither provide a clear definition of the terms they use nor adhere to definitions from philosophy. Finally, (c) we give three recommendations for future research in the field. We hope our work will lead to a more informed, careful, and sound discussion of morality in language technology.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.368.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Adapting Pretrained Text-to-Text Models for Long Text Sequences",
        "authors": [
            "Wenhan Xiong",
            "Anchit Gupta",
            "Shubham Toshniwal",
            "Yashar Mehdad",
            "Scott Yih"
        ],
        "published": "2023",
        "summary": "We present an empirical study of adapting an existing pretrained text-to-text model for long-sequence inputs. Through a comprehensive study along three axes of the pretraining pipeline – model architecture, optimization objective, and pretraining corpus, we propose an effective recipe to build long-context models from existing short-context models. Specifically, we replace the full attention in transformers with pooling-augmented blockwise attention, and pretrain the model with a masked-span prediction task with spans of varying lengths. In terms of the pretraining corpus, we find that using randomly concatenated short-documents from a large open-domain corpus results in better performance than using existing long document corpora, which are typically limited in their domain coverage. With these findings, we build a long-context model that achieves competitive performance on long-text QA tasks and establishes the new state of the art on five long-text summarization datasets, often outperforming previous methods with larger model sizes.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.370.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we replace the full attention in transformers with pooling-augmented blockwise attention\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"we replace the full attention in transformers with pooling-augmented blockwise attention\""
    },
    {
        "title": "xDial-Eval: A Multilingual Open-Domain Dialogue Evaluation Benchmark",
        "authors": [
            "Chen Zhang",
            "Luis D’Haro",
            "Chengguang Tang",
            "Ke Shi",
            "Guohua Tang",
            "Haizhou Li"
        ],
        "published": "2023",
        "summary": "Recent advancements in reference-free learned metrics for open-domain dialogue evaluation have been driven by the progress in pre-trained language models and the availability of dialogue data with high-quality human annotations. However, current studies predominantly concentrate on English dialogues, and the generalization of these metrics to other languages has not been fully examined. This is largely due to the absence of a multilingual dialogue evaluation benchmark. To address the issue, we introduce xDial-Eval, built on top of open-source English dialogue evaluation datasets. xDial-Eval includes 12 turn-level and 6 dialogue-level English datasets, comprising 14930 annotated turns and 8691 annotated dialogues respectively. The English dialogue data are extended to nine other languages with commercial machine translation systems. On xDial-Eval, we conduct comprehensive analyses of previous BERT-based metrics and the recently-emerged large language models. Lastly, we establish strong self-supervised and multilingual baselines. In terms of average Pearson correlations over all datasets and languages, the best baseline outperforms OpenAI’s ChatGPT by absolute improvements of 6.5% and 4.6% at the turn and dialogue levels respectively, albeit with much fewer parameters. The data and code are publicly available at https://github.com/e0397123/xDial-Eval.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.371.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, current studies predominantly concentrate on English dialogues, and the generalization of these metrics to other languages has not been fully examined.\"\n\nThis rating is given because the abstract mentions a limitation of current studies (generalization to other languages), which is related to LLMs, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, current studies predominantly concentrate on English dialogues, and the generalization of these metrics to other languages has not been fully examined.\"\n\nThis rating is given because the abstract mentions a limitation of current studies (generalization to other languages), which is related to LLMs, but it is not the primary focus of the paper."
    },
    {
        "title": "MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems",
        "authors": [
            "Jakub Macina",
            "Nico Daheim",
            "Sankalan Chowdhury",
            "Tanmay Sinha",
            "Manu Kapur",
            "Iryna Gurevych",
            "Mrinmaya Sachan"
        ],
        "published": "2023",
        "summary": "While automatic dialogue tutors hold great potential in making education personalized and more accessible, research on such systems has been hampered by a lack of sufficiently large and high-quality datasets. Collecting such datasets remains challenging, as recording tutoring sessions raises privacy concerns and crowdsourcing leads to insufficient data quality. To address this, we propose a framework to generate such dialogues by pairing human teachers with a Large Language Model (LLM) prompted to represent common student errors. We describe how we use this framework to collect MathDial, a dataset of 3k one-to-one teacher-student tutoring dialogues grounded in multi-step math reasoning problems. While models like GPT-3 are good problem solvers, they fail at tutoring because they generate factually incorrect feedback or are prone to revealing solutions to students too early. To overcome this, we let teachers provide learning opportunities to students by guiding them using various scaffolding questions according to a taxonomy of teacher moves. We demonstrate MathDial and its extensive annotations can be used to finetune models to be more effective tutors (and not just solvers). We confirm this by automatic and human evaluation, notably in an interactive setting that measures the trade-off between student solving success and telling solutions. The dataset is released publicly.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.372.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While models like GPT-3 are good problem solvers, they fail at tutoring because they generate factually incorrect feedback or are prone to revealing solutions to students too early.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"While models like GPT-3 are good problem solvers, they fail at tutoring because they generate factually incorrect feedback or are prone to revealing solutions to students too early.\""
    },
    {
        "title": "Towards Making the Most of ChatGPT for Machine Translation",
        "authors": [
            "Keqin Peng",
            "Liang Ding",
            "Qihuang Zhong",
            "Li Shen",
            "Xuebo Liu",
            "Min Zhang",
            "Yuanxin Ouyang",
            "Dacheng Tao"
        ],
        "published": "2023",
        "summary": "ChatGPT shows remarkable capabilities for machine translation (MT). Several prior studies have shown that it achieves comparable results to commercial systems for high-resource languages, but lags behind in complex tasks, e.g, low-resource and distant-language-pairs translation. However, they usually adopt simple prompts which can not fully elicit the capability of ChatGPT. In this report, we aim to further mine ChatGPT’s translation ability by revisiting several aspects: temperature, task information, and domain information, and correspondingly propose two (simple but effective) prompts: Task-Specific Prompts (TSP) and Domain-Specific Prompts (DSP). We show that: 1) The performance of ChatGPT depends largely on temperature, and a lower temperature usually can achieve better performance; 2) Emphasizing the task information further improves ChatGPT’s performance, particularly in complex MT tasks; 3) Introducing domain information can elicit ChatGPT’s generalization ability and improve its performance in the specific domain; 4) ChatGPT tends to generate hallucinations for non-English-centric MT tasks, which can be partially addressed by our proposed prompts but still need to be highlighted for the MT/NLP community. We also explore the effects of advanced in-context learning strategies and find a (negative but interesting) observation: the powerful chain-of-thought prompt leads to word-by-word translation behavior, thus bringing significant translation degradation.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.373.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they usually adopt simple prompts which can not fully elicit the capability of ChatGPT.\" and \"ChatGPT tends to generate hallucinations for non-English-centric MT tasks, which can be partially addressed by our proposed prompts but still need to be highlighted for the MT/NLP community.\"\n\nThis rating is chosen because the paper mentions a couple of limitations of LLMs",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, they usually adopt simple prompts which can not fully elicit the capability of ChatGPT.\" and \"ChatGPT tends to generate hallucinations for non-English-centric MT tasks, which can be partially addressed by our proposed prompts but still need to be highlighted for the MT/NLP community.\"\n\nThis rating is chosen because the paper mentions a couple of limitations of LLMs"
    },
    {
        "title": "Enhancing Reasoning Capabilities by Instruction Learning and Chain-of-Thoughts for Implicit Discourse Relation Recognition",
        "authors": [
            "Yuxiang Lu",
            "Yu Hong",
            "Zhipang Wang",
            "Guodong Zhou"
        ],
        "published": "2023",
        "summary": "The aim of implicit discourse relation recognition is to comprehend the sense of connection between two arguments. In this work, we present a classification method that is solely based on generative models. Our proposed approach employs a combination of instruction templates and in-context learning to refine the generative model for effectively addressing the implicit discourse relation recognition task. Furthermore, we utilize Chain-of-Thoughts to partition the inference process into a sequence of three successive stages. This strategy enables us to fully utilize the autoregressive generative model’s potential for knowledge acquisition and inference, ultimately leading to enhanced performance on this natural language understanding task. The results of our experiments, evaluated on benchmark datasets PDTB 2.0, PDTB 3.0, and the CoNLL16 shared task, demonstrate superior performance compared to previous state-of-the-art models.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.374.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but the abstract implies that previous models had limitations in addressing implicit discourse relation recognition, as it claims to achieve \"enhanced performance\" and \"superior performance compared to previous state-of-the-art models\".",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations, but the abstract implies that previous models had limitations in addressing implicit discourse relation recognition, as it claims to achieve \"enhanced performance\" and \"superior performance compared to previous state-of-the-art models\"."
    },
    {
        "title": "Measuring and Narrowing the Compositionality Gap in Language Models",
        "authors": [
            "Ofir Press",
            "Muru Zhang",
            "Sewon Min",
            "Ludwig Schmidt",
            "Noah Smith",
            "Mike Lewis"
        ],
        "published": "2023",
        "summary": "We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning. We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly instead of implicitly. We present a new method, self-ask, that further improves on chain of thought. In our method, the model explicitly asks itself (and then answers) follow-up questions before answering the initial question. We finally show that self-ask’s structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.378.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning.\""
    },
    {
        "title": "Unsupervised Candidate Answer Extraction through Differentiable Masker-Reconstructor Model",
        "authors": [
            "Zhuoer Wang",
            "Yicheng Wang",
            "Ziwei Zhu",
            "James Caverlee"
        ],
        "published": "2023",
        "summary": "Question generation is a widely used data augmentation approach with extensive applications, and extracting qualified candidate answers from context passages is a critical step for most question generation systems. However, existing methods for candidate answer extraction are reliant on linguistic rules or annotated data that face the partial annotation issue and challenges in generalization. To overcome these limitations, we propose a novel unsupervised candidate answer extraction approach that leverages the inherent structure of context passages through a Differentiable Masker-Reconstructor (DMR) Model with the enforcement of self-consistency for picking up salient information tokens. We curated two datasets with exhaustively-annotated answers and benchmark a comprehensive set of supervised and unsupervised candidate answer extraction methods. We demonstrate the effectiveness of the DMR model by showing its performance is superior among unsupervised methods and comparable to supervised methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.379.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "HoneyBee: Progressive Instruction Finetuning of Large Language Models for Materials Science",
        "authors": [
            "Yu Song",
            "Santiago Miret",
            "Huan Zhang",
            "Bang Liu"
        ],
        "published": "2023",
        "summary": "We propose an instruction-based process for trustworthy data curation in materials science (MatSci-Instruct), which we then apply to finetune a LLaMa-based language model targeted for materials science (HoneyBee). MatSci-Instruct helps alleviate the scarcity of relevant, high-quality materials science textual data available in the open literature, and HoneyBee is the first billion-parameter language model specialized to materials science. In MatSci-Instruct we improve the trustworthiness of generated data by prompting multiple commercially available large language models for generation with an Instructor module (e.g. Chat-GPT) and verification from an independent Verifier module (e.g. Claude). Using MatSci-Instruct, we construct a dataset of multiple tasks and measure the quality of our dataset along multiple dimensions, including accuracy against known facts, relevance to materials science, as well as completeness and reasonableness of the data. Moreover, we iteratively generate more targeted instructions and instruction-data in a finetuning-evaluation-feedback loop leading to progressively better performance for our finetuned HoneyBee models. Our evaluation on the MatSci-NLP benchmark shows HoneyBee’s outperformance of existing language models on materials science tasks and iterative improvement in successive stages of instruction-data refinement. We study the quality of HoneyBee’s language modeling through automatic evaluation and analyze case studies to further understand the model’s capabilities and limitations. Our code and relevant datasets are publicly available at https://github.com/BangLab-UdeM-Mila/NLP4MatSci-HoneyBee.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.380.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We study the quality of HoneyBee’s language modeling through automatic evaluation and analyze case studies to further understand the model’s capabilities and limitations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We study the quality of HoneyBee’s language modeling through automatic evaluation and analyze case studies to further understand the model’s capabilities and limitations.\""
    },
    {
        "title": "Prompt-Based Editing for Text Style Transfer",
        "authors": [
            "Guoqing Luo",
            "Yu Han",
            "Lili Mou",
            "Mauajama Firdaus"
        ],
        "published": "2023",
        "summary": "Prompting approaches have been recently explored in text style transfer, where a textual prompt is used to query a pretrained language model (PLM) to generate style-transferred texts word by word in an autoregressive manner. However, such a generation process is less controllable and early prediction errors may affect future word predictions. In this paper, we propose a prompt-based editing approach to text style transfer. Specifically, we prompt a PLM for style classification and use the classification probability to compute a style score. Then, we perform discrete search with word-level editing to maximize a comprehensive scoring function for the style-transfer task. In this way, we transform a prompt-based generation problem into a classification one, which does not suffer from the error accumulation problem and is more controllable than the autoregressive generation of sentences. In our experiments, we performed both automatic and human evaluation on three style-transfer benchmark datasets, and show that our approach largely outperforms the existing systems that have 20 times more parameters. Additional empirical analyses further demonstrate the effectiveness of our approach.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.381.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, such a generation process is less controllable and early prediction errors may affect future word predictions.\"\n\nThis abstract mentions a limitation of LLMs (less controllability and error accumulation problem) but does not explore it in depth.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, such a generation process is less controllable and early prediction errors may affect future word predictions.\"\n\nThis abstract mentions a limitation of LLMs (less controllability and error accumulation problem) but does not explore it in depth."
    },
    {
        "title": "Representativeness as a Forgotten Lesson for Multilingual and Code-switched Data Collection and Preparation",
        "authors": [
            "A. Seza Doğruöz",
            "Sunayana Sitaram",
            "Zheng Xin Yong"
        ],
        "published": "2023",
        "summary": "Multilingualism is widespread around the world and code-switching (CSW) is a common practice among different language pairs/tuples across locations and regions. However, there is still not much progress in building successful CSW systems, despite the recent advances in Massive Multilingual Language Models (MMLMs). We investigate the reasons behind this setback through a critical study about the existing CSW data sets (68) across language pairs in terms of the collection and preparation (e.g. transcription and annotation) stages. This in-depth analysis reveals that a) most CSW data involves English ignoring other language pairs/tuples b) there are flaws in terms of representativeness in data collection and preparation stages due to ignoring the location based, socio-demographic and register variation in CSW. In addition, lack of clarity on the data selection and filtering stages shadow the representativeness of CSW data sets. We conclude by providing a short check-list to improve the representativeness for forthcoming studies involving CSW data collection and preparation.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.382.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"despite the recent advances in Massive Multilingual Language Models (MMLMs)\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"despite the recent advances in Massive Multilingual Language Models (MMLMs)\""
    },
    {
        "title": "Towards a Unified Framework for Reference Retrieval and Related Work Generation",
        "authors": [
            "Zhengliang Shi",
            "Shen Gao",
            "Zhen Zhang",
            "Xiuying Chen",
            "Zhumin Chen",
            "Pengjie Ren",
            "Zhaochun Ren"
        ],
        "published": "2023",
        "summary": "The task of related work generation aims to generate a comprehensive survey of related research topics automatically, saving time and effort for authors. Existing methods simplify this task by using human-annotated references in a large-scale scientific corpus as information sources, which is time- and cost-intensive. To this end, we propose a Unified Reference Retrieval and Related Work Generation Model (UR3WG), which combines reference retrieval and related work generation processes in a unified framework based on the large language model (LLM). Specifically, UR3WG first leverages the world knowledge of LLM to extend the abstract and generate the query for the subsequent retrieval stage. Then a lexicon-enhanced dense retrieval is proposed to search relevant references, where an importance-aware representation of the lexicon is introduced. We also propose multi-granularity contrastive learning to optimize our retriever. Since this task is not simply summarizing the main points in references, it should analyze the complex relationships and present them logically. We propose an instruction-tuning method to leverage LLM to generate related work. Extensive experiments on two wide-applied datasets demonstrate that our model outperforms the state-of-the-art baselines in both generation and retrieval metrics.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.385.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned in the abstract, but the paper leverages the \"world knowledge of LLM\" and proposes methods to optimize and fine-tune the LLM, implying that the authors are aware of the model's limitations and are trying to address them, but without explicitly stating them.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned in the abstract, but the paper leverages the \"world knowledge of LLM\" and proposes methods to optimize and fine-tune the LLM, implying that the authors are aware of the model's limitations and are trying to address them, but without explicitly stating them."
    },
    {
        "title": "Visual Storytelling with Question-Answer Plans",
        "authors": [
            "Danyang Liu",
            "Mirella Lapata",
            "Frank Keller"
        ],
        "published": "2023",
        "summary": "Visual storytelling aims to generate compelling narratives from image sequences. Existing models often focus on enhancing the representation of the image sequence, e.g., with external knowledge sources or advanced graph structures. Despite recent progress, the stories are often repetitive, illogical, and lacking in detail. To mitigate these issues, we present a novel framework which integrates visual representations with pretrained language models and planning. Our model translates the image sequence into a visual prefix, a sequence of continuous embeddings which language models can interpret. It also leverages a sequence of question-answer pairs as a blueprint plan for selecting salient visual concepts and determining how they should be assembled into a narrative. Automatic and human evaluation on the VIST benchmark demonstrates that blueprint-based models generate stories that are more coherent, interesting, and natural compared to competitive baselines and state-of-the-art systems.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.386.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite recent progress, the stories are often repetitive, illogical, and lacking in detail.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Despite recent progress, the stories are often repetitive, illogical, and lacking in detail.\""
    },
    {
        "title": "ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models",
        "authors": [
            "Alex Mei",
            "Sharon Levy",
            "William Wang"
        ],
        "published": "2023",
        "summary": "As large language models are integrated into society, robustness toward a suite of prompts is increasingly important to maintain reliability in a high-variance environment.Robustness evaluations must comprehensively encapsulate the various settings in which a user may invoke an intelligent system. This paper proposes ASSERT, Automated Safety Scenario Red Teaming, consisting of three methods – semantically aligned augmentation, target bootstrapping, and adversarial knowledge injection. For robust safety evaluation, we apply these methods in the critical domain of AI safety to algorithmically generate a test suite of prompts covering diverse robustness settings – semantic equivalence, related scenarios, and adversarial. We partition our prompts into four safety domains for a fine-grained analysis of how the domain affects model performance. Despite dedicated safeguards in existing state-of-the-art models, we find statistically significant performance differences of up to 11% in absolute classification accuracy among semantically related scenarios and error rates of up to 19% absolute error in zero-shot adversarial settings, raising concerns for users’ physical safety.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.388.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite dedicated safeguards in existing state-of-the-art models, we find statistically significant performance differences of up to 11% in absolute classification accuracy among semantically related scenarios and error rates of up to 19% absolute error in zero-shot adversarial settings, raising concerns for users’ physical safety.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Despite dedicated safeguards in existing state-of-the-art models, we find statistically significant performance differences of up to 11% in absolute classification accuracy among semantically related scenarios and error rates of up to 19% absolute error in zero-shot adversarial settings, raising concerns for users’ physical safety.\""
    },
    {
        "title": "Co2PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning",
        "authors": [
            "Xiangjue Dong",
            "Ziwei Zhu",
            "Zhuoer Wang",
            "Maria Teleki",
            "James Caverlee"
        ],
        "published": "2023",
        "summary": "Pre-trained Language Models are widely used in many important real-world applications. However, recent studies show that these models can encode social biases from large pre-training corpora and even amplify biases in downstream applications. To address this challenge, we propose Co2PT, an efficient and effective *debias-while-prompt tuning* method for mitigating biases via counterfactual contrastive prompt tuning on downstream tasks. Our experiments conducted on three extrinsic bias benchmarks demonstrate the effectiveness of Co2PT on bias mitigation during the prompt tuning process and its adaptability to existing upstream debiased language models. These findings indicate the strength of Co2PT and provide promising avenues for further enhancement in bias mitigation on downstream tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.390.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, recent studies show that these models can encode social biases from large pre-training corpora and even amplify biases in downstream applications.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, recent studies show that these models can encode social biases from large pre-training corpora and even amplify biases in downstream applications.\""
    },
    {
        "title": "A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization",
        "authors": [
            "Chenhui Shen",
            "Liying Cheng",
            "Xuan-Phi Nguyen",
            "Yang You",
            "Lidong Bing"
        ],
        "published": "2023",
        "summary": "Pre-trained language models (PLMs) have achieved outstanding achievements in abstractive single-document summarization (SDS). However, such benefits may not fully extend to multi-document summarization (MDS), where the handling of cross-document information is more complex. Previous works either design new MDS architectures or apply PLMs bluntly with concatenated source documents as a reformulated SDS task. While the former does not utilize previous pre-training efforts and may not generalize well across different domains, the latter may not sufficiently attend to the intricate cross-document relationships unique to MDS tasks. Instead, we enforce hierarchy on both the encoder and decoder to better utilize a PLM to facilitate multi-document interactions for the MDS task. Across 10 MDS benchmarks from various domains, our method outperforms or is competitive with the previous best models, including those with additional MDS pre-training or with more parameters. It outperforms its corresponding PLM backbone by up to 3 Rouge-L and is favored by humans.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.391.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, such benefits may not fully extend to multi-document summarization (MDS), where the handling of cross-document information is more complex... the latter may not sufficiently attend to the intricate cross-document relationships unique to MDS tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, such benefits may not fully extend to multi-document summarization (MDS), where the handling of cross-document information is more complex... the latter may not sufficiently attend to the intricate cross-document relationships unique to MDS tasks.\""
    },
    {
        "title": "Aligning Language Models to User Opinions",
        "authors": [
            "EunJeong Hwang",
            "Bodhisattwa Majumder",
            "Niket Tandon"
        ],
        "published": "2023",
        "summary": "An important aspect of developing LLMs that interact with humans is to align models’ behavior to their users. It is possible to prompt an LLM into behaving as a certain persona, especially a user group or ideological persona the model captured during its pertaining stage. But, how to best align an LLM with a specific user and not a demographic or ideological group remains an open question. Mining public opinion surveys (by PEW research), we find that the opinions of a user and their demographics and ideologies are not mutual predictors. We use this insight to align LLMs by modeling relevant past user opinions in addition to user demographics and ideology, achieving up to 7 points accuracy gains in predicting public opinions from survey questions across a broad set of topics. Our work opens up the research avenues to bring user opinions as an important ingredient in aligning language models.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.393.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"But, how to best align an LLM with a specific user and not a demographic or ideological group remains an open question.\"\n\nThis rating is given because the paper mentions a limitation of LLMs in passing, but it is not the primary focus of the paper. The limitation is used to justify the paper's goal of aligning LLMs with user opinions, but it is",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"But, how to best align an LLM with a specific user and not a demographic or ideological group remains an open question.\"\n\nThis rating is given because the paper mentions a limitation of LLMs in passing, but it is not the primary focus of the paper. The limitation is used to justify the paper's goal of aligning LLMs with user opinions, but it is"
    },
    {
        "title": "Miracle: Towards Personalized Dialogue Generation with Latent-Space Multiple Personal Attribute Control",
        "authors": [
            "Zhenyi Lu",
            "Wei Wei",
            "Xiaoye Qu",
            "Xian-Ling Mao",
            "Dangyang Chen",
            "Jixiong Chen"
        ],
        "published": "2023",
        "summary": "Personalized dialogue systems aim to endow the chatbot agent with more anthropomorphic traits for human-like interactions. Previous approaches have explored explicitly user profile modeling using text descriptions, implicit derivation of user embeddings, or utilizing handicraft prompts for ChatGPT-like models. However, textual personas are limited in describing multi-faceted attributes (e.g., language style, inner character nuances), implicit embedding suffers from personality sparsity, and handicraft prompts lack fine-grained and stable controllability. Hence, these approaches may struggle with complex personalized dialogue generation tasks that require generating controllable responses with multiple personal attributes. To this end, we propose Miracle, a novel personalized dialogue generation method through MultIple PeRsonal Attributes Control within Latent-Space Energy-based Models. ttributes Control within Latent-Space Energy-based Models. Specifically, our approach first disentangles complex personality into multi-faceted attributes. Subsequently, we employ a conditional variational auto-encoder to align with the dense personalized responses within a latent joint attribute space. We have also tailored a dedicated energy function and customized the ordinary differential equations sampling method to offer flexible attribute composition and precise attribute control. Extensive experiments demonstrate that Miracle outperforms several strong baselines in terms of personality controllability and response generation quality. Our dataset and code are available at https://github.com/LZY-the-boys/MIRACLE",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.395.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"textual personas are limited in describing multi-faceted attributes (e.g., language style, inner character nuances), implicit embedding suffers from personality sparsity, and handicraft prompts lack fine-grained and stable controllability.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"textual personas are limited in describing multi-faceted attributes (e.g., language style, inner character nuances), implicit embedding suffers from personality sparsity, and handicraft prompts lack fine-grained and stable controllability.\""
    },
    {
        "title": "Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting",
        "authors": [
            "Fanghua Ye",
            "Meng Fang",
            "Shenghui Li",
            "Emine Yilmaz"
        ],
        "published": "2023",
        "summary": "Query rewriting plays a vital role in enhancing conversational search by transforming context-dependent user queries into standalone forms. Existing approaches primarily leverage human-rewritten queries as labels to train query rewriting models. However, human rewrites may lack sufficient information for optimal retrieval performance. To overcome this limitation, we propose utilizing large language models (LLMs) as query rewriters, enabling the generation of informative query rewrites through well-designed instructions. We define four essential properties for well-formed rewrites and incorporate all of them into the instruction. In addition, we introduce the role of rewrite editors for LLMs when initial query rewrites are available, forming a “rewrite-then-edit” process. Furthermore, we propose distilling the rewriting capabilities of LLMs into smaller models to reduce rewriting latency. Our experimental evaluation on the QReCC dataset demonstrates that informative query rewrites can yield substantially improved retrieval performance compared to human rewrites, especially with sparse retrievers.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.398.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, human rewrites may lack sufficient information for optimal retrieval performance.\"\n\nNote that the limitation mentioned is not specific to LLMs, but rather to human rewrites, which is used as a baseline in the paper. The paper does discuss the capabilities of LLMs in generating informative query rewrites, but does not explicitly mention any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, human rewrites may lack sufficient information for optimal retrieval performance.\"\n\nNote that the limitation mentioned is not specific to LLMs, but rather to human rewrites, which is used as a baseline in the paper. The paper does discuss the capabilities of LLMs in generating informative query rewrites, but does not explicitly mention any limitations of LLMs."
    },
    {
        "title": "Distilling ChatGPT for Explainable Automated Student Answer Assessment",
        "authors": [
            "Jiazheng Li",
            "Lin Gui",
            "Yuxiang Zhou",
            "David West",
            "Cesare Aloisi",
            "Yulan He"
        ],
        "published": "2023",
        "summary": "Providing explainable and faithful feedback is crucial for automated student answer assessment. In this paper, we introduce a novel framework that explores using ChatGPT, a cutting-edge large language model, for the concurrent tasks of student answer scoring and rationale generation. We identify the appropriate instructions by prompting ChatGPT with different templates to collect the rationales, where inconsistent rationales are refined to align with marking standards. The refined ChatGPT outputs enable us to fine-tune a smaller language model that simultaneously assesses student answers and provides rationales. Extensive experiments on the benchmark dataset show that the proposed method improves the overall QWK score by 11% compared to ChatGPT. Furthermore, our thorough analysis and human evaluation demonstrate that the rationales generated by our proposed method are comparable to those of ChatGPT. Our approach provides a viable solution to achieve explainable automated assessment in education",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.399.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper assumes LLMs can be used for a specific task, implying that the limitations of LLMs are not the focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper assumes LLMs can be used for a specific task, implying that the limitations of LLMs are not the focus of the paper."
    },
    {
        "title": "Domain Private Transformers for Multi-Domain Dialog Systems",
        "authors": [
            "Anmol Kabra",
            "Ethan Elenberg"
        ],
        "published": "2023",
        "summary": "Large, general purpose language models have demonstrated impressive performance across many different conversational domains. While multi-domain language models achieve low overall perplexity, their outputs are not guaranteed to stay within the domain of a given input prompt. This paper proposes domain privacy as a novel way to quantify how likely a conditional language model will leak across domains. We also develop policy functions based on token-level domain classification, and propose an efficient fine-tuning method to improve the trained model’s domain privacy. Experiments on membership inference attacks show that our proposed method has comparable resiliency to methods adapted from recent literature on differentially private language models.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.402.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While multi-domain language models achieve low overall perplexity, their outputs are not guaranteed to stay within the domain of a given input prompt.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While multi-domain language models achieve low overall perplexity, their outputs are not guaranteed to stay within the domain of a given input prompt.\""
    },
    {
        "title": "Visual Elements Mining as Prompts for Instruction Learning for Target-Oriented Multimodal Sentiment Classification",
        "authors": [
            "Bin Yang",
            "Jinlong Li"
        ],
        "published": "2023",
        "summary": "Target-oriented Multimodal Sentiment Classification (TMSC) aims to incorporate visual modality with text modality to identify the sentiment polarity towards a specific target within a sentence. To address this task, we propose a Visual Elements Mining as Prompts (VEMP) method, which describes the semantic information of visual elements with Text Symbols Embedded in the Image (TSEI), Target-aware Adjective-Noun Pairs (TANPs) and image scene caption, and then transform them into prompts for instruction learning of the model Tk-Instruct. In our VEMP, the text symbols embedded in the image may contain the textual descriptions of fine-grained visual elements, and are extracted as input TSEI; we extract adjective-noun pairs from the image and align them with the target to obtain TANPs, in which the adjectives provide emotional embellishments for the relevant target; finally, to effectively fuse these visual elements with text modality for sentiment prediction, we integrate them to construct instruction prompts for instruction-tuning Tk-Instruct which possesses powerful learning capabilities under instructions. Extensive experimental results show that our method achieves state-of-the-art performance on two benchmark datasets. And further analysis demonstrates the effectiveness of each component of our method.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.403.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but \"instruction learning of the model Tk-Instruct\" implies the need for additional guidance or training for the model, which might be seen as a minor limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations, but \"instruction learning of the model Tk-Instruct\" implies the need for additional guidance or training for the model, which might be seen as a minor limitation."
    },
    {
        "title": "NASH: A Simple Unified Framework of Structured Pruning for Accelerating Encoder-Decoder Language Models",
        "authors": [
            "Jongwoo Ko",
            "Seungjoon Park",
            "Yujin Kim",
            "Sumyeong Ahn",
            "Du-Seong Chang",
            "Euijai Ahn",
            "Se-Young Yun"
        ],
        "published": "2023",
        "summary": "Structured pruning methods have proven effective in reducing the model size and accelerating inference speed in various network architectures such as Transformers. Despite the versatility of encoder-decoder models in numerous NLP tasks, the structured pruning methods on such models are relatively less explored compared to encoder-only models. In this study, we investigate the behavior of the structured pruning of the encoder-decoder models in the decoupled pruning perspective of the encoder and decoder component, respectively. Our findings highlight two insights: (1) the number of decoder layers is the dominant factor of inference speed, and (2) low sparsity in the pruned encoder network enhances generation quality. Motivated by these findings, we propose a simple and effective framework, NASH, that narrows the encoder and shortens the decoder networks of encoder-decoder models. Extensive experiments on diverse generation and inference tasks validate the effectiveness of our method in both speedup and output quality.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.404.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but the motivation for the study is that \"structured pruning methods on [encoder-decoder models] are relatively less explored compared to encoder-only models\", implying a limitation in the current state of pruning methods for encoder-decoder models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations, but the motivation for the study is that \"structured pruning methods on [encoder-decoder models] are relatively less explored compared to encoder-only models\", implying a limitation in the current state of pruning methods for encoder-decoder models."
    },
    {
        "title": "GBT: Generative Boosting Training Approach for Paraphrase Identification",
        "authors": [
            "Rui Peng",
            "Zhiling Jin",
            "Yu Hong"
        ],
        "published": "2023",
        "summary": "Paraphrase Identification (PI), a task of determining whether a pair of sentences express the same meaning, is widely applied in Information Retrieval and Question Answering. Data Augmentation (DA) is proven effective in tackling the PI task. However, the majority of DA methods still suffer from two limitations: inefficiency and poor quality. In this study, we propose the Generative Boosting Training (GBT) approach for PI. GBT designs a boosting learning method for a single model based on the human learning process, utilizing seq2seq model to perform DA on misclassified instances periodically. We conduct experiments on the benchmark corpora QQP and LCQMC, towards both English and Chinese PI tasks. Experimental results show that our method yields significant improvements on a variety of Pre-trained Language Model (PLM) based baselines with good efficiency and effectiveness. It is noteworthy that a single BERT model (with a linear classifier) can outperform the state-of-the-art PI models with the boosting of GBT.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.405.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the majority of DA methods still suffer from two limitations: inefficiency and poor quality.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the majority of DA methods still suffer from two limitations: inefficiency and poor quality.\""
    },
    {
        "title": "Probing LLMs for hate speech detection: strengths and vulnerabilities",
        "authors": [
            "Sarthak Roy",
            "Ashish Harshvardhan",
            "Animesh Mukherjee",
            "Punyajoy Saha"
        ],
        "published": "2023",
        "summary": "Recently efforts have been made by social media platforms as well as researchers to detect hateful or toxic language using large language models. However, none of these works aim to use explanation, additional context and victim community information in the detection process. We utilise different prompt variation, input information and evaluate large language models in zero shot setting (without adding any in-context examples). We select two large language models (GPT-3.5 and text-davinci) and three datasets - HateXplain, implicit hate and ToxicSpans. We find that on average including the target information in the pipeline improves the model performance substantially (∼20-30%) over the baseline across the datasets. There is also a considerable effect of adding the rationales/explanations into the pipeline (∼10-20%) over the baseline across the datasets. In addition, we further provide a typology of the error cases where these large language models fail to (i) classify and (ii) explain the reason for the decisions they take. Such vulnerable points automatically constitute ‘jailbreak’ prompts for these models and industry scale safeguard techniques need to be developed to make the models robust against such prompts.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.407.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Such vulnerable points automatically constitute ‘jailbreak’ prompts for these models and industry scale safeguard techniques need to be developed to make the models robust against such prompts.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Such vulnerable points automatically constitute ‘jailbreak’ prompts for these models and industry scale safeguard techniques need to be developed to make the models robust against such prompts.\""
    },
    {
        "title": "Finding Support Examples for In-Context Learning",
        "authors": [
            "Xiaonan Li",
            "Xipeng Qiu"
        ],
        "published": "2023",
        "summary": "In-context learning is a new learning paradigm where a language model observes a few examples and directly outputs the test input’s prediction. Previous works have shown that it is sensitive to the provided examples and randomly sampled examples probably cause inferior performance. In this paper, we propose finding “support examples” for in-context learning: Given a training dataset, it aims to select one permutation of a few examples, which can well characterize the task for in-context learning and thus lead to superior performance. Although for traditional gradient-based training, there are extensive methods to find a coreset from the entire dataset, they struggle to find important in-context examples, because in-context learning occurs in the language model’s forward process without gradients or parameter updates and thus has a significant gap with traditional training. Additionally, the strong dependence among in-context examples makes it an NP-hard combinatorial optimization problem and enumerating all permutations is infeasible. Hence we propose **LENS**, a fi**L**ter-th**EN**-**S**earch method to tackle this challenge in two stages: irst we filter the dataset to obtain individually informative in-context examples. Specifically, we propose a novel metric, InfoScore, to evaluate the example’s in-context informativeness based on the language model’s feedback, and further propose a progressive filtering process to filter out uninformative examples. Then we propose diversity-guided example search which iteratively refines and evaluates the selected example permutations, to find examples that fully depict the task. The experimental results show that LENS significantly outperforms a wide range of baselines and further analyses show that each component contribute critically to the improvements and shed light on the principles of supporting examples and in-context learning.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.411.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although for traditional gradient-based training, there are extensive methods to find a coreset from the entire dataset, they struggle to find important in-context examples, because in-context learning occurs in the language model’s forward process without gradients or parameter updates and thus has a significant gap with traditional training.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although for traditional gradient-based training, there are extensive methods to find a coreset from the entire dataset, they struggle to find important in-context examples, because in-context learning occurs in the language model’s forward process without gradients or parameter updates and thus has a significant gap with traditional training.\""
    },
    {
        "title": "Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models",
        "authors": [
            "Haoran Wang",
            "Kai Shu"
        ],
        "published": "2023",
        "summary": "Claim verification plays a crucial role in combating misinformation. While existing works on claim verification have shown promising results, a crucial piece of the puzzle that remains unsolved is to understand how to verify claims without relying on human-annotated data, which is expensive to create at a large scale. Additionally, it is important for models to provide comprehensive explanations that can justify their decisions and assist human fact-checkers. This paper presents First-Order-Logic-Guided Knowledge-Grounded (FOLK) Reasoning that can verify complex claims and generate explanations without the need for annotated evidence using Large Language Models (LLMs). FOLK leverages the in-context learning ability of LLMs to translate the claim into a First-Order-Logic (FOL) clause consisting of predicates, each corresponding to a sub-claim that needs to be verified. Then, FOLK performs FOL-Guided reasoning over a set of knowledge-grounded question-and-answer pairs to make veracity predictions and generate explanations to justify its decision-making process. This process makes our model highly explanatory, providing clear explanations of its reasoning process in human-readable form. Our experiment results indicate that FOLK outperforms strong baselines on three datasets encompassing various claim verification challenges. Our code and data are available.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.416.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation of LLMs is mentioned, but the paper aims to address the challenge of verifying claims without relying on human-annotated data, which can be seen as a limitation of existing approaches using LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation of LLMs is mentioned, but the paper aims to address the challenge of verifying claims without relying on human-annotated data, which can be seen as a limitation of existing approaches using LLMs."
    },
    {
        "title": "Strong and Efficient Baselines for Open Domain Conversational Question Answering",
        "authors": [
            "Andrei Coman",
            "Gianni Barlacchi",
            "Adrià de Gispert"
        ],
        "published": "2023",
        "summary": "Unlike the Open Domain Question Answering (ODQA) setting, the conversational (ODConvQA) domain has received limited attention when it comes to reevaluating baselines for both efficiency and effectiveness. In this paper, we study the State-of-the-Art (SotA) Dense Passage Retrieval (DPR) retriever and Fusion-in-Decoder (FiD) reader pipeline, and show that it significantly underperforms when applied to ODConvQA tasks due to various limitations. We then propose and evaluate strong yet simple and efficient baselines, by introducing a fast reranking component between the retriever and the reader, and by performing targeted finetuning steps. Experiments on two ODConvQA tasks, namely TopiOCQA and OR-QuAC, show that our method improves the SotA results, while reducing reader’s latency by 60%. Finally, we provide new and valuable insights into the development of challenging baselines that serve as a reference for future, more intricate approaches, including those that leverage Large Language Models (LLMs).",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.417.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"while reducing reader’s latency by 60%. Finally, we provide new and valuable insights into the development of challenging baselines that serve as a reference for future, more intricate approaches, including those that leverage Large Language Models (LLMs)\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"while reducing reader’s latency by 60%. Finally, we provide new and valuable insights into the development of challenging baselines that serve as a reference for future, more intricate approaches, including those that leverage Large Language Models (LLMs)\""
    },
    {
        "title": "Efficient Continue Training of Temporal Language Model with Structural Information",
        "authors": [
            "Zhaochen Su",
            "Juntao Li",
            "Zikang Zhang",
            "Zihan Zhou",
            "Min Zhang"
        ],
        "published": "2023",
        "summary": "Current language models are mainly trained on snap-shots of data gathered at a particular time, which decreases their capability to generalize over time and model language change. To model the time variable, existing works have explored temporal language models (e.g., TempoBERT) by directly incorporating the timestamp into the training process. While effective to some extent, these methods are limited by the superficial temporal information brought by timestamps, which fails to learn the inherent changes of linguistic components. In this paper, we empirically confirm that the performance of pre-trained language models (PLMs) is closely affiliated with syntactically changed tokens. Based on this observation, we propose a simple yet effective method named Syntax-Guided Temporal Language Model (SG-TLM), which could learn the inherent language changes by capturing an intrinsic relationship between the time prefix and the tokens with salient syntactic change. Experiments on two datasets and three tasks demonstrate that our model outperforms existing temporal language models in both memorization and generalization capabilities. Extensive results further confirm the effectiveness of our approach across different model frameworks, including both encoder-only and decoder-only models (e.g., LLaMA). Our code is available at https://github.com/zhaochen0110/TempoLM.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.418.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Current language models are mainly trained on snap-shots of data gathered at a particular time, which decreases their capability to generalize over time and model language change.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Current language models are mainly trained on snap-shots of data gathered at a particular time, which decreases their capability to generalize over time and model language change.\""
    },
    {
        "title": "Retrieval-Augmented Parsing for Complex Graphs by Exploiting Structure and Uncertainty",
        "authors": [
            "Zi Lin",
            "Quan Yuan",
            "Panupong Pasupat",
            "Jeremiah Liu",
            "Jingbo Shang"
        ],
        "published": "2023",
        "summary": "Retrieval augmentation enhances generative language models by retrieving informative exemplars relevant for output prediction. However, in realistic graph parsing problems where the output space is large and complex, classic retrieval methods based on input-sentence similarity can fail to identify the most informative exemplars that target graph elements the model is most struggling about, leading to suboptimal retrieval and compromised prediction under limited retrieval budget. In this work, we improve retrieval-augmented parsing for complex graph problems by exploiting two unique sources of information (1) structural similarity and (2) model uncertainty. We propose Structure-aware and Uncertainty-Guided Adaptive Retrieval (SUGAR) that first quantify the model uncertainty in graph prediction and identify its most uncertain subgraphs, and then retrieve exemplars based on their structural similarity with the identified uncertain subgraphs. On a suite of real-world parsing benchmarks with non-trivial graph structure (SMCalflow and E-commerce), SUGAR exhibits a strong advantage over its classic counterparts that do not leverage structure or model uncertainty.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.419.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, in realistic graph parsing problems where the output space is large and complex, classic retrieval methods based on input-sentence similarity can fail to identify the most informative exemplars that target graph elements the model is most struggling about, leading to suboptimal retrieval and compromised prediction under limited retrieval budget.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, in realistic graph parsing problems where the output space is large and complex, classic retrieval methods based on input-sentence similarity can fail to identify the most informative exemplars that target graph elements the model is most struggling about, leading to suboptimal retrieval and compromised prediction under limited retrieval budget.\""
    },
    {
        "title": "Schema-adaptable Knowledge Graph Construction",
        "authors": [
            "Hongbin Ye",
            "Honghao Gui",
            "Xin Xu",
            "Xi Chen",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "published": "2023",
        "summary": "Conventional Knowledge Graph Construction (KGC) approaches typically follow the static information extraction paradigm with a closed set of pre-defined schema. As a result, such approaches fall short when applied to dynamic scenarios or domains, whereas a new type of knowledge emerges. This necessitates a system that can handle evolving schema automatically to extract information for KGC. To address this need, we propose a new task called schema-adaptable KGC, which aims to continually extract entity, relation, and event based on a dynamically changing schema graph without re-training. We first split and convert existing datasets based on three principles to build a benchmark, i.e., horizontal schema expansion, vertical schema expansion, and hybrid schema expansion; then investigate the schema-adaptable performance of several well-known approaches such as Text2Event, TANL, UIE and GPT-3.5. We further propose a simple yet effective baseline dubbed AdaKGC, which contains schema-enriched prefix instructor and schema-conditioned dynamic decoding to better handle evolving schema. Comprehensive experimental results illustrate that AdaKGC can outperform baselines but still have room for improvement. We hope the proposed work can deliver benefits to the community.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.425.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Comprehensive experimental results illustrate that AdaKGC can outperform baselines but still have room for improvement.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Comprehensive experimental results illustrate that AdaKGC can outperform baselines but still have room for improvement.\""
    },
    {
        "title": "Evaluating the Knowledge Base Completion Potential of GPT",
        "authors": [
            "Blerta Veseli",
            "Simon Razniewski",
            "Jan-Christoph Kalo",
            "Gerhard Weikum"
        ],
        "published": "2023",
        "summary": "Structured knowledge bases (KBs) are an asset for search engines and other applications but are inevitably incomplete. Language models (LMs) have been proposed for unsupervised knowledge base completion (KBC), yet, their ability to do this at scale and with high accuracy remains an open question. Prior experimental studies mostly fall short because they only evaluate on popular subjects, or sample already existing facts from KBs. In this work, we perform a careful evaluation of GPT’s potential to complete the largest public KB: Wikidata. We find that, despite their size and capabilities, models like GPT-3, ChatGPT and GPT-4 do not achieve fully convincing results on this task. Nonetheless, it provides solid improvements over earlier approaches with smaller LMs. In particular, we show that it is feasible to extend Wikidata by 27M facts at 90% precision.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.426.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that, despite their size and capabilities, models like GPT-3, ChatGPT and GPT-4 do not achieve fully convincing results on this task.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We find that, despite their size and capabilities, models like GPT-3, ChatGPT and GPT-4 do not achieve fully convincing results on this task.\""
    },
    {
        "title": "Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset",
        "authors": [
            "Haoyi Wu",
            "Wenyang Hui",
            "Yezeng Chen",
            "Weiqi Wu",
            "Kewei Tu",
            "Yi Zhou"
        ],
        "published": "2023",
        "summary": "Mathematical understanding and reasoning are crucial tasks for assessing the capabilities of artificial intelligence (AI). However, existing benchmarks either require just a few steps of reasoning, or only contain a small amount of data in one specific topic, making it hard to analyse AI’s behaviour with reference to different problems within a specific topic in detail. In this work, we propose Conic10K, a challenging math problem dataset on conic sections in Chinese senior high school education. Our dataset contains various problems with different reasoning depths, while only the knowledge from conic sections is required. Since the dataset only involves a narrow range of knowledge, it is easy to separately analyse the knowledge a model possesses and the reasoning ability it has. For each problem, we provide a high-quality formal representation, the reasoning steps, and the final solution. Experiments show that existing large language models, including GPT-4, exhibit weak performance on complex reasoning. We hope that our findings could inspire more advanced techniques for precise natural language understanding and reasoning. Our dataset and codes are available at https://github.com/whyNLP/Conic10K.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.427.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experiments show that existing large language models, including GPT-4, exhibit weak performance on complex reasoning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Experiments show that existing large language models, including GPT-4, exhibit weak performance on complex reasoning.\""
    },
    {
        "title": "Inductive Relation Inference of Knowledge Graph Enhanced by Ontology Information",
        "authors": [
            "Wentao Zhou",
            "Jun Zhao",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2023",
        "summary": "The inductive inference of the knowledge graph aims to complete the potential relations between the new unknown entities in the graph. Most existing methods are based on entity-independent features such as graph structure information and relationship information to inference. However, the neighborhood of these new entities is often too sparse to obtain enough information to build these features effectively. In this work, we propose a knowledge graph inductive inference method that fuses ontology information. Based on the enclosing subgraph, we bring in feature embeddings of concepts corresponding to entities to learn the semantic information implicit in the ontology. Considering that the ontology information of entities may be missing, we build a type constraint regular loss to explicitly model the semantic connections between entities and concepts, and thus capture the missing concepts of entities. Experimental results show that our approach significantly outperforms large language models like ChatGPT on two benchmark datasets, YAGO21K-610 and DB45K-165, and improves the MRR metrics by 15.4% and 44.1%, respectively, when compared with the state-of-the-art methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.431.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experimental results show that our approach significantly outperforms large language models like ChatGPT on two benchmark datasets, YAGO21K-610 and DB45K-165, and improves the MRR metrics by 15.4% and 44.1%, respectively, when compared with the state-of-the-art methods.\"\n\nThis rating is based on the fact that the abstract mentions",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Experimental results show that our approach significantly outperforms large language models like ChatGPT on two benchmark datasets, YAGO21K-610 and DB45K-165, and improves the MRR metrics by 15.4% and 44.1%, respectively, when compared with the state-of-the-art methods.\"\n\nThis rating is based on the fact that the abstract mentions"
    },
    {
        "title": "Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements",
        "authors": [
            "Yushan Qian",
            "Weinan Zhang",
            "Ting Liu"
        ],
        "published": "2023",
        "summary": "Empathetic dialogue is an indispensable part of building harmonious social relationships and contributes to the development of a helpful AI. Previous approaches are mainly based on fine small-scale language models. With the advent of ChatGPT, the application effect of large language models (LLMs) in this field has attracted great attention. This work empirically investigates the performance of LLMs in generating empathetic responses and proposes three improvement methods of semantically similar in-context learning, two-stage interactive generation, and combination with the knowledge base. Extensive experiments show that LLMs can significantly benefit from our proposed methods and is able to achieve state-of-the-art performance in both automatic and human evaluations. Additionally, we explore the possibility of GPT-4 simulating human evaluators.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.433.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but the need for improvement methods (\"three improvement methods\") implies the existence of limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None explicitly mentioned, but the need for improvement methods (\"three improvement methods\") implies the existence of limitations."
    },
    {
        "title": "GPT Deciphering Fedspeak: Quantifying Dissent Among Hawks and Doves",
        "authors": [
            "Denis Peskoff",
            "Adam Visokay",
            "Sander Schulhoff",
            "Benjamin Wachspress",
            "Alan Blinder",
            "Brandon Stewart"
        ],
        "published": "2023",
        "summary": "Markets and policymakers around the world hang on the consequential monetary policy decisions made by the Federal Open Market Committee (FOMC). Publicly available textual documentation of their meetings provides insight into members’ attitudes about the economy. We use GPT-4 to quantify dissent among members on the topic of inflation. We find that transcripts and minutes reflect the diversity of member views about the macroeconomic outlook in a way that is lost or omitted from the public statements. In fact, diverging opinions that shed light upon the committee’s “true” attitudes are almost entirely omitted from the final statements. Hence, we argue that forecasting FOMC sentiment based solely on statements will not sufficiently reflect dissent among the hawks and doves.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.434.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Hence, we argue that forecasting FOMC sentiment based solely on statements will not sufficiently reflect dissent among the hawks and doves.\"\n\nThis evidence suggests a limitation of LLMs in capturing or reflecting certain aspects of the data, specifically dissent among committee members, but it is not a major focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Hence, we argue that forecasting FOMC sentiment based solely on statements will not sufficiently reflect dissent among the hawks and doves.\"\n\nThis evidence suggests a limitation of LLMs in capturing or reflecting certain aspects of the data, specifically dissent among committee members, but it is not a major focus of the paper."
    },
    {
        "title": "MM-Reasoner: A Multi-Modal Knowledge-Aware Framework for Knowledge-Based Visual Question Answering",
        "authors": [
            "Mahmoud Khademi",
            "Ziyi Yang",
            "Felipe Frujeri",
            "Chenguang Zhu"
        ],
        "published": "2023",
        "summary": "Thanks to the strong reasoning capabilities of Large Language Models (LLMs), recent approaches to knowledge-based visual question answering (KVQA) utilize LLMs with a global caption of an input image to answer a question. However, these approaches may miss key visual information that is not captured by the caption. Moreover, they cannot fully utilize the visual information required to answer the question. To address these issues, we introduce a new framework called Multi-Modal Knowledge-Aware Reasoner (MM-Reasoner) for KVQA. MM-Reasoner first utilizes a set of vision APIs, such as dense captioners, object detectors, and OCR, to extract detailed information from the image in textual format. Then, it prompts an LLM to extract query-specific knowledge from the extracted textual information to provide a rich representation that contains external knowledge, commonsense, explicit supporting facts, and rationales required for reasoning. Finally, the knowledge, query, and visual input are used to fine-tune a Vision-Language Model (VLM). At test time, MM-Reasoner uses the potential answers predicted by the VLM to iteratively update and optimize the prompt, refining its answer. Empirical studies show that MM-Reasoner achieves state-of-the-art performance on several KVQA datasets.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.437.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these approaches may miss key visual information that is not captured by the caption. Moreover, they cannot fully utilize the visual information required to answer the question.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these approaches may miss key visual information that is not captured by the caption. Moreover, they cannot fully utilize the visual information required to answer the question.\""
    },
    {
        "title": "From Chaos to Clarity: Claim Normalization to Empower Fact-Checking",
        "authors": [
            "Megha Sundriyal",
            "Tanmoy Chakraborty",
            "Preslav Nakov"
        ],
        "published": "2023",
        "summary": "With the proliferation of social media platforms, users are exposed to vast information, including posts containing misleading claims. However, the pervasive noise inherent in these posts presents a challenge in identifying precise and prominent claims that require verification. Extracting the core assertions from such posts is arduous and time-consuming. We introduce a novel task, called Claim Normalization (aka ClaimNorm) that aims to decompose complex and noisy social media posts into more straightforward and understandable forms, termed normalized claims. We propose CACN , a pioneering approach that leverages chain-of-thought and claim check-worthiness estimation, mimicking human reasoning processes, to comprehend intricate claims. Moreover, we capitalize on large language models’ powerful in-context learning abilities to provide guidance and improve the claim normalization process. To evaluate the effectiveness of our proposed model, we meticulously compile a comprehensive real-world dataset, CLAN, comprising more than 6k instances of social media posts alongside their respective normalized claims. Experimentation demonstrates that CACN outperforms several baselines across various evaluation measures. A rigorous error analysis validates CACN‘s capabilities and pitfalls. We release our dataset and code at https://github.com/LCS2-IIITD/CACN-EMNLP-2023.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.439.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Moreover, we capitalize on large language models’ powerful in-context learning abilities to provide guidance and improve the claim normalization process.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Moreover, we capitalize on large language models’ powerful in-context learning abilities to provide guidance and improve the claim normalization process.\""
    },
    {
        "title": "InstructoR: Instructing Unsupervised Conversational Dense Retrieval with Large Language Models",
        "authors": [
            "Zhuoran Jin",
            "Pengfei Cao",
            "Yubo Chen",
            "Kang Liu",
            "Jun Zhao"
        ],
        "published": "2023",
        "summary": "Compared to traditional single-turn ad-hoc retrieval, conversational retrieval needs to handle the multi-turn conversation and understand the user’s real query intent. However, most existing methods simply fine-tune the pre-trained ad-hoc retriever on limited supervised data, making it challenging for the retriever to fully grasp the entirety of the conversation. In this paper, we find that large language models (LLMs) can accurately discover the user’s query intent from the complex conversation context and provide the supervised signal to instruct the retriever in an unsupervised manner. Therefore, we propose a novel method termed InstructoR to Instruct unsupervised conversational dense Retrieval with LLMs. We design an unsupervised training framework that employs LLMs to estimate the session-passage relevance score as the soft label to guide the retriever’s training. Specially, we devise three instructing strategies from context, query and response perspectives to calculate the relevance score more precisely, including conversational retrieval as conversation generation, question rewrite as latent variable and question response as posterior guide. Experimental results show InstructoR can bring significant improvements across various ad-hoc retrievers, even surpassing the current supervised state-of-the-art method. We also demonstrate the effectiveness of our method under low-resource and zero-shot settings. Our code is publicly available at https://github.com/jinzhuoran/InstructoR/.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.443.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Language Models",
        "authors": [
            "H S V N S Kowndinya Renduchintala",
            "Krishnateja Killamsetty",
            "Sumit Bhatia",
            "Milan Aggarwal",
            "Ganesh Ramakrishnan",
            "Rishabh Iyer",
            "Balaji Krishnamurthy"
        ],
        "published": "2023",
        "summary": "A salient characteristic of pre-trained language models (PTLMs) is a remarkable improvement in their generalization capability and emergence of new capabilities with increasing model capacity and pre-training dataset size. Consequently, we are witnessing the development of enormous models pushing the state-of-the-art. It is, however, imperative to realize that this inevitably leads to prohibitively long training times, extortionate computing costs, and a detrimental environmental impact. Significant efforts are underway to make PTLM training more efficient through innovations in model architectures, training pipelines, and loss function design, with scant attention being paid to optimizing the utility of training data. The key question that we ask is whether it is possible to train PTLMs by employing only highly informative subsets of the training data while maintaining downstream performance? Building upon the recent progress in informative data subset selection, we show how we can employ submodular optimization to select highly representative subsets of the training corpora and demonstrate that the proposed framework can be applied to efficiently train multiple PTLMs (BERT, BioBERT, GPT-2) using only a fraction of data. Further, we perform a rigorous empirical evaluation to show that the resulting models achieve up to ~99% of the performance of the fully-trained models. We made our framework publicly available at https://github.com/Efficient-AI/ingenious.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.445.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"it inevitably leads to prohibitively long training times, extortionate computing costs, and a detrimental environmental impact.\"\n\nThis paper mentions a limitation of large pre-trained language models (PTLMs), which is related to their training efficiency, environmental impact, and computational costs, but it does not explore this limitation in depth and instead focuses on proposing a solution to this problem.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"it inevitably leads to prohibitively long training times, extortionate computing costs, and a detrimental environmental impact.\"\n\nThis paper mentions a limitation of large pre-trained language models (PTLMs), which is related to their training efficiency, environmental impact, and computational costs, but it does not explore this limitation in depth and instead focuses on proposing a solution to this problem."
    },
    {
        "title": "Comparing Prompt-Based and Standard Fine-Tuning for Urdu Text Classification",
        "authors": [
            "Faizad Ullah",
            "Ubaid Azam",
            "Ali Faheem",
            "Faisal Kamiran",
            "Asim Karim"
        ],
        "published": "2023",
        "summary": "Recent advancements in natural language processing have demonstrated the efficacy of pre-trained language models for various downstream tasks through prompt-based fine-tuning. In contrast to standard fine-tuning, which relies solely on labeled examples, prompt-based fine-tuning combines a few labeled examples (few shot) with guidance through prompts tailored for the specific language and task. For low-resource languages, where labeled examples are limited, prompt-based fine-tuning appears to be a promising alternative. In this paper, we compare prompt-based and standard fine-tuning for the popular task of text classification in Urdu and Roman Urdu languages. We conduct experiments using five datasets, covering different domains, and pre-trained multilingual transformers. The results reveal that significant improvement of up to 13% in accuracy is achieved by prompt-based fine-tuning over standard fine-tuning approaches. This suggests the potential of prompt-based fine-tuning as a valuable approach for low-resource languages with limited labeled data.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.449.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions \"low-resource languages, where labeled examples are limited\", which implies a limitation of LLMs in low-resource languages, however, it is not explicitly stated as a limitation of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions \"low-resource languages, where labeled examples are limited\", which implies a limitation of LLMs in low-resource languages, however, it is not explicitly stated as a limitation of LLMs."
    },
    {
        "title": "Explore the Way: Exploring Reasoning Path by Bridging Entities for Effective Cross-Document Relation Extraction",
        "authors": [
            "Junyoung Son",
            "Jinsung Kim",
            "Jungwoo Lim",
            "Yoonna Jang",
            "Heuiseok Lim"
        ],
        "published": "2023",
        "summary": "Cross-document relation extraction (CodRED) task aims to infer the relation between two entities mentioned in different documents within a reasoning path. Previous studies have concentrated on merely capturing implicit relations between the entities. However, humans usually utilize explicit information chains such as hyperlinks or additional searches to find the relations between two entities. Inspired by this, we propose Path wIth expLOraTion (PILOT) that provides the enhanced reasoning path by exploring the explicit clue information within the documents. PILOT finds the bridging entities which directly guide the paths between the entities and then employs them as stepstones to navigate desirable paths. We show that models with PILOT outperform the baselines in the CodRED task. Furthermore, we offer a variety of analyses to verify the validity of the reasoning paths constructed through PILOT, including evaluations using large language models such as ChatGPT.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.450.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"including evaluations using large language models such as ChatGPT.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"including evaluations using large language models such as ChatGPT.\""
    },
    {
        "title": "The student becomes the master: Outperforming GPT3 on Scientific Factual Error Correction",
        "authors": [
            "Dhananjay Ashok",
            "Atharva Kulkarni",
            "Hai Pham",
            "Barnabas Poczos"
        ],
        "published": "2023",
        "summary": "Due to the prohibitively high cost of creating error correction datasets, most Factual Claim Correction methods rely on a powerful verification model to guide the correction process. This leads to a significant drop in performance in domains like Scientific Claim Correction, where good verification models do not always exist. In this work we introduce SciFix, a claim correction system that does not require a verifier but is able to outperform existing methods by a considerable margin — achieving correction accuracy of 84% on the SciFact dataset, 77% on SciFact-Open and 72.75% on the CovidFact dataset, compared to next best accuracies of 7.6%, 5% and 15% on the same datasets respectively. Our method leverages the power of prompting with LLMs during training to create a richly annotated dataset that can be used for fully supervised training and regularization. We additionally use a claim-aware decoding procedure to improve the quality of corrected claims. Our method outperforms the very LLM that was used to generate the annotated dataset — with FewShot Prompting on GPT3.5 achieving 58%, 61% and 64% on the respective datasets, a consistently lower correction accuracy, despite using nearly 800 times as many parameters as our model.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.451.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our method outperforms the very LLM that was used to generate the annotated dataset — with FewShot Prompting on GPT3.5 achieving 58%, 61% and 64% on the respective datasets, a consistently lower correction accuracy, despite using nearly 800 times as many parameters as our model.\"\n\nThis abstract mentions a limitation of LLMs (lower correction",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our method outperforms the very LLM that was used to generate the annotated dataset — with FewShot Prompting on GPT3.5 achieving 58%, 61% and 64% on the respective datasets, a consistently lower correction accuracy, despite using nearly 800 times as many parameters as our model.\"\n\nThis abstract mentions a limitation of LLMs (lower correction"
    },
    {
        "title": "Leveraging Structured Information for Explainable Multi-hop Question Answering and Reasoning",
        "authors": [
            "Ruosen Li",
            "Xinya Du"
        ],
        "published": "2023",
        "summary": "Neural models, including large language models (LLMs), achieve superior performance on multi-hop question-answering. To elicit reasoning capabilities from LLMs, recent works propose using the chain-of-thought (CoT) mechanism to generate both the reasoning chain and the answer, which enhances the model’s capabilities in conducting multi-hop reasoning. However, several challenges still remain: such as struggling with inaccurate reasoning, hallucinations, and lack of interpretability. On the other hand, information extraction (IE) identifies entities, relations, and events grounded to the text. The extracted structured information can be easily interpreted by humans and machines (Grishman, 2019). In this work, we investigate constructing and leveraging extracted semantic structures (graphs) for multi-hop question answering, especially the reasoning process. Empirical results and human evaluations show that our framework: generates more faithful reasoning chains and substantially improves the QA performance on two benchmark datasets. Moreover, the extracted structures themselves naturally provide grounded explanations that are preferred by humans, as compared to the generated reasoning chains and saliency-based explanations.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.452.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, several challenges still remain: such as struggling with inaccurate reasoning, hallucinations, and lack of interpretability.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, several challenges still remain: such as struggling with inaccurate reasoning, hallucinations, and lack of interpretability.\""
    },
    {
        "title": "Hierarchical Catalogue Generation for Literature Review: A Benchmark",
        "authors": [
            "Kun Zhu",
            "Xiaocheng Feng",
            "Xiachong Feng",
            "Yingsheng Wu",
            "Bing Qin"
        ],
        "published": "2023",
        "summary": "Scientific literature review generation aims to extract and organize important information from an abundant collection of reference papers and produces corresponding reviews while lacking a clear and logical hierarchy. We observe that a high-quality catalogue-guided generation process can effectively alleviate this problem. Therefore, we present an atomic and challenging task named Hierarchical Catalogue Generation for Literature Review as the first step for review generation, which aims to produce a hierarchical catalogue of a review paper given various references. We construct a novel English Hierarchical Catalogues of Literature Reviews Dataset with 7.6k literature review catalogues and 389k reference papers. To accurately assess the model performance, we design two evaluation metrics for informativeness and similarity to ground truth from semantics and structure. Our extensive analyses verify the high quality of our dataset and the effectiveness of our evaluation metrics. We further benchmark diverse experiments on state-of-the-art summarization models like BART and large language models like ChatGPT to evaluate their capabilities. We further discuss potential directions for this task to motivate future research.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.453.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We further benchmark diverse experiments on state-of-the-art summarization models like BART and large language models like ChatGPT to evaluate their capabilities.\"\n\nThis rating is given because the abstract mentions the evaluation of large language models, but does not explicitly discuss their limitations. The focus is on the benchmark and the task of Hierarchical Catalogue Generation for Literature Review.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We further benchmark diverse experiments on state-of-the-art summarization models like BART and large language models like ChatGPT to evaluate their capabilities.\"\n\nThis rating is given because the abstract mentions the evaluation of large language models, but does not explicitly discuss their limitations. The focus is on the benchmark and the task of Hierarchical Catalogue Generation for Literature Review."
    },
    {
        "title": "MCC-KD: Multi-CoT Consistent Knowledge Distillation",
        "authors": [
            "Hongzhan Chen",
            "Siyue Wu",
            "Xiaojun Quan",
            "Rui Wang",
            "Ming Yan",
            "Ji Zhang"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have showcased remarkable capabilities in complex reasoning through chain of thought (CoT) prompting. Recently, there has been a growing interest in transferring these reasoning abilities from LLMs to smaller models. However, achieving both the diversity and consistency in rationales presents a challenge. In this paper, we focus on enhancing these two aspects and propose Multi-CoT Consistent Knowledge Distillation (MCC-KD) to efficiently distill the reasoning capabilities. In MCC-KD, we generate multiple rationales for each question and enforce consistency among their predictions by minimizing the bidirectional KL-divergence between the answer distributions. We conduct comprehensive experiments to investigate the effectiveness of MCC-KD with different model architectures (LLaMA/FlanT5) and various model scales (3B/7B/11B/13B) on both mathematical reasoning and commonsense reasoning benchmarks. The empirical results demonstrate that MCC-KD achieves superior performance on in-distribution datasets and exhibits a strong generalization ability on out-of-distribution datasets.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.454.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, achieving both the diversity and consistency in rationales presents a challenge.\"\n\nThis abstract mentions a limitation of LLMs (difficulty in achieving diversity and consistency in rationales) but only briefly, and the primary focus of the paper is on the proposed solution, MCC-KD.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, achieving both the diversity and consistency in rationales presents a challenge.\"\n\nThis abstract mentions a limitation of LLMs (difficulty in achieving diversity and consistency in rationales) but only briefly, and the primary focus of the paper is on the proposed solution, MCC-KD."
    },
    {
        "title": "Conditional Natural Language Inference",
        "authors": [
            "Youngwoo Kim",
            "Razieh Rahimi",
            "James Allan"
        ],
        "published": "2023",
        "summary": "To properly explain sentence pairs that provide contradictory (different) information for different conditions, we introduce the task of conditional natural language inference (Cond-NLI) and focus on automatically extracting contradictory aspects and their conditions from a sentence pair. Cond-NLI can help to provide a full spectrum of information, such as when there are multiple answers to a question each addressing a specific condition, or reviews with different opinions for different conditions. We show that widely-used feature-attribution explanation models are not suitable for finding conditions, especially when sentences are long and are written independently. We propose a simple yet effective model for the original NLI task that can successfully extract conditions while not requiring token-level annotations. Our model enhances the interpretability of the NLI task while maintaining comparable accuracy. To evaluate models for the Cond-NLI, we build and release a token-level annotated dataset BioClaim which contains potentially contradictory claims from the biomedical domain. Our experiments show that our proposed model outperforms the full cross-encoder and other baselines in extracting conditions. It also performs on-par with GPT-3 which has an order of magnitude more parameters and trained on a huge amount of data.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.456.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "not extracted",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"It also performs on-par with GPT-3 which has an order of magnitude more parameters and trained on a huge amount of data.\"\n\n(Note: The abstract does not explicitly discuss limitations of LLMs, but rather mentions a comparison with a large language model (GPT-3) that implies some limitations in terms of parameter size and training data. However, this is not a direct",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs:  \nEvidence: \"It also performs on-par with GPT-3 which has an order of magnitude more parameters and trained on a huge amount of data.\"\n\n(Note: The abstract does not explicitly discuss limitations of LLMs, but rather mentions a comparison with a large language model (GPT-3) that implies some limitations in terms of parameter size and training data. However, this is not a direct"
    },
    {
        "title": "KEPLET: Knowledge-Enhanced Pretrained Language Model with Topic Entity Awareness",
        "authors": [
            "Yichuan Li",
            "Jialong Han",
            "Kyumin Lee",
            "Chengyuan Ma",
            "Benjamin Yao",
            "Xiaohu Liu"
        ],
        "published": "2023",
        "summary": "In recent years, Pre-trained Language Models (PLMs) have shown their superiority by pre-training on unstructured text corpus and then fine-tuning on downstream tasks. On entity-rich textual resources like Wikipedia, Knowledge-Enhanced PLMs (KEPLMs) incorporate the interactions between tokens and mentioned entities in pre-training, and are thus more effective on entity-centric tasks such as entity linking and relation classification. Although exploiting Wikipedia’s rich structures to some extent, conventional KEPLMs still neglect a unique layout of the corpus where each Wikipedia page is around a topic entity (identified by the page URL and shown in the page title). In this paper, we demonstrate that KEPLMs without incorporating the topic entities will lead to insufficient entity interaction and biased (relation) word semantics. We thus propose KEPLET, a novel Knowledge-Énhanced Pre-trained LanguagE model with Topic entity awareness. In an end-to-end manner, KEPLET identifies where to add the topic entity’s information in a Wikipedia sentence, fuses such information into token and mentioned entities representations, and supervises the network learning, through which it takes topic entities back into consideration. Experiments demonstrated the generality and superiority of KEPLET which was applied to two representative KEPLMs, achieving significant improvements on four entity-centric tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.458.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although exploiting Wikipedia’s rich structures to some extent, conventional KEPLMs still neglect a unique layout of the corpus where each Wikipedia page is around a topic entity (identified by the page URL and shown in the page title). We thus propose KEPLET, a novel Knowledge-Énhanced Pre-trained LanguagE model with Topic entity awareness.\"\n\nThis abstract mentions a limitation of",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although exploiting Wikipedia’s rich structures to some extent, conventional KEPLMs still neglect a unique layout of the corpus where each Wikipedia page is around a topic entity (identified by the page URL and shown in the page title). We thus propose KEPLET, a novel Knowledge-Énhanced Pre-trained LanguagE model with Topic entity awareness.\"\n\nThis abstract mentions a limitation of"
    },
    {
        "title": "Revisiting Large Language Models as Zero-shot Relation Extractors",
        "authors": [
            "Guozheng Li",
            "Peng Wang",
            "Wenjun Ke"
        ],
        "published": "2023",
        "summary": "Relation extraction (RE) consistently involves a certain degree of labeled or unlabeled data even if under zero-shot setting. Recent studies have shown that large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt, which provides the possibility of extracting relations from text without any data and parameter tuning. This work focuses on the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors. On the one hand, we analyze the drawbacks of existing RE prompts and attempt to incorporate recent prompt techniques such as chain-of-thought (CoT) to improve zero-shot RE. We propose the summarize-and-ask (SumAsk) prompting, a simple prompt recursively using LLMs to transform RE inputs to the effective question answering (QA) format. On the other hand, we conduct comprehensive experiments on various benchmarks and settings to investigate the capabilities of LLMs on zero-shot RE. Specifically, we have the following findings: (i) SumAsk consistently and significantly improves LLMs performance on different model sizes, benchmarks and settings; (ii) Zero-shot prompting with ChatGPT achieves competitive or superior results compared with zero-shot and fully supervised methods; (iii) LLMs deliver promising performance in extracting overlapping relations; (iv) The performance varies greatly regarding different relations. Different from small language models, LLMs are effective in handling challenge none-of-the-above (NoTA) relation.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.459.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The performance varies greatly regarding different relations.\"\n\nThis abstract mentions a limitation of LLMs in passing, specifically that their performance varies greatly regarding different relations, but does not elaborate on this limitation further.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The performance varies greatly regarding different relations.\"\n\nThis abstract mentions a limitation of LLMs in passing, specifically that their performance varies greatly regarding different relations, but does not elaborate on this limitation further."
    },
    {
        "title": "Towards large language model-based personal agents in the enterprise: Current trends and open problems",
        "authors": [
            "Vinod Muthusamy",
            "Yara Rizk",
            "Kiran Kate",
            "Praveen Venkateswaran",
            "Vatche Isahagian",
            "Ashu Gulati",
            "Parijat Dube"
        ],
        "published": "2023",
        "summary": "There is an emerging trend to use large language models (LLMs) to reason about complex goals and orchestrate a set of pluggable tools or APIs to accomplish a goal. This functionality could, among other use cases, be used to build personal assistants for knowledge workers. While there are impressive demos of LLMs being used as autonomous agents or for tool composition, these solutions are not ready mission-critical enterprise settings. For example, they are brittle to input changes, and can produce inconsistent results for the same inputs. These use cases have many open problems in an exciting area of NLP research, such as trust and explainability, consistency and reproducibility, adherence to guardrails and policies, best practices for composable tool design, and the need for new metrics and benchmarks. This vision paper illustrates some examples of LLM-based autonomous agents that reason and compose tools, highlights cases where they fail, surveys some of the recent efforts in this space, and lays out the research challenges to make these solutions viable for enterprises.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.461.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"they are brittle to input changes, and can produce inconsistent results for the same inputs\", \"highlights cases where they fail\", \"trust and explainability, consistency and reproducibility, adherence to guardrails and policies\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"they are brittle to input changes, and can produce inconsistent results for the same inputs\", \"highlights cases where they fail\", \"trust and explainability, consistency and reproducibility, adherence to guardrails and policies\""
    },
    {
        "title": "CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models",
        "authors": [
            "Cheng Qian",
            "Chi Han",
            "Yi Fung",
            "Yujia Qin",
            "Zhiyuan Liu",
            "Heng Ji"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have made significant progress in utilizing tools, but their ability is limited by API availability and the instability of implicit reasoning, particularly when both planning and execution are involved. To overcome these limitations, we propose CREATOR, a novel framework that enables LLMs to create their own tools using documentation and code realization. CREATOR disentangles abstract tool creation and concrete decision execution, resulting in improved performance. We evaluate CREATOR on MATH and TabMWP benchmarks, respectively consisting of challenging math competition problems and diverse tabular contents. Remarkably, CREATOR outperforms existing chain-of-thought, program-of-thought, and tool-using baselines. Additionally, we introduce the Creation Challenge dataset, featuring 2K diverse questions, to emphasize the necessity and benefits of LLMs’ tool creation ability. Further research demonstrates that leveraging LLMs as tool creators facilitates knowledge transfer, and LLMs exhibit varying levels of tool creation abilities, enabling them to adapt to diverse situations. The tool creation ability revolutionizes the LLM’s problem-solving paradigm, driving us closer to the next frontier of artificial intelligence.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.462.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large Language Models (LLMs) have made significant progress in utilizing tools, but their ability is limited by API availability and the instability of implicit reasoning, particularly when both planning and execution are involved.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Large Language Models (LLMs) have made significant progress in utilizing tools, but their ability is limited by API availability and the instability of implicit reasoning, particularly when both planning and execution are involved.\""
    },
    {
        "title": "Evaluating Verifiability in Generative Search Engines",
        "authors": [
            "Nelson Liu",
            "Tianyi Zhang",
            "Percy Liang"
        ],
        "published": "2023",
        "summary": "Generative search engines directly generate responses to user queries, along with in-line citations. A prerequisite trait of a trustworthy generative search engine is verifiability, i.e., systems should cite comprehensively (high citation recall; all statements are fully supported by citations) and accurately (high citation precision; every cite supports its associated statement). We conduct human evaluation to audit four popular generative search engines—Bing Chat, NeevaAI, perplexity.ai, and YouChat—across a diverse set of queries from a variety of sources (e.g., historical Google user queries, dynamically-collected open-ended questions on Reddit, etc.). We find that responses from existing generative search engines are fluent and appear informative, but frequently contain unsupported statements and inaccurate citations: on average, a mere 51.5% of generated sentences are fully supported by citations and only 74.5% of citations support their associated sentence. We believe that these results are concerningly low for systems that may serve as a primary tool for information-seeking users, especially given their facade of trustworthiness. We hope that our results further motivate the development of trustworthy generative search engines and help researchers and users better understand the shortcomings of existing commercial systems.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.467.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that responses from existing generative search engines are fluent and appear informative, but frequently contain unsupported statements and inaccurate citations: on average, a mere 51.5% of generated sentences are fully supported by citations and only 74.5% of citations support their associated sentence.\"\n\nThis abstract discusses limitations of generative search engines, which are likely powered by LLMs,",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We find that responses from existing generative search engines are fluent and appear informative, but frequently contain unsupported statements and inaccurate citations: on average, a mere 51.5% of generated sentences are fully supported by citations and only 74.5% of citations support their associated sentence.\"\n\nThis abstract discusses limitations of generative search engines, which are likely powered by LLMs,"
    },
    {
        "title": "Visually Grounded Continual Language Learning with Selective Specialization",
        "authors": [
            "Kyra Ahrens",
            "Lennart Bengtson",
            "Jae Hee Lee",
            "Stefan Wermter"
        ],
        "published": "2023",
        "summary": "A desirable trait of an artificial agent acting in the visual world is to continually learn a sequence of language-informed tasks while striking a balance between sufficiently specializing in each task and building a generalized knowledge for transfer. Selective specialization, i.e., a careful selection of model components to specialize in each task, is a strategy to provide control over this trade-off. However, the design of selection strategies requires insights on the role of each model component in learning rather specialized or generalizable representations, which poses a gap in current research. Thus, our aim with this work is to provide an extensive analysis of selection strategies for visually grounded continual language learning. Due to the lack of suitable benchmarks for this purpose, we introduce two novel diagnostic datasets that provide enough control and flexibility for a thorough model analysis. We assess various heuristics for module specialization strategies as well as quantifiable measures for two different types of model architectures. Finally, we design conceptually simple approaches based on our analysis that outperform common continual learning baselines. Our results demonstrate the need for further efforts towards better aligning continual learning algorithms with the learning behaviors of individual model parts.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.469.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering",
        "authors": [
            "Victor Zhong",
            "Weijia Shi",
            "Wen-tau Yih",
            "Luke Zettlemoyer"
        ],
        "published": "2023",
        "summary": "We introduce RoMQA, the first benchmark for robust, multi-evidence, multi-answer question answering (QA). RoMQA contains clusters of questions that are derived from related constraints mined from the Wikidata knowledge graph. RoMQA evaluates robustness of QA models to varying constraints by measuring worst-case performance within each question cluster. Compared to prior QA datasets, RoMQA has more human-written questions that require reasoning over more evidence text and have, on average, many more correct answers. In addition, human annotators rate RoMQA questions as more natural or likely to be asked by people. We evaluate state-of-the-art large language models in zero-shot, few-shot, and fine-tuning settings, and find that RoMQA is challenging: zeroshot and few-shot models perform similarly to naive baselines, while supervised retrieval methods perform well below gold evidence upper bounds. Moreover, existing models are not robust to variations in question constraints, but can be made more robust by tuning on clusters of related questions. Our results show that RoMQA is a challenging benchmark for large language models, and provides a quantifiable test to build more robust QA methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.470.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We evaluate state-of-the-art large language models in zero-shot, few-shot, and fine-tuning settings, and find that RoMQA is challenging: zeroshot and few-shot models perform similarly to naive baselines, while supervised retrieval methods perform well below gold evidence upper bounds. Moreover, existing models are not robust to variations in question constraints, but can be made more robust by",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We evaluate state-of-the-art large language models in zero-shot, few-shot, and fine-tuning settings, and find that RoMQA is challenging: zeroshot and few-shot models perform similarly to naive baselines, while supervised retrieval methods perform well below gold evidence upper bounds. Moreover, existing models are not robust to variations in question constraints, but can be made more robust by"
    },
    {
        "title": "Summarizing Multiple Documents with Conversational Structure for Meta-Review Generation",
        "authors": [
            "Miao Li",
            "Eduard Hovy",
            "Jey Lau"
        ],
        "published": "2023",
        "summary": "We present PeerSum, a novel dataset for generating meta-reviews of scientific papers. The meta-reviews can be interpreted as abstractive summaries of reviews, multi-turn discussions and the paper abstract. These source documents have a rich inter-document relationship with an explicit hierarchical conversational structure, cross-references and (occasionally) conflicting information. To introduce the structural inductive bias into pre-trained language models, we introduce RAMMER (Relationship-aware Multi-task Meta-review Generator), a model that uses sparse attention based on the conversational structure and a multi-task training objective that predicts metadata features (e.g., review ratings). Our experimental results show that RAMMER outperforms other strong baseline models in terms of a suite of automatic evaluation metrics. Further analyses, however, reveal that RAMMER and other models struggle to handle conflicts in source documents, suggesting meta-review generation is a challenging task and a promising avenue for further research.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.472.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Further analyses, however, reveal that RAMMER and other models struggle to handle conflicts in source documents, suggesting meta-review generation is a challenging task and a promising avenue for further research.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Further analyses, however, reveal that RAMMER and other models struggle to handle conflicts in source documents, suggesting meta-review generation is a challenging task and a promising avenue for further research.\""
    },
    {
        "title": "VIPHY: Probing “Visible” Physical Commonsense Knowledge",
        "authors": [
            "Shikhar Singh",
            "Ehsan Qasemi",
            "Muhao Chen"
        ],
        "published": "2023",
        "summary": "Vision-language models (VLMs) have shown remarkable performance on visual reasoning tasks (e.g. attributes, location). While such tasks measure the requisite knowledge to ground and reason over a given visual instance, they do not, however, measure the ability of VLMs to retain and generalize such knowledge. In this work, we evaluate VLMs’ ability to acquire “visible” physical knowledge – the information that is easily accessible from images of static scenes, particularly along the dimensions of object color, size, and space. We build an automatic pipeline to derive a comprehensive knowledge resource for calibrating and probing these models. Our results indicate a severe gap between model and human performance across all three dimensions. Furthermore, we demonstrate that a caption pretrained LM significantly outperforms VLMs on both size and spatial tasks – highlighting that despite sufficient access to ground language with visual modality, they struggle to retain such knowledge.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.473.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our results indicate a severe gap between model and human performance across all three dimensions. Furthermore, we demonstrate that a caption pretrained LM significantly outperforms VLMs on both size and spatial tasks – highlighting that despite sufficient access to ground language with visual modality, they struggle to retain such knowledge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Our results indicate a severe gap between model and human performance across all three dimensions. Furthermore, we demonstrate that a caption pretrained LM significantly outperforms VLMs on both size and spatial tasks – highlighting that despite sufficient access to ground language with visual modality, they struggle to retain such knowledge.\""
    },
    {
        "title": "Two Directions for Clinical Data Generation with Large Language Models: Data-to-Label and Label-to-Data",
        "authors": [
            "Rumeng Li",
            "Xun Wang",
            "Hong Yu"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) can generate natural language texts for various domains and tasks, but their potential for clinical text mining, a domain with scarce, sensitive, and imbalanced medical data, is under-explored. We investigate whether LLMs can augment clinical data for detecting Alzheimer’s Disease (AD)-related signs and symptoms from electronic health records (EHRs), a challenging task that requires high expertise. We create a novel pragmatic taxonomy for AD sign and symptom progression based on expert knowledge and generated three datasets: (1) a gold dataset annotated by human experts on longitudinal EHRs of AD patients; (2) a silver dataset created by the data-to-label method, which labels sentences from a public EHR collection with AD-related signs and symptoms; and (3) a bronze dataset created by the label-to-data method which generates sentences with AD-related signs and symptoms based on the label definition. We train a system to detect AD-related signs and symptoms from EHRs. We find that the silver and bronze datasets improves the system performance, outperforming the system using only the gold dataset. This shows that LLMs can generate synthetic clinical data for a complex task by incorporating expert knowledge, and our label-to-data method can produce datasets that are free of sensitive information, while maintaining acceptable quality.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.474.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract discusses the potential of LLMs in clinical text mining and their ability to generate synthetic clinical data, but does not mention any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract discusses the potential of LLMs in clinical text mining and their ability to generate synthetic clinical data, but does not mention any limitations of LLMs."
    },
    {
        "title": "Probing LLMs for Joint Encoding of Linguistic Categories",
        "authors": [
            "Giulio Starace",
            "Konstantinos Papakostas",
            "Rochelle Choenni",
            "Apostolos Panagiotopoulos",
            "Matteo Rosati",
            "Alina Leidinger",
            "Ekaterina Shutova"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) exhibit impressive performance on a range of NLP tasks, due to the general-purpose linguistic knowledge acquired during pretraining. Existing model interpretability research (Tenney et al., 2019) suggests that a linguistic hierarchy emerges in the LLM layers, with lower layers better suited to solving syntactic tasks and higher layers employed for semantic processing. Yet, little is known about how encodings of different linguistic phenomena interact within the models and to what extent processing of linguistically-related categories relies on the same, shared model representations. In this paper, we propose a framework for testing the joint encoding of linguistic categories in LLMs. Focusing on syntax, we find evidence of joint encoding both at the same (related part-of-speech (POS) classes) and different (POS classes and related syntactic dependency relations) levels of linguistic hierarchy. Our cross-lingual experiments show that the same patterns hold across languages in multilingual LLMs.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.476.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Yet, little is known about how encodings of different linguistic phenomena interact within the models and to what extent processing of linguistically-related categories relies on the same, shared model representations.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs in understanding how different linguistic phenomena interact within the models, but it is not the primary focus of the paper and is only briefly mentioned",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Yet, little is known about how encodings of different linguistic phenomena interact within the models and to what extent processing of linguistically-related categories relies on the same, shared model representations.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs in understanding how different linguistic phenomena interact within the models, but it is not the primary focus of the paper and is only briefly mentioned"
    },
    {
        "title": "On Robustness of Finetuned Transformer-based NLP Models",
        "authors": [
            "Pavan Kalyan Reddy Neerudu",
            "Subba Oota",
            "Mounika Marreddy",
            "Venkateswara Kagita",
            "Manish Gupta"
        ],
        "published": "2023",
        "summary": "Transformer-based pretrained models like BERT, GPT-2 and T5 have been finetuned for a large number of natural language processing (NLP) tasks, and have been shown to be very effective. However, while finetuning, what changes across layers in these models with respect to pretrained checkpoints is under-studied. Further, how robust are these models to perturbations in input text? Does the robustness vary depending on the NLP task for which the models have been finetuned? While there exists some work on studying robustness of BERT finetuned for a few NLP tasks, there is no rigorous study which compares this robustness across encoder only, decoder only and encoder-decoder models. In this paper, we characterize changes between pretrained and finetuned language model representations across layers using two metrics: CKA and STIR. Further, we study the robustness of three language models (BERT, GPT-2 and T5) with eight different text perturbations on classification tasks from General Language Understanding Evaluation (GLUE) benchmark, and generation tasks like summarization, free-form generation and question generation. GPT-2 representations are more robust than BERT and T5 across multiple types of input perturbation. Although models exhibit good robustness broadly, dropping nouns, verbs or changing characters are the most impactful. Overall, this study provides valuable insights into perturbation-specific weaknesses of popular Transformer-based models which should be kept in mind when passing inputs.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.477.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, while finetuning, what changes across layers in these models with respect to pretrained checkpoints is under-studied. Further, how robust are these models to perturbations in input text?\"; \"GPT-2 representations are more robust than BERT and T5 across multiple types of input perturbation. Although models exhibit good robustness broadly, dropping nouns, verbs",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, while finetuning, what changes across layers in these models with respect to pretrained checkpoints is under-studied. Further, how robust are these models to perturbations in input text?\"; \"GPT-2 representations are more robust than BERT and T5 across multiple types of input perturbation. Although models exhibit good robustness broadly, dropping nouns, verbs"
    },
    {
        "title": "Measuring and Mitigating Constraint Violations of In-Context Learning for Utterance-to-API Semantic Parsing",
        "authors": [
            "Shufan Wang",
            "Sébastien Jean",
            "Sailik Sengupta",
            "James Gung",
            "Nikolaos Pappas",
            "Yi Zhang"
        ],
        "published": "2023",
        "summary": "In executable task-oriented semantic parsing, the system aims to translate users’ utterances in natural language to machine-interpretable programs (API calls) that can be executed according to pre-defined API specifications. With the popularity of Large Language Models (LLMs), in-context learning offers a strong baseline for such scenarios, especially in data-limited regimes. However, LLMs are known to hallucinate and therefore pose a formidable challenge in constraining generated content. Thus, it remains uncertain if LLMs can effectively perform task-oriented utterance-to-API generation, where respecting the API’s structural and task-specific constraints is crucial. In this work, we seek to measure, analyze and mitigate such constraints violations. First, we identify the categories of various constraints in obtaining API-semantics from task-oriented utterances, and define fine-grained metrics that complement traditional ones. Second, we leverage these metrics to conduct a detailed error analysis of constraints violations seen in state-of-the-art LLMs, which motivates us to investigate two popular mitigation strategies– Semantic-Retrieval of Demonstrations (SRD) and API-aware Constrained Decoding (API-CD). Our experiments show that these strategies are effective at reducing constraints violations and improving the quality of the generated API calls, but require careful consideration given their implementation complexity and latency.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.478.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, LLMs are known to hallucinate and therefore pose a formidable challenge in constraining generated content.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, LLMs are known to hallucinate and therefore pose a formidable challenge in constraining generated content.\""
    },
    {
        "title": "Topic-DPR: Topic-based Prompts for Dense Passage Retrieval",
        "authors": [
            "Qingfa Xiao",
            "Shuangyin Li",
            "Lei Chen"
        ],
        "published": "2023",
        "summary": "Prompt-based learning’s efficacy across numerous natural language processing tasks has led to its integration into dense passage retrieval. Prior research has mainly focused on enhancing the semantic understanding of pre-trained language models by optimizing a single vector as a continuous prompt. This approach, however, leads to a semantic space collapse; identical semantic information seeps into all representations, causing their distributions to converge in a restricted region. This hinders differentiation between relevant and irrelevant passages during dense retrieval. To tackle this issue, we present Topic-DPR, a dense passage retrieval model that uses topic-based prompts. Unlike the single prompt method, multiple topic-based prompts are established over a probabilistic simplex and optimized simultaneously through contrastive learning. This encourages representations to align with their topic distributions, improving space uniformity. Furthermore, we introduce a novel positive and negative sampling strategy, leveraging semi-structured data to boost dense retrieval efficiency. Experimental results from two datasets affirm that our method surpasses previous state-of-the-art retrieval techniques.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.480.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This approach, however, leads to a semantic space collapse; identical semantic information seeps into all representations, causing their distributions to converge in a restricted region. This hinders differentiation between relevant and irrelevant passages during dense retrieval.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"This approach, however, leads to a semantic space collapse; identical semantic information seeps into all representations, causing their distributions to converge in a restricted region. This hinders differentiation between relevant and irrelevant passages during dense retrieval.\""
    },
    {
        "title": "Quantifying the Dialect Gap and its Correlates Across Languages",
        "authors": [
            "Anjali Kantharuban",
            "Ivan Vulić",
            "Anna Korhonen"
        ],
        "published": "2023",
        "summary": "Historically, researchers and consumers have noticed a decrease in quality when applying NLP tools to minority variants of languages (i.e. Puerto Rican Spanish or Swiss German), but studies exploring this have been limited to a select few languages. Additionally, past studies have mainly been conducted in a monolingual context, so cross-linguistic trends have not been identified and tied to external factors. In this work, we conduct a comprehensive evaluation of the most influential, state-of-the-art large language models (LLMs) across two high-use applications, machine translation and automatic speech recognition, to assess their functionality on the regional dialects of several high- and low-resource languages. Additionally, we analyze how the regional dialect gap is correlated with economic, social, and linguistic factors. The impact of training data, including related factors like dataset size and its construction procedure, is shown to be significant but not consistent across models or languages, meaning a one-size-fits-all approach cannot be taken in solving the dialect gap. This work will lay the foundation for furthering the field of dialectal NLP by laying out evident disparities and identifying possible pathways for addressing them through mindful data collection.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.481.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Additionally, past studies have mainly been conducted in a monolingual context, so cross-linguistic trends have not been identified and tied to external factors.\"; \"The impact of training data, including related factors like dataset size and its construction procedure, is shown to be significant but not consistent across models or languages, meaning a one-size-fits-all approach cannot be taken in solving",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Additionally, past studies have mainly been conducted in a monolingual context, so cross-linguistic trends have not been identified and tied to external factors.\"; \"The impact of training data, including related factors like dataset size and its construction procedure, is shown to be significant but not consistent across models or languages, meaning a one-size-fits-all approach cannot be taken in solving"
    },
    {
        "title": "Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment",
        "authors": [
            "Zhen Zhang",
            "Jialu Wang",
            "Xin Wang"
        ],
        "published": "2023",
        "summary": "Pre-trained vision and language models such as CLIP have witnessed remarkable success in connecting images and texts with a primary focus on English texts. Despite recent efforts to extend CLIP to support other languages, disparities in performance among different languages have been observed due to uneven resource availability. Additionally, current cross-lingual transfer methods of those pre-trained models would consume excessive resources for a large number of languages. Therefore, we propose a new parameter-efficient cross-lingual transfer learning framework that utilizes a translation-based alignment method to mitigate multilingual disparities and explores parameter-efficient fine-tuning methods for parameter-efficient cross-lingual transfer. Extensive experiments on XTD and Multi30K datasets, covering 11 languages under zero-shot, few-shot, and full-dataset learning scenarios, show that our framework significantly reduces the multilingual disparities among languages and improves cross-lingual transfer results, especially in low-resource scenarios, while only keeping and fine-tuning an extremely small number of parameters compared to the full model (e.g., Our framework only requires 0.16% additional parameters of a full-model for each language in the few-shot learning scenario).",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.483.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"disparities in performance among different languages have been observed due to uneven resource availability.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"disparities in performance among different languages have been observed due to uneven resource availability.\""
    },
    {
        "title": "Pseudointelligence: A Unifying Lens on Language Model Evaluation",
        "authors": [
            "Shikhar Murty",
            "Orr Paradise",
            "Pratyusha Sharma"
        ],
        "published": "2023",
        "summary": "With large language models surpassing human performance on an increasing number of benchmarks, we must take a principled approach for targeted evaluation of model capabilities. Inspired by pseudorandomness, we propose pseudointelligence, which captures the maxim that “(perceived) intelligence lies in the eye of the beholder.” That is, that claims of intelligence are meaningful only when their evaluator is taken into account. Concretely, we propose a complexity-theoretic framework of model evaluation cast as a dynamic interaction between a model and a learned evaluator. We demonstrate that this framework can be used to reason about two case studies in language model evaluation, as well as analyze existing evaluation methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.485.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"With large language models surpassing human performance on an increasing number of benchmarks, we must take a principled approach for targeted evaluation of model capabilities.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"With large language models surpassing human performance on an increasing number of benchmarks, we must take a principled approach for targeted evaluation of model capabilities.\""
    },
    {
        "title": "GDA: Grammar-based Data Augmentation for Text Classification using Slot Information",
        "authors": [
            "Joonghyuk Hahn",
            "Hyunjoon Cheon",
            "Elizabeth Orwig",
            "Su-Hyeon Kim",
            "Sang-Ki Ko",
            "Yo-Sub Han"
        ],
        "published": "2023",
        "summary": "Recent studies propose various data augmentation approaches to resolve the low-resource problem in natural language processing tasks. Data augmentation is a successful solution to this problem and recent strategies give variation on sentence structures to boost performance. However, these approaches can potentially lead to semantic errors and produce semantically noisy data due to the unregulated variation of sentence structures. In an effort to combat these semantic errors, we leverage slot information, the representation of the context of keywords from a sentence, and form a data augmentation strategy which we propose, called GDA. Our strategy employs algorithms that construct and manipulate rules of context-aware grammar, utilizing this slot information. The algorithms extract recurrent patterns by distinguishing words with slots and form the “rules of grammar”—a set of injective relations between a sentence’s semantics and its syntactical structure—to augment the dataset. The augmentation is done in an automated manner with the constructed rules and thus, GDA is explainable and reliable without any human intervention. We evaluate GDA with state-of-the-art data augmentation techniques, including those using pre-trained language models, and the result illustrates that GDA outperforms all other data augmentation methods by 19.38%. Extensive experiments show that GDA is an effective data augmentation strategy that incorporates word semantics for more accurate and diverse data.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.486.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"including those using pre-trained language models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"including those using pre-trained language models\""
    },
    {
        "title": "Dynamic Stashing Quantization for Efficient Transformer Training",
        "authors": [
            "Guo Yang",
            "Daniel Lo",
            "Robert Mullins",
            "Yiren Zhao"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have demonstrated impressive performance on a range of Natural Language Processing (NLP) tasks. Unfortunately, the immense amount of computations and memory accesses required for LLM training makes them prohibitively expensive in terms of hardware cost, and thus challenging to deploy in use cases such as on-device learning. In this paper, motivated by the observation that LLM training is memory-bound, we propose a novel dynamic quantization strategy, termed Dynamic Stashing Quantization (DSQ), that puts a special focus on reducing the memory operations, but also enjoys the other benefits of low precision training, such as the reduced arithmetic cost. We conduct a thorough study on two translation tasks (trained-from-scratch) and three classification tasks (fine-tuning). DSQ reduces the amount of arithmetic operations by 20.95× and the number of DRAM operations by 2.55× on IWSLT17 compared to the standard 16-bit fixed-point, which is widely used in on-device learning.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.489.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unfortunately, the immense amount of computations and memory accesses required for LLM training makes them prohibitively expensive in terms of hardware cost, and thus challenging to deploy in use cases such as on-device learning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Unfortunately, the immense amount of computations and memory accesses required for LLM training makes them prohibitively expensive in terms of hardware cost, and thus challenging to deploy in use cases such as on-device learning.\""
    },
    {
        "title": "A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction",
        "authors": [
            "Ruihao Shui",
            "Yixin Cao",
            "Xiang Wang",
            "Tat-Seng Chua"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have demonstrated great potential for domain-specific applications, such as the law domain. However, recent disputes over GPT-4’s law evaluation raise questions concerning their performance in real-world legal tasks. To systematically investigate their competency in the law, we design practical baseline solutions based on LLMs and test on the task of legal judgment prediction. In our solutions, LLMs can work alone to answer open questions or coordinate with an information retrieval (IR) system to learn from similar cases or solve simplified multi-choice questions. We show that similar cases and multi-choice options, namely label candidates, included in prompts can help LLMs recall domain knowledge that is critical for expertise legal reasoning. We additionally present an intriguing paradox wherein an IR system surpasses the performance of LLM+IR due to limited gains acquired by weaker LLMs from powerful IR systems. In such case, the role of LLMs becomes redundant. Our evaluation pipeline can be easily extended into other tasks to facilitate evaluations in other domains. Code is available at https://github.com/srhthu/LM-CompEval-Legal",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.490.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, recent disputes over GPT-4’s law evaluation raise questions concerning their performance in real-world legal tasks.\"; \"We additionally present an intriguing paradox wherein an IR system surpasses the performance of LLM+IR due to limited gains acquired by weaker LLMs from powerful IR systems.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, recent disputes over GPT-4’s law evaluation raise questions concerning their performance in real-world legal tasks.\"; \"We additionally present an intriguing paradox wherein an IR system surpasses the performance of LLM+IR due to limited gains acquired by weaker LLMs from powerful IR systems.\""
    },
    {
        "title": "TabPrompt: Graph-based Pre-training and Prompting for Few-shot Table Understanding",
        "authors": [
            "Rihui Jin",
            "Jianan Wang",
            "Wei Tan",
            "Yongrui Chen",
            "Guilin Qi",
            "Wang Hao"
        ],
        "published": "2023",
        "summary": "Table Understanding (TU) is a crucial aspect of information extraction that enables machines to comprehend the semantics behind tabular data. However, existing methods of TU cannot deal with the scarcity of labeled tabular data. In addition, these methods primarily focus on the textual content within the table, disregarding the inherent topological information of the table. This can lead to a misunderstanding of the tabular semantics. In this paper, we propose TabPrompt, a new framework to tackle the above challenges. Prompt-based learning has gained popularity due to its exceptional performance in few-shot learning. Thus, we introduce prompt-based learning to handle few-shot TU. Furthermore, Graph Contrastive Learning (Graph CL) demonstrates remarkable capabilities in capturing topological information, making Graph Neural Networks an ideal method for encoding tables. Hence, we develop a novel Graph CL method tailored to tabular data. This method serves as the pretext task during the pre-training phase, allowing the generation of vector representations that incorporate the table’s topological information. The experimental results of outperforming all strong baselines demonstrate the strength of our method in few-shot table understanding tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.493.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Improving Seq2Seq Grammatical Error Correction via Decoding Interventions",
        "authors": [
            "Houquan Zhou",
            "Yumeng Liu",
            "Zhenghua Li",
            "Min Zhang",
            "Bo Zhang",
            "Chen Li",
            "Ji Zhang",
            "Fei Huang"
        ],
        "published": "2023",
        "summary": "The sequence-to-sequence (Seq2Seq) approach has recently been widely used in grammatical error correction (GEC) and shows promising performance. However, the Seq2Seq GEC approach still suffers from two issues. First, a Seq2Seq GEC model can only be trained on parallel data, which, in GEC task, is often noisy and limited in quantity. Second, the decoder of a Seq2Seq GEC model lacks an explicit awareness of the correctness of the token being generated. In this paper, we propose a unified decoding intervention framework that employs an external critic to assess the appropriateness of the token to be generated incrementally, and then dynamically influence the choice of the next token. We discover and investigate two types of critics: a pre-trained left-to-right language model critic and an incremental target-side grammatical error detector critic. Through extensive experiments on English and Chinese datasets, our framework consistently outperforms strong baselines and achieves results competitive with state-of-the-art methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.495.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Exploring the Potential of Large Language Models in Generating Code-Tracing Questions for Introductory Programming Courses",
        "authors": [
            "Aysa Fan",
            "Ranran Haoran Zhang",
            "Luc Paquette",
            "Rui Zhang"
        ],
        "published": "2023",
        "summary": "In this paper, we explore the application of large language models (LLMs) for generating code-tracing questions in introductory programming courses. We designed targeted prompts for GPT4, guiding it to generate code-tracing questions based on code snippets and descriptions. We established a set of human evaluation metrics to assess the quality of questions produced by the model compared to those created by human experts. Our analysis provides insights into the capabilities and potential of LLMs in generating diverse code-tracing questions. Additionally, we present a unique dataset of human and LLM-generated tracing questions, serving as a valuable resource for both the education and NLP research communities. This work contributes to the ongoing dialogue on the potential uses of LLMs in educational settings.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.496.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the abstract implies that the paper will discuss the \"capabilities\" of LLMs, which could include limitations, but it does not explicitly state any limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the abstract implies that the paper will discuss the \"capabilities\" of LLMs, which could include limitations, but it does not explicitly state any limitations."
    },
    {
        "title": "Learning Easily Updated General Purpose Text Representations with Adaptable Task-Specific Prefix",
        "authors": [
            "Kuan-Hao Huang",
            "Liang Tan",
            "Rui Hou",
            "Sinong Wang",
            "Amjad Almahairi",
            "Ruty Rinott"
        ],
        "published": "2023",
        "summary": "Many real-world applications require making multiple predictions from the same text. Fine-tuning a large pre-trained language model for each downstream task causes computational burdens in the inference time due to several times of forward passes. To amortize the computational cost, freezing the language model and building lightweight models for downstream tasks based on fixed text representations are common solutions. Accordingly, how to learn fixed but general text representations that can generalize well to unseen downstream tasks becomes a challenge. Previous works have shown that the generalizability of representations can be improved by fine-tuning the pre-trained language model with some source tasks in a multi-tasking way. In this work, we propose a prefix-based method to learn the fixed text representations with source tasks. We learn a task-specific prefix for each source task independently and combine them to get the final representations. Our experimental results show that prefix-based training performs better than multi-tasking training and can update the text representations at a smaller computational cost than multi-tasking training.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.497.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Fine-tuning a large pre-trained language model for each downstream task causes computational burdens in the inference time due to several times of forward passes.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Fine-tuning a large pre-trained language model for each downstream task causes computational burdens in the inference time due to several times of forward passes.\""
    },
    {
        "title": "Reasoning Makes Good Annotators : An Automatic Task-specific Rules Distilling Framework for Low-resource Relation Extraction",
        "authors": [
            "Yilin Lu",
            "Juncheng Li",
            "Xiaoqiang Wang",
            "Haochen Shi",
            "Tao Chen",
            "Siliang Tang"
        ],
        "published": "2023",
        "summary": "Relation extraction is often challenged by insufficient labeled data. Previous methods exploit knowledge from unlabeled data by generating pseudo labels in a self-training pipeline, which suffers a gradual drift problem. Logic rules, a transferable and explainable form of expert knowledge, have achieved promising success by improving the model with weak labels. But manually writing comprehensive rules set is challenging and tedious. To alleviate the human labor of writing high-quality rules, in this work, we propose ARIA, an Automatic task-specific Rules distilling framework. Specifically, we guide the pre-trained language model to reason rules as experts and compose them into robust compound rules for data labeling. Besides, ARIA could continuously enrich the rules set to power the labeling ability by discovering reliable model-labeled data for distinguishable rules generation. Experiments on two public datasets demonstrate the effectiveness of ARIA in a low-resource scenario.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.499.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Previous methods... suffer a gradual drift problem.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Previous methods... suffer a gradual drift problem.\""
    },
    {
        "title": "Co-training and Co-distillation for Quality Improvement and Compression of Language Models",
        "authors": [
            "Hayeon Lee",
            "Rui Hou",
            "Jongpil Kim",
            "Davis Liang",
            "Hongbo Zhang",
            "Sung Hwang",
            "Alexander Min"
        ],
        "published": "2023",
        "summary": "Knowledge Distillation (KD) compresses computationally expensive pre-trained language models (PLMs) by transferring their knowledge to smaller models, allowing their use in resource-constrained or real-time settings. However, most smaller models fail to surpass the performance of the original larger model, resulting in sacrificing performance to improve inference speed. To address this issue, we propose Co-Training and Co-Distillation (CTCD), a novel framework that improves performance and inference speed together by co-training two models while mutually distilling knowledge. The CTCD framework successfully achieves this based on two significant findings: 1) Distilling knowledge from the smaller model to the larger model during co-training improves the performance of the larger model. 2) The enhanced performance of the larger model further boosts the performance of the smaller model. The CTCD framework shows promise as it can be combined with existing techniques like architecture design or data augmentation, replacing one-way KD methods, to achieve further performance improvement. Extensive ablation studies demonstrate the effectiveness of CTCD, and the small model distilled by CTCD outperforms the original larger model by a significant margin of 1.66 on the GLUE benchmark.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.500.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, most smaller models fail to surpass the performance of the original larger model, resulting in sacrificing performance to improve inference speed.\"\n\nThis abstract mentions a limitation of LLMs in passing, specifically the trade-off between performance and inference speed when compressing large models. However, this limitation is not the primary focus of the paper, which is on the proposed Co-Training and Co-D",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, most smaller models fail to surpass the performance of the original larger model, resulting in sacrificing performance to improve inference speed.\"\n\nThis abstract mentions a limitation of LLMs in passing, specifically the trade-off between performance and inference speed when compressing large models. However, this limitation is not the primary focus of the paper, which is on the proposed Co-Training and Co-D"
    },
    {
        "title": "ReadPrompt: A Readable Prompting Method for Reliable Knowledge Probing",
        "authors": [
            "Zezhong Wang",
            "Luyao Ye",
            "Hongru Wang",
            "Wai-Chung Kwan",
            "David Ho",
            "Kam-Fai Wong"
        ],
        "published": "2023",
        "summary": "Knowledge probing is a task to assess the knowledge encoded within pre-trained language models (PLMs) by having the PLM complete prompts such as “Italy is located in __,”. The model’s prediction precision serves as a lower bound for the amount of knowledge it contains. Subsequent works explore training a series of vectors as prompts to guide PLMs towards more accurate predictions. However, these methods compromise the readability of the prompts. We cannot directly understand these prompts from their literal meaning, making it difficult to verify whether they are correct. Consequently, the credibility of probing results derived from these prompts is diminished. To address the issue, we propose a novel method called ReadPrompt, which aims to identify meaningful sentences to serve as prompts. Experiments show that ReadPrompt achieves state-of-the-art performance on the current knowledge probing benchmark. Moreover, since the prompt is readable, we discovered a misalignment between constructed prompts and knowledge, which is also present in current prompting methods verified by an attack experiment. We claim that the probing outcomes of the current prompting methods are unreliable that overestimate the knowledge contained within PLMs.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.501.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Consequently, the credibility of probing results derived from these prompts is diminished.\"; \"We claim that the probing outcomes of the current prompting methods are unreliable that overestimate the knowledge contained within PLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Consequently, the credibility of probing results derived from these prompts is diminished.\"; \"We claim that the probing outcomes of the current prompting methods are unreliable that overestimate the knowledge contained within PLMs.\""
    },
    {
        "title": "How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench",
        "authors": [
            "Qinyuan Ye",
            "Harvey Fu",
            "Xiang Ren",
            "Robin Jia"
        ],
        "published": "2023",
        "summary": "We investigate the predictability of large language model (LLM) capabilities: given records of past experiments using different model families, numbers of parameters, tasks, and numbers of in-context examples, can we accurately predict LLM performance on new experiment configurations? Answering this question has practical implications for LLM users (e.g., deciding which models to try), developers (e.g., prioritizing evaluation on representative tasks), and the research community (e.g., identifying hard-to-predict capabilities that warrant further investigation). We study the performance prediction problem on experiment records from BIG-bench. On a random train-test split, an MLP-based predictor achieves an R2 score greater than 95%, indicating the presence of learnable patterns within the experiment records. We then formulate the problem of searching for “small-bench,” an informative subset of BIG-bench tasks from which the performance on the full set can be maximally recovered. We find a subset as informative as BIG-bench Hard for evaluating new model families, while being 3× smaller. Additionally, we find competitive subsets by clustering task representations learned by our MLP-based predictor and selecting tasks close to cluster centroids, highlighting the importance of task diversity in constructing “small-bench.”",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.503.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"given records of past experiments using different model families, numbers of parameters, tasks, and numbers of in-context examples, can we accurately predict LLM performance on new experiment configurations?\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"given records of past experiments using different model families, numbers of parameters, tasks, and numbers of in-context examples, can we accurately predict LLM performance on new experiment configurations?\""
    },
    {
        "title": "POSQA: Probe the World Models of LLMs with Size Comparisons",
        "authors": [
            "Chang Shu",
            "Jiuzhou Han",
            "Fangyu Liu",
            "Ehsan Shareghi",
            "Nigel Collier"
        ],
        "published": "2023",
        "summary": "Embodied language comprehension emphasizes that language understanding is not solely a matter of mental processing in the brain but also involves interactions with the physical and social environment. With the explosive growth of Large Language Models (LLMs) and their already ubiquitous presence in our daily lives, it is becoming increasingly necessary to verify their real-world understanding. Inspired by cognitive theories, we propose POSQA: a Physical Object Size Question Answering dataset with simple size comparison questions to examine the extremity and analyze the potential mechanisms of the embodied comprehension of the latest LLMs. We show that even the largest LLMs today perform poorly under the zero-shot setting. We then push their limits with advanced prompting techniques and external knowledge augmentation. Furthermore, we investigate whether their real-world comprehension primarily derives from contextual information or internal weights and analyse the impact of prompt formats and report bias of different objects. Our results show that real-world understanding that LLMs shaped from textual data can be vulnerable to deception and confusion by the surface form of prompts, which makes it less aligned with human behaviours.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.504.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We show that even the largest LLMs today perform poorly under the zero-shot setting.\" and \"Our results show that real-world understanding that LLMs shaped from textual data can be vulnerable to deception and confusion by the surface form of prompts, which makes it less aligned with human behaviours.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We show that even the largest LLMs today perform poorly under the zero-shot setting.\" and \"Our results show that real-world understanding that LLMs shaped from textual data can be vulnerable to deception and confusion by the surface form of prompts, which makes it less aligned with human behaviours.\""
    },
    {
        "title": "Hierarchical Fusion for Online Multimodal Dialog Act Classification",
        "authors": [
            "Md Messal Monem Miah",
            "Adarsh Pyarelal",
            "Ruihong Huang"
        ],
        "published": "2023",
        "summary": "We propose a framework for online multimodal dialog act (DA) classification based on raw audio and ASR-generated transcriptions of current and past utterances. Existing multimodal DA classification approaches are limited by ineffective audio modeling and late-stage fusion. We showcase significant improvements in multimodal DA classification by integrating modalities at a more granular level and incorporating recent advancements in large language and audio models for audio feature extraction. We further investigate the effectiveness of self-attention and cross-attention mechanisms in modeling utterances and dialogs for DA classification. We achieve a substantial increase of 3 percentage points in the F1 score relative to current state-of-the-art models on two prominent DA classification datasets, MRDA and EMOTyDA.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.505.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing multimodal DA classification approaches are limited by ineffective audio modeling and late-stage fusion.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing multimodal DA classification approaches are limited by ineffective audio modeling and late-stage fusion.\""
    },
    {
        "title": "STEER: Unified Style Transfer with Expert Reinforcement",
        "authors": [
            "Skyler Hallinan",
            "Faeze Brahman",
            "Ximing Lu",
            "Jaehun Jung",
            "Sean Welleck",
            "Yejin Choi"
        ],
        "published": "2023",
        "summary": "While text style transfer has many applications across natural language processing, the core premise of transferring from a single source style is unrealistic in a real-world setting. In this work, we focus on arbitrary style transfer: rewriting a text from an arbitrary, unknown style to a target style. We propose STEER: Unified Style Transfer with Expert Reinforcement, a unified frame-work developed to overcome the challenge of limited parallel data for style transfer. STEER involves automatically generating a corpus of style-transfer pairs using a product of experts during decoding. The generated offline data is then used to pre-train an initial policy before switching to online, off-policy reinforcement learning for further improvements via fine-grained reward signals. STEER is unified and can transfer to multiple target styles from an arbitrary, unknown source style, making it particularly flexible and efficient. Experimental results on a challenging dataset with text from a diverse set of styles demonstrate state-of-the-art results compared to competitive baselines. Remarkably, STEER outperforms the 175B parameter instruction-tuned GPT-3 on overall style transfer quality, despite being 226 times smaller in size. We also show STEER is robust, maintaining its style transfer capabilities on out-of-domain data, and surpassing nearly all baselines across various styles. The success of our method highlights the potential of RL algorithms when augmented with controllable decoding to overcome the challenge of limited data supervision.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.506.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Remarkably, STEER outperforms the 175B parameter instruction-tuned GPT-3 on overall style transfer quality, despite being 226 times smaller in size.\"\n\nThis paper mentions a limitation of a specific LLM (GPT-3) in passing, but does not elaborate on it and focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Remarkably, STEER outperforms the 175B parameter instruction-tuned GPT-3 on overall style transfer quality, despite being 226 times smaller in size.\"\n\nThis paper mentions a limitation of a specific LLM (GPT-3) in passing, but does not elaborate on it and focuses on the proposed solution."
    },
    {
        "title": "Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate",
        "authors": [
            "Kai Xiong",
            "Xiao Ding",
            "Yixin Cao",
            "Ting Liu",
            "Bing Qin"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have shown impressive capabilities in various applications, but they still face various inconsistency issues. Existing works primarily focus on the inconsistency issues within a single LLM, while we complementarily explore the inter-consistency among multiple LLMs for collaboration. To examine whether LLMs can collaborate effectively to achieve a consensus for a shared goal, we focus on commonsense reasoning, and introduce a formal debate framework (FORD) to conduct a three-stage debate among LLMs with real-world scenarios alignment: fair debate, mismatched debate, and roundtable debate. Through extensive experiments on various datasets, LLMs can effectively collaborate to reach a consensus despite noticeable inter-inconsistencies, but imbalances in their abilities can lead to domination by superior LLMs. Leveraging a more advanced LLM like GPT-4 as an authoritative judge can boost collaboration performance. Our work contributes to understanding the inter-consistency among LLMs and lays the foundation for developing future collaboration methods. Codes and data are available at https://github.com/Waste-Wood/FORD.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.508.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing works primarily focus on the inconsistency issues within a single LLM... imbalances in their abilities can lead to domination by superior LLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Existing works primarily focus on the inconsistency issues within a single LLM... imbalances in their abilities can lead to domination by superior LLMs.\""
    },
    {
        "title": "Culturally Aware Natural Language Inference",
        "authors": [
            "Jing Huang",
            "Diyi Yang"
        ],
        "published": "2023",
        "summary": "Humans produce and consume language in a particular cultural context, which includes knowledge about specific norms and practices. A listener’s awareness of the cultural context is critical for interpreting the speaker’s meaning. A simple expression like *I didn’t leave a tip* implies a strong sense of dissatisfaction when tipping is assumed to be the norm. As NLP systems reach users from different cultures, achieving culturally aware language understanding becomes increasingly important. However, current research has focused on building cultural knowledge bases without studying how such knowledge leads to contextualized interpretations of texts. In this work, we operationalize cultural variations in language understanding through a natural language inference (NLI) task that surfaces cultural variations as label disagreement between annotators from different cultural groups. We introduce the first Culturally Aware Natural Language Inference (CALI) dataset with 2.7K premise-hypothesis pairs annotated by two cultural groups located in the U.S. and India. With CALI, we categorize how cultural norms affect language understanding and present an evaluation framework to assess at which levels large language models are culturally aware. Our dataset is available at https://github.com/SALT-NLP/CulturallyAwareNLI.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.509.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"present an evaluation framework to assess at which levels large language models are culturally aware.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"present an evaluation framework to assess at which levels large language models are culturally aware.\""
    },
    {
        "title": "Evaluating Dependencies in Fact Editing for Language Models: Specificity and Implication Awareness",
        "authors": [
            "Zichao Li",
            "Ines Arous",
            "Siva Reddy",
            "Jackie Cheung"
        ],
        "published": "2023",
        "summary": "The potential of using a large language model (LLM) as a knowledge base (KB) has sparked significant interest. To maintain the knowledge acquired by LLMs, we need to ensure that the editing of learned facts respects internal logical constraints, which are known as dependency of knowledge. Existing work on editing LLMs has partially addressed the issue of dependency, when the editing of a fact should apply to its lexical variations without disrupting irrelevant ones. However, they neglect the dependency between a fact and its logical implications. We propose an evaluation protocol with an accompanying question-answering dataset, StandUp, that provides a comprehensive assessment of the editing process considering the above notions of dependency. Our protocol involves setting up a controlled environment in which we edit facts and monitor their impact on LLMs, along with their implications based on If-Then rules. Extensive experiments on StandUp show that existing knowledge editing methods are sensitive to the surface form of knowledge, and that they have limited performance in inferring the implications of edited facts.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.511.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing work on editing LLMs has partially addressed the issue of dependency, when the editing of a fact should apply to its lexical variations without disrupting irrelevant ones. However, they neglect the dependency between a fact and its logical implications.\"; \"Extensive experiments on StandUp show that existing knowledge editing methods are sensitive to the surface form of knowledge, and that they have limited performance in inf",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Existing work on editing LLMs has partially addressed the issue of dependency, when the editing of a fact should apply to its lexical variations without disrupting irrelevant ones. However, they neglect the dependency between a fact and its logical implications.\"; \"Extensive experiments on StandUp show that existing knowledge editing methods are sensitive to the surface form of knowledge, and that they have limited performance in inf"
    },
    {
        "title": "Are Language Models Worse than Humans at Following Prompts? It’s Complicated",
        "authors": [
            "Albert Webson",
            "Alyssa Loo",
            "Qinan Yu",
            "Ellie Pavlick"
        ],
        "published": "2023",
        "summary": "Prompts have been the center of progress in advancing language models’ zero-shot and few-shot performance. However, recent work finds that models can perform surprisingly well when given intentionally irrelevant or misleading prompts. Such results may be interpreted as evidence that model behavior is not “human like’. In this study, we challenge a central assumption in such work: that humans would perform badly when given pathological instructions. We find that humans are able to reliably ignore irrelevant instructions and thus, like models, perform well on the underlying task despite an apparent lack of signal regarding the task they are being asked to do. However, when given deliberately misleading instructions, humans follow the instructions faithfully, whereas models do not. Thus, our conclusion is mixed with respect to prior work. We argue against the earlier claim that high performance with irrelevant prompts constitutes evidence against models’ instruction understanding, but we reinforce the claim that models’ failure to follow misleading instructions raises concerns. More broadly, we caution that future research should not idealize human behaviors as a monolith and should not train or evaluate models to mimic assumptions about these behaviors without first validating humans’ behaviors empirically.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.514.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, when given deliberately misleading instructions, humans follow the instructions faithfully, whereas models do not. Thus, our conclusion is mixed with respect to prior work. We argue against the earlier claim that high performance with irrelevant prompts constitutes evidence against models’ instruction understanding, but we reinforce the claim that models’ failure to follow misleading instructions raises concerns.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, when given deliberately misleading instructions, humans follow the instructions faithfully, whereas models do not. Thus, our conclusion is mixed with respect to prior work. We argue against the earlier claim that high performance with irrelevant prompts constitutes evidence against models’ instruction understanding, but we reinforce the claim that models’ failure to follow misleading instructions raises concerns.\""
    },
    {
        "title": "Generating Extractive Answers: Gated Recurrent Memory Reader for Conversational Question Answering",
        "authors": [
            "Xuanyu Zhang",
            "Qing Yang"
        ],
        "published": "2023",
        "summary": "Conversational question answering (CQA) is a more complicated task than traditional single-turn machine reading comprehension (MRC). Different from large language models (LLMs) like ChatGPT, the models of CQA need to extract answers from given contents to answer follow-up questions according to conversation history. In this paper, we propose a novel architecture, i.e., Gated Recurrent Memory Reader (GRMR), which integrates traditional extractive MRC models into a generalized sequence-to-sequence framework. After the passage is encoded, the decoder will generate the extractive answers turn by turn. Different from previous models that concatenate the previous questions and answers as context superficially and redundantly, our model can use less storage space and consider historical memory deeply and selectively. Experiments on the Conversational Question Answering (CoQA) dataset show that our model achieves comparable results to most models with the least space occupancy.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.516.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Different from large language models (LLMs) like ChatGPT, the models of CQA need to extract answers from given contents to answer follow-up questions according to conversation history.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Different from large language models (LLMs) like ChatGPT, the models of CQA need to extract answers from given contents to answer follow-up questions according to conversation history.\""
    },
    {
        "title": "Impact of Co-occurrence on Factual Knowledge of Large Language Models",
        "authors": [
            "Cheongwoong Kang",
            "Jaesik Choi"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) often make factually incorrect responses despite their success in various applications. In this paper, we hypothesize that relying heavily on simple co-occurrence statistics of the pre-training corpora is one of the main factors that cause factual errors. Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer. Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning. We show that co-occurrence bias remains despite scaling up model sizes or finetuning. Therefore, we suggest finetuning on a debiased dataset to mitigate the bias by filtering out biased samples whose subject-object co-occurrence count is high. Although debiased finetuning allows LLMs to memorize rare facts in the training set, it is not effective in recalling rare facts unseen during finetuning. Further research in mitigation will help build reliable language models by preventing potential errors. The code is available at https://github.com/CheongWoong/impact_of_cooccurrence.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.518.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models (LLMs) often make factually incorrect responses despite their success in various applications. Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer. Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Large language models (LLMs) often make factually incorrect responses despite their success in various applications. Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer. Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they"
    },
    {
        "title": "CTQScorer: Combining Multiple Features for In-context Example Selection for Machine Translation",
        "authors": [
            "Aswanth Kumar",
            "Ratish Puduppully",
            "Raj Dabre",
            "Anoop Kunchukuttan"
        ],
        "published": "2023",
        "summary": "Large language models have demonstrated the capability to perform on machine translation when the input is prompted with a few examples (in-context learning). Translation quality depends on various features of the selected examples, such as their quality and relevance, but previous work has predominantly focused on individual features in isolation. In this paper, we propose a general framework for combining different features influencing example selection. We learn a regression model, CTQ Scorer (Contextual Translation Quality), that selects examples based on multiple features in order to maximize the translation quality. On multiple language pairs and language models, we show that CTQ Scorer helps significantly outperform random selection as well as strong single-factor baselines reported in the literature. We also see an improvement of over 2.5 COMET points on average with respect to a strong BM25 retrieval-based baseline.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.519.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned, but the paper aims to improve the performance of LLMs in machine translation tasks by proposing a framework for combining different features influencing example selection.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned, but the paper aims to improve the performance of LLMs in machine translation tasks by proposing a framework for combining different features influencing example selection."
    },
    {
        "title": "Swap and Predict – Predicting the Semantic Changes in Words across Corpora by Context Swapping",
        "authors": [
            "Taichi Aida",
            "Danushka Bollegala"
        ],
        "published": "2023",
        "summary": "Meanings of words change over time and across domains. Detecting the semantic changes of words is an important task for various NLP applications that must make time-sensitive predictions. We consider the problem of predicting whether a given target word, w, changes its meaning between two different text corpora, 𝒞1 and 𝒞2. For this purpose, we propose Swapping-based Semantic Change Detection (SSCD), an unsupervised method that randomly swaps contexts between 𝒞1 and 𝒞2 where w occurs. We then look at the distribution of contextualised word embeddings of w, obtained from a pretrained masked language model (MLM), representing the meaning of w in its occurrence contexts in 𝒞1 and 𝒞2. Intuitively, if the meaning of w does not change between 𝒞1 and 𝒞2, we would expect the distributions of contextualised word embeddings of w to remain the same before and after this random swapping process. Despite its simplicity, we demonstrate that even by using pretrained MLMs without any fine-tuning, our proposed context swapping method accurately predicts the semantic changes of words in four languages (English, German, Swedish, and Latin) and across different time spans (over 50 years and about five years). Moreover, our method achieves significant performance improvements compared to strong baselines for the English semantic change prediction task. Source code is available at https://github.com/a1da4/svp-swap .",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.520.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but \"using pretrained MLMs without any fine-tuning\" implies a limitation of MLMs that the method tries to overcome.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but \"using pretrained MLMs without any fine-tuning\" implies a limitation of MLMs that the method tries to overcome."
    },
    {
        "title": "Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency",
        "authors": [
            "Lingfeng Shen",
            "Weiting Tan",
            "Boyuan Zheng",
            "Daniel Khashabi"
        ],
        "published": "2023",
        "summary": "With growing capabilities of large language models, prompting them has become the dominant way to access them. This has motivated the development of strategies for automatically selecting effective language prompts. In this paper, we introduce **pFlat** (prompt flatness), a new metric to quantify the expected utility of a language prompt. This metric is inspired by *flatness* regularization in statistical learning that quantifies the robustness of the model towards its parameter perturbations. We provide theoretical foundations for this metric and its relationship with other prompt selection metrics, providing a comprehensive understanding of existing methods. Empirically, we show that combining **pFlat** with existing metrics improves both performance and sample efficiency. Our metric outperforms the previous prompt selection metrics with an average increase of 10% in Pearson correlation across 6 classification benchmarks, and the prompt selected by our metric gains 5% higher accuracy than previous metrics across the benchmarks.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.523.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation of LLMs is mentioned in the abstract, but LLMs are mentioned as the context for the prompt selection strategy.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation of LLMs is mentioned in the abstract, but LLMs are mentioned as the context for the prompt selection strategy."
    },
    {
        "title": "Detecting Erroneously Recognized Handwritten Byzantine Text",
        "authors": [
            "John Pavlopoulos",
            "Vasiliki Kougia",
            "Paraskevi Platanou",
            "Holger Essler"
        ],
        "published": "2023",
        "summary": "Handwritten text recognition (HTR) yields textual output that comprises errors, which are considerably more compared to that of recognised printed (OCRed) text. Post-correcting methods can eliminate such errors but may also introduce errors. In this study, we investigate the issues arising from this reality in Byzantine Greek. We investigate the properties of the texts that lead post-correction systems to this adversarial behaviour and we experiment with text classification systems that learn to detect incorrect recognition output. A large masked language model, pre-trained in modern and fine-tuned in Byzantine Greek, achieves an Average Precision score of 95%. The score improves to 97% when using a model that is pre-trained in modern and then in ancient Greek, the two language forms Byzantine Greek combines elements from. A century-based analysis shows that the advantage of the classifier that is further-pre-trained in ancient Greek concerns texts of older centuries. The application of this classifier before a neural post-corrector on HTRed text reduced significantly the post-correction mistakes.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.524.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Post-correcting methods can eliminate such errors but may also introduce errors.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Post-correcting methods can eliminate such errors but may also introduce errors.\""
    },
    {
        "title": "Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment",
        "authors": [
            "Boyang Xue",
            "Weichao Wang",
            "Hongru Wang",
            "Fei Mi",
            "Rui Wang",
            "Yasheng Wang",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu",
            "Kam-Fai Wong"
        ],
        "published": "2023",
        "summary": "Pretrained language models (PLMs) based knowledge-grounded dialogue systems are prone to generate responses that are factually inconsistent with the provided knowledge source. In such inconsistent responses, the dialogue models fail to accurately express the external factual knowledge they rely upon. Inspired by previous work which identified that feedforward networks (FFNs) within Transformers are responsible for factual knowledge expressions, we investigate two methods to efficiently improve the factual expression capability of FFNs by knowledge enhancement and alignment respectively. We first propose K-Dial, which explicitly introduces extended FFNs in Transformers to enhance factual knowledge expressions given the specific patterns of knowledge-grounded dialogue inputs. Additionally, we apply the reinforcement learning for factual consistency (RLFC) method to implicitly adjust FFNs’ expressions in responses by aligning with gold knowledge for the factual consistency preference. To comprehensively assess the factual consistency and dialogue quality of responses, we employ extensive automatic measures and human evaluations including sophisticated fine-grained NLI-based metrics. Experimental results on WoW and CMU_DoG datasets demonstrate that our methods efficiently enhance the ability of the FFN module to convey factual knowledge, validating the efficacy of improving factual consistency for knowledge-grounded dialogue systems.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.525.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Pretrained language models (PLMs) based knowledge-grounded dialogue systems are prone to generate responses that are factually inconsistent with the provided knowledge source.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Pretrained language models (PLMs) based knowledge-grounded dialogue systems are prone to generate responses that are factually inconsistent with the provided knowledge source.\""
    },
    {
        "title": "Frequency Balanced Datasets Lead to Better Language Models",
        "authors": [
            "Rodolfo Zevallos",
            "Mireia Farrús",
            "Núria Bel"
        ],
        "published": "2023",
        "summary": "This paper reports on the experiments aimed to improve our understanding of the role of the amount of data required for training attention-based transformer language models. Specifically, we investigate the impact of reducing the immense amounts of required pre-training data through sampling strategies that identify and reduce high-frequency tokens as different studies have indicated that the existence of very high-frequency tokens in pre-training data might bias learning, causing undesired effects. In this light, we describe our sampling algorithm that iteratively assesses token frequencies and removes sentences that contain still high-frequency tokens, eventually delivering a balanced, linguistically correct dataset. We evaluate the results in terms of model perplexity and fine-tuning linguistic probing tasks, NLP downstream tasks as well as more semantic SuperGlue tasks. The results show that pre-training with the resulting balanced dataset allows reducing up to three times the pre-training data.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.527.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the existence of very high-frequency tokens in pre-training data might bias learning, causing undesired effects.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the existence of very high-frequency tokens in pre-training data might bias learning, causing undesired effects.\""
    },
    {
        "title": "Uncertainty-aware Parameter-Efficient Self-training for Semi-supervised Language Understanding",
        "authors": [
            "Jianing Wang",
            "Qiushi Sun",
            "Nuo Chen",
            "Chengyu Wang",
            "Jun Huang",
            "Ming Gao",
            "Xiang Li"
        ],
        "published": "2023",
        "summary": "The recent success of large pre-trained language models (PLMs) heavily hinges on massive labeled data, which typically produces inferior performance in low-resource scenarios. To remedy this dilemma, we study self-training as one of the predominant semi-supervised learning (SSL) approaches, which utilizes large-scale unlabeled data to generate synthetic examples. However, too many noisy labels will hurt the model performance, and the self-training procedure requires multiple training iterations making it more expensive if all the model parameters of the PLM are updated. This paper presents UPET, a novel Uncertainty-aware Parameter-Efficient self-Training framework to effectively and efficiently address the labeled data scarcity issue. Specifically, we incorporate Monte Carlo (MC) dropout in Bayesian neural network (BNN) to perform uncertainty estimation for the teacher model and then judiciously select reliable pseudo-labeled examples based on confidence and certainty. During the student training, we introduce multiple parameter-efficient learning (PEL) paradigms that allow optimizes only a small percentage of parameters. We also propose a novel Easy-Hard Contrastive Tuning to enhance the robustness and generalization. Extensive experiments over multiple downstream tasks demonstrate that UPET achieves a substantial improvement in terms of performance and efficiency. Our codes and data are released at https: //github.com/wjn1996/UPET.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.528.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The recent success of large pre-trained language models (PLMs) heavily hinges on massive labeled data, which typically produces inferior performance in low-resource scenarios.\"\n\nThis paper mentions a limitation of LLMs (requirement of massive labeled data and inferior performance in low-resource scenarios) in passing, but does not explore it in depth.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2**\nEvidence: \"The recent success of large pre-trained language models (PLMs) heavily hinges on massive labeled data, which typically produces inferior performance in low-resource scenarios.\"\n\nThis paper mentions a limitation of LLMs (requirement of massive labeled data and inferior performance in low-resource scenarios) in passing, but does not explore it in depth."
    },
    {
        "title": "Analysis of Style-Shifting on Social Media: Using Neural Language Model Conditioned by Social Meanings",
        "authors": [
            "Seiya Kawano",
            "Shota Kanezaki",
            "Angel Fernando Garcia Contreras",
            "Akishige Yuguchi",
            "Marie Katsurai",
            "Koichiro Yoshino"
        ],
        "published": "2023",
        "summary": "In this paper, we propose a novel framework for evaluating style-shifting in social media conversations. Our proposed framework captures changes in an individual’s conversational style based on surprisals predicted by a personalized neural language model for individuals. Our personalized language model integrates not only the linguistic contents of conversations but also non-linguistic factors, such as social meanings, including group membership, personal attributes, and individual beliefs. We incorporate these factors directly or implicitly into our model, leveraging large, pre-trained language models and feature vectors derived from a relationship graph on social media. Compared to existing models, our personalized language model demonstrated superior performance in predicting an individual’s language in a test set. Furthermore, an analysis of style-shifting utilizing our proposed metric based on our personalized neural language model reveals a correlation between our metric and various conversation factors as well as human evaluation of style-shifting.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.531.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"leveraging large, pre-trained language models\"\n\nThis paper mentions pre-trained language models but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"leveraging large, pre-trained language models\"\n\nThis paper mentions pre-trained language models but does not discuss any limitations of LLMs."
    },
    {
        "title": "MCLF: A Multi-grained Contrastive Learning Framework for ASR-robust Spoken Language Understanding",
        "authors": [
            "Zhiqi Huang",
            "Dongsheng Chen",
            "Zhihong Zhu",
            "Xuxin Cheng"
        ],
        "published": "2023",
        "summary": "Enhancing the robustness towards Automatic Speech Recognition (ASR) errors is of great importance for Spoken Language Understanding (SLU). Trending ASR-robust SLU systems have witnessed impressive improvements through global contrastive learning. However, although most ASR errors occur only at local positions of utterances, they can easily lead to severe semantic changes, and utterance-level classification or comparison is difficult to distinguish such differences. To address the problem, we propose a two-stage multi-grained contrastive learning framework dubbed MCLF. Technically, we first adapt the pre-trained language models to downstream SLU datasets via the proposed multi-grained contrastive learning objective and then fine-tune it on the corresponding dataset. Besides, to facilitate contrastive learning in the pre-training stage, we explore several data augmentation methods to expand the training data. Experimental results and detailed analyses on four datasets and four BERT-like backbone models demonstrate the effectiveness of our approach.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.533.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"although most ASR errors occur only at local positions of utterances, they can easily lead to severe semantic changes, and utterance-level classification or comparison is difficult to distinguish such differences.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"although most ASR errors occur only at local positions of utterances, they can easily lead to severe semantic changes, and utterance-level classification or comparison is difficult to distinguish such differences.\""
    },
    {
        "title": "Beyond Candidates : Adaptive Dialogue Agent Utilizing Persona and Knowledge",
        "authors": [
            "Jungwoo Lim",
            "Myunghoon Kang",
            "Jinsung Kim",
            "Jeongwook Kim",
            "Yuna Hur",
            "Heuiseok Lim"
        ],
        "published": "2023",
        "summary": "To build ultimate dialogue agents, previous studies suggest models that ground both persona and knowledge. However, applying the dialogue system directly to the usual conversation is still limited because the system requires a complete sentence-formed persona and knowledge candidate sets from the given dataset. In contrast to the dialogue setting in the dataset, humans utilize semantic concepts in their minds rather than a set of pre-defined candidate sentences. Following this manner of human dialogue, we suggest an adaptive dialogue system that is applicable to situations where complete sentence-formed candidates are not given. Our model generates consistent and relevant persona descriptions and identifies relevant knowledge for engaging and knowledgeable responses, even with fragmentary information. We show that our model outperforms previous baselines that utilize persona and knowledge candidate sentences and conduct the human evaluation on the machine-generated responses. In addition, we conduct ablation studies to demonstrate the effectiveness of each component of our model. Furthermore, we apply our model to other dialogue datasets that only ground knowledge or persona to showcase its adaptability. Our code is available at https://github.com/dlawjddn803/BeCand.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.534.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding",
        "authors": [
            "Uri Shaham",
            "Maor Ivgi",
            "Avia Efrat",
            "Jonathan Berant",
            "Omer Levy"
        ],
        "published": "2023",
        "summary": "We introduce ZeroSCROLLS, a zero-shot benchmark for natural language understanding over long texts, which contains only test and small validation sets, without training data. We adapt six tasks from the SCROLLS benchmark, and add four new datasets, including two novel information fusing tasks, such as aggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a comprehensive evaluation of both open-source and closed large language models, finding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest average score. However, there is still room for improvement on multiple open challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to pass the naive baseline. As the state of the art is a moving target, we invite researchers to evaluate their ideas on the live ZeroSCROLLS leaderboard.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.536.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, there is still room for improvement on multiple open challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to pass the naive baseline.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, there is still room for improvement on multiple open challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to pass the naive baseline.\""
    },
    {
        "title": "Romanization-based Large-scale Adaptation of Multilingual Language Models",
        "authors": [
            "Sukannya Purkayastha",
            "Sebastian Ruder",
            "Jonas Pfeiffer",
            "Iryna Gurevych",
            "Ivan Vulić"
        ],
        "published": "2023",
        "summary": "Large multilingual pretrained language models (mPLMs) have become the de facto state of the art for cross-lingual transfer in NLP. However, their large-scale deployment to many languages, besides pretraining data scarcity, is also hindered by the increase in vocabulary size and limitations in their parameter budget. In order to boost the capacity of mPLMs to deal with low-resource and unseen languages, we explore the potential of leveraging transliteration on a massive scale. In particular, we explore the UROMAN transliteration tool, which provides mappings from UTF-8 to Latin characters for all the writing systems, enabling inexpensive romanization for virtually any language. We first focus on establishing how UROMAN compares against other language-specific and manually curated transliterators for adapting multilingual PLMs. We then study and compare a plethora of data- and parameter-efficient strategies for adapting the mPLMs to romanized and non-romanized corpora of 14 diverse low-resource languages. Our results reveal that UROMAN-based transliteration can offer strong performance for many languages, with particular gains achieved in the most challenging setups: on languages with unseen scripts and with limited training data without any vocabulary augmentation. Further analyses reveal that an improved tokenizer based on romanized data can even outperform non-transliteration-based methods in the majority of languages.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.538.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their large-scale deployment to many languages, besides pretraining data scarcity, is also hindered by the increase in vocabulary size and limitations in their parameter budget.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, their large-scale deployment to many languages, besides pretraining data scarcity, is also hindered by the increase in vocabulary size and limitations in their parameter budget.\""
    },
    {
        "title": "Measuring bias in Instruction-Following models with P-AT",
        "authors": [
            "Dario Onorati",
            "Elena Sofia Ruzzetti",
            "Davide Venditti",
            "Leonardo Ranaldi",
            "Fabio Massimo Zanzotto"
        ],
        "published": "2023",
        "summary": "Instruction-Following Language Models (IFLMs) are promising and versatile tools for solving many downstream, information-seeking tasks. Given their success, there is an urgent need to have a shared resource to determine whether existing and new IFLMs are prone to produce biased language interactions. In this paper, we propose Prompt Association Test (P-AT): a new resource for testing the presence of social biases in IFLMs. P-AT stems from WEAT (Caliskan et al., 2017) and generalizes the notion of measuring social biases to IFLMs. Basically, we cast WEAT word tests in promptized classification tasks, and we associate a metric - the bias score. Our resource consists of 2310 prompts. We then experimented with several families of IFLMs discovering gender and race biases in all the analyzed models. We expect P-AT to be an important tool for quantifying bias across different dimensions and, therefore, for encouraging the creation of fairer IFLMs before their distortions have consequences in the real world.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.539.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We then experimented with several families of IFLMs discovering gender and race biases in all the analyzed models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We then experimented with several families of IFLMs discovering gender and race biases in all the analyzed models.\""
    },
    {
        "title": "Open-ended Commonsense Reasoning with Unrestricted Answer Candidates",
        "authors": [
            "Chen Ling",
            "Xuchao Zhang",
            "Xujiang Zhao",
            "Yanchi Liu",
            "Wei Cheng",
            "Mika Oishi",
            "Takao Osaki",
            "Katsushi Matsuda",
            "Haifeng Chen",
            "Liang Zhao"
        ],
        "published": "2023",
        "summary": "Open-ended Commonsense Reasoning is defined as solving a commonsense question without providing 1) a short list of answer candidates and 2) a pre-defined answer scope. Conventional ways of formulating the commonsense question into a question-answering form or utilizing external knowledge to learn retrieval-based methods are less applicable in the open-ended setting due to an inherent challenge. Without pre-defining an answer scope or a few candidates, open-ended commonsense reasoning entails predicting answers by searching over an extremely large searching space. Moreover, most questions require implicit multi-hop reasoning, which presents even more challenges to our problem. In this work, we leverage pre-trained language models to iteratively retrieve reasoning paths on the external knowledge base, which does not require task-specific supervision. The reasoning paths can help to identify the most precise answer to the commonsense question. We conduct experiments on two commonsense benchmark datasets. Compared to other approaches, our proposed method achieves better performance both quantitatively and qualitatively.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.540.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Without pre-defining an answer scope or a few candidates, open-ended commonsense reasoning entails predicting answers by searching over an extremely large searching space. Moreover, most questions require implicit multi-hop reasoning, which presents even more challenges to our problem.\"\n\nThis evidence suggests that the paper mentions some challenges related to open-ended commonsense reasoning with large language models, but it is not the primary focus",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Without pre-defining an answer scope or a few candidates, open-ended commonsense reasoning entails predicting answers by searching over an extremely large searching space. Moreover, most questions require implicit multi-hop reasoning, which presents even more challenges to our problem.\"\n\nThis evidence suggests that the paper mentions some challenges related to open-ended commonsense reasoning with large language models, but it is not the primary focus"
    },
    {
        "title": "New Datasets and Controllable Iterative Data Augmentation Method for Code-switching ASR Error Correction",
        "authors": [
            "Zhaohong Wan",
            "Xiaojun Wan",
            "Wei Peng",
            "Rongjun Li"
        ],
        "published": "2023",
        "summary": "With the wide use of automatic speech recognition(ASR) systems, researchers pay more attention to the ASR error correction task to improve the quality of recognition results. In particular, ASR in bilingual or multilingual settings, namely code-switching ASR, has greater challenges and research value. In this paper, we first present code-switching ASR correction datasets obtained from solid ASR systems and automatic annotators. The datasets contain Chinese-English code-switching dialogues of bilingual speakers in Singapore, Malaysia, and Hong Kong. Based on this task, we propose a controllable iterative (CI) data augmentation method for improving the performance of mainstream ASR error correction systems. With a small amount of training data, our proposed method has the ability to iteratively produce abundant pseudo parallel data from the monolingual corpus for Chinese-English code-switching ASR correction. Results of experiments show that our method achieves the best performance compared with the rule-based, back-translation-based data augmentation methods and large language model ChatGPT.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.543.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"large language model ChatGPT\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"large language model ChatGPT\""
    },
    {
        "title": "Isotropic Representation Can Improve Zero-Shot Cross-Lingual Transfer on Multilingual Language Models",
        "authors": [
            "Yixin Ji",
            "Jikai Wang",
            "Juntao Li",
            "Hai Ye",
            "Min Zhang"
        ],
        "published": "2023",
        "summary": "With the development of multilingual pre-trained language models (mPLMs), zero-shot cross-lingual transfer shows great potential. To further improve the performance of cross-lingual transfer, many studies have explored representation misalignment caused by morphological differences but neglected the misalignment caused by the anisotropic distribution of contextual representations. In this work, we propose enhanced isotropy and constrained code-switching for zero-shot cross-lingual transfer to alleviate the problem of misalignment caused by the anisotropic representations and maintain syntactic structural knowledge. Extensive experiments on three zero-shot cross-lingual transfer tasks demonstrate that our method gains significant improvements over strong mPLM backbones and further improves the state-of-the-art methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.545.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"representation misalignment caused by morphological differences\" and \"misalignment caused by the anisotropic distribution of contextual representations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"representation misalignment caused by morphological differences\" and \"misalignment caused by the anisotropic distribution of contextual representations.\""
    },
    {
        "title": "Blackbird language matrices (BLM), a new task for rule-like generalization in neural networks: Can Large Language Models pass the test?",
        "authors": [
            "Paola Merlo"
        ],
        "published": "2023",
        "summary": "How do we evaluate Large Language Models (LLMs) and determine the aspects and limits of their intelligent behaviour? It is currently conjectured that shortcomings of LLMs in multi-linguality and reasoning are due to a lack of ability to generalize. It has been argued that, instead, humans are better at generalization because they have a tendency at extracting rules from complex data. We propose a method to evaluate LLMs ability to rule-based generalization. When exposed to tests of analytic intelligence, for example the visual RAVEN IQ test, human problem-solvers identify the relevant objects in the picture and their relevant attributes and reason based on rules applied to them. Based on the induced rules, they are able to provide a generalisation and a solution to the test. An analogous language task has recently been proposed (called BLM) for LLM. In this paper, we argue that we can use this task to investigate what linguistic reasoning LLM develop, by asking them to solve some simple variants of the BLM task. We find that current state-of-the-art generative models, such as ChatGPT, can handle the task in the sense that they easily understand the instructions and can provide step-by-step reasoning that shows that it can solve two of the main cognitive hurdles: correspondence finding (object and attribute identification) and item novelty. However, overall they cannot find the correct answer, even with considerable help. In particular, they never identify the structure of the problem, exhibiting, we hypothesize, a lack of goal and subgoal management abilities, an ability that has been argued to measure differential abilities in humans. We argue that this finding supports the usefulness of the task as a method to test the limits and specific properties of generalisation ability in Large Language Models, providing an intrinsic evaluation method inspired by tests of human intelligence.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.546.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, overall they cannot find the correct answer, even with considerable help. In particular, they never identify the structure of the problem, exhibiting, we hypothesize, a lack of goal and subgoal management abilities, an ability that has been argued to measure differential abilities in humans.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"However, overall they cannot find the correct answer, even with considerable help. In particular, they never identify the structure of the problem, exhibiting, we hypothesize, a lack of goal and subgoal management abilities, an ability that has been argued to measure differential abilities in humans.\""
    },
    {
        "title": "Getting MoRE out of Mixture of Language Model Reasoning Experts",
        "authors": [
            "Chenglei Si",
            "Weijia Shi",
            "Chen Zhao",
            "Luke Zettlemoyer",
            "Jordan Boyd-Graber"
        ],
        "published": "2023",
        "summary": "While recent large language models (LLMs) improve on various question answering (QA) datasets, it remains difficult for a single model to generalize across question types that require distinct reasoning abilities. We provide empirical evidence that state-of-the-art LLMs suffer from poor generalizability on reasoning types beyond those seen in the prompt. To remedy this, we propose a Mixture-of-Reasoning-Experts (MORE) framework that ensembles diverse specialized language models. We specialize the backbone language model with prompts optimized for different reasoning categories, including factual, multihop, mathematical, and commonsense reasoning. Our key insight is to leverage agreement among the specialized experts to select the best answer for each question, or to abstain from answering. This gives MORE higher accuracy than any single specialized model on a collection of 12 QA datasets from four reasoning types. Beyond generalizability, the interpretable design of MORE improves selective question answering results compared to baselines without incorporating inter-expert agreement. This framework is also more interpretable and useful to human consumers of QA outputs. Our human study confirms that presenting expert predictions and the answer selection process helps annotators more accurately calibrate when to trust the system’s output. We release all code and data to facilitate future work.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.552.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"it remains difficult for a single model to generalize across question types that require distinct reasoning abilities. We provide empirical evidence that state-of-the-art LLMs suffer from poor generalizability on reasoning types beyond those seen in the prompt.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"it remains difficult for a single model to generalize across question types that require distinct reasoning abilities. We provide empirical evidence that state-of-the-art LLMs suffer from poor generalizability on reasoning types beyond those seen in the prompt.\""
    },
    {
        "title": "“You Are An Expert Linguistic Annotator”: Limits of LLMs as Analyzers of Abstract Meaning Representation",
        "authors": [
            "Allyson Ettinger",
            "Jena Hwang",
            "Valentina Pyatkin",
            "Chandra Bhagavatula",
            "Yejin Choi"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) demonstrate an amazing proficiency and fluency in the use of language. Does that mean that they have also acquired insightful linguistic knowledge about the language, to an extent that they can serve as an “expert linguistic annotator’? In this paper, we examine the successes and limitations of the GPT-3, ChatGPT, and GPT-4 models, focusing on the Abstract Meaning Representation (AMR) parsing formalism (Banarescu et al., 2013), which provides rich graphical representations of sentence meaning structure while abstracting away from surface forms. We compare models’ analysis of this semantic structure across two settings: 1) direct production of AMR parses based on zero- and few-shot examples, and 2) indirect partial reconstruction of AMR via metalinguistic natural language queries (e.g., “Identify the primary event of this sentence, and the predicate corresponding to that event.”). Across these settings, we find that models can reliably reproduce the basic format of AMR, as well as some core event, argument, and modifier structure-however, model outputs are prone to frequent and major errors, and holistic analysis of parse acceptability shows that even with few-shot demonstrations, models have virtually 0% success in producing fully accurate parses. Eliciting responses in natural language produces similar patterns of errors. Overall, our findings indicate that these models out-of-the-box can accurately identify some core aspects of semantic structure, but there remain key limitations in their ability to support fully accurate semantic analyses or parses.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.553.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"model outputs are prone to frequent and major errors, and holistic analysis of parse acceptability shows that even with few-shot demonstrations, models have virtually 0% success in producing fully accurate parses.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"model outputs are prone to frequent and major errors, and holistic analysis of parse acceptability shows that even with few-shot demonstrations, models have virtually 0% success in producing fully accurate parses.\""
    },
    {
        "title": "Isotropy-Enhanced Conditional Masked Language Models",
        "authors": [
            "Pei Guo",
            "Yisheng Xiao",
            "Juntao Li",
            "Yixin Ji",
            "Min Zhang"
        ],
        "published": "2023",
        "summary": "Non-autoregressive models have been widely used for various text generation tasks to accelerate the inference process but at the cost of generation quality to some extent. To achieve a good balance between inference speedup and generation quality, iterative NAR models like CMLM and Disco are proposed. Researchers have made much follow-up progress based on them, and some recent iterative models can achieve very promising performance while maintaining significant speedup. In this paper, we give more insights into iterative NAR models by exploring the anisotropic problem, i.e., the representations of distinct predicted target tokens are similar and indiscriminative. Upon the confirmation of the anisotropic problem in iterative NAR models, we first analyze the effectiveness of the contrastive learning method and further propose the Look Neighbors strategy to enhance the learning of token representations during training. Experiments on 4 WMT datasets show that our methods consistently improve the performance as well as alleviate the anisotropic problem of the conditional masked language model, even outperforming the current SoTA result on WMT14 EN → DE.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.555.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Non-autoregressive models have been widely used for various text generation tasks to accelerate the inference process but at the cost of generation quality to some extent.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Non-autoregressive models have been widely used for various text generation tasks to accelerate the inference process but at the cost of generation quality to some extent.\""
    },
    {
        "title": "Scaling Law for Document Neural Machine Translation",
        "authors": [
            "Zhang Zhuocheng",
            "Shuhao Gu",
            "Min Zhang",
            "Yang Feng"
        ],
        "published": "2023",
        "summary": "The scaling laws of language models have played a significant role in advancing large language models. In order to promote the development of document translation, we systematically examine the scaling laws in this field. In this paper, we carry out an in-depth analysis of the influence of three factors on translation quality: model scale, data scale, and sequence length. Our findings reveal that increasing sequence length effectively enhances model performance when model size is limited. However, sequence length cannot be infinitely extended; it must be suitably aligned with the model scale and corpus volume. Further research shows that providing adequate context can effectively enhance the translation quality of a document’s initial portion. Nonetheless, exposure bias remains the primary factor hindering further improvement in translation quality for the latter half of the document.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.556.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, sequence length cannot be infinitely extended; it must be suitably aligned with the model scale and corpus volume. Further research shows that providing adequate context can effectively enhance the translation quality of a document’s initial portion. Nonetheless, exposure bias remains the primary factor hindering further improvement in translation quality for the latter half of the document.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, sequence length cannot be infinitely extended; it must be suitably aligned with the model scale and corpus volume. Further research shows that providing adequate context can effectively enhance the translation quality of a document’s initial portion. Nonetheless, exposure bias remains the primary factor hindering further improvement in translation quality for the latter half of the document.\""
    },
    {
        "title": "Monte Carlo Thought Search: Large Language Model Querying for Complex Scientific Reasoning in Catalyst Design",
        "authors": [
            "Henry Sprueill",
            "Carl Edwards",
            "Mariefel Olarte",
            "Udishnu Sanyal",
            "Heng Ji",
            "Sutanay Choudhury"
        ],
        "published": "2023",
        "summary": "Discovering novel catalysts requires complex reasoning involving multiple chemical properties and resultant trade-offs, leading to a combinatorial growth in the search space. While large language models (LLM) have demonstrated novel capabilities for chemistry through complex instruction following capabilities and high quality reasoning, a goal-driven combinatorial search using LLMs has not been explored in detail. In this work, we present a Monte Carlo Tree Search-based approach that improves beyond state-of-the-art chain-of-thought prompting variants to augment scientific reasoning. We introduce two new reasoning datasets: 1) a curation of computational chemistry simulations, and 2) diverse questions written by catalysis researchers for reasoning about novel chemical conversion processes. We improve over the best baseline by 25.8% and find that our approach can augment scientist’s reasoning and discovery process with novel insights.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.560.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"a goal-driven combinatorial search using LLMs has not been explored in detail.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"a goal-driven combinatorial search using LLMs has not been explored in detail.\""
    },
    {
        "title": "Injecting structural hints: Using language models to study inductive biases in language learning",
        "authors": [
            "Isabel Papadimitriou",
            "Dan Jurafsky"
        ],
        "published": "2023",
        "summary": "Both humans and transformer language models are able to learn language without explicit structural supervision. What cognitive inductive biases make this learning possible? Here, we examine the effect of different inductive learning biases by actively controlling the inductive biases of artificial learners: we structurally bias models by pretraining on synthetic formally-structured data, and evaluate these structural biases by fine-tuning on three typologically-distant human languages: English, Japanese, and Basque. We investigate the effect on downstream language perplexity of three types of inductive bias: 1) recursive, hierarchical processing 2) unrestricted token-token dependencies that can’t be modeled by context-free grammars, and 3) a Zipfian power-law vocabulary distribution. We show that complex, non-context-free interactions between tokens form the best inductive biases. Our study leverages the capabilities of transformer models to run controlled language learning experiments that are not possible to run on humans, and surfaces hypotheses about the structures that facilitate language learning in both humans and machines.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.563.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but the study of inductive biases in language learning could be interpreted as a limitation of current LLMs in terms of understanding and replicating human language learning.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None explicitly mentioned, but the study of inductive biases in language learning could be interpreted as a limitation of current LLMs in terms of understanding and replicating human language learning."
    },
    {
        "title": "Unleashing the Power of Language Models in Text-Attributed Graph",
        "authors": [
            "Haoyu Kuang",
            "Jiarong Xu",
            "Haozhe Zhang",
            "Zuyu Zhao",
            "Qi Zhang",
            "Xuanjing Huang",
            "Zhongyu Wei"
        ],
        "published": "2023",
        "summary": "Representation learning on graph has been demonstrated to be a powerful tool for solving real-world problems. Text-attributed graph carries both semantic and structural information among different types of graphs. Existing works have paved the way for knowledge extraction of this type of data by leveraging language models or graph neural networks or combination of them. However, these works suffer from issues like underutilization of relationships between nodes or words or unaffordable memory cost. In this paper, we propose a Node Representation Update Pre-training Architecture based on Co-modeling Text and Graph (NRUP). In NRUP, we construct a hierarchical text-attributed graph that incorporates both original nodes and word nodes. Meanwhile, we apply four self-supervised tasks for different level of constructed graph. We further design the pre-training framework to update the features of nodes during training epochs. We conduct the experiment on the benchmark dataset ogbn-arxiv. Our method achieves outperformance compared to baselines, fully demonstrating its validity and generalization.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.565.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these works suffer from issues like underutilization of relationships between nodes or words or unaffordable memory cost.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these works suffer from issues like underutilization of relationships between nodes or words or unaffordable memory cost.\""
    },
    {
        "title": "Locally Differentially Private Document Generation Using Zero Shot Prompting",
        "authors": [
            "Saiteja Utpala",
            "Sara Hooker",
            "Pin-Yu Chen"
        ],
        "published": "2023",
        "summary": "Numerous studies have highlighted the privacy risks associated with large language models. Our research offers a unique perspective by demonstrating that pretrained large language models can effectively contribute to privacy preservation. We propose a locally differentially private mechanism called DP-Prompt, which leverages the power of pretrained large language models and zero-shot prompting to counter author de-anonymization attacks while minimizing the impact on downstream utility. When DP-Prompt is used with a powerful language model like ChatGPT (gpt-3.5), we observe a notable reduction in the success rate of de-anonymization attacks, showing that it surpasses existing approaches by a considerable margin despite its simpler design. For instance, in the case of the IMDB dataset, DP-Prompt (with ChatGPT) perfectly recovers the clean sentiment F1 score while achieving a 46% reduction in author identification F1 score against static attackers and a 26% reduction against adaptive attackers. We conduct extensive experiments across six open-source large language models, ranging up to 7 billion parameters, to analyze various effects of the privacy-utility tradeoff.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.566.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Numerous studies have highlighted the privacy risks associated with large language models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Numerous studies have highlighted the privacy risks associated with large language models.\""
    },
    {
        "title": "Contrastive Deterministic Autoencoders For Language Modeling",
        "authors": [
            "Amur Ghose",
            "Pascal Poupart"
        ],
        "published": "2023",
        "summary": "Variational autoencoders (VAEs) are a popular family of generative models with wide applicability. Training VAEs, especially for text, often runs into the issue of posterior collapse, resulting in loss of representation quality. Deterministic autoencoders avoid this issue, and have been explored particularly well for images. It is however unclear how to best modify a deterministic model designed for images into a successful one for text. We show that with suitable adaptations, we can significantly improve on batch-normed VAEs (BN-VAEs), a strong benchmark for language modeling with VAEs, by replacing them with analogous deterministic models. We employ techniques from contrastive learning to control the entropy of the aggregate posterior of these models to make it Gaussian. The resulting models skip reparametrization steps in VAE modeling and avoid posterior collapse, while outperforming a broad range of VAE models on text generation and downstream tasks from representations. These improvements are shown to be consistent across both LSTM and Transformer-based VAE architectures. Appropriate comparisons to BERT/GPT-2 based results are also included. We also qualitatively examine the latent space through interpolation to supplement the quantitative aspects of the model.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.567.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models",
        "authors": [
            "Denis McInerney",
            "Geoffrey Young",
            "Jan-Willem van de Meent",
            "Byron Wallace"
        ],
        "published": "2023",
        "summary": "We propose CHiLL (Crafting High-Level Latents), an approach for natural-language specification of features for linear models. CHiLL prompts LLMs with expert-crafted queries to generate interpretable features from health records. The resulting noisy labels are then used to train a simple linear classifier. Generating features based on queries to an LLM can empower physicians to use their domain expertise to craft features that are clinically meaningful for a downstream task of interest, without having to manually extract these from raw EHR. We are motivated by a real-world risk prediction task, but as a reproducible proxy, we use MIMIC-III and MIMIC-CXR data and standard predictive tasks (e.g., 30-day readmission) to evaluate this approach. We find that linear models using automatically extracted features are comparably performant to models using reference features, and provide greater interpretability than linear models using “Bag-of-Words” features. We verify that learned feature weights align well with clinical expectations.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.568.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, but \"noisy labels\" may imply some limitations in the quality of the generated features.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, but \"noisy labels\" may imply some limitations in the quality of the generated features."
    },
    {
        "title": "Guiding LLM to Fool Itself: Automatically Manipulating Machine Reading Comprehension Shortcut Triggers",
        "authors": [
            "Mosh Levy",
            "Shauli Ravfogel",
            "Yoav Goldberg"
        ],
        "published": "2023",
        "summary": "Recent applications of LLMs in Machine Reading Comprehension (MRC) systems have shown impressive results, but the use of shortcuts, mechanisms triggered by features spuriously correlated to the true label, has emerged as a potential threat to their reliability. We analyze the problem from two angles: LLMs as editors, guided to edit text to mislead LLMs; and LLMs as readers, who answer questions based on the edited text. We introduce a framework that guides an editor to add potential shortcuts-triggers to samples. Using GPT4 as the editor, we find it can successfully edit trigger shortcut in samples that fool LLMs. Analysing LLMs as readers, we observe that even capable LLMs can be deceived using shortcut knowledge. Strikingly, we discover that GPT4 can be deceived by its own edits (15% drop in F1). Our findings highlight inherent vulnerabilities of LLMs to shortcut manipulations. We publish ShortcutQA, a curated dataset generated by our framework for future research.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.569.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We analyze the problem from two angles: LLMs as editors, guided to edit text to mislead LLMs; and LLMs as readers, who answer questions based on the edited text... Our findings highlight inherent vulnerabilities of LLMs to shortcut manipulations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We analyze the problem from two angles: LLMs as editors, guided to edit text to mislead LLMs; and LLMs as readers, who answer questions based on the edited text... Our findings highlight inherent vulnerabilities of LLMs to shortcut manipulations.\""
    },
    {
        "title": "Large Language Models Meet Harry Potter: A Dataset for Aligning Dialogue Agents with Characters",
        "authors": [
            "Nuo Chen",
            "Yan Wang",
            "Haiyun Jiang",
            "Deng Cai",
            "Yuhan Li",
            "Ziyang Chen",
            "Longyue Wang",
            "Jia Li"
        ],
        "published": "2023",
        "summary": "In recent years, Dialogue-style Large Language Models (LLMs) such as ChatGPT and GPT4 have demonstrated immense potential in constructing open-domain dialogue agents. However, aligning these agents with specific characters or individuals remains a considerable challenge due to the complexities of character representation and the lack of comprehensive annotations. In this paper, we introduce the Harry Potter Dialogue (HPD) dataset, designed to advance the study of dialogue agents and character alignment. The dataset encompasses all dialogue sessions (in both English and Chinese) from the Harry Potter series and is annotated with vital background information, including dialogue scenes, speakers, character relationships, and attributes. These extensive annotations may empower LLMs to unlock character-driven dialogue capabilities. Furthermore, it can serve as a universal benchmark for evaluating how well can a LLM aligning with a specific character. We benchmark LLMs on HPD using both fine-tuning and in-context learning settings. Evaluation results reveal that although there is substantial room for improvement in generating high-quality, character-aligned responses, the proposed dataset is valuable in guiding models toward responses that better align with the character of Harry Potter.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.570.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, aligning these agents with specific characters or individuals remains a considerable challenge due to the complexities of character representation and the lack of comprehensive annotations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, aligning these agents with specific characters or individuals remains a considerable challenge due to the complexities of character representation and the lack of comprehensive annotations.\""
    },
    {
        "title": "SIR-ABSC: Incorporating Syntax into RoBERTa-based Sentiment Analysis Models with a Special Aggregator Token",
        "authors": [
            "Ikhyun Cho",
            "Yoonhwa Jung",
            "Julia Hockenmaier"
        ],
        "published": "2023",
        "summary": "We present a simple, but effective method to incorporate syntactic dependency information directly into transformer-based language models (e.g. RoBERTa) for tasks such as Aspect-Based Sentiment Classification (ABSC), where the desired output depends on specific input tokens. In contrast to prior approaches to ABSC that capture syntax by combining language models with graph neural networks over dependency trees, our model, Syntax-Integrated RoBERTa for ABSC (SIR-ABSC) incorporates syntax directly into the language model by using a novel aggregator token. Yet, SIR-ABSC outperforms these more complex models, yielding new state-of-the-art results on ABSC.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.572.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"In contrast to prior approaches to ABSC that capture syntax by combining language models with graph neural networks over dependency trees\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"In contrast to prior approaches to ABSC that capture syntax by combining language models with graph neural networks over dependency trees\""
    },
    {
        "title": "Intersectional Stereotypes in Large Language Models: Dataset and Analysis",
        "authors": [
            "Weicheng Ma",
            "Brian Chiang",
            "Tong Wu",
            "Lili Wang",
            "Soroush Vosoughi"
        ],
        "published": "2023",
        "summary": "Despite many stereotypes targeting intersectional demographic groups, prior studies on stereotypes within Large Language Models (LLMs) primarily focus on broader, individual categories. This research bridges this gap by introducing a novel dataset of intersectional stereotypes, curated with the assistance of the ChatGPT model and manually validated. Moreover, this paper offers a comprehensive analysis of intersectional stereotype propagation in three contemporary LLMs by leveraging this dataset. The findings underscore the urgency of focusing on intersectional biases in ongoing efforts to reduce stereotype prevalence in LLMs.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.575.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite many stereotypes targeting intersectional demographic groups, prior studies on stereotypes within Large Language Models (LLMs) primarily focus on broader, individual categories... The findings underscore the urgency of focusing on intersectional biases in ongoing efforts to reduce stereotype prevalence in LLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Despite many stereotypes targeting intersectional demographic groups, prior studies on stereotypes within Large Language Models (LLMs) primarily focus on broader, individual categories... The findings underscore the urgency of focusing on intersectional biases in ongoing efforts to reduce stereotype prevalence in LLMs.\""
    },
    {
        "title": "Retrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model Compression",
        "authors": [
            "Jiduan Liu",
            "Jiahao Liu",
            "Qifan Wang",
            "Jingang Wang",
            "Xunliang Cai",
            "Dongyan Zhao",
            "Ran Wang",
            "Rui Yan"
        ],
        "published": "2023",
        "summary": "Large-scale pre-trained language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks. However, the massive size of these models poses huge challenges for their deployment in real-world applications. While numerous model compression techniques have been proposed, most of them are not well-suited for achieving extreme model compression when there is a significant gap in model scale. In this paper, we introduce a novel compression paradigm called Retrieval-based Knowledge Transfer (RetriKT), which effectively transfers the knowledge of LLMs to extremely small-scale models (e.g., 1%). In particular, our approach extracts knowledge from LLMs to construct a knowledge store, from which the small-scale model can retrieve relevant information and leverage it for effective inference. To improve the quality of the model, soft prompt tuning and Proximal Policy Optimization (PPO) reinforcement learning techniques are employed. Extensive experiments are conducted on low-resource tasks from SuperGLUE and GLUE benchmarks. The results demonstrate that the proposed approach significantly enhances the performance of small-scale models by leveraging the knowledge from LLMs.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.578.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the massive size of these models poses huge challenges for their deployment in real-world applications.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, the massive size of these models poses huge challenges for their deployment in real-world applications.\""
    },
    {
        "title": "KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion",
        "authors": [
            "Yanbin Wei",
            "Qiushi Huang",
            "Yu Zhang",
            "James Kwok"
        ],
        "published": "2023",
        "summary": "Knowledge Graph Completion (KGC) is crucial for addressing knowledge graph incompleteness and supporting downstream applications. Many models have been proposed for KGC and they can be categorized into two main classes, including triple-based and test-based approaches. Triple-based methods struggle with long-tail entities due to limited structural information and imbalanced distributions of entities. Text-based methods alleviate this issue but require costly training for language models and specific finetuning for knowledge graphs, which limits their efficiency. To alleviate the limitations in the two approaches, in this paper, we propose KICGPT, a framework that integrates a large language model (LLM) and a triple-based KGC retriever, to alleviate the long-tail problem without incurring additional training overhead. In the proposed KICGPT model, we propose an in-context learning strategy called Knowledge Prompt, which encodes structural knowledge into demonstrations to guide LLM. Empirical results on benchmark datasets demonstrate the effectiveness of the proposed KICGPT model with lighter training overhead and no finetuning.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.580.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Text-based methods... require costly training for language models and specific finetuning for knowledge graphs, which limits their efficiency.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Text-based methods... require costly training for language models and specific finetuning for knowledge graphs, which limits their efficiency.\""
    },
    {
        "title": "Show, Write, and Retrieve: Entity-aware Article Generation and Retrieval",
        "authors": [
            "Zhongping Zhang",
            "Yiwen Gu",
            "Bryan Plummer"
        ],
        "published": "2023",
        "summary": "Article comprehension is an important challenge in natural language processing with many applications such as article generation or image-to-article retrieval. Prior work typically encodes all tokens in articles uniformly using pretrained language models. However, in many applications, such as understanding news stories, these articles are based on real-world events and may reference many named entities that are difficult to accurately recognize and predict by language models. To address this challenge, we propose an ENtity-aware article GeneratIoN and rEtrieval (ENGINE) framework, to explicitly incorporate named entities into language models. ENGINE has two main components: a named-entity extraction module to extract named entities from both metadata and embedded images associated with articles, and an entity-aware mechanism that enhances the model’s ability to recognize and predict entity names. We conducted experiments on three public datasets: GoodNews, VisualNews, and WikiText, where our results demonstrate that our model can boost both article generation and article retrieval performance, with a 4-5 perplexity improvement in article generation and a 3-4% boost in recall@1 in article retrieval. We release our implementation at [this http URL](https://github.com/Zhongping-Zhang/ENGINE).",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.581.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"named entities that are difficult to accurately recognize and predict by language models.\"\n\nThis rating is given because the abstract mentions a limitation of language models in recognizing and predicting named entities, but it is not the primary focus of the paper and is used to motivate the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"named entities that are difficult to accurately recognize and predict by language models.\"\n\nThis rating is given because the abstract mentions a limitation of language models in recognizing and predicting named entities, but it is not the primary focus of the paper and is used to motivate the proposed solution."
    },
    {
        "title": "A Language Model with Limited Memory Capacity Captures Interference in Human Sentence Processing",
        "authors": [
            "William Timkey",
            "Tal Linzen"
        ],
        "published": "2023",
        "summary": "Two of the central factors believed to underpin human sentence processing difficulty are expectations and retrieval from working memory. A recent attempt to create a unified cognitive model integrating these two factors have relied on the parallels between the self-attention mechanism of transformer language models and cue-based retrieval theories of working memory in human sentence processing (Ryu and Lewis 2021). While the authors show that attention patterns in specialized attention heads of GPT-2 are consistent with a key prediction of cue-based retrieval models, similarity-based interference effects, their method requires the identification of syntactically specialized attention heads, and makes an cognitively implausible implicit assumption that hundreds of memory retrieval operations take place in parallel. In the present work, we develop a recurrent neural language model with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories. We show that our model’s single attention head can capture semantic and syntactic interference effects observed in human experiments.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.582.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While the authors show that attention patterns in specialized attention heads of GPT-2 are consistent with a key prediction of cue-based retrieval models, similarity-based interference effects, their method requires the identification of syntactically specialized attention heads, and makes an cognitively implausible implicit assumption that hundreds of memory retrieval operations take place in parallel.\"\n\nNote: The paper discusses limitations of a specific",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While the authors show that attention patterns in specialized attention heads of GPT-2 are consistent with a key prediction of cue-based retrieval models, similarity-based interference effects, their method requires the identification of syntactically specialized attention heads, and makes an cognitively implausible implicit assumption that hundreds of memory retrieval operations take place in parallel.\"\n\nNote: The paper discusses limitations of a specific"
    },
    {
        "title": "Parameter Efficient Multi-task Fine-tuning by Learning to Transfer Token-wise Prompts",
        "authors": [
            "Muling Wu",
            "Wenhao Liu",
            "Jianhan Xu",
            "Changze Lv",
            "Zixuan Ling",
            "Tianlong Li",
            "Longtao Huang",
            "Xiaoqing Zheng",
            "Xuanjing Huang"
        ],
        "published": "2023",
        "summary": "Prompt tuning has been proven to be successful on various tasks by incorporating a small number of trainable parameters while freezing large pre-trained language models (PLMs). However, it is still unsettled how to generate more proper prompts for any individual examples and how to extend prompt tuning to multi-task learning scenarios by leveraging cross-task features. To address these challenges, we propose a token-wise prompt tuning (TPT), in which a bank of finer-grained soft prompt tokens is built for multi-task learning by memory network. The tokens are retrieved from the bank against an input example and assembled to an instance-dependent prompt. Extensive experimental results on 14 datasets demonstrated that the models enhanced by our TPT performed far better than full parameter fine-tuned models and achieved state-of-the-art by tuning only 0.035% parameters.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.584.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the abstract mentions \"large pre-trained language models (PLMs)\" which is related to LLMs, but does not discuss any limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the abstract mentions \"large pre-trained language models (PLMs)\" which is related to LLMs, but does not discuss any limitations."
    },
    {
        "title": "EARA: Improving Biomedical Semantic Textual Similarity with Entity-Aligned Attention and Retrieval Augmentation",
        "authors": [
            "Ying Xiong",
            "Xin Yang",
            "Linjing Liu",
            "Ka-Chun Wong",
            "Qingcai Chen",
            "Yang Xiang",
            "Buzhou Tang"
        ],
        "published": "2023",
        "summary": "Measuring Semantic Textual Similarity (STS) is a fundamental task in biomedical text processing, which aims at quantifying the similarity between two input biomedical sentences. Unfortunately, the STS datasets in the biomedical domain are relatively smaller but more complex in semantics than common domain, often leading to overfitting issues and insufficient text representation even based on Pre-trained Language Models (PLMs) due to too many biomedical entities. In this paper, we propose EARA, an entity-aligned, attention-based and retrieval-augmented PLMs. Our proposed EARA first aligns the same type of fine-grained entity information in each sentence pair with an entity alignment matrix. Then, EARA regularizes the attention mechanism with an entity alignment matrix with an auxiliary loss. Finally, we add a retrieval module that retrieves similar instances to expand the scope of entity pairs and improve the model’s generalization. The comprehensive experiments reflect that EARA can achieve state-of-the-art performance on both in-domain and out-of-domain datasets. Source code is available.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.586.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"often leading to overfitting issues and insufficient text representation even based on Pre-trained Language Models (PLMs) due to too many biomedical entities.\"\n\nThis paper mentions a limitation of pre-trained language models (PLMs), which are related to LLMs, in the biomedical domain, specifically that they can suffer from overfitting issues and insufficient text representation due to the complexity of biomedical",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"often leading to overfitting issues and insufficient text representation even based on Pre-trained Language Models (PLMs) due to too many biomedical entities.\"\n\nThis paper mentions a limitation of pre-trained language models (PLMs), which are related to LLMs, in the biomedical domain, specifically that they can suffer from overfitting issues and insufficient text representation due to the complexity of biomedical"
    },
    {
        "title": "Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking",
        "authors": [
            "Shengyao Zhuang",
            "Bing Liu",
            "Bevan Koopman",
            "Guido Zuccon"
        ],
        "published": "2023",
        "summary": "In the field of information retrieval, Query Likelihood Models (QLMs) rank documents based on the probability of generating the query given the content of a document. Recently, advanced large language models (LLMs) have emerged as effective QLMs, showcasing promising ranking capabilities. This paper focuses on investigating the genuine zero-shot ranking effectiveness of recent LLMs, which are solely pre-trained on unstructured text data without supervised instruction fine-tuning. Our findings reveal the robust zero-shot ranking ability of such LLMs, highlighting that additional instruction fine-tuning may hinder effectiveness unless a question generation task is present in the fine-tuning dataset. Furthermore, we introduce a novel state-of-the-art ranking system that integrates LLM-based QLMs with a hybrid zero-shot retriever, demonstrating exceptional effectiveness in both zero-shot and few-shot scenarios. We make our codebase publicly available at https://github.com/ielab/llm-qlm.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.590.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"highlighting that additional instruction fine-tuning may hinder effectiveness unless a question generation task is present in the fine-tuning dataset.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs, specifically that additional instruction fine-tuning may hinder effectiveness, but it is not the primary focus of the paper and is mentioned briefly.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"highlighting that additional instruction fine-tuning may hinder effectiveness unless a question generation task is present in the fine-tuning dataset.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs, specifically that additional instruction fine-tuning may hinder effectiveness, but it is not the primary focus of the paper and is mentioned briefly."
    },
    {
        "title": "On General Language Understanding",
        "authors": [
            "David Schlangen"
        ],
        "published": "2023",
        "summary": "Natural Language Processing prides itself to be an empirically-minded, if not outright empiricist field, and yet lately it seems to get itself into essentialist debates on issues of meaning and measurement (“Do Large Language Models Understand Language, And If So, How Much?”). This is not by accident: Here, as everywhere, the evidence underspecifies the understanding. As a remedy, this paper sketches the outlines of a model of understanding, which can ground questions of the adequacy of current methods of measurement of model quality. The paper makes three claims: A) That different language use situation types have different characteristics, B) That language understanding is a multifaceted phenomenon, bringing together individualistic and social processes, and C) That the choice of Understanding Indicator marks the limits of benchmarking, and the beginnings of considerations of the ethics of NLP use.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.591.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Do Large Language Models Understand Language, And If So, How Much?”",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Do Large Language Models Understand Language, And If So, How Much?”"
    },
    {
        "title": "USB: A Unified Summarization Benchmark Across Tasks and Domains",
        "authors": [
            "Kundan Krishna",
            "Prakhar Gupta",
            "Sanjana Ramprasad",
            "Byron Wallace",
            "Jeffrey Bigham",
            "Zachary Lipton"
        ],
        "published": "2023",
        "summary": "While the NLP community has produced numerous summarization benchmarks, none provide the rich annotations required to simultaneously address many important problems related to control and reliability. We introduce a Wikipedia-derived benchmark, complemented by a rich set of crowd-sourced annotations, that supports 8 interrelated tasks: (i) extractive summarization; (ii) abstractive summarization; (iii) topic-based summarization; (iv) compressing selected sentences into a one-line summary; (v) surfacing evidence for a summary sentence; (vi) predicting the factual accuracy of a summary sentence; (vii) identifying unsubstantiated spans in a summary sentence; (viii) correcting factual errors in summaries. We compare various methods on this benchmark and discover that on multiple tasks, moderately-sized fine-tuned models consistently outperform much larger few-shot prompted language models. For factuality-related tasks, we also evaluate existing heuristics to create training data and find that training on them results in worse performance than training on 20× less human-labeled data. Our articles draw from 6 domains, facilitating cross-domain analysis. On some tasks, the amount of training data matters more than the domain where it comes from, while for other tasks training specifically on data from the target domain, even if limited, is more beneficial.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.592.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"on multiple tasks, moderately-sized fine-tuned models consistently outperform much larger few-shot prompted language models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"on multiple tasks, moderately-sized fine-tuned models consistently outperform much larger few-shot prompted language models.\""
    },
    {
        "title": "tagE: Enabling an Embodied Agent to Understand Human Instructions",
        "authors": [
            "Chayan Sarkar",
            "Avik Mitra",
            "Pradip Pramanick",
            "Tapas Nayak"
        ],
        "published": "2023",
        "summary": "Natural language serves as the primary mode of communication when an intelligent agent with a physical presence engages with human beings. While a plethora of research focuses on natural language understanding (NLU), encompassing endeavors such as sentiment analysis, intent prediction, question answering, and summarization, the scope of NLU directed at situations necessitating tangible actions by an embodied agent remains limited. The inherent ambiguity and incompleteness inherent in natural language present challenges for intelligent agents striving to decipher human intention. To tackle this predicament head-on, we introduce a novel system known as task and argument grounding for Embodied agents (tagE). At its core, our system employs an inventive neural network model designed to extract a series of tasks from complex task instructions expressed in natural language. Our proposed model adopts an encoder-decoder framework enriched with nested decoding to effectively extract tasks and their corresponding arguments from these intricate instructions. These extracted tasks are then mapped (or grounded) to the robot’s established collection of skills, while the arguments find grounding in objects present within the environment. To facilitate the training and evaluation of our system, we have curated a dataset featuring complex instructions. The results of our experiments underscore the prowess of our approach, as it outperforms robust baseline models.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.593.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs."
    },
    {
        "title": "An Intent-based and Annotation-free Method for Duplicate Question Detection in CQA Forums",
        "authors": [
            "Yubo Shu",
            "Hansu Gu",
            "Peng Zhang",
            "Tun Lu",
            "Ning Gu"
        ],
        "published": "2023",
        "summary": "With the advent of large language models (LLMs), Community Question Answering (CQA) forums offer well-curated questions and answers that can be utilized for instruction-tuning, effectively training LLMs to be aligned with human intents. However, the issue of duplicate questions arises as the volume of content within CQA continues to grow, posing a threat to content quality. Recent research highlights the benefits of detecting and eliminating duplicate content. It not only enhances the LLMs’ ability to generalize across diverse intents but also improves the efficiency of training data utilization while addressing concerns related to information leakage. However, existing methods for detecting duplicate questions in CQA typically rely on generic text-pair matching models, overlooking the intent behind the questions. In this paper, we propose a novel intent-based duplication detector named Intent-DQD that comprehensively leverages intent information to address the problem of duplicate question detection in CQA. Intent-DQD first leverages the characteristics in CQA forums and extracts training labels to recognize and match intents without human annotation. Intent-DQD then effectively aggregates intent-level relations and establishes question-level relations to enable intent-aware duplication detection. Experimental results on fifteen distinct domains from both CQADupStack and Stack Overflow datasets demonstrate the effectiveness of Intent-DQD. Reproducible codes and datasets will be released upon publication of the paper.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.596.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the issue of duplicate questions arises as the volume of content within CQA continues to grow, posing a threat to content quality.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the issue of duplicate questions arises as the volume of content within CQA continues to grow, posing a threat to content quality.\""
    },
    {
        "title": "A Closer Look into Using Large Language Models for Automatic Evaluation",
        "authors": [
            "Cheng-Han Chiang",
            "Hung-yi Lee"
        ],
        "published": "2023",
        "summary": "Using large language models (LLMs) to evaluate text quality has recently gained popularity. Some existing prior works explore the idea of using LLMs for evaluation, while they differ in some details of the evaluation process. In this paper, we analyze *LLM evaluation* and *G-Eval*, and we discuss how those details in the evaluation process change how well the ratings given by LLMs correlate with human ratings. We find that the auto Chain-of-Thought (CoT) used in G-Eval does not always make G-Eval more aligned with human ratings. We also show that forcing the LLM to output only a numeric rating, as in G-Eval, is suboptimal. Last, we reveal that asking the LLM to explain its own ratings consistently improves the correlation between the ChatGPT and human ratings and pushes state-of-the-art (SoTA) correlations on two meta-evaluation datasets.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.599.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that the auto Chain-of-Thought (CoT) used in G-Eval does not always make G-Eval more aligned with human ratings. We also show that forcing the LLM to output only a numeric rating, as in G-Eval, is suboptimal.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We find that the auto Chain-of-Thought (CoT) used in G-Eval does not always make G-Eval more aligned with human ratings. We also show that forcing the LLM to output only a numeric rating, as in G-Eval, is suboptimal.\""
    },
    {
        "title": "Connecting the Dots: What Graph-Based Text Representations Work Best for Text Classification using Graph Neural Networks?",
        "authors": [
            "Margarita Bugueño",
            "Gerard de Melo"
        ],
        "published": "2023",
        "summary": "Given the success of Graph Neural Networks (GNNs) for structure-aware machine learning, many studies have explored their use for text classification, but mostly in specific domains with limited data characteristics. Moreover, some strategies prior to GNNs relied on graph mining and classical machine learning, making it difficult to assess their effectiveness in modern settings. This work extensively investigates graph representation methods for text classification, identifying practical implications and open challenges. We compare different graph construction schemes using a variety of GNN architectures and setups across five datasets, encompassing short and long documents as well as unbalanced scenarios in diverse domains. Two Transformer-based large language models are also included to complement the study. The results show that i) although the effectiveness of graphs depends on the textual input features and domain, simple graph constructions perform better the longer the documents are, ii) graph representations are especially beneficial for longer documents, outperforming Transformer-based models, iii) graph methods are particularly efficient for solving the task.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.600.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"ii) graph representations are especially beneficial for longer documents, outperforming Transformer-based models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"ii) graph representations are especially beneficial for longer documents, outperforming Transformer-based models\""
    },
    {
        "title": "Text Classification via Large Language Models",
        "authors": [
            "Xiaofei Sun",
            "Xiaoya Li",
            "Jiwei Li",
            "Fei Wu",
            "Shangwei Guo",
            "Tianwei Zhang",
            "Guoyin Wang"
        ],
        "published": "2023",
        "summary": "Despite the remarkable success of large-scale Language Models (LLMs) such as GPT-3, their performances still significantly underperform fine-tuned models in the task of text classification.This is due to (1) the lack of reasoning ability in addressing complex linguistic phenomena (e.g., intensification, contrast, irony etc); (2) limited number of tokens allowed in in-context learning. In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adopts a progressive reasoning strategy tailored to addressing the complex linguistic phenomena involved in text classification: CARP first prompts LLMs to find superficial clues (e.g., keywords, tones, semantic relations, references, etc), based on which a diagnostic reasoning process is induced for final decisions. To further address the limited-token issue, CARP uses a fine-tuned model on the supervised dataset for kNN demonstration search in the in-context learning, allowing the model to take the advantage of both LLM’s generalization ability and the task-specific evidence provided by the full labeled dataset. Remarkably, CARP yields new SOTA performances on 4 out of 5 widely-used text-classification benchmarks, 97.39 (+1.24) on SST-2, 96.40 (+0.72) on AGNews, 98.78 (+0.25) on R8 and 96.95 (+0.6) on R52, and a performance comparable to SOTA on MR (92.39 v.s. 93.3). More importantly, we find that CARP delivers impressive abilities on low-resource and domain-adaptation setups. Specifically, using 16 examples per class, CARP achieves comparable performances to supervised models with 1,024 examples per class.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.603.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the remarkable success of large-scale Language Models (LLMs) such as GPT-3, their performances still significantly underperform fine-tuned models in the task of text classification.This is due to (1) the lack of reasoning ability in addressing complex linguistic phenomena (e.g., intensification, contrast, irony etc); (2) limited number of tokens allowed in in-context",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Despite the remarkable success of large-scale Language Models (LLMs) such as GPT-3, their performances still significantly underperform fine-tuned models in the task of text classification.This is due to (1) the lack of reasoning ability in addressing complex linguistic phenomena (e.g., intensification, contrast, irony etc); (2) limited number of tokens allowed in in-context"
    },
    {
        "title": "DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM",
        "authors": [
            "Weijie Xu",
            "Wenxiang Hu",
            "Fanyou Wu",
            "Srinivasan Sengamedu"
        ],
        "published": "2023",
        "summary": "In the burgeoning field of natural language processing, Neural Topic Models (NTMs) and Large Language Models (LLMs) have emerged as areas of significant research interest. Despite this, NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic generation. Our study addresses this gap by introducing a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME). DeTiME leverages Encoder-Decoder-based LLMs to produce highly clusterable embeddings that could generate topics that exhibit both superior clusterability and enhanced semantic coherence compared to existing methods. Additionally, by exploiting the power of diffusion, our framework also provides the capability to generate content relevant to the identified topics. This dual functionality allows users to efficiently produce highly clustered topics and related content simultaneously. DeTiME’s potential extends to generating clustered embeddings as well. Notably, our proposed framework proves to be efficient to train and exhibits high adaptability, demonstrating its potential for a wide array of applications.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.606.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic generation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic generation.\""
    },
    {
        "title": "Energy and Carbon Considerations of Fine-Tuning BERT",
        "authors": [
            "Xiaorong Wang",
            "Clara Na",
            "Emma Strubell",
            "Sorelle Friedler",
            "Sasha Luccioni"
        ],
        "published": "2023",
        "summary": "Despite the popularity of the pre-train then fine-tune paradigm in the NLP community, existing work quantifying energy costs and associated carbon emissions has largely focused on language model pre-training. Although a single pre-training run draws substantially more energy than fine-tuning, fine-tuning is performed more frequently by many more individual actors, and thus must be accounted for when considering the energy and carbon footprint of NLP. In order to better characterize the role of fine-tuning in the landscape of energy and carbon emissions in NLP, we perform a careful empirical study of the computational costs of fine-tuning across tasks, datasets, hardware infrastructure and measurement modalities. Our experimental results allow us to place fine-tuning energy and carbon costs into perspective with respect to pre-training and inference, and outline recommendations to NLP researchers and practitioners who wish to improve their fine-tuning energy efficiency.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.607.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the popularity of the pre-train then fine-tune paradigm in the NLP community, existing work quantifying energy costs and associated carbon emissions has largely focused on language model pre-training.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite the popularity of the pre-train then fine-tune paradigm in the NLP community, existing work quantifying energy costs and associated carbon emissions has largely focused on language model pre-training.\""
    },
    {
        "title": "Democratizing LLMs: An Exploration of Cost-Performance Trade-offs in Self-Refined Open-Source Models",
        "authors": [
            "Sumuk Shashidhar",
            "Abhinav Chinta",
            "Vaibhav Sahai",
            "Zhenhailong Wang",
            "Heng Ji"
        ],
        "published": "2023",
        "summary": "The dominance of proprietary LLMs has led to restricted access and raised information privacy concerns. The SoTA open-source alternatives are crucial for information-sensitive and high-volume applications but often lag behind in performance. To address this gap, we propose (1) A generalized variant of iterative self-critique and self-refinement devoid of external influence. (2) A novel ranking metric - Performance, Refinement, and Inference Cost Score (PeRFICS) - to find the optimal model for a given task considering refined performance and cost. Our experiments show that SoTA open source models of varying sizes from 7B - 65B, on average, improve 8.2% from their baseline performance. Strikingly, even models with extremely small memory footprints, such as Vicuna-7B, show a 11.74% improvement overall and up to a 25.39% improvement in high-creativity, open ended tasks on the Vicuna benchmark. Vicuna-13B takes it a step further and outperforms ChatGPT post-refinement. This work has profound implications for resource-constrained and information-sensitive environments seeking to leverage LLMs without incurring prohibitive costs, compromising on performance and privacy. The domain-agnostic self-refinement process coupled with our novel ranking metric facilitates informed decision-making in model selection, thereby reducing costs and democratizing access to high-performing language models, as evidenced by three case studies on personal computing, gaming and enterprise solutions.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.608.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The SoTA open-source alternatives are crucial for information-sensitive and high-volume applications but often lag behind in performance.\"\n\nThis paper mentions a limitation of LLMs (open-source models lagging behind in performance) but does not explore it in depth. The primary focus of the paper is on the proposed solution to address this limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The SoTA open-source alternatives are crucial for information-sensitive and high-volume applications but often lag behind in performance.\"\n\nThis paper mentions a limitation of LLMs (open-source models lagging behind in performance) but does not explore it in depth. The primary focus of the paper is on the proposed solution to address this limitation."
    },
    {
        "title": "Example-based Hypernetworks for Multi-source Adaptation to Unseen Domains",
        "authors": [
            "Tomer Volk",
            "Eyal Ben-David",
            "Ohad Amosy",
            "Gal Chechik",
            "Roi Reichart"
        ],
        "published": "2023",
        "summary": "As Natural Language Processing (NLP) algorithms continually achieve new milestones, out-of-distribution generalization remains a significant challenge. This paper addresses the issue of multi-source adaptation for unfamiliar domains: We leverage labeled data from multiple source domains to generalize to unknown target domains at training. Our innovative framework employs example-based Hypernetwork adaptation: a T5 encoder-decoder initially generates a unique signature from an input example, embedding it within the source domains’ semantic space. This signature is subsequently utilized by a Hypernetwork to generate the task classifier’s weights. In an advanced version, the signature also enriches the input example’s representation. We evaluated our method across two tasks—sentiment classification and natural language inference—in 29 adaptation scenarios, where it outpaced established algorithms. We also compare our finetuned architecture to few-shot GPT-3, demonstrating its effectiveness in essential use cases. To the best of our knowledge, this marks the first application of Hypernetworks to the adaptation for unknown domains.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.610.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"out-of-distribution generalization remains a significant challenge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"out-of-distribution generalization remains a significant challenge.\""
    },
    {
        "title": "Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models",
        "authors": [
            "Hongzhan Lin",
            "Ziyang Luo",
            "Jing Ma",
            "Long Chen"
        ],
        "published": "2023",
        "summary": "The age of social media is rife with memes. Understanding and detecting harmful memes pose a significant challenge due to their implicit meaning that is not explicitly conveyed through the surface text and image. However, existing harmful meme detection approaches only recognize superficial harm-indicative signals in an end-to-end classification manner but ignore in-depth cognition of the meme text and image. In this paper, we attempt to detect harmful memes based on advanced reasoning over the interplay of multimodal information in memes. Inspired by the success of Large Language Models (LLMs) on complex reasoning, we first conduct abductive reasoning with LLMs. Then we propose a novel generative framework to learn reasonable thoughts from LLMs for better multimodal fusion and lightweight fine-tuning, which consists of two training stages: 1) Distill multimodal reasoning knowledge from LLMs; and 2) Fine-tune the generative framework to infer harmfulness. Extensive experiments conducted on three meme datasets demonstrate that our proposed approach achieves superior performance than state-of-the-art methods on the harmful meme detection task.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.611.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing harmful meme detection approaches only recognize superficial harm-indicative signals in an end-to-end classification manner but ignore in-depth cognition of the meme text and image.\"\n\nThis abstract mentions a limitation of existing approaches, but not specifically of LLMs. However, the paper uses LLMs for abductive reasoning, implying that LLMs have some limitations in this context,",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, existing harmful meme detection approaches only recognize superficial harm-indicative signals in an end-to-end classification manner but ignore in-depth cognition of the meme text and image.\"\n\nThis abstract mentions a limitation of existing approaches, but not specifically of LLMs. However, the paper uses LLMs for abductive reasoning, implying that LLMs have some limitations in this context,"
    },
    {
        "title": "Domain Adaptation for Conversational Query Production with the RAG Model Feedback",
        "authors": [
            "Ante Wang",
            "Linfeng Song",
            "Ge Xu",
            "Jinsong Su"
        ],
        "published": "2023",
        "summary": "Conversational query production is an emerging fundamental task for the dialogue system, where search queries are generated to explore the vast and continually updating knowledge from a search engine. To accelerate this line of research, previous studies have released several datasets with human-annotated search queries. However, the limited annotations still can not cover conversations of various domains. To solve this challenge, we propose a novel domain adaptation framework. It is inspired by a weakly supervised learning algorithm from previous work that guides a model using reinforcement learning with BM25 scores as feedback. Though effective, it is fragile facing noisy content on webpages from a commercial search engine and variance in conversations because of ignoring deep semantic information of dialogue contexts. Thus, we improve the algorithm by taking the advance of retrieval-augmented generation (RAG) and exploring several practical techniques such as knowledge distillation for stable training. We conduct experiments in multiple settings across different languages. Guided by the RAG model feedback, our model is more robust and performs significantly better especially in a more challenging setting over strong baselines.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.612.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Though effective, it is fragile facing noisy content on webpages from a commercial search engine and variance in conversations because of ignoring deep semantic information of dialogue contexts.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Though effective, it is fragile facing noisy content on webpages from a commercial search engine and variance in conversations because of ignoring deep semantic information of dialogue contexts.\""
    },
    {
        "title": "LEGO: A Multi-agent Collaborative Framework with Role-playing and Iterative Feedback for Causality Explanation Generation",
        "authors": [
            "Zhitao He",
            "Pengfei Cao",
            "Yubo Chen",
            "Kang Liu",
            "Ruopeng Li",
            "Mengshu Sun",
            "Jun Zhao"
        ],
        "published": "2023",
        "summary": "Causality Explanation Generation refers to generate an explanation in natural language given an initial cause-effect pair. It demands rigorous explicit rationales to demonstrate the acquisition of implicit commonsense knowledge, which is unlikely to be easily memorized, making it challenging for large language models since they are often suffering from spurious causal associations when they encounter the content that does not exist in their memory. In this work, we introduce LEGO, a Multi-agent Collaborative Framework with Role-playing and Iterative Feedback for causality explanation generation. Specifically, we treat LLM as character malleable LEGO block and utilize role-playing to assign specific roles to five LLMs. We firstly devise a Fine-grained World Knowledge Integration Module to augment information about tasks for alleviating the phenomenon of spurious causal associations. Then, we leverage an Iterative Feedback and Refinement Module to improve the generated explanation by multi-aspect feedback. Extensive experiments on widely used WIKIWHY and e-CARE datasets show the superiority of our multi-agent framework in terms of reasoning about the causality among cause and effect.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.613.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"large language models... are often suffering from spurious causal associations when they encounter the content that does not exist in their memory.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"large language models... are often suffering from spurious causal associations when they encounter the content that does not exist in their memory.\""
    },
    {
        "title": "Ranking LLM-Generated Loop Invariants for Program Verification",
        "authors": [
            "Saikat Chakraborty",
            "Shuvendu Lahiri",
            "Sarah Fakhoury",
            "Akash Lal",
            "Madanlal Musuvathi",
            "Aseem Rastogi",
            "Aditya Senthilnathan",
            "Rahul Sharma",
            "Nikhil Swamy"
        ],
        "published": "2023",
        "summary": "Synthesizing inductive loop invariants is fundamental to automating program verification. In this work we observe that Large Language Models (such as gpt-3.5 or gpt-4) are capable of synthesizing loop invariants for a class of programs in a 0-shot setting, yet require several samples to generate the correct invariants. This can lead to a large number a calls to a program verifier to establish an invariant. To address this issue, we propose a re-ranking approach for the generated results of LLMs. We have designed a ranker that can distinguish between correct inductive invariants and incorrect attempts based on the problem definition. The ranker is optimized as a contrastive ranker. Experimental results demonstrate that this re-ranking mechanism significantly improves the ranking of correct invariants among the generated candidates, leading to a notable reduction in the number of calls to a verifier.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.614.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"yet require several samples to generate the correct invariants. This can lead to a large number a calls to a program verifier to establish an invariant.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"yet require several samples to generate the correct invariants. This can lead to a large number a calls to a program verifier to establish an invariant.\""
    },
    {
        "title": "Knowledge Corpus Error in Question Answering",
        "authors": [
            "Yejoon Lee",
            "Philhoon Oh",
            "James Thorne"
        ],
        "published": "2023",
        "summary": "Recent works in open-domain question answering (QA) have explored generating context passages from large language models (LLMs), replacing the traditional retrieval step in the QA pipeline. However, it is not well understood why generated passages can be more effective than retrieved ones. This study revisits the conventional formulation of QA and introduces the concept of knowledge corpus error. This error arises when the knowledge corpus used for retrieval is only a subset of the entire string space, potentially excluding more helpful passages that exist outside the corpus. LLMs may mitigate this shortcoming by generating passages in a larger space. We come up with an experiment of paraphrasing human-annotated gold context using LLMs to observe knowledge corpus error empirically. Our results across three QA benchmarks reveal an increased performance (10% - 13%) when using paraphrased passage, indicating a signal for the existence of knowledge corpus error.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.616.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it is not well understood why generated passages can be more effective than retrieved ones.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of the traditional QA pipeline, but the focus is on the concept of knowledge corpus error and how LLMs can mitigate it, rather than exploring the limitations of LLMs themselves.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, it is not well understood why generated passages can be more effective than retrieved ones.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of the traditional QA pipeline, but the focus is on the concept of knowledge corpus error and how LLMs can mitigate it, rather than exploring the limitations of LLMs themselves."
    },
    {
        "title": "The language of prompting: What linguistic properties make a prompt successful?",
        "authors": [
            "Alina Leidinger",
            "Robert van Rooij",
            "Ekaterina Shutova"
        ],
        "published": "2023",
        "summary": "The latest generation of LLMs can be prompted to achieve impressive zero-shot or few-shot performance in many NLP tasks. However, since performance is highly sensitive to the choice of prompts, considerable effort has been devoted to crowd-sourcing prompts or designing methods for prompt optimisation. Yet, we still lack a systematic understanding of how linguistic properties of prompts correlate with the task performance. In this work, we investigate how LLMs of different sizes, pre-trained and instruction-tuned, perform on prompts that are semantically equivalent, but vary in linguistic structure. We investigate both grammatical properties such as mood, tense, aspect and modality, as well as lexico-semantic variation through the use of synonyms. Our findings contradict the common assumption that LLMs achieve optimal performance on prompts which reflect language use in pretraining or instruction-tuning data. Prompts transfer poorly between datasets or models, and performance cannot generally be explained by perplexity, word frequency, word sense ambiguity or prompt length. Based on our results, we put forward a proposal for a more robust and comprehensive evaluation standard for prompting research.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.618.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, since performance is highly sensitive to the choice of prompts...\"; \"Prompts transfer poorly between datasets or models, and performance cannot generally be explained by perplexity, word frequency, word sense ambiguity or prompt length.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, since performance is highly sensitive to the choice of prompts...\"; \"Prompts transfer poorly between datasets or models, and performance cannot generally be explained by perplexity, word frequency, word sense ambiguity or prompt length.\""
    },
    {
        "title": "When and Why Does Bias Mitigation Work?",
        "authors": [
            "Abhilasha Ravichander",
            "Joe Stacey",
            "Marek Rei"
        ],
        "published": "2023",
        "summary": "Neural models have been shown to exploit shallow surface features to perform language understanding tasks, rather than learning the deeper language understanding and reasoning skills that practitioners desire. Previous work has developed debiasing techniques to pressure models away from spurious features or artifacts in datasets, with the goal of having models instead learn useful, task-relevant representations. However, what do models actually learn as a result of such debiasing procedures? In this work, we evaluate three model debiasing strategies, and through a set of carefully designed tests we show how debiasing can actually increase the model’s reliance on hidden biases, instead of learning robust features that help it solve a task. Further, we demonstrate how even debiasing models against all shallow features in a dataset may still not help models address NLP tasks. As a result, we suggest that debiasing existing models may not be sufficient for many language understanding tasks, and future work should consider new learning paradigms, to address complex challenges such as commonsense reasoning and inference.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.619.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy",
        "authors": [
            "Zhihong Shao",
            "Yeyun Gong",
            "Yelong Shen",
            "Minlie Huang",
            "Nan Duan",
            "Weizhu Chen"
        ],
        "published": "2023",
        "summary": "Retrieval-augmented generation has raise extensive attention as it is promising to address the limitations of large language models including outdated knowledge and hallucinations. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to guide retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner: a model’s response to a task input shows what might be needed to finish the task, and thus can serve as an informative context for retrieving more relevant knowledge which in turn helps generate a better response in another iteration. Compared with recent work which interleaves retrieval with generation when completing a single output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.620.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Retrieval-augmented generation has raise extensive attention as it is promising to address the limitations of large language models including outdated knowledge and hallucinations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Retrieval-augmented generation has raise extensive attention as it is promising to address the limitations of large language models including outdated knowledge and hallucinations.\""
    },
    {
        "title": "Dynamic Low-rank Estimation for Transformer-based Language Models",
        "authors": [
            "Ting Hua",
            "Xiao Li",
            "Shangqian Gao",
            "Yen-Chang Hsu",
            "Yilin Shen",
            "Hongxia Jin"
        ],
        "published": "2023",
        "summary": "Matrix decomposition methods, such as Singular Value Decomposition (SVD) and its importance-weighted variants, have been widely used for compressing Transformer-based language models. While importance-weighted decomposition methods alleviate the strong assumption of equal importance for each parameter in SVD, they still rely on two fundamental assumptions: 1) unchanged importance distribution during further fine-tuning, 2) equal importance across weight matrices in different layers. Furthermore, these methods necessitate a well-trained task-specific model as the starting point and require additional fine-tuning after compression. In this work, we proposed RankDyna, a matrix decomposition method that enables dynamic rank resource allocation among matrices across different layers during the training process. Starting from a general pre-trained model, RankDyna accomplishes the dual goals of compression and adaptation to the downstream task, all within a single round of fine-tuning. The extensive evaluations demonstrate that RankDyna can outperform current SOTA methods under various parameter budget levels, and the advantage of RankDyna is further enhanced with higher compression rates.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.621.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"they still rely on two fundamental assumptions:",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"they still rely on two fundamental assumptions:"
    },
    {
        "title": "Compositional Generalization for Data-to-Text Generation",
        "authors": [
            "Xinnuo Xu",
            "Ivan Titov",
            "Mirella Lapata"
        ],
        "published": "2023",
        "summary": "Data-to-text generation involves transforming structured data, often represented as predicate-argument tuples, into coherent textual descriptions. Despite recent advances, systems still struggle when confronted with unseen combinations of predicates, producing unfaithful descriptions (e.g.,hallucinations or omissions). We refer to this issue as compositional generalisation, and it encouraged us to create a benchmark for assessing the performance of different approaches on this specific problem. Furthermore, we propose a novel model that addresses compositional generalization by clustering predicates into groups. Our model generates text in a sentence-by-sentence manner, relying on one cluster of predicates at a time. This approach significantly outperforms T5-baselines across all evaluation metrics. Notably, it achieved a 31% improvement over T5 in terms of a metric focused on maintaining faithfulness to the input.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.623.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite recent advances, systems still struggle when confronted with unseen combinations of predicates, producing unfaithful descriptions (e.g.,hallucinations or omissions).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite recent advances, systems still struggle when confronted with unseen combinations of predicates, producing unfaithful descriptions (e.g.,hallucinations or omissions).\""
    },
    {
        "title": "In-Context Learning Creates Task Vectors",
        "authors": [
            "Roee Hendel",
            "Mor Geva",
            "Amir Globerson"
        ],
        "published": "2023",
        "summary": "In-context learning (ICL) in Large Language Models (LLMs) has emerged as a powerful new learning paradigm. However, its underlying mechanism is still not well understood. In particular, it is challenging to map it to the “standard’ machine learning framework, where one uses a training set S to find a best-fitting function f(x) in some hypothesis class. Here we make progress on this problem by showing that the functions learned by ICL often have a very simple structure: they correspond to the transformer LLM whose only inputs are the query x and a single “task vector’ calculated from the training set. Thus, ICL can be seen as compressing S into a single task vector 𝜃(S) and then using this task vector to modulate the transformer to produce the output. We support the above claim via comprehensive experiments across a range of models and tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.624.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, its underlying mechanism is still not well understood.\"\n\nNote that the paper mentions a limitation of the understanding of the underlying mechanism of in-context learning in LLMs, but it does not elaborate on this limitation further and instead focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, its underlying mechanism is still not well understood.\"\n\nNote that the paper mentions a limitation of the understanding of the underlying mechanism of in-context learning in LLMs, but it does not elaborate on this limitation further and instead focuses on the proposed solution."
    },
    {
        "title": "TalkUp: Paving the Way for Understanding Empowering Language",
        "authors": [
            "Lucille Njoo",
            "Chan Park",
            "Octavia Stappart",
            "Marvin Thielk",
            "Yi Chu",
            "Yulia Tsvetkov"
        ],
        "published": "2023",
        "summary": "Empowering language is important in many real-world contexts, from education to workplace dynamics to healthcare. Though language technologies are growing more prevalent in these contexts, empowerment has seldom been studied in NLP, and moreover, it is inherently challenging to operationalize because of its implicit nature. This work builds from linguistic and social psychology literature to explore what characterizes empowering language. We then crowdsource a novel dataset of Reddit posts labeled for empowerment, reasons why these posts are empowering to readers, and the social relationships between posters and readers. Our preliminary analyses show that this dataset, which we call TalkUp, can be used to train language models that capture empowering and disempowering language. More broadly, TalkUp provides an avenue to explore implication, presuppositions, and how social context influences the meaning of language.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.625.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"capture empowering and disempowering language\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"capture empowering and disempowering language\""
    },
    {
        "title": "Unifying Text, Tables, and Images for Multimodal Question Answering",
        "authors": [
            "Haohao Luo",
            "Ying Shen",
            "Yang Deng"
        ],
        "published": "2023",
        "summary": "Multimodal question answering (MMQA), which aims to derive the answer from multiple knowledge modalities (e.g., text, tables, and images), has received increasing attention due to its board applications. Current approaches to MMQA often rely on single-modal or bi-modal QA models, which limits their ability to effectively integrate information across all modalities and leverage the power of pre-trained language models. To address these limitations, we propose a novel framework called UniMMQA, which unifies three different input modalities into a text-to-text format by employing position-enhanced table linearization and diversified image captioning techniques. Additionally, we enhance cross-modal reasoning by incorporating a multimodal rationale generator, which produces textual descriptions of cross-modal relations for adaptation into the text-to-text generation process. Experimental results on three MMQA benchmark datasets show the superiority of UniMMQA in both supervised and unsupervised settings.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.626.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Current approaches to MMQA often rely on single-modal or bi-modal QA models, which limits their ability to effectively integrate information across all modalities and leverage the power of pre-trained language models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Current approaches to MMQA often rely on single-modal or bi-modal QA models, which limits their ability to effectively integrate information across all modalities and leverage the power of pre-trained language models.\""
    },
    {
        "title": "Unsupervised Lexical Simplification with Context Augmentation",
        "authors": [
            "Takashi Wada",
            "Timothy Baldwin",
            "Jey Han Lau"
        ],
        "published": "2023",
        "summary": "We propose a new unsupervised lexical simplification method that uses only monolingual data and pre-trained language models. Given a target word and its context, our method generates substitutes based on the target context and also additional contexts sampled from monolingual data. We conduct experiments in English, Portuguese, and Spanish on the TSAR-2022 shared task, and show that our model substantially outperforms other unsupervised systems across all languages. We also establish a new state-of-the-art by ensembling our model with GPT-3.5. Lastly, we evaluate our model on the SWORDS lexical substitution data set, achieving a state-of-the-art result.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.627.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"uses only monolingual data and pre-trained language models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"uses only monolingual data and pre-trained language models\""
    },
    {
        "title": "Multilingual Lottery Tickets to Pretrain Language Models",
        "authors": [
            "Jaeseong Lee",
            "Seung-won Hwang"
        ],
        "published": "2023",
        "summary": "The curse of multilinguality in training multilingual pretrained language models (mPLMs) refers to the negative interference between languages, especially when the capacity is limited. While increasing the capacity may appear intuitive for overcoming this curse, it negatively affects both training and inference costs. Our distinction is pursuing the competing goals of reducing negative interference, while keeping capacity per each language more or less the same. Specifically, we first scale the model to reduce interference, then search for a per-language subnetwork, or a lottery ticket, with comparable performance to the full model. According to lottery ticket hypothesis, this scale-then-find-ticket approach alleviates interfering signals as in the scaled model, but redistributes parameters to keep the parameters reduced. Finally, to avoid the cost of multiple retraining for searching multilingual tickets, we explore zero-shot neural architecture search (NAS) methods. We investigate the most appropriate zero-shot NAS method to find multilingual tickets. Our proposed multilingual tickets reduce the inference cost of models for each languages, while boosting the performances. The ticket search cost is negligible and tickets found qualitatively preserve linguistic similarity. Our code is publicly available.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.629.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The curse of multilinguality in training multilingual pretrained language models (mPLMs) refers to the negative interference between languages, especially when the capacity is limited.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The curse of multilinguality in training multilingual pretrained language models (mPLMs) refers to the negative interference between languages, especially when the capacity is limited.\""
    },
    {
        "title": "Target-Aware Spatio-Temporal Reasoning via Answering Questions in Dynamic Audio-Visual Scenarios",
        "authors": [
            "Yuanyuan Jiang",
            "Jianqin Yin"
        ],
        "published": "2023",
        "summary": "Audio-visual question answering (AVQA) is a challenging task that requires multistep spatio-temporal reasoning over multimodal contexts. Recent works rely on elaborate target-agnostic parsing of audio-visual scenes for spatial grounding while mistreating audio and video as separate entities for temporal grounding. This paper proposes a new target-aware joint spatio-temporal grounding network for AVQA. It consists of two key components: the target-aware spatial grounding module (TSG) and the single-stream joint audio-visual temporal grounding module (JTG). The TSG can focus on audio-visual cues relevant to the query subject by utilizing explicit semantics from the question. Unlike previous two-stream temporal grounding modules that required an additional audio-visual fusion module, JTG incorporates audio-visual fusion and question-aware temporal grounding into one module with a simpler single-stream architecture. The temporal synchronization between audio and video in the JTG is facilitated by our proposed cross-modal synchrony loss (CSL). Extensive experiments verified the effectiveness of our proposed method over existing state-of-the-art methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.630.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models",
        "authors": [
            "Jiho Kim",
            "Yeonsu Kwon",
            "Yohan Jo",
            "Edward Choi"
        ],
        "published": "2023",
        "summary": "While large language models (LLMs) have made considerable advancements in understanding and generating unstructured text, their application in structured data remains underexplored. Particularly, using LLMs for complex reasoning tasks on knowledge graphs (KGs) remains largely untouched. To address this, we propose KG-GPT, a multi-purpose framework leveraging LLMs for tasks employing KGs. KG-GPT comprises three steps: Sentence Segmentation, Graph Retrieval, and Inference, each aimed at partitioning sentences, retrieving relevant graph components, and deriving logical conclusions, respectively. We evaluate KG-GPT using KG-based fact verification and KGQA benchmarks, with the model showing competitive and robust performance, even outperforming several fully-supervised models. Our work, therefore, marks a significant step in unifying structured and unstructured data processing within the realm of LLMs.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.631.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While large language models (LLMs) have made considerable advancements in understanding and generating unstructured text, their application in structured data remains underexplored. Particularly, using LLMs for complex reasoning tasks on knowledge graphs (KGs) remains largely untouched.\"\n\n(Note: The abstract mentions the limitation of LLMs in structured data and complex reasoning tasks, but it's a minor detail",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While large language models (LLMs) have made considerable advancements in understanding and generating unstructured text, their application in structured data remains underexplored. Particularly, using LLMs for complex reasoning tasks on knowledge graphs (KGs) remains largely untouched.\"\n\n(Note: The abstract mentions the limitation of LLMs in structured data and complex reasoning tasks, but it's a minor detail"
    },
    {
        "title": "Breaking the Language Barrier: Improving Cross-Lingual Reasoning with Structured Self-Attention",
        "authors": [
            "Negar Foroutan",
            "Mohammadreza Banaei",
            "Karl Aberer",
            "Antoine Bosselut"
        ],
        "published": "2023",
        "summary": "In this work, we study whether multilingual language models (MultiLMs) can transfer logical reasoning abilities to other languages when they are fine-tuned for reasoning in a different language. We evaluate the cross-lingual reasoning abilities of MultiLMs in two schemes: (1) where the language of the context and the question remain the same in the new languages that are tested (i.e., the reasoning is still monolingual, but the model must transfer the learned reasoning ability across languages), and (2) where the language of the context and the question is different (which we term code-switched reasoning). On two logical reasoning datasets, RuleTaker and LeapOfThought, we demonstrate that although MultiLMs can transfer reasoning ability across languages in a monolingual setting, they struggle to transfer reasoning abilities in a code-switched setting. Following this observation, we propose a novel attention mechanism that uses a dedicated set of parameters to encourage cross-lingual attention in code-switched sequences, which improves the reasoning performance by up to 14% and 4% on the RuleTaker and LeapOfThought datasets, respectively.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.632.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"although MultiLMs can transfer reasoning ability across languages in a monolingual setting, they struggle to transfer reasoning abilities in a code-switched setting.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"although MultiLMs can transfer reasoning ability across languages in a monolingual setting, they struggle to transfer reasoning abilities in a code-switched setting.\""
    },
    {
        "title": "CITB: A Benchmark for Continual Instruction Tuning",
        "authors": [
            "Zihan Zhang",
            "Meng Fang",
            "Ling Chen",
            "Mohammad-Reza Namazi-Rad"
        ],
        "published": "2023",
        "summary": "Continual learning (CL) is a paradigm that aims to replicate the human ability to learn and accumulate knowledge continually without forgetting previous knowledge and transferring it to new tasks. Recent instruction tuning (IT) involves fine-tuning models to make them more adaptable to solving NLP tasks in general. However, it is still uncertain how instruction tuning works in the context of CL tasks. This challenging yet practical problem is formulated as Continual Instruction Tuning (CIT). In this work, we establish a CIT benchmark consisting of learning and evaluation protocols. We curate two long dialogue task streams of different types, InstrDialog and InstrDialog++, to study various CL methods systematically. Our experiments show that existing CL methods do not effectively leverage the rich natural language instructions, and fine-tuning an instruction-tuned model sequentially can yield similar or better results. We further explore different aspects that might affect the learning of CIT. We hope this benchmark will facilitate more research in this direction.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.633.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our experiments show that existing CL methods do not effectively leverage the rich natural language instructions, and fine-tuning an instruction-tuned model sequentially can yield similar or better results.\"\n\nThis rating is given because the abstract mentions a limitation of existing Continual Learning (CL) methods in leveraging natural language instructions, but it is not the primary focus of the paper and is used to motivate the need",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Our experiments show that existing CL methods do not effectively leverage the rich natural language instructions, and fine-tuning an instruction-tuned model sequentially can yield similar or better results.\"\n\nThis rating is given because the abstract mentions a limitation of existing Continual Learning (CL) methods in leveraging natural language instructions, but it is not the primary focus of the paper and is used to motivate the need"
    },
    {
        "title": "Mixture-of-Linguistic-Experts Adapters for Improving and Interpreting Pre-trained Language Models",
        "authors": [
            "Raymond Li",
            "Gabriel Murray",
            "Giuseppe Carenini"
        ],
        "published": "2023",
        "summary": "In this work, we propose a method that combines two popular research areas by injecting linguistic structures into pre-trained language models in the parameter-efficient fine-tuning (PEFT) setting. In our approach, parallel adapter modules encoding different linguistic structures are combined using a novel Mixture-of-Linguistic-Experts architecture, where Gumbel-Softmax gates are used to determine the importance of these modules at each layer of the model. To reduce the number of parameters, we first train the model for a fixed small number of steps before pruning the experts based on their important scores. Our experiment results with three different pre-trained models show that our approach can outperform state-of-the-art PEFT methods with a comparable number of parameters. In addition, we provide additional analysis to examine the experts selected by each model at each layer to provide insights for future studies.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.634.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but the motivation for the work is implied to be improving pre-trained language models, suggesting that they may have limitations that need to be addressed.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations, but the motivation for the work is implied to be improving pre-trained language models, suggesting that they may have limitations that need to be addressed."
    },
    {
        "title": "Towards Better Representations for Multi-Label Text Classification with Multi-granularity Information",
        "authors": [
            "Fangfang Li",
            "Puzhen Su",
            "Junwen Duan",
            "Weidong Xiao"
        ],
        "published": "2023",
        "summary": "Multi-label text classification (MLTC) aims to assign multiple labels to a given text. Previous works have focused on text representation learning and label correlations modeling using pre-trained language models (PLMs). However, studies have shown that PLMs generate word frequency-oriented text representations, causing texts with different labels to be closely distributed in a narrow region, which is difficult to classify. To address this, we present a novel framework CL( ̲Contrastive  ̲Learning)-MIL ( ̲Multi-granularity  ̲Information  ̲Learning) to refine the text representation for MLTC task. We first use contrastive learning to generate uniform initial text representation and incorporate label frequency implicitly. Then, we design a multi-task learning module to integrate multi-granularity (diverse text-labels correlations, label-label relations and label frequency) information into text representations, enhancing their discriminative ability. Experimental results demonstrate the complementarity of the modules in CL-MIL, improving the quality of text representations and yielding stable and competitive improvements for MLTC.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.635.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, studies have shown that PLMs generate word frequency-oriented text representations, causing texts with different labels to be closely distributed in a narrow region, which is difficult to classify.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of pre-trained language models (PLMs) in generating word frequency-oriented text representations, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, studies have shown that PLMs generate word frequency-oriented text representations, causing texts with different labels to be closely distributed in a narrow region, which is difficult to classify.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of pre-trained language models (PLMs) in generating word frequency-oriented text representations, but it is not the primary focus of the paper."
    },
    {
        "title": "Is GPT-4 a Good Data Analyst?",
        "authors": [
            "Liying Cheng",
            "Xingxuan Li",
            "Lidong Bing"
        ],
        "published": "2023",
        "summary": "As large language models (LLMs) have demonstrated their powerful capabilities in plenty of domains and tasks, including context understanding, code generation, language generation, data storytelling, etc., many data analysts may raise concerns if their jobs will be replaced by artificial intelligence (AI). This controversial topic has drawn great attention in public. However, we are still at a stage of divergent opinions without any definitive conclusion. Motivated by this, we raise the research question of “is GPT-4 a good data analyst?” in this work and aim to answer it by conducting head-to-head comparative studies. In detail, we regard GPT-4 as a data analyst to perform end-to-end data analysis with databases from a wide range of domains. We propose a framework to tackle the problems by carefully designing the prompts for GPT-4 to conduct experiments. We also design several task-specific evaluation metrics to systematically compare the performance between several professional human data analysts and GPT-4. Experimental results show that GPT-4 can achieve comparable performance to humans. We also provide in-depth discussions about our results to shed light on further studies before reaching the conclusion that GPT-4 can replace data analysts.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.637.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, we are still at a stage of divergent opinions without any definitive conclusion.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, we are still at a stage of divergent opinions without any definitive conclusion.\""
    },
    {
        "title": "Estimating Large Language Model Capabilities without Labeled Test Data",
        "authors": [
            "Harvey Fu",
            "Qinyuan Ye",
            "Albert Xu",
            "Xiang Ren",
            "Robin Jia"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have exhibited an impressive ability to perform in-context learning (ICL) from only a few examples, but the success of ICL varies widely from task to task. Thus, it is important to quickly determine whether ICL is applicable to a new task, but directly evaluating ICL accuracy can be expensive in situations where test data is expensive to annotate—the exact situations where ICL is most appealing. In this paper, we propose the task of ICL accuracy estimation, in which we predict the accuracy of an LLM when doing in-context learning on a new task given only unlabeled test data for that task. To perform ICL accuracy estimation, we propose a method that trains a meta-model using LLM confidence scores as features. We compare our method to several strong accuracy estimation baselines on a new benchmark that covers 4 LLMs and 3 task collections. The meta-model improves over all baselines across 7 out of 12 settings and achieves the same estimation performance as directly evaluating on 40 collected labeled test examples per task. At the same time, no existing approach provides an accurate and reliable ICL accuracy estimation in every setting, highlighting the need for better ways to measure the uncertainty of LLM predictions.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.639.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"At the same time, no existing approach provides an accurate and reliable ICL accuracy estimation in every setting, highlighting the need for better ways to measure the uncertainty of LLM predictions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"At the same time, no existing approach provides an accurate and reliable ICL accuracy estimation in every setting, highlighting the need for better ways to measure the uncertainty of LLM predictions.\""
    },
    {
        "title": "Large Language Models as Source Planner for Personalized Knowledge-grounded Dialogues",
        "authors": [
            "Hongru Wang",
            "Minda Hu",
            "Yang Deng",
            "Rui Wang",
            "Fei Mi",
            "Weichao Wang",
            "Yasheng Wang",
            "Wai-Chung Kwan",
            "Irwin King",
            "Kam-Fai Wong"
        ],
        "published": "2023",
        "summary": "Open-domain dialogue system usually requires different sources of knowledge to generate more informative and evidential responses. However, existing knowledge-grounded dialogue systems either focus on a single knowledge source or overlook the dependency between multiple sources of knowledge, which may result in generating inconsistent or even paradoxical responses. To incorporate multiple knowledge sources and dependencies between them, we propose SAFARI, a novel framework that leverages the exceptional capabilities of large language models (LLMs) in planning, understanding, and incorporating under both supervised and unsupervised settings. Specifically, SAFARI decouples the knowledge grounding into multiple sources and response generation, which allows easy extension to various knowledge sources including the possibility of not using any sources. To study the problem, we construct a personalized knowledge-grounded dialogue dataset Knowledge Behind Persona (KBP), which is the first to consider the dependency between persona and implicit knowledge. Experimental results on the KBP dataset demonstrate that the SAFARI framework can effectively produce persona-consistent and knowledge-enhanced responses.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.641.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper does not mention any limitations of LLMs, it only discusses leveraging their capabilities.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper does not mention any limitations of LLMs, it only discusses leveraging their capabilities."
    },
    {
        "title": "Toxicity in Multilingual Machine Translation at Scale",
        "authors": [
            "Marta Costa-jussà",
            "Eric Smith",
            "Christophe Ropers",
            "Daniel Licht",
            "Jean Maillard",
            "Javier Ferrando",
            "Carlos Escolano"
        ],
        "published": "2023",
        "summary": "Machine Translation systems can produce different types of errors, some of which are characterized as critical or catastrophic due to the specific negative impact that they can have on users. In this paper we focus on one type of critical error: added toxicity. We evaluate and analyze added toxicity when translating a large evaluation dataset (HOLISTICBIAS, over 472k sentences, covering 13 demographic axes) from English into 164 languages. An automatic toxicity evaluation shows that added toxicity across languages varies from 0% to 5%. The output languages with the most added toxicity tend to be low-resource ones, and the demographic axes with the most added toxicity include sexual orientation, gender and sex, and ability. We also perform human evaluation on a subset of 8 translation directions, confirming the prevalence of true added toxicity. We use a measurement of the amount of source contribution to the translation, where a low source contribution implies hallucination, to interpret what causes toxicity. Making use of the input attributions allows us to explain toxicity, because the source contributions significantly correlate with toxicity for 84% of languages studied. Given our findings, our recommendations to reduce added toxicity are to curate training data to avoid mistranslations, mitigate hallucination and check unstable translations.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.642.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Machine Translation systems can produce different types of errors... We use a measurement of the amount of source contribution to the translation, where a low source contribution implies hallucination, to interpret what causes toxicity.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Machine Translation systems can produce different types of errors... We use a measurement of the amount of source contribution to the translation, where a low source contribution implies hallucination, to interpret what causes toxicity.\""
    },
    {
        "title": "Conversational Recommender System and Large Language Model Are Made for Each Other in E-commerce Pre-sales Dialogue",
        "authors": [
            "Yuanxing Liu",
            "Weinan Zhang",
            "Yifan Chen",
            "Yuchi Zhang",
            "Haopeng Bai",
            "Fan Feng",
            "Hengbin Cui",
            "Yongbin Li",
            "Wanxiang Che"
        ],
        "published": "2023",
        "summary": "E-commerce pre-sales dialogue aims to understand and elicit user needs and preferences for the items they are seeking so as to provide appropriate recommendations. Conversational recommender systems (CRSs) learn user representation and provide accurate recommendations based on dialogue context, but rely on external knowledge. Large language models (LLMs) generate responses that mimic pre-sales dialogues after fine-tuning, but lack domain-specific knowledge for accurate recommendations. Intuitively, the strengths of LLM and CRS in E-commerce pre-sales dialogues are complementary, yet no previous work has explored this. This paper investigates the effectiveness of combining LLM and CRS in E-commerce pre-sales dialogues, proposing two collaboration methods: CRS assisting LLM and LLM assisting CRS. We conduct extensive experiments on a real-world dataset of E-commerce pre-sales dialogues. We analyze the impact of two collaborative approaches with two CRSs and two LLMs on four tasks of E-commerce pre-sales dialogue. We find that collaborations between CRS and LLM can be very effective in some cases.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.643.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models (LLMs) generate responses that mimic pre-sales dialogues after fine-tuning, but lack domain-specific knowledge for accurate recommendations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large language models (LLMs) generate responses that mimic pre-sales dialogues after fine-tuning, but lack domain-specific knowledge for accurate recommendations.\""
    },
    {
        "title": "VIP5: Towards Multimodal Foundation Models for Recommendation",
        "authors": [
            "Shijie Geng",
            "Juntao Tan",
            "Shuchang Liu",
            "Zuohui Fu",
            "Yongfeng Zhang"
        ],
        "published": "2023",
        "summary": "Computer Vision (CV), Natural Language Processing (NLP), and Recommender Systems (RecSys) are three prominent AI applications that have traditionally developed independently, resulting in disparate modeling and engineering methodologies. This has impeded the ability for these fields to directly benefit from each other’s advancements. With the recent development of foundation models, large language models have emerged as a potential general-purpose interface for unifying different modalities and problem formulations. In light of this, we propose the development of a multimodal foundation model (MFM) considering visual, textual, and personalization modalities under the P5 recommendation paradigm, thus named VIP5 (Visual P5), to unify various modalities and recommendation tasks. This will enable the processing of multiple modalities in a shared architecture for improved recommendations. To achieve this, we introduce multimodal personalized prompts to accommodate multiple modalities under a shared format. Additionally, we propose a parameter-efficient training method for foundation models, which involves freezing the P5 backbone and fine-tuning lightweight adapters, resulting in improved recommendation performance and increased efficiency in terms of training time and memory usage. Code and data of VIP5 are available at https://github.com/jeykigung/VIP5.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.644.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but mentions \"foundation models\" and \"large language models\" in the context of their potential as a general-purpose interface, but does not discuss limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but mentions \"foundation models\" and \"large language models\" in the context of their potential as a general-purpose interface, but does not discuss limitations."
    },
    {
        "title": "Learning to Follow Object-Centric Image Editing Instructions Faithfully",
        "authors": [
            "Tuhin Chakrabarty",
            "Kanishk Singh",
            "Arkadiy Saakyan",
            "Smaranda Muresan"
        ],
        "published": "2023",
        "summary": "Natural language instructions are a powerful interface for editing the outputs of text-to-image diffusion models. However, several challenges need to be addressed: 1) underspecification (the need to model the implicit meaning of instructions) 2) grounding (the need to localize where the edit has to be performed), 3) faithfulness (the need to preserve the elements of the image not affected by the edit instruction). Current approaches focusing on image editing with natural language instructions rely on automatically generated paired data, which, as shown in our investigation, is noisy and sometimes nonsensical, exacerbating the above issues. Building on recent advances in segmentation, Chain-of-Thought prompting, and visual question answering, we significantly improve the quality of the paired data. In addition, we enhance the supervision signal by highlighting parts of the image that need to be changed by the instruction. The model fine-tuned on the improved data is capable of performing fine-grained object-centric edits better than state-of-the-art baselines, mitigating the problems outlined above, as shown by automatic and human evaluations. Moreover, our model is capable of generalizing to domains unseen during training, such as visual metaphors.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.646.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Zero-shot Topical Text Classification with LLMs - an Experimental Study",
        "authors": [
            "Shai Gretz",
            "Alon Halfon",
            "Ilya Shnayderman",
            "Orith Toledo-Ronen",
            "Artem Spector",
            "Lena Dankin",
            "Yannis Katsis",
            "Ofir Arviv",
            "Yoav Katz",
            "Noam Slonim",
            "Liat Ein-Dor"
        ],
        "published": "2023",
        "summary": "Topical Text Classification (TTC) is an ancient, yet timely research area in natural language processing, with many practical applications. The recent dramatic advancements in large LMs raise the question of how well these models can perform in this task in a zero-shot scenario. Here, we share a first comprehensive study, comparing the zero-shot performance of a variety of LMs over TTC23, a large benchmark collection of 23 publicly available TTC datasets, covering a wide range of domains and styles. In addition, we leverage this new TTC benchmark to create LMs that are specialized in TTC, by fine-tuning these LMs over a subset of the datasets and evaluating their performance over the remaining, held-out datasets. We show that the TTC-specialized LMs obtain the top performance on our benchmark, by a significant margin. Our code and model are made available for the community. We hope that the results presented in this work will serve as a useful guide for practitioners interested in topical text classification.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.647.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, the paper only discusses the performance of LLMs in a zero-shot scenario and presents a study on their capabilities.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, the paper only discusses the performance of LLMs in a zero-shot scenario and presents a study on their capabilities."
    },
    {
        "title": "Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems",
        "authors": [
            "Yixin Wan",
            "Jieyu Zhao",
            "Aman Chadha",
            "Nanyun Peng",
            "Kai-Wei Chang"
        ],
        "published": "2023",
        "summary": "Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations. We define generic personas to represent demographic groups, such as “an Asian person”, whereas specific personas may take the form of specific popular Asian names like “Yumi”. While the adoption of personas enriches user experiences by making dialogue systems more engaging and approachable, it also casts a shadow of potential risk by exacerbating social biases within model responses, thereby causing societal harm through interactions with users. In this paper, we systematically study “persona biases”, which we define to be the sensitivity of dialogue models’ harmful behaviors contingent upon the personas they adopt. We categorize persona biases into biases in harmful expression and harmful agreement, and establish a comprehensive evaluation framework to measure persona biases in five aspects: Offensiveness, Toxic Continuation, Regard, Stereotype Agreement, and Toxic Agreement. Additionally, we propose to investigate persona biases by experimenting with UNIVERSALPERSONA, a systematically constructed persona dataset encompassing various types of both generic and specific model personas. Through benchmarking on four different models- including Blender, ChatGPT, Alpaca, and Vicuna- our study uncovers significant persona biases in dialogue systems. Our findings also underscore the pressing need to revisit the use of personas in dialogue agents to ensure safe application.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.648.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"it also casts a shadow of potential risk by exacerbating social biases within model responses, thereby causing societal harm through interactions with users.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"it also casts a shadow of potential risk by exacerbating social biases within model responses, thereby causing societal harm through interactions with users.\""
    },
    {
        "title": "CASSI: Contextual and Semantic Structure-based Interpolation Augmentation for Low-Resource NER",
        "authors": [
            "Tanmay Surana",
            "Thi-Nga Ho",
            "Kyaw Tun",
            "Eng Siong Chng"
        ],
        "published": "2023",
        "summary": "While text augmentation methods have been successful in improving performance in the low-resource setting, they suffer from annotation corruption for a token-level task like NER. Moreover, existing methods cannot reliably add context diversity to the dataset, which has been shown to be crucial for low-resource NER. In this work, we propose Contextual and Semantic Structure-based Interpolation (CASSI), a novel augmentation scheme that generates high-quality contextually diverse augmentations while avoiding annotation corruption by structurally combining a pair of semantically similar sentences to generate a new sentence while maintaining semantic correctness and fluency. To accomplish this, we generate candidate augmentations by performing multiple dependency parsing-based exchanges in a pair of semantically similar sentences that are filtered via scoring with a pretrained Masked Language Model and a metric to promote specificity. Experiments show that CASSI consistently outperforms existing methods at multiple low resource levels, in multiple languages, and for noisy and clean text.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.651.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"To accomplish this, we generate candidate augmentations by performing multiple dependency parsing-based exchanges in a pair of semantically similar sentences that are filtered via scoring with a pretrained Masked Language Model\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"To accomplish this, we generate candidate augmentations by performing multiple dependency parsing-based exchanges in a pair of semantically similar sentences that are filtered via scoring with a pretrained Masked Language Model\""
    },
    {
        "title": "NEWTON: Are Large Language Models Capable of Physical Reasoning?",
        "authors": [
            "Yi Wang",
            "Jiafei Duan",
            "Dieter Fox",
            "Siddhartha Srinivasa"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs), through their contextualized representations, have been empirically proven to encapsulate syntactic, semantic, word sense, and common-sense knowledge. However, there has been limited exploration of their physical reasoning abilities, specifically concerning the crucial attributes for comprehending everyday objects. To address this gap, we introduce NEWTON, a repository and benchmark for evaluating the physics reasoning skills of LLMs. Further, to enable domain-specific adaptation of this benchmark, we present a pipeline to enable researchers to generate a variant of this benchmark that has been customized to the objects and attributes relevant for their application. The NEWTON repository comprises a collection of 2800 object-attribute pairs, providing the foundation for generating infinite-scale assessment templates. The NEWTON benchmark consists of 160K QA questions, curated using the NEWTON repository to investigate the physical reasoning capabilities of several mainstream language models across foundational, explicit, and implicit reasoning tasks. Through extensive empirical analysis, our results highlight the capabilities of LLMs for physical reasoning. We find that LLMs like GPT-4 demonstrate strong reasoning capabilities in scenario-based tasks but exhibit less consistency in object-attribute reasoning compared to humans (50% vs. 84%). Furthermore, the NEWTON platform demonstrates its potential for evaluating and enhancing language models, paving the way for their integration into physically grounded settings, such as robotic manipulation. Project site: https://newtonreasoning.github.io",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.652.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that LLMs like GPT-4 demonstrate strong reasoning capabilities in scenario-based tasks but exhibit less consistency in object-attribute reasoning compared to humans (50% vs. 84%).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We find that LLMs like GPT-4 demonstrate strong reasoning capabilities in scenario-based tasks but exhibit less consistency in object-attribute reasoning compared to humans (50% vs. 84%).\""
    },
    {
        "title": "On the Calibration of Large Language Models and Alignment",
        "authors": [
            "Chiwei Zhu",
            "Benfeng Xu",
            "Quan Wang",
            "Yongdong Zhang",
            "Zhendong Mao"
        ],
        "published": "2023",
        "summary": "As large language models attract increasing attention and find widespread application, concurrent challenges of reliability also arise at the same time. Confidence calibration, an effective analysis method for gauging the reliability of deep models, serves as a crucial tool for assessing and improving their reliability. However, such investigation has been comparatively underexplored. In this work, we conduct a systematic examination of the calibration of aligned language models throughout the entire construction process, including pretraining and alignment training. At each stage, we investigate how different training settings, such as parameter scales and training data, affect model calibration. To thoroughly assess model calibration, we evaluate models on three most concerned aspects: generation, factuality and understanding. Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.654.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, such investigation has been comparatively underexplored.\"; \"Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, such investigation has been comparatively underexplored.\"; \"Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.\""
    },
    {
        "title": "TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction",
        "authors": [
            "Junyi Liu",
            "Liangzhi Li",
            "Tong Xiang",
            "Bowen Wang",
            "Yiming Qian"
        ],
        "published": "2023",
        "summary": "Since ChatGPT released its API for public use, the number of applications built on top of commercial large language models (LLMs) increase exponentially. One popular usage of such models is leveraging its in-context learning ability and generating responses given user queries leveraging knowledge obtained by retrieval augmentation. One problem of deploying commercial retrieval-augmented LLMs is the cost due to the additionally retrieved context that largely increases the input token size of the LLMs. To mitigate this, we propose a token compression scheme that includes two methods: summarization compression and semantic compression. The first method applies a T5-based model that is fine-tuned by datasets generated using self-instruct containing samples with varying lengths and reduce token size by doing summarization. The second method further compresses the token size by removing words with lower impact on the semantic. In order to adequately evaluate the effectiveness of the proposed methods, we propose and utilize a dataset called Food-Recommendation DB (FRDB) focusing on food recommendation for women around pregnancy period or infants. Our summarization compression can reduce 65% of the retrieval token size with further 0.3% improvement on the accuracy; semantic compression provides a more flexible way to trade-off the token size with performance, for which we can reduce the token size by 20% with only 1.6% of accuracy drop.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.655.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"One problem of deploying commercial retrieval-augmented LLMs is the cost due to the additionally retrieved context that largely increases the input token size of the LLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"One problem of deploying commercial retrieval-augmented LLMs is the cost due to the additionally retrieved context that largely increases the input token size of the LLMs.\""
    },
    {
        "title": "Identifying Conspiracy Theories News based on Event Relation Graph",
        "authors": [
            "Yuanyuan Lei",
            "Ruihong Huang"
        ],
        "published": "2023",
        "summary": "Conspiracy theories, as a type of misinformation, are narratives that explains an event or situation in an irrational or malicious manner. While most previous work examined conspiracy theory in social media short texts, limited attention was put on such misinformation in long news documents. In this paper, we aim to identify whether a news article contains conspiracy theories. We observe that a conspiracy story can be made up by mixing uncorrelated events together, or by presenting an unusual distribution of relations between events. Achieving a contextualized understanding of events in a story is essential for detecting conspiracy theories. Thus, we propose to incorporate an event relation graph for each article, in which events are nodes, and four common types of event relations, coreference, temporal, causal, and subevent relations, are considered as edges. Then, we integrate the event relation graph into conspiracy theory identification in two ways: an event-aware language model is developed to augment the basic language model with the knowledge of events and event relations via soft labels; further, a heterogeneous graph attention network is designed to derive a graph embedding based on hard labels. Experiments on a large benchmark dataset show that our approach based on event relation graph improves both precision and recall of conspiracy theory identification, and generalizes well for new unseen media sources.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.656.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"an event-aware language model is developed to augment the basic language model with the knowledge of events and event relations via soft labels\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"an event-aware language model is developed to augment the basic language model with the knowledge of events and event relations via soft labels\""
    },
    {
        "title": "Salespeople vs SalesBot: Exploring the Role of Educational Value in Conversational Recommender Systems",
        "authors": [
            "Lidiya Murakhovs’ka",
            "Philippe Laban",
            "Tian Xie",
            "Caiming Xiong",
            "Chien-Sheng Wu"
        ],
        "published": "2023",
        "summary": "Making big purchases requires consumers to research or consult a salesperson to gain domain expertise. However, existing conversational recommender systems (CRS) often overlook users’ lack of background knowledge, focusing solely on gathering preferences. In this work, we define a new problem space for conversational agents that aim to provide both product recommendations and educational value through mixed-type mixed-initiative dialog. We introduce SalesOps, a framework that facilitates the simulation and evaluation of such systems by leveraging recent advancements in large language models (LLMs). We build SalesBot and ShopperBot, a pair of LLM-powered agents that can simulate either side of the framework. A comprehensive human study compares SalesBot against professional salespeople, revealing that although SalesBot approaches professional performance in terms of fluency and informativeness, it lags behind in recommendation quality. We emphasize the distinct limitations both face in providing truthful information, highlighting the challenges of ensuring faithfulness in the CRS context. We release our code and make all data available.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.657.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We emphasize the distinct limitations both face in providing truthful information, highlighting the challenges of ensuring faithfulness in the CRS context.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We emphasize the distinct limitations both face in providing truthful information, highlighting the challenges of ensuring faithfulness in the CRS context.\""
    },
    {
        "title": "Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models",
        "authors": [
            "Zhihan Zhang",
            "Shuohang Wang",
            "Wenhao Yu",
            "Yichong Xu",
            "Dan Iter",
            "Qingkai Zeng",
            "Yang Liu",
            "Chenguang Zhu",
            "Meng Jiang"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) can perform a wide range of tasks by following natural language instructions, without the necessity of task-specific fine-tuning. Unfortunately, the performance of LLMs is greatly influenced by the quality of these instructions, and manually writing effective instructions for each task is a laborious and subjective process. In this paper, we introduce Auto-Instruct, a novel method to automatically improve the quality of instructions provided to LLMs. Our method leverages the inherent generative ability of LLMs to produce diverse candidate instructions for a given task, and then ranks them using a scoring model trained on a variety of 575 existing NLP tasks. In experiments on 118 out-of-domain tasks, Auto-Instruct surpasses both human-written instructions and existing baselines of LLM-generated instructions. Furthermore, our method exhibits notable generalizability even with other LLMs that are not incorporated into its training process.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.659.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unfortunately, the performance of LLMs is greatly influenced by the quality of these instructions, and manually writing effective instructions for each task is a laborious and subjective process.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Unfortunately, the performance of LLMs is greatly influenced by the quality of these instructions, and manually writing effective instructions for each task is a laborious and subjective process.\""
    },
    {
        "title": "M2C: Towards Automatic Multimodal Manga Complement",
        "authors": [
            "Hongcheng Guo",
            "Boyang Wang",
            "Jiaqi Bai",
            "Jiaheng Liu",
            "Jian Yang",
            "Zhoujun Li"
        ],
        "published": "2023",
        "summary": "Multimodal manga analysis focuses on enhancing manga understanding with visual and textual features, which has attracted considerable attention from both natural language processing and computer vision communities. Currently, most comics are hand-drawn and prone to problems such as missing pages, text contamination, and text aging, resulting in missing comic text content and seriously hindering human comprehension. In other words, the Multimodal Manga Complement (M2C) task has not been investigated, which aims to handle the aforementioned issues by providing a shared semantic space for vision and language understanding. To this end, we first propose the Multimodal Manga Complement task by establishing a new M2C benchmark dataset covering two languages. First, we design a manga argumentation method called MCoT to mine event knowledge in comics with large language models. Then, an effective baseline FVP-M2 using fine-grained visual prompts is proposed to support manga complement. Extensive experimental results show the effectiveness of FVP-M2 method for Multimodal Mange Complement.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.661.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we design a manga argumentation method called MCoT to mine event knowledge in comics with large language models.\"\n\nThis paper mentions LLMs but does not discuss any limitations of the models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"we design a manga argumentation method called MCoT to mine event knowledge in comics with large language models.\"\n\nThis paper mentions LLMs but does not discuss any limitations of the models."
    },
    {
        "title": "Learn Your Tokens: Word-Pooled Tokenization for Language Modeling",
        "authors": [
            "Avijit Thawani",
            "Saurabh Ghanekar",
            "Xiaoyuan Zhu",
            "Jay Pujara"
        ],
        "published": "2023",
        "summary": "Language models typically tokenize text into subwords, using a deterministic, hand-engineered heuristic of combining characters into longer surface-level strings such as ‘ing’ or whole words. Recent literature has repeatedly shown the limitations of such a tokenization strategy, particularly for documents not written in English and for representing numbers. On the other extreme, byte/character-level language models are much less restricted but suffer from increased sequence description lengths and a subsequent quadratic expansion in self-attention computation. Recent attempts to compress and limit these context lengths with fixed size convolutions is helpful but completely ignores the word boundary. This paper considers an alternative ‘learn your tokens’ scheme which utilizes the word boundary to pool bytes/characters into word representations, which are fed to the primary language model, before again decoding individual characters/bytes per word in parallel. We find that our moderately expressive and moderately fast end-to-end tokenizer outperform by over ‘300%‘ both subwords and byte/character models over the intrinsic language modeling metric of next-word prediction across datasets. It particularly outshines on rare words, outperforming by a factor of 30! We extensively study the language modeling setup for all three categories of tokenizers and theoretically analyze how our end-to-end models can also be a strong trade-off in efficiency and robustness.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.662.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent literature has repeatedly shown the limitations of such a tokenization strategy, particularly for documents not written in English and for representing numbers.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Recent literature has repeatedly shown the limitations of such a tokenization strategy, particularly for documents not written in English and for representing numbers.\""
    },
    {
        "title": "Towards Detecting Contextual Real-Time Toxicity for In-Game Chat",
        "authors": [
            "Zachary Yang",
            "Nicolas Grenon-Godbout",
            "Reihaneh Rabbany"
        ],
        "published": "2023",
        "summary": "Real-time toxicity detection in online environments poses a significant challenge, due to the increasing prevalence of social media and gaming platforms. We introduce ToxBuster, a simple and scalable model that reliably detects toxic content in real-time for a line of chat by including chat history and metadata. ToxBuster consistently outperforms conventional toxicity models across popular multiplayer games, including Rainbow Six Siege, For Honor, and DOTA 2. We conduct an ablation study to assess the importance of each model component and explore ToxBuster’s transferability across the datasets. Furthermore, we showcase ToxBuster’s efficacy in post-game moderation, successfully flagging 82.1% of chat-reported players at a precision level of 90.0%. Additionally, we show how an additional 6% of unreported toxic players can be proactively moderated.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.663.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Do Stochastic Parrots have Feelings Too? Improving Neural Detection of Synthetic Text via Emotion Recognition",
        "authors": [
            "Alan Cowap",
            "Yvette Graham",
            "Jennifer Foster"
        ],
        "published": "2023",
        "summary": "Recent developments in generative AI have shone a spotlight on high-performance synthetic text generation technologies. The now wide availability and ease of use of such models highlights the urgent need to provide equally powerful technologies capable of identifying synthetic text. With this in mind, we draw inspiration from psychological studies which suggest that people can be driven by emotion and encode emotion in the text they compose. We hypothesize that pretrained language models (PLMs) have an affective deficit because they lack such an emotional driver when generating text and consequently may generate synthetic text which has affective incoherence i.e. lacking the kind of emotional coherence present in human-authored text. We subsequently develop an emotionally aware detector by fine-tuning a PLM on emotion. Experiment results indicate that our emotionally-aware detector achieves improvements across a range of synthetic text generators, various sized models, datasets, and domains. Finally, we compare our emotionally-aware synthetic text detector to ChatGPT in the task of identification of its own output and show substantial gains, reinforcing the potential of emotion as a signal to identify synthetic text. Code, models, and datasets are available at https: //github.com/alanagiasi/emoPLMsynth",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.665.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We hypothesize that pretrained language models (PLMs) have an affective deficit because they lack such an emotional driver when generating text and consequently may generate synthetic text which has affective incoherence i.e. lacking the kind of emotional coherence present in human-authored text.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We hypothesize that pretrained language models (PLMs) have an affective deficit because they lack such an emotional driver when generating text and consequently may generate synthetic text which has affective incoherence i.e. lacking the kind of emotional coherence present in human-authored text.\""
    },
    {
        "title": "Variator: Accelerating Pre-trained Models with Plug-and-Play Compression Modules",
        "authors": [
            "Chaojun Xiao",
            "Yuqi Luo",
            "Wenbin Zhang",
            "Pengle Zhang",
            "Xu Han",
            "Yankai Lin",
            "Zhengyan Zhang",
            "Ruobing Xie",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Jie Zhou"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have achieved remarkable results on NLP tasks but at the expense of huge parameter sizes and the consequent computational costs. In this paper, we propose Variator, a parameter-efficient acceleration method that enhances computational efficiency through plug-and-play compression plugins. Compression plugins are designed to reduce the sequence length via compressing multiple hidden vectors into one and trained with original LLMs frozen. Different from traditional model acceleration methods, which compress LLMs to smaller sizes, Variator offers two distinct advantages: (1) In real-world applications, the plug-and-play nature of our compression plugins enables dynamic selection of different compression plugins with varying acceleration ratios based on the current workload. (2) The compression plugin comprises a few compact neural network layers with minimal parameters, significantly saving storage and memory overhead, particularly in scenarios with a growing number of tasks. We validate the effectiveness of Variator on seven datasets. Experimental results show that Variator can save 53% computational costs using only 0.9% additional parameters with a performance drop of less than 2%. Moreover, when the model scales to billions of parameters, Variator matches the strong performance of uncompressed LLMs. Our code and checkpoints will be released to facilitate future work.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.666.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models (LLMs) have achieved remarkable results on NLP tasks but at the expense of huge parameter sizes and the consequent computational costs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large language models (LLMs) have achieved remarkable results on NLP tasks but at the expense of huge parameter sizes and the consequent computational costs.\""
    },
    {
        "title": "PivotFEC: Enhancing Few-shot Factual Error Correction with a Pivot Task Approach using Large Language Models",
        "authors": [
            "Xingwei He",
            "A-Long Jin",
            "Jun Ma",
            "Yuan Yuan",
            "Siu Yiu"
        ],
        "published": "2023",
        "summary": "Factual Error Correction (FEC) aims to rectify false claims by making minimal revisions to align them more accurately with supporting evidence. However, the lack of datasets containing false claims and their corresponding corrections has impeded progress in this field. Existing distantly supervised models typically employ the mask-then-correct paradigm, where a masker identifies problematic spans in false claims, followed by a corrector to predict the masked portions. Unfortunately, accurately identifying errors in claims is challenging, leading to issues like over-erasure and incorrect masking. To overcome these challenges, we present PivotFEC, a method that enhances few-shot FEC with a pivot task approach using large language models (LLMs). Specifically, we introduce a pivot task called factual error injection, which leverages LLMs (e.g., ChatGPT) to intentionally generate text containing factual errors under few-shot settings; then, the generated text with factual errors can be used to train the FEC corrector. Our experiments on a public dataset demonstrate the effectiveness of PivotFEC in two significant ways: Firstly, it improves the widely-adopted SARI metrics by 11.3 compared to the best-performing distantly supervised methods. Secondly, it outperforms its few-shot counterpart (i.e., LLMs are directly used to solve FEC) by 7.9 points in SARI, validating the efficacy of our proposed pivot task.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.667.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unfortunately, accurately identifying errors in claims is challenging, leading to issues like over-erasure and incorrect masking.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (accurately identifying errors in claims) in passing, but it is not the primary focus of the paper. The main focus is on the proposed solution, PivotFEC, and its",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Unfortunately, accurately identifying errors in claims is challenging, leading to issues like over-erasure and incorrect masking.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (accurately identifying errors in claims) in passing, but it is not the primary focus of the paper. The main focus is on the proposed solution, PivotFEC, and its"
    },
    {
        "title": "LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis",
        "authors": [
            "Shih-Chieh Dai",
            "Aiping Xiong",
            "Lun-Wei Ku"
        ],
        "published": "2023",
        "summary": "Thematic analysis (TA) has been widely used for analyzing qualitative data in many disciplines and fields. To ensure reliable analysis, the same piece of data is typically assigned to at least two human coders. Moreover, to produce meaningful and useful analysis, human coders develop and deepen their data interpretation and coding over multiple iterations, making TA labor-intensive and time-consuming. Recently the emerging field of large language models (LLMs) research has shown that LLMs have the potential replicate human-like behavior in various tasks: in particular, LLMs outperform crowd workers on text-annotation tasks, suggesting an opportunity to leverage LLMs on TA. We propose a human–LLM collaboration framework (i.e., LLM-in-the-loop) to conduct TA with in-context learning (ICL). This framework provides the prompt to frame discussions with a LLM (e.g., GPT-3.5) to generate the final codebook for TA. We demonstrate the utility of this framework using survey datasets on the aspects of the music listening experience and the usage of a password manager. Results of the two case studies show that the proposed framework yields similar coding quality to that of human coders but reduces TA’s labor and time demands.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.669.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recently the emerging field of large language models (LLMs) research has shown that LLMs have the potential replicate human-like behavior in various tasks\"\n\n(Note: The paper does not explicitly mention any limitations of LLMs, but rather highlights their potential in replicating human-like behavior, which is why it is rated as 1)",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Recently the emerging field of large language models (LLMs) research has shown that LLMs have the potential replicate human-like behavior in various tasks\"\n\n(Note: The paper does not explicitly mention any limitations of LLMs, but rather highlights their potential in replicating human-like behavior, which is why it is rated as 1)"
    },
    {
        "title": "LLM aided semi-supervision for efficient Extractive Dialog Summarization",
        "authors": [
            "Nishant Mishra",
            "Gaurav Sahu",
            "Iacer Calixto",
            "Ameen Abu-Hanna",
            "Issam Laradji"
        ],
        "published": "2023",
        "summary": "Generating high-quality summaries for chat dialogs often requires large labeled datasets. We propose a method to efficiently use unlabeled data for extractive summarization of customer-agent dialogs. In our method, we frame summarization as a question-answering problem and use state-of-the-art large language models (LLMs) to generate pseudo-labels for a dialog. We then use these pseudo-labels to fine-tune a chat summarization model, effectively transferring knowledge from the large LLM into a smaller specialized model. We demonstrate our method on the TWEETSUMM dataset, and show that using 10% of the original labelled data set we can achieve 65.9/57.0/61.0 ROUGE-1/-2/-L, whereas the current state-of-the-art trained on the entire training data set obtains 65.16/55.81/64.37 ROUGE-1/-2/-L. In other words, in the worst case (i.e., ROUGE-L) we still effectively retain 94.7% of the performance while using only 10% of the data.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.670.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"use state-of-the-art large language models (LLMs) to generate pseudo-labels for a dialog.\"\n\nThis paper mentions LLMs but does not discuss any limitations of the models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"use state-of-the-art large language models (LLMs) to generate pseudo-labels for a dialog.\"\n\nThis paper mentions LLMs but does not discuss any limitations of the models."
    },
    {
        "title": "Balaur: Language Model Pretraining with Lexical Semantic Relations",
        "authors": [
            "Andrei Mircea",
            "Jackie Cheung"
        ],
        "published": "2023",
        "summary": "Lexical semantic relations (LSRs) characterize meaning relationships between words and play an important role in systematic generalization on lexical inference tasks. Notably, several tasks that require knowledge of hypernymy still pose a challenge for pretrained language models (LMs) such as BERT, underscoring the need to better align their linguistic behavior with our knowledge of LSRs. In this paper, we propose Balaur, a model that addresses this challenge by modeling LSRs directly in the LM’s hidden states throughout pretraining. Motivating our approach is the hypothesis that the internal representations of LMs can provide an interface to their observable linguistic behavior, and that by controlling one we can influence the other. We validate our hypothesis and demonstrate that Balaur generally improves the performance of large transformer-based LMs on a comprehensive set of hypernymy-informed tasks, as well as on the original LM objective. Code and data are made available at https://github.com/mirandrom/balaur",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.674.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Notably, several tasks that require knowledge of hypernymy still pose a challenge for pretrained language models (LMs) such as BERT, underscoring the need to better align their linguistic behavior with our knowledge of LSRs.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs (difficulty with tasks requiring knowledge of hypernymy) but does not",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Notably, several tasks that require knowledge of hypernymy still pose a challenge for pretrained language models (LMs) such as BERT, underscoring the need to better align their linguistic behavior with our knowledge of LSRs.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs (difficulty with tasks requiring knowledge of hypernymy) but does not"
    },
    {
        "title": "Exploring In-Context Learning for Knowledge Grounded Dialog Generation",
        "authors": [
            "Qinyu Chen",
            "Wenhao Wu",
            "Sujian Li"
        ],
        "published": "2023",
        "summary": "Large neural-based dialog generation models have been applied in many real-life scenarios, yet they are prone to hallucination and tend to produce factually inaccurate outputs which raise great concerns. To alleviate this problem, we propose a plug-and-play retrieval-based framework IKA, which leverages in-context learning and retrieval techniques to enhance LLMs on knowledge grounded dialog generation. We design thorough experiments on a large-scale knowledge graph with 1M+ facts to investigate the effectiveness and generalization of our framework. Experiments show that our method surpasses previous training-based SOTA by a large margin, specifically 46.67% in BLEU4, 26.01% in ROUGE-L, 122.90% in BARTScore and 30.50% in Entity Coverage F1. Further analysis show promising abilities of LLMs to perform knowledge-intensive tasks, which is previously considered weak and understudied.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.675.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large neural-based dialog generation models... are prone to hallucination and tend to produce factually inaccurate outputs which raise great concerns.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large neural-based dialog generation models... are prone to hallucination and tend to produce factually inaccurate outputs which raise great concerns.\""
    },
    {
        "title": "Are NLP Models Good at Tracing Thoughts: An Overview of Narrative Understanding",
        "authors": [
            "Lixing Zhu",
            "Runcong Zhao",
            "Lin Gui",
            "Yulan He"
        ],
        "published": "2023",
        "summary": "Narrative understanding involves capturing the author’s cognitive processes, providing insights into their knowledge, intentions, beliefs, and desires. Although large language models (LLMs) excel in generating grammatically coherent text, their ability to comprehend the author’s thoughts remains uncertain. This limitation hinders the practical applications of narrative understanding. In this paper, we conduct a comprehensive survey of narrative understanding tasks, thoroughly examining their key features, definitions, taxonomy, associated datasets, training objectives, evaluation metrics, and limitations. Furthermore, we explore the potential of expanding the capabilities of modularized LLMs to address novel narrative understanding tasks. By framing narrative understanding as the retrieval of the author’s imaginative cues that outline the narrative structure, our study introduces a fresh perspective on enhancing narrative comprehension.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.677.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although large language models (LLMs) excel in generating grammatically coherent text, their ability to comprehend the author’s thoughts remains uncertain. This limitation hinders the practical applications of narrative understanding.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Although large language models (LLMs) excel in generating grammatically coherent text, their ability to comprehend the author’s thoughts remains uncertain. This limitation hinders the practical applications of narrative understanding.\""
    },
    {
        "title": "Demystifying Prompts in Language Models via Perplexity Estimation",
        "authors": [
            "Hila Gonen",
            "Srini Iyer",
            "Terra Blevins",
            "Noah Smith",
            "Luke Zettlemoyer"
        ],
        "published": "2023",
        "summary": "Language models can be prompted to perform a wide variety of tasks with zero- and few-shot in-context learning. However, performance varies significantly with the choice of prompt, and we do not yet understand why this happens. In this paper, we analyze the factors that contribute to this variance and establish a new empirical hypothesis: the performance of a prompt is predicted by the extent to which the model is familiar with the language it contains. Over a wide range of tasks, we show that the lower the perplexity of the prompt, the better it is able to perform the task, when considering reasonable prompts that are related to it. As part of our analysis, we also devise a method to automatically extend a small seed set of manually written prompts by paraphrasing with GPT3 and backtranslation. This larger set allows us to verify that perplexity is a strong predictor of the success of a prompt and we show that the lowest perplexity prompts are consistently effective.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.679.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, performance varies significantly with the choice of prompt, and we do not yet understand why this happens.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, performance varies significantly with the choice of prompt, and we do not yet understand why this happens.\""
    },
    {
        "title": "ROME: Evaluating Pre-trained Vision-Language Models on Reasoning beyond Visual Common Sense",
        "authors": [
            "Kankan Zhou",
            "Eason Lai",
            "Wei Bin Au Yeong",
            "Kyriakos Mouratidis",
            "Jing Jiang"
        ],
        "published": "2023",
        "summary": "Humans possess a strong capability for reasoning beyond common sense. For example, given an unconventional image of a goldfish laying on the table next to an empty fishbowl, a human would effortlessly determine that the fish is not inside the fishbowl. The case, however, may be different for a vision-language model, whose reasoning could gravitate towards the common scenario that the fish is inside the bowl, despite the visual input. In this paper, we introduce a novel probing dataset named ROME (reasoning beyond commonsense knowledge) to evaluate whether the state-of-the-art pre-trained vision-language models have the reasoning capability to correctly interpret counter-intuitive content. ROME contains images that defy commonsense knowledge with regards to color, shape, material, size and positional relation. Experiments on the state-of-the-art pre-trained vision-language models reveal that most of these models are still largely incapable of interpreting counter-intuitive scenarios. We hope that ROME will spur further investigations on reasoning beyond commonsense knowledge in vision-language research.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.683.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experiments on the state-of-the-art pre-trained vision-language models reveal that most of these models are still largely incapable of interpreting counter-intuitive scenarios.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Experiments on the state-of-the-art pre-trained vision-language models reveal that most of these models are still largely incapable of interpreting counter-intuitive scenarios.\""
    },
    {
        "title": "Hierarchical Prompting Assists Large Language Model on Web Navigation",
        "authors": [
            "Robert Lo",
            "Abishek Sridhar",
            "Frank Xu",
            "Hao Zhu",
            "Shuyan Zhou"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) struggle on processing complicated observations in interactive decision making. To alleviate this issue, we propose a simple hierarchical prompting approach. Diverging from previous prompting approaches that always put the full observation (a web page) to the prompt, we propose to first construct an action-aware observation which is more condensed and relevant with a dedicated Summarizer prompt. The Actor prompt then predicts the next action based on the summarized history. While our method has broad applicability, we particularly demonstrate its efficacy in the complex domain of web navigation where a full observation often contains redundant and irrelevant information. Our approach outperforms the previous state-of-the-art prompting mechanism with the same LLM by 6.2% on task success rate, demonstrating its potential on interactive decision making tasks with long observation traces.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.685.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models (LLMs) struggle on processing complicated observations in interactive decision making.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large language models (LLMs) struggle on processing complicated observations in interactive decision making.\""
    },
    {
        "title": "Can Large Language Models Fix Data Annotation Errors? An Empirical Study Using Debatepedia for Query-Focused Text Summarization",
        "authors": [
            "Md Tahmid Rahman Laskar",
            "Mizanur Rahman",
            "Israt Jahan",
            "Enamul Hoque",
            "Jimmy Huang"
        ],
        "published": "2023",
        "summary": "Debatepedia is a publicly available dataset consisting of arguments and counter-arguments on controversial topics that has been widely used for the single-document query-focused abstractive summarization task in recent years. However, it has been recently found that this dataset is limited by noise and even most queries in this dataset do not have any relevance to the respective document. In this paper, we study whether large language models (LLMs) can be utilized to clean the Debatepedia dataset to make it suitable for query-focused abstractive summarization. More specifically, we harness the language generation capabilities of two LLMs, namely, ChatGPT and PaLM to regenerate its queries. Based on our experiments, we find that solely depending on large language models for query correction may not be very useful for data cleaning. However, we observe that leveraging a rule-based approach for data sampling followed by query regeneration using LLMs (especially ChatGPT) for the sampled instances may ensure a higher quality version of this dataset suitable for the development of more generalized query-focused text summarization models.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.686.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, we find that solely depending on large language models for query correction may not be very useful for data cleaning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, we find that solely depending on large language models for query correction may not be very useful for data cleaning.\""
    },
    {
        "title": "TSTR: Target Similarity Tuning Meets the Real World",
        "authors": [
            "Anirudh Khatry",
            "Sumit Gulwani",
            "Priyanshu Gupta",
            "Vu Le",
            "Mukul Singh",
            "Ananya Singha",
            "Gust Verbruggen"
        ],
        "published": "2023",
        "summary": "Target similarity tuning (TST) is a method of selecting relevant examples in natural language (NL) to code generation through large language models (LLMs) to improve performance. Its goal is to adapt a sentence embedding model to have the similarity between two NL inputs match the similarity between their associated code outputs. In this paper, we propose different methods to apply and improve TST in the real world. First, we replace the sentence transformer with embeddings from a larger model, which reduces sensitivity to the language distribution and thus provides more flexibility in synthetic generation of examples, and we train a tiny model that transforms these embeddings to a space where embedding similarity matches code similarity, which allows the model to remain a black box and only requires a few matrix multiplications at inference time. Second, we how to efficiently select a smaller number of training examples to train the TST model. Third, we introduce a ranking-based evaluation for TST that does not require end-to-end code generation experiments, which can be expensive to perform.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.687.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper mentions the goal of improving LLMs' performance through target similarity tuning, but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper mentions the goal of improving LLMs' performance through target similarity tuning, but does not discuss any limitations of LLMs."
    },
    {
        "title": "Unraveling Downstream Gender Bias from Large Language Models: A Study on AI Educational Writing Assistance",
        "authors": [
            "Thiemo Wambsganss",
            "Xiaotian Su",
            "Vinitra Swamy",
            "Seyed Neshaei",
            "Roman Rietsche",
            "Tanja Käser"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) are increasingly utilized in educational tasks such as providing writing suggestions to students. Despite their potential, LLMs are known to harbor inherent biases which may negatively impact learners. Previous studies have investigated bias in models and data representations separately, neglecting the potential impact of LLM bias on human writing. In this paper, we investigate how bias transfers through an AI writing support pipeline. We conduct a large-scale user study with 231 students writing business case peer reviews in German. Students are divided into five groups with different levels of writing support: one in-classroom group with recommender system feature-based suggestions and four groups recruited from Prolific – a control group with no assistance, two groups with suggestions from fine-tuned GPT-2 and GPT-3 models, and one group with suggestions from pre-trained GPT-3.5. Using GenBit gender bias analysis and Word Embedding Association Tests (WEAT), we evaluate the gender bias at various stages of the pipeline: in reviews written by students, in suggestions generated by the models, and in model embeddings directly. Our results demonstrate that there is no significant difference in gender bias between the resulting peer reviews of groups with and without LLM suggestions. Our research is therefore optimistic about the use of AI writing support in the classroom, showcasing a context where bias in LLMs does not transfer to students’ responses.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.689.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite their potential, LLMs are known to harbor inherent biases which may negatively impact learners.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite their potential, LLMs are known to harbor inherent biases which may negatively impact learners.\""
    },
    {
        "title": "Self-Knowledge Guided Retrieval Augmentation for Large Language Models",
        "authors": [
            "Yile Wang",
            "Peng Li",
            "Maosong Sun",
            "Yang Liu"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have shown superior performance without task-specific fine-tuning. Despite the success, the knowledge stored in the parameters of LLMs could still be incomplete and difficult to update due to the computational costs. As complementary, retrieval-based methods can offer non-parametric world knowledge and improve the performance on tasks such as question answering. However, we find that the retrieved knowledge does not always help and even has a negative impact on original responses occasionally. To better make use of both internal knowledge and external world knowledge, we investigate eliciting the model’s ability to recognize what they know and do not know (which is also called “self-knowledge”) and propose Self-Knowledge guided Retrieval augmentation (SKR), a simple yet effective method which can let LLMs refer to the questions they have previously encountered and adaptively call for external resources when dealing with new questions. We evaluate SKR on multiple datasets and demonstrate that it outperforms chain-of-thought based and fully retrieval-based methods by using either InstructGPT or ChatGPT.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.691.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the knowledge stored in the parameters of LLMs could still be incomplete and difficult to update due to the computational costs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"the knowledge stored in the parameters of LLMs could still be incomplete and difficult to update due to the computational costs.\""
    },
    {
        "title": "Pretraining Language Models with Text-Attributed Heterogeneous Graphs",
        "authors": [
            "Tao Zou",
            "Le Yu",
            "Yifei Huang",
            "Leilei Sun",
            "Bowen Du"
        ],
        "published": "2023",
        "summary": "In many real-world scenarios (e.g., academic networks, social platforms), different types of entities are not only associated with texts but also connected by various relationships, which can be abstracted as Text-Attributed Heterogeneous Graphs (TAHGs). Current pretraining tasks for Language Models (LMs) primarily focus on separately learning the textual information of each entity and overlook the crucial aspect of capturing topological connections among entities in TAHGs. In this paper, we present a new pretraining framework for LMs that explicitly considers the topological and heterogeneous information in TAHGs. Firstly, we define a context graph as neighborhoods of a target node within specific orders and propose a topology-aware pretraining task to predict nodes involved in the context graph by jointly optimizing an LM and an auxiliary heterogeneous graph neural network. Secondly, based on the observation that some nodes are text-rich while others have little text, we devise a text augmentation strategy to enrich textless nodes with their neighbors’ texts for handling the imbalance issue. We conduct link prediction and node classification tasks on three datasets from various domains. Experimental results demonstrate the superiority of our approach over existing methods and the rationality of each design. Our code is available at https://github.com/Hope-Rita/THLM.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.692.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Current pretraining tasks for Language Models (LMs) primarily focus on separately learning the textual information of each entity and overlook the crucial aspect of capturing topological connections among entities in TAHGs.\"\n\nThis rating is chosen because the abstract mentions a limitation of current pretraining tasks for LMs, but it is not the primary focus of the paper. The limitation is used to motivate the",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Current pretraining tasks for Language Models (LMs) primarily focus on separately learning the textual information of each entity and overlook the crucial aspect of capturing topological connections among entities in TAHGs.\"\n\nThis rating is chosen because the abstract mentions a limitation of current pretraining tasks for LMs, but it is not the primary focus of the paper. The limitation is used to motivate the"
    },
    {
        "title": "CReTIHC: Designing Causal Reasoning Tasks about Temporal Interventions and Hallucinated Confoundings",
        "authors": [
            "Changwoo Chun",
            "SongEun Lee",
            "Jaehyung Seo",
            "Heuiseok Lim"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their ability to establish causal relationships, particularly in the context of temporal interventions and language hallucinations, remains challenging. This paper presents CReTIHC, a novel dataset designed to test and enhance the causal reasoning abilities of LLMs. The dataset is constructed using a unique approach that incorporates elements of verbal hallucinations and temporal interventions through the reengineering of existing causal inference datasets. This transformation creates complex scenarios that push LLMs to critically evaluate the information presented and identify cause-and-effect relationships. The CReTIHC dataset serves as a pioneering tool for improving LLM’s causal inference capabilities, paving the way for a more nuanced understanding of causal relationships in natural language processing (NLP) tasks. The whole dataset is publicly accessible at: (https://github.com/ChangwooChun/CReTIHC)",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.693.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their ability to establish causal relationships, particularly in the context of temporal interventions and language hallucinations, remains challenging.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, their ability to establish causal relationships, particularly in the context of temporal interventions and language hallucinations, remains challenging.\""
    },
    {
        "title": "Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention",
        "authors": [
            "Huiyin Xue",
            "Nikolaos Aletras"
        ],
        "published": "2023",
        "summary": "Scaling pre-trained language models has resulted in large performance gains in various natural language processing tasks but comes with a large cost in memory requirements. Inspired by the position embeddings in transformers, we aim to simplify and reduce the memory footprint of the multi-head attention (MHA) mechanism. We propose an alternative module that uses only a single shared projection matrix and multiple head embeddings (MHE), i.e. one per head. We empirically demonstrate that our MHE attention is substantially more memory efficient compared to alternative attention mechanisms while achieving high predictive performance retention ratio to vanilla MHA on several downstream tasks. MHE attention only requires a negligible fraction of additional parameters (3nd, where n is the number of attention heads and d the size of the head embeddings) compared to a single-head attention, while MHA requires (3n2-3n)d2-3nd additional parameters.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.695.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Scaling pre-trained language models has resulted in large performance gains in various natural language processing tasks but comes with a large cost in memory requirements.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Scaling pre-trained language models has resulted in large performance gains in various natural language processing tasks but comes with a large cost in memory requirements.\""
    },
    {
        "title": "RSVP: Customer Intent Detection via Agent Response Contrastive and Generative Pre-Training",
        "authors": [
            "Yu-Chien Tang",
            "Wei-Yao Wang",
            "An-Zi Yen",
            "Wen-Chih Peng"
        ],
        "published": "2023",
        "summary": "The dialogue systems in customer services have been developed with neural models to provide users with precise answers and round-the-clock support in task-oriented conversations by detecting customer intents based on their utterances. Existing intent detection approaches have highly relied on adaptively pre-training language models with large-scale datasets, yet the predominant cost of data collection may hinder their superiority. In addition, they neglect the information within the conversational responses of the agents, which have a lower collection cost, but are significant to customer intent as agents must tailor their replies based on the customers’ intent. In this paper, we propose RSVP, a self-supervised framework dedicated to task-oriented dialogues, which utilizes agent responses for pre-training in a two-stage manner. Specifically, we introduce two pre-training tasks to incorporate the relations of utterance-response pairs: 1) Response Retrieval by selecting a correct response from a batch of candidates, and 2) Response Generation by mimicking agents to generate the response to a given utterance. Our benchmark results for two real-world customer service datasets show that RSVP significantly outperforms the state-of-the-art baselines by 4.95% for accuracy, 3.4% for MRR@3, and 2.75% for MRR@5 on average. Extensive case studies are investigated to show the validity of incorporating agent responses into the pre-training stage.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.698.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing intent detection approaches have highly relied on adaptively pre-training language models with large-scale datasets, yet the predominant cost of data collection may hinder their superiority.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing intent detection approaches have highly relied on adaptively pre-training language models with large-scale datasets, yet the predominant cost of data collection may hinder their superiority.\""
    },
    {
        "title": "InstructSafety: A Unified Framework for Building Multidimensional and Explainable Safety Detector through Instruction Tuning",
        "authors": [
            "Zhexin Zhang",
            "Jiale Cheng",
            "Hao Sun",
            "Jiawen Deng",
            "Minlie Huang"
        ],
        "published": "2023",
        "summary": "Safety detection has been an increasingly important topic in recent years and it has become even more necessary to develop reliable safety detection systems with the rapid development of large language models. However, currently available safety detection systems have limitations in terms of their versatility and interpretability. In this paper, we first introduce InstructSafety, a safety detection framework that unifies 7 common sub-tasks for safety detection. These tasks are unified into a similar form through different instructions. We then conduct a comprehensive survey of existing safety detection datasets and process 39 human-annotated datasets for instruction tuning. We also construct adversarial samples to enhance the model’s robustness. After fine-tuning Flan-T5 on the collected data, we have developed Safety-Flan-T5, a multidimensional and explainable safety detector. We conduct comprehensive experiments on a variety of datasets and tasks, and demonstrate the strong performance of Safety-Flan-T5 in comparison to supervised baselines and served APIs (Perspective API, ChatGPT and InstructGPT). We will release the processed data, fine-tuned Safety-Flan-T5 and related code for public use.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.700.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, currently available safety detection systems have limitations in terms of their versatility and interpretability.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, currently available safety detection systems have limitations in terms of their versatility and interpretability.\""
    },
    {
        "title": "“A Tale of Two Movements’: Identifying and Comparing Perspectives in #BlackLivesMatter and #BlueLivesMatter Movements-related Tweets using Weakly Supervised Graph-based Structured Prediction",
        "authors": [
            "Shamik Roy",
            "Dan Goldwasser"
        ],
        "published": "2023",
        "summary": "Social media has become a major driver of social change, by facilitating the formation of online social movements. Automatically understanding the perspectives driving the movement and the voices opposing it, is a challenging task as annotated data is difficult to obtain. We propose a weakly supervised graph-based approach that explicitly models perspectives in #BackLivesMatter-related tweets. Our proposed approach utilizes a social-linguistic representation of the data. We convert the text to a graph by breaking it into structured elements and connect it with the social network of authors, then structured prediction is done over the elements for identifying perspectives. Our approach uses a small seed set of labeled examples. We experiment with large language models for generating artificial training examples, compare them to manual annotation, and find that it achieves comparable performance. We perform quantitative and qualitative analyses using a human-annotated test set. Our model outperforms multitask baselines by a large margin, successfully characterizing the perspectives supporting and opposing #BLM.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.701.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We experiment with large language models for generating artificial training examples, compare them to manual annotation, and find that it achieves comparable performance.\"\n\nThis rating is given because the paper mentions LLMs but does not discuss any limitations of LLMs. It only mentions using LLMs for generating artificial training examples and compares their performance to manual annotation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"We experiment with large language models for generating artificial training examples, compare them to manual annotation, and find that it achieves comparable performance.\"\n\nThis rating is given because the paper mentions LLMs but does not discuss any limitations of LLMs. It only mentions using LLMs for generating artificial training examples and compares their performance to manual annotation."
    },
    {
        "title": "ClusterPrompt: Cluster Semantic Enhanced Prompt Learning for New Intent Discovery",
        "authors": [
            "Jinggui Liang",
            "Lizi Liao"
        ],
        "published": "2023",
        "summary": "The discovery of new intent categories from user utterances is a crucial task in expanding agent skills. The key lies in how to efficiently solicit semantic evidence from utterances and properly transfer knowledge from existing intents to new intents. However, previous methods laid too much emphasis on relations among utterances or clusters for transfer learning, while paying less attention to the usage of semantics. As a result, these methods suffer from in-domain over-fitting and often generate meaningless new intent clusters due to data distortion. In this paper, we present a novel approach called Cluster Semantic Enhanced Prompt Learning (CsePL) for discovering new intents. Our method leverages two-level contrastive learning with label semantic alignment to learn meaningful representations of intent clusters. These learned intent representations are then utilized as soft prompt initializations for discriminating new intents, reducing the dominance of existing intents. Extensive experiments conducted on three public datasets demonstrate the superiority of our proposed method. It not only outperforms existing methods but also suggests meaningful intent labels and enables early detection of new intents.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.702.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!",
        "authors": [
            "Yubo Ma",
            "Yixin Cao",
            "Yong Hong",
            "Aixin Sun"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have made remarkable strides in various tasks. Whether LLMs are competitive few-shot solvers for information extraction (IE) tasks, however, remains an open problem. In this work, we aim to provide a thorough answer to this question. Through extensive experiments on nine datasets across four IE tasks, we demonstrate that current advanced LLMs consistently exhibit inferior performance, higher latency, and increased budget requirements compared to fine-tuned SLMs under most settings. Therefore, we conclude that LLMs are not effective few-shot information extractors in general. Nonetheless, we illustrate that with appropriate prompting strategies, LLMs can effectively complement SLMs and tackle challenging samples that SLMs struggle with. And moreover, we propose an adaptive filter-then-rerank paradigm to combine the strengths of LLMs and SLMs. In this paradigm, SLMs serve as filters and LLMs serve as rerankers. By prompting LLMs to rerank a small portion of difficult samples identified by SLMs, our preliminary system consistently achieves promising improvements (2.4% F1-gain on average) on various IE tasks, with an acceptable time and cost investment.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.710.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Through extensive experiments on nine datasets across four IE tasks, we demonstrate that current advanced LLMs consistently exhibit inferior performance, higher latency, and increased budget requirements compared to fine-tuned SLMs under most settings.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Through extensive experiments on nine datasets across four IE tasks, we demonstrate that current advanced LLMs consistently exhibit inferior performance, higher latency, and increased budget requirements compared to fine-tuned SLMs under most settings.\""
    },
    {
        "title": "Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration",
        "authors": [
            "Yang Deng",
            "Lizi Liao",
            "Liang Chen",
            "Hongru Wang",
            "Wenqiang Lei",
            "Tat-Seng Chua"
        ],
        "published": "2023",
        "summary": "Conversational systems based on Large Language Models (LLMs), such as ChatGPT, show exceptional proficiency in context understanding and response generation. However, they still possess limitations, such as failing to ask clarifying questions to ambiguous queries or refuse users’ unreasonable requests, both of which are considered as key aspects of a conversational agent’s proactivity. This raises the question of whether LLM-based conversational systems are equipped to handle proactive dialogue problems. In this work, we conduct a comprehensive analysis of LLM-based conversational systems, specifically focusing on three key aspects of proactive dialogues: clarification, target-guided, and non-collaborative dialogues. To trigger the proactivity of LLMs, we propose the Proactive Chain-of-Thought prompting scheme, which augments LLMs with the goal planning capability over descriptive reasoning chains. Empirical findings are discussed to promote future studies on LLM-based proactive dialogue systems.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.711.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they still possess limitations, such as failing to ask clarifying questions to ambiguous queries or refuse users’ unreasonable requests, both of which are considered as key aspects of a conversational agent’s proactivity.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, they still possess limitations, such as failing to ask clarifying questions to ambiguous queries or refuse users’ unreasonable requests, both of which are considered as key aspects of a conversational agent’s proactivity.\""
    },
    {
        "title": "Ecologically Valid Explanations for Label Variation in NLI",
        "authors": [
            "Nan-Jiang Jiang",
            "Chenhao Tan",
            "Marie-Catherine de Marneffe"
        ],
        "published": "2023",
        "summary": "Human label variation, or annotation disagreement, exists in many natural language processing (NLP) tasks, including natural language inference (NLI). To gain direct evidence of how NLI label variation arises, we build LiveNLI, an English dataset of 1,415 ecologically valid explanations (annotators explain the NLI labels they chose) for 122 MNLI items (at least 10 explanations per item). The LiveNLI explanations confirm that people can systematically vary on their interpretation and highlight within-label variation: annotators sometimes choose the same label for different reasons. This suggests that explanations are crucial for navigating label interpretations in general. We few-shot prompt large language models to generate explanations but the results are inconsistent: they sometimes produces valid and informative explanations, but it also generates implausible ones that do not support the label, highlighting directions for improvement.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.712.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"they sometimes produces valid and informative explanations, but it also generates implausible ones that do not support the label, highlighting directions for improvement.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"they sometimes produces valid and informative explanations, but it also generates implausible ones that do not support the label, highlighting directions for improvement.\""
    },
    {
        "title": "SummIt: Iterative Text Summarization via ChatGPT",
        "authors": [
            "Haopeng Zhang",
            "Xiao Liu",
            "Jiawei Zhang"
        ],
        "published": "2023",
        "summary": "Existing text summarization systems have made significant progress in recent years, but typically generate summaries in a single step. The one-shot summarization setting is sometimes inadequate, however, as the generated summary may contain hallucinations or overlook important details related to the reader’s interests. In this paper, we address this limitation by proposing SummIt, an iterative text summarization framework based on large language models like ChatGPT. Our framework enables the model to refine the generated summary iteratively through self-evaluation and feedback, closely resembling the iterative process humans undertake when drafting and revising summaries. Furthermore, we explore the potential benefits of integrating knowledge and topic extractors into the framework to enhance summary faithfulness and controllability. We evaluate the performance of our framework on three benchmark summarization datasets through empirical and qualitative analyses. We also conduct a human evaluation to validate the effectiveness of the model’s refinements and find a potential issue of over-correction.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.714.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the one-shot summarization setting is sometimes inadequate, however, as the generated summary may contain hallucinations or overlook important details related to the reader’s interests.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the one-shot summarization setting is sometimes inadequate, however, as the generated summary may contain hallucinations or overlook important details related to the reader’s interests.\""
    },
    {
        "title": "Orthogonal Subspace Learning for Language Model Continual Learning",
        "authors": [
            "Xiao Wang",
            "Tianze Chen",
            "Qiming Ge",
            "Han Xia",
            "Rong Bao",
            "Rui Zheng",
            "Qi Zhang",
            "Tao Gui",
            "Xuanjing Huang"
        ],
        "published": "2023",
        "summary": "Benefiting from massive corpora and advanced hardware, large language models (LLMs) exhibit remarkable capabilities in language understanding and generation. However, their performance degrades in scenarios where multiple tasks are encountered sequentially, also known as catastrophic forgetting. In this paper, we propose orthogonal low-rank adaptation (O-LoRA), a simple and efficient approach for continual learning in language models, effectively mitigating catastrophic forgetting while learning new tasks. Specifically, O-LoRA learns tasks in different (low-rank) vector subspaces that are kept orthogonal to each other in order to minimize interference. Our method induces only marginal additional parameter costs and requires no user data storage for replay. Experimental results on continual learning benchmarks show that our method outperforms state-of-the-art methods. Furthermore, compared to previous approaches, our method excels in preserving the generalization ability of LLMs on unseen tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.715.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their performance degrades in scenarios where multiple tasks are encountered sequentially, also known as catastrophic forgetting.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, their performance degrades in scenarios where multiple tasks are encountered sequentially, also known as catastrophic forgetting.\""
    },
    {
        "title": "Hi-ToM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models",
        "authors": [
            "Yufan Wu",
            "Yinghui He",
            "Yilin Jia",
            "Rada Mihalcea",
            "Yulong Chen",
            "Naihao Deng"
        ],
        "published": "2023",
        "summary": "Theory of Mind (ToM) is the ability to reason about one’s own and others’ mental states. ToM plays a critical role in the development of intelligence, language understanding, and cognitive processes. While previous work has primarily focused on first and second-order ToM, we explore higher-order ToM, which involves recursive reasoning on others’ beliefs. %We also incorporate a new deception mechanism in ToM reasoning. We introduce Hi-ToM, a Higher Order Theory of Mind benchmark. Our experimental evaluation using various Large Language Models (LLMs) indicates a decline in performance on higher-order ToM tasks, demonstrating the limitations of current LLMs. We conduct a thorough analysis of different failure cases of LLMs, and share our thoughts on the implications of our findings on the future of NLP.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.717.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our experimental evaluation using various Large Language Models (LLMs) indicates a decline in performance on higher-order ToM tasks, demonstrating the limitations of current LLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Our experimental evaluation using various Large Language Models (LLMs) indicates a decline in performance on higher-order ToM tasks, demonstrating the limitations of current LLMs.\""
    },
    {
        "title": "Mind the Gap Between Conversations for Improved Long-Term Dialogue Generation",
        "authors": [
            "Qiang Zhang",
            "Jason Naradowsky",
            "Yusuke Miyao"
        ],
        "published": "2023",
        "summary": "Knowing how to end and resume conversations over time is a natural part of communication, allowing for discussions to span weeks, months, or years. The duration of gaps between conversations dictates which topics are relevant and which questions to ask, and dialogue systems which do not explicitly model time may generate responses that are unnatural. In this work we explore the idea of making dialogue models aware of time, and present GapChat, a multi-session dialogue dataset in which the time between each session varies. While the dataset is constructed in real-time, progress on events in speakers’ lives is simulated in order to create realistic dialogues occurring across a long timespan. We expose time information to the model and compare different representations of time and event progress. In human evaluation we show that time-aware models perform better in metrics that judge the relevance of the chosen topics and the information gained from the conversation.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.720.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"dialogue systems which do not explicitly model time may generate responses that are unnatural.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"dialogue systems which do not explicitly model time may generate responses that are unnatural.\""
    },
    {
        "title": "NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark",
        "authors": [
            "Oscar Sainz",
            "Jon Campos",
            "Iker García-Ferrero",
            "Julen Etxaniz",
            "Oier Lopez de Lacalle",
            "Eneko Agirre"
        ],
        "published": "2023",
        "summary": "In this position paper we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not straightforward to measure. Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded. This position paper defines different levels of data contamination and argues for a community effort, including the development of automatic and semi-automatic measures to detect when data from a benchmark was exposed to a model, and suggestions for flagging papers with conclusions that are compromised by data contamination.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.722.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark.\""
    },
    {
        "title": "Improving Pacing in Long-Form Story Planning",
        "authors": [
            "Yichen Wang",
            "Kevin Yang",
            "Xiaoming Liu",
            "Dan Klein"
        ],
        "published": "2023",
        "summary": "Existing LLM-based systems for writing long-form stories or story outlines frequently suffer from unnatural pacing, whether glossing over important events or over-elaborating on insignificant details, resulting in a jarring experience for the reader. We propose a **CONC**rete **O**utline **C**on**T**rol (CONCOCT) system to improve pacing when automatically generating story outlines. We first train a *concreteness evaluator* to judge which of two events is more concrete (low-level-detailed). This evaluator can then be used to control pacing in hierarchical outline generation; in this work, we explore a *vaguest-first* expansion procedure that aims for uniform pacing. We further use the evaluator to filter new outline items based on predicted concreteness. Compared to a baseline hierarchical outline generator, humans judge CONCOCT’s pacing to be more consistent over 57% of the time across multiple outline lengths; the gains also translate to downstream stories. All code, data, and models are open-sourced.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.723.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing LLM-based systems for writing long-form stories or story outlines frequently suffer from unnatural pacing, whether glossing over important events or over-elaborating on insignificant details, resulting in a jarring experience for the reader.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing LLM-based systems for writing long-form stories or story outlines frequently suffer from unnatural pacing, whether glossing over important events or over-elaborating on insignificant details, resulting in a jarring experience for the reader.\""
    },
    {
        "title": "Argument mining as a multi-hop generative machine reading comprehension task",
        "authors": [
            "Boyang Liu",
            "Viktor Schlegel",
            "Riza Batista-Navarro",
            "Sophia Ananiadou"
        ],
        "published": "2023",
        "summary": "Argument mining (AM) is a natural language processing task that aims to generate an argumentative graph given an unstructured argumentative text. An argumentative graph that consists of argumentative components and argumentative relations contains completed information of an argument and exhibits the logic of an argument. As the argument structure of an argumentative text can be regarded as an answer to a “why” question, the whole argument structure is therefore similar to the “chain of thought” concept, i.e., the sequence of ideas that lead to a specific conclusion for a given argument (Wei et al., 2022). For argumentative texts in the same specific genre, the “chain of thought” of such texts is usually similar, i.e., in a student essay, there is usually a major claim supported by several claims, and then a number of premises which are related to the claims are included (Eger et al., 2017). In this paper, we propose a new perspective which transfers the argument mining task into a multi-hop reading comprehension task, allowing the model to learn the argument structure as a “chain of thought”. We perform a comprehensive evaluation of our approach on two AM benchmarks and find that we surpass SOTA results. A detailed analysis shows that specifically the “chain of thought” information is helpful for the argument mining task.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.724.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "HuatuoGPT, Towards Taming Language Model to Be a Doctor",
        "authors": [
            "Hongbo Zhang",
            "Junying Chen",
            "Feng Jiang",
            "Fei Yu",
            "Zhihong Chen",
            "Guiming Chen",
            "Jianquan Li",
            "Xiangbo Wu",
            "Zhang Zhiyi",
            "Qingying Xiao",
            "Xiang Wan",
            "Benyou Wang",
            "Haizhou Li"
        ],
        "published": "2023",
        "summary": "In this paper, we present HuatuoGPT, a Large Language Model (LLM) for medical consultation. The core recipe of HuatuoGPT is to leverage both distilled data from **ChatGPT** and real-world data from **doctors** in the supervised fine-tuning stage. This is not only because purely using **ChatGPT**-distilled data might cause ‘model collapse’, but also because real-world data from **doctors** would be complementary to **ChatGPT**-distilled data. The responses from ChatGPT are usually detailed, well-presented, fluent, and instruction-followed, but it cannot perform like a doctor in many aspects, e.g. for interactive diagnosis. Therefore, the extra doctors’ data could tame a distilled language model to perform like doctors. To synergize the strengths of both data sources, we introduce RLMF (Reinforcement Learning from Mixed Feedback) where a reward model is trained to align the language model with the merits that both sources (ChatGPT and doctors) bring. Experimental results (in GPT-4 evaluation, human evaluation, and medical benchmark datasets) demonstrate that HuatuoGPT achieves state-of-the-art results in performing medical consultation among open-source LLMs. It is worth noting that by using additional real-world data and RLMF, the distilled language model (i.e., HuatuoGPT) outperforms its teacher model (i.e., ChatGPT) in most cases.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.725.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the responses from ChatGPT are usually detailed, well-presented, fluent, and instruction-followed, but it cannot perform like a doctor in many aspects, e.g. for interactive diagnosis.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the responses from ChatGPT are usually detailed, well-presented, fluent, and instruction-followed, but it cannot perform like a doctor in many aspects, e.g. for interactive diagnosis.\""
    },
    {
        "title": "Debias NLU Datasets via Training-free Perturbations",
        "authors": [
            "Qi Guo",
            "Yuanhang Tang",
            "Yawen Ouyang",
            "Zhen Wu",
            "Xinyu Dai"
        ],
        "published": "2023",
        "summary": "Several recent studies have shown that advanced models for natural language understanding (NLU) are prone to capture biased features that are independent of the task but spuriously correlated to labels. Such models often perform well on in-distribution (ID) datasets but fail to generalize to out-of-distribution (OOD) datasets. Existing solutions can be separated into two orthogonal approaches: model-centric methods and data-centric methods. Model-centric methods improve OOD performance at the expense of ID performance. Data-centric strategies usually boost both of them via data-level manipulations such as generative data augmentation. However, the high cost of fine-tuning a generator to produce valid samples limits the potential of such approaches. To address this issue, we propose PDD, a framework that conducts training-free Perturbations on samples containing biased features to Debias NLU Datasets. PDD works by iteratively conducting perturbations via pre-trained mask language models (MLM). PDD exhibits the advantage of low cost by adopting a training-free perturbation strategy and further improves the label consistency by utilizing label information during perturbations. Extensive experiments demonstrate that PDD shows competitive performance with previous state-of-the-art debiasing strategies. When combined with the model-centric debiasing methods, PDD establishes a new state-of-the-art.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.726.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Several recent studies have shown that advanced models for natural language understanding (NLU) are prone to capture biased features that are independent of the task but spuriously correlated to labels.\"\n\nNote: Although this paper discusses limitations of advanced NLU models, which include LLMs, the focus is on proposing a solution to address these limitations rather than exploring the limitations themselves.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Several recent studies have shown that advanced models for natural language understanding (NLU) are prone to capture biased features that are independent of the task but spuriously correlated to labels.\"\n\nNote: Although this paper discusses limitations of advanced NLU models, which include LLMs, the focus is on proposing a solution to address these limitations rather than exploring the limitations themselves."
    },
    {
        "title": "Robustness of Named-Entity Replacements for In-Context Learning",
        "authors": [
            "Saeed Goodarzi",
            "Nikhil Kagita",
            "Dennis Minn",
            "Shufan Wang",
            "Roberto Dessi",
            "Shubham Toshniwal",
            "Adina Williams",
            "Jack Lanchantin",
            "Koustuv Sinha"
        ],
        "published": "2023",
        "summary": "A key feature of modern large language models (LLMs) is their ability to perform in-context learning, a prompting technique where query- answer demonstrations are shown before the final query. This allows for generalization to novel distributions at inference time where the LLM can learn new rules without parameter updates. However, the choice of demonstrations and their relationship to a particular query can have a profound impact on model accuracy, raising concerns about the true in-context generalization capabilities (Zhao et al., 2021). In this work, we explore the robustness of the in-context learning paradigm by focusing on entities. In particular, we seek to understand the robustness of LLM in-context learning with respect to named entity replacements. We discover a significant variance in downstream performance based on the choice of the named entities, across three popular reasoning tasks and two popular LLMs. Specifically, model accuracy on the test sets can fluctuate between -2.7 to +8.0 points depending on the choice of named entity replacements. Our analysis exposes the sensitivity of LLM in-context learning with respect to named entities, and offers a simple recipe to improve test performance by hyper-parameter tuning the named entities for a given dataset. Code and datasets for reproducing the results are publicly available.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.728.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the choice of demonstrations and their relationship to a particular query can have a profound impact on model accuracy, raising concerns about the true in-context generalization capabilities\"; \"Our analysis exposes the sensitivity of LLM in-context learning with respect to named entities.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, the choice of demonstrations and their relationship to a particular query can have a profound impact on model accuracy, raising concerns about the true in-context generalization capabilities\"; \"Our analysis exposes the sensitivity of LLM in-context learning with respect to named entities.\""
    },
    {
        "title": "Legally Enforceable Hate Speech Detection for Public Forums",
        "authors": [
            "Chu Luo",
            "Rohan Bhambhoria",
            "Samuel Dahan",
            "Xiaodan Zhu"
        ],
        "published": "2023",
        "summary": "Hate speech causes widespread and deep-seated societal issues. Proper enforcement of hate speech laws is key for protecting groups of people against harmful and discriminatory language. However, determining what constitutes hate speech is a complex task that is highly open to subjective interpretations. Existing works do not align their systems with enforceable definitions of hate speech, which can make their outputs inconsistent with the goals of regulators. This research introduces a new perspective and task for enforceable hate speech detection centred around legal definitions, and a dataset annotated on violations of eleven possible definitions by legal experts. Given the challenge of identifying clear, legally enforceable instances of hate speech, we augment the dataset with expert-generated samples and an automatically mined challenge set. We experiment with grounding the model decision in these definitions using zero-shot and few-shot prompting. We then report results on several large language models (LLMs). With this task definition, automatic hate speech detection can be more closely aligned to enforceable laws, and hence assist in more rigorous enforcement of legal protections against harmful speech in public forums.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.730.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions \"the challenge of identifying clear, legally enforceable instances of hate speech\" which is related to the limitations of LLMs in detecting hate speech.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None, but the paper mentions \"the challenge of identifying clear, legally enforceable instances of hate speech\" which is related to the limitations of LLMs in detecting hate speech."
    },
    {
        "title": "ConPrompt: Pre-training a Language Model with Machine-Generated Data for Implicit Hate Speech Detection",
        "authors": [
            "Youngwook Kim",
            "Shinwoo Park",
            "Youngsoo Namgoong",
            "Yo-Sub Han"
        ],
        "published": "2023",
        "summary": "Implicit hate speech detection is a challenging task in text classification since no explicit cues (e.g., swear words) exist in the text. While some pre-trained language models have been developed for hate speech detection, they are not specialized in implicit hate speech. Recently, an implicit hate speech dataset with a massive number of samples has been proposed by controlling machine generation. We propose a pre-training approach, ConPrompt, to fully leverage such machine-generated data. Specifically, given a machine-generated statement, we use example statements of its origin prompt as positive samples for contrastive learning. Through pre-training with ConPrompt, we present ToxiGen-ConPrompt, a pre-trained language model for implicit hate speech detection. We conduct extensive experiments on several implicit hate speech datasets and show the superior generalization ability of ToxiGen-ConPrompt compared to other pre-trained models. Additionally, we empirically show that ConPrompt is effective in mitigating identity term bias, demonstrating that it not only makes a model more generalizable but also reduces unintended bias. We analyze the representation quality of ToxiGen-ConPrompt and show its ability to consider target group and toxicity, which are desirable features in terms of implicit hate speeches.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.731.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While some pre-trained language models have been developed for hate speech detection, they are not specialized in implicit hate speech.\"\n\nThis paper mentions a limitation of pre-trained language models in detecting implicit hate speech, but it is a minor detail and not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While some pre-trained language models have been developed for hate speech detection, they are not specialized in implicit hate speech.\"\n\nThis paper mentions a limitation of pre-trained language models in detecting implicit hate speech, but it is a minor detail and not the primary focus of the paper."
    },
    {
        "title": "Incorporating Syntactic Knowledge into Pre-trained Language Model using Optimization for Overcoming Catastrophic Forgetting",
        "authors": [
            "Ran Iwamoto",
            "Issei Yoshida",
            "Hiroshi Kanayama",
            "Takuya Ohko",
            "Masayasu Muraoka"
        ],
        "published": "2023",
        "summary": "Syntactic knowledge is invaluable information for many tasks which handle complex or long sentences, but typical pre-trained language models do not contain sufficient syntactic knowledge. Thus it results in failures in downstream tasks that require syntactic knowledge. In this paper, we explore additional training to incorporate syntactic knowledge to a language model. We designed four pre-training tasks that learn different syntactic perspectives. For adding new syntactic knowledge and keeping a good balance between the original and additional knowledge, we addressed the problem of catastrophic forgetting that prevents the model from keeping semantic information when the model learns additional syntactic knowledge. We demonstrated that additional syntactic training produced consistent performance gains while clearly avoiding catastrophic forgetting.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.732.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"typical pre-trained language models do not contain sufficient syntactic knowledge. Thus it results in failures in downstream tasks that require syntactic knowledge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"typical pre-trained language models do not contain sufficient syntactic knowledge. Thus it results in failures in downstream tasks that require syntactic knowledge.\""
    },
    {
        "title": "Toward Human Readable Prompt Tuning: Kubrick’s The Shining is a good movie, and a good prompt too?",
        "authors": [
            "Weijia Shi",
            "Xiaochuang Han",
            "Hila Gonen",
            "Ari Holtzman",
            "Yulia Tsvetkov",
            "Luke Zettlemoyer"
        ],
        "published": "2023",
        "summary": "Large language models can perform downstream tasks in a zero-shot fashion, given natural language prompts that specify the desired behavior. Such prompts are typically hand engineered, but can also be learned with gradient-based methods from labeled data. However, it is underexplored what factors make the prompts effective, especially when the prompts are in natural language. In this paper, we investigate common attributes shared by effective prompts in classification problems. We first propose a human readable prompt tuning method (FluentPrompt) based on Langevin dynamics that incorporates a fluency constraint to find a distribution of effective and fluent prompts. Our analysis reveals that effective prompts are topically related to the task domain and calibrate the prior probability of output labels. Based on these findings, we also propose a method for generating prompts using only unlabeled data, outperforming strong baselines by an average of 7.0% accuracy across three tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.733.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"it is underexplored what factors make the prompts effective, especially when the prompts are in natural language.\"\n\nThis rating is given because the paper mentions a limitation of LLMs (the lack of understanding of what makes effective prompts) but does not elaborate on it and instead focuses on proposing a solution to this problem.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"it is underexplored what factors make the prompts effective, especially when the prompts are in natural language.\"\n\nThis rating is given because the paper mentions a limitation of LLMs (the lack of understanding of what makes effective prompts) but does not elaborate on it and instead focuses on proposing a solution to this problem."
    },
    {
        "title": "Chain-of-Thought Reasoning in Tabular Language Models",
        "authors": [
            "Mingyu Zheng",
            "Hao Yang",
            "Wenbin Jiang",
            "Zheng Lin",
            "Yajuan Lyu",
            "Qiaoqiao She",
            "Weiping Wang"
        ],
        "published": "2023",
        "summary": "Tabular mathematical reasoning task requires models to perform multi-step operations including information look-up and numerical calculation, based on heterogeneous data from tables and questions. Existing solutions tend to extend chain-of-thought (CoT) reasoning into powerful large language models (LLMs) to promote multi-hop mathematical reasoning. However, such LLM-based approaches are not a viable solution in the scenario of privatization deployment or limited resources. To address this problem, we revisit small-scale tabular language models (TaLMs) and extend chain-of-thought reasoning into TaLMs for the first time. Specifically, we propose a novel framework, TaCo, which coordinates two TaLMs responsible for CoT generation and answer inference, respectively. Besides, our framework can be combined with an external calculator to enhance accurate numerical calculation. On the TABMWP dataset, TaCo outperforms the state-of-the-art ChatGPT by 9.55% (82.60%→92.15% in accuracy) with much less parameters (0.8B). The code will be released along with the paper.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.734.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, such LLM-based approaches are not a viable solution in the scenario of privatization deployment or limited resources.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, such LLM-based approaches are not a viable solution in the scenario of privatization deployment or limited resources.\""
    },
    {
        "title": "Diffusion Language Model with Query-Document Relevance for Query-Focused Summarization",
        "authors": [
            "Shaoyao Huang",
            "Luozheng Qin",
            "Ziqiang Cao"
        ],
        "published": "2023",
        "summary": "Query-Focused Summarization (QFS) aims to generate summaries from source documents that can answer specific queries. Although the QFS task has gained increasing attention recently, its development is constrained by the fact that mainstream QFS models are BART variants, which are autoregressive and suffer from long-term dependencies and exposure bias. To address these problems, we adopt a diffusion language model that performs well in non-autoregressive scenarios to effectively resolve issues related to autoregressive methods. However, QFS requires guidance from queries to generate adequate summaries, while diffusion language models have limited sensitivity to queries. In this paper, we propose QFS-DLM, a non-autoregressive diffusion language model that incorporates query-document fragment relevance and query-document global relevance to enhance the adaptability of QFS tasks. Firstly, we extract key fragments from documents based on queries and assign higher weights to them, thereby emphasizing crucial and continuous information within the document. Secondly, we calculate global relevance scores between queries and documents, and then integrate these scores into the model’s loss function, enabling the model to prefer high-quality data and distance itself from low-quality data. Overall, our method achieves state-of-the-art performance on Debatepedia and PubMedQA datasets in ROUGE scores, GPT-4, and human evaluations.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.735.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although the QFS task has gained increasing attention recently, its development is constrained by the fact that mainstream QFS models are BART variants, which are autoregressive and suffer from long-term dependencies and exposure bias.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although the QFS task has gained increasing attention recently, its development is constrained by the fact that mainstream QFS models are BART variants, which are autoregressive and suffer from long-term dependencies and exposure bias.\""
    },
    {
        "title": "Boosting Inference Efficiency: Unleashing the Power of Parameter-Shared Pre-trained Language Models",
        "authors": [
            "Weize Chen",
            "Xiaoyue Xu",
            "Xu Han",
            "Yankai Lin",
            "Ruobing Xie",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Jie Zhou"
        ],
        "published": "2023",
        "summary": "Parameter-shared pre-trained language models (PLMs) have emerged as a successful approach in resource-constrained environments, enabling substantial reductions in model storage and memory costs without significant performance compromise. However, it is important to note that parameter sharing does not alleviate computational burdens associated with inference, thus impeding its practicality in situations characterized by limited stringent latency requirements or computational resources. Building upon neural ordinary differential equations (ODEs), we introduce a straightforward technique to enhance the inference efficiency of parameter-shared PLMs. Additionally, we propose a simple pre-training technique that leads to fully or partially shared models capable of achieving even greater inference acceleration. The experimental results demonstrate the effectiveness of our methods on both autoregressive and autoencoding PLMs, providing novel insights into more efficient utilization of parameter-shared models in resource-constrained settings.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.738.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it is important to note that parameter sharing does not alleviate computational burdens associated with inference, thus impeding its practicality in situations characterized by limited stringent latency requirements or computational resources.\"\n\nThis evidence mentions a limitation of parameter-shared pre-trained language models (PLMs), a type of LLMs, but it is not a major focus of the paper and is used to motivate the",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, it is important to note that parameter sharing does not alleviate computational burdens associated with inference, thus impeding its practicality in situations characterized by limited stringent latency requirements or computational resources.\"\n\nThis evidence mentions a limitation of parameter-shared pre-trained language models (PLMs), a type of LLMs, but it is not a major focus of the paper and is used to motivate the"
    },
    {
        "title": "Natural Response Generation for Chinese Reading Comprehension",
        "authors": [
            "Nuo Chen",
            "Hongguang Li",
            "Yinan Bao",
            "Baoyuan Wang",
            "Jia Li"
        ],
        "published": "2023",
        "summary": "Machine reading comprehension (MRC) is an important area of conversation agents and draws a lot of attention. However, there is a notable limitation to current MRC benchmarks: The labeled answers are mostly either spans extracted from the target corpus or the choices of the given candidates, ignoring the natural aspect of high-quality responses. As a result, MRC models trained on these datasets can not generate human-like responses in real QA scenarios. To this end, we construct a new dataset called Penguin to promote the research of MRC, providing a training and test bed for natural response generation to real scenarios. Concretely, Penguin consists of 200k training data with high-quality fluent, and well-informed responses. Penguin is the first benchmark towards natural response generation in Chinese MRC on a relatively large scale. To address the challenges in Penguin, we develop two strong baselines: end-to-end and two-stage frameworks. Following that, we further design Prompt-BART: fine-tuning the pre-trained generative language models with a mixture of prefix prompts in Penguin. Extensive experiments validated the effectiveness of this design.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.739.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"MRC models trained on these datasets can not generate human-like responses in real QA scenarios.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"MRC models trained on these datasets can not generate human-like responses in real QA scenarios.\""
    },
    {
        "title": "Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking",
        "authors": [
            "Yuxiang Wu",
            "Guanting Dong",
            "Weiran Xu"
        ],
        "published": "2023",
        "summary": "Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring and annotating task-oriented dialogues, which can be time-consuming and costly. However, DST extends beyond simple slot-filling and requires effective updating strategies for tracking dialogue state as conversations progress. In this paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to introduce additional intricate updating strategies in zero-shot DST. Our approach reformulates the DST task by leveraging powerful Large Language Models (LLMs) and translating the original dialogue text to JSON through semantic parsing as an intermediate state. We also design a novel framework that includes more modules to ensure the effectiveness of updating strategies in the text-to-JSON process. Experimental results demonstrate that our approach outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to existing ICL methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.741.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but the abstract assumes that existing zero-shot DST methods are not effective enough, implying a limitation in current approaches that the paper aims to address.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations, but the abstract assumes that existing zero-shot DST methods are not effective enough, implying a limitation in current approaches that the paper aims to address."
    },
    {
        "title": "Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation",
        "authors": [
            "Jinglong Gao",
            "Xiao Ding",
            "Bing Qin",
            "Ting Liu"
        ],
        "published": "2023",
        "summary": "Causal reasoning ability is crucial for numerous NLP applications. Despite the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear how well ChatGPT performs in causal reasoning. In this paper, we conduct the first comprehensive evaluation of the ChatGPT’s causal reasoning capabilities. Experiments show that ChatGPT is not a good causal reasoner, but a good causal interpreter. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT’s upgrading processes, such as RLHF. The In-Context Learning (ICL) and Chain-of-Thought (COT) techniques can further exacerbate such causal hallucination. Additionally, the causal reasoning ability of ChatGPT is sensitive to the words used to express the causal concept in prompts, and close-ended prompts perform better than open-ended prompts. For events in sentences, ChatGPT excels at capturing explicit causality rather than implicit causality, and performs better in sentences with lower event density and smaller lexical distance between events.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.743.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experiments show that ChatGPT is not a good causal reasoner, but a good causal interpreter. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT’s upgrading processes, such as RLHF.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"Experiments show that ChatGPT is not a good causal reasoner, but a good causal interpreter. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT’s upgrading processes, such as RLHF.\""
    },
    {
        "title": "Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning",
        "authors": [
            "Duarte Alves",
            "Nuno Guerreiro",
            "João Alves",
            "José Pombal",
            "Ricardo Rei",
            "José de Souza",
            "Pierre Colombo",
            "Andre Martins"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) are a promising avenue for machine translation (MT). However, current LLM-based MT systems are brittle: their effectiveness highly depends on the choice of few-shot examples and they often require extra post-processing due to overgeneration. Alternatives such as finetuning on translation instructions are computationally expensive and may weaken in-context learning capabilities, due to overspecialization. In this paper, we provide a closer look at this problem. We start by showing that adapter-based finetuning with LoRA matches the performance of traditional finetuning while reducing the number of training parameters by a factor of 50. This method also outperforms few-shot prompting and eliminates the need for post-processing or in-context examples. However, we show that finetuning generally degrades few-shot performance, hindering adaptation capabilities. Finally, to obtain the best of both worlds, we propose a simple approach that incorporates few-shot examples during finetuning. Experiments on 10 language pairs show that our proposed approach recovers the original few-shot capabilities while keeping the added benefits of finetuning.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.744.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, current LLM-based MT systems are brittle: their effectiveness highly depends on the choice of few-shot examples and they often require extra post-processing due to overgeneration. Alternatives such as finetuning on translation instructions are computationally expensive and may weaken in-context learning capabilities, due to overspecialization.... we show that finetuning generally degrades few-shot performance,",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, current LLM-based MT systems are brittle: their effectiveness highly depends on the choice of few-shot examples and they often require extra post-processing due to overgeneration. Alternatives such as finetuning on translation instructions are computationally expensive and may weaken in-context learning capabilities, due to overspecialization.... we show that finetuning generally degrades few-shot performance,"
    },
    {
        "title": "How Many Demonstrations Do You Need for In-context Learning?",
        "authors": [
            "Jiuhai Chen",
            "Lichang Chen",
            "Chen Zhu",
            "Tianyi Zhou"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) are capable to perform complex reasoning by in-context learning (ICL) when provided with a few input-output demonstrations (demos) and more powerful when intermediate reasoning steps (chain of thoughts (CoT)) of the demos are given. Is it necessary to use multi-demo in ICL? In this paper, we study ICL using fewer demos for each test query on the tasks in (Wei et al., 2022). Surprisingly, we do not observe significant degradation when using only one randomly chosen demo. To study this phenomenon, for each test query, we categorize demos into “positive demos” leading to the correct answer, and “negative demos” resulting in wrong answers. Our analysis reveals an inherent bias in those widely studied datasets and the redundancy of demos: most demos are positive for a majority of test queries, which explains the good performance of ICL with one random demo. Moreover, ICL (with and w/o CoT) using only one positive demo significantly outperforms multi-demo ICL adopted by most previous works, indicating the weakness of LLMs in finding positive demo(s) for input queries, which is difficult to evaluate on the biased datasets. Furthermore, we observe a counterintuitive behavior of ICL using multi-demo, i.e., its accuracy degrades(improves) when given more positive(negative) demos. This implies that ICL can be easily misguided by interference among demos and their spurious correlations. Our analyses highlight several fundamental challenges that need to be addressed in LLMs training, ICL, and benchmark design.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.745.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Moreover, ICL (with and w/o CoT) using only one positive demo significantly outperforms multi-demo ICL adopted by most previous works, indicating the weakness of LLMs in finding positive demo(s) for input queries, which is difficult to evaluate on the biased datasets. Furthermore, we observe a counterintuitive behavior of ICL using multi-demo, i.e",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Moreover, ICL (with and w/o CoT) using only one positive demo significantly outperforms multi-demo ICL adopted by most previous works, indicating the weakness of LLMs in finding positive demo(s) for input queries, which is difficult to evaluate on the biased datasets. Furthermore, we observe a counterintuitive behavior of ICL using multi-demo, i.e"
    },
    {
        "title": "Long-Horizon Dialogue Understanding for Role Identification in the Game of Avalon with Large Language Models",
        "authors": [
            "Simon Stepputtis",
            "Joseph Campbell",
            "Yaqi Xie",
            "Zhengyang Qi",
            "Wenxin Zhang",
            "Ruiyi Wang",
            "Sanketh Rangreji",
            "Charles Lewis",
            "Katia Sycara"
        ],
        "published": "2023",
        "summary": "Deception and persuasion play a critical role in long-horizon dialogues between multiple parties, especially when the interests, goals, and motivations of the participants are not aligned. Such complex tasks pose challenges for current Large Language Models (LLM) as deception and persuasion can easily mislead them, especially in long-horizon multi-party dialogues. To this end, we explore the game of Avalon: The Resistance, a social deduction game in which players must determine each other’s hidden identities to complete their team’s objective. We introduce an online testbed and a dataset containing 20 carefully collected and labeled games among human players that exhibit long-horizon deception in a cooperative-competitive setting. We discuss the capabilities of LLMs to utilize deceptive long-horizon conversations between six human players to determine each player’s goal and motivation. Particularly, we discuss the multimodal integration of the chat between the players and the game’s state that grounds the conversation, providing further insights into the true player identities. We find that even current state-of-the-art LLMs do not reach human performance, making our dataset a compelling benchmark to investigate the decision-making and language-processing capabilities of LLMs. Our dataset and online testbed can be found at our project website: https://sstepput.github.io/Avalon-NLU/",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.748.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Such complex tasks pose challenges for current Large Language Models (LLM) as deception and persuasion can easily mislead them, especially in long-horizon multi-party dialogues.\"; \"We find that even current state-of-the-art LLMs do not reach human performance...\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Such complex tasks pose challenges for current Large Language Models (LLM) as deception and persuasion can easily mislead them, especially in long-horizon multi-party dialogues.\"; \"We find that even current state-of-the-art LLMs do not reach human performance...\""
    },
    {
        "title": "Improving Sequential Model Editing with Fact Retrieval",
        "authors": [
            "Xiaoqi Han",
            "Ru Li",
            "Hongye Tan",
            "Wang Yuanlong",
            "Qinghua Chai",
            "Jeff Pan"
        ],
        "published": "2023",
        "summary": "The task of sequential model editing is to fix erroneous knowledge in Pre-trained Language Models (PLMs) efficiently, precisely and continuously. Although existing methods can deal with a small number of modifications, these methods experience a performance decline or require additional annotated data, when the number of edits increases. In this paper, we propose a Retrieval Augmented Sequential Model Editing framework (RASE) that leverages factual information to enhance editing generalization and to guide the identification of edits by retrieving related facts from the fact-patch memory we constructed. Our main findings are: (i) State-of-the-art models can hardly correct massive mistakes stably and efficiently; (ii) Even if we scale up to thousands of edits, RASE can significantly enhance editing generalization and maintain consistent performance and efficiency; (iii) RASE can edit large-scale PLMs and increase the performance of different editors. Moreover, it can integrate with ChatGPT and further improve performance. Our code and data are available at: https://github.com/sev777/RASE.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.749.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although existing methods can deal with a small number of modifications, these methods experience a performance decline or require additional annotated data, when the number of edits increases.\"; \"Our main findings are: (i) State-of-the-art models can hardly correct massive mistakes stably and efficiently;\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Although existing methods can deal with a small number of modifications, these methods experience a performance decline or require additional annotated data, when the number of edits increases.\"; \"Our main findings are: (i) State-of-the-art models can hardly correct massive mistakes stably and efficiently;\""
    },
    {
        "title": "Battle of the Large Language Models: Dolly vs LLaMA vs Vicuna vs Guanaco vs Bard vs ChatGPT - A Text-to-SQL Parsing Comparison",
        "authors": [
            "Shuo Sun",
            "Yuchen Zhang",
            "Jiahuan Yan",
            "Yuze Gao",
            "Donovan Ong",
            "Bin Chen",
            "Jian Su"
        ],
        "published": "2023",
        "summary": "The success of ChatGPT has ignited an AI race, with researchers striving to develop new large language models (LLMs) that can match or surpass the language understanding and generation abilities of commercial ones. In recent times, a number of models have emerged, claiming performance near that of GPT-3.5 or GPT-4 through various instruction-tuning methods. As practitioners of Text-to-SQL parsing, we are grateful for their valuable contributions to open-source research. However, it is important to approach these claims with a sense of scrutiny and ascertain the actual effectiveness of these models. Therefore, we pit six popular large language models against each other, systematically evaluating their Text-to-SQL parsing capability on nine benchmark datasets with five different prompting strategies, covering both zero-shot and few-shot scenarios. Regrettably, the open-sourced models fell significantly short of the performance achieved by closed-source models like GPT-3.5, highlighting the need for further work to bridge the performance gap between these models.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.750.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Regrettably, the open-sourced models fell significantly short of the performance achieved by closed-source models like GPT-3.5, highlighting the need for further work to bridge the performance gap between these models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3**\nEvidence: \"Regrettably, the open-sourced models fell significantly short of the performance achieved by closed-source models like GPT-3.5, highlighting the need for further work to bridge the performance gap between these models.\""
    },
    {
        "title": "KBioXLM: A Knowledge-anchored Biomedical Multilingual Pretrained Language Model",
        "authors": [
            "Lei Geng",
            "Xu Yan",
            "Ziqiang Cao",
            "Juntao Li",
            "Wenjie Li",
            "Sujian Li",
            "Xinjie Zhou",
            "Yang Yang",
            "Jun Zhang"
        ],
        "published": "2023",
        "summary": "Most biomedical pretrained language models are monolingual and cannot handle the growing cross-lingual requirements. The scarcity of non-English domain corpora, not to mention parallel data, poses a significant hurdle in training multilingual biomedical models. Since knowledge forms the core of domain-specific corpora and can be translated into various languages accurately, we propose a model called KBioXLM, which transforms the multilingual pretrained model XLM-R into the biomedical domain using a knowledge-anchored approach. We achieve a biomedical multilingual corpus by incorporating three granularity knowledge alignments (entity, fact, and passage levels) into monolingual corpora. Then we design three corresponding training tasks (entity masking, relation masking, and passage relation prediction) and continue training on top of the XLM-R model to enhance its domain cross-lingual ability. To validate the effectiveness of our model, we translate the English benchmarks of multiple tasks into Chinese. Experimental results demonstrate that our model significantly outperforms monolingual and multilingual pretrained models in cross-lingual zero-shot and few-shot scenarios, achieving improvements of up to 10+ points.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.751.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Most biomedical pretrained language models are monolingual and cannot handle the growing cross-lingual requirements.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Most biomedical pretrained language models are monolingual and cannot handle the growing cross-lingual requirements.\""
    },
    {
        "title": "Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading Time Relationship?",
        "authors": [
            "Sathvik Nair",
            "Philip Resnik"
        ],
        "published": "2023",
        "summary": "An important assumption that comes with using LLMs on psycholinguistic data has gone unverified. LLM-based predictions are based on subword tokenization, not decomposition of words into morphemes. Does that matter? We carefully test this by comparing surprisal estimates using orthographic, morphological, and BPE tokenization against reading time data. Our results replicate previous findings and provide evidence that *in the aggregate*, predictions using BPE tokenization do not suffer relative to morphological and orthographic segmentation. However, a finer-grained analysis points to potential issues with relying on BPE-based tokenization, as well as providing promising results involving morphologically-aware surprisal estimates and suggesting a new method for evaluating morphological prediction.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.752.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, a finer-grained analysis points to potential issues with relying on BPE-based tokenization, as well as providing promising results involving morphologically-aware surprisal estimates and suggesting a new method for evaluating morphological prediction.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, a finer-grained analysis points to potential issues with relying on BPE-based tokenization, as well as providing promising results involving morphologically-aware surprisal estimates and suggesting a new method for evaluating morphological prediction.\""
    },
    {
        "title": "A Zero-Shot Language Agent for Computer Control with Structured Reflection",
        "authors": [
            "Tao Li",
            "Gang Li",
            "Zhiwei Deng",
            "Bryan Wang",
            "Yang Li"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have shown increasing capacity at planning and executing a high-level goal in a live computer environment (e.g. MiniWoB++). To perform a task, recent works often require a model to learn from trace examples of the task via either supervised learning or few/many-shot prompting. Without these trace examples, it remains a challenge how an agent can autonomously learn and improve its control on a computer, which limits the ability of an agent to perform a new task. We approach this problem with a zero-shot agent that requires no given expert traces. Our agent plans for executable actions on a partially observed environment, and iteratively progresses a task by identifying and learning from its mistakes via self-reflection and structured thought management. On the easy tasks of MiniWoB++, we show that our zero-shot agent often outperforms recent SoTAs, with more efficient reasoning. For tasks with more complexity, our reflective agent performs on par with prior best models, even though previous works had the advantages of accessing expert traces or additional screen information.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.753.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Without these trace examples, it remains a challenge how an agent can autonomously learn and improve its control on a computer, which limits the ability of an agent to perform a new task.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Without these trace examples, it remains a challenge how an agent can autonomously learn and improve its control on a computer, which limits the ability of an agent to perform a new task.\""
    },
    {
        "title": "SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF",
        "authors": [
            "Yi Dong",
            "Zhilin Wang",
            "Makesh Sreedhar",
            "Xianchao Wu",
            "Oleksii Kuchaiev"
        ],
        "published": "2023",
        "summary": "Model alignment with human preferences is an essential step in making Large Language Models (LLMs) helpful and consistent with human values. It typically consists of supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) stages. However, RLHF faces inherent limitations stemming from a complex training setup and its tendency to align the model with implicit values that end users cannot control at run-time. Moreover, reward models in RLHF stage commonly rely on single-dimensional feedback as opposed to explicit, multifaceted signals that indicate attributes such as helpfulness, humor, and toxicity. To address these limitations, we propose SteerLM, a supervised fine-tuning method that empowers end-users to control responses during inference. SteerLM conditions responses to conform to an explicitly defined multi-dimensional set of attributes, thereby empowering a steerable AI capable of generating helpful and high-quality responses while maintaining customizability. Experiments show that SteerLM trained on open source datasets generates responses that are preferred by human and automatic evaluators to many state-of-the-art baselines trained with RLHF while being much easier to train. Try SteerLM at https://huggingface.co/nvidia/SteerLM-llama2-13B",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.754.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, RLHF faces inherent limitations stemming from a complex training setup and its tendency to align the model with implicit values that end users cannot control at run-time.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, RLHF faces inherent limitations stemming from a complex training setup and its tendency to align the model with implicit values that end users cannot control at run-time.\""
    },
    {
        "title": "IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models",
        "authors": [
            "Haoxuan You",
            "Rui Sun",
            "Zhecan Wang",
            "Long Chen",
            "Gengyu Wang",
            "Hammad Ayyubi",
            "Kai-Wei Chang",
            "Shih-Fu Chang"
        ],
        "published": "2023",
        "summary": "The field of vision-and-language (VL) understanding has made unprecedented progress with end-to-end large pre-trained VL models (VLMs). However, they still fall short in zero-shot reasoning tasks that require multi-step inferencing. To achieve this goal, previous works resort to a divide-and-conquer pipeline. In this paper, we argue that previous efforts have several inherent shortcomings: 1) They rely on domain-specific sub-question decomposing models. 2) They force models to predict the final answer even if the sub-questions or sub-answers provide insufficient information. We address these limitations via IdealGPT, a framework that iteratively decomposes VL reasoning using large language models (LLMs). Specifically, IdealGPT utilizes an LLM to generate sub-questions, a VLM to provide corresponding sub-answers, and another LLM to reason to achieve the final answer. These three modules perform the divide-and-conquer procedure iteratively until the model is confident about the final answer to the main question. We evaluate IdealGPT on multiple challenging VL reasoning tasks under a zero-shot setting. In particular, our IdealGPT outperforms the best existing GPT-4-like models by an absolute 10% on VCR and 15% on SNLI-VE. Code is available at https://github.com/Hxyou/IdealGPT.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.755.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they still fall short in zero-shot reasoning tasks that require multi-step inferencing.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, they still fall short in zero-shot reasoning tasks that require multi-step inferencing.\""
    },
    {
        "title": "PersonaLM: Language Model Personalization via Domain-distributed Span Aggregated K-Nearest N-gram Retrieval Augmentation",
        "authors": [
            "Puneet Mathur",
            "Zhe Liu",
            "Ke Li",
            "Yingyi Ma",
            "Gil Keren",
            "Zeeshan Ahmed",
            "Dinesh Manocha",
            "Xuedong Zhang"
        ],
        "published": "2023",
        "summary": "We introduce PersonaLM - Domain-distributed Span-Aggregated K-nearest N-gram retrieval augmentation to improve language modeling for Automatic Speech Recognition (ASR) personalization. PersonaLM leverages contextually similar n-gram word frequencies for recognizing rare word patterns associated with unseen domains. It aggregates the next-word probability distribution based on the relative importance of different domains to the input query. To achieve this, we propose a Span Aggregated Group-Contrastive Neural (SCAN) retriever that learns to rank external domains/users by utilizing a group-wise contrastive span loss that pulls together span representations belonging to the same group while pushing away spans from unrelated groups in the semantic space. We propose ASAP benchmark for ASR LM personalization that consists of three user-specific speech-to-text tasks for meetings, TED talks, and financial earnings calls. Extensive experiments show that PersonaLM significantly outperforms strong baselines with a 10-16% improvement in perplexity and a 5-8% reduction in Word Error Rates on popular Wikitext-103, UserLibri, and our ASAP dataset. We further demonstrate the usefulness of the SCAN retriever for improving user-personalized text generation and classification by retrieving relevant context for zero-shot prompting and few-shot fine-tuning of LLMs by 7-12% on the LAMP benchmark.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.757.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but \"improving user-personalized text generation and classification by retrieving relevant context for zero-shot prompting and few-shot fine-tuning of LLMs\" implies that LLMs have limitations in these areas, however, this is not explicitly stated.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but \"improving user-personalized text generation and classification by retrieving relevant context for zero-shot prompting and few-shot fine-tuning of LLMs\" implies that LLMs have limitations in these areas, however, this is not explicitly stated."
    },
    {
        "title": "Scaling Vision-Language Models with Sparse Mixture of Experts",
        "authors": [
            "Sheng Shen",
            "Zhewei Yao",
            "Chunyuan Li",
            "Trevor Darrell",
            "Kurt Keutzer",
            "Yuxiong He"
        ],
        "published": "2023",
        "summary": "The field of natural language processing (NLP) has made significant strides in recent years, particularly in the development of large-scale vision-language models (VLMs). These models aim to bridge the gap between text and visual information, enabling a more comprehensive understanding of multimedia data. However, as these models become larger and more complex, they also become more challenging to train and deploy. One approach to addressing this challenge is the use of sparsely-gated mixture-of-experts (MoE) techniques, which divide the model into smaller, specialized sub-models that can jointly solve a task. In this paper, we explore the effectiveness of MoE in scaling vision-language models, demonstrating its potential to achieve state-of-the-art performance on a range of benchmarks over dense models of equivalent computational cost. Our research offers valuable insights into stabilizing the training of MoE models, understanding the impact of MoE on model interpretability, and balancing the trade-offs between compute performance when scaling VLMs. We hope our work will inspire further research into the use of MoE for scaling large-scale vision-language models and other multimodal machine learning applications.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.758.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, as these models become larger and more complex, they also become more challenging to train and deploy.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, as these models become larger and more complex, they also become more challenging to train and deploy.\""
    },
    {
        "title": "Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement",
        "authors": [
            "Zhiheng Xi",
            "Senjie Jin",
            "Yuhao Zhou",
            "Rui Zheng",
            "Songyang Gao",
            "Jia Liu",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2023",
        "summary": "To enhance the multi-step reasoning capabilities of large language models, researchers have extensively explored prompting methods, notably the Chain-of-Thought (CoT) method which explicitly elicits human-like rationales. However, they have inadvertently overlooked the potential of enhancing model reasoning performance by formulating higher-quality problems. In this work, we start from the problem side and propose Self-Polish (SP), a novel method that facilitates the model’s reasoning by guiding it to progressively refine the given problems to be more comprehensible and solvable. We also explore several automatic prompting varients and propose the Self-Polish prompt bank for the community. SP is orthogonal to all other prompting methods of answer/reasoning side like CoT, allowing for seamless integration with state-of-the-art techniques for further improvement. Thorough experiments show that the proposed method attains notable and consistent effectiveness on five reasoning benchmarks across different models. Furthermore, our method also showcases impressive performance on robustness evaluation. Codes and prompts are available at https://github.com/WooooDyy/Self-Polish.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.762.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they have inadvertently overlooked the potential of enhancing model reasoning performance by formulating higher-quality problems.\"\n\n(Note: The abstract mentions a limitation of LLMs in passing, but it's not the primary focus of the paper. The limitation is that the existing methods have overlooked the potential of enhancing model reasoning performance by formulating higher-quality problems, which is briefly mentioned to motivate the proposed",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, they have inadvertently overlooked the potential of enhancing model reasoning performance by formulating higher-quality problems.\"\n\n(Note: The abstract mentions a limitation of LLMs in passing, but it's not the primary focus of the paper. The limitation is that the existing methods have overlooked the potential of enhancing model reasoning performance by formulating higher-quality problems, which is briefly mentioned to motivate the proposed"
    },
    {
        "title": "Eyes Show the Way: Modelling Gaze Behaviour for Hallucination Detection",
        "authors": [
            "Kishan Maharaj",
            "Ashita Saxena",
            "Raja Kumar",
            "Abhijit Mishra",
            "Pushpak Bhattacharyya"
        ],
        "published": "2023",
        "summary": "Detecting hallucinations in natural language processing (NLP) is a critical undertaking that demands a deep understanding of both the semantic and pragmatic aspects of languages. Cognitive approaches that leverage users’ behavioural signals, such as gaze, have demonstrated effectiveness in addressing NLP tasks with similar linguistic complexities. However, their potential in the context of hallucination detection remains largely unexplored. In this paper, we propose a novel cognitive approach for hallucination detection that leverages gaze signals from humans. We first collect and introduce an eye tracking corpus (IITB-HGC: IITB-Hallucination Gaze corpus) consisting of 500 instances, annotated by five annotators for hallucination detection. Our analysis reveals that humans selectively attend to relevant parts of the text based on distributional similarity, similar to the attention bias phenomenon in psychology. We identify two attention strategies employed by humans: global attention, which focuses on the most informative sentence, and local attention, which focuses on important words within a sentence. Leveraging these insights, we propose a novel cognitive framework for hallucination detection that incorporates these attention biases. Experimental evaluations on the FactCC dataset demonstrate the efficacy of our approach, obtaining a balanced accuracy of 87.1%. Our study highlights the potential of gaze-based approaches in addressing the task of hallucination detection and sheds light on the cognitive processes employed by humans in identifying inconsistencies.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.764.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Diversifying language models for lesser-studied languages and language-usage contexts: A case of second language Korean",
        "authors": [
            "Hakyung Sung",
            "Gyu-Ho Shin"
        ],
        "published": "2023",
        "summary": "This study investigates the extent to which currently available morpheme parsers/taggers apply to lesser-studied languages and language-usage contexts, with a focus on second language (L2) Korean. We pursue this inquiry by (1) training a neural-network model (pre-trained on first language [L1] Korean data) on varying L2 datasets and (2) measuring its morpheme parsing/POS tagging performance on L2 test sets from both the same and different sources of the L2 train sets. Results show that the L2 trained models generally excel in domain-specific tokenization and POS tagging compared to the L1 pre-trained baseline model. Interestingly, increasing the size of the L2 training data does not lead to improving model performance consistently.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.767.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Interestingly, increasing the size of the L2 training data does not lead to improving model performance consistently.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Interestingly, increasing the size of the L2 training data does not lead to improving model performance consistently.\""
    },
    {
        "title": "Improving generalization in large langue model by learning prefix subspaces",
        "authors": [
            "Louis Falissard",
            "Vincent Guigue",
            "Laure Soulier"
        ],
        "published": "2023",
        "summary": "This article focuses on large language models (LLMs) fine-tuning in the scarce data regime (also known as “few-shot learning setting”). We propose a method to increase the generalization capabilities of LLMs based on neural network subspaces. This optimization method, recently introduced in computer vision, aims to improve model generalization by identifying wider local optima through the joint optimization of an entire simplex of models in parameter space. Although this property would be highly beneficial in the context of training large language models in the “few-shot learning” setting, its adaptation to massive, pretrained transformers poses some challenges. First, their considerable number of parameters make it difficult to train several model jointly, and second, their deterministic parameter initialisation schemes make them unfit to the subspace method as originaly proposed. We show in this paper that its application to “Parameter Efficient Fine-Tuning” (PEFT) methods, however, is relatively natural, and we propose to apply it to prefix-tuning, by learning entire simplexes of continous prefixes. We test our method on a variant of the GLUE benchmark adapted to the few-shot learning setting, and show that both our contributions (learning prefix simplexes, and non-deterministic validation metric inference) jointly lead to a gain in average performances compared to state of the art methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.768.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although this property would be highly beneficial in the context of training large language models in the “few-shot learning” setting, its adaptation to massive, pretrained transformers poses some challenges. First, their considerable number of parameters make it difficult to train several model jointly, and second, their deterministic parameter initialisation schemes make them unfit to the subspace method as originaly proposed.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although this property would be highly beneficial in the context of training large language models in the “few-shot learning” setting, its adaptation to massive, pretrained transformers poses some challenges. First, their considerable number of parameters make it difficult to train several model jointly, and second, their deterministic parameter initialisation schemes make them unfit to the subspace method as originaly proposed.\""
    },
    {
        "title": "EconBERTa: Towards Robust Extraction of Named Entities in Economics",
        "authors": [
            "Karim Lasri",
            "Pedro Vitor Quinta de Castro",
            "Mona Schirmer",
            "Luis Eduardo San Martin",
            "Linxi Wang",
            "Tomáš Dulka",
            "Haaya Naushan",
            "John Pougué-Biyong",
            "Arianna Legovini",
            "Samuel Fraiberger"
        ],
        "published": "2023",
        "summary": "Adapting general-purpose language models has proven to be effective in tackling downstream tasks within specific domains. In this paper, we address the task of extracting entities from the economics literature on impact evaluation. To this end, we release EconBERTa, a large language model pretrained on scientific publications in economics, and ECON-IE, a new expert-annotated dataset of economics abstracts for Named Entity Recognition (NER). We find that EconBERTa reaches state-of-the-art performance on our downstream NER task. Additionally, we extensively analyze the model’s generalization capacities, finding that most errors correspond to detecting only a subspan of an entity or failure to extrapolate to longer sequences. This limitation is primarily due to an inability to detect part-of-speech sequences unseen during training, and this effect diminishes when the number of unique instances in the training set increases. Examining the generalization abilities of domain-specific language models paves the way towards improving the robustness of NER models for causal knowledge extraction.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.774.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This limitation is primarily due to an inability to detect part-of-speech sequences unseen during training, and this effect diminishes when the number of unique instances in the training set increases.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"This limitation is primarily due to an inability to detect part-of-speech sequences unseen during training, and this effect diminishes when the number of unique instances in the training set increases.\""
    },
    {
        "title": "PMIndiaSum: Multilingual and Cross-lingual Headline Summarization for Languages in India",
        "authors": [
            "Ashok Urlana",
            "Pinzhen Chen",
            "Zheng Zhao",
            "Shay Cohen",
            "Manish Shrivastava",
            "Barry Haddow"
        ],
        "published": "2023",
        "summary": "This paper introduces PMIndiaSum, a multilingual and massively parallel summarization corpus focused on languages in India. Our corpus provides a training and testing ground for four language families, 14 languages, and the largest to date with 196 language pairs. We detail our construction workflow including data acquisition, processing, and quality assurance. Furthermore, we publish benchmarks for monolingual, cross-lingual, and multilingual summarization by fine-tuning, prompting, as well as translate-and-summarize. Experimental results confirm the crucial role of our data in aiding summarization between Indian languages. Our dataset is publicly available and can be freely modified and re-distributed.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.777.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Beyond Labels: Empowering Human Annotators with Natural Language Explanations through a Novel Active-Learning Architecture",
        "authors": [
            "Bingsheng Yao",
            "Ishan Jindal",
            "Lucian Popa",
            "Yannis Katsis",
            "Sayan Ghosh",
            "Lihong He",
            "Yuxuan Lu",
            "Shashank Srivastava",
            "Yunyao Li",
            "James Hendler",
            "Dakuo Wang"
        ],
        "published": "2023",
        "summary": "Real-world domain experts (e.g., doctors) rarely annotate only a decision label in their day-to-day workflow without providing explanations. Yet, existing low-resource learning techniques, such as Active Learning (AL), that aim to support human annotators mostly focus on the label while neglecting the natural language explanation of a data point. This work proposes a novel AL architecture to support experts’ real-world need for label and explanation annotations in low-resource scenarios. Our AL architecture leverages an explanation-generation model to produce explanations guided by human explanations, a prediction model that utilizes generated explanations toward prediction faithfully, and a novel data diversity-based AL sampling strategy that benefits from the explanation annotations. Automated and human evaluations demonstrate the effectiveness of incorporating explanations into AL sampling and the improved human annotation efficiency and trustworthiness with our AL architecture. Additional ablation studies illustrate the potential of our AL architecture for transfer learning, generalizability, and integration with large language models (LLMs). While LLMs exhibit exceptional explanation-generation capabilities for relatively simple tasks, their effectiveness in complex real-world tasks warrants further in-depth study.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.778.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While LLMs exhibit exceptional explanation-generation capabilities for relatively simple tasks, their effectiveness in complex real-world tasks warrants further in-depth study.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs, but only briefly and as a minor detail, without exploring it in depth. The primary focus of the paper is on the proposed AL architecture and its effectiveness.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While LLMs exhibit exceptional explanation-generation capabilities for relatively simple tasks, their effectiveness in complex real-world tasks warrants further in-depth study.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs, but only briefly and as a minor detail, without exploring it in depth. The primary focus of the paper is on the proposed AL architecture and its effectiveness."
    },
    {
        "title": "Decoding Stumpers: Large Language Models vs. Human Problem-Solvers",
        "authors": [
            "Alon Goldstein",
            "Miriam Havin",
            "Roi Reichart",
            "Ariel Goldstein"
        ],
        "published": "2023",
        "summary": "This paper investigates the problem-solving capabilities of Large Language Models (LLMs) by evaluating their performance on stumpers, unique single-step intuition problems that pose challenges for human solvers but are easily verifiable. We compare the performance of four state-of-the-art LLMs (Davinci-2, Davinci-3, GPT-3.5-Turbo, GPT-4) to human participants. Our findings reveal that the new-generation LLMs excel in solving stumpers and surpass human performance. However, humans exhibit superior skills in verifying solutions to the same problems. This research enhances our understanding of LLMs’ cognitive abilities and provides insights for enhancing their problem-solving potential across various domains.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.779.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, humans exhibit superior skills in verifying solutions to the same problems.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, humans exhibit superior skills in verifying solutions to the same problems.\""
    },
    {
        "title": "Efficient Cross-Task Prompt Tuning for Few-Shot Conversational Emotion Recognition",
        "authors": [
            "Yige Xu",
            "Zhiwei Zeng",
            "Zhiqi Shen"
        ],
        "published": "2023",
        "summary": "Emotion Recognition in Conversation (ERC) has been widely studied due to its importance in developing emotion-aware empathetic machines. The rise of pre-trained language models (PLMs) has further pushed the limit of ERC performance. However, most recent works on ERC using PLMs are heavily data-driven, and requires fine-tuning the entire PLMs. To improve both sample and computational efficiency, we propose a derivative-free optimization method called Cross-Task Prompt Tuning (CTPT) for few-shot conversational emotion recognition. Unlike existing methods that learn independent knowledge from individual tasks, CTPT leverages sharable cross-task knowledge by exploiting external knowledge from other source tasks to improve learning performance under the few-shot setting. Moreover, CTPT only needs to optimize a vector under the low intrinsic dimensionality without gradient, which is highly parameter-efficient compared with existing approaches. Experiments on five different contextual conversation datasets demonstrate that our CTPT method has superior results on both few-shot scenarios and zero-shot transfers.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.780.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, most recent works on ERC using PLMs are heavily data-driven, and requires fine-tuning the entire PLMs.\"\n\nThis abstract mentions a limitation of pre-trained language models (PLMs), which is a type of LLM, in that they require fine-tuning the entire model, which is a limitation in terms of computational efficiency.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, most recent works on ERC using PLMs are heavily data-driven, and requires fine-tuning the entire PLMs.\"\n\nThis abstract mentions a limitation of pre-trained language models (PLMs), which is a type of LLM, in that they require fine-tuning the entire model, which is a limitation in terms of computational efficiency."
    },
    {
        "title": "SYMPTOMIFY: Transforming Symptom Annotations with Language Model Knowledge Harvesting",
        "authors": [
            "Bosung Kim",
            "Ndapa Nakashole"
        ],
        "published": "2023",
        "summary": "Given the high-stakes nature of healthcare decision-making, we aim to improve the efficiency of human annotators rather than replacing them with fully automated solutions. We introduce a new comprehensive resource, SYMPTOMIFY, a dataset of annotated vaccine adverse reaction reports detailing individual vaccine reactions. The dataset, consisting of over 800k reports, surpasses previous datasets in size. Notably, it features reasoning-based explanations alongside background knowledge obtained via language model knowledge harvesting. We evaluate performance across various methods and learning paradigms, paving the way for future comparisons and benchmarking.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.781.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"background knowledge obtained via language model knowledge harvesting.\"\n\nThis paper mentions LLMs but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"background knowledge obtained via language model knowledge harvesting.\"\n\nThis paper mentions LLMs but does not discuss any limitations of LLMs."
    },
    {
        "title": "TokenDrop + BucketSampler: Towards Efficient Padding-free Fine-tuning of Language Models",
        "authors": [
            "Amrit Nagarajan",
            "Anand Raghunathan"
        ],
        "published": "2023",
        "summary": "The great success of Language Models (LMs) for various Natural Language Processing (NLP) tasks is accompanied by computational challenges during both pre-training and fine-tuning. Pre-training has attracted significant attention due to its huge computational footprint. We focus on the fine-tuning of pre-trained LMs, which is expected to be performed much more frequently as the pre-trained models are adapted to downstream tasks. During fine-tuning, the presence of variable-length input sequences necessitates the use of padding tokens when batching sequences. These padding tokens lead to ineffectual computations, adversely impacting the efficiency of fine-tuning. We also observe that LMs memorize the limited task-specific training data despite the use of known regularization methods. Based on these insights, we present TokenDrop + BucketSampler, a framework that simultaneously improves efficiency and accuracy of LM fine-tuning. BucketSampler generates batches of samples with lower variance in sequence lengths to reduce the number of padding tokens, but does so without the accompanying accuracy drop seen in previous approaches. TokenDrop is a new regularizer that prunes a random subset of insignificant tokens from each input sequence in every epoch to prevent overfitting. TokenDrop drops more tokens from the longer sequences in each batch to further reduce variance in input lengths and the need for padding. TokenDrop + BucketSampler accelerates fine-tuning on diverse downstream tasks by up to 10.61X, while also producing models that are up to 1.17% more accurate compared to conventional fine-tuning. Code is available at https://github.com/amrnag/TokenDrop-BucketSampler. .",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.782.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We also observe that LMs memorize the limited task-specific training data despite the use of known regularization methods.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We also observe that LMs memorize the limited task-specific training data despite the use of known regularization methods.\""
    },
    {
        "title": "Unified Representation for Non-compositional and Compositional Expressions",
        "authors": [
            "Ziheng Zeng",
            "Suma Bhat"
        ],
        "published": "2023",
        "summary": "Accurate processing of non-compositional language relies on generating good representations for such expressions. In this work, we study the representation of language non-compositionality by proposing a language model, PIER+, that builds on BART and can create semantically meaningful and contextually appropriate representations for English potentially idiomatic expressions (PIEs). PIEs are characterized by their non-compositionality and contextual ambiguity in their literal and idiomatic interpretations. Via intrinsic evaluation on embedding quality and extrinsic evaluation on PIE processing and NLU tasks, we show that representations generated by PIER+ result in 33% higher homogeneity score for embedding clustering than BART, whereas 3.12% and 3.29% gains in accuracy and sequence accuracy for PIE sense classification and span detection compared to the state-of-the-art IE representation model, GIEA. These gains are achieved without sacrificing PIER+’s performance on NLU tasks (+/- 1% accuracy) compared to BART.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.783.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but the paper aims to improve upon existing models (BART and GIEA) and mentions the challenges of processing non-compositional language, implying that current models may struggle with this task.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: No explicit mention of limitations, but the paper aims to improve upon existing models (BART and GIEA) and mentions the challenges of processing non-compositional language, implying that current models may struggle with this task."
    },
    {
        "title": "Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering",
        "authors": [
            "Kosuke Akimoto",
            "Kunihiro Takeoka",
            "Masafumi Oyamada"
        ],
        "published": "2023",
        "summary": "Retrieval-augmented generation models augment knowledge encoded in a language model by providing additional relevant external knowledge (context) during generation. Although it has been shown that the quantity and quality of context impact the performance of retrieval-augmented generation models during inference, limited research explores how these characteristics affect model training. This paper explores how context quantity and quality during model training affect the performance of Fusion-in-Decoder (FiD), the state-of-the-art retrieval-augmented generation model, in extractive open-domain question answering tasks. Experimental results suggest that FiD models overfit to context quality during training and show suboptimal performance when evaluated on different context quality. Through the experimental results, we also reveal FiD models trained with different context quality have different cross-attention distribution patterns. Specifically, as context quality during training increases, FiD models tend to attend more uniformly to each passage in context. Finally, based on these observations, we propose a method to mitigate overfitting to specific context quality by introducing bias to the cross-attention distribution, which we demonstrate to be effective in improving the performance of FiD models on different context quality.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.784.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experimental results suggest that FiD models overfit to context quality during training and show suboptimal performance when evaluated on different context quality.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Experimental results suggest that FiD models overfit to context quality during training and show suboptimal performance when evaluated on different context quality.\""
    },
    {
        "title": "Error Detection for Text-to-SQL Semantic Parsing",
        "authors": [
            "Shijie Chen",
            "Ziru Chen",
            "Huan Sun",
            "Yu Su"
        ],
        "published": "2023",
        "summary": "Despite remarkable progress in text-to-SQL semantic parsing in recent years, the performance of existing parsers is still far from perfect. Specifically, modern text-to-SQL parsers based on deep learning are often over-confident, thus casting doubt on their trustworthiness when deployed for real use. In this paper, we propose a parser-independent error detection model for text-to-SQL semantic parsing. Using a language model of code as its bedrock, we enhance our error detection model with graph neural networks that learn structural features of both natural language questions and SQL queries. We train our model on realistic parsing errors collected from a cross-domain setting, which leads to stronger generalization ability. Experiments with three strong text-to-SQL parsers featuring different decoding mechanisms show that our approach outperforms parser-dependent uncertainty metrics. Our model could also effectively improve the performance and usability of text-to-SQL semantic parsers regardless of their architectures.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.785.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"modern text-to-SQL parsers based on deep learning are often over-confident, thus casting doubt on their trustworthiness when deployed for real use.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"modern text-to-SQL parsers based on deep learning are often over-confident, thus casting doubt on their trustworthiness when deployed for real use.\""
    },
    {
        "title": "Multilingual Coarse Political Stance Classification of Media. The Editorial Line of a ChatGPT and Bard Newspaper",
        "authors": [
            "Cristina España-Bonet"
        ],
        "published": "2023",
        "summary": "Neutrality is difficult to achieve and, in politics, subjective. Traditional media typically adopt an editorial line that can be used by their potential readers as an indicator of the media bias. Several platforms currently rate news outlets according to their political bias. The editorial line and the ratings help readers in gathering a balanced view of news. But in the advent of instruction-following language models, tasks such as writing a newspaper article can be delegated to computers. Without imposing a biased persona, where would an AI-based news outlet lie within the bias ratings? In this work, we use the ratings of authentic news outlets to create a multilingual corpus of news with coarse stance annotations (Left and Right) along with automatically extracted topic annotations. We show that classifiers trained on this data are able to identify the editorial line of most unseen newspapers in English, German, Spanish and Catalan. We then apply the classifiers to 101 newspaper-like articles written by ChatGPT and Bard in the 4 languages at different time periods. We observe that, similarly to traditional newspapers, ChatGPT editorial line evolves with time and, being a data-driven system, the stance of the generated articles differs among languages.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.787.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Without imposing a biased persona, where would an AI-based news outlet lie within the bias ratings?\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Without imposing a biased persona, where would an AI-based news outlet lie within the bias ratings?\""
    },
    {
        "title": "Let’s Synthesize Step by Step: Iterative Dataset Synthesis with Large Language Models by Extrapolating Errors from Small Models",
        "authors": [
            "Ruida Wang",
            "Wangchunshu Zhou",
            "Mrinmaya Sachan"
        ],
        "published": "2023",
        "summary": "*Data Synthesis* is a promising way to train a small model with very little labeled data. One approach for data synthesis is to leverage the rich knowledge from large language models to synthesize pseudo training examples for small models, making it possible to achieve both data and compute efficiency at the same time. However, a key challenge in data synthesis is that the synthesized dataset often suffers from a large distributional discrepancy from the *real task* data distribution. Thus, in this paper, we propose *Synthesis Step by Step* (**S3**), a data synthesis framework that shrinks this distribution gap by iteratively extrapolating the errors made by a small model trained on the synthesized dataset on a small real-world validation dataset using a large language model. Extensive experiments on multiple NLP tasks show that our approach improves the performance of a small model by reducing the gap between the synthetic dataset and the real data, resulting in significant improvement compared to several baselines: 9.48% improvement compared to ZeroGen and 2.73% compared to GoldGen, and at most 15.17% improvement compared to the small model trained on human-annotated data.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.791.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, a key challenge in data synthesis is that the synthesized dataset often suffers from a large distributional discrepancy from the *real task* data distribution.\"\n\nThis abstract mentions a limitation of LLMs in data synthesis, but it is not the primary focus of the paper. The limitation is mentioned briefly to justify the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, a key challenge in data synthesis is that the synthesized dataset often suffers from a large distributional discrepancy from the *real task* data distribution.\"\n\nThis abstract mentions a limitation of LLMs in data synthesis, but it is not the primary focus of the paper. The limitation is mentioned briefly to justify the proposed solution."
    },
    {
        "title": "Identifying Early Maladaptive Schemas from Mental Health Question Texts",
        "authors": [
            "Sujatha Gollapalli",
            "Beng Ang",
            "See-Kiong Ng"
        ],
        "published": "2023",
        "summary": "In Psychotherapy, maladaptive schemas– negative perceptions that an individual has of the self, others, or the world that endure despite objective reality–often lead to resistance to treatments and relapse of mental health issues such as depression, anxiety, panic attacks etc. Identification of early maladaptive schemas (EMS) is thus a crucial step during Schema Therapy-based counseling sessions, where patients go through a detailed and lengthy EMS questionnaire. However, such an approach is not practical in ‘offline’ counseling scenarios, such as community QA forums which are gaining popularity for people seeking mental health support. In this paper, we investigate both LLM (Large Language Models) and non-LLM approaches for identifying EMS labels using resources from Schema Therapy. Our evaluation indicates that recent LLMs can be effective for identifying EMS but their predictions lack explainability and are too sensitive to precise ‘prompts’. Both LLM and non-LLM methods are unable to reliably address the null cases, i.e. cases with no EMS labels. However, we posit that the two approaches show complementary properties and together, they can be used to further devise techniques for EMS identification.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.792.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, such an approach is not practical in ‘offline’ counseling scenarios...\"; \"their predictions lack explainability and are too sensitive to precise ‘prompts’.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, such an approach is not practical in ‘offline’ counseling scenarios...\"; \"their predictions lack explainability and are too sensitive to precise ‘prompts’.\""
    },
    {
        "title": "Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning",
        "authors": [
            "Zhuolin Yang",
            "Wei Ping",
            "Zihan Liu",
            "Vijay Korthikanti",
            "Weili Nie",
            "De-An Huang",
            "Linxi Fan",
            "Zhiding Yu",
            "Shiyi Lan",
            "Bo Li",
            "Mohammad Shoeybi",
            "Ming-Yu Liu",
            "Yuke Zhu",
            "Bryan Catanzaro",
            "Chaowei Xiao",
            "Anima Anandkumar"
        ],
        "published": "2023",
        "summary": "Augmenting pretrained language models (LMs) with a vision encoder (e.g., Flamingo) has obtained state-of-the-art results in image-to-text generation. However, these models store all the knowledge within their parameters, thus often requiring enormous model parameters to model the abundant visual concepts and very rich text descriptions. Additionally, they are inefficient in incorporating new data, requiring a computational-expensive fine-tuning process. In this work, we introduce a Retrieval-augmented Visual Language Model, Re-ViLM, built upon the Flamingo, that supports retrieving the relevant knowledge from the external database for zero and in-context few-shot image-to-text generations. By storing certain knowledge explicitly in the external database, our approach reduces the number of model parameters and can easily accommodate new data during evaluation by simply updating the database. We also construct an interleaved image and text data that facilitates in-context few-shot learning capabilities.We demonstrate that Re-ViLM significantly boosts performance for image-to-text generation tasks, especially for zero-shot and few-shot generation in out-of-domain settings with 4x less parameters compared with baseline methods.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.793.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these models store all the knowledge within their parameters, thus often requiring enormous model parameters to model the abundant visual concepts and very rich text descriptions. Additionally, they are inefficient in incorporating new data, requiring a computational-expensive fine-tuning process.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, these models store all the knowledge within their parameters, thus often requiring enormous model parameters to model the abundant visual concepts and very rich text descriptions. Additionally, they are inefficient in incorporating new data, requiring a computational-expensive fine-tuning process.\""
    },
    {
        "title": "Syntax Matters: Towards Spoken Language Understanding via Syntax-Aware Attention",
        "authors": [
            "Yifeng Xie",
            "Zhihong Zhu",
            "Xuxin Cheng",
            "Zhiqi Huang",
            "Dongsheng Chen"
        ],
        "published": "2023",
        "summary": "Spoken Language Understanding (SLU), a crucial component of task-oriented dialogue systems, has consistently garnered attention from both academic and industrial communities. Although incorporating syntactic information into models has the potential to enhance the comprehension of user utterances and yield impressive results, its application in SLU systems remains largely unexplored. In this paper, we propose a carefully designed model termed Syntax-aware attention (SAT) to enhance SLU, where attention scopes are constrained based on relationships within the syntactic structure. Experimental results on three datasets show that our model achieves substantial improvements and excellent performance. Moreover, SAT can be integrated into other BERT-based language models to further boost their performance.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.794.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate",
        "authors": [
            "Boshi Wang",
            "Xiang Yue",
            "Huan Sun"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) such as ChatGPT and GPT-4 have shown impressive performance in complex reasoning tasks. However, it is difficult to know whether the models are reasoning based on deep understandings of truth and logic, or leveraging their memorized patterns in a relatively superficial way. In this work, we explore testing LLMs’ reasoning by engaging with them in a debate-like conversation, where given a question, the LLM and the user need to discuss to make the correct decision starting from opposing arguments. Upon mitigating the Clever Hans effect, our task requires the LLM to not only achieve the correct answer on its own, but also be able to hold and defend its belief instead of blindly believing or getting misled by the user’s (invalid) arguments and critiques, thus testing in greater depth whether the LLM grasps the essence of the reasoning required to solve the problem. Across a range of complex reasoning benchmarks spanning math, commonsense, logic and BIG-Bench tasks, we find that despite their impressive performance as reported in existing work on generating correct step-by-step solutions in the beginning, LLMs like ChatGPT cannot maintain their beliefs in truth for a significant portion of examples when challenged by oftentimes absurdly invalid arguments. Our work points to danger zones of model alignment, and also suggests more careful treatments and interpretations of the recent findings that LLMs can improve their responses based on feedback.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.795.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it is difficult to know whether the models are reasoning based on deep understandings of truth and logic, or leveraging their memorized patterns in a relatively superficial way.\"; \"Across a range of complex reasoning benchmarks spanning math, commonsense, logic and BIG-Bench tasks, we find that despite their impressive performance as reported in existing work on generating correct step-by-step solutions in the",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, it is difficult to know whether the models are reasoning based on deep understandings of truth and logic, or leveraging their memorized patterns in a relatively superficial way.\"; \"Across a range of complex reasoning benchmarks spanning math, commonsense, logic and BIG-Bench tasks, we find that despite their impressive performance as reported in existing work on generating correct step-by-step solutions in the"
    },
    {
        "title": "Using In-Context Learning to Improve Dialogue Safety",
        "authors": [
            "Nicholas Meade",
            "Spandana Gella",
            "Devamanyu Hazarika",
            "Prakhar Gupta",
            "Di Jin",
            "Siva Reddy",
            "Yang Liu",
            "Dilek Hakkani-Tur"
        ],
        "published": "2023",
        "summary": "While large neural-based conversational models have become increasingly proficient dialogue agents, recent work has highlighted safety issues with these systems. For example, these systems can be goaded into generating toxic content, often perpetuating social biases or stereotypes. We investigate a retrieval-based approach for reducing bias and toxicity in responses from chatbots. It uses in-context learning to steer a model towards safer generations. Concretely, to generate a response to an unsafe dialogue context, we retrieve demonstrations of safe responses to similar dialogue contexts. We find our method performs competitively with existing approaches to dialogue safety without requiring training. We also show, using automatic and human evaluation, that reductions in toxicity obtained using our approach are not at the cost engagingness or coherency. Finally, we note our method can be used in compliment to existing dialogue safety approaches, such as RLHF.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.796.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While large neural-based conversational models have become increasingly proficient dialogue agents, recent work has highlighted safety issues with these systems. For example, these systems can be goaded into generating toxic content, often perpetuating social biases or stereotypes.\"\n\nThis abstract mentions limitations of LLMs (safety issues, generating toxic content, social biases, stereotypes) but does not explore them in detail,",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While large neural-based conversational models have become increasingly proficient dialogue agents, recent work has highlighted safety issues with these systems. For example, these systems can be goaded into generating toxic content, often perpetuating social biases or stereotypes.\"\n\nThis abstract mentions limitations of LLMs (safety issues, generating toxic content, social biases, stereotypes) but does not explore them in detail,"
    },
    {
        "title": "Defining a New NLP Playground",
        "authors": [
            "Sha Li",
            "Chi Han",
            "Pengfei Yu",
            "Carl Edwards",
            "Manling Li",
            "Xingyao Wang",
            "Yi Fung",
            "Charles Yu",
            "Joel Tetreault",
            "Eduard Hovy",
            "Heng Ji"
        ],
        "published": "2023",
        "summary": "The recent explosion of performance of large language models (LLMs) has changed the field of Natural Language Processing (NLP) more abruptly and seismically than any other shift in the field’s 80 year history. This has resulted in concerns that the field will become homogenized and resource-intensive. This new status quo has put many academic researchers, especially PhD students, at a disadvantage. This paper aims to define a new NLP playground by proposing 20+ PhD-dissertation-worthy research directions, covering theoretical analysis, new and challenging problems, learning paradigms and interdisciplinary applications.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.799.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This has resulted in concerns that the field will become homogenized and resource-intensive. This new status quo has put many academic researchers, especially PhD students, at a disadvantage.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"This has resulted in concerns that the field will become homogenized and resource-intensive. This new status quo has put many academic researchers, especially PhD students, at a disadvantage.\""
    },
    {
        "title": "Leveraging GPT-4 for Automatic Translation Post-Editing",
        "authors": [
            "Vikas Raunak",
            "Amr Sharaf",
            "Yiren Wang",
            "Hany Awadalla",
            "Arul Menezes"
        ],
        "published": "2023",
        "summary": "While Neural Machine Translation (NMT) represents the leading approach to Machine Translation (MT), the outputs of NMT models still require translation post-editing to rectify errors and enhance quality under critical settings. In this work, we formalize the task of direct translation post-editing with Large Language Models (LLMs) and explore the use of GPT-4 to automatically post-edit NMT outputs across several language pairs. Our results demonstrate that GPT-4 is adept at translation post-editing, producing meaningful and trustworthy edits to translations that help improve its general quality as well as remove different classes of major errors in translations. In particular, human evaluations on assessing edit trustworthiness show that GPT-4 exhibits a large improvement over the prior state-of-the-art LLM. Notably, we improve upon state-of-the-art performance on WMT-22 English-Chinese, English-German, Chinese-English and German-English language pairs using GPT-4 based post-editing, as evaluated by state-of-the-art MT quality metrics. However, we also show that GPT-4 could produce hallucinated edits, thereby urging caution in its use as an expert translation post-editor.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.804.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, we also show that GPT-4 could produce hallucinated edits, thereby urging caution in its use as an expert translation post-editor.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, we also show that GPT-4 could produce hallucinated edits, thereby urging caution in its use as an expert translation post-editor.\""
    },
    {
        "title": "Uniform Complexity for Text Generation",
        "authors": [
            "Joseph Marvin Imperial",
            "Harish Tayyar Madabushi"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have shown promising results in a wide array of generative NLP tasks, such as summarization and machine translation. In the context of narrative generation, however, existing models still do not capture factors that contribute to producing consistent text. For instance, it is logical that a piece of text or a story should be uniformly readable throughout and that this form of complexity should be controllable. As such, if the complexity of an input text prompt is rated first-grade reading level in the Flesch Reading Ease test, then the generated text continuing the plot should also be within this range of complexity. With this in mind, we introduce Uniform Complexity for Text Generation (UCTG), a new benchmark test which raises the challenge of making generative models observe uniform linguistic properties with respect to prompts. We experiment with over 150+ linguistically and cognitively motivated features for evaluating text complexity in humans and generative models. From our results, we find that models such as GPT-2 struggle to preserve the complexity of input prompts used in its generations, even if finetuned with professionally written texts.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.805.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"existing models still do not capture factors that contribute to producing consistent text... models such as GPT-2 struggle to preserve the complexity of input prompts used in its generations, even if finetuned with professionally written texts.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"existing models still do not capture factors that contribute to producing consistent text... models such as GPT-2 struggle to preserve the complexity of input prompts used in its generations, even if finetuned with professionally written texts.\""
    },
    {
        "title": "Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with LLMs",
        "authors": [
            "Hongru Wang",
            "Rui Wang",
            "Fei Mi",
            "Yang Deng",
            "Zezhong Wang",
            "Bin Liang",
            "Ruifeng Xu",
            "Kam-Fai Wong"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs), such as ChatGPT, greatly empower dialogue systems with strong language understanding and generation capabilities. However, most of the previous works prompt the LLMs to directly generate a response based on the dialogue context, overlooking the underlying linguistic cues about the user status exhibited in the context. Such in-depth dialogue scenarios are challenging for existing LLMs to figure out the user’s hidden needs and respond satisfactorily through a single-step inference. To this end, we propose a novel linguistic cue-based chain-of-thoughts (Cue-CoT), which enhances the LLMs inference with an intermediate reasoning step to find cues exhibited in the dialogue, aiming to provide a more personalized and engaging response. To evaluate the approach, we build a benchmark with in-depth dialogue questions, consisting of 6 datasets in both Chinese and English, targeting 3 major linguistic cues during the conversation: personality, emotion, and psychology. We conducted experiments on the proposed benchmark with 5 LLMs under both zero-shot and one-shot settings. Empirical results demonstrate our proposed Cue-CoT method outperforms standard prompting methods in terms of both helpfulness and acceptability on all datasets.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.806.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, most of the previous works prompt the LLMs to directly generate a response based on the dialogue context, overlooking the underlying linguistic cues about the user status exhibited in the context. Such in-depth dialogue scenarios are challenging for existing LLMs to figure out the user’s hidden needs and respond satisfactorily through a single-step inference.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, most of the previous works prompt the LLMs to directly generate a response based on the dialogue context, overlooking the underlying linguistic cues about the user status exhibited in the context. Such in-depth dialogue scenarios are challenging for existing LLMs to figure out the user’s hidden needs and respond satisfactorily through a single-step inference.\""
    },
    {
        "title": "Towards Anytime Fine-tuning: Continually Pre-trained Language Models with Hypernetwork Prompts",
        "authors": [
            "Gangwei Jiang",
            "Caigao Jiang",
            "Siqiao Xue",
            "James Zhang",
            "Jun Zhou",
            "Defu Lian",
            "Ying Wei"
        ],
        "published": "2023",
        "summary": "Continual pre-training has been urgent for adapting a pre-trained model to a multitude of domains and tasks in the fast-evolving world. In practice, a continually pre-trained model is expected to demonstrate not only greater capacity when fine-tuned on pre-trained domains but also a non-decreasing performance on unseen ones. In this work, we first investigate such anytime fine-tuning effectiveness of existing continual pre-training approaches, concluding with unanimously decreased performance on unseen domains. To this end, we propose a prompt-guided continual pre-training method, where we train a hypernetwork to generate domain-specific prompts by both agreement and disagreement losses. The agreement loss maximally preserves the generalization of a pre-trained model to new domains, and the disagreement one guards the exclusiveness of the generated hidden states for each domain. Remarkably, prompts by the hypernetwork alleviate the domain identity when fine-tuning and promote knowledge transfer across domains. Our method achieved improvements of 3.57% and 3.4% on two real-world datasets (including domain shift and temporal shift), respectively, demonstrating its efficacy.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.808.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"concluding with unanimously decreased performance on unseen domains.\"\n\nThis abstract mentions a limitation of LLMs (decreased performance on unseen domains) in passing, but does not elaborate on it further and focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"concluding with unanimously decreased performance on unseen domains.\"\n\nThis abstract mentions a limitation of LLMs (decreased performance on unseen domains) in passing, but does not elaborate on it further and focuses on the proposed solution."
    },
    {
        "title": "Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts",
        "authors": [
            "Deepanway Ghosal",
            "Navonil Majumder",
            "Roy Lee",
            "Rada Mihalcea",
            "Soujanya Poria"
        ],
        "published": "2023",
        "summary": "Visual question answering (VQA) is the task of answering questions about an image. The task assumes an understanding of both the image and the question to provide a natural language answer. VQA has gained popularity in recent years due to its potential applications in a wide range of fields, including robotics, education, and healthcare. In this paper, we focus on knowledge-augmented VQA, where answering the question requires commonsense knowledge, world knowledge, and reasoning about ideas and concepts not present in the image. We propose a multimodal framework that uses language guidance (LG) in the form of rationales, image captions, scene graphs, etc to answer questions more accurately. We benchmark our method on the multi-choice question-answering task of the A-OKVQA, Science-QA, VSR, and IconQA datasets using CLIP and BLIP models. We show that the use of language guidance is a simple but powerful and effective strategy for visual question answering. Our language guidance improves the performance of CLIP by 7.6% and BLIP-2 by 4.8% in the challenging A-OKVQA dataset. We also observe consistent improvement in performance on the Science-QA, VSR, and IconQA datasets when using the proposed language guidances. The implementation of LG-VQA is publicly available at https://github.com/declare-lab/LG-VQA.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.809.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation of LLMs is mentioned in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation of LLMs is mentioned in the abstract."
    },
    {
        "title": "Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data",
        "authors": [
            "Kashun Shum",
            "Shizhe Diao",
            "Tong Zhang"
        ],
        "published": "2023",
        "summary": "Chain-of-thought (CoT) advances the reasoning abilities of large language models (LLMs) and achieves superior performance in complex reasoning tasks. However, most CoT studies rely on carefully designed human-annotated rational chains to prompt LLMs, posing challenges for real-world applications where labeled data is available without rational chains. This paper proposes a new strategy, AutomateCoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought), that can bypass human engineering of CoT by automatically augmenting rational chains from a small labeled dataset, and then pruning low-quality chains to construct a candidate pool of machinegenerated rationale chains based on the labels. Finally, it selects the optimal combination of several rationale chains from the pool for CoT prompting by employing a variance-reduced policy gradient strategy to estimate the significance of each example. Automate-CoT enables a quick adaptation of the CoT technique to different tasks. Experimental results demonstrate the effectiveness of our method, where competitive results are achieved on arithmetic reasoning (+2.7%), commonsense reasoning (+3.4%), symbolic reasoning (+3.2%), and non-reasoning tasks (+2.5%).",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.811.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, most CoT studies rely on carefully designed human-annotated rational chains to prompt LLMs, posing challenges for real-world applications where labeled data is available without rational chains.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, most CoT studies rely on carefully designed human-annotated rational chains to prompt LLMs, posing challenges for real-world applications where labeled data is available without rational chains.\""
    },
    {
        "title": "What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations",
        "authors": [
            "Kavel Rao",
            "Liwei Jiang",
            "Valentina Pyatkin",
            "Yuling Gu",
            "Niket Tandon",
            "Nouha Dziri",
            "Faeze Brahman",
            "Yejin Choi"
        ],
        "published": "2023",
        "summary": "Moral or ethical judgments rely heavily on the specific contexts in which they occur. Understanding varying shades of defeasible contextualizations (i.e., additional information that strengthens or attenuates the moral acceptability of an action) is critical to accurately represent the subtlety and intricacy of grounded human moral judgment in real-life scenarios. We introduce defeasible moral reasoning: a task to provide grounded contexts that make an action more or less morally acceptable, along with commonsense rationales that justify the reasoning. To elicit high-quality task data, we take an iterative self-distillation approach that starts from a small amount of unstructured seed knowledge from GPT-3 and then alternates between (1) self-distillation from student models; (2) targeted filtering with a critic model trained by human judgment (to boost validity) and NLI (to boost diversity); (3) self-imitation learning (to amplify the desired data quality). This process yields a student model that produces defeasible contexts with improved validity, diversity, and defeasibility. From this model we distill a high-quality dataset, 𝛿-Rules-of-Thumb, of 1.2M entries of contextualizations and rationales for 115K defeasible moral actions rated highly by human annotators 85.9% to 99.8% of the time. Using 𝛿-RoT we obtain a final student model that wins over all intermediate student models by a notable margin.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.812.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"starts from a small amount of unstructured seed knowledge from GPT-3\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"starts from a small amount of unstructured seed knowledge from GPT-3\""
    },
    {
        "title": "An Empirical Study on Multiple Knowledge from ChatGPT for Emotion Recognition in Conversations",
        "authors": [
            "Geng Tu",
            "Bin Liang",
            "Bing Qin",
            "Kam-Fai Wong",
            "Ruifeng Xu"
        ],
        "published": "2023",
        "summary": "Multiple knowledge (e.g., co-reference, topics, emotional causes, etc) has been demonstrated effective for emotion detection. However, exploring this knowledge in Emotion Recognition in Conversations (ERC) is currently a blank slate due to the lack of annotated data and the high cost involved in obtaining such knowledge. Fortunately, the emergence of Large Language Models (LLMs) holds promise in filling this void. Therefore, we propose a Multiple Knowledge Fusion Model (MKFM) to effectively integrate such knowledge generated by LLMs for ERC and empirically study its impact on the model. Experimental results on three public datasets have demonstrated the effectiveness of multiple knowledge for ERC. Furthermore, we conduct a detailed analysis of the contribution and complementarity of this knowledge.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.813.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Fortunately, the emergence of Large Language Models (LLMs) holds promise in filling this void.\"\n\nThis rating is chosen because the abstract mentions LLMs but does not discuss any limitations of LLMs. The mention of LLMs is brief and serves to introduce the proposed model that utilizes knowledge generated by LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Fortunately, the emergence of Large Language Models (LLMs) holds promise in filling this void.\"\n\nThis rating is chosen because the abstract mentions LLMs but does not discuss any limitations of LLMs. The mention of LLMs is brief and serves to introduce the proposed model that utilizes knowledge generated by LLMs."
    },
    {
        "title": "One For All & All For One: Bypassing Hyperparameter Tuning with Model Averaging for Cross-Lingual Transfer",
        "authors": [
            "Fabian David Schmidt",
            "Ivan Vulić",
            "Goran Glavaš"
        ],
        "published": "2023",
        "summary": "Multilingual language models enable zero-shot cross-lingual transfer (ZS-XLT): fine-tuned on sizable source-language task data, they perform the task in target languages without labeled instances. The effectiveness of ZS-XLT hinges on the linguistic proximity between languages and the amount of pretraining data for a language. Because of this, model selection based on source-language validation is unreliable: it picks model snapshots with suboptimal target-language performance. As a remedy, some work optimizes ZS-XLT by extensively tuning hyperparameters: the follow-up work then routinely struggles to replicate the original results. Other work searches over narrower hyperparameter grids, reporting substantially lower performance. In this work, we therefore propose an unsupervised evaluation protocol for ZS-XLT that decouples performance maximization from hyperparameter tuning. As a robust and more transparent alternative to extensive hyperparameter tuning, we propose to accumulatively average snapshots from different runs into a single model. We run broad ZS-XLT experiments on both higher-level semantic tasks (NLI, extractive QA) and a lower-level token classification task (NER) and find that conventional model selection based on source-language validation quickly plateaus to suboptimal ZS-XLT performance. On the other hand, our accumulative run-by-run averaging of models trained with different hyperparameters boosts ZS-XLT performance and closely correlates with “oracle” ZS-XLT, i.e., model selection based on target-language validation performance.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.815.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Because of this, model selection based on source-language validation is unreliable: it picks model snapshots with suboptimal target-language performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Because of this, model selection based on source-language validation is unreliable: it picks model snapshots with suboptimal target-language performance.\""
    },
    {
        "title": "MetaReVision: Meta-Learning with Retrieval for Visually Grounded Compositional Concept Acquisition",
        "authors": [
            "Guangyue Xu",
            "Parisa Kordjamshidi",
            "Joyce Chai"
        ],
        "published": "2023",
        "summary": "Humans have the ability to learn novel compositional concepts by recalling primitive concepts acquired from past experience and generalizing these primitive concepts to novel compositions. Inspired by the above human’s compositional learning procedure, in this paper, we propose MetaReVision, a retrievalenhanced meta-learning model to solve the visually grounded compositional concept learning problem. The proposed MetaReVision consists of a retrieval module and a meta-learning module which are designed to incorporate retrieved primitive concepts as supporting set to meta-train visual-language models for grounded compositional concept recognition. Through meta-learning from episodes constructed by the retriever, MetaReVision learns a generic compositional representation that can be fast updated to recognize novel composi tional concepts. We create CompCOCO and CompFlickr to benchmark the grounded compositional concept learning. Our experimental results show MetaReVision outperforms other competitive baselines and the retrieval module does plays an important role in this compositional learning process.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.818.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding",
        "authors": [
            "Yu Zhang",
            "Hao Cheng",
            "Zhihong Shen",
            "Xiaodong Liu",
            "Ye-Yi Wang",
            "Jianfeng Gao"
        ],
        "published": "2023",
        "summary": "Scientific literature understanding tasks have gained significant attention due to their potential to accelerate scientific discovery. Pre-trained language models (LMs) have shown effectiveness in these tasks, especially when tuned via contrastive learning. However, jointly utilizing pre-training data across multiple heterogeneous tasks (e.g., extreme multi-label paper classification, citation prediction, and literature search) remains largely unexplored. To bridge this gap, we propose a multi-task contrastive learning framework, SciMult, with a focus on facilitating common knowledge sharing across different scientific literature understanding tasks while preventing task-specific skills from interfering with each other. To be specific, we explore two techniques – task-aware specialization and instruction tuning. The former adopts a Mixture-of-Experts Transformer architecture with task-aware sub-layers; the latter prepends task-specific instructions to the input text so as to produce task-aware outputs. Extensive experiments on a comprehensive collection of benchmark datasets verify the effectiveness of our task-aware specialization strategy, where we outperform state-of-the-art scientific pre-trained LMs. Code, datasets, and pre-trained models can be found at https://scimult.github.io/.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.820.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, jointly utilizing pre-training data across multiple heterogeneous tasks (e.g., extreme multi-label paper classification, citation prediction, and literature search) remains largely unexplored.\"\n\nThis rating is chosen because the abstract mentions a limitation of pre-trained LMs in a specific context (joint utilization of pre-training data across multiple tasks), but it is not the primary focus of the paper and is",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, jointly utilizing pre-training data across multiple heterogeneous tasks (e.g., extreme multi-label paper classification, citation prediction, and literature search) remains largely unexplored.\"\n\nThis rating is chosen because the abstract mentions a limitation of pre-trained LMs in a specific context (joint utilization of pre-training data across multiple tasks), but it is not the primary focus of the paper and is"
    },
    {
        "title": "BLM-s/lE: A structured dataset of English spray-load verb alternations for testing generalization in LLMs",
        "authors": [
            "Giuseppe Samo",
            "Vivi Nastase",
            "Chunyang Jiang",
            "Paola Merlo"
        ],
        "published": "2023",
        "summary": "Current NLP models appear to be achieving performance comparable to human capabilities on well-established benchmarks. New benchmarks are now necessary to test deeper layers of understanding of natural languages by these models. Blackbird’s Language Matrices are a recently developed framework that draws inspiration from tests of human analytic intelligence. The BLM task has revealed that successful performances in previously studied linguistic problems do not yet stem from a deep understanding of the generative factors that define these problems. In this study, we define a new BLM task for predicate-argument structure, and develop a structured dataset for its investigation, concentrating on the spray-load verb alternations in English, as a case study. The context sentences include one alternant from the spray-load alternation and the target sentence is the other alternant, to be chosen among a minimally contrastive and adversarial set of answers. We describe the generation process of the dataset and the reasoning behind the generating rules. The dataset aims to facilitate investigations into how verb information is encoded in sentence embeddings and how models generalize to the complex properties of argument structures. Benchmarking experiments conducted on the dataset and qualitative error analysis on the answer set reveal the inherent challenges associated with the problem even for current high-performing representations.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.821.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The BLM task has revealed that successful performances in previously studied linguistic problems do not yet stem from a deep understanding of the generative factors that define these problems.\"; \"Benchmarking experiments conducted on the dataset and qualitative error analysis on the answer set reveal the inherent challenges associated with the problem even for current high-performing representations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"The BLM task has revealed that successful performances in previously studied linguistic problems do not yet stem from a deep understanding of the generative factors that define these problems.\"; \"Benchmarking experiments conducted on the dataset and qualitative error analysis on the answer set reveal the inherent challenges associated with the problem even for current high-performing representations.\""
    },
    {
        "title": "Geographical Erasure in Language Generation",
        "authors": [
            "Pola Schwöbel",
            "Jacek Golebiowski",
            "Michele Donini",
            "Cedric Archambeau",
            "Danish Pruthi"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) encode vast amounts of world knowledge. However, since these models are trained on large swaths of internet data, they are at risk of inordinately capturing information about dominant groups. This imbalance can propagate into generated language. In this work, we study and operationalise a form of geographical erasure wherein language models underpredict certain countries. We demonstrate consistent instances of erasure across a range of LLMs. We discover that erasure strongly correlates with low frequencies of country mentions in the training corpus. Lastly, we mitigate erasure by finetuning using a custom objective.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.823.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, since these models are trained on large swaths of internet data, they are at risk of inordinately capturing information about dominant groups. This imbalance can propagate into generated language.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, since these models are trained on large swaths of internet data, they are at risk of inordinately capturing information about dominant groups. This imbalance can propagate into generated language.\""
    },
    {
        "title": "Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting",
        "authors": [
            "Haoyang Huang",
            "Tianyi Tang",
            "Dongdong Zhang",
            "Xin Zhao",
            "Ting Song",
            "Yan Xia",
            "Furu Wei"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) demonstrate impressive multilingual capability, but their performance varies substantially across different languages. In this work, we introduce a simple yet effective method, called cross-lingual-thought prompting (XLT), to systematically improve the multilingual capability of LLMs. Specifically, XLT is a generic template prompt that stimulates cross-lingual and logical reasoning skills to enhance task performance across languages. We conduct comprehensive evaluations on 7 typical benchmarks related to reasoning, understanding, and generation tasks, covering both high-resource and low-resource languages. Experimental results show that XLT not only remarkably enhances the performance of various multilingual tasks but also significantly reduces the gap between the average performance and the best performance of each task in different languages. Notably, XLT brings over 10 points of average improvement in arithmetic reasoning and open-domain question-answering tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.826.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models (LLMs) demonstrate impressive multilingual capability, but their performance varies substantially across different languages.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large language models (LLMs) demonstrate impressive multilingual capability, but their performance varies substantially across different languages.\""
    },
    {
        "title": "DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text",
        "authors": [
            "Jinyan Su",
            "Terry Zhuo",
            "Di Wang",
            "Preslav Nakov"
        ],
        "published": "2023",
        "summary": "With the rapid progress of Large language models (LLMs) and the huge amount of text they generate, it becomes impractical to manually distinguish whether a text is machine-generated. The growing use of LLMs in social media and education, prompts us to develop methods to detect machine-generated text, preventing malicious use such as plagiarism, misinformation, and propaganda. In this paper, we introduce two novel zero-shot methods for detecting machine-generated text by leveraging the Log-Rank information. One is called DetectLLM-LRR, which is fast and efficient, and the other is called DetectLLM-NPR, which is more accurate, but slower due to the need for perturbations. Our experiments on three datasets and seven language models show that our proposed methods improve over the state of the art by 3.9 and 1.75 AUROC points absolute. Moreover, DetectLLM-NPR needs fewer perturbations than previous work to achieve the same level of performance, which makes it more practical for real-world use. We also investigate the efficiency-performance trade-off based on users’ preference for these two measures and provide intuition for using them in practice effectively. We release the data and the code of both methods in https://github.com/mbzuai-nlp/DetectLLM.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.827.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The growing use of LLMs in social media and education, prompts us to develop methods to detect machine-generated text, preventing malicious use such as plagiarism, misinformation, and propaganda.\"\n\nThis rating is given because the paper mentions a limitation of LLMs (malicious use) but does not explore it in depth. The primary focus of the paper is on the proposed solution for detecting",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The growing use of LLMs in social media and education, prompts us to develop methods to detect machine-generated text, preventing malicious use such as plagiarism, misinformation, and propaganda.\"\n\nThis rating is given because the paper mentions a limitation of LLMs (malicious use) but does not explore it in depth. The primary focus of the paper is on the proposed solution for detecting"
    },
    {
        "title": "From Complex to Simple: Unraveling the Cognitive Tree for Reasoning with Small Language Models",
        "authors": [
            "Yan Junbing",
            "Chengyu Wang",
            "Taolin Zhang",
            "Xiaofeng He",
            "Jun Huang",
            "Wei Zhang"
        ],
        "published": "2023",
        "summary": "Reasoning is a distinctive human capacity, enabling us to address complex problems by breaking them down into a series of manageable cognitive steps. Yet, complex logical reasoning is still cumbersome for language models. Based on the dual process theory in cognitive science, we are the first to unravel the cognitive reasoning abilities of language models. Our framework employs an iterative methodology to construct a Cognitive Tree (CogTree). The root node of this tree represents the initial query, while the leaf nodes consist of straightforward questions that can be answered directly. This construction involves two main components: the implicit extraction module (referred to as the intuitive system) and the explicit reasoning module (referred to as the reflective system). The intuitive system rapidly generates multiple responses by utilizing in-context examples, while the reflective system scores these responses using comparative learning. The scores guide the intuitive system in its subsequent generation step.Our experimental results on two popular and challenging reasoning tasks indicate that it is possible to achieve a performance level comparable to that of GPT-3.5 (with 175B parameters), using a significantly smaller language model that contains fewer parameters (<=7B) than 5% of GPT-3.5.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.828.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Yet, complex logical reasoning is still cumbersome for language models.\"\n\nThis paper mentions a limitation of language models in passing, specifically their struggle with complex logical reasoning, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Yet, complex logical reasoning is still cumbersome for language models.\"\n\nThis paper mentions a limitation of language models in passing, specifically their struggle with complex logical reasoning, but it is not the primary focus of the paper."
    },
    {
        "title": "Macedon: Minimizing Representation Coding Rate Reduction for Cross-Lingual Natural Language Understanding",
        "authors": [
            "Haoyu Wang",
            "Yaqing Wang",
            "Huaxiu Yao",
            "Jing Gao"
        ],
        "published": "2023",
        "summary": "Cross-lingual natural language understanding(NLU) is one of the fundamental tasks of NLP. The goal is to learn a model which can generalize well on both high-resource and low-resource language data. Recent pre-trained multilingual language models, e.g., multilingual BERT, XLM, have shown impressive performance on cross-lingual NLU tasks. However, such promising results request the use of sufficient training data, which is a difficult condition to satisfy for low-resource language. When the data is limited in those low resource languages, the accuracy of existing models will drop. In light of this challenge, we investigate the important task of how to train the cross-lingual model with abundant high-source language data and limited low-resource language data. Existing methods typically learn language-agnostic representation via adversarial training and mutual information estimation. Existing approaches may suffer When data is very limited (e.g., low-resource language) because it is challenging to estimate data distribution accurately. To tackle this issue, we propose a conceptually innovative approach to remove language-associated information via minimizing representation coding rate reduction(Macedon). Specifically, Macedon avoids using extra codes to encode language-related information, which is measured by the rate-distortion function. To validate the effectiveness of Macedon, we conduct extensive experiments on three tasks, including paraphrase identification, natural language inference, and query advertisement matching. The experiment results show that the proposed Macedon outperforms state-of-the-art cross-lingual NLU approaches.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.829.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"When the data is limited in those low resource languages, the accuracy of existing models will drop.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"When the data is limited in those low resource languages, the accuracy of existing models will drop.\""
    },
    {
        "title": "Adversarial Robustness for Large Language NER models using Disentanglement and Word Attributions",
        "authors": [
            "Xiaomeng Jin",
            "Bhanukiran Vinzamuri",
            "Sriram Venkatapathy",
            "Heng Ji",
            "Pradeep Natarajan"
        ],
        "published": "2023",
        "summary": "Large language models (LLM’s) have been widely used for several applications such as question answering, text classification and clustering. While the preliminary results across the aforementioned tasks looks promising, recent work has dived deep into LLM’s performing poorly for complex Named Entity Recognition (NER) tasks in comparison to fine-tuned pre-trained language models (PLM’s). To enhance wider adoption of LLM’s, our paper investigates the robustness of such LLM NER models and its instruction fine-tuned variants to adversarial attacks. In particular, we propose a novel attack which relies on disentanglement and word attribution techniques where the former aids in learning an embedding capturing both entity and non-entity influences separately, and the latter aids in identifying important words across both components. This is in stark contrast to most techniques which primarily leverage non-entity words for perturbations limiting the space being explored to synthesize effective adversarial examples. Adversarial training results based on our method improves the F1 score over original LLM NER model by 8% and 18% on CoNLL-2003 and Ontonotes 5.0 datasets respectively.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.830.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"recent work has dived deep into LLM’s performing poorly for complex Named Entity Recognition (NER) tasks in comparison to fine-tuned pre-trained language models (PLM’s)\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"recent work has dived deep into LLM’s performing poorly for complex Named Entity Recognition (NER) tasks in comparison to fine-tuned pre-trained language models (PLM’s)\""
    },
    {
        "title": "LLMs – the Good, the Bad or the Indispensable?: A Use Case on Legal Statute Prediction and Legal Judgment Prediction on Indian Court Cases",
        "authors": [
            "Shaurya Vats",
            "Atharva Zope",
            "Somsubhra De",
            "Anurag Sharma",
            "Upal Bhattacharya",
            "Shubham Kumar Nigam",
            "Shouvik Guha",
            "Koustav Rudra",
            "Kripabandhu Ghosh"
        ],
        "published": "2023",
        "summary": "The Large Language Models (LLMs) have impacted many real-life tasks. To examine the efficacy of LLMs in a high-stake domain like law, we have applied state-of-the-art LLMs for two popular tasks: Statute Prediction and Judgment Prediction, on Indian Supreme Court cases. We see that while LLMs exhibit excellent predictive performance in Statute Prediction, their performance dips in Judgment Prediction when compared with many standard models. The explanations generated by LLMs (along with prediction) are of moderate to decent quality. We also see evidence of gender and religious bias in the LLM-predicted results. In addition, we present a note from a senior legal expert on the ethical concerns of deploying LLMs in these critical legal tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.831.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We see that while LLMs exhibit excellent predictive performance in Statute Prediction, their performance dips in Judgment Prediction when compared with many standard models. The explanations generated by LLMs (along with prediction) are of moderate to decent quality. We also see evidence of gender and religious bias in the LLM-predicted results.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We see that while LLMs exhibit excellent predictive performance in Statute Prediction, their performance dips in Judgment Prediction when compared with many standard models. The explanations generated by LLMs (along with prediction) are of moderate to decent quality. We also see evidence of gender and religious bias in the LLM-predicted results.\""
    },
    {
        "title": "Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers",
        "authors": [
            "Wencong You",
            "Zayd Hammoudeh",
            "Daniel Lowd"
        ],
        "published": "2023",
        "summary": "Backdoor attacks manipulate model predictions by inserting innocuous triggers into training and test data. We focus on more realistic and more challenging clean-label attacks where the adversarial training examples are correctly labeled. Our attack, LLMBkd, leverages language models to automatically insert diverse style-based triggers into texts. We also propose a poison selection technique to improve the effectiveness of both LLMBkd as well as existing textual backdoor attacks. Lastly, we describe REACT, a baseline defense to mitigate backdoor attacks via antidote training examples. Our evaluations demonstrate LLMBkd’s effectiveness and efficiency, where we consistently achieve high attack success rates across a wide range of styles with little effort and no model training.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.833.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Backdoor attacks manipulate model predictions by inserting innocuous triggers into training and test data.\"\n\nNote: The abstract mentions a limitation of LLMs in the context of backdoor attacks, but it is not the primary focus of the paper. The paper focuses more on proposing a new attack method and a defense mechanism.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Backdoor attacks manipulate model predictions by inserting innocuous triggers into training and test data.\"\n\nNote: The abstract mentions a limitation of LLMs in the context of backdoor attacks, but it is not the primary focus of the paper. The paper focuses more on proposing a new attack method and a defense mechanism."
    },
    {
        "title": "Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance",
        "authors": [
            "Song Wang",
            "Zhen Tan",
            "Ruocheng Guo",
            "Jundong Li"
        ],
        "published": "2023",
        "summary": "Adopting a two-stage paradigm of pretraining followed by fine-tuning, Pretrained Language Models (PLMs) have achieved substantial advancements in the field of natural language processing. However, in real-world scenarios, data labels are often noisy due to the complex annotation process, making it essential to develop strategies for fine-tuning PLMs with such noisy labels. To this end, we introduce an innovative approach for fine-tuning PLMs using noisy labels, which incorporates the guidance of Large Language Models (LLMs) like ChatGPT. This guidance assists in accurately distinguishing between clean and noisy samples and provides supplementary information beyond the noisy labels, thereby boosting the learning process during fine-tuning PLMs. Extensive experiments on synthetic and real-world noisy datasets further demonstrate the superior advantages of our framework over the state-of-the-art baselines.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.834.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, only mentions LLMs as a tool to provide guidance.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, only mentions LLMs as a tool to provide guidance."
    },
    {
        "title": "Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions",
        "authors": [
            "Shulin Cao",
            "Jiajie Zhang",
            "Jiaxin Shi",
            "Xin Lv",
            "Zijun Yao",
            "Qi Tian",
            "Lei Hou",
            "Juanzi Li"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) are capable of answering knowledge-intensive complex questions with chain-of-thought (CoT) reasoning. However, they tend to generate factually incorrect reasoning steps when the required knowledge is not available or up-to-date in models’ parameters. Recent works turn to retrieving external knowledge to augment CoT reasoning. Despite being promising, these chain-based methods suffer from: 1) Negative retrieval. Unnecessary or incorrect retrieval may mislead the reasoning; 2) Limited sight. Lacking the ability to look backward or forward, a local error in one step will propagate along the chain. In this paper, we propose a novel approach: Probabilistic Tree-of-thought Reasoning (ProbTree). First, LLMs translate a complex question into a query tree, in which each non-root node denotes a sub-question of its parent node. Then, probabilistic reasoning is conducted over the tree, by solving questions from leaf to root considering the confidence of both question decomposing and answering. During reasoning, for leaf nodes, LLMs choose a more confident answer from Closed-book QA that employs parametric knowledge and Open-book QA that employs retrieved external knowledge, thus eliminating the negative retrieval problem. For non-leaf nodes, with the hierarchical structure, LLMs have broader sights and are able to globally reason with the information from child nodes, thus recovering from local errors. The experiments on three Complex QA datasets under the open-domain setting show that our approach outperforms SOTA methods significantly, demonstrating the effect of probabilistic tree-of-thought reasoning.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.835.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they tend to generate factually incorrect reasoning steps when the required knowledge is not available or up-to-date in models’ parameters.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, they tend to generate factually incorrect reasoning steps when the required knowledge is not available or up-to-date in models’ parameters.\""
    },
    {
        "title": "Ensemble-Instruct: Instruction Tuning Data Generation with a Heterogeneous Mixture of LMs",
        "authors": [
            "Young-Suk Lee",
            "Md Sultan",
            "Yousef El-Kurdi",
            "Tahira Naseem",
            "Asim Munawar",
            "Radu Florian",
            "Salim Roukos",
            "Ramón Astudillo"
        ],
        "published": "2023",
        "summary": "Using in-context learning (ICL) for data generation, techniques such as Self-Instruct (Wang et al., 2023) or the follow-up Alpaca (Taori et al., 2023) can train strong conversational agents with only a small amount of human supervision. One limitation of these approaches is that they resort to very large language models (around 175B parameters) that are also proprietary and non-public. Here we explore the application of such techniques to language models that are much smaller (around 10B–40B parameters) and have permissive licenses. We find the Self-Instruct approach to be less effective at these sizes and propose new ICL methods that draw on two main ideas: (a) categorization and simplification of the ICL templates to make prompt learning easier for the LM, and (b) ensembling over multiple LM outputs to help select high-quality synthetic examples. Our algorithm leverages the 175 Self-Instruct seed tasks and employs separate pipelines for instructions that require an input and instructions that do not. Empirical investigations with different LMs show that: (1) Our proposed method yields higher-quality instruction tuning data than Self-Instruct, (2) It improves performances of both vanilla and instruction-tuned LMs by significant margins, and (3) Smaller instruction-tuned LMs generate more useful examples than their larger un-tuned counterparts.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.836.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"One limitation of these approaches is that they resort to very large language models (around 175B parameters) that are also proprietary and non-public.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"One limitation of these approaches is that they resort to very large language models (around 175B parameters) that are also proprietary and non-public.\""
    },
    {
        "title": "The Less the Merrier? Investigating Language Representation in Multilingual Models",
        "authors": [
            "Hellina Nigatu",
            "Atnafu Tonja",
            "Jugal Kalita"
        ],
        "published": "2023",
        "summary": "Multilingual Language Models offer a way to incorporate multiple languages in one model and utilize cross-language transfer learning to improve performance for different Natural Language Processing (NLP) tasks. Despite progress in multilingual models, not all languages are supported as well, particularly in low-resource settings. In this work, we investigate the linguistic representation of different languages in multilingual models. We start by asking the question which languages are supported in popular multilingual models and which languages are left behind. Then, for included languages, we look at models’ learned representations based on language family and dialect and try to understand how models’ learned representations for (1) seen and (2) unseen languages vary across different language groups. In addition, we test and analyze performance on downstream tasks such as text generation and Named Entity Recognition. We observe from our experiments that community-centered models—models that focus on languages of a given family or geographical location and are built by communities who speak them—perform better at distinguishing between languages in the same family for low-resource languages. Our paper contributes to the literature in understanding multilingual models and their shortcomings and offers insights on potential ways to improve them.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.837.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite progress in multilingual models, not all languages are supported as well, particularly in low-resource settings.\"; \"Our paper contributes to the literature in understanding multilingual models and their shortcomings\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Despite progress in multilingual models, not all languages are supported as well, particularly in low-resource settings.\"; \"Our paper contributes to the literature in understanding multilingual models and their shortcomings\""
    },
    {
        "title": "Probing Representations for Document-level Event Extraction",
        "authors": [
            "Barry Wang",
            "Xinya Du",
            "Claire Cardie"
        ],
        "published": "2023",
        "summary": "The probing classifiers framework has been employed for interpreting deep neural network models for a variety of natural language processing (NLP) applications. Studies, however, have largely focused on sentencelevel NLP tasks. This work is the first to apply the probing paradigm to representations learned for document-level information extraction (IE). We designed eight embedding probes to analyze surface, semantic, and event-understanding capabilities relevant to document-level event extraction. We apply them to the representations acquired by learning models from three different LLM-based document-level IE approaches on a standard dataset. We found that trained encoders from these models yield embeddings that can modestly improve argument detections and labeling but only slightly enhance event-level tasks, albeit trade-offs in information helpful for coherence and event-type prediction. We further found that encoder models struggle with document length and cross-sentence discourse.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.844.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We found that trained encoders from these models yield embeddings that can modestly improve argument detections and labeling but only slightly enhance event-level tasks, albeit trade-offs in information helpful for coherence and event-type prediction. We further found that encoder models struggle with document length and cross-sentence discourse.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We found that trained encoders from these models yield embeddings that can modestly improve argument detections and labeling but only slightly enhance event-level tasks, albeit trade-offs in information helpful for coherence and event-type prediction. We further found that encoder models struggle with document length and cross-sentence discourse.\""
    },
    {
        "title": "Language-Agnostic Bias Detection in Language Models with Bias Probing",
        "authors": [
            "Abdullatif Köksal",
            "Omer Yalcin",
            "Ahmet Akbiyik",
            "M. Kilavuz",
            "Anna Korhonen",
            "Hinrich Schuetze"
        ],
        "published": "2023",
        "summary": "Pretrained language models (PLMs) are key components in NLP, but they contain strong social biases. Quantifying these biases is challenging because current methods focusing on fill-the-mask objectives are sensitive to slight changes in input. To address this, we propose a bias probing technique called LABDet, for evaluating social bias in PLMs with a robust and language-agnostic method. For nationality as a case study, we show that LABDet “surfaces” nationality bias by training a classifier on top of a frozen PLM on non-nationality sentiment detection. We find consistent patterns of nationality bias across monolingual PLMs in six languages that align with historical and political context. We also show for English BERT that bias surfaced by LABDet correlates well with bias in the pretraining data; thus, our work is one of the few studies that directly links pretraining data to PLM behavior. Finally, we verify LABDet’s reliability and applicability to different templates and languages through an extensive set of robustness checks. We publicly share our code and dataset in https://github.com/akoksal/LABDet.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.848.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Pretrained language models (PLMs) are key components in NLP, but they contain strong social biases.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Pretrained language models (PLMs) are key components in NLP, but they contain strong social biases.\""
    },
    {
        "title": "Improving Multi-Criteria Chinese Word Segmentation through Learning Sentence Representation",
        "authors": [
            "Chun Lin",
            "Ying-Jia Lin",
            "Chia-Jen Yeh",
            "Yi-Ting Li",
            "Ching-Wen Yang",
            "Hung-Yu Kao"
        ],
        "published": "2023",
        "summary": "Recent Chinese word segmentation (CWS) models have shown competitive performance with pre-trained language models’ knowledge. However, these models tend to learn the segmentation knowledge through in-vocabulary words rather than understanding the meaning of the entire context. To address this issue, we introduce a context-aware approach that incorporates unsupervised sentence representation learning over different dropout masks into the multi-criteria training framework. We demonstrate that our approach reaches state-of-the-art (SoTA) performance on F1 scores for six of the nine CWS benchmark datasets and out-of-vocabulary (OOV) recalls for eight of nine. Further experiments discover that substantial improvements can be brought with various sentence representation objectives.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.850.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these models tend to learn the segmentation knowledge through in-vocabulary words rather than understanding the meaning of the entire context.\"\n\nThis abstract mentions a limitation of pre-trained language models in passing, specifically that they tend to learn segmentation knowledge through in-vocabulary words rather than understanding the meaning of the entire context.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these models tend to learn the segmentation knowledge through in-vocabulary words rather than understanding the meaning of the entire context.\"\n\nThis abstract mentions a limitation of pre-trained language models in passing, specifically that they tend to learn segmentation knowledge through in-vocabulary words rather than understanding the meaning of the entire context."
    },
    {
        "title": "Don’t Add, don’t Miss: Effective Content Preserving Generation from Pre-Selected Text Spans",
        "authors": [
            "Aviv Slobodkin",
            "Avi Caciularu",
            "Eran Hirsch",
            "Ido Dagan"
        ],
        "published": "2023",
        "summary": "The recently introduced Controlled Text Reduction (CTR) task isolates the text generation step within typical summarization-style tasks. It does so by challenging models to generate coherent text conforming to pre-selected content within the input text (“highlights”). This framing enables increased modularity in summarization-like tasks, allowing to couple a single CTR model with various content-selection setups and modules. However, there are currently no reliable CTR models, while the performance of the existing baseline for the task is mediocre, falling short of practical utility. Here, we address this gap by introducing a high-quality, open-source CTR model that tackles two prior key limitations: inadequate enforcement of the content-preservation constraint, and suboptimal silver training data. Addressing these, we amplify the content-preservation constraint in both training, via RL, and inference, via a controlled decoding strategy. Further, we substantially improve the silver training data quality via GPT-4 distillation. Overall, pairing the distilled dataset with the highlight-adherence strategies yields marked gains over the current baseline, of up to 30 ROUGE-L points, providing a reliable CTR model for downstream use.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.852.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, there are currently no reliable CTR models, while the performance of the existing baseline for the task is mediocre, falling short of practical utility.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, there are currently no reliable CTR models, while the performance of the existing baseline for the task is mediocre, falling short of practical utility.\""
    },
    {
        "title": "A Computational Interface to Translate Strategic Intent from Unstructured Language in a Low-Data Setting",
        "authors": [
            "Pradyumna Tambwekar",
            "Lakshita Dodeja",
            "Nathan Vaska",
            "Wei Xu",
            "Matthew Gombolay"
        ],
        "published": "2023",
        "summary": "Many real-world tasks involve a mixed-initiative setup, wherein humans and AI systems collaboratively perform a task. While significant work has been conducted towards enabling humans to specify, through language, exactly how an agent should complete a task (i.e., low-level specification), prior work lacks on interpreting the high-level strategic intent of the human commanders. Parsing strategic intent from language will allow autonomous systems to independently operate according to the user’s plan without frequent guidance or instruction. In this paper, we build a computational interface capable of translating unstructured language strategies into actionable intent in the form of goals and constraints. Leveraging a game environment, we collect a dataset of over 1000 examples, mapping language strategies to the corresponding goals and constraints, and show that our model, trained on this dataset, significantly outperforms human interpreters in inferring strategic intent (i.e., goals and constraints) from language (p < 0.05). Furthermore, we show that our model (125M parameters) significantly outperforms ChatGPT for this task (p < 0.05) in a low-data setting.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.853.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we show that our model (125M parameters) significantly outperforms ChatGPT for this task (p < 0.05) in a low-data setting.\"\n\nThis rating is given because the paper mentions a limitation of a specific LLM (ChatGPT) in a low-data setting, but does not explore this limitation in depth and uses it primarily to justify the contribution",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"we show that our model (125M parameters) significantly outperforms ChatGPT for this task (p < 0.05) in a low-data setting.\"\n\nThis rating is given because the paper mentions a limitation of a specific LLM (ChatGPT) in a low-data setting, but does not explore this limitation in depth and uses it primarily to justify the contribution"
    },
    {
        "title": "Misery Loves Complexity: Exploring Linguistic Complexity in the Context of Emotion Detection",
        "authors": [
            "Pranaydeep Singh",
            "Luna De Bruyne",
            "Orphée De Clercq",
            "Els Lefever"
        ],
        "published": "2023",
        "summary": "Given the omnipresence of social media in our society, thoughts and opinions are being shared online in an unprecedented manner. This means that both positive and negative emotions can be equally and freely expressed. However, the negativity bias posits that human beings are inherently drawn to and more moved by negativity and, as a consequence, negative emotions get more traffic. Correspondingly, when writing about emotions this negativity bias could lead to expressions of negative emotions that are linguistically more complex. In this paper, we attempt to use readability and linguistic complexity metrics to better understand the manifestation of emotions on social media platforms like Reddit based on the widely-used GoEmotions dataset. We demonstrate that according to most metrics, negative emotions indeed tend to generate more complex text than positive emotions. In addition, we examine whether a higher complexity hampers the automatic identification of emotions. To answer this question, we fine-tuned three state-of-the-art transformers (BERT, RoBERTa, and SpanBERT) on the same emotion detection dataset. We demonstrate that these models often fail to predict emotions for the more complex texts. More advanced LLMs like RoBERTa and SpanBERT also fail to improve by significant margins on complex samples. This calls for a more nuanced interpretation of the emotion detection performance of transformer models. We make the automatically annotated data available for further research at: https://huggingface.co/datasets/pranaydeeps/CAMEO",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.857.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We demonstrate that these models often fail to predict emotions for the more complex texts. More advanced LLMs like RoBERTa and SpanBERT also fail to improve by significant margins on complex samples.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We demonstrate that these models often fail to predict emotions for the more complex texts. More advanced LLMs like RoBERTa and SpanBERT also fail to improve by significant margins on complex samples.\""
    },
    {
        "title": "Probing the “Creativity” of Large Language Models: Can models produce divergent semantic association?",
        "authors": [
            "Honghua Chen",
            "Nai Ding"
        ],
        "published": "2023",
        "summary": "Large language models possess remarkable capacity for processing language, but it remains unclear whether these models can further generate creative content. The present study aims to investigate the creative thinking of large language models through a cognitive perspective. We utilize the divergent association task (DAT), an objective measurement of creativity that asks models to generate unrelated words and calculates the semantic distance between them. We compare the results across different models and decoding strategies. Our findings indicate that: (1) When using the greedy search strategy, GPT-4 outperforms 96% of humans, while GPT-3.5-turbo exceeds the average human level. (2) Stochastic sampling and temperature scaling are effective to obtain higher DAT scores for models except GPT-4, but face a trade-off between creativity and stability. These results imply that advanced large language models have divergent semantic associations, which is a fundamental process underlying creativity.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.858.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Stochastic sampling and temperature scaling are effective to obtain higher DAT scores for models except GPT-4, but face a trade-off between creativity and stability.\"\n\nThis rating is chosen because the paper mentions a limitation of LLMs in passing, specifically that stochastic sampling and temperature scaling face a trade-off between creativity and stability, but this is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Stochastic sampling and temperature scaling are effective to obtain higher DAT scores for models except GPT-4, but face a trade-off between creativity and stability.\"\n\nThis rating is chosen because the paper mentions a limitation of LLMs in passing, specifically that stochastic sampling and temperature scaling face a trade-off between creativity and stability, but this is not the primary focus of the paper."
    },
    {
        "title": "DiffusionSL: Sequence Labeling via Tag Diffusion Process",
        "authors": [
            "Ziyang Huang",
            "Pengfei Cao",
            "Jun Zhao",
            "Kang Liu"
        ],
        "published": "2023",
        "summary": "Sequence Labeling (SL) is long-standing in Natural Language Processing (NLP). Traditionally, discriminative models have been widely used to capture the conditional distribution of sequence tags, rather than generative models. In this paper, we present DiffusionSL, a framework that utilizes a conditional discrete diffusion model for generating discrete tag data, resulting in a Tag Diffusion Process. We treat the natural language sequence as the conditional signal and the sequence tags as the generation target, iteratively refining the noisy tags to obtain clean ones. To address the discreteness issue, we propose the Bit-Tag Converter (BTConverter) to model the target in continuous data space. Furthermore, we introduce the Bit Diffusion Transformer (BitDiT) to model the process of noise elimination. Leveraging the powerful iterative refinement capability of the diffusion model, DiffusionSL achieves superior performance against previous state-of-the-art (SOTA) baselines and outperforms gpt-3.5-turbo significantly across multiple benchmark datasets and various tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.860.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Leveraging the powerful iterative refinement capability of the diffusion model, DiffusionSL achieves superior performance against previous state-of-the-art (SOTA) baselines and outperforms gpt-3.5-turbo significantly across multiple benchmark datasets and various tasks.\"\n\nThis paper mentions a specific LLM (gpt-3.5-turbo) but does not discuss any",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Leveraging the powerful iterative refinement capability of the diffusion model, DiffusionSL achieves superior performance against previous state-of-the-art (SOTA) baselines and outperforms gpt-3.5-turbo significantly across multiple benchmark datasets and various tasks.\"\n\nThis paper mentions a specific LLM (gpt-3.5-turbo) but does not discuss any"
    },
    {
        "title": "From Words to Wires: Generating Functioning Electronic Devices from Natural Language Descriptions",
        "authors": [
            "Peter Jansen"
        ],
        "published": "2023",
        "summary": "In this work, we show that contemporary language models have a previously unknown skill – the capacity for electronic circuit design from high-level textual descriptions, akin to code generation. We introduce two benchmarks: PINS100, assessing model knowledge of electrical components, and MICRO25, evaluating a model’s capability to design common microcontroller circuits and code in the Arduino ecosystem that involve input, output, sensors, motors, protocols, and logic – with models such as GPT-4 and Claude-V1 achieving between 60% to 96% Pass@1 on generating full devices. We include six case studies of using language models as a design assistant for moderately complex devices, such as a radiation-powered random number generator, an emoji keyboard, a visible spectrometer, and several assistive devices, while offering a qualitative analysis performance, outlining evaluation challenges, and suggesting areas of development to improve complex circuit design and practical utility. With this work, we aim to spur research at the juncture of natural language processing and electronic design.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.864.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"outlining evaluation challenges\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"outlining evaluation challenges\""
    },
    {
        "title": "Roles of Scaling and Instruction Tuning in Language Perception: Model vs. Human Attention",
        "authors": [
            "Changjiang Gao",
            "Shujian Huang",
            "Jixing Li",
            "Jiajun Chen"
        ],
        "published": "2023",
        "summary": "Recent large language models (LLMs) have revealed strong abilities to understand natural language. Since most of them share the same basic structure, i.e. the transformer block, possible contributors to their success in the training process are scaling and instruction tuning. However, how these factors affect the models’ language perception is unclear. This work compares the self-attention of several existing LLMs (LLaMA, Alpaca and Vicuna) in different sizes (7B, 13B, 30B, 65B), together with eye saccade, an aspect of human reading attention, to assess the effect of scaling and instruction tuning on language perception. Results show that scaling enhances the human resemblance and improves the effective attention by reducing the trivial pattern reliance, while instruction tuning does not. However, instruction tuning significantly enhances the models’ sensitivity to instructions. We also find that current LLMs are consistently closer to non-native than native speakers in attention, suggesting a sub-optimal language perception of all models. Our code and data used in the analysis is available on GitHub.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.868.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We also find that current LLMs are consistently closer to non-native than native speakers in attention, suggesting a sub-optimal language perception of all models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We also find that current LLMs are consistently closer to non-native than native speakers in attention, suggesting a sub-optimal language perception of all models.\""
    },
    {
        "title": "Efficient Data Learning for Open Information Extraction with Pre-trained Language Models",
        "authors": [
            "Zhiyuan Fan",
            "Shizhu He"
        ],
        "published": "2023",
        "summary": "Open Information Extraction (OpenIE) is a fundamental yet challenging task in Natural Language Processing, which involves extracting all triples (subject, predicate, object) from a given sentence. While labelling-based methods have their merits, generation-based techniques offer unique advantages, such as the ability to generate tokens not present in the original sentence. However, these generation-based methods often require a significant amount of training data to learn the task form of OpenIE and substantial training time to overcome slow model convergence due to the order penalty. In this paper, we introduce a novel framework, OK-IE, that ingeniously transforms the task form of OpenIE into the pre-training task form of the T5 model, thereby reducing the need for extensive training data. Furthermore, we introduce an innovative concept of ‘anchors’ to control the sequence of model outputs, effectively eliminating the impact of order penalty on model convergence and significantly reducing training time. Experimental results indicate that, compared to previous SOTA methods, OK-IE requires only 1/100 of the training data (900 instances) and 1/120 of the training time (3 minutes) to achieve comparable results.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.869.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these generation-based methods often require a significant amount of training data to learn the task form of OpenIE and substantial training time to overcome slow model convergence due to the order penalty.\"\n\nThis paper mentions a limitation of generation-based methods using pre-trained language models, specifically the need for extensive training data and slow model convergence, but does not explore it in depth.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these generation-based methods often require a significant amount of training data to learn the task form of OpenIE and substantial training time to overcome slow model convergence due to the order penalty.\"\n\nThis paper mentions a limitation of generation-based methods using pre-trained language models, specifically the need for extensive training data and slow model convergence, but does not explore it in depth."
    },
    {
        "title": "Survival of the Most Influential Prompts: Efficient Black-Box Prompt Search via Clustering and Pruning",
        "authors": [
            "Han Zhou",
            "Xingchen Wan",
            "Ivan Vulić",
            "Anna Korhonen"
        ],
        "published": "2023",
        "summary": "Prompt-based learning has been an effective paradigm for large pretrained language models (LLM), enabling few-shot or even zero-shot learning. Black-box prompt search has received growing interest recently for its distinctive properties of gradient-free optimization, proven particularly useful and powerful for model-as-a-service usage. However, the discrete nature and the complexity of combinatorial optimization hinder the efficiency of modern black-box approaches. Despite extensive research on search algorithms, the crucial aspect of search space design and optimization has been largely overlooked. In this paper, we first conduct a sensitivity analysis by prompting LLM, revealing that only a small number of tokens exert a disproportionate amount of influence on LLM predictions. Leveraging this insight, we propose the Clustering and Pruning for Efficient Black-box Prompt Search (ClaPS), a simple black-box search method that first clusters and prunes the search space to focus exclusively on influential prompt tokens. By employing even simple search methods within the pruned search space, ClaPS achieves state-of-the-art performance across various tasks and LLMs, surpassing the performance of complex approaches while significantly reducing search costs. Our findings underscore the critical role of search space design and optimization in enhancing both the usefulness and the efficiency of black-box prompt-based learning.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.870.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the discrete nature and the complexity of combinatorial optimization hinder the efficiency of modern black-box approaches.\"\n\nThis abstract mentions a limitation of LLMs in passing, specifically the inefficiency of black-box approaches due to the discrete nature and complexity of combinatorial optimization. However, it does not elaborate on this limitation and focuses on the proposed solution to address this issue.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the discrete nature and the complexity of combinatorial optimization hinder the efficiency of modern black-box approaches.\"\n\nThis abstract mentions a limitation of LLMs in passing, specifically the inefficiency of black-box approaches due to the discrete nature and complexity of combinatorial optimization. However, it does not elaborate on this limitation and focuses on the proposed solution to address this issue."
    },
    {
        "title": "LLMaAA: Making Large Language Models as Active Annotators",
        "authors": [
            "Ruoyu Zhang",
            "Yanzeng Li",
            "Yongliang Ma",
            "Ming Zhou",
            "Lei Zou"
        ],
        "published": "2023",
        "summary": "Prevalent supervised learning methods in natural language processing (NLP) are notoriously data-hungry, which demand large amounts of high-quality annotated data. In practice, acquiring such data is a costly endeavor. Recently, the superior few-shot performance of large language models (LLMs) has propelled the development of dataset generation, where the training data are solely synthesized from LLMs. However, such an approach usually suffers from low-quality issues, and requires orders of magnitude more labeled data to achieve satisfactory performance. To fully exploit the potential of LLMs and make use of massive unlabeled data, we propose LLMaAA, which takes LLMs as annotators and puts them into an active learning loop to determine what to annotate efficiently. To learn robustly with pseudo labels, we optimize both the annotation and training processes: (1) we draw k-NN examples from a small demonstration pool as in-context examples, and (2) we adopt the example reweighting technique to assign training samples with learnable weights. Compared with previous approaches, LLMaAA features both efficiency and reliability. We conduct experiments and analysis on two classic NLP tasks, named entity recognition and relation extraction. With LLMaAA, task-specific models trained from LLM-generated labels can outperform the teacher within only hundreds of annotated examples, which is much more cost-effective than other baselines.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.872.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, such an approach usually suffers from low-quality issues, and requires orders of magnitude more labeled data to achieve satisfactory performance.\"\n\nThis paper mentions a limitation of LLMs in passing, specifically the low-quality issues with dataset generation and the need for large amounts of labeled data. However, the focus of the paper is on the proposed solution, LLMaAA, rather than exploring",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, such an approach usually suffers from low-quality issues, and requires orders of magnitude more labeled data to achieve satisfactory performance.\"\n\nThis paper mentions a limitation of LLMs in passing, specifically the low-quality issues with dataset generation and the need for large amounts of labeled data. However, the focus of the paper is on the proposed solution, LLMaAA, rather than exploring"
    },
    {
        "title": "NLMs: Augmenting Negation in Language Models",
        "authors": [
            "Rituraj Singh",
            "Rahul Kumar",
            "Vivek Sridhar"
        ],
        "published": "2023",
        "summary": "Negation is the fundamental component in a natural language that reverses the semantic meaning of a sentence. It plays an extremely important role across a wide range of applications, yet they are underrepresented in pre-trained language models (LMs), resulting often in wrong inferences. In this work, we try to improve the underlying understanding of the negation in the pre-trained LMs. To augment negation understanding, we propose a language model objective with a weighted cross-entropy loss and elastic weight consolidation regularization. We reduce the mean top 1 error rate for BERT-base to 1.1%, BERT-large to 0.78%, RoBERTA-base to 3.74%, RoBERTA-large to 0.01% on the negated LAMA dataset. It minimizes the BERT error rate by a margin of 8% and also outperform the existing negation models. We also provide empirical evidences that negated augmented models outperform the classical models on original as well as negation benchmarks on natural language inference tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.873.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Negation is the fundamental component in a natural language that reverses the semantic meaning of a sentence... yet they are underrepresented in pre-trained language models (LMs), resulting often in wrong inferences.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Negation is the fundamental component in a natural language that reverses the semantic meaning of a sentence... yet they are underrepresented in pre-trained language models (LMs), resulting often in wrong inferences.\""
    },
    {
        "title": "X-SNS: Cross-Lingual Transfer Prediction through Sub-Network Similarity",
        "authors": [
            "Taejun Yun",
            "Jinhyeon Kim",
            "Deokyeong Kang",
            "Seonghoon Lim",
            "Jihoon Kim",
            "Taeuk Kim"
        ],
        "published": "2023",
        "summary": "Cross-lingual transfer (XLT) is an emergent ability of multilingual language models that preserves their performance on a task to a significant extent when evaluated in languages that were not included in the fine-tuning process. While English, due to its widespread usage, is typically regarded as the primary language for model adaption in various tasks, recent studies have revealed that the efficacy of XLT can be amplified by selecting the most appropriate source languages based on specific conditions. In this work, we propose the utilization of sub-network similarity between two languages as a proxy for predicting the compatibility of the languages in the context of XLT. Our approach is model-oriented, better reflecting the inner workings of foundation models. In addition, it requires only a moderate amount of raw text from candidate languages, distinguishing it from the majority of previous methods that rely on external resources. In experiments, we demonstrate that our method is more effective than baselines across diverse tasks. Specifically, it shows proficiency in ranking candidates for zero-shot XLT, achieving an improvement of 4.6% on average in terms of NDCG@3. We also provide extensive analyses that confirm the utility of sub-networks for XLT prediction.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.875.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "Towards Concept-Aware Large Language Models",
        "authors": [
            "Chen Shani",
            "Jilles Vreeken",
            "Dafna Shahaf"
        ],
        "published": "2023",
        "summary": "Concepts play a pivotal role in various human cognitive functions, including learning, reasoning and communication. However, there is very little work on endowing machines with the ability to form and reason with concepts. In particular, state-of-the-art large language models (LLMs) work at the level of tokens, not concepts. In this work, we analyze how well contemporary LLMs capture human concepts and their structure. We then discuss ways to develop concept-aware LLMs, taking place at different stages of the pipeline. We sketch a method for pretraining LLMs using concepts, and also explore the simpler approach that uses the output of existing LLMs. Despite its simplicity, our proof-of-concept is shown to better match human intuition, as well as improve the robustness of predictions. These preliminary results underscore the promise of concept-aware LLMs.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.877.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, there is very little work on endowing machines with the ability to form and reason with concepts. In particular, state-of-the-art large language models (LLMs) work at the level of tokens, not concepts.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (working at the level of tokens, not concepts) and discusses it in moderate detail",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, there is very little work on endowing machines with the ability to form and reason with concepts. In particular, state-of-the-art large language models (LLMs) work at the level of tokens, not concepts.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (working at the level of tokens, not concepts) and discusses it in moderate detail"
    },
    {
        "title": "ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning",
        "authors": [
            "Viet Dac Lai",
            "Nghia Ngo",
            "Amir Pouran Ben Veyseh",
            "Hieu Man",
            "Franck Dernoncourt",
            "Trung Bui",
            "Thien Huu Nguyen"
        ],
        "published": "2023",
        "summary": "Over the last few years, large language models (LLMs) have emerged as the most important breakthroughs in natural language processing (NLP) that fundamentally transform research and developments in the field. ChatGPT represents one of the most exciting LLM systems developed recently to showcase impressive skills for language generation and highly attract public attention. Among various exciting applications discovered for ChatGPT in English, the model can process and generate texts for multiple languages due to its multilingual training data. Given the broad adoption of ChatGPT for English in different problems and areas, a natural question is whether ChatGPT can also be applied effectively for other languages or it is necessary to develop more language-specific technologies. The answer to this question requires a thorough evaluation of ChatGPT over multiple tasks with diverse languages and large datasets (i.e., beyond reported anecdotes), which is still missing or limited in current research. Our work aims to fill this gap for the evaluation of ChatGPT and similar LLMs to provide more comprehensive information for multilingual NLP applications. In particular, we evaluate ChatGPT on 7 different tasks, covering 37 diverse languages with high, medium, low, and extremely low resources. Compared to the performance of previous models, our extensive experiments demonstrate the worse performance of ChatGPT for different NLP tasks and languages, calling for further research to develop better models and understanding for multilingual learning.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.878.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Compared to the performance of previous models, our extensive experiments demonstrate the worse performance of ChatGPT for different NLP tasks and languages, calling for further research to develop better models and understanding for multilingual learning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Compared to the performance of previous models, our extensive experiments demonstrate the worse performance of ChatGPT for different NLP tasks and languages, calling for further research to develop better models and understanding for multilingual learning.\""
    },
    {
        "title": "Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training",
        "authors": [
            "Max Müller-Eberstein",
            "Rob van der Goot",
            "Barbara Plank",
            "Ivan Titov"
        ],
        "published": "2023",
        "summary": "Representational spaces learned via language modeling are fundamental to Natural Language Processing (NLP), however there has been limited understanding regarding how and when during training various types of linguistic information emerge and interact. Leveraging a novel information theoretic probing suite, which enables direct comparisons of not just task performance, but their representational subspaces, we analyze nine tasks covering syntax, semantics and reasoning, across 2M pre-training steps and five seeds. We identify critical learning phases across tasks and time, during which subspaces emerge, share information, and later disentangle to specialize. Across these phases, syntactic knowledge is acquired rapidly after 0.5% of full training. Continued performance improvements primarily stem from the acquisition of open-domain knowledge, while semantics and reasoning tasks benefit from later boosts to long-range contextualization and higher specialization. Measuring cross-task similarity further reveals that linguistically related tasks share information throughout training, and do so more during the critical phase of learning than before or after. Our findings have implications for model interpretability, multi-task learning, and learning from limited data.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.879.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Across these phases, syntactic knowledge is acquired rapidly after 0.5% of full training. Continued performance improvements primarily stem from the acquisition of open-domain knowledge, while semantics and reasoning tasks benefit from later boosts to long-range contextualization and higher specialization.\"\n\n(Note: Although the paper does not directly mention limitations of LLMs, it implies that LLMs have limitations in acquiring",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Across these phases, syntactic knowledge is acquired rapidly after 0.5% of full training. Continued performance improvements primarily stem from the acquisition of open-domain knowledge, while semantics and reasoning tasks benefit from later boosts to long-range contextualization and higher specialization.\"\n\n(Note: Although the paper does not directly mention limitations of LLMs, it implies that LLMs have limitations in acquiring"
    },
    {
        "title": "Not All Demonstration Examples are Equally Beneficial: Reweighting Demonstration Examples for In-Context Learning",
        "authors": [
            "Zhe Yang",
            "Damai Dai",
            "Peiyi Wang",
            "Zhifang Sui"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs) have recently gained the In-Context Learning (ICL) ability with the models scaling up, allowing them to quickly adapt to downstream tasks with only a few demonstration examples prepended in the input sequence. Nonetheless, the current practice of ICL treats all demonstration examples equally, which still warrants improvement, as the quality of examples is usually uneven. In this paper, we investigate how to determine approximately optimal weights for demonstration examples and how to apply them during ICL. To assess the quality of weights in the absence of additional validation data, we design a masked self-prediction (MSP) score that exhibits a strong correlation with the final ICL performance. To expedite the weight-searching process, we discretize the continuous weight space and adopt beam search. With approximately optimal weights obtained, we further propose two strategies to apply them to demonstrations at different model positions. Experimental results on 8 text classification tasks show that our approach outperforms conventional ICL by a large margin. Our code are publicly available at https:github.com/Zhe-Young/WICL.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.880.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the quality of examples is usually uneven.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the quality of examples is usually uneven.\""
    },
    {
        "title": "Learn From One Specialized Sub-Teacher: One-to-One Mapping for Feature-Based Knowledge Distillation",
        "authors": [
            "Khouloud Saadi",
            "Jelena Mitrović",
            "Michael Granitzer"
        ],
        "published": "2023",
        "summary": "Knowledge distillation is known as an effective technique for compressing over-parameterized language models. In this work, we propose to break down the global feature distillation task into N local sub-tasks. In this new framework, we consider each neuron in the last hidden layer of the teacher network as a specialized sub-teacher. We also consider each neuron in the last hidden layer of the student network as a focused sub-student. We make each focused sub-student learn from one corresponding specialized sub-teacher and ignore the others. This will facilitate the task for the sub-student and keep it focused. Our proposed method is novel and can be combined with other distillation techniques. Empirical results show that our proposed approach outperforms the state-of-the-art methods by maintaining higher performance on most benchmark datasets. Furthermore, we propose a randomized variant of our approach, called Masked One-to-One Mapping. Rather than learning all the N sub-tasks simultaneously, we focus on learning a subset of these sub-tasks at each optimization step. This variant enables the student to digest the received flow of knowledge more effectively and yields superior results.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.882.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Knowledge distillation is known as an effective technique for compressing over-parameterized language models.\"\n\nThis paper mentions LLMs but does not discuss their limitations explicitly, it only mentions the need for knowledge distillation due to over-parameterization, which is a limitation, but it is not explored further.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Knowledge distillation is known as an effective technique for compressing over-parameterized language models.\"\n\nThis paper mentions LLMs but does not discuss their limitations explicitly, it only mentions the need for knowledge distillation due to over-parameterization, which is a limitation, but it is not explored further."
    },
    {
        "title": "Conditioning on Dialog Acts improves Empathy Style Transfer",
        "authors": [
            "Renyi Qu",
            "Lyle Ungar",
            "João Sedoc"
        ],
        "published": "2023",
        "summary": "We explore the role of dialog acts in style transfer, specifically empathy style transfer – rewriting a sentence to make it more empathetic without changing its meaning. Specifically, we use two novel few-shot prompting strategies: target prompting, which only uses examples of the target style (unlike traditional prompting with source/target pairs), and dialog-act-conditioned prompting, which first estimates the dialog act of the source sentence and then makes it more empathetic using few-shot examples of the same dialog act. Our study yields two key findings: (1) Target prompting typically improves empathy more effectively while maintaining the same level of semantic similarity; (2) Dialog acts matter. Dialog-act-conditioned prompting enhances empathy while preserving both semantics and the dialog-act type. Different dialog acts benefit differently from different prompting methods, highlighting the need for further investigation of the role of dialog acts in style transfer.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.884.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs."
    },
    {
        "title": "Systematic Assessment of Factual Knowledge in Large Language Models",
        "authors": [
            "Linhao Luo",
            "Trang Vu",
            "Dinh Phung",
            "Reza Haf"
        ],
        "published": "2023",
        "summary": "Previous studies have relied on existing question-answering benchmarks to evaluate the knowledge stored in large language models (LLMs). However, this approach has limitations regarding factual knowledge coverage, as it mostly focuses on generic domains which may overlap with the pretraining data. This paper proposes a framework to systematically assess the factual knowledge of LLMs by leveraging knowledge graphs (KGs). Our framework automatically generates a set of questions and expected answers from the facts stored in a given KG, and then evaluates the accuracy of LLMs in answering these questions. We systematically evaluate the state-of-the-art LLMs with KGs in generic and specific domains. The experiment shows that ChatGPT is consistently the top performer across all domains. We also find that LLMs performance depends on the instruction finetuning, domain and question complexity and is prone to adversarial context.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.885.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, this approach has limitations regarding factual knowledge coverage, as it mostly focuses on generic domains which may overlap with the pretraining data.\"; \"We also find that LLMs performance depends on the instruction finetuning, domain and question complexity and is prone to adversarial context.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, this approach has limitations regarding factual knowledge coverage, as it mostly focuses on generic domains which may overlap with the pretraining data.\"; \"We also find that LLMs performance depends on the instruction finetuning, domain and question complexity and is prone to adversarial context.\""
    },
    {
        "title": "From Speculation Detection to Trustworthy Relational Tuples in Information Extraction",
        "authors": [
            "Kuicai Dong",
            "Aixin Sun",
            "Jung-jae Kim",
            "Xiaoli Li"
        ],
        "published": "2023",
        "summary": "Speculation detection is an important NLP task to identify text factuality. However, the extracted speculative information (e.g., speculative polarity, cue, and scope) lacks structure and poses challenges for direct utilization in downstream tasks. Open Information Extraction (OIE), on the other hand, extracts structured tuples as facts, without examining the certainty of these tuples. Bridging this gap between speculation detection and information extraction becomes imperative to generate structured speculative information and trustworthy relational tuples. Existing studies on speculation detection are defined at sentence level; but even if a sentence is determined to be speculative, not all factual tuples extracted from it are speculative. In this paper, we propose to study speculations in OIE tuples and determine whether a tuple is speculative. We formally define the research problem of tuple-level speculation detection. We then conduct detailed analysis on the LSOIE dataset which provides labels for speculative tuples. Lastly, we propose a baseline model SpecTup for this new research task.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.886.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Tokenization Consistency Matters for Generative Models on Extractive NLP Tasks",
        "authors": [
            "Kaiser Sun",
            "Peng Qi",
            "Yuhao Zhang",
            "Lan Liu",
            "William Wang",
            "Zhiheng Huang"
        ],
        "published": "2023",
        "summary": "Generative models have been widely applied to solve extractive tasks, where parts of the input is extracted to form the desired output, and achieved significant success. For example, in extractive question answering (QA), generative models have constantly yielded state-of-the-art results. In this work, we study the issue of tokenization inconsistency that is commonly neglected in training these models. This issue damages the extractive nature of these tasks after the input and output are tokenized inconsistently by the tokenizer, and thus leads to performance drop as well as hallucination. We propose a simple yet effective fix to this issue and conduct a case study on extractive QA. We show that, with consistent tokenization, the model performs better in both in-domain and out-of-domain datasets, with a notable average of +1.7 F1 gain when a BART model is trained on SQuAD and evaluated on 8 QA datasets. Further, the model converges faster, and becomes less likely to generate out-of-context answers. Our results demonstrate the need for increased scrutiny regarding how tokenization is done in extractive tasks and the benefits of consistent tokenization during training.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.887.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This issue damages the extractive nature of these tasks after the input and output are tokenized inconsistently by the tokenizer, and thus leads to performance drop as well as hallucination.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"This issue damages the extractive nature of these tasks after the input and output are tokenized inconsistently by the tokenizer, and thus leads to performance drop as well as hallucination.\""
    },
    {
        "title": "LogicAttack: Adversarial Attacks for Evaluating Logical Consistency of Natural Language Inference",
        "authors": [
            "Mutsumi Nakamura",
            "Santosh Mashetty",
            "Mihir Parmar",
            "Neeraj Varshney",
            "Chitta Baral"
        ],
        "published": "2023",
        "summary": "Recently Large Language Models (LLMs) such as GPT-3, ChatGPT, and FLAN have led to impressive progress in Natural Language Inference (NLI) tasks. However, these models may rely on simple heuristics or artifacts in the evaluation data to achieve their high performance, which suggests that they still suffer from logical inconsistency. To assess the logical consistency of these models, we propose a LogicAttack, a method to attack NLI models using diverse logical forms of premise and hypothesis, providing a more robust evaluation of their performance. Our approach leverages a range of inference rules from propositional logic, such as Modus Tollens and Bidirectional Dilemma, to generate effective adversarial attacks and identify common vulnerabilities across multiple NLI models. We achieve an average ~53% Attack Success Rate (ASR) across multiple logic-based attacks. Moreover, we demonstrate that incorporating generated attack samples into training enhances the logical reasoning ability of the target model and decreases its vulnerability to logic-based attacks. Data and source code are available at https://github.com/msantoshmadhav/LogicAttack.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.889.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these models may rely on simple heuristics or artifacts in the evaluation data to achieve their high performance, which suggests that they still suffer from logical inconsistency.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, these models may rely on simple heuristics or artifacts in the evaluation data to achieve their high performance, which suggests that they still suffer from logical inconsistency.\""
    },
    {
        "title": "SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting",
        "authors": [
            "Xiaoying Zhang",
            "Baolin Peng",
            "Kun Li",
            "Jingyan Zhou",
            "Helen Meng"
        ],
        "published": "2023",
        "summary": "Building and maintaining end-to-end task bots using minimal human effort is a long-standing challenge in dialog research. In this work, we introduce SGP-TOD, Schema-Guided Prompting for building Task-Oriented Dialog systems effortlessly based on large language models (LLMs). Utilizing the predefined task schema, i.e., belief instruction and dialog policy, we instruct fixed LLMs to generate appropriate responses on novel tasks, without the need for training data. Specifically, SGP-TOD comprises three components: an LLM for interacting with users, a Dialog State Tracking (DST) Prompter to aid the LLM in tracking dialog states with the given belief instruction, and a Policy Prompter to direct the LLM to generate proper responses adhering to the provided dialog policy. Experimental results on Multiwoz, RADDLE, and STAR datasets show that our training-free strategy, SGP-TOD, yields state-of-the-art (SOTA) zero-shot performance, significantly surpassing the few-shot approaches. In a domain-extension setting, SGP-TOD aptly adapts to new functionalities by merely adding supplementary schema rules. We make our code and data publicly available.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.891.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, only utilization of LLMs for building Task-Oriented Dialog systems.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, only utilization of LLMs for building Task-Oriented Dialog systems."
    },
    {
        "title": "Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context Ethical Policies in LLMs",
        "authors": [
            "Abhinav Sukumar Rao",
            "Aditi Khandelwal",
            "Kumar Tanmay",
            "Utkarsh Agarwal",
            "Monojit Choudhury"
        ],
        "published": "2023",
        "summary": "In this position paper, we argue that instead of morally aligning LLMs to specific set of ethical principles, we should infuse generic ethical reasoning capabilities into them so that they can handle value pluralism at a global scale. When provided with an ethical policy, an LLM should be capable of making decisions that are ethically consistent to the policy. We develop a framework that integrates moral dilemmas with moral principles pertaining to different foramlisms of normative ethics, and at different levels of abstractions. Initial experiments with GPT-x models shows that while GPT-4 is a nearly perfect ethical reasoner, the models still have bias towards the moral values of Western and English speaking societies.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.892.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Initial experiments with GPT-x models shows that while GPT-4 is a nearly perfect ethical reasoner, the models still have bias towards the moral values of Western and English speaking societies.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Initial experiments with GPT-x models shows that while GPT-4 is a nearly perfect ethical reasoner, the models still have bias towards the moral values of Western and English speaking societies.\""
    },
    {
        "title": "Rethinking the Construction of Effective Metrics for Understanding the Mechanisms of Pretrained Language Models",
        "authors": [
            "You Li",
            "Jinhui Yin",
            "Yuming Lin"
        ],
        "published": "2023",
        "summary": "Pretrained language models are expected to effectively map input text to a set of vectors while preserving the inherent relationships within the text. Consequently, designing a white-box model to compute metrics that reflect the presence of specific internal relations in these vectors has become a common approach for post-hoc interpretability analysis of pretrained language models. However, achieving interpretability in white-box models and ensuring the rigor of metric computation becomes challenging when the source model lacks inherent interpretability. Therefore, in this paper, we discuss striking a balance in this trade-off and propose a novel line to constructing metrics for understanding the mechanisms of pretrained language models. We have specifically designed a family of metrics along this line of investigation, and the model used to compute these metrics is referred to as the tree topological probe. We conducted measurements on BERT-large by using these metrics. Based on the experimental results, we propose a speculation regarding the working mechanism of BERT-like pretrained language models, as well as a strategy for enhancing fine-tuning performance by leveraging the topological probe to improve specific submodules.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.894.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, achieving interpretability in white-box models and ensuring the rigor of metric computation becomes challenging when the source model lacks inherent interpretability.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, achieving interpretability in white-box models and ensuring the rigor of metric computation becomes challenging when the source model lacks inherent interpretability.\""
    },
    {
        "title": "Synthesize, if you do not have: Effective Synthetic Dataset Creation Strategies for Self-Supervised Opinion Summarization in E-commerce",
        "authors": [
            "Tejpalsingh Siledar",
            "Suman Banerjee",
            "Amey Patil",
            "Sudhanshu Singh",
            "Muthusamy Chelliah",
            "Nikesh Garera",
            "Pushpak Bhattacharyya"
        ],
        "published": "2023",
        "summary": "In e-commerce, opinion summarization is the process of condensing the opinions presented in product reviews. However, the absence of large amounts of supervised datasets presents challenges in generating both aspect-specific and general opinion summaries. Existing approaches have attempted to address these challenges through synthetic dataset creation (SDC). However, general opinion summarization models struggle to generate summaries faithful to the input reviews whereas aspect-specific opinion summarization models are limited due to their reliance on human-specified aspects and seed words. To address this, we propose SDC strategies tailored for general and aspect-specific opinion summarization. We experimented on three e-commerce test sets: Oposum+, Amazon, and Flipkart. For general opinion summarization, pre-trained language model (PLM) fine-tuned on our general synthetic dataset surpass the SOTA on average by 2.3 R1 points. Faithfulness evaluation metrics and human evaluations indicate that our model-generated summaries are more faithful to the input compared to others. For aspect-specific opinion summarization, PLM fine-tuned on our aspect-specific synthetic dataset surpass SOTA by ~ 1 R1 point without the aid of any human-specified aspects or seed words.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.899.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"general opinion summarization models struggle to generate summaries faithful to the input reviews whereas aspect-specific opinion summarization models are limited due to their reliance on human-specified aspects and seed words.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"general opinion summarization models struggle to generate summaries faithful to the input reviews whereas aspect-specific opinion summarization models are limited due to their reliance on human-specified aspects and seed words.\""
    },
    {
        "title": "Beyond Testers’ Biases: Guiding Model Testing with Knowledge Bases using LLMs",
        "authors": [
            "Chenyang Yang",
            "Rishabh Rustogi",
            "Rachel Brower-Sinning",
            "Grace Lewis",
            "Christian Kaestner",
            "Tongshuang Wu"
        ],
        "published": "2023",
        "summary": "Current model testing work has mostly focused on creating test cases. Identifying what to test is a step that is largely ignored and poorly supported. We propose Weaver, an interactive tool that supports requirements elicitation for guiding model testing. Weaver uses large language models to generate knowledge bases and recommends concepts from them interactively, allowing testers to elicit requirements for further testing. Weaver provides rich external knowledge to testers and encourages testers to systematically explore diverse concepts beyond their own biases. In a user study, we show that both NLP experts and non-experts identified more, as well as more diverse concepts worth testing when using Weaver. Collectively, they found more than 200 failing test cases for stance detection with zero-shot ChatGPT. Our case studies further show that Weaver can help practitioners test models in real-world settings, where developers define more nuanced application scenarios (e.g., code understanding and transcript summarization) using LLMs.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.901.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but the abstract implies that testers may have biases that LLMs can help mitigate, however, it does not discuss any limitations of LLMs themselves.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None explicitly mentioned, but the abstract implies that testers may have biases that LLMs can help mitigate, however, it does not discuss any limitations of LLMs themselves."
    },
    {
        "title": "CAR: Conceptualization-Augmented Reasoner for Zero-Shot Commonsense Question Answering",
        "authors": [
            "Weiqi Wang",
            "Tianqing Fang",
            "Wenxuan Ding",
            "Baixuan Xu",
            "Xin Liu",
            "Yangqiu Song",
            "Antoine Bosselut"
        ],
        "published": "2023",
        "summary": "The task of zero-shot commonsense question answering evaluates models on their capacity to reason about general scenarios beyond those presented in specific datasets. Existing approaches for tackling this task leverage external knowledge from CommonSense Knowledge Bases (CSKBs) by pre-training the model on synthetic QA pairs constructed from CSKBs. In these approaches, negative examples (distractors) are formulated by randomly sampling from CSKBs using fairly primitive keyword constraints. However, two bottlenecks limit these approaches: the inherent incompleteness of CSKBs limits the semantic coverage of synthetic QA pairs, and the lack of human annotations makes the sampled negative examples potentially uninformative and contradictory. To tackle these limitations above, we propose Conceptualization-Augmented Reasoner (CAR), a zero-shot commonsense question-answering framework that fully leverages the power of conceptualization. Specifically, CAR abstracts a commonsense knowledge triple to many higher-level instances, which increases the coverage of the CSKB and expands the ground-truth answer space, reducing the likelihood of selecting false negative distractors. Extensive experiments demonstrate that CAR more robustly generalizes to answering questions about zero-shot commonsense scenarios than existing methods, including large language models, such as GPT3.5 and ChatGPT. Our code, data, and model checkpoints are available at https://github.com/HKUST-KnowComp/CAR.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.902.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"including large language models, such as GPT3.5 and ChatGPT\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"including large language models, such as GPT3.5 and ChatGPT\""
    },
    {
        "title": "InstOptima: Evolutionary Multi-objective Instruction Optimization via Large Language Model-based Instruction Operators",
        "authors": [
            "Heng Yang",
            "Ke Li"
        ],
        "published": "2023",
        "summary": "Instruction-based language modeling has received significant attention in pretrained language models. However, the efficiency of instruction engineering remains low and hinders the development of instruction studies. Recent studies have focused on automating instruction generation, but they primarily aim to improve performance without considering other crucial objectives that impact instruction quality, such as instruction length and perplexity. Therefore, we propose a novel approach (i.e., InstOptima) that treats instruction generation as an evolutionary multi-objective optimization problem. In contrast to text edition-based methods, our approach utilizes a large language model (LLM) to simulate instruction operators, including mutation and crossover. Furthermore, we introduce an objective-guided mechanism for these operators, allowing the LLM to comprehend the objectives and enhance the quality of the generated instructions. Experimental results demonstrate improved fine-tuning performance and the generation of a diverse set of high-quality instructions.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.907.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the abstract mentions that \"the efficiency of instruction engineering remains low\" which could be related to LLMs, however, it does not explicitly discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the abstract mentions that \"the efficiency of instruction engineering remains low\" which could be related to LLMs, however, it does not explicitly discuss any limitations of LLMs."
    },
    {
        "title": "Less than One-shot: Named Entity Recognition via Extremely Weak Supervision",
        "authors": [
            "Letian Peng",
            "Zihan Wang",
            "Jingbo Shang"
        ],
        "published": "2023",
        "summary": "We study the named entity recognition (NER) problem under the extremely weak supervision (XWS) setting, where only one example entity per type is given in a context-free way. While one can see that XWS is lighter than one-shot in terms of the amount of supervision, we propose a novel method X-NER that can outperform the state-of-the-art one-shot NER methods. We first mine entity spans that are similar to the example entities from an unlabelled training corpus. Instead of utilizing entity span representations from language models, we find it more effective to compare the context distributions before and after the span is replaced by the entity example. We then leverage the top-ranked spans as pseudo-labels to train an NER tagger. Extensive experiments and analyses on 4 NER datasets show the superior end-to-end NER performance of X-NER, outperforming the state-of-the-art few-shot methods with 1-shot supervision and ChatGPT annotations significantly. Finally, our X-NER possesses several notable properties, such as inheriting the cross-lingual abilities of the underlying language models.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.908.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Instead of utilizing entity span representations from language models, we find it more effective to compare the context distributions before and after the span is replaced by the entity example.\"\n\nThis rating is given because the abstract mentions a limitation of using entity span representations from language models, but does not elaborate on it and focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Instead of utilizing entity span representations from language models, we find it more effective to compare the context distributions before and after the span is replaced by the entity example.\"\n\nThis rating is given because the abstract mentions a limitation of using entity span representations from language models, but does not elaborate on it and focuses on the proposed solution."
    },
    {
        "title": "Time-Aware Language Modeling for Historical Text Dating",
        "authors": [
            "Han Ren",
            "Hai Wang",
            "Yajie Zhao",
            "Yafeng Ren"
        ],
        "published": "2023",
        "summary": "Automatic text dating(ATD) is a challenging task since explicit temporal mentions usually do not appear in texts. Existing state-of-the-art approaches learn word representations via language models, whereas most of them ignore diachronic change of words, which may affect the efforts of text modeling. Meanwhile, few of them consider text modeling for long diachronic documents. In this paper, we present a time-aware language model named TALM, to learn temporal word representations by transferring language models of general domains to those of time-specific ones. We also build a hierarchical modeling approach to represent diachronic documents by encoding them with temporal word representations. Experiments on a Chinese diachronic corpus show that our model effectively captures implicit temporal information of words, and outperforms state-of-the-art approaches in historical text dating as well.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.911.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing state-of-the-art approaches... ignore diachronic change of words, which may affect the efforts of text modeling.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing state-of-the-art approaches... ignore diachronic change of words, which may affect the efforts of text modeling.\""
    },
    {
        "title": "HANSEN: Human and AI Spoken Text Benchmark for Authorship Analysis",
        "authors": [
            "Nafis Tripto",
            "Adaku Uchendu",
            "Thai Le",
            "Mattia Setzu",
            "Fosca Giannotti",
            "Dongwon Lee"
        ],
        "published": "2023",
        "summary": "Authorship Analysis, also known as stylometry, has been an essential aspect of Natural Language Processing (NLP) for a long time. Likewise, the recent advancement of Large Language Models (LLMs) has made authorship analysis increasingly crucial for distinguishing between human-written and AI-generated texts. However, these authorship analysis tasks have primarily been focused on written texts, not considering spoken texts. Thus, we introduce the largest benchmark for spoken texts - HANSEN( ̲Human  ̲ANd ai  ̲Spoken t ̲Ext be ̲Nchmark). HANSEN encompasses meticulous curation of existing speech datasets accompanied by transcripts, alongside the creation of novel AI-generated spoken text datasets. Together, it comprises 17 human datasets, and AI-generated spoken texts created using 3 prominent LLMs: ChatGPT, PaLM2, and Vicuna13B. To evaluate and demonstrate the utility of HANSEN, we perform Authorship Attribution (AA) & Author Verification (AV) on human-spoken datasets and conducted Human vs. AI text detection using state-of-the-art (SOTA) models. While SOTA methods, such as, character n-gram or Transformer-based model, exhibit similar AA & AV performance in human-spoken datasets compared to written ones, there is much room for improvement in AI-generated spoken text detection. The HANSEN benchmark is available at: https://huggingface.co/datasets/HANSEN-REPO/HANSEN",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.916.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While SOTA methods... exhibit similar AA & AV performance in human-spoken datasets compared to written ones, there is much room for improvement in AI-generated spoken text detection.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs in the context of authorship analysis, specifically in detecting AI-generated spoken texts, but it does not elaborate on this limitation in detail and focuses",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While SOTA methods... exhibit similar AA & AV performance in human-spoken datasets compared to written ones, there is much room for improvement in AI-generated spoken text detection.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs in the context of authorship analysis, specifically in detecting AI-generated spoken texts, but it does not elaborate on this limitation in detail and focuses"
    },
    {
        "title": "Enhancing Scalability of Pre-trained Language Models via Efficient Parameter Sharing",
        "authors": [
            "Peiyu Liu",
            "Ze-Feng Gao",
            "Yushuo Chen",
            "Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2023",
        "summary": "In this paper, we propose a highly parameter-efficient approach to scaling pre-trained language models (PLMs) to a deeper model depth. Unlike prior work that shares all parameters or uses extra blocks, we design a more capable parameter-sharing architecture based on matrix product operator (MPO), an efficient tensor decomposition method to factorize the parameter matrix into a set of local tensors. Based on such a decomposition, we share the important local tensor across all layers for reducing the model size and meanwhile keep layer-specific tensors (also using Adapters) for enhancing the adaptation flexibility. To improve the model training, we further propose a stable initialization algorithm tailored for the MPO-based architecture. Extensive experiments have demonstrated the effectiveness of our proposed model in enhancing scalability and achieving higher performance (i.e., with fewer parameters than BERT-base, we successfully scale the model depth by a factor of 4x and even achieve 0.1 points higher than BERT-large for GLUE score). The code to reproduce the results of this paper can be found at https://github.com/RUCAIBox/MPOBERT-code.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.920.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unlike prior work that shares all parameters or uses extra blocks, we design a more capable parameter-sharing architecture...\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Unlike prior work that shares all parameters or uses extra blocks, we design a more capable parameter-sharing architecture...\""
    },
    {
        "title": "On the Impact of Cross-Domain Data on German Language Models",
        "authors": [
            "Amin Dada",
            "Aokun Chen",
            "Cheng Peng",
            "Kaleb Smith",
            "Ahmad Idrissi-Yaghir",
            "Constantin Seibold",
            "Jianning Li",
            "Lars Heiliger",
            "Christoph Friedrich",
            "Daniel Truhn",
            "Jan Egger",
            "Jiang Bian",
            "Jens Kleesiek",
            "Yonghui Wu"
        ],
        "published": "2023",
        "summary": "Traditionally, large language models have been either trained on general web crawls or domain-specific data. However, recent successes of generative large language models, have shed light on the benefits of cross-domain datasets. To examine the significance of prioritizing data diversity over quality, we present a German dataset comprising texts from five domains, along with another dataset aimed at containing high-quality data. Through training a series of models ranging between 122M and 750M parameters on both datasets, we conduct a comprehensive benchmark on multiple downstream tasks. Our findings demonstrate that the models trained on the cross-domain dataset outperform those trained on quality data alone, leading to improvements up to 4.45% over the previous state-of-the-art.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.922.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper discusses the benefits of cross-domain datasets for training LLMs, implying that traditional training methods may have limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper discusses the benefits of cross-domain datasets for training LLMs, implying that traditional training methods may have limitations."
    },
    {
        "title": "A Parallel Corpus for Vietnamese Central-Northern Dialect Text Transfer",
        "authors": [
            "Thang Le",
            "Anh Luu"
        ],
        "published": "2023",
        "summary": "The Vietnamese language embodies dialectal variants closely attached to the nation’s three macro-regions: the Northern, Central and Southern regions. As the northern dialect forms the basis of the standard language, it’s considered the prestige dialect. While the northern dialect differs from the remaining two in certain aspects, it almost shares an identical lexicon with the southern dialect, making the textual attributes nearly interchangeable. In contrast, the central dialect possesses a number of unique vocabularies and is less mutually intelligible to the standard dialect. Through preliminary experiments, we observe that current NLP models do not possess understandings of the Vietnamese central dialect text, which most likely originates from the lack of resources. To facilitate research on this domain, we introduce a new parallel corpus for Vietnamese central-northern dialect text transfer. Via exhaustive benchmarking, we discover monolingual language models’ superiority over their multilingual counterparts on the dialect transfer task. We further demonstrate that fine-tuned transfer models can seamlessly improve the performance of existing NLP systems on the central dialect domain with dedicated results in translation and text-image retrieval tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.925.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Through preliminary experiments, we observe that current NLP models do not possess understandings of the Vietnamese central dialect text, which most likely originates from the lack of resources.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Through preliminary experiments, we observe that current NLP models do not possess understandings of the Vietnamese central dialect text, which most likely originates from the lack of resources.\""
    },
    {
        "title": "A Comprehensive Evaluation of Tool-Assisted Generation Strategies",
        "authors": [
            "Alon Jacovi",
            "Avi Caciularu",
            "Jonathan Herzig",
            "Roee Aharoni",
            "Bernd Bohnet",
            "Mor Geva"
        ],
        "published": "2023",
        "summary": "A growing area of research investigates augmenting language models with tools (e.g., search engines, calculators) to overcome their shortcomings (e.g., missing or incorrect knowledge, incorrect logical inferences). Various few-shot tool-usage strategies have been proposed. However, there is no systematic and fair comparison across different strategies, or between these strategies and strong baselines that do not leverage tools. We conduct an extensive empirical analysis, finding that (1) across various datasets, example difficulty levels, and models, strong no-tool baselines are competitive to tool-assisted strategies, implying that effectively using tools with in-context demonstrations is a difficult unsolved problem; (2) for knowledge-retrieval tasks, strategies that *refine* incorrect outputs with tools outperform strategies that retrieve relevant information *ahead of* or *during generation*; (3) tool-assisted strategies are expensive in the number of tokens they require to work—incurring additional costs by orders of magnitude—which does not translate into significant improvement in performance. Overall, our findings suggest that few-shot tool integration is still an open challenge, emphasizing the need for comprehensive evaluations of future strategies to accurately assess their *benefits* and *costs*.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.926.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A growing area of research investigates augmenting language models with tools (e.g., search engines, calculators) to overcome their shortcomings (e.g., missing or incorrect knowledge, incorrect logical inferences)\"; \"Overall, our findings suggest that few-shot tool integration is still an open challenge, emphasizing the need for comprehensive evaluations of future strategies to accurately assess their *benefits* and",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"A growing area of research investigates augmenting language models with tools (e.g., search engines, calculators) to overcome their shortcomings (e.g., missing or incorrect knowledge, incorrect logical inferences)\"; \"Overall, our findings suggest that few-shot tool integration is still an open challenge, emphasizing the need for comprehensive evaluations of future strategies to accurately assess their *benefits* and"
    },
    {
        "title": "InheritSumm: A General, Versatile and Compact Summarizer by Distilling from GPT",
        "authors": [
            "Yichong Xu",
            "Ruochen Xu",
            "Dan Iter",
            "Yang Liu",
            "Shuohang Wang",
            "Chenguang Zhu",
            "Michael Zeng"
        ],
        "published": "2023",
        "summary": "While large models such as GPT-3 demonstrate exceptional performance in zeroshot and fewshot summarization tasks, their extensive serving and fine-tuning costs hinder their utilization in various applications. Conversely, previous studies have found that although automatic metrics tend to favor smaller fine-tuned models, the quality of the summaries they generate is inferior to that of larger models like GPT-3 when assessed by human evaluators. To address this issue, we propose InheritSumm, a versatile and compact summarization model derived from GPT-3.5 through distillation. InheritSumm not only exhibits comparable zeroshot and fewshot summarization capabilities to GPT-3.5 but is also sufficiently compact for fine-tuning purposes. Experimental results demonstrate that InheritSumm achieves similar or superior performance to GPT-3.5 in zeroshot and fewshot settings. Furthermore, it outperforms the previously established best small models in both prefix-tuning and full-data fine-tuning scenarios.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.927.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While large models such as GPT-3 demonstrate exceptional performance in zeroshot and fewshot summarization tasks, their extensive serving and fine-tuning costs hinder their utilization in various applications.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While large models such as GPT-3 demonstrate exceptional performance in zeroshot and fewshot summarization tasks, their extensive serving and fine-tuning costs hinder their utilization in various applications.\""
    },
    {
        "title": "Can ChatGPT Perform Reasoning Using the IRAC Method in Analyzing Legal Scenarios Like a Lawyer?",
        "authors": [
            "Xiaoxi Kang",
            "Lizhen Qu",
            "Lay-Ki Soon",
            "Adnan Trakic",
            "Terry Zhuo",
            "Patrick Emerton",
            "Genevieve Grant"
        ],
        "published": "2023",
        "summary": "Large Language Models (LLMs), such as ChatGPT, have drawn a lot of attentions recently in the legal domain due to its emergent ability to tackle a variety of legal tasks. However, it is still unknown if LLMs are able to analyze a legal case and perform reasoning in the same manner as lawyers. Therefore, we constructed a novel corpus consisting of scenarios pertain to Contract Acts Malaysia and Australian Social Act for Dependent Child. ChatGPT is applied to perform analysis on the corpus using the IRAC method, which is a framework widely used by legal professionals for organizing legal analysis. Each scenario in the corpus is annotated with a complete IRAC analysis in a semi-structured format so that both machines and legal professionals are able to interpret and understand the annotations. In addition, we conducted the first empirical assessment of ChatGPT for IRAC analysis in order to understand how well it aligns with the analysis of legal professionals. Our experimental results shed lights on possible future research directions to improve alignments between LLMs and legal experts in terms of legal reasoning.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.929.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it is still unknown if LLMs are able to analyze a legal case and perform reasoning in the same manner as lawyers.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, it is still unknown if LLMs are able to analyze a legal case and perform reasoning in the same manner as lawyers.\""
    },
    {
        "title": "Coverage-based Example Selection for In-Context Learning",
        "authors": [
            "Shivanshu Gupta",
            "Matt Gardner",
            "Sameer Singh"
        ],
        "published": "2023",
        "summary": "In-context learning (ICL), the ability of large language models to perform novel tasks by conditioning on a prompt with a few task examples, requires these examples to be informative about the test instance. The standard approach of independently ranking and selecting the most similar examples selects redundant examples while omitting important information. In this work, we show that BERTScore-Recall (BSR) selects better examples that demonstrate more of the salient aspects, e.g. reasoning patterns, of the test input. We further extend BSR and many standard metrics to easily optimizable set-level metrics, giving still better coverage of those salient aspects. On 15 datasets spanning 6 tasks and with 7 diverse LLMs, we show that (1) BSR is the superior metric for in-context example selection across the board, and (2) for compositional tasks, set selection using Set-BSR outperforms independent ranking by up to 17 points on average and, despite being training-free, surpasses methods that leverage task or LLM-specific training.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.930.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The standard approach of independently ranking and selecting the most similar examples selects redundant examples while omitting important information.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The standard approach of independently ranking and selecting the most similar examples selects redundant examples while omitting important information.\""
    },
    {
        "title": "Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization",
        "authors": [
            "Ningyu Xu",
            "Qi Zhang",
            "Jingting Ye",
            "Menghan Zhang",
            "Xuanjing Huang"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have exhibited considerable cross-lingual generalization abilities, whereby they implicitly transfer knowledge across languages. However, the transfer is not equally successful for all languages, especially for low-resource ones, which poses an ongoing challenge. It is unclear whether we have reached the limits of implicit cross-lingual generalization and if explicit knowledge transfer is viable. In this paper, we investigate the potential for explicitly aligning conceptual correspondence between languages to enhance cross-lingual generalization. Using the syntactic aspect of language as a testbed, our analyses of 43 languages reveal a high degree of alignability among the spaces of structural concepts within each language for both encoder-only and decoder-only LLMs. We then propose a meta-learning-based method to learn to align conceptual spaces of different languages, which facilitates zero-shot and few-shot generalization in concept classification and also offers insights into the cross-lingual in-context learning phenomenon. Experiments on syntactic analysis tasks show that our approach achieves competitive results with state-of-the-art methods and narrows the performance gap between languages, particularly benefiting those with limited resources.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.931.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the transfer is not equally successful for all languages, especially for low-resource ones, which poses an ongoing challenge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the transfer is not equally successful for all languages, especially for low-resource ones, which poses an ongoing challenge.\""
    },
    {
        "title": "Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing",
        "authors": [
            "Lucie-Aimée Kaffee",
            "Arnav Arora",
            "Zeerak Talat",
            "Isabelle Augenstein"
        ],
        "published": "2023",
        "summary": "Dual use, the intentional, harmful reuse of technology and scientific artefacts, is an ill-defined problem within the context of Natural Language Processing (NLP). As large language models (LLMs) have advanced in their capabilities and become more accessible, the risk of their intentional misuse becomes more prevalent. To prevent such intentional malicious use, it is necessary for NLP researchers and practitioners to understand and mitigate the risks of their research. Hence, we present an NLP-specific definition of dual use informed by researchers and practitioners in the field. Further, we propose a checklist focusing on dual-use in NLP, that can be integrated into existing conference ethics-frameworks. The definition and checklist are created based on a survey of NLP researchers and practitioners.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.932.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As large language models (LLMs) have advanced in their capabilities and become more accessible, the risk of their intentional misuse becomes more prevalent.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"As large language models (LLMs) have advanced in their capabilities and become more accessible, the risk of their intentional misuse becomes more prevalent.\""
    },
    {
        "title": "BYOC: Personalized Few-Shot Classification with Co-Authored Class Descriptions",
        "authors": [
            "Arth Bohra",
            "Govert Verkes",
            "Artem Harutyunyan",
            "Pascal Weinberger",
            "Giovanni Campagna"
        ],
        "published": "2023",
        "summary": "Text classification is a well-studied and versatile building block for many NLP applications. Yet, existing approaches require either large annotated corpora to train a model with or, when using large language models as a base, require carefully crafting the prompt as well as using a long context that can fit many examples. As a result, it is not possible for end-users to build classifiers for themselves. To address this issue, we propose a novel approach to few-shot text classification using an LLM. Rather than few-shot examples, the LLM is prompted with descriptions of the salient features of each class. These descriptions are coauthored by the user and the LLM interactively: while the user annotates each few-shot example, the LLM asks relevant questions that the user answers. Examples, questions, and answers are summarized to form the classification prompt. Our experiments show that our approach yields high accuracy classifiers, within 79% of the performance of models trained with significantly larger datasets while using only 1% of their training sets. Additionally, in a study with 30 participants, we show that end-users are able to build classifiers to suit their specific needs. The personalized classifiers show an average accuracy of 90%, which is 15% higher than the state-of-the-art approach.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.933.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Yet, existing approaches require either large annotated corpora to train a model with or, when using large language models as a base, require carefully crafting the prompt as well as using a long context that can fit many examples.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Yet, existing approaches require either large annotated corpora to train a model with or, when using large language models as a base, require carefully crafting the prompt as well as using a long context that can fit many examples.\""
    },
    {
        "title": "DialGuide: Aligning Dialogue Model Behavior with Developer Guidelines",
        "authors": [
            "Prakhar Gupta",
            "Yang Liu",
            "Di Jin",
            "Behnam Hedayatnia",
            "Spandana Gella",
            "Sijia Liu",
            "Patrick Lange",
            "Julia Hirschberg",
            "Dilek Hakkani-Tur"
        ],
        "published": "2023",
        "summary": "Dialogue models are able to generate coherent and fluent responses, but they can still be challenging to control and may produce non-engaging, unsafe results. This unpredictability diminishes user trust and can hinder the use of the models in the real world. To address this, we introduce DialGuide, a novel framework for controlling dialogue model behavior using natural language rules, or guidelines. These guidelines provide information about the context they are applicable to and what should be included in the response, allowing the models to generate responses that are more closely aligned with the developer’s expectations and intent. We evaluate DialGuide on three tasks in open-domain dialogue response generation: guideline selection, response generation, and response entailment verification. Our dataset contains 10,737 positive and 15,467 negative dialogue context-response-guideline triplets across two domains - chit-chat and safety. We provide baseline models for the tasks and benchmark their performance. We also demonstrate that DialGuide is effective in the dialogue safety domain, producing safe and engaging responses that follow developer guidelines.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.935.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Dialogue models are able to generate coherent and fluent responses, but they can still be challenging to control and may produce non-engaging, unsafe results.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Dialogue models are able to generate coherent and fluent responses, but they can still be challenging to control and may produce non-engaging, unsafe results.\""
    },
    {
        "title": "Who Wrote it and Why? Prompting Large-Language Models for Authorship Verification",
        "authors": [
            "Chia-Yu Hung",
            "Zhiqiang Hu",
            "Yujia Hu",
            "Roy Lee"
        ],
        "published": "2023",
        "summary": "Authorship verification (AV) is a fundamental task in natural language processing (NLP) and computational linguistics, with applications in forensic analysis, plagiarism detection, and identification of deceptive content. Existing AV techniques, including traditional stylometric and deep learning approaches, face limitations in terms of data requirements and lack of explainability. To address these limitations, this paper proposes PromptAV, a novel technique that leverages Large-Language Models (LLMs) for AV by providing step-by-step stylometric explanation prompts. PromptAV outperforms state-of-the-art baselines, operates effectively with limited training data, and enhances interpretability through intuitive explanations, showcasing its potential as an effective and interpretable solution for the AV task.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.937.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing AV techniques, including traditional stylometric and deep learning approaches, face limitations in terms of data requirements and lack of explainability.\"\n\nThis rating is given because the paper mentions limitations of existing AV techniques, but it does not specifically discuss limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing AV techniques, including traditional stylometric and deep learning approaches, face limitations in terms of data requirements and lack of explainability.\"\n\nThis rating is given because the paper mentions limitations of existing AV techniques, but it does not specifically discuss limitations of LLMs."
    },
    {
        "title": "VISIT: Visualizing and Interpreting the Semantic Information Flow of Transformers",
        "authors": [
            "Shahar Katz",
            "Yonatan Belinkov"
        ],
        "published": "2023",
        "summary": "Recent advances in interpretability suggest we can project weights and hidden states of transformer-based language models (LMs) to their vocabulary, a transformation that makes them more human interpretable. In this paper, we investigate LM attention heads and memory values, the vectors the models dynamically create and recall while processing a given input. By analyzing the tokens they represent through this projection, we identify patterns in the information flow inside the attention mechanism. Based on our discoveries, we create a tool to visualize a forward pass of Generative Pre-trained Transformers (GPTs) as an interactive flow graph, with nodes representing neurons or hidden states and edges representing the interactions between them. Our visualization simplifies huge amounts of data into easy-to-read plots that can reflect the models’ internal processing, uncovering the contribution of each component to the models’ final prediction. Our visualization also unveils new insights about the role of layer norms as semantic filters that influence the models’ output, and about neurons that are always activated during forward passes and act as regularization vectors.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.939.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation is mentioned, but the paper aims to \"uncover\" the internal processing of LLMs, which might imply a lack of understanding of their inner workings.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation is mentioned, but the paper aims to \"uncover\" the internal processing of LLMs, which might imply a lack of understanding of their inner workings."
    },
    {
        "title": "Arabic Mini-ClimateGPT : A Climate Change and Sustainability Tailored Arabic LLM",
        "authors": [
            "Sahal Shaji Mullappilly",
            "Abdelrahman Shaker",
            "Omkar Thawakar",
            "Hisham Cholakkal",
            "Rao Muhammad Anwer",
            "Salman Khan",
            "Fahad Khan"
        ],
        "published": "2023",
        "summary": "Climate change is one of the most significant challenges we face together as a society. Creating awareness and educating policy makers the wide-ranging impact of climate change is an essential step towards a sustainable future. Recently, Large Language Models (LLMs) like ChatGPT and Bard have shown impressive conversational abilities and excel in a wide variety of NLP tasks. While these models are close-source, recently alternative open-source LLMs such as Stanford Alpaca and Vicuna have shown promising results. However, these open-source models are not specifically tailored for climate related domain specific information and also struggle to generate meaningful responses in other languages such as, Arabic. To this end, we propose a light-weight Arabic Mini-ClimateGPT that is built on an open-source LLM and is specifically fine-tuned on a conversational-style instruction tuning curated Arabic dataset Clima500-Instruct with over 500k instructions about climate change and sustainability. Further, our model also utilizes a vector embedding based retrieval mechanism during inference. We validate our proposed model through quantitative and qualitative evaluations on climate-related queries. Our model surpasses the baseline LLM in 88.3% of cases during ChatGPT-based evaluation. Furthermore, our human expert evaluation reveals an 81.6% preference for our model’s responses over multiple popular open-source models. Our open-source demos, models and curated instruction sets are available here : https://github.com/mbzuai-oryx/ClimateGPT",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.941.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these open-source models are not specifically tailored for climate related domain specific information and also struggle to generate meaningful responses in other languages such as, Arabic.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these open-source models are not specifically tailored for climate related domain specific information and also struggle to generate meaningful responses in other languages such as, Arabic.\""
    },
    {
        "title": "Interpreting Answers to Yes-No Questions in User-Generated Content",
        "authors": [
            "Shivam Mathur",
            "Keun Park",
            "Dhivya Chinnappa",
            "Saketh Kotamraju",
            "Eduardo Blanco"
        ],
        "published": "2023",
        "summary": "Interpreting answers to yes-no questions in social media is difficult. Yes and no keywords are uncommon, and the few answers that include them are rarely to be interpreted what the keywords suggest. In this paper, we present a new corpus of 4,442 yes-no question-answer pairs from Twitter. We discuss linguistic characteristics of answers whose interpretation is yes or no, as well as answers whose interpretation is unknown. We show that large language models are far from solving this problem, even after fine-tuning and blending other corpora for the same problem but outside social media.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.942.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We show that large language models are far from solving this problem, even after fine-tuning and blending other corpora for the same problem but outside social media.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We show that large language models are far from solving this problem, even after fine-tuning and blending other corpora for the same problem but outside social media.\""
    },
    {
        "title": "Selective Demonstrations for Cross-domain Text-to-SQL",
        "authors": [
            "Shuaichen Chang",
            "Eric Fosler-Lussier"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) with in-context learning have demonstrated impressive generalization capabilities in the cross-domain text-to-SQL task, without the use of in-domain annotations. However, incorporating in-domain demonstration examples has been found to greatly enhance LLMs’ performance. In this paper, we delve into the key factors within in-domain examples that contribute to the improvement and explore whether we can harness these benefits without relying on in-domain annotations. Based on our findings, we propose a demonstration selection framework, ODIS, which utilizes both out-of-domain examples and synthetically generated in-domain examples to construct demonstrations. By retrieving demonstrations from hybrid sources, ODIS leverages the advantages of both, showcasing its effectiveness compared to baseline methods that rely on a single data source. Furthermore, ODIS outperforms state-of-the-art approaches on two cross-domain text-to-SQL datasets, with improvements of 1.1 and 11.8 points in execution accuracy, respectively.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.944.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, incorporating in-domain demonstration examples has been found to greatly enhance LLMs’ performance.\"\n\nThis abstract mentions one limitation of the Large Language Models (the need for in-domain demonstration examples to enhance performance) in passing, but does not elaborate on it further and focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, incorporating in-domain demonstration examples has been found to greatly enhance LLMs’ performance.\"\n\nThis abstract mentions one limitation of the Large Language Models (the need for in-domain demonstration examples to enhance performance) in passing, but does not elaborate on it further and focuses on the proposed solution."
    },
    {
        "title": "TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks",
        "authors": [
            "Shubhra Kanti Karmaker Santu",
            "Dongji Feng"
        ],
        "published": "2023",
        "summary": "While LLMs have shown great success in understanding and generating text in traditional conversational settings, their potential for performing ill-defined complex tasks is largely under-studied and yet to be benchmarked. However, conducting such benchmarking studies is challenging because of the large variations in LLMs’ performance when different prompt types/styles are used and different degrees of detail are provided in the prompts. To address this issue, this paper proposes a general taxonomy that can be used to design prompts with specific properties in order to perform a wide range of complex tasks. This taxonomy will allow future benchmarking studies to report the specific categories of prompts used as part of the study, enabling meaningful comparisons across different studies. Also, by establishing a common standard through this taxonomy, researchers will be able to draw more accurate conclusions about LLMs’ performance on a specific complex task.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.946.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, conducting such benchmarking studies is challenging because of the large variations in LLMs’ performance when different prompt types/styles are used and different degrees of detail are provided in the prompts.\"\n\nThis evidence suggests that the paper mentions a limitation of LLMs (variations in performance due to prompt types/styles and detail) but does not explore it in depth, and instead focuses on",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, conducting such benchmarking studies is challenging because of the large variations in LLMs’ performance when different prompt types/styles are used and different degrees of detail are provided in the prompts.\"\n\nThis evidence suggests that the paper mentions a limitation of LLMs (variations in performance due to prompt types/styles and detail) but does not explore it in depth, and instead focuses on"
    },
    {
        "title": "Is the Answer in the Text? Challenging ChatGPT with Evidence Retrieval from Instructive Text",
        "authors": [
            "Sophie Henning",
            "Talita Anthonio",
            "Wei Zhou",
            "Heike Adel",
            "Mohsen Mesgar",
            "Annemarie Friedrich"
        ],
        "published": "2023",
        "summary": "Generative language models have recently shown remarkable success in generating answers to questions in a given textual context. However, these answers may suffer from hallucination, wrongly cite evidence, and spread misleading information. In this work, we address this problem by employing ChatGPT, a state-of-the-art generative model, as a machine-reading system. We ask it to retrieve answers to lexically varied and open-ended questions from trustworthy instructive texts. We introduce WHERE (WikiHow Evidence REtrieval), a new high-quality evaluation benchmark of a set of WikiHow articles exhaustively annotated with evidence sentences to questions that comes with a special challenge: All questions are about the article’s topic, but not all can be answered using the provided context. We interestingly find that when using a regular question-answering prompt, ChatGPT neglects to detect the unanswerable cases. When provided with a few examples, it learns to better judge whether a text provides answer evidence or not. Alongside this important finding, our dataset defines a new benchmark for evidence retrieval in question answering, which we argue is one of the necessary next steps for making large language models more trustworthy.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.949.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these answers may suffer from hallucination, wrongly cite evidence, and spread misleading information.\"; \"We interestingly find that when using a regular question-answering prompt, ChatGPT neglects to detect the unanswerable cases.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, these answers may suffer from hallucination, wrongly cite evidence, and spread misleading information.\"; \"We interestingly find that when using a regular question-answering prompt, ChatGPT neglects to detect the unanswerable cases.\""
    },
    {
        "title": "PaRaDe: Passage Ranking using Demonstrations with LLMs",
        "authors": [
            "Andrew Drozdov",
            "Honglei Zhuang",
            "Zhuyun Dai",
            "Zhen Qin",
            "Razieh Rahimi",
            "Xuanhui Wang",
            "Dana Alon",
            "Mohit Iyyer",
            "Andrew McCallum",
            "Donald Metzler",
            "Kai Hui"
        ],
        "published": "2023",
        "summary": "Recent studies show that large language models (LLMs) can be instructed to effectively perform zero-shot passage re-ranking, in which the results of a first stage retrieval method, such as BM25, are rated and reordered to improve relevance. In this work, we improve LLM-based re-ranking by algorithmically selecting few-shot demonstrations to include in the prompt. Our analysis investigates the conditions where demonstrations are most helpful, and shows that adding even one demonstration is significantly beneficial. We propose a novel demonstration selection strategy based on difficulty rather than the commonly used semantic similarity. Furthermore, we find that demonstrations helpful for ranking are also effective at question generation. We hope our work will spur more principled research into question generation and passage ranking.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.950.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions \"conditions where demonstrations are most helpful\" which implies some limitations in the effectiveness of LLM-based re-ranking, but it is not explicitly stated.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions \"conditions where demonstrations are most helpful\" which implies some limitations in the effectiveness of LLM-based re-ranking, but it is not explicitly stated."
    },
    {
        "title": "The Locality and Symmetry of Positional Encodings",
        "authors": [
            "Lihu Chen",
            "Gael Varoquaux",
            "Fabian Suchanek"
        ],
        "published": "2023",
        "summary": "Positional Encodings (PEs) are used to inject word-order information into transformer-based language models. While they can significantly enhance the quality of sentence representations, their specific contribution to language models is not fully understood, especially given recent findings that various positional encodings are insensitive to word order. In this work, we conduct a systematic study of positional encodings in Bidirectional Masked Language Models (BERT-style) , which complements existing work in three aspects: (1) We uncover the core function of PEs by identifying two common properties, Locality and Symmetry; (2) We show that the two properties are closely correlated with the performances of downstream tasks; (3) We quantify the weakness of current PEs by introducing two new probing tasks, on which current PEs perform poorly. We believe that these results are the basis for developing better PEs for transformer-based language models.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.955.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We quantify the weakness of current PEs by introducing two new probing tasks, on which current PEs perform poorly.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We quantify the weakness of current PEs by introducing two new probing tasks, on which current PEs perform poorly.\""
    },
    {
        "title": "Unnatural language processing: How do language models handle machine-generated prompts?",
        "authors": [
            "Corentin Kervadec",
            "Francesca Franzon",
            "Marco Baroni"
        ],
        "published": "2023",
        "summary": "Language model prompt optimization research has shown that semantically and grammatically well-formed manually crafted prompts are routinely outperformed by automatically generated token sequences with no apparent meaning or syntactic structure, including sequences of vectors from a model’s embedding space. We use machine-generated prompts to probe how models respond to input that is not composed of natural language expressions. We study the behavior of models of different sizes in multiple semantic tasks in response to both continuous and discrete machine-generated prompts, and compare it to the behavior in response to human-generated natural-language prompts. Even when producing a similar output, machine-generated and human prompts trigger different response patterns through the network processing pathways, including different perplexities, different attention and output entropy distributions, and different unit activation profiles. We provide preliminary insight into the nature of the units activated by different prompt types, suggesting that only natural language prompts recruit a genuinely linguistic circuit.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.959.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Even when producing a similar output, machine-generated and human prompts trigger different response patterns through the network processing pathways, including different perplexities, different attention and output entropy distributions, and different unit activation profiles.\"\n\nThis paper discusses the limitations of LLMs in handling machine-generated prompts, highlighting differences in response patterns and unit activation profiles compared to human-generated natural-language prompts.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Even when producing a similar output, machine-generated and human prompts trigger different response patterns through the network processing pathways, including different perplexities, different attention and output entropy distributions, and different unit activation profiles.\"\n\nThis paper discusses the limitations of LLMs in handling machine-generated prompts, highlighting differences in response patterns and unit activation profiles compared to human-generated natural-language prompts."
    },
    {
        "title": "Gradually Excavating External Knowledge for Implicit Complex Question Answering",
        "authors": [
            "Chang Liu",
            "Xiaoguang Li",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu",
            "Edmund Lam",
            "Ngai Wong"
        ],
        "published": "2023",
        "summary": "Recently, large language models (LLMs) have gained much attention for the emergence of human-comparable capabilities and huge potential. However, for open-domain implicit question-answering problems, LLMs may not be the ultimate solution due to the reasons of: 1) uncovered or out-of-date domain knowledge, 2) one-shot generation and hence restricted comprehensiveness. To this end, this work proposes a gradual knowledge excavation framework for open-domain complex question answering, where LLMs iteratively and actively acquire extrinsic information, then reason based on acquired historical knowledge. Specifically, during each step of the solving process, the model selects an action to execute, such as querying external knowledge or performing a single logical reasoning step, to gradually progress toward a final answer. Our method can effectively leverage plug-and-play external knowledge and dynamically adjust the strategy for solving complex questions. Evaluated on the StrategyQA dataset, our method achieves 78.17% accuracy with less than 6% parameters of its competitors, setting new SOTA in the ~10B LLM class.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.961.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, for open-domain implicit question-answering problems, LLMs may not be the ultimate solution due to the reasons of: 1) uncovered or out-of-date domain knowledge, 2) one-shot generation and hence restricted comprehensiveness.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, for open-domain implicit question-answering problems, LLMs may not be the ultimate solution due to the reasons of: 1) uncovered or out-of-date domain knowledge, 2) one-shot generation and hence restricted comprehensiveness.\""
    },
    {
        "title": "Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models",
        "authors": [
            "Hongli Zhan",
            "Desmond Ong",
            "Junyi Jessy Li"
        ],
        "published": "2023",
        "summary": "The emotions we experience involve complex processes; besides physiological aspects, research in psychology has studied cognitive appraisals where people assess their situations subjectively, according to their own values (Scherer, 2005). Thus, the same situation can often result in different emotional experiences. While the detection of emotion is a well-established task, there is very limited work so far on the automatic prediction of cognitive appraisals. This work fills the gap by presenting CovidET-Appraisals, the most comprehensive dataset to-date that assesses 24 appraisal dimensions, each with a natural language rationale, across 241 Reddit posts. CovidET-Appraisals presents an ideal testbed to evaluate the ability of large language models — excelling at a wide range of NLP tasks — to automatically assess and explain cognitive appraisals. We found that while the best models are performant, open-sourced LLMs fall short at this task, presenting a new challenge in the future development of emotionally intelligent models. We release our dataset at https://github.com/honglizhan/CovidET-Appraisals-Public.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.962.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We found that while the best models are performant, open-sourced LLMs fall short at this task, presenting a new challenge in the future development of emotionally intelligent models.\"\n\nThis rating is chosen because the paper discusses a specific limitation of LLMs (their inability to assess cognitive appraisals) and provides some analysis and evidence for this limitation, but the primary focus of",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We found that while the best models are performant, open-sourced LLMs fall short at this task, presenting a new challenge in the future development of emotionally intelligent models.\"\n\nThis rating is chosen because the paper discusses a specific limitation of LLMs (their inability to assess cognitive appraisals) and provides some analysis and evidence for this limitation, but the primary focus of"
    },
    {
        "title": "A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing",
        "authors": [
            "Carlos Gómez-Rodríguez",
            "Paul Williams"
        ],
        "published": "2023",
        "summary": "We evaluate a range of recent LLMs on English creative writing, a challenging and complex task that requires imagination, coherence, and style. We use a difficult, open-ended scenario chosen to avoid training data reuse: an epic narration of a single combat between Ignatius J. Reilly, the protagonist of the Pulitzer Prize-winning novel A Confederacy of Dunces (1980), and a pterodactyl, a prehistoric flying reptile. We ask several LLMs and humans to write such a story and conduct a human evalution involving various criteria such as fluency, coherence, originality, humor, and style. Our results show that some state-of-the-art commercial LLMs match or slightly outperform our writers in most dimensions; whereas open-source LLMs lag behind. Humans retain an edge in creativity, while humor shows a binary divide between LLMs that can handle it comparably to humans and those that fail at it. We discuss the implications and limitations of our study and suggest directions for future research.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.966.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We discuss the implications and limitations of our study\"; \"Humans retain an edge in creativity, while humor shows a binary divide between LLMs that can handle it comparably to humans and those that fail at it.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We discuss the implications and limitations of our study\"; \"Humans retain an edge in creativity, while humor shows a binary divide between LLMs that can handle it comparably to humans and those that fail at it.\""
    },
    {
        "title": "Context-faithful Prompting for Large Language Models",
        "authors": [
            "Wenxuan Zhou",
            "Sheng Zhang",
            "Hoifung Poon",
            "Muhao Chen"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs’ contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs’ faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator’s statement and inquire about the narrator’s opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. Code and data are released at https://github.com/wzhouad/context-faithful-llm.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.968.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks).\""
    },
    {
        "title": "Drilling Down into the Discourse Structure with LLMs for Long Document Question Answering",
        "authors": [
            "Inderjeet Nair",
            "Shwetha Somasundaram",
            "Apoorv Saxena",
            "Koustava Goswami"
        ],
        "published": "2023",
        "summary": "We address the task of evidence retrieval for long document question answering, which involves locating relevant paragraphs within a document to answer a question. We aim to assess the applicability of large language models (LLMs) in the task of zero-shot long document evidence retrieval, owing to their unprecedented performance across various NLP tasks. However, currently the LLMs can consume limited context lengths as input, thus providing document chunks as inputs might overlook the global context while missing out on capturing the inter-segment dependencies. Moreover, directly feeding the large input sets can incur significant computational costs, particularly when processing the entire document (and potentially incurring monetary expenses with enterprise APIs like OpenAI’s GPT variants). To address these challenges, we propose a suite of techniques that exploit the discourse structure commonly found in documents. By utilizing this structure, we create a condensed representation of the document, enabling a more comprehensive understanding and analysis of relationships between different parts. We retain 99.6% of the best zero-shot approach’s performance, while processing only 26% of the total tokens used by the best approach in the information seeking evidence retrieval setup. We also show how our approach can be combined with *self-ask* reasoning agent to achieve best zero-shot performance in complex multi-hop question answering, just ≈ 4% short of zero-shot performance using gold evidence.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.972.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, currently the LLMs can consume limited context lengths as input, thus providing document chunks as inputs might overlook the global context while missing out on capturing the inter-segment dependencies. Moreover, directly feeding the large input sets can incur significant computational costs, particularly when processing the entire document (and potentially incurring monetary expenses with enterprise APIs like OpenAI’s GPT variants).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, currently the LLMs can consume limited context lengths as input, thus providing document chunks as inputs might overlook the global context while missing out on capturing the inter-segment dependencies. Moreover, directly feeding the large input sets can incur significant computational costs, particularly when processing the entire document (and potentially incurring monetary expenses with enterprise APIs like OpenAI’s GPT variants).\""
    },
    {
        "title": "Emergent Inabilities? Inverse Scaling Over the Course of Pretraining",
        "authors": [
            "James Michaelov",
            "Ben Bergen"
        ],
        "published": "2023",
        "summary": "Does inverse scaling only occur as a function of model size, or can it also occur over the course of training? We carry out an exploratory study investigating whether the performance of language models on specific tasks can decrease (while general performance remains high) during training on the language modeling task. We find 8 tasks on which Pythia 12B (Biderman et al., 2023) shows decreased performance over the course of training. Five of these tasks (TruthfulQA-MC1, TruthfulQA-MC2, Hindsight Neglect, Memo Trap, and Pattern Match Suppression) additionally show a consistent relationship whereby larger language models show a greater decrease in performance the more they are trained, despite showing standard (positive) scaling overall. This highlights the importance of testing performance at all relevant benchmarks any time models are trained on additional data, even if their overall performance improves.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.973.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find 8 tasks on which Pythia 12B (Biderman et al., 2023) shows decreased performance over the course of training. Five of these tasks (TruthfulQA-MC1, TruthfulQA-MC2, Hindsight Neglect, Memo Trap, and Pattern Match Suppression) additionally show a consistent relationship whereby larger language models show",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We find 8 tasks on which Pythia 12B (Biderman et al., 2023) shows decreased performance over the course of training. Five of these tasks (TruthfulQA-MC1, TruthfulQA-MC2, Hindsight Neglect, Memo Trap, and Pattern Match Suppression) additionally show a consistent relationship whereby larger language models show"
    },
    {
        "title": "Representation Projection Invariance Mitigates Representation Collapse",
        "authors": [
            "Anastasia Razdaibiedina",
            "Ashish Khetan",
            "Zohar Karnin",
            "Daniel Khashabi",
            "Vivek Madan"
        ],
        "published": "2023",
        "summary": "Fine-tuning contextualized representations learned by pre-trained language models remains a prevalent practice in NLP. However, fine-tuning can lead to representation degradation (also known as representation collapse), which may result in instability, sub-optimal performance, and weak generalization. In this paper, we propose Representation Projection Invariance (REPINA), a novel regularization method to maintain the information content of representation and reduce representation collapse during fine-tuning by discouraging undesirable changes in the representations. We study the empirical behavior of the proposed regularization in comparison to 5 comparable baselines across 13 language understanding tasks (GLUE benchmark and six additional datasets). When evaluating in-domain performance, REPINA consistently outperforms other baselines on most tasks (10 out of 13). Additionally, REPINA improves out-of-distribution performance. We also demonstrate its effectiveness in few-shot settings and robustness to label perturbation. As a by-product, we extend previous studies of representation collapse and propose several metrics to quantify it. Our empirical findings show that our approach is significantly more effective at mitigating representation collapse.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.975.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, fine-tuning can lead to representation degradation (also known as representation collapse), which may result in instability, sub-optimal performance, and weak generalization.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, fine-tuning can lead to representation degradation (also known as representation collapse), which may result in instability, sub-optimal performance, and weak generalization.\""
    },
    {
        "title": "Tunable Soft Prompts are Messengers in Federated Learning",
        "authors": [
            "Chenhe Dong",
            "Yuexiang Xie",
            "Bolin Ding",
            "Ying Shen",
            "Yaliang Li"
        ],
        "published": "2023",
        "summary": "Federated learning (FL) enables multiple participants to collaboratively train machine learning models using decentralized data sources, alleviating privacy concerns that arise from directly sharing local data. However, the lack of model privacy protection in FL becomes an unneglectable challenge, especially when people want to federally finetune models based on a proprietary large language model. In this study, we propose a novel FL training approach that accomplishes information exchange among participants via tunable soft prompts. These soft prompts, updated and transmitted between the server and clients, assume the role of the global model parameters and serve as messengers to deliver useful knowledge from the local data and global model. As the global model itself is not required to be shared and the local training is conducted based on an auxiliary model with fewer parameters than the global model, the proposed approach provides protection for the global model while reducing communication and computation costs in FL. Extensive experiments show the effectiveness of the proposed approach compared to several baselines. We have released the source code at https://github.com/alibaba/FederatedScope/tree/fedsp/federatedscope/nlp/fedsp.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.976.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the lack of model privacy protection in FL becomes an unneglectable challenge, especially when people want to federally finetune models based on a proprietary large language model.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the lack of model privacy protection in FL becomes an unneglectable challenge, especially when people want to federally finetune models based on a proprietary large language model.\""
    },
    {
        "title": "Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting",
        "authors": [
            "Benjamin Yan",
            "Ruochen Liu",
            "David Kuo",
            "Subathra Adithan",
            "Eduardo Reis",
            "Stephen Kwak",
            "Vasantha Venugopal",
            "Chloe O’Connell",
            "Agustina Saenz",
            "Pranav Rajpurkar",
            "Michael Moor"
        ],
        "published": "2023",
        "summary": "Automatically generated reports from medical images promise to improve the workflow of radiologists. Existing methods consider an image-to-report modeling task by directly generating a fully-fledged report from an image. However, this conflates the content of the report (e.g., findings and their attributes) with its style (e.g., format and choice of words), which can lead to clinically inaccurate reports. To address this, we propose a two-step approach for radiology report generation. First, we extract the content from an image; then, we verbalize the extracted content into a report that matches the style of a specific radiologist. For this, we leverage RadGraph—a graph representation of reports—together with large language models (LLMs). In our quantitative evaluations, we find that our approach leads to beneficial performance. Our human evaluation with clinical raters highlights that the AI-generated reports are indistinguishably tailored to the style of individual radiologist despite leveraging only a few examples as context.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.977.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, this conflates the content of the report (e.g., findings and their attributes) with its style (e.g., format and choice of words), which can lead to clinically inaccurate reports.\"\n\nThis paper discusses LLMs but does not explicitly mention any limitations of the models themselves, only a limitation of the existing methods that can be addressed by the proposed approach.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, this conflates the content of the report (e.g., findings and their attributes) with its style (e.g., format and choice of words), which can lead to clinically inaccurate reports.\"\n\nThis paper discusses LLMs but does not explicitly mention any limitations of the models themselves, only a limitation of the existing methods that can be addressed by the proposed approach."
    },
    {
        "title": "Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs",
        "authors": [
            "Yuxin Zuo",
            "Bei Li",
            "Chuanhao Lv",
            "Tong Zheng",
            "Tong Xiao",
            "JingBo Zhu"
        ],
        "published": "2023",
        "summary": "This paper presents an in-depth study of multimodal machine translation (MMT), examining the prevailing understanding that MMT systems exhibit decreased sensitivity to visual information when text inputs are complete. Instead, we attribute this phenomenon to insufficient cross-modal interaction, rather than image information redundancy. A novel approach is proposed to generate parallel Visual Question-Answering (VQA) style pairs from the source text, fostering more robust cross-modal interaction. Using Large Language Models (LLMs), we explicitly model the probing signal in MMT to convert it into VQA-style data to create the Multi30K-VQA dataset. An MMT-VQA multitask learning framework is introduced to incorporate explicit probing signals from the dataset into the MMT training process. Experimental results on two widely-used benchmarks demonstrate the effectiveness of this novel approach. Our code and data would be available at: https://github.com/libeineu/MMT-VQA.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.978.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Using Large Language Models (LLMs), we explicitly model the probing signal in MMT to convert it into VQA-style data to create the Multi30K-VQA dataset.\"\n\nThis paper mentions LLMs but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Using Large Language Models (LLMs), we explicitly model the probing signal in MMT to convert it into VQA-style data to create the Multi30K-VQA dataset.\"\n\nThis paper mentions LLMs but does not discuss any limitations of LLMs."
    },
    {
        "title": "Efficient Multilingual Language Model Compression through Vocabulary Trimming",
        "authors": [
            "Asahi Ushio",
            "Yi Zhou",
            "Jose Camacho-Collados"
        ],
        "published": "2023",
        "summary": "Multilingual language models (LMs) have become a powerful tool in NLP, especially for non-English languages. Nevertheless, model parameters of multilingual LMs remain large due to the larger embedding matrix of the vocabulary covering tokens in different languages. Instead, monolingual LMs can be trained in a target language with the language-specific vocabulary only. In this paper, we propose vocabulary-trimming (VT), a method to reduce a multilingual LM vocabulary to a target language by deleting potentially irrelevant tokens from its vocabulary. In theory, VT can compress any existing multilingual LM to any language covered by the original model. In our experiments, we show that VT can retain the original performance of the multilingual LM, while being considerably smaller in size than the original multilingual LM. The evaluation is performed over four NLP tasks (two generative and two classification tasks) among four widely used multilingual LMs in seven languages. The results show that this methodology can keep the best of both monolingual and multilingual worlds by keeping a small size as monolingual models without the need for specifically retraining them, and can even help limit potentially harmful social biases.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.981.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"model parameters of multilingual LMs remain large due to the larger embedding matrix of the vocabulary covering tokens in different languages.\"\n\nThis abstract mentions a limitation of multilingual LMs (large model parameters) in passing, but the primary focus is on the proposed solution (vocabulary-trimming) to address this limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"model parameters of multilingual LMs remain large due to the larger embedding matrix of the vocabulary covering tokens in different languages.\"\n\nThis abstract mentions a limitation of multilingual LMs (large model parameters) in passing, but the primary focus is on the proposed solution (vocabulary-trimming) to address this limitation."
    },
    {
        "title": "ICU: Conquering Language Barriers in Vision-and-Language Modeling by Dividing the Tasks into Image Captioning and Language Understanding",
        "authors": [
            "Guojun Wu"
        ],
        "published": "2023",
        "summary": "Most multilingual vision-and-language (V&L) research aims to accomplish multilingual and multimodal capabilities within one model. However, the scarcity of multilingual captions for images has hindered the development. To overcome this obstacle, we propose ICU, Image Caption Understanding, which divides a V&L task into two stages: a V&L model performs image captioning in English, and a multilingual language model (mLM), in turn, takes the caption as the alt text and performs cross-lingual language understanding. The burden of multilingual processing is lifted off V&L model and placed on mLM. Since the multilingual text data is relatively of higher abundance and quality, ICU can facilitate the conquering of language barriers for V&L models. In experiments on two tasks across 9 languages in the IGLUE benchmark, we show that ICU can achieve new state-of-the-art results for five languages, and comparable results for the rest.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.982.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the scarcity of multilingual captions for images has hindered the development.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, the scarcity of multilingual captions for images has hindered the development.\""
    },
    {
        "title": "GTA: Gated Toxicity Avoidance for LM Performance Preservation",
        "authors": [
            "Heegyu Kim",
            "Hyunsouk Cho"
        ],
        "published": "2023",
        "summary": "Caution: This paper includes offensive words that could potentially cause unpleasantness. The fast-paced evolution of generative language models such as GPT-4 has demonstrated outstanding results in various NLP generation tasks. However, due to the potential generation of offensive words related to race or gender, various Controllable Text Generation (CTG) methods have been proposed to mitigate the occurrence of harmful words. However, existing CTG methods not only reduce toxicity but also negatively impact several aspects of the language model’s generation performance, including topic consistency, grammar, and perplexity. This paper explores the limitations of previous methods and introduces a novel solution in the form of a simple Gated Toxicity Avoidance (GTA) that can be applied to any CTG method. We also evaluate the effectiveness of the proposed GTA by comparing it with state-of-the-art CTG methods across various datasets. Our findings reveal that gated toxicity avoidance efficiently achieves comparable levels of toxicity reduction to the original CTG methods while preserving the generation performance of the language model.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.983.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing CTG methods not only reduce toxicity but also negatively impact several aspects of the language model’s generation performance, including topic consistency, grammar, and perplexity.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, existing CTG methods not only reduce toxicity but also negatively impact several aspects of the language model’s generation performance, including topic consistency, grammar, and perplexity.\""
    },
    {
        "title": "LMGQS: A Large-scale Dataset for Query-focused Summarization",
        "authors": [
            "Ruochen Xu",
            "Song Wang",
            "Yang Liu",
            "Shuohang Wang",
            "Yichong Xu",
            "Dan Iter",
            "Pengcheng He",
            "Chenguang Zhu",
            "Michael Zeng"
        ],
        "published": "2023",
        "summary": "Query-focused summarization (QFS) aims to extract or generate a summary of an input document that directly answers or is relevant to a given query. The lack of large-scale datasets in the form of documents, queries, and summaries has hindered model development in this area. In contrast, multiple large-scale high-quality datasets for generic summarization exist. We hypothesize that there is a hidden query for each summary sentence in a generic summarization annotation, and we utilize a large-scale pretrained language model to recover it. In this way, we convert four generic summarization benchmarks into a new QFS benchmark dataset, LMGQS, which consists of over 1 million document-query-summary samples. We thoroughly investigate the properties of our proposed dataset and establish baselines with state-of-the-art summarization models. By fine-tuning a language model on LMGQS, we achieve state-of-the-art zero-shot and supervised performance on multiple existing QFS benchmarks, demonstrating the high quality and diversity of LMGQS.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.984.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"utilize a large-scale pretrained language model to recover it.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"utilize a large-scale pretrained language model to recover it.\""
    },
    {
        "title": "ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models",
        "authors": [
            "Zhipeng Chen",
            "Kun Zhou",
            "Beichen Zhang",
            "Zheng Gong",
            "Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2023",
        "summary": "Although large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning. To improve the reasoning abilities, we propose ChatCoT, a tool-augmented chain-of-thought reasoning framework for chat-based LLMs (e.g., ChatGPT). In ChatCoT, we model the chain-of-thought (CoT) reasoning as multi-turn conversations, to utilize tools in a more natural way through chatting. At each turn, LLMs can either interact with tools or perform the reasoning. Our approach can effectively leverage the multi-turn conversation ability of chat-based LLMs, and integrate the thought chain following and tools manipulation in a unified way. Specially, we initialize the early turns of the conversation by the knowledge about tools, tasks, and reasoning format, and propose an iterative tool-augmented reasoning step to perform step-by-step tool-augmented reasoning. The experiment results on two complex reasoning datasets (MATH and HotpotQA) have shown the effectiveness of ChatCoT on complex reasoning tasks, achieving a 7.9% relative improvement over the state-of-the-art baseline.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.985.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning.\""
    },
    {
        "title": "Exploring the Effectiveness of Multi-Lingual Commonsense Knowledge-Aware Open-Domain Dialogue Response Generation",
        "authors": [
            "Sixing Wu",
            "Jiong Yu",
            "Tianshi Che",
            "Yang Zhou",
            "Wei Zhou"
        ],
        "published": "2023",
        "summary": "Prior works have shown the promising results of commonsense knowledge-aware models in improving informativeness while reducing the hallucination issue. Nonetheless, prior works often can only use monolingual knowledge whose language is consistent with the dialogue context. Except for a few high-resource languages, such as English and Chinese, most languages suffer from insufficient knowledge issues, especially minority languages. To this end, this work proposes a new task, Multi-Lingual Commonsense Knowledge-Aware Response Generation (MCKRG), which tries to use commonsense knowledge in other languages to enhance the current dialogue generation. Then, we construct a MCKRG dataset MCK-Dialog of seven languages with multiple alignment methods. Finally, we verify the effectiveness of using multi-lingual commonsense knowledge with a proposed MCK-T5 model. Extensive experimental results demonstrate the great potential of using multi-lingual commonsense knowledge in high-resource and low-resource languages. To the best of our knowledge, this work is the first to explore Multi-Lingual Commonsense Knowledge-Aware Response Generation.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.987.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"most languages suffer from insufficient knowledge issues, especially minority languages.\"\n\nThis rating is given because the paper mentions a limitation of LLMs (insufficient knowledge issues in minority languages) but does not explore it in detail and focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"most languages suffer from insufficient knowledge issues, especially minority languages.\"\n\nThis rating is given because the paper mentions a limitation of LLMs (insufficient knowledge issues in minority languages) but does not explore it in detail and focuses on the proposed solution."
    },
    {
        "title": "Mixture of Soft Prompts for Controllable Data Generation",
        "authors": [
            "Derek Chen",
            "Celine Lee",
            "Yunan Lu",
            "Domenic Rosati",
            "Zhou Yu"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) effectively generate fluent text when the target output follows natural language patterns. However, structured prediction tasks confine the output format to a limited ontology, causing even very large models to struggle since they were never trained with such restrictions in mind. The difficulty of using LLMs for direct prediction is exacerbated in few-shot learning scenarios, which commonly arise due to domain shift and resource limitations. We flip the problem on its head by leveraging the LLM as a tool for data augmentation rather than direct prediction. Our proposed Mixture of Soft Prompts (MSP) serves as a parameter-efficient procedure for generating multi-attribute data in a controlled manner. Denoising mechanisms are further applied to improve the quality of synthesized data. Automatic metrics show our method is capable of producing diverse and natural text, while preserving label semantics. Moreover, MSP achieves state-of-the-art results on three benchmarks when compared against strong baselines. Our method offers an alternate data-centric approach for applying LLMs to complex prediction tasks.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.988.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, structured prediction tasks confine the output format to a limited ontology, causing even very large models to struggle since they were never trained with such restrictions in mind. The difficulty of using LLMs for direct prediction is exacerbated in few-shot learning scenarios, which commonly arise due to domain shift and resource limitations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, structured prediction tasks confine the output format to a limited ontology, causing even very large models to struggle since they were never trained with such restrictions in mind. The difficulty of using LLMs for direct prediction is exacerbated in few-shot learning scenarios, which commonly arise due to domain shift and resource limitations.\""
    },
    {
        "title": "Prefix-Tuning Based Unsupervised Text Style Transfer",
        "authors": [
            "Huiyu Mai",
            "Wenhao Jiang",
            "Zhi-Hong Deng"
        ],
        "published": "2023",
        "summary": "Unsupervised text style transfer aims at training a generative model that can alter the style of the input sentence while preserving its content without using any parallel data. In this paper, we employ powerful pre-trained large language models and present a new prefix-tuning-based method for unsupervised text style transfer. We construct three different kinds of prefixes, i.e., shared prefix, style prefix, and content prefix, to encode task-specific information, target style, and the content information of the input sentence, respectively. Compared to embeddings used by previous works, the proposed prefixes can provide richer information for the model. Furthermore, we adopt a recursive way of using language models in the process of style transfer. This strategy provides a more effective way for the interactions between the input sentence and GPT-2, helps the model construct more informative prefixes, and thus, helps improve the performance. Evaluations on the well-known datasets show that our method outperforms the state-of-the-art baselines. Results, analysis of ablation studies, and subjective evaluations from humans are also provided for a deeper understanding of the proposed method.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.990.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract only mentions using pre-trained large language models and GPT-2, but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract only mentions using pre-trained large language models and GPT-2, but does not discuss any limitations of LLMs."
    },
    {
        "title": "Improving Input-label Mapping with Demonstration Replay for In-context Learning",
        "authors": [
            "Zhuocheng Gong",
            "Jiahao Liu",
            "Qifan Wang",
            "Jingang Wang",
            "Xunliang Cai",
            "Dongyan Zhao",
            "Rui Yan"
        ],
        "published": "2023",
        "summary": "In-context learning (ICL) is an emerging capability of large autoregressive language models where a few input-label demonstrations are appended to the input to enhance the model’s understanding of downstream NLP tasks, without directly adjusting the model parameters. The effectiveness of ICL can be attributed to the strong language modeling capabilities of large language models (LLMs), which enable them to learn the mapping between input and labels based on in-context demonstrations. Despite achieving promising results, the causal nature of language modeling in ICL restricts the attention to be backward only, i.e., a token only attends to its previous tokens, failing to capture the full input-label information and limiting the model’s performance. In this paper, we propose a novel ICL method called Repeated Demonstration with Sliding Causal Attention, (RdSca). Specifically, we duplicate later demonstrations and concatenate them to the front, allowing the model to ‘observe’ the later information even under the causal restriction. Besides, we introduce sliding causal attention, which customizes causal attention to avoid information leakage. Experimental results show that our method significantly improves the input-label mapping in ICL demonstrations. We also conduct an in-depth analysis of how to customize the causal attention without training, which has been an unexplored area in previous research.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.995.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the causal nature of language modeling in ICL restricts the attention to be backward only, i.e., a token only attends to its previous tokens, failing to capture the full input-label information and limiting the model’s performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the causal nature of language modeling in ICL restricts the attention to be backward only, i.e., a token only attends to its previous tokens, failing to capture the full input-label information and limiting the model’s performance.\""
    },
    {
        "title": "Enhancing Text-to-SQL Capabilities of Large Language Models: A Study on Prompt Design Strategies",
        "authors": [
            "Linyong Nan",
            "Yilun Zhao",
            "Weijin Zou",
            "Narutatsu Ri",
            "Jaesung Tae",
            "Ellen Zhang",
            "Arman Cohan",
            "Dragomir Radev"
        ],
        "published": "2023",
        "summary": "In-context learning (ICL) has emerged as a new approach to various natural language processing tasks, utilizing large language models (LLMs) to make predictions based on context that has been supplemented with a few examples or task-specific instructions. In this paper, we aim to extend this method to question answering tasks that utilize structured knowledge sources, and improve Text-to-SQL systems by exploring various prompt design strategies for employing LLMs. We conduct a systematic investigation into different demonstration selection methods and optimal instruction formats for prompting LLMs in the Text-to-SQL task. Our approach involves leveraging the syntactic structure of an example’s SQL query to retrieve demonstrations, and we demonstrate that pursuing both diversity and similarity in demonstration selection leads to enhanced performance. Furthermore, we show that LLMs benefit from database-related knowledge augmentations. Our most effective strategy outperforms the state-of-the-art system by 2.5 points (Execution Accuracy) and the best fine-tuned system by 5.1 points on the Spider dataset. These results highlight the effectiveness of our approach in adapting LLMs to the Text-to-SQL task, and we present an analysis of the factors contributing to the success of our strategy.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.996.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract discusses the effectiveness of the approach in adapting LLMs to the Text-to-SQL task, but does not mention any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract discusses the effectiveness of the approach in adapting LLMs to the Text-to-SQL task, but does not mention any limitations of LLMs."
    },
    {
        "title": "Aligning Predictive Uncertainty with Clarification Questions in Grounded Dialog",
        "authors": [
            "Kata Naszadi",
            "Putra Manggala",
            "Christof Monz"
        ],
        "published": "2023",
        "summary": "Asking for clarification is fundamental to effective collaboration. An interactive artificial agent must know when to ask a human instructor for more information in order to ascertain their goals. Previous work bases the timing of questions on supervised models learned from interactions between humans. Instead of a supervised classification task, we wish to ground the need for questions in the acting agent’s predictive uncertainty. In this work, we investigate if ambiguous linguistic instructions can be aligned with uncertainty in neural models. We train an agent using the T5 encoder-decoder architecture to solve the Minecraft Collaborative Building Task and identify uncertainty metrics that achieve better distributional separation between clear and ambiguous instructions. We further show that well-calibrated prediction probabilities benefit the detection of ambiguous instructions. Lastly, we provide a novel empirical analysis on the relationship between uncertainty and dialog history length and highlight an important property that poses a difficulty for detection.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.999.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Lastly, we provide a novel empirical analysis on the relationship between uncertainty and dialog history length and highlight an important property that poses a difficulty for detection.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Lastly, we provide a novel empirical analysis on the relationship between uncertainty and dialog history length and highlight an important property that poses a difficulty for detection.\""
    },
    {
        "title": "Cache me if you Can: an Online Cost-aware Teacher-Student framework to Reduce the Calls to Large Language Models",
        "authors": [
            "Ilias Stogiannidis",
            "Stavros Vassos",
            "Prodromos Malakasiotis",
            "Ion Androutsopoulos"
        ],
        "published": "2023",
        "summary": "Prompting Large Language Models (LLMs) performs impressively in zero- and few-shot settings. Hence, small and medium-sized enterprises (SMEs) that cannot afford the cost of creating large task-specific training datasets, but also the cost of pretraining their own LLMs, are increasingly turning to third-party services that allow them to prompt LLMs. However, such services currently require a payment per call, which becomes a significant operating expense (OpEx). Furthermore, customer inputs are often very similar over time, hence SMEs end-up prompting LLMs with very similar instances. We propose a framework that allows reducing the calls to LLMs by caching previous LLM responses and using them to train a local inexpensive model on the SME side. The framework includes criteria for deciding when to trust the local model or call the LLM, and a methodology to tune the criteria and measure the tradeoff between performance and cost. For experimental purposes, we instantiate our framework with two LLMs, GPT-3.5 or GPT-4, and two inexpensive students, a k-NN classifier or a Multi-Layer Perceptron, using two common business tasks, intent recognition and sentiment analysis. Experimental results indicate that significant OpEx savings can be obtained with only slightly lower performance.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1000.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, such services currently require a payment per call, which becomes a significant operating expense (OpEx). Furthermore, customer inputs are often very similar over time, hence SMEs end-up prompting LLMs with very similar instances.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs (high operating expense due to the need for frequent calls) but does not explore",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, such services currently require a payment per call, which becomes a significant operating expense (OpEx). Furthermore, customer inputs are often very similar over time, hence SMEs end-up prompting LLMs with very similar instances.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs (high operating expense due to the need for frequent calls) but does not explore"
    },
    {
        "title": "ParroT: Translating during Chat using Large Language Models tuned with Human Translation and Feedback",
        "authors": [
            "Wenxiang Jiao",
            "Jen-tse Huang",
            "Wenxuan Wang",
            "Zhiwei He",
            "Tian Liang",
            "Xing Wang",
            "Shuming Shi",
            "Zhaopeng Tu"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) like ChatGPT have exhibited remarkable abilities on a wide range of natural language processing (NLP) tasks, including various machine translation abilities accomplished during chat. However, these models are only accessible through restricted APIs, which creates barriers to new research and advancements in the field. Therefore, we propose ParroT, a framework to enhance and regulate the translation abilities during chat based on open-source LLMs (e.g., LLaMA), human-written translation and feedback data. Specifically, ParroT reformulates translation data into the instruction-following style, and introduces a “Hint” field for incorporating extra requirements to regulate the translation process. Accordingly, we propose three instruction types for finetuning ParroT models, including translation instruction, contrastive instruction, and error-guided instruction. Experiments on Flores subsets and WMT22 test sets suggest that translation instruction improves the translation performance of vanilla LLMs significantly while error-guided instruction can lead to further improvement, which demonstrates the importance of learning from low-quality translations annotated by humans. We also demonstrate the potential of automatic evaluation tools in providing quality information of translations, when constructing error-guided instructions for directions that lack human annotation data. Please refer to our Github project for more implementation details: https://github.com/wxjiao/ParroT.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1001.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these models are only accessible through restricted APIs, which creates barriers to new research and advancements in the field.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (restricted accessibility) but does not elaborate on it and focuses on the proposed solution (ParroT framework) to address this limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these models are only accessible through restricted APIs, which creates barriers to new research and advancements in the field.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of LLMs (restricted accessibility) but does not elaborate on it and focuses on the proposed solution (ParroT framework) to address this limitation."
    },
    {
        "title": "NarrativeXL: a Large-scale Dataset for Long-Term Memory Models",
        "authors": [
            "Arsenii Moskvichev",
            "Ky-Vinh Mai"
        ],
        "published": "2023",
        "summary": "We propose a new large-scale (nearly a million questions) ultra-long-context (more than 50,000 words average document length) reading comprehension dataset. Using GPT 3.5, we summarized each scene in 1,500 hand-curated fiction books from Project Gutenberg, which resulted in approximately 150 scene-level summaries per book. After that, we created a number of reading comprehension questions based on these summaries, including three types of multiple-choice scene recognition questions, as well as free-form narrative reconstruction questions. With 990,595 total questions, our dataset is an order of magnitude larger than the closest alternatives. Crucially, most questions have a known “retention demand”, indicating how long-term of a memory is needed to answer them, which should aid long-term memory performance evaluation. We validate our data in four small-scale experiments: one with human labelers, and three with existing language models. We show that our questions 1) adequately represent the source material 2) can be used to diagnose a model’s memory capacity 3) are not trivial for modern language models even when the memory demand does not exceed those models’ context lengths. Lastly, we provide our code which can be used to further expand the dataset with minimal human labor.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1005.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"are not trivial for modern language models even when the memory demand does not exceed those models’ context lengths.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"are not trivial for modern language models even when the memory demand does not exceed those models’ context lengths.\""
    },
    {
        "title": "PIVOINE: Instruction Tuning for Open-world Entity Profiling",
        "authors": [
            "Keming Lu",
            "Xiaoman Pan",
            "Kaiqiang Song",
            "Hongming Zhang",
            "Dong Yu",
            "Jianshu Chen"
        ],
        "published": "2023",
        "summary": "This work considers the problem of Open-world Entity Profiling, a sub-domain of Open-world Information Extraction (Open-world IE). Unlike the conventional closed-world IE, Open-world IE is considered a more general situation where entities and relations could be beyond a predefined ontology. We seek to develop a large language model (LLM) that can perform Open-world Entity Profiling with instruction tuning to extract desirable entity profiles characterized by (possibly fine-grained) natural language instructions. In particular, we construct INSTRUCTOPENWIKI, a substantial instruction-tuning dataset for Open-world Entity Profiling enriched with a comprehensive corpus, extensive annotations, and diverse instructions. We finetune pretrained BLOOM models on INSTRUCTOPENWIKI and obtain PIVOINE, an LLM for Open-world Entity Profiling with strong instruction-following capabilities. Our experiments demonstrate that PIVOINE significantly outperforms traditional methods and ChatGPT-based baselines, displaying impressive generalization capabilities on both unseen instructions and out-of-ontology cases. Consequently, PIVOINE emerges as a promising solution to tackle the open-world challenge of entity profiling.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1009.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We seek to develop a large language model (LLM) that can perform Open-world Entity Profiling with instruction tuning to extract desirable entity profiles characterized by (possibly fine-grained) natural language instructions.\"\n\nThis paper discusses LLMs, but does not mention any limitations of the models in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"We seek to develop a large language model (LLM) that can perform Open-world Entity Profiling with instruction tuning to extract desirable entity profiles characterized by (possibly fine-grained) natural language instructions.\"\n\nThis paper discusses LLMs, but does not mention any limitations of the models in the abstract."
    },
    {
        "title": "Tuna: Instruction Tuning using Feedback from Large Language Models",
        "authors": [
            "Haoran Li",
            "Yiran Liu",
            "Xingxing Zhang",
            "Wei Lu",
            "Furu Wei"
        ],
        "published": "2023",
        "summary": "Instruction tuning of open-source large language models (LLMs) like LLaMA, using direct outputs from more powerful LLMs such as Instruct-GPT and GPT-4, has proven to be a cost-effective way to align model behaviors with human preferences. However, the instruction-tuned model has only seen one response per instruction, lacking the knowledge of potentially better responses. In this paper, we propose finetuning an instruction-tuned LLM using our novel probabilistic ranking and contextual ranking approaches to increase the likelihood of generating better responses. Probabilistic ranking enables the instruction-tuned model to inherit the relative rankings of high-quality and low-quality responses from the teacher LLM. On the other hand, learning with contextual ranking allows the model to refine its own response distribution using the contextual understanding ability of stronger LLMs. Furthermore, we apply probabilistic ranking and contextual ranking sequentially to the instruction-tuned LLM. The resulting model, which we call Tuna, consistently improves the performance on Super Natural Instructions (119 test tasks), LMentry (25 test tasks), Vicuna QA, and can even obtain better results than several strong reinforcement learning baselines. Our code and data are available at https://github.com/microsoft/LMOps.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1011.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the instruction-tuned model has only seen one response per instruction, lacking the knowledge of potentially better responses.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the instruction-tuned model has only seen one response per instruction, lacking the knowledge of potentially better responses.\""
    },
    {
        "title": "Emptying the Ocean with a Spoon: Should We Edit Models?",
        "authors": [
            "Yuval Pinter",
            "Michael Elhadad"
        ],
        "published": "2023",
        "summary": "We call into question the recently popularized method of direct model editing as a means of correcting factual errors in LLM generations. We contrast model editing with three similar but distinct approaches that pursue better defined objectives: (1) retrieval-based architectures, which decouple factual memory from inference and linguistic capabilities embodied in LLMs; (2) concept erasure methods, which aim at preventing systemic bias in generated text; and (3) attribution methods, which aim at grounding generations into identified textual sources. We argue that direct model editing cannot be trusted as a systematic remedy for the disadvantages inherent to LLMs, and while it has proven potential in improving model explainability, it opens risks by reinforcing the notion that models can be trusted for factuality. We call for cautious promotion and application of model editing as part of the LLM deployment process, and for responsibly limiting the use cases of LLMs to those not relying on editing as a critical component.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1012.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We argue that direct model editing cannot be trusted as a systematic remedy for the disadvantages inherent to LLMs, and while it has proven potential in improving model explainability, it opens risks by reinforcing the notion that models can be trusted for factuality.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We argue that direct model editing cannot be trusted as a systematic remedy for the disadvantages inherent to LLMs, and while it has proven potential in improving model explainability, it opens risks by reinforcing the notion that models can be trusted for factuality.\""
    },
    {
        "title": "A Causal View of Entity Bias in (Large) Language Models",
        "authors": [
            "Fei Wang",
            "Wenjie Mo",
            "Yiwei Wang",
            "Wenxuan Zhou",
            "Muhao Chen"
        ],
        "published": "2023",
        "summary": "Entity bias widely affects pretrained (large) language models, causing them to rely on (biased) parametric knowledge to make unfaithful predictions. Although causality-inspired methods have shown great potential to mitigate entity bias, it is hard to precisely estimate the parameters of underlying causal models in practice. The rise of black-box LLMs also makes the situation even worse, because of their inaccessible parameters and uncalibrated logits. To address these problems, we propose a specific structured causal model (SCM) whose parameters are comparatively easier to estimate. Building upon this SCM, we propose causal intervention techniques to mitigate entity bias for both white-box and black-box settings. The proposed causal intervention perturbs the original entity with neighboring entities. This intervention reduces specific biasing information pertaining to the original entity while still preserving sufficient semantic information from similar entities. Under the white-box setting, our training-time intervention improves OOD performance of PLMs on relation extraction (RE) and machine reading comprehension (MRC) by 5.7 points and by 9.1 points, respectively. Under the black-box setting, our in-context intervention effectively reduces the entity-based knowledge conflicts of GPT-3.5, achieving up to 20.5 points of improvement of exact match accuracy on MRC and up to 17.6 points of reduction in memorization ratio on RE.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1013.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Entity bias widely affects pretrained (large) language models, causing them to rely on (biased) parametric knowledge to make unfaithful predictions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Entity bias widely affects pretrained (large) language models, causing them to rely on (biased) parametric knowledge to make unfaithful predictions.\""
    },
    {
        "title": "T-Projection: High Quality Annotation Projection for Sequence Labeling Tasks",
        "authors": [
            "Iker García-Ferrero",
            "Rodrigo Agerri",
            "German Rigau"
        ],
        "published": "2023",
        "summary": "In the absence of readily available labeled data for a given sequence labeling task and language, annotation projection has been proposed as one of the possible strategies to automatically generate annotated data. Annotation projection has often been formulated as the task of transporting, on parallel corpora, the labels pertaining to a given span in the source language into its corresponding span in the target language. In this paper we present T-Projection, a novel approach for annotation projection that leverages large pretrained text2text language models and state-of-the-art machine translation technology. T-Projection decomposes the label projection task into two subtasks: (i) A candidate generation step, in which a set of projection candidates using a multilingual T5 model is generated and, (ii) a candidate selection step, in which the generated candidates are ranked based on translation probabilities. We conducted experiments on intrinsic and extrinsic tasks in 5 Indo-European and 8 low-resource African languages. We demostrate that T-projection outperforms previous annotation projection methods by a wide margin. We believe that T-Projection can help to automatically alleviate the lack of high-quality training data for sequence labeling tasks. Code and data are publicly available.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1015.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"leverages large pretrained text2text language models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"leverages large pretrained text2text language models\""
    },
    {
        "title": "MTGER: Multi-view Temporal Graph Enhanced Temporal Reasoning over Time-Involved Document",
        "authors": [
            "Zheng Chu",
            "Zekun Wang",
            "Jiafeng Liang",
            "Ming Liu",
            "Bing Qin"
        ],
        "published": "2023",
        "summary": "The facts and time in the document are intricately intertwined, making temporal reasoning over documents challenging. Previous work models time implicitly, making it difficult to handle such complex relationships. To address this issue, we propose MTGER, a novel Multi-view Temporal Graph Enhanced Reasoning framework for temporal reasoning over time-involved documents. Concretely, MTGER explicitly models the temporal relationships among facts by multi-view temporal graphs. On the one hand, the heterogeneous temporal graphs explicitly model the temporal and discourse relationships among facts; on the other hand, the multi-view mechanism captures both time-focused and fact-focused information, allowing the two views to complement each other through adaptive fusion. To further improve the implicit reasoning capability of the model, we design a self-supervised time-comparing objective. Extensive experimental results demonstrate the effectiveness of our method on the TimeQA and SituatedQA datasets. Furthermore, MTGER gives more consistent answers under question perturbations.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1016.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Masked Path Modeling for Vision-and-Language Navigation",
        "authors": [
            "Zi-Yi Dou",
            "Feng Gao",
            "Nanyun Peng"
        ],
        "published": "2023",
        "summary": "Vision-and-language navigation (VLN) agents are trained to navigate in real-world environments based on natural language instructions. A major challenge in VLN is the limited available training data, which hinders the models’ ability to generalize effectively. Previous approaches have attempted to alleviate this issue by using external tools to generate pseudo-labeled data or integrating web-scaled image-text pairs during training. However, these methods often rely on automatically-generated or out-of-domain data, leading to challenges such as suboptimal data quality and domain mismatch. In this paper, we introduce a masked path modeling (MPM) objective. MPM pretrains an agent using self-collected data for subsequent navigation tasks, eliminating the need for external tools. Specifically, our method allows the agent to explore navigation environments and record the paths it traverses alongside the corresponding agent actions. Subsequently, we train the agent on this collected data to reconstruct the original action sequence when given a randomly masked subsequence of the original path. This approach enables the agent to accumulate a diverse and substantial dataset, facilitating the connection between visual observations of paths and the agent’s actions, which is the foundation of the VLN task. Importantly, the collected data are in-domain, and the training process avoids synthetic data with uncertain quality, addressing previous issues. We conduct experiments on various VLN datasets and demonstrate the applications of MPM across different levels of instruction complexity. Our results exhibit significant improvements in success rates, with enhancements of 1.3%, 1.1%, and 1.2% on the val-unseen split of the Room-to-Room, Room-for-Room, and Room-across-Room datasets, respectively. Additionally, we underscore the adaptability of MPM as well as the potential for additional improvements when the agent is allowed to explore unseen environments prior to testing.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1019.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Learning Interpretable Style Embeddings via Prompting LLMs",
        "authors": [
            "Ajay Patel",
            "Delip Rao",
            "Ansh Kothary",
            "Kathleen McKeown",
            "Chris Callison-Burch"
        ],
        "published": "2023",
        "summary": "Style representation learning builds content-independent representations of author style in text. To date, no large dataset of texts with stylometric annotations on a wide range of style dimensions has been compiled, perhaps because the linguistic expertise to perform such annotation would be prohibitively expensive. Therefore, current style representation approaches make use of unsupervised neural methods to disentangle style from content to create style vectors. These approaches, however, result in uninterpretable representations, complicating their usage in downstream applications like authorship attribution where auditing and explainability is critical. In this work, we use prompting to perform stylometry on a large number of texts to generate a synthetic stylometry dataset. We use this synthetic data to then train human-interpretable style representations we call LISA embeddings. We release our synthetic dataset (StyleGenome) and our interpretable style embedding model (LISA) as resources.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1020.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper discusses the limitations of current style representation approaches, which might be indirectly related to LLMs, however, it does not explicitly discuss LLM limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper discusses the limitations of current style representation approaches, which might be indirectly related to LLMs, however, it does not explicitly discuss LLM limitations."
    },
    {
        "title": "GRACE: Discriminator-Guided Chain-of-Thought Reasoning",
        "authors": [
            "Muhammad Khalifa",
            "Lajanugen Logeswaran",
            "Moontae Lee",
            "Honglak Lee",
            "Lu Wang"
        ],
        "published": "2023",
        "summary": "In the context of multi-step reasoning, e.g., with chain-of-thought, language models (LMs) can easily assign a high likelihood to incorrect steps. As a result, decoding strategies that optimize for solution likelihood often yield incorrect solutions. To address this issue, we propose Guiding chain-of-thought ReAsoning with a CorrectnEss Discriminator (GRACE), a stepwise decoding approach that steers the decoding process towards producing correct reasoning steps. GRACE employs a discriminator trained with a contrastive loss over correct and incorrect steps, which is used during decoding to score next-step candidates based on their correctness. Importantly, GRACE only requires sampling from the LM, without the need for LM training or fine-tuning. Using models from FLAN-T5 and LLaMA families, we evaluate GRACE over four math and two symbolic reasoning tasks, where it exhibits substantial performance gains compared to greedy decoding, verifiers, and self-consistency in most settings. When further combined with self-consistency, GRACE outperforms all the baselines by sizeable margins. Human and LLM evaluations over GSM8K show that GRACE not only improves the final answer accuracy but also the correctness of the intermediate reasoning.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1022.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"In the context of multi-step reasoning, e.g., with chain-of-thought, language models (LMs) can easily assign a high likelihood to incorrect steps.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"In the context of multi-step reasoning, e.g., with chain-of-thought, language models (LMs) can easily assign a high likelihood to incorrect steps.\""
    },
    {
        "title": "QADYNAMICS: Training Dynamics-Driven Synthetic QA Diagnostic for Zero-Shot Commonsense Question Answering",
        "authors": [
            "Haochen Shi",
            "Weiqi Wang",
            "Tianqing Fang",
            "Baixuan Xu",
            "Wenxuan Ding",
            "Xin Liu",
            "Yangqiu Song"
        ],
        "published": "2023",
        "summary": "Zero-shot commonsense Question-Answering (QA) requires models to reason about general situations beyond specific benchmarks. State-of-the-art approaches fine-tune language models on QA pairs constructed from CommonSense Knowledge Bases (CSKBs) to equip the models with more commonsense knowledge in a QA context. However, current QA synthesis protocols may introduce noise from the CSKBs and generate ungrammatical questions and false negative options, which impede the model’s ability to generalize. To address these issues, we propose QADYNAMICS, a training dynamics-driven framework for QA diagnostics and refinement. Our approach analyzes the training dynamics of each QA pair at both the question level and option level, discarding machine-detectable artifacts by removing uninformative QA pairs and mislabeled or false-negative options. Extensive experiments demonstrate the effectiveness of our approach, which outperforms all baselines while using only 33% of the synthetic data, even including LLMs such as ChatGPT. Moreover, expert evaluations confirm that our framework significantly improves the quality of QA synthesis. Our code and model checkpoints are available at https://github.com/HKUST-KnowComp/QaDynamics.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1023.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, current QA synthesis protocols may introduce noise from the CSKBs and generate ungrammatical questions and false negative options, which impede the model’s ability to generalize.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, current QA synthesis protocols may introduce noise from the CSKBs and generate ungrammatical questions and false negative options, which impede the model’s ability to generalize.\""
    },
    {
        "title": "Does Listener Gaze in Face-to-Face Interaction Follow the Entropy Rate Constancy Principle: An Empirical Study",
        "authors": [
            "Yu Wang",
            "Hendrik Buschmeier"
        ],
        "published": "2023",
        "summary": "It is generally assumed that language (written and spoken) follows the entropy rate constancy (ERC) principle, which states that the information density of a text is constant over time. Recently, this has also been found for nonverbal gestures used in monologue, but it is still unclear whether the ERC principle also applies to listeners’ nonverbal signals. We focus on listeners’ gaze behaviour extracted from video-recorded conversations and trained a transformer-based neural sequence model to process the gaze data of the dialogues and compute its information density. We also compute the information density of the corresponding speech using a pre-trained language model. Our results show (1) that listeners’ gaze behaviour in dialogues roughly follows the ERC principle, as well as (2) a congruence between information density of speech and listeners’ gaze behaviour.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1026.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We also compute the information density of the corresponding speech using a pre-trained language model.\"\n\n(Note: Although this paper uses a pre-trained language model, it does not discuss any limitations of LLMs, hence the rating is 1)",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"We also compute the information density of the corresponding speech using a pre-trained language model.\"\n\n(Note: Although this paper uses a pre-trained language model, it does not discuss any limitations of LLMs, hence the rating is 1)"
    },
    {
        "title": "Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data",
        "authors": [
            "Mubashara Akhtar",
            "Abhilash Shankarampeta",
            "Vivek Gupta",
            "Arpit Patil",
            "Oana Cocarascu",
            "Elena Simperl"
        ],
        "published": "2023",
        "summary": "Numerical data plays a crucial role in various real-world domains like finance, economics, and science. Thus, understanding and reasoning with numbers are essential in these fields. Recent benchmarks have assessed the numerical reasoning abilities of language models, revealing their limitations in limited and specific numerical aspects. In this paper, we propose a complete hierarchical taxonomy for numerical reasoning skills, encompassing over ten reasoning types across four levels: representation, number sense, manipulation, and complex reasoning. We conduct a comprehensive evaluation of state-of-the-art models on all reasoning types. To identify challenging reasoning types for different model types, we develop a diverse and extensive set of numerical probes and measure performance shifts. By employing a semi-automated approach, we focus on the tabular Natural Language Inference (TNLI) task as a case study. While no single model excels in all reasoning types, FlanT5 (few-/zero-shot) and GPT3.5 (few-shot) demonstrate strong overall numerical reasoning skills compared to other models in our probes.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1028.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent benchmarks have assessed the numerical reasoning abilities of language models, revealing their limitations in limited and specific numerical aspects.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Recent benchmarks have assessed the numerical reasoning abilities of language models, revealing their limitations in limited and specific numerical aspects.\""
    },
    {
        "title": "Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks",
        "authors": [
            "Ruixiang Tang",
            "Gord Lueck",
            "Rodolfo Quispe",
            "Huseyin Inan",
            "Janardhan Kulkarni",
            "Xia Hu"
        ],
        "published": "2023",
        "summary": "Large language models have revolutionized the field of NLP by achieving state-of-the-art performance on various tasks. However, there is a concern that these models may disclose information in the training data. In this study, we focus on the summarization task and investigate the membership inference (MI) attack: given a sample and black-box access to a model’s API, it is possible to determine if the sample was part of the training data. We exploit text similarity and the model’s resistance to document modifications as potential MI signals and evaluate their effectiveness on widely used datasets. Our results demonstrate that summarization models are at risk of exposing data membership, even in cases where the reference summary is not available. Furthermore, we discuss several safeguards for training summarization models to protect against MI attacks and discuss the inherent trade-off between privacy and utility.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1029.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, there is a concern that these models may disclose information in the training data.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, there is a concern that these models may disclose information in the training data.\""
    },
    {
        "title": "Extrapolating Multilingual Understanding Models as Multilingual Generators",
        "authors": [
            "Bohong Wu",
            "Fei Yuan",
            "Hai Zhao",
            "Lei Li",
            "Jingjing Xu"
        ],
        "published": "2023",
        "summary": "Multilingual understanding models (or encoder-based), pre-trained via masked language modeling, have achieved promising results on many language understanding tasks (e.g., mBERT). However, these models are not capable of generating high-quality text compared with decoder-based causal language models. Can we transform a pre-trained language understanding model into an effective language generation model? We propose a Semantic-Guided Alignment-then-Denoising (SGA) approach to adapt a multilingual encoder to a multilingual generator with a small number of additional parameters. Experiments show that the proposed approach is an effective adaption method, outperforming widely-used initialization-based methods with gains of 9.4 BLEU on machine translation, 8.1 Rouge-L on question generation, and 5.5 METEOR on story generation on XLM-Rlarge. On the other hand, we observe that XLM-R is still inferior to mBART in supervised settings despite better results on zero-shot settings, indicating that more exploration is required to make understanding models strong generators. Our code is available at https://github.com/chengzhipanpan/XLMR4MT.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1031.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these models are not capable of generating high-quality text compared with decoder-based causal language models... On the other hand, we observe that XLM-R is still inferior to mBART in supervised settings despite better results on zero-shot settings, indicating that more exploration is required to make understanding models strong generators.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these models are not capable of generating high-quality text compared with decoder-based causal language models... On the other hand, we observe that XLM-R is still inferior to mBART in supervised settings despite better results on zero-shot settings, indicating that more exploration is required to make understanding models strong generators.\""
    },
    {
        "title": "SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency",
        "authors": [
            "Jiaxin Zhang",
            "Zhuohang Li",
            "Kamalika Das",
            "Bradley Malin",
            "Sricharan Kumar"
        ],
        "published": "2023",
        "summary": "Hallucination detection is a critical step toward understanding the trustworthiness of modern language models (LMs). To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone. Building upon this discovery, we propose a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking. Our SAC3 approach incorporates additional mechanisms to detect both question-level and model-level hallucinations by leveraging advances including semantically equivalent question perturbation and cross-model response consistency checking. Through extensive and systematic empirical analysis, we demonstrate that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1032.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone.\""
    },
    {
        "title": "Test-Time Self-Adaptive Small Language Models for Question Answering",
        "authors": [
            "Soyeong Jeong",
            "Jinheon Baek",
            "Sukmin Cho",
            "Sung Hwang",
            "Jong Park"
        ],
        "published": "2023",
        "summary": "Recent instruction-finetuned large language models (LMs) have achieved notable performances in various tasks, such as question-answering (QA). However, despite their ability to memorize a vast amount of general knowledge across diverse tasks, they might be suboptimal on specific tasks due to their limited capacity to transfer and adapt knowledge to target tasks. Moreover, further finetuning LMs with labeled datasets is often infeasible due to their absence, but it is also questionable if we can transfer smaller LMs having limited knowledge only with unlabeled test data. In this work, we show and investigate the capabilities of smaller self-adaptive LMs, only with unlabeled test data. In particular, we first stochastically generate multiple answers, and then ensemble them while filtering out low-quality samples to mitigate noise from inaccurate labels. Our proposed self-adaption strategy demonstrates significant performance improvements on benchmark QA datasets with higher robustness across diverse prompts, enabling LMs to stay stable. Code is available at: https://github.com/starsuzi/T-SAS.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1033.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, despite their ability to memorize a vast amount of general knowledge across diverse tasks, they might be suboptimal on specific tasks due to their limited capacity to transfer and adapt knowledge to target tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, despite their ability to memorize a vast amount of general knowledge across diverse tasks, they might be suboptimal on specific tasks due to their limited capacity to transfer and adapt knowledge to target tasks.\""
    },
    {
        "title": "ExpNote: Black-box Large Language Models are better Task Solvers with Experience Notebook",
        "authors": [
            "Wangtao Sun",
            "Xuanqing Yu",
            "Shizhu He",
            "Jun Zhao",
            "Kang Liu"
        ],
        "published": "2023",
        "summary": "Black-box Large Language Models (LLMs) have shown great power in solving various tasks and are considered general problem solvers. However, LLMs still fail in many specific tasks although understand the task instruction. In this paper, we focus on the problem of boosting the ability of black-box LLMs to solve downstream tasks. We propose ExpNote, an automated framework to help LLMs better adapt to unfamiliar tasks through reflecting and noting experiences from training data and retrieving them from external memory during testing. We evaluate ExpNote on multiple tasks and the experimental results demonstrate that the proposed method significantly improves the performance of black-box LLMs. The data and code are available at https://github.com/forangel2014/ExpNote.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1034.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, LLMs still fail in many specific tasks although understand the task instruction.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, LLMs still fail in many specific tasks although understand the task instruction.\""
    },
    {
        "title": "Evaluating Parameter-Efficient Finetuning Approaches for Pre-trained Models on the Financial Domain",
        "authors": [
            "Isabella Olariu",
            "Cedric Lothritz",
            "Jacques Klein",
            "Tegawendé Bissyandé",
            "Siwen Guo",
            "Shohreh Haddadan"
        ],
        "published": "2023",
        "summary": "Large-scale language models with millions, billions, or trillions of trainable parameters are becoming increasingly popular. However, they risk becoming rapidly over-parameterized and the adaptation cost of fully fine-tuning them increases significantly. Storing them becomes progressively impractical as it requires keeping a separate copy of all the fine-tuned weights for each task. By freezing all pre-trained weights during fine-tuning, parameter-efficient tuning approaches have become an appealing alternative to traditional fine-tuning. The performance of these approaches has been evaluated on common NLP tasks of the GLUE benchmark and shown to match full fine-tuning performance, however, their impact is less researched in domain-specific fields such as finance. This work compares the performance of a set of financial BERT-like models to their fully fine-tuned counterparts by leveraging different parameter-efficient tuning methods. We see that results are comparable to traditional fine-tuning while gaining in time and resource efficiency.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1035.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they risk becoming rapidly over-parameterized and the adaptation cost of fully fine-tuning them increases significantly.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, they risk becoming rapidly over-parameterized and the adaptation cost of fully fine-tuning them increases significantly.\""
    },
    {
        "title": "Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model",
        "authors": [
            "Parishad BehnamGhader",
            "Santiago Miret",
            "Siva Reddy"
        ],
        "published": "2023",
        "summary": "Augmenting pretrained language models with retrievers has shown promise in effectively solving common NLP problems, such as language modeling and question answering. In this paper, we evaluate the strengths and weaknesses of popular retriever-augmented language models, namely kNN-LM, REALM, DPR + FiD, Contriever + ATLAS, and Contriever + Flan-T5, in reasoning over retrieved statements across different tasks. Our findings indicate that the simple similarity metric employed by retrievers is insufficient for retrieving all the necessary statements for reasoning. Additionally, the language models do not exhibit strong reasoning even when provided with only the required statements. Furthermore, when combined with imperfect retrievers, the performance of the language models becomes even worse, e.g., Flan-T5’s performance drops by 28.6% when retrieving 5 statements using Contriever. While larger language models improve performance, there is still a substantial room for enhancement. Our further analysis indicates that multihop retrieve-and-read is promising for large language models like GPT-3.5, but does not generalize to other language models like Flan-T5-xxl. The code is available at https://github.com/McGill-NLP/retriever-lm-reasoning.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1036.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our findings indicate that the simple similarity metric employed by retrievers is insufficient for retrieving all the necessary statements for reasoning. Additionally, the language models do not exhibit strong reasoning even when provided with only the required statements. Furthermore, when combined with imperfect retrievers, the performance of the language models becomes even worse, e.g., Flan-T5’s performance drops by 28.6",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Our findings indicate that the simple similarity metric employed by retrievers is insufficient for retrieving all the necessary statements for reasoning. Additionally, the language models do not exhibit strong reasoning even when provided with only the required statements. Furthermore, when combined with imperfect retrievers, the performance of the language models becomes even worse, e.g., Flan-T5’s performance drops by 28.6"
    },
    {
        "title": "BERTwich: Extending BERT’s Capabilities to Model Dialectal and Noisy Text",
        "authors": [
            "Aarohi Srivastava",
            "David Chiang"
        ],
        "published": "2023",
        "summary": "Real-world NLP applications often deal with nonstandard text (e.g., dialectal, informal, or misspelled text). However, language models like BERT deteriorate in the face of dialect variation or noise. How do we push BERT’s modeling capabilities to encompass nonstandard text? Fine-tuning helps, but it is designed for specializing a model to a task and does not seem to bring about the deeper, more pervasive changes needed to adapt a model to nonstandard language. In this paper, we introduce the novel idea of sandwiching BERT’s encoder stack between additional encoder layers trained to perform masked language modeling on noisy text. We find that our approach, paired with recent work on including character-level noise in fine-tuning data, can promote zero-shot transfer to dialectal text, as well as reduce the distance in the embedding space between words and their noisy counterparts.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1037.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, language models like BERT deteriorate in the face of dialect variation or noise.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, language models like BERT deteriorate in the face of dialect variation or noise.\""
    },
    {
        "title": "Revisiting Entropy Rate Constancy in Text",
        "authors": [
            "Vivek Verma",
            "Nicholas Tomlin",
            "Dan Klein"
        ],
        "published": "2023",
        "summary": "The uniform information density (UID) hypothesis states that humans tend to distribute information roughly evenly across an utterance or discourse. Early evidence in support of the UID hypothesis came from Genzel and Charniak (2002), which proposed an entropy rate constancy principle based on the probability of English text under n-gram language models. We re-evaluate the claims of Genzel and Charniak (2002) with neural language models, failing to find clear evidence in support of entropy rate constancy. We conduct a range of experiments across datasets, model sizes, and languages and discuss implications for the uniform information density hypothesis and linguistic theories of efficient communication more broadly.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1039.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"failing to find clear evidence in support of entropy rate constancy.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"failing to find clear evidence in support of entropy rate constancy.\""
    },
    {
        "title": "Calibrated Seq2seq Models for Efficient and Generalizable Ultra-fine Entity Typing",
        "authors": [
            "Yanlin Feng",
            "Adithya Pratapa",
            "David Mortensen"
        ],
        "published": "2023",
        "summary": "Ultra-fine entity typing plays a crucial role in information extraction by predicting fine-grained semantic types for entity mentions in text. However, this task poses significant challenges due to the massive number of entity types in the output space. The current state-of-the-art approaches, based on standard multi-label classifiers or cross-encoder models, suffer from poor generalization performance or inefficient inference speed. In this paper, we present CASENT, a seq2seq model designed for ultra-fine entity typing that predicts ultra-fine types with calibrated confidence scores. Our model takes an entity mention as input and employs constrained beam search to generate multiple types autoregressively. The raw sequence probabilities associated with the predicted types are then transformed into confidence scores using a novel calibration method. We conduct extensive experiments on the UFET dataset which contains over 10k types. Our method outperforms the previous state-of-the-art in terms of F1 score and calibration error, while achieving an inference speedup of over 50 times. Additionally, we demonstrate the generalization capabilities of our model by evaluating it in zero-shot and few-shot settings on five specialized domain entity typing datasets that are unseen during training. Remarkably, our model outperforms large language models with 10 times more parameters in the zero-shot setting, and when fine-tuned on 50 examples, it significantly outperforms ChatGPT on all datasets.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1040.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Remarkably, our model outperforms large language models with 10 times more parameters in the zero-shot setting, and when fine-tuned on 50 examples, it significantly outperforms ChatGPT on all datasets.\"\n\nThis rating is given because the abstract mentions a limitation of large language models (having more parameters but performing poorly in certain settings) in passing, but the primary",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Remarkably, our model outperforms large language models with 10 times more parameters in the zero-shot setting, and when fine-tuned on 50 examples, it significantly outperforms ChatGPT on all datasets.\"\n\nThis rating is given because the abstract mentions a limitation of large language models (having more parameters but performing poorly in certain settings) in passing, but the primary"
    },
    {
        "title": "Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models",
        "authors": [
            "Paul Youssef",
            "Osman Koraş",
            "Meijie Li",
            "Jörg Schlötterer",
            "Christin Seifert"
        ],
        "published": "2023",
        "summary": "Pre-trained Language Models (PLMs) are trained on vast unlabeled data, rich in world knowledge. This fact has sparked the interest of the community in quantifying the amount of factual knowledge present in PLMs, as this explains their performance on downstream tasks, and potentially justifies their use as knowledge bases. In this work, we survey methods and datasets that are used to probe PLMs for factual knowledge. Our contributions are: (1) We propose a categorization scheme for factual probing methods that is based on how their inputs, outputs and the probed PLMs are adapted; (2) We provide an overview of the datasets used for factual probing; (3) We synthesize insights about knowledge retention and prompt optimization in PLMs, analyze obstacles to adopting PLMs as knowledge bases and outline directions for future work.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1043.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"analyze obstacles to adopting PLMs as knowledge bases\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"analyze obstacles to adopting PLMs as knowledge bases\""
    },
    {
        "title": "Is ChatGPT the ultimate Data Augmentation Algorithm?",
        "authors": [
            "Frédéric Piedboeuf",
            "Philippe Langlais"
        ],
        "published": "2023",
        "summary": "In the aftermath of GPT-3.5, commonly known as ChatGPT, research have attempted to assess its capacity for lowering annotation cost, either by doing zero-shot learning, generating new data, or replacing human annotators. Some studies have also investigated its use for data augmentation (DA), but only in limited contexts, which still leaves the question of how ChatGPT performs compared to state-of-the-art algorithms. In this paper, we use ChatGPT to create new data both with paraphrasing and with zero-shot generation, and compare it to seven other algorithms. We show that while ChatGPT performs exceptionally well on some simpler data, it overall does not perform better than the other algorithms, yet demands a much larger implication from the practitioner due to the ChatGPT often refusing to answer due to sensitive content in the datasets.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1044.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"yet demands a much larger implication from the practitioner due to the ChatGPT often refusing to answer due to sensitive content in the datasets.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"yet demands a much larger implication from the practitioner due to the ChatGPT often refusing to answer due to sensitive content in the datasets.\""
    },
    {
        "title": "Enhanced Simultaneous Machine Translation with Word-level Policies",
        "authors": [
            "Kang Kim",
            "Hankyu Cho"
        ],
        "published": "2023",
        "summary": "Recent years have seen remarkable advances in the field of Simultaneous Machine Translation (SiMT) due to the introduction of innovative policies that dictate whether to READ or WRITE at each step of the translation process. However, a common assumption in many existing studies is that operations are carried out at the subword level, even though the standard unit for input and output in most practical scenarios is typically at the word level. This paper demonstrates that policies devised and validated at the subword level are surpassed by those operating at the word level, which process multiple subwords to form a complete word in a single step. Additionally, we suggest a method to boost SiMT models using language models (LMs), wherein the proposed word-level policy plays a vital role in addressing the subword disparity between LMs and SiMT models. Code is available at https://github.com/xl8-ai/WordSiMT.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1045.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"addressing the subword disparity between LMs and SiMT models.\"\n\nThis paper mentions LLMs, but only briefly and in the context of a proposed solution, without discussing any specific limitations of LLMs in detail.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"addressing the subword disparity between LMs and SiMT models.\"\n\nThis paper mentions LLMs, but only briefly and in the context of a proposed solution, without discussing any specific limitations of LLMs in detail."
    },
    {
        "title": "Unveiling the Power of Argument Arrangement in Online Persuasive Discussions",
        "authors": [
            "Nailia Mirzakhmedova",
            "Johannes Kiesel",
            "Khalid Al-Khatib",
            "Benno Stein"
        ],
        "published": "2023",
        "summary": "Previous research on argumentation in online discussions has largely focused on examining individual comments and neglected the interactive nature of discussions. In line with previous work, we represent individual comments as sequences of semantic argumentative unit types. However, because it is intuitively necessary for dialogical argumentation to address the opposing viewpoints, we extend this model by clustering type sequences into different argument arrangement patterns and representing discussions as sequences of these patterns. These sequences of patterns are a symbolic representation of argumentation strategies that capture the overall structure of discussions. Using this novel approach, we conduct an in-depth analysis of the strategies in 34,393 discussions from the online discussion forum Change My View and show that our discussion model is effective for persuasiveness prediction, outperforming LLM-based classifiers on the same data. Our results provide valuable insights into argumentation dynamics in online discussions and, through the presented prediction procedure, are of practical importance for writing assistance and persuasive text generation systems.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1048.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"outperforming LLM-based classifiers on the same data.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"outperforming LLM-based classifiers on the same data.\""
    },
    {
        "title": "VER: Unifying Verbalizing Entities and Relations",
        "authors": [
            "Jie Huang",
            "Kevin Chang"
        ],
        "published": "2023",
        "summary": "Entities and relationships between entities are vital in the real world. Essentially, we understand the world by understanding entities and relations. For instance, to understand a field, e.g., computer science, we need to understand the relevant concepts, e.g., machine learning, and the relationships between concepts, e.g., machine learning and artificial intelligence. To understand a person, we should first know who he/she is and how he/she is related to others. To understand entities and relations, humans may refer to natural language descriptions. For instance, when learning a new scientific term, people usually start by reading its definition in dictionaries or encyclopedias. To know the relationship between two entities, humans tend to create a sentence to connect them. In this paper, we propose VER: a unified model for Verbalizing Entities and Relations. Specifically, we attempt to build a system that takes any entity or entity set as input and generates a sentence to represent entities and relations. Extensive experiments demonstrate that our model can generate high-quality sentences describing entities and entity relationships and facilitate various tasks on entities and relations, including definition modeling, relation modeling, and generative commonsense reasoning.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1051.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "The Linearity of the Effect of Surprisal on Reading Times across Languages",
        "authors": [
            "Weijie Xu",
            "Jason Chon",
            "Tianran Liu",
            "Richard Futrell"
        ],
        "published": "2023",
        "summary": "In psycholinguistics, surprisal theory posits that the amount of online processing effort expended by a human comprehender per word positively correlates with the surprisal of that word given its preceding context. In addition to this overall correlation, more importantly, the specific quantitative form taken by the processing effort as a function of surprisal offers insights into the underlying cognitive mechanisms of language processing. Focusing on English, previous studies have looked into the linearity of surprisal on reading times. Here, we extend the investigation by examining eyetracking corpora of seven languages: Danish, Dutch, English, German, Japanese, Mandarin, and Russian. We find evidence for superlinearity in some languages, but the results are highly sensitive to which language model is used to estimate surprisal.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1052.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find evidence for superlinearity in some languages, but the results are highly sensitive to which language model is used to estimate surprisal.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We find evidence for superlinearity in some languages, but the results are highly sensitive to which language model is used to estimate surprisal.\""
    },
    {
        "title": "Adversarial Text Generation by Search and Learning",
        "authors": [
            "Guoyi Li",
            "Bingkang Shi",
            "Zongzhen Liu",
            "Dehan Kong",
            "Yulei Wu",
            "Xiaodan Zhang",
            "Longtao Huang",
            "Honglei Lyu"
        ],
        "published": "2023",
        "summary": "Recent research has shown that evaluating the robustness of natural language processing models using textual attack methods is significant. However, most existing text attack methods only use heuristic replacement strategies or language models to generate replacement words at the word level. The blind pursuit of high attack success rates makes it difficult to ensure the quality of the generated adversarial text. As a result, adversarial text is often difficult for humans to understand. In fact, many methods that perform well in terms of text attacks often generate adversarial text with poor quality. To address this important gap, our work treats black-box text attack as an unsupervised text generation problem and proposes a search and learning framework for Adversarial Text Generation by Search and Learning (ATGSL) and develops three adversarial attack methods (ATGSL-SA, ATGSL-BM, ATGSL-FUSION) for black box text attacks. We first apply a heuristic search attack algorithm (ATGSL-SA) and a linguistic thesaurus to generate adversarial samples with high semantic similarity. After this process, we train a conditional generative model to learn from the search results while smoothing out search noise. Moreover, we design an efficient ATGSL-BM attack algorithm based on the text generator. Furthermore, we propose a hybrid attack method (ATGSL-FUSION) that integrates the advantages of ATGSL-SA and ATGSL-BM to enhance attack effectiveness. Our proposed attack algorithms are significantly superior to the most advanced methods in terms of attack efficiency and adversarial text quality.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1053.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Measuring Pointwise 𝒱-Usable Information In-Context-ly",
        "authors": [
            "Sheng Lu",
            "Shan Chen",
            "Yingya Li",
            "Danielle Bitterman",
            "Guergana Savova",
            "Iryna Gurevych"
        ],
        "published": "2023",
        "summary": "In-context learning (ICL) is a new learning paradigm that has gained popularity along with the development of large language models. In this work, we adapt a recently proposed hardness metric, pointwise 𝒱-usable information (PVI), to an in-context version (in-context PVI). Compared to the original PVI, in-context PVI is more efficient in that it requires only a few exemplars and does not require fine-tuning. We conducted a comprehensive empirical analysis to evaluate the reliability of in-context PVI. Our findings indicate that in-context PVI estimates exhibit similar characteristics to the original PVI. Specific to the in-context setting, we show that in-context PVI estimates remain consistent across different exemplar selections and numbers of shots. The variance of in-context PVI estimates across different exemplar selections is insignificant, which suggests that in-context PVI estimates are stable. Furthermore, we demonstrate how in-context PVI can be employed to identify challenging instances. Our work highlights the potential of in-context PVI and provides new insights into the capabilities of ICL.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1054.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"In-context learning (ICL) is a new learning paradigm that has gained popularity along with the development of large language models.\"\n\nThis paper discusses LLMs but does not mention any limitations of the models in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"In-context learning (ICL) is a new learning paradigm that has gained popularity along with the development of large language models.\"\n\nThis paper discusses LLMs but does not mention any limitations of the models in the abstract."
    },
    {
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
            "Dong Zhang",
            "Shimin Li",
            "Xin Zhang",
            "Jun Zhan",
            "Pengyu Wang",
            "Yaqian Zhou",
            "Xipeng Qiu"
        ],
        "published": "2023",
        "summary": "Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-modal content. With discrete speech representations, we construct SpeechInstruct, the first large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow cross-modal human instructions and highlight the potential of handling multiple modalities with one model. Code and models are available in https://github.com/0nutation/SpeechGPT. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1055.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer.\"\n\nThis paper mentions a limitation of current speech-language models, but it is not the primary focus of the abstract, and the limitation is not explored in depth.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer.\"\n\nThis paper mentions a limitation of current speech-language models, but it is not the primary focus of the abstract, and the limitation is not explored in depth."
    },
    {
        "title": "A Thorough Examination on Zero-shot Dense Retrieval",
        "authors": [
            "Ruiyang Ren",
            "Yingqi Qu",
            "Jing Liu",
            "Xin Zhao",
            "Qifei Wu",
            "Yuchen Ding",
            "Hua Wu",
            "Haifeng Wang",
            "Ji-Rong Wen"
        ],
        "published": "2023",
        "summary": "Recent years have witnessed the significant advance in dense retrieval (DR) based on powerful pre-trained language models (PLM). DR models have achieved excellent performance in several benchmark datasets, while they are shown to be not as competitive as traditional sparse retrieval models (e.g., BM25) in a zero-shot retrieval setting. However, in the related literature, there still lacks a detailed and comprehensive study on zero-shot retrieval. In this paper, we present the first thorough examination of the zero-shot capability of DR models. We aim to identify the key factors and analyze how they affect zero-shot retrieval performance. In particular, we discuss the effect of several key factors related to source training set, analyze the potential bias from the target dataset, and review and compare existing zero-shot DR models. Our findings provide important evidence to better understand and develop zero-shot DR models.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1057.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"DR models have achieved excellent performance in several benchmark datasets, while they are shown to be not as competitive as traditional sparse retrieval models (e.g., BM25) in a zero-shot retrieval setting.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"DR models have achieved excellent performance in several benchmark datasets, while they are shown to be not as competitive as traditional sparse retrieval models (e.g., BM25) in a zero-shot retrieval setting.\""
    },
    {
        "title": "Mitigating Intrinsic Named Entity-Related Hallucinations of Abstractive Text Summarization",
        "authors": [
            "Jianbin Shen",
            "Junyu Xuan",
            "Christy Liang"
        ],
        "published": "2023",
        "summary": "Abstractive text summarization (ATS) is both important and challenging. Recent studies have shown that ATS still faces various forms of hallucination. Our study also indicates that a significant portion of hallucinations is named entity-related. They might appear in different forms, such as mistaken entities and erroneous entity references. The underlying causes implicit in data are complex: data samples pose varying learning conditions. Despite recent research efforts dedicated to named entity-related hallucinations, the solutions have not adequately addressed the varying learning conditions posed by data. This paper aims to bridge the gap in pursuit of reducing intrinsic named entity-related hallucinations. To do so, we propose an adaptive margin ranking loss to facilitate two entity-alignment learning methods to tackle them. Our experiment results show that our methods improve the used baseline model on automatic evaluation scores. The human evaluation also indicates that our methods jointly reduce the intrinsic named entity-related hallucinations considerably compared to the used baseline model.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1059.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Towards Informative Few-Shot Prompt with Maximum Information Gain for In-Context Learning",
        "authors": [
            "Hongfu Liu",
            "Ye Wang"
        ],
        "published": "2023",
        "summary": "Large Language models (LLMs) possess the capability to engage In-context Learning (ICL) by leveraging a few demonstrations pertaining to a new downstream task as conditions. However, this particular learning paradigm suffers from high instability stemming from substantial variances induced by factors such as the input distribution of selected examples, their ordering, and prompt formats. In this work, we demonstrate that even when all these factors are held constant, the random selection of examples still results in high variance. Consequently, we aim to explore the informative ability of data examples by quantifying the Information Gain (IG) obtained in prediction after observing a given example candidate. Then we propose to sample those with maximum IG. Additionally, we identify the presence of template bias, which can lead to unfair evaluations of IG during the sampling process. To mitigate this bias, we introduce Calibration Before Sampling strategy. The experimental results illustrate that our proposed method can yield an average relative improvement of 14.3% across six classification tasks using three LLMs.",
        "pdf_link": "https://aclanthology.org/2023.findings-emnlp.1060.pdf",
        "source": "emnlp2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, this particular learning paradigm suffers from high instability stemming from substantial variances induced by factors such as the input distribution of selected examples, their ordering, and prompt formats.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, this particular learning paradigm suffers from high instability stemming from substantial variances induced by factors such as the input distribution of selected examples, their ordering, and prompt formats.\""
    }
]