[
    {
        "title": "PLOT: Prompt Learning with Optimal Transport for Vision-Language Models",
        "authors": [
            "Guangyi Chen",
            "Weiran Yao",
            "Xiangchen Song",
            "Xinyue Li",
            "Yongming Rao",
            "Kun Zhang"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "With the increasing attention to large vision-language models such as CLIP, there has been a significant amount of effort dedicated to building efficient prompts. Unlike conventional methods of only learning one single prompt, we propose to learn multiple comprehensive prompts to describe diverse characteristics of categories such as intrinsic attributes or extrinsic contexts. However, directly matching each prompt to the same visual feature is problematic, as it pushes the prompts to converge to one point. To solve this problem, we propose to apply optimal transport to match the vision and text modalities. Specifically, we first model images and the categories with visual and textual feature sets. Then, we apply a two-stage optimization strategy to learn the prompts. In the inner loop, we optimize the optimal transport distance to align visual features and prompts by the Sinkhorn algorithm, while in the outer loop, we learn the prompts by this distance from the supervised data. Extensive experiments are conducted on the few-shot recognition task and the improvement demonstrates the superiority of our method. The code is available at https://github.com/CHENGY12/PLOT.",
        "pdf_link": "https://openreview.net/pdf/ddf150416bd1ce46f5512042c2aaa162c8ad10b7.pdf",
        "forum_url": "https://openreview.net/forum?id=zqwryBoXYnh",
        "keywords": [
            "transport",
            "plot",
            "prompt learning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, directly matching each prompt to the same visual feature is problematic, as it pushes the prompts to converge to one point.\"\n\nThis abstract mentions a limitation of large vision-language models (a type of LLM) in passing, specifically the problem of matching prompts to visual features, but does not elaborate on it and focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, directly matching each prompt to the same visual feature is problematic, as it pushes the prompts to converge to one point.\"\n\nThis abstract mentions a limitation of large vision-language models (a type of LLM) in passing, specifically the problem of matching prompts to visual features, but does not elaborate on it and focuses on the proposed solution."
    },
    {
        "title": "Minimax Optimal Kernel Operator Learning via Multilevel Training",
        "authors": [
            "Jikai Jin",
            "Yiping Lu",
            "Jose Blanchet",
            "Lexing Ying"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Learning mappings between infinite-dimensional function spaces have achieved empirical success in many disciplines of machine learning, including generative modeling, functional data analysis, causal inference, and multi-agent reinforcement learning. In this paper, we study the statistical limit of learning a Hilbert-Schmidt operator between two infinite-dimensional Sobolev reproducing kernel Hilbert spaces. We establish the information-theoretic lower bound in terms of the Sobolev Hilbert-Schmidt norm and show that a regularization that learns the spectral components below the bias contour and ignores the ones above the variance contour can achieve the optimal learning rate. At the same time, the spectral components between the bias and variance contours give us flexibility in designing computationally feasible machine learning algorithms. Based on this observation, we develop a multilevel kernel operator learning algorithm that is optimal when learning linear operators between infinite-dimensional function spaces.",
        "pdf_link": "https://openreview.net/pdf/5133e05de4997ce07895732d586909263b656b92.pdf",
        "forum_url": "https://openreview.net/forum?id=zEn1BhaNYsC",
        "keywords": [
            "reproducing kernel hilbert spaces",
            "multilevel kernel operator learning",
            "hilbert schmidt norm"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning",
        "authors": [
            "Olga Golovneva",
            "Moya Peng Chen",
            "Spencer Poff",
            "Martin Corredor",
            "Luke Zettlemoyer",
            "Maryam Fazel-Zarandi",
            "Asli Celikyilmaz"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Large language models show improved downstream task performance when prompted to generate step-by-step reasoning to justify their final answers. These reasoning steps greatly improve model interpretability and verification, but objectively studying their correctness (independent of the final answer) is difficult without reliable methods for automatic evaluation. We simply do not know how often the stated reasoning steps actually support the final end task predictions. In this work, we present ROSCOE, a suite of interpretable, unsupervised automatic scores that improve and extend previous text generation evaluation metrics. To evaluate ROSCOE against baseline metrics, we design a typology of reasoning errors and collect synthetic and human evaluation scores on commonly used reasoning datasets. In contrast with existing metrics, ROSCOE can measure semantic consistency, logicality, informativeness, fluency, and factuality — among other traits — by leveraging properties of step-by-step rationales. We empirically verify the strength of our metrics on five human annotated and six programmatically perturbed diagnostics datasets - covering a diverse set of tasks that require reasoning skills and show that ROSCOE can consistently outperform baseline metrics.",
        "pdf_link": "https://openreview.net/pdf/3f6164615b8f835462171508e65f188740d76ee8.pdf",
        "forum_url": "https://openreview.net/forum?id=xYlJRpzZtsY",
        "keywords": [
            "step reasoning",
            "reasoning steps",
            "reasoning",
            "diagnostics",
            "verification",
            "metrics",
            "reasoning errors",
            "scoring step",
            "typology"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"objectively studying their correctness (independent of the final answer) is difficult without reliable methods for automatic evaluation. We simply do not know how often the stated reasoning steps actually support the final end task predictions.\"\n\nThis abstract mentions a limitation of LLMs in that it is difficult to evaluate the correctness of the reasoning steps generated by LLMs, but it does not elaborate on this",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"objectively studying their correctness (independent of the final answer) is difficult without reliable methods for automatic evaluation. We simply do not know how often the stated reasoning steps actually support the final end task predictions.\"\n\nThis abstract mentions a limitation of LLMs in that it is difficult to evaluate the correctness of the reasoning steps generated by LLMs, but it does not elaborate on this"
    },
    {
        "title": "Moving Forward by Moving Backward: Embedding Action Impact over Action Semantics",
        "authors": [
            "Kuo-Hao Zeng",
            "Luca Weihs",
            "Roozbeh Mottaghi",
            "Ali Farhadi"
        ],
        "published": "ICLR 2023 notable top 5%",
        "summary": "A common assumption when training embodied agents is that the impact of taking an action is stable; for instance, executing the ``move ahead'' action will always move the agent forward by a fixed distance, perhaps with some small amount of actuator-induced noise. This assumption is limiting; an agent may encounter settings that dramatically alter the impact of actions: a move ahead action on a wet floor may send the agent twice as far as it expects and using the same action with a broken wheel might transform the expected translation into a rotation. Instead of relying that the impact of an action stably reflects its pre-defined semantic meaning, we propose to model the impact of actions on-the-fly using latent embeddings. By combining these latent action embeddings with a novel, transformer-based, policy head, we design an Action Adaptive Policy (AAP). We evaluate our AAP on two challenging visual navigation tasks in the AI2-THOR and Habitat environments and show that our AAP is highly performant even when faced, at inference-time, with missing actions and, previously unseen, perturbed action spaces. Moreover, we observe significant improvement in robustness against these actions when evaluating in real-world scenarios.",
        "pdf_link": "https://openreview.net/pdf/5fd307801a722f24990855f8235ae461cabf66fa.pdf",
        "forum_url": "https://openreview.net/forum?id=vmjctNUSWI",
        "keywords": [
            "action semantics",
            "aap",
            "action impact"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Transformers are Sample-Efficient World Models",
        "authors": [
            "Vincent Micheli",
            "Eloi Alonso",
            "François Fleuret"
        ],
        "published": "ICLR 2023 notable top 5%",
        "summary": "Deep reinforcement learning agents are notoriously sample inefficient, which considerably limits their application to real-world problems. Recently, many model-based methods have been designed to address this issue, with learning in the imagination of a world model being one of the most prominent approaches. However, while virtually unlimited interaction with a simulated environment sounds appealing, the world model has to be accurate over extended periods of time. Motivated by the success of Transformers in sequence modeling tasks, we introduce IRIS, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer. With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games, setting a new state of the art for methods without lookahead search. To foster future research on Transformers and world models for sample-efficient reinforcement learning, we release our code and models at https://github.com/eloialonso/iris.",
        "pdf_link": "https://openreview.net/pdf/f23ea2080e754e26ad7f8a9f9a55865dd11f0a73.pdf",
        "forum_url": "https://openreview.net/forum?id=vhFu1Acb0xb",
        "keywords": [
            "transformers",
            "world models",
            "reinforcement learning agents",
            "sample efficient reinforcement learning",
            "sample efficient world models"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "WikiWhy: Answering and Explaining Cause-and-Effect Questions",
        "authors": [
            "Matthew Ho",
            "Aditya Sharma",
            "Justin Chang",
            "Michael Saxon",
            "Sharon Levy",
            "Yujie Lu",
            "William Yang Wang"
        ],
        "published": "ICLR 2023 notable top 5%",
        "summary": "As large language models (LLMs) grow larger and more sophisticated, assessing their \"reasoning\" capabilities in natural language grows more challenging. Recent question answering (QA) benchmarks that attempt to assess reasoning are often limited by a narrow scope of covered situations and subject matters. We introduce WikiWhy, a QA dataset built around a novel auxiliary task: explaining why an answer is true in natural language. WikiWhy contains over 9,000 \"why\" question-answer-rationale triples, grounded on Wikipedia facts across a diverse set of topics. Each rationale is a set of supporting statements connecting the question to the answer. WikiWhy serves as a benchmark for the reasoning capabilities of LLMs because it demands rigorous explicit rationales for each answer to demonstrate the acquisition of implicit commonsense knowledge, which is unlikely to be easily memorized. GPT-3 baselines achieve only 38.7% human-evaluated correctness in the end-to-end answer & explain condition, leaving significant room for future improvements.",
        "pdf_link": "https://openreview.net/pdf/dd230e9938db73b0fff7ee629cb682af034688fc.pdf",
        "forum_url": "https://openreview.net/forum?id=vaxnu-Utr4l",
        "keywords": [
            "large language models",
            "effect questions",
            "natural language",
            "cause",
            "answering",
            "wikiwhy"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"GPT-3 baselines achieve only 38.7% human-evaluated correctness in the end-to-end answer & explain condition, leaving significant room for future improvements.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"GPT-3 baselines achieve only 38.7% human-evaluated correctness in the end-to-end answer & explain condition, leaving significant room for future improvements.\""
    },
    {
        "title": "Generative Augmented Flow Networks",
        "authors": [
            "Ling Pan",
            "Dinghuai Zhang",
            "Aaron Courville",
            "Longbo Huang",
            "Yoshua Bengio"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "The Generative Flow Network is a probabilistic framework where an agent learns a stochastic policy for object generation, such that the probability of generating an object is proportional to a given reward function. Its effectiveness has been shown in discovering high-quality and diverse solutions, compared to reward-maximizing reinforcement learning-based methods. Nonetheless, GFlowNets only learn from rewards of the terminal states, which can limit its applicability. Indeed, intermediate rewards play a critical role in learning, for example from intrinsic motivation to provide intermediate feedback even in particularly challenging sparse reward tasks. Inspired by this, we propose Generative Augmented Flow Networks (GAFlowNets), a novel learning framework to incorporate intermediate rewards into GFlowNets. We specify intermediate rewards by intrinsic motivation to tackle the exploration problem in sparse reward environments. GAFlowNets can leverage edge-based and state-based intrinsic rewards in a joint way to improve exploration. Based on extensive experiments on the GridWorld task, we demonstrate the effectiveness and efficiency of GAFlowNet in terms of convergence, performance, and diversity of solutions. We further show that GAFlowNet is scalable to a more complex and large-scale molecule generation domain, where it achieves consistent and significant performance improvement.",
        "pdf_link": "https://openreview.net/pdf/6f7969e92eef7ad5bb4561e7dbd141decf138128.pdf",
        "forum_url": "https://openreview.net/forum?id=urF_CBK5XC0",
        "keywords": [
            "augmented flow networks"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Learning About Progress From Experts",
        "authors": [
            "Jake Bruce",
            "Ankit Anand",
            "Bogdan Mazoure",
            "Rob Fergus"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Many important tasks involve some notion of long-term progress in multiple phases: e.g. to clean a shelf it must be cleared of items, cleaning products applied, and then the items placed back on the shelf. In this work, we explore the use of expert demonstrations in long-horizon tasks to learn a monotonically increasing function that summarizes progress. This function can then be used to aid agent exploration in environments with sparse rewards. As a case study we consider the NetHack environment, which requires long-term progress at a variety of scales and is far from being solved by existing approaches. In this environment, we demonstrate that by learning a model of long-term progress from expert data containing only observations, we can achieve efficient exploration in challenging sparse tasks, well beyond what is possible with current state-of-the-art approaches. We have made the curated gameplay dataset used in this work available at https://github.com/deepmind/nao_top10.",
        "pdf_link": "https://openreview.net/pdf/6c536f4956e9ba6881ac817930a21ec1d8526220.pdf",
        "forum_url": "https://openreview.net/forum?id=sKc6fgce1zs",
        "keywords": [
            "agent exploration"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Towards Effective and Interpretable Human-Agent Collaboration in MOBA Games: A Communication Perspective",
        "authors": [
            "Yiming Gao",
            "Feiyu Liu",
            "Liang Wang",
            "Zhenjie Lian",
            "Weixuan Wang",
            "Siqin Li",
            "Xianliang Wang",
            "Xianhan Zeng",
            "Rundong Wang",
            "jiawei wang",
            "QIANG FU",
            "Yang Wei",
            "Lanxiao Huang",
            "Wei Liu"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "MOBA games, e.g., Dota2 and Honor of Kings, have been actively used as the testbed for the recent AI research on games, and various AI systems have been developed at the human level so far. However, these AI systems mainly focus on how to compete with humans, less on exploring how to collaborate with humans. To this end, this paper makes the first attempt to investigate human-agent collaboration in MOBA games. In this paper, we propose to enable humans and agents to collaborate through explicit communication by designing an efficient and interpretable Meta-Command Communication-based framework, dubbed MCC, for accomplishing effective human-agent collaboration in MOBA games. The MCC framework consists of two pivotal modules: 1) an interpretable communication protocol, i.e., the Meta-Command, to bridge the communication gap between humans and agents; 2) a meta-command value estimator, i.e., the Meta-Command Selector, to select a valuable meta-command for each agent to achieve effective human-agent collaboration. Experimental results in Honor of Kings demonstrate that MCC agents can collaborate reasonably well with human teammates and even generalize to collaborate with different levels and numbers of human teammates. Videos are available at https://sites.google.com/view/mcc-demo.",
        "pdf_link": "https://openreview.net/pdf/05be94f6c3da0d1f97a06aaecf42515ddc07d159.pdf",
        "forum_url": "https://openreview.net/forum?id=q3F0UBAruO",
        "keywords": [
            "collaborate",
            "agent collaboration",
            "human agent collaboration",
            "agents",
            "communication",
            "games",
            "moba games",
            "interpretable communication"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs or language models in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs or language models in the abstract."
    },
    {
        "title": "Learning on Large-scale Text-attributed Graphs via Variational Inference",
        "authors": [
            "Jianan Zhao",
            "Meng Qu",
            "Chaozhuo Li",
            "Hao Yan",
            "Qian Liu",
            "Rui Li",
            "Xing Xie",
            "Jian Tang"
        ],
        "published": "ICLR 2023 notable top 5%",
        "summary": "This paper studies learning on text-attributed graphs (TAGs), where each node is associated with a text description. An ideal solution for such a problem would be integrating both the text and graph structure information with large language models and graph neural networks (GNNs). However, the problem becomes very challenging when graphs are large due to the high computational complexity brought by training large language models and  GNNs together. In this paper, we propose an efficient and effective solution to learning on large text-attributed graphs by fusing graph structure and language learning with a variational Expectation-Maximization (EM) framework, called GLEM. Instead of simultaneously training large language models and GNNs on big graphs, GLEM proposes to alternatively update the two modules in the E-step and M-step. Such a procedure allows training the two modules separately while simultaneously allowing the two modules to interact and mutually enhance each other. Extensive experiments on multiple data sets demonstrate the efficiency and effectiveness of the proposed approach.",
        "pdf_link": "https://openreview.net/pdf/d5933681412eb0329ac9f838744d30d98d4f8c3d.pdf",
        "forum_url": "https://openreview.net/forum?id=q0nmYciuuZN",
        "keywords": [
            "attributed graphs",
            "variational inference",
            "text attributed graphs",
            "large text attributed graphs",
            "graph neural networks"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the problem becomes very challenging when graphs are large due to the high computational complexity brought by training large language models and GNNs together.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the problem becomes very challenging when graphs are large due to the high computational complexity brought by training large language models and GNNs together.\""
    },
    {
        "title": "Post-hoc Concept Bottleneck Models",
        "authors": [
            "Mert Yuksekgonul",
            "Maggie Wang",
            "James Zou"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Concept Bottleneck Models (CBMs) map the inputs onto a set of interpretable concepts (``the bottleneck'') and use the concepts to make predictions. A concept bottleneck enhances interpretability since it can be investigated to understand what concepts the model \"sees\" in an input and which of these concepts are deemed important. However, CBMs are restrictive in practice as they require dense concept annotations in the training data to learn the bottleneck. Moreover, CBMs often do not match the accuracy of an unrestricted neural network, reducing the incentive to deploy them in practice. In this work, we address these limitations of CBMs by introducing Post-hoc Concept Bottleneck models (PCBMs). We show that we can turn any neural network into a PCBM without sacrificing model performance while still retaining the interpretability benefits. When concept annotations are not available on the training data, we show that PCBM can transfer concepts from other datasets or from natural language descriptions of concepts via multimodal models. A key benefit of PCBM is that it enables users to quickly debug and update the model to reduce spurious correlations and improve generalization to new distributions. PCBM allows for global model edits, which can be more efficient than previous works on local interventions that fix a specific prediction. Through a model-editing user study, we show that editing PCBMs via concept-level feedback can provide significant performance gains without using data from the target domain or model retraining.",
        "pdf_link": "https://openreview.net/pdf/bd9522b16fb6b3a1e89ec20c6aa411c7a84f0fb3.pdf",
        "forum_url": "https://openreview.net/forum?id=nA5AZ8CEyow",
        "keywords": [
            "bottleneck",
            "concept bottleneck",
            "concept bottleneck models"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Efficiently Computing Nash Equilibria in Adversarial Team Markov Games",
        "authors": [
            "Fivos Kalogiannis",
            "Ioannis Anagnostides",
            "Ioannis Panageas",
            "Emmanouil-Vasileios Vlatakis-Gkaragkounis",
            "Vaggos Chatziafratis",
            "Stelios Andrew Stavroulakis"
        ],
        "published": "ICLR 2023 notable top 5%",
        "summary": "    Computing Nash equilibrium policies is a central problem in multi-agent reinforcement learning that has received extensive attention both in theory and in practice. However, in light of computational intractability barriers in general-sum games, provable guarantees have been thus far either limited to fully competitive or cooperative scenarios or impose strong assumptions that are difficult to meet in most practical applications.\n    \n    In this work, we depart from those prior results by investigating infinite-horizon \\emph{adversarial team Markov games}, a natural and well-motivated class of games in which a team of identically-interested players---in the absence of any explicit coordination or communication---is competing against an adversarial player. This setting allows for a unifying treatment of zero-sum Markov games and Markov potential games, and serves as a step to model more realistic strategic interactions that feature both competing and cooperative interests. Our main contribution is the first algorithm for computing stationary $\\epsilon$-approximate Nash equilibria in adversarial team Markov games with computational complexity that is polynomial in all the natural parameters of the game, as well as $1/\\epsilon$.\n    \n    The proposed algorithm is based on performing independent policy gradient steps for each player in the team, in tandem with best responses from the side of the adversary; in turn, the policy for the adversary is then obtained by solving a carefully constructed linear program. Our analysis leverages non-standard techniques to establish the KKT optimality conditions for a nonlinear program with nonconvex constraints, thereby leading to a natural interpretation of the induced Lagrange multipliers.",
        "pdf_link": "https://openreview.net/pdf/3e531dec92de6b02fcbeef7a63d114423e73b571.pdf",
        "forum_url": "https://openreview.net/forum?id=mjzm6btqgV",
        "keywords": [
            "nash equilibrium",
            "team markov games",
            "markov potential games",
            "nash equilibria",
            "cooperative interests"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model",
        "authors": [
            "Xi Chen",
            "Xiao Wang",
            "Soravit Changpinyo",
            "AJ Piergiovanni",
            "Piotr Padlewski",
            "Daniel Salz",
            "Sebastian Goodman",
            "Adam Grycner",
            "Basil Mustafa",
            "Lucas Beyer",
            "Alexander Kolesnikov",
            "Joan Puigcerver",
            "Nan Ding",
            "Keran Rong",
            "Hassan Akbari",
            "Gaurav Mishra",
            "Linting Xue",
            "Ashish V Thapliyal",
            "James Bradbury",
            "Weicheng Kuo",
            "Mojtaba Seyedhosseini",
            "Chao Jia",
            "Burcu Karagol Ayan",
            "Carlos Riquelme Ruiz",
            "Andreas Peter Steiner",
            "Anelia Angelova",
            "Xiaohua Zhai",
            "Neil Houlsby",
            "Radu Soricut"
        ],
        "published": "ICLR 2023 notable top 5%",
        "summary": "Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI, a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pretrained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.",
        "pdf_link": "https://openreview.net/pdf/1870a0455d0e7a6ed7d8f02e8e156cf63f5d6b6a.pdf",
        "forum_url": "https://openreview.net/forum?id=mWVoBz4W0u",
        "keywords": [
            "language image model",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations of LLMs, but \"Since existing Transformers for language are much larger than their vision counterparts\" implies a limitation in terms of size and scalability.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations of LLMs, but \"Since existing Transformers for language are much larger than their vision counterparts\" implies a limitation in terms of size and scalability."
    },
    {
        "title": "Emergence of Maps in the Memories of Blind Navigation Agents",
        "authors": [
            "Erik Wijmans",
            "Manolis Savva",
            "Irfan Essa",
            "Stefan Lee",
            "Ari S. Morcos",
            "Dhruv Batra"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Animal navigation research posits that organisms build and maintain internal spa- tial representations, or maps, of their environment. We ask if machines – specifically, artificial intelligence (AI) navigation agents – also build implicit (or ‘mental’) maps. A positive answer to this question would (a) explain the surprising phenomenon in recent literature of ostensibly map-free neural-networks achieving strong performance, and (b) strengthen the evidence of mapping as a fundamental mechanism for navigation by intelligent embodied agents, whether they be biological or artificial. Unlike animal navigation, we can judiciously design the agent’s perceptual system and control the learning paradigm to nullify alternative navigation mechanisms. Specifically, we train ‘blind’ agents – with sensing limited to only egomotion and no other sensing of any kind – to perform PointGoal navigation (‘go to $\\Delta$x, $\\Delta$y’) via reinforcement learning. Our agents are composed of navigation-agnostic components (fully-connected and recurrent neural networks), and our experimental setup provides no inductive bias towards mapping. Despite these harsh conditions, we find that blind agents are (1) surprisingly effective navigators in new environments (∼95% success); (2) they utilize memory over long horizons (remembering ∼1,000 steps of past experience in an episode); (3) this memory enables them to exhibit intelligent behavior (following walls, detecting collisions, taking shortcuts); (4) there is emergence of maps and collision detection neurons in the representations of the environment built by a blind agent as it navigates; and (5) the emergent maps are selective and task dependent (e.g. the agent ‘forgets’ exploratory detours). Overall, this paper presents no new techniques for the AI audience, but a surprising finding, an insight, and an explanation.",
        "pdf_link": "https://openreview.net/pdf/6aff51942ab3664378283e5da2b36db1cd04db62.pdf",
        "forum_url": "https://openreview.net/forum?id=lTt4KjHSsyl",
        "keywords": [
            "maps",
            "agents",
            "navigation",
            "blind navigation agents",
            "blind agents",
            "blind",
            "animal navigation",
            "intelligent",
            "emergence",
            "embodied agents",
            "artificial intelligence",
            "neural networks",
            "memories",
            "reinforcement",
            "collision"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs."
    },
    {
        "title": "Binding Language Models in Symbolic Languages",
        "authors": [
            "Zhoujun Cheng",
            "Tianbao Xie",
            "Peng Shi",
            "Chengzu Li",
            "Rahul Nadkarni",
            "Yushi Hu",
            "Caiming Xiong",
            "Dragomir Radev",
            "Mari Ostendorf",
            "Luke Zettlemoyer",
            "Noah A. Smith",
            "Tao Yu"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at anonymized.",
        "pdf_link": "https://openreview.net/pdf/d226e827fb59bcd4253c7eb8ce07d339ef5d519d.pdf",
        "forum_url": "https://openreview.net/forum?id=lH1PV42cbF",
        "keywords": [
            "language",
            "symbolic languages",
            "binding language models",
            "binding",
            "programming language",
            "grammar"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned, but the paper aims to \"extend its grammar coverage\" and enable tackling \"more diverse questions\", implying that LLMs may have limitations in these aspects.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned, but the paper aims to \"extend its grammar coverage\" and enable tackling \"more diverse questions\", implying that LLMs may have limitations in these aspects."
    },
    {
        "title": "Out-of-Distribution Detection and Selective Generation for Conditional Language Models",
        "authors": [
            "Jie Ren",
            "Jiaming Luo",
            "Yao Zhao",
            "Kundan Krishna",
            "Mohammad Saleh",
            "Balaji Lakshminarayanan",
            "Peter J Liu"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Machine learning algorithms typically assume independent and identically distributed samples in training and at test time (IID). Much work has shown that high-performing ML classifiers can degrade significantly and provide overly-confident, wrong classification predictions,  particularly for out-of-distribution (OOD) inputs. Conditional language models (CLMs) are predominantly trained to classify the next token in an output sequence, and may suffer even worse degradation on OOD inputs as the prediction is done auto-regressively over many steps. Furthermore, the space of potential low-quality outputs is larger as arbitrary text can be generated and it is important to know when to trust the generated output. We present a highly accurate and lightweight OOD detection method for CLMs, and demonstrate its effectiveness on abstractive summarization and translation. We also show how our method can be used under the common and realistic setting of distribution shift for selective generation (analogous to selective prediction for classification) of high-quality outputs, while  automatically abstaining from low-quality ones, enabling safer deployment of generative language models.",
        "pdf_link": "https://openreview.net/pdf/f47874745e38526618ae5e9fd6012d1584ed30a1.pdf",
        "forum_url": "https://openreview.net/forum?id=kJUS5nD0vPB",
        "keywords": [
            "selective generation",
            "conditional language models",
            "distribution detection",
            "selective prediction",
            "machine learning"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Conditional language models (CLMs) are predominantly trained to classify the next token in an output sequence, and may suffer even worse degradation on OOD inputs as the prediction is done auto-regressively over many steps. Furthermore, the space of potential low-quality outputs is larger as arbitrary text can be generated and it is important to know when to trust the generated output.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Conditional language models (CLMs) are predominantly trained to classify the next token in an output sequence, and may suffer even worse degradation on OOD inputs as the prediction is done auto-regressively over many steps. Furthermore, the space of potential low-quality outputs is larger as arbitrary text can be generated and it is important to know when to trust the generated output.\""
    },
    {
        "title": "A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics",
        "authors": [
            "Qing Li",
            "Siyuan Huang",
            "Yining Hong",
            "Yixin Zhu",
            "Ying Nian Wu",
            "Song-Chun Zhu"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Inspired by humans' exceptional ability to master arithmetic and generalize to new problems, we present a new dataset, HINT, to examine machines' capability of learning generalizable concepts at three levels: perception, syntax, and semantics. In HINT, machines are tasked with learning how concepts are perceived from raw signals such as images (i.e., perception), how multiple concepts are structurally combined to form a valid expression (i.e., syntax), and how concepts are realized to afford various reasoning tasks (i.e., semantics), all in a weakly supervised manner. Focusing on systematic generalization, we carefully design a five-fold test set to evaluate both the interpolation and the extrapolation of learned concepts w.r.t the three levels. Further, we design a few-shot learning split to determine whether or not models can rapidly learn new concepts and generalize them to more complex scenarios. To comprehend existing models' limitations, we undertake extensive experiments with various sequence-to-sequence models, including RNNs, Transformers, and GPT-3 (with the chain of thought prompting). The results indicate that current models struggle to extrapolate to long-range syntactic dependency and semantics. Models exhibit a considerable gap toward human-level generalization when evaluated with new concepts in a few-shot setting. Moreover, we discover that it is infeasible to solve HINT by merely scaling up the dataset and the model size; this strategy contributes little to the extrapolation of syntax and semantics. Finally, in zero-shot GPT-3 experiments, the chain of thought prompting exhibits impressive results and significantly boosts the test accuracy. We believe the HINT dataset and the experimental findings are of great interest to the learning community on systematic generalization.%",
        "pdf_link": "https://openreview.net/pdf/09b973c9c84fd934195e0c087cb7af065e9c6829.pdf",
        "forum_url": "https://openreview.net/forum?id=kIPyTuEZuAK",
        "keywords": [
            "generalize",
            "hint",
            "semantics",
            "hint dataset",
            "syntax and semantics",
            "perception",
            "extrapolate",
            "interpolation",
            "design",
            "test accuracy"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"To comprehend existing models' limitations, we undertake extensive experiments with various sequence-to-sequence models, including RNNs, Transformers, and GPT-3 (with the chain of thought prompting). The results indicate that current models struggle to extrapolate to long-range syntactic dependency and semantics. Models exhibit a considerable gap toward human-level generalization when evaluated with new concepts in a few",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"To comprehend existing models' limitations, we undertake extensive experiments with various sequence-to-sequence models, including RNNs, Transformers, and GPT-3 (with the chain of thought prompting). The results indicate that current models struggle to extrapolate to long-range syntactic dependency and semantics. Models exhibit a considerable gap toward human-level generalization when evaluated with new concepts in a few"
    },
    {
        "title": "Visual Classification via Description from Large Language Models",
        "authors": [
            "Sachit Menon",
            "Carl Vondrick"
        ],
        "published": "ICLR 2023 notable top 5%",
        "summary": "Vision-language models such as CLIP have shown promising performance on a variety of recognition tasks using the standard zero-shot classification procedure -- computing similarity between the query image and the embedded words for each category. By only using the category name, they neglect to make use of the rich context of additional information that language affords. The procedure gives no intermediate understanding of why a category is chosen, and furthermore provides no mechanism for adjusting the criteria used towards this decision. We present an alternative framework for classification with VLMs, which we call classification by description. We ask VLMs to check for descriptive features rather than broad categories: to find a tiger, look for its stripes; its claws; and more. By basing decisions on these descriptors, we can provide additional cues that encourage using the features we want to be used. In the process, we can get a clear idea of what the model ``thinks\" it is seeing to make its decision; it gains some level of inherent explainability. We query large language models (e.g., GPT-3) for these descriptors to obtain them in a scalable way. Extensive experiments show our framework has numerous advantages past interpretability. We show improvements in accuracy on ImageNet across distribution shifts; demonstrate the ability to adapt VLMs to recognize concepts unseen during training; and illustrate how descriptors can be edited to effectively mitigate bias compared to the baseline. ",
        "pdf_link": "https://openreview.net/pdf/d171255a976821dd4ebfacb7a012082c4b888b7a.pdf",
        "forum_url": "https://openreview.net/forum?id=jlAjNL8z5cs",
        "keywords": [
            "language models",
            "visual classification",
            "imagenet"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The procedure gives no intermediate understanding of why a category is chosen, and furthermore provides no mechanism for adjusting the criteria used towards this decision.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The procedure gives no intermediate understanding of why a category is chosen, and furthermore provides no mechanism for adjusting the criteria used towards this decision.\""
    },
    {
        "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis",
        "authors": [
            "Erik Nijkamp",
            "Bo Pang",
            "Hiroaki Hayashi",
            "Lifu Tu",
            "Huan Wang",
            "Yingbo Zhou",
            "Silvio Savarese",
            "Caiming Xiong"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.",
        "pdf_link": "https://openreview.net/pdf/003bbce081e6ee9edeead69fcdba6fbe3882de42.pdf",
        "forum_url": "https://openreview.net/forum?id=iaYcJKpY2B_",
        "keywords": [
            "program synthesis",
            "multi turn program synthesis",
            "multi step paradigm",
            "large language model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"though limited training resources and data impede open access to such models.\"\n\nThis abstract mentions a limitation of LLMs (limited training resources and data) in passing, but the primary focus of the paper is on the proposed model and its applications.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"though limited training resources and data impede open access to such models.\"\n\nThis abstract mentions a limitation of LLMs (limited training resources and data) in passing, but the primary focus of the paper is on the proposed model and its applications."
    },
    {
        "title": "CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks",
        "authors": [
            "Tuomas Oikarinen",
            "Tsui-Wei Weng"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "In this paper, we propose CLIP-Dissect, a new technique to automatically describe the function of individual hidden neurons inside vision networks. CLIP-Dissect leverages recent advances in multimodal vision/language models to label internal neurons with open-ended concepts without the need for any labeled data or human examples. We show that CLIP-Dissect provides more accurate descriptions than existing methods for last layer neurons where the ground-truth is available as well as qualitatively good descriptions for hidden layer neurons. In addition, our method is very flexible: it is model agnostic, can easily handle new concepts and can be extended to take advantage of better multimodal models in the future. Finally CLIP-Dissect is computationally efficient and can label all neurons from five layers of ResNet-50 in just 4 minutes, which is more than 10$\\times$ faster than existing methods. Our code is available at https://github.com/Trustworthy-ML-Lab/CLIP-dissect.",
        "pdf_link": "https://openreview.net/pdf/a302e0072a6e15c8c0361c022bb9d3518f1a7127.pdf",
        "forum_url": "https://openreview.net/forum?id=iPWiwWHc1V",
        "keywords": [
            "clip dissect",
            "deep vision"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"CLIP-Dissect leverages recent advances in multimodal vision/language models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"CLIP-Dissect leverages recent advances in multimodal vision/language models\""
    },
    {
        "title": "Neuro-Symbolic Procedural Planning with Commonsense Prompting",
        "authors": [
            "Yujie Lu",
            "Weixi Feng",
            "Wanrong Zhu",
            "Wenda Xu",
            "Xin Eric Wang",
            "Miguel Eckstein",
            "William Yang Wang"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Procedural planning aims to implement complex high-level goals by decomposition into simpler low-level steps. Although procedural planning is a basic skill set for humans in daily life, it remains a challenge for large language models (LLMs) that lack a deep understanding of the cause-effect relations in procedures. Previous methods require manual exemplars to acquire procedural planning knowledge from LLMs in the zero-shot setting. However, such elicited pre-trained knowledge in LLMs induces spurious correlations between goals and steps, which impair the model generalization to unseen tasks. In contrast, this paper proposes a neuro-symbolic procedural PLANner (PLAN) that elicits procedural planning knowledge from the LLMs with commonsense-infused prompting. To mitigate spurious goal-step correlations, we use symbolic program executors on the latent procedural representations to formalize prompts from commonsense knowledge bases as a causal intervention toward the Structural Causal Model. Both automatic and human evaluations on WikiHow and RobotHow show the superiority of PLAN on procedural planning without further training or manual exemplars.",
        "pdf_link": "https://openreview.net/pdf/3af66a16e02e6ec05187d765b1d2da8cabae2719.pdf",
        "forum_url": "https://openreview.net/forum?id=iOc57X9KM54",
        "keywords": [
            "commonsense",
            "procedural planning",
            "commonsense prompting",
            "neuro symbolic procedural planning"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although procedural planning is a basic skill set for humans in daily life, it remains a challenge for large language models (LLMs) that lack a deep understanding of the cause-effect relations in procedures.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Although procedural planning is a basic skill set for humans in daily life, it remains a challenge for large language models (LLMs) that lack a deep understanding of the cause-effect relations in procedures.\""
    },
    {
        "title": "Towards Interpretable Deep Reinforcement Learning with Human-Friendly Prototypes",
        "authors": [
            "Eoin M. Kenny",
            "Mycal Tucker",
            "Julie Shah"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Despite recent success of deep learning models in research settings, their application in sensitive domains remains limited because of their opaque decision-making processes. Taking to this challenge, people have proposed various eXplainable AI (XAI) techniques designed to calibrate trust and understandability of black-box models, with the vast majority of work focused on supervised learning. Here, we focus on making an \"interpretable-by-design\" deep reinforcement learning agent which is forced to use human-friendly prototypes in its decisions, thus making its reasoning process clear. Our proposed method, dubbed Prototype-Wrapper Network (PW-Net), wraps around any neural agent backbone, and results indicate that it does not worsen performance relative to black-box models. Most importantly, we found in a user study that PW-Nets supported better trust calibration and task performance relative to standard interpretability approaches and black-boxes.\n",
        "pdf_link": "https://openreview.net/pdf/89dc907add1447b8730b63f4562410d7d9346676.pdf",
        "forum_url": "https://openreview.net/forum?id=hWwY_Jq0xsN",
        "keywords": [
            "friendly prototypes",
            "human friendly prototypes",
            "prototype wrapper network",
            "reinforcement learning",
            "learning",
            "deep reinforcement learning",
            "trust calibration",
            "agent"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Self-Guided Noise-Free Data Generation for Efficient Zero-Shot Learning",
        "authors": [
            "Jiahui Gao",
            "Renjie Pi",
            "LIN Yong",
            "Hang Xu",
            "Jiacheng Ye",
            "Zhiyong Wu",
            "WEIZHONG ZHANG",
            "Xiaodan Liang",
            "Zhenguo Li",
            "Lingpeng Kong"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "There is a rising interest in further exploring the zero-shot learning potential of large pre-trained language models (PLMs). A new paradigm called data-generation-based zero-shot learning has achieved impressive success. In this paradigm, the synthesized data from the PLM acts as the carrier of knowledge, which is used to train a task-specific model with orders of magnitude fewer parameters than the PLM, achieving both higher performance and efficiency than prompt-based zero-shot learning methods on PLMs. The main hurdle of this approach is that the synthesized data from PLM usually contains a significant portion of low-quality samples. Fitting on such data will greatly hamper the performance of the task-specific model, making it unreliable for deployment. Previous methods remedy this issue mainly by filtering synthetic data using heuristic metrics(e.g., output confidence), or refining the data with the help of a human expert, which comes with excessive manual tuning or expensive costs. In this paper, we propose a novel noise-robust re-weighting framework SunGen to automatically construct high-quality data for zero-shot classification problems. Our framework features the ability to learn the sample weights indicating data quality without requiring any human annotation. We theoretically and empirically verify the ability of our method to help construct good-quality synthetic datasets. Notably, SunGen-LSTM yields a 9.8% relative improvement than the baseline on average accuracy across eight different established text classification tasks.",
        "pdf_link": "https://openreview.net/pdf/82812310fbf1dff5ce1f72fe99e2d46523ca8d5a.pdf",
        "forum_url": "https://openreview.net/forum?id=h5OpjGd_lo6",
        "keywords": [
            "zero shot learning",
            "noise robust"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The main hurdle of this approach is that the synthesized data from PLM usually contains a significant portion of low-quality samples.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The main hurdle of this approach is that the synthesized data from PLM usually contains a significant portion of low-quality samples.\""
    },
    {
        "title": "TEMPERA: Test-Time Prompt Editing via Reinforcement Learning",
        "authors": [
            "Tianjun Zhang",
            "Xuezhi Wang",
            "Denny Zhou",
            "Dale Schuurmans",
            "Joseph E. Gonzalez"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Careful prompt design is critical to the use of large language models in zero-shot or few-shot learning.  As a consequence, there is a growing interest in automated methods to design optimal prompts. In this work, we propose Test-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast to prior prompt generation methods, TEMPERA can efficiently leverage prior knowledge, is adaptive to different queries and provides an interpretable prompt for every query. To achieve this, we design a novel action space that allows flexible editing of the initial prompts covering a wide set of commonly-used components like instructions, few-shot exemplars, and verbalizers. The proposed method achieves significant gains compared with recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a variety of tasks including sentiment analysis, topic classification, natural language inference, and reading comprehension. Our method achieves 5.33x on average improvement in sample efficiency when compared to the traditional fine-tuning methods.",
        "pdf_link": "https://openreview.net/pdf/1cf8438e4df114f4ea13408da5952d5636eabb99.pdf",
        "forum_url": "https://openreview.net/forum?id=gSHyqBijPFO",
        "keywords": [
            "test time",
            "action space",
            "tempera",
            "adaptive",
            "prior knowledge",
            "reinforcement learning"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Careful prompt design is critical to the use of large language models in zero-shot or few-shot learning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Careful prompt design is critical to the use of large language models in zero-shot or few-shot learning.\""
    },
    {
        "title": "Can We Find Nash Equilibria at a Linear Rate in Markov Games?",
        "authors": [
            "Zhuoqing Song",
            "Jason D. Lee",
            "Zhuoran Yang"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "We study decentralized learning in two-player zero-sum discounted Markov games where the goal is to design a policy optimization algorithm for either agent satisfying two properties. First, the player does not need to know the policy of the opponent to update its policy. Second, when both players adopt the algorithm, their joint policy converges to a Nash equilibrium of the game. To this end, we construct a meta-algorithm, dubbed as $\\texttt{Homotopy-PO}$, which provably finds a Nash equilibrium at a global linear rate. In particular, $\\texttt{Homotopy-PO}$ interweaves two base algorithms $\\texttt{Local-Fast}$ and $\\texttt{Global-Slow}$ via homotopy continuation. $\\texttt{Local-Fast}$ is an algorithm that enjoys local linear convergence while $\\texttt{Global-Slow}$ is an algorithm that converges globally but at a slower sublinear rate. By switching between these two base algorithms, $\\texttt{Global-Slow}$ essentially serves as a ``guide'' which identifies a benign neighborhood where $\\texttt{Local-Fast}$ enjoys fast convergence. However, since the exact size of such a neighborhood is unknown, we apply a doubling trick to switch between these two base algorithms. The switching scheme is delicately designed so that the aggregated performance of the algorithm is driven by $\\texttt{Local-Fast}$. Furthermore, we prove that $\\texttt{Local-Fast}$ and $\\texttt{Global-Slow}$ can both be instantiated by variants of optimistic gradient descent/ascent (OGDA) method, which is of independent interest.",
        "pdf_link": "https://openreview.net/pdf/f6285c79bac699974cbfa7334d3e42800b338c64.pdf",
        "forum_url": "https://openreview.net/forum?id=eQzLwwGyQrb",
        "keywords": [
            "nash equilibrium",
            "markov games",
            "nash equilibria",
            "discounted markov games",
            "rate",
            "decentralized learning"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "On the Usefulness of Embeddings, Clusters and Strings for Text Generation Evaluation",
        "authors": [
            "Tiago Pimentel",
            "Clara Isabel Meister",
            "Ryan Cotterell"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "A good automatic evaluation metric for language generation ideally correlates highly with human judgements of text quality.  Yet, there is a dearth of such metrics, which inhibits the rapid and efficient progress of language generators. One exception is  the recently proposed Mauve. In theory, Mauve measures an information-theoretic divergence between two probability distributions over strings: one representing the language generator under evaluation; the other representing the true natural language distribution. Mauve's authors argue that its success comes from the qualitative properties of their proposed divergence.  Yet in practice, as this divergence is uncomputable, Mauve approximates it by measuring the divergence between multinomial distributions over clusters instead, where cluster assignments are attained by grouping strings based on a pretrained language model's embeddings. As we show, however, this is not a tight approximation---in either theory or practice. This begs the question: why does Mauve work so well? In this work, we show that \\mauve was right for the wrong reasons, and that its newly proposed divergence is not necessary for its high performance. In fact, classical divergences paired with its proposed cluster-based approximation may actually serve as better evaluation metrics. We finish the paper with a probing analysis; this analysis leads us to conclude that---by encoding syntactic- and coherence-level features of text, while ignoring surface-level features---such cluster-based approximations to string distributions may simply be better for evaluating state-of-the-art language generators.",
        "pdf_link": "https://openreview.net/pdf/ecbd3cf3099fb64e9d4d2614aa66862f601c3328.pdf",
        "forum_url": "https://openreview.net/forum?id=bvpkw7UIRdU",
        "keywords": [
            "embeddings",
            "language generators",
            "clusters",
            "multinomial distributions",
            "evaluating",
            "evaluation metrics",
            "text generation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As we show, however, this is not a tight approximation---in either theory or practice.\"\n\nNote that this paper does not directly discuss limitations of LLMs but discusses the limitations of a metric (Mauve) that is used to evaluate language generators, which may include LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"As we show, however, this is not a tight approximation---in either theory or practice.\"\n\nNote that this paper does not directly discuss limitations of LLMs but discusses the limitations of a metric (Mauve) that is used to evaluate language generators, which may include LLMs."
    },
    {
        "title": "Ask Me Anything: A simple strategy for prompting language models",
        "authors": [
            "Simran Arora",
            "Avanika Narayan",
            "Mayee F Chen",
            "Laurel Orr",
            "Neel Guha",
            "Kush Bhatia",
            "Ines Chami",
            "Christopher Re"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt that demonstrates how to perform the task and no additional training. Prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions, and therefore significant effort is dedicated towards designing a painstakingly crafted \"perfect prompt\" for a task. To mitigate the high degree of effort, we instead ask whether collecting multiple decent, yet imperfect, prompts and aggregating them can lead to a high quality prompting strategy. Our observations motivate our proposed method, Ask Me Anything (AMA). We first develop an understanding of the effective prompt formats, finding question-answering (QA) prompts, which encourage open-ended generation (\"Who went to the park?\") tend to outperform those that restrict the model outputs (\"John went to the park. True or False?\"). AMA recursively uses the LLM to transform task inputs to the effective QA format. AM generates multiple questions per input and applies these prompts to collect several noisy \"votes\" for the input's true label. We find the prompts have varying accuracies and dependencies and thus propose to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions. We evaluate AMA across open-source model families (EleutherAI, BLOOM, OPT, and T0) and sizes (125M-175B parameters), demonstrating an average performance lift of 10.2\\% over the few-shot baseline. This simple strategy enables the open-source GPT-J-6B model to match and exceed the performance of few-shot GPT3-175B  on 15 of 20 popular benchmarks. Averaged across these tasks, the GPT-J-6B model outperforms few-shot GPT3-175B. We release our code here: https://github.com/HazyResearch/ama_prompting.",
        "pdf_link": "https://openreview.net/pdf/5b1bcdac167fa4b294480f303ac3722afa8a9aac.pdf",
        "forum_url": "https://openreview.net/forum?id=bhUPJnS2g0X",
        "keywords": [
            "prompting language models",
            "ask me anything",
            "prompting strategy",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions\""
    },
    {
        "title": "Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models",
        "authors": [
            "Xiaoman Pan",
            "Wenlin Yao",
            "Hongming Zhang",
            "Dian Yu",
            "Dong Yu",
            "Jianshu Chen"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Fully-parametric language models generally require a huge number of model parameters to store the necessary knowledge for solving multiple natural language tasks in zero/few-shot settings. In addition, it is hard to adapt to the evolving world knowledge without the costly model re-training. In this paper, we develop a novel semi-parametric language model architecture, Knowledge-in-Context (KiC), which empowers a parametric text-to-text language model with a knowledge-rich external memory. Specifically, the external memory contains six different types of knowledge:  entity,  dictionary, commonsense, event, script, and causality knowledge. For each input instance, the KiC model adaptively selects a knowledge type and retrieves the most helpful pieces of knowledge. The input instance along with its knowledge augmentation is fed into a text-to-text model (e.g., T5) to generate the output answer, where both the input and the output are in natural language forms after prompting. Interestingly, we find that KiC can be identified as a special mixture-of-experts (MoE) model, where the knowledge selector plays the role of a router that is used to determine the sequence-to-expert assignment in MoE. This key observation inspires us to develop a novel algorithm for training KiC with an instance-adaptive knowledge selector. As a knowledge-rich semi-parametric language model, KiC only needs a much smaller parametric part to achieve superior zero-shot performance on unseen tasks. By evaluating on 40+ different tasks, we show that KiC-Large with 770M parameters easily outperforms large language models that are 4-39x larger. In addition, KiC also exhibits emergent abilities at a much smaller model scale compared to the fully-parametric models.",
        "pdf_link": "https://openreview.net/pdf/2f1a38a721bba2dcfa96af632678ce02c41b26bd.pdf",
        "forum_url": "https://openreview.net/forum?id=a2jNdqE2102",
        "keywords": [
            "knowledge",
            "language model",
            "knowledge selector",
            "semi",
            "semi parametric language models",
            "parametric language models",
            "commonsense",
            "causality knowledge",
            "algorithm"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Fully-parametric language models generally require a huge number of model parameters to store the necessary knowledge for solving multiple natural language tasks in zero/few-shot settings. In addition, it is hard to adapt to the evolving world knowledge without the costly model re-training.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Fully-parametric language models generally require a huge number of model parameters to store the necessary knowledge for solving multiple natural language tasks in zero/few-shot settings. In addition, it is hard to adapt to the evolving world knowledge without the costly model re-training.\""
    },
    {
        "title": "Symbolic Physics Learner: Discovering governing equations via Monte Carlo tree search",
        "authors": [
            "Fangzheng Sun",
            "Yang Liu",
            "Jian-Xun Wang",
            "Hao Sun"
        ],
        "published": "ICLR 2023 notable top 5%",
        "summary": "Nonlinear dynamics is ubiquitous in nature and commonly seen in various science and engineering disciplines. Distilling analytical expressions that govern nonlinear dynamics from limited data remains vital but challenging. To tackle this fundamental issue, we propose a novel Symbolic Physics Learner (SPL) machine to discover the mathematical structure of nonlinear dynamics. The key concept is to interpret mathematical operations and system state variables by computational rules and symbols, establish symbolic reasoning of mathematical formulas via expression trees, and employ a Monte Carlo tree search (MCTS) agent to explore optimal expression trees based on measurement data. The MCTS agent obtains an optimistic selection policy through the traversal of expression trees, featuring the one that maps to the arithmetic expression of underlying physics. Salient features of the proposed framework include search flexibility and enforcement of parsimony for discovered equations. The efficacy and superiority of the SPL machine are demonstrated by numerical examples, compared with state-of-the-art baselines.",
        "pdf_link": "https://openreview.net/pdf/0c815f206ac64432f9caf1f36b816f9e368dee15.pdf",
        "forum_url": "https://openreview.net/forum?id=ZTK3SefE8_Z",
        "keywords": [
            "monte carlo tree search",
            "nonlinear dynamics",
            "physics",
            "mathematical formulas",
            "symbolic reasoning"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "DocPrompting: Generating Code by Retrieving the Docs",
        "authors": [
            "Shuyan Zhou",
            "Uri Alon",
            "Frank F. Xu",
            "Zhengbao Jiang",
            "Graham Neubig"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Publicly available source-code libraries are continuously growing and changing. This makes it impossible for models of code\nto keep current with all available APIs by simply training these models on existing code repositories. Thus, existing models inherently cannot generalize to using unseen functions and libraries, because these would never appear in the training data. In contrast, when human programmers use functions and libraries for the first time, they frequently refer to textual resources such as code manuals and documentation, to explore and understand the available functionality. Inspired by this observation, we introduce DocPrompting: a natural-language-to-code generation approach that explicitly leverages documentation by (1) retrieving the relevant documentation pieces given an NL intent, and (2) generating code based on the NL intent and the retrieved documentation. DocPrompting is general: it can be applied to any programming language and is agnostic to the underlying neural model. We demonstrate that DocPrompting consistently improves NL-to-code models: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to absolute 6.9% exact match.",
        "pdf_link": "https://openreview.net/pdf/c9881a374e0bce9d005809d63e83dfdae53d9d40.pdf",
        "forum_url": "https://openreview.net/forum?id=ZTCxT2t2Ru",
        "keywords": [],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"existing models inherently cannot generalize to using unseen functions and libraries, because these would never appear in the training data.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"existing models inherently cannot generalize to using unseen functions and libraries, because these would never appear in the training data.\""
    },
    {
        "title": "Dr.Spider: A Diagnostic Evaluation Benchmark towards Text-to-SQL Robustness",
        "authors": [
            "Shuaichen Chang",
            "Jun Wang",
            "Mingwen Dong",
            "Lin Pan",
            "Henghui Zhu",
            "Alexander Hanbo Li",
            "Wuwei Lan",
            "Sheng Zhang",
            "Jiarong Jiang",
            "Joseph Lilien",
            "Steve Ash",
            "William Yang Wang",
            "Zhiguo Wang",
            "Vittorio Castelli",
            "Patrick Ng",
            "Bing Xiang"
        ],
        "published": "ICLR 2023 notable top 5%",
        "summary": "Neural text-to-SQL models have achieved remarkable performance in translating natural language questions into SQL queries. However, recent studies reveal that text-to-SQL models are vulnerable to task-specific perturbations. Previous curated robustness test sets usually focus on individual phenomena. In this paper, we propose a comprehensive robustness benchmark based on Spider, a cross-domain text-to-SQL benchmark, to diagnose the model robustness. We design 17 perturbations on databases, natural language questions, and SQL queries to measure the robustness from different angles. In order to collect more diversified natural question perturbations, we utilize large pretrained language models (PLMs) to simulate human behaviors in creating natural questions. We conduct a diagnostic study of the state-of-the-art models on the robustness set. Experimental results reveal that even the most robust model suffers from a 14.0% performance drop overall and a 50.7% performance drop on the most challenging perturbation. We also present a breakdown analysis regarding text-to-SQL model designs and provide insights for improving model robustness.",
        "pdf_link": "https://openreview.net/pdf/28dd8eb27d485f652c4874af1d995452557ae2b3.pdf",
        "forum_url": "https://openreview.net/forum?id=Wc5bmZZU9cy",
        "keywords": [
            "robustness",
            "robustness benchmark",
            "sql benchmark",
            "language models",
            "sql models",
            "diagnostic evaluation benchmark",
            "text to sql model",
            "text"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, recent studies reveal that text-to-SQL models are vulnerable to task-specific perturbations.\" and \"Experimental results reveal that even the most robust model suffers from a 14.0% performance drop overall and a 50.7% performance drop on the most challenging perturbation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, recent studies reveal that text-to-SQL models are vulnerable to task-specific perturbations.\" and \"Experimental results reveal that even the most robust model suffers from a 14.0% performance drop overall and a 50.7% performance drop on the most challenging perturbation.\""
    },
    {
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
        "authors": [
            "Shunyu Yao",
            "Jeffrey Zhao",
            "Dian Yu",
            "Nan Du",
            "Izhak Shafran",
            "Karthik R Narasimhan",
            "Yuan Cao"
        ],
        "published": "ICLR 2023 notable top 5%",
        "summary": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.",
        "pdf_link": "https://openreview.net/pdf/bc117919562a4ccddbe5c5b24ee364d14289cdee.pdf",
        "forum_url": "https://openreview.net/forum?id=WE_vluYUL-X",
        "keywords": [
            "language models",
            "large language models",
            "action plan generation",
            "synergizing reasoning",
            "acting"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning\""
    },
    {
        "title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation",
        "authors": [
            "Lorenz Kuhn",
            "Yarin Gal",
            "Sebastian Farquhar"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of \"semantic equivalence\"—different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy—an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines. ",
        "pdf_link": "https://openreview.net/pdf/535617ff3bfc4d3297922f5c320dee127d1ce6cc.pdf",
        "forum_url": "https://openreview.net/forum?id=VD-AYtP0dve",
        "keywords": [
            "predictive",
            "uncertainty",
            "semantic uncertainty",
            "uncertainty estimation",
            "natural language generation",
            "semantic entropy"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We show that measuring uncertainty in natural language is challenging because of \"semantic equivalence\"—different sentences can mean the same thing.\"\n\nThis abstract discusses the challenge of measuring uncertainty in LLMs due to semantic equivalence, but the primary focus is on the proposed solution, semantic entropy, rather than exploring the limitation in depth.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We show that measuring uncertainty in natural language is challenging because of \"semantic equivalence\"—different sentences can mean the same thing.\"\n\nThis abstract discusses the challenge of measuring uncertainty in LLMs due to semantic equivalence, but the primary focus is on the proposed solution, semantic entropy, rather than exploring the limitation in depth."
    },
    {
        "title": "Generating Diverse Cooperative Agents by Learning Incompatible Policies",
        "authors": [
            "Rujikorn Charakorn",
            "Poramate Manoonpong",
            "Nat Dilokthanakul"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Training a robust cooperative agent requires diverse partner agents. However, obtaining those agents is difficult. Previous works aim to learn diverse behaviors by changing the state-action distribution of agents. But, without information about the task's goal, the diversified agents are not guided to find other important, albeit sub-optimal, solutions: the agents might learn only variations of the same solution. In this work, we propose to learn diverse behaviors via policy compatibility. Conceptually, policy compatibility measures whether policies of interest can coordinate effectively. We theoretically show that incompatible policies are not similar. Thus, policy compatibility—which has been used exclusively as a measure of robustness—can be used as a proxy for learning diverse behaviors. Then, we incorporate the proposed objective into a population-based training scheme to allow concurrent training of multiple agents. Additionally, we use state-action information to induce local variations of each policy. Empirically, the proposed method consistently discovers more solutions than baseline methods across various multi-goal cooperative environments. Finally, in multi-recipe Overcooked, we show that our method produces populations of behaviorally diverse agents, which enables generalist agents trained with such a population to be more robust.\n\nSee our project page at https://bit.ly/marl-lipo\n",
        "pdf_link": "https://openreview.net/pdf/ac9e4f47a8a7afc2d31fe69575bb97700dd88071.pdf",
        "forum_url": "https://openreview.net/forum?id=UkU05GOH7_6",
        "keywords": [
            "incompatible policies",
            "learning incompatible policies",
            "cooperative agent",
            "diverse cooperative agents",
            "learning diverse",
            "population based training",
            "policy compatibility"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Building a Subspace of Policies for Scalable Continual Learning",
        "authors": [
            "Jean-Baptiste Gaya",
            "Thang Doan",
            "Lucas Caccia",
            "Laure Soulier",
            "Ludovic Denoyer",
            "Roberta Raileanu"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "The ability to continuously acquire new knowledge and skills is crucial for autonomous agents. Existing methods are typically based on either fixed-size models that struggle to learn a large number of diverse behaviors, or growing-size models that scale poorly with the number of tasks. In this work, we aim to strike a better balance between scalability and performance by designing a method whose size grows adaptively depending on the task sequence. We introduce Continual Subspace of Policies (CSP), a new approach that incrementally builds a subspace of policies for training a reinforcement learning agent on a sequence of tasks. The subspace's high expressivity allows CSP to perform well for many different tasks while growing more slowly than the number of tasks. Our method does not suffer from forgetting and also displays positive transfer to new tasks. CSP outperforms a number of popular baselines on a wide range of scenarios from two challenging domains, Brax (locomotion) and Continual World (robotic manipulation). Interactive visualizations of the subspace can be found at https://share.streamlit.io/continual-subspace/policies/main.",
        "pdf_link": "https://openreview.net/pdf/ab8b649c5427a1281f061c035a6c4c3a82699f57.pdf",
        "forum_url": "https://openreview.net/forum?id=UKr0MwZM6fL",
        "keywords": [
            "continual subspace of policies",
            "subspace of policies",
            "reinforcement learning",
            "scalable continual learning"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Quantifying Memorization Across Neural Language Models",
        "authors": [
            "Nicholas Carlini",
            "Daphne Ippolito",
            "Matthew Jagielski",
            "Katherine Lee",
            "Florian Tramer",
            "Chiyuan Zhang"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others).\nWe describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.",
        "pdf_link": "https://openreview.net/pdf/6b4201e769d9dc79c8462750821d94951ee50a84.pdf",
        "forum_url": "https://openreview.net/forum?id=TatRHT_1cK",
        "keywords": [
            "memorization",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes complicated when generalizing these results across model families. On the whole, we find that memorization in LMs",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes complicated when generalizing these results across model families. On the whole, we find that memorization in LMs"
    },
    {
        "title": "Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs",
        "authors": [
            "Albert Qiaochu Jiang",
            "Sean Welleck",
            "Jin Peng Zhou",
            "Timothee Lacroix",
            "Jiacheng Liu",
            "Wenda Li",
            "Mateja Jamnik",
            "Guillaume Lample",
            "Yuhuai Wu"
        ],
        "published": "ICLR 2023 notable top 5%",
        "summary": "The formalization of existing mathematical proofs is a notoriously difficult process. Despite decades of research on automation and proof assistants, writing formal proofs remains arduous and only accessible to a few experts. While previous studies to automate formalization focused on powerful search algorithms, no attempts were made to take advantage of available informal proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems. We investigate two relevant setups where informal proofs are either written by humans or generated by a language model. Our experiments and ablation studies show that large language models are able to produce well-structured formal sketches that follow the same reasoning steps as the informal proofs. Guiding an automated prover with these sketches enhances its performance from $20.9\\%$ to $39.3\\%$ on a collection of mathematical competition problems.",
        "pdf_link": "https://openreview.net/pdf/cfd03f19d20263d9c1d1cc026a2b3528392fc857.pdf",
        "forum_url": "https://openreview.net/forum?id=SMa9EAovKMC",
        "keywords": [
            "informal proofs",
            "proofs",
            "formal theorem provers",
            "formal proofs",
            "formal proof sketches",
            "sketches",
            "automate formalization",
            "automated",
            "maps informal proofs",
            "draft"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned, but LLMs are used as a tool to generate informal proofs, implying their potential limitations are not the focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned, but LLMs are used as a tool to generate informal proofs, implying their potential limitations are not the focus of the paper."
    },
    {
        "title": "Choreographer: Learning and Adapting Skills in Imagination",
        "authors": [
            "Pietro Mazzaglia",
            "Tim Verbelen",
            "Bart Dhoedt",
            "Alexandre Lacoste",
            "Sai Rajeswar"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Unsupervised skill learning aims to learn a rich repertoire of behaviors without external supervision, providing artificial agents with the ability to control and influence the environment. However, without appropriate knowledge and exploration, skills may provide control only over a restricted area of the environment, limiting their applicability. Furthermore, it is unclear how to leverage the learned skill behaviors for adapting to downstream tasks in a data-efficient manner. We present Choreographer, a model-based agent that exploits its world model to learn and adapt skills in imagination. Our method decouples the exploration and skill learning processes, being able to discover skills in the latent state space of the model. During adaptation, the agent uses a meta-controller to evaluate and adapt the learned skills efficiently by deploying them in parallel in imagination. Choreographer is able to learn skills both from offline data, and by collecting data simultaneously with an exploration policy. The skills can be used to effectively adapt to downstream tasks, as we show in the URL benchmark, where we outperform previous approaches from both pixels and states inputs. The skills also explore the environment thoroughly, finding sparse rewards more frequently, as shown in goal-reaching tasks from the DMC Suite and Meta-World. \nProject website: https://skillchoreographer.github.io/",
        "pdf_link": "https://openreview.net/pdf/3b9c0c356a7760d7f70096b567ff2aef51b26f98.pdf",
        "forum_url": "https://openreview.net/forum?id=PhkWyijGi5b",
        "keywords": [
            "skills",
            "adapt skills",
            "adaptation",
            "choreographer",
            "skill learning",
            "learn skills",
            "unsupervised skill learning"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs."
    },
    {
        "title": "Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier",
        "authors": [
            "Pierluca D'Oro",
            "Max Schwarzer",
            "Evgenii Nikishin",
            "Pierre-Luc Bacon",
            "Marc G Bellemare",
            "Aaron Courville"
        ],
        "published": "ICLR 2023 notable top 5%",
        "summary": "Increasing the replay ratio, the number of updates of an agent's parameters per environment interaction, is an appealing strategy for improving the sample efficiency of deep reinforcement learning algorithms. In this work, we show that fully or partially resetting the parameters of deep reinforcement learning agents causes better replay ratio scaling capabilities to emerge. We push the limits of the sample efficiency of carefully-modified algorithms by training them using an order of magnitude more updates than usual, significantly improving their performance in the Atari 100k and DeepMind Control Suite benchmarks. We then provide an analysis of the design choices required for favorable replay ratio scaling to be possible and discuss inherent limits and tradeoffs.",
        "pdf_link": "https://openreview.net/pdf/c891095f8e46b891138ef064f19d6b0e2d84dcb2.pdf",
        "forum_url": "https://openreview.net/forum?id=OpC-9aBBVJe",
        "keywords": [
            "replay",
            "sample",
            "reinforcement learning",
            "sample efficient reinforcement learning",
            "sample efficiency",
            "barrier"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Guarded Policy Optimization with Imperfect Online Demonstrations",
        "authors": [
            "Zhenghai Xue",
            "Zhenghao Peng",
            "Quanyi Li",
            "Zhihan Liu",
            "Bolei Zhou"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "The Teacher-Student Framework (TSF) is a reinforcement learning setting where a teacher agent guards the training of a student agent by intervening and providing online demonstrations. Assuming optimal, the teacher policy has the perfect timing and capability to intervene in the learning process of the student agent, providing safety guarantee and exploration guidance. Nevertheless, in many real-world settings it is expensive or even impossible to obtain a well-performing teacher policy. In this work, we relax the assumption of a well-performing teacher and develop a new method that can incorporate arbitrary teacher policies with modest or inferior performance. We instantiate an Off-Policy Reinforcement Learning algorithm, termed Teacher-Student Shared Control (TS2C), which incorporates teacher intervention based on trajectory-based value estimation. Theoretical analysis validates that the proposed TS2C algorithm attains efficient exploration and substantial safety guarantee without being affected by the teacher's own performance. Experiments on various continuous control tasks show that our method can exploit teacher policies at different performance levels while maintaining a low training cost. Moreover, the student policy surpasses the imperfect teacher policy in terms of higher accumulated reward in held-out testing environments. Code is available at https://metadriverse.github.io/TS2C.",
        "pdf_link": "https://openreview.net/pdf/e19dee281e43ab70ef8f8640d6ccb689bed45bd8.pdf",
        "forum_url": "https://openreview.net/forum?id=O5rKg7IRQIO",
        "keywords": [
            "reinforcement learning",
            "guarded policy optimization",
            "online demonstrations"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Human-Guided Fair Classification for Natural Language Processing",
        "authors": [
            "Florian E. Dorner",
            "Momchil Peychev",
            "Nikola Konstantinov",
            "Naman Goel",
            "Elliott Ash",
            "Martin Vechev"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Text classifiers have promising applications in high-stake tasks such as resume screening and content moderation. These classifiers must be fair and avoid discriminatory decisions by being invariant to perturbations of sensitive attributes such as gender or ethnicity. However, there is a gap between human intuition about these perturbations and the formal similarity specifications capturing them. While existing research has started to address this gap, current methods are based on hardcoded word replacements, resulting in specifications with limited expressivity or ones that fail to fully align with human intuition (e.g., in cases of asymmetric counterfactuals). This work proposes novel methods for bridging this gap by discovering expressive and intuitive individual fairness specifications. We show how to leverage unsupervised style transfer and GPT-3's zero-shot capabilities to automatically generate expressive candidate pairs of semantically similar sentences that differ along sensitive attributes. We then validate the generated pairs via an extensive crowdsourcing study, which confirms that a lot of these pairs align with human intuition about fairness in the context of toxicity classification. Finally, we show how limited amounts of human feedback can be leveraged to learn a similarity specification that can be used to train downstream fairness-aware models. ",
        "pdf_link": "https://openreview.net/pdf/09b5568016529de9fe0127852626c933cb6af627.pdf",
        "forum_url": "https://openreview.net/forum?id=N_g8TT9Cy7f",
        "keywords": [
            "fair",
            "natural language processing",
            "fair classification",
            "fairness aware"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While existing research has started to address this gap, current methods are based on hardcoded word replacements, resulting in specifications with limited expressivity or ones that fail to fully align with human intuition (e.g., in cases of asymmetric counterfactuals).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While existing research has started to address this gap, current methods are based on hardcoded word replacements, resulting in specifications with limited expressivity or ones that fail to fully align with human intuition (e.g., in cases of asymmetric counterfactuals).\""
    },
    {
        "title": "Mass-Editing Memory in a Transformer",
        "authors": [
            "Kevin Meng",
            "Arnab Sen Sharma",
            "Alex J Andonian",
            "Yonatan Belinkov",
            "David Bau"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by an order of magnitude. Our code and data will be open-sourced upon publication.",
        "pdf_link": "https://openreview.net/pdf/5d2ff18d2f074c0f0b7bda40d118bb08e13bcd43.pdf",
        "forum_url": "https://openreview.net/forum?id=MkbcAHIYgyS",
        "keywords": [
            "memit",
            "transformer",
            "code",
            "mass editing memory",
            "model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, this line of work is predominantly limited to updating single associations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, this line of work is predominantly limited to updating single associations.\""
    },
    {
        "title": "Does Zero-Shot Reinforcement Learning Exist?",
        "authors": [
            "Ahmed Touati",
            "Jérémy Rapin",
            "Yann Ollivier"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "A zero-shot RL agent is an agent that can solve any RL task in a given environment, instantly with no additional planning or learning, after an initial reward-free learning phase. This marks a shift from the reward-centric RL paradigm towards controllable agents that can follow arbitrary instructions in an environment. Current RL agents can solve families of related tasks at best, or require planning anew for each task. Strategies for approximate zero-shot RL have been suggested using successor features (SFs) (Borsa et al., 2018) or forward-backward (FB) representations (Touati & Ollivier, 2021), but testing has been limited. \nAfter clarifying the relationships between these schemes, we introduce improved losses and new SF models, and test the viability of zero-shot RL schemes systematically on tasks from the Unsupervised RL benchmark (Laskin et al., 2021). To disentangle universal representation learning from exploration, we work in an offline setting and repeat the tests on several existing replay buffers.\nSFs appear to suffer from the choice of the elementary state features. SFs with Laplacian eigenfunctions do well, while SFs based on auto-encoders, inverse curiosity, transition models, low-rank transition matrix, contrastive learning, or diversity (APS),  perform unconsistently. In contrast, FB representations jointly learn the elementary and successor features from a single, principled criterion. They perform best and consistently across the board, reaching $85\\%$ of supervised RL performance with a good replay buffer, in a zero-shot manner.",
        "pdf_link": "https://openreview.net/pdf/63a8b5a5af811abc3b027de6cccef1854dbedc3c.pdf",
        "forum_url": "https://openreview.net/forum?id=MYEap_OcQI",
        "keywords": [
            "reinforcement learning"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "The Role of Coverage in Online Reinforcement Learning",
        "authors": [
            "Tengyang Xie",
            "Dylan J Foster",
            "Yu Bai",
            "Nan Jiang",
            "Sham M. Kakade"
        ],
        "published": "ICLR 2023 notable top 5%",
        "summary": "Coverage conditions---which assert that the data logging distribution adequately covers the state space---play a fundamental role in determining the sample complexity of offline reinforcement learning. While such conditions might seem irrelevant to online reinforcement learning at first glance, we establish a new connection by showing---somewhat surprisingly---that the mere existence of a data distribution with good coverage can enable sample-efficient online RL. Concretely, we show that coverability---that is, existence of a data distribution that satisfies a ubiquitous coverage condition called concentrability---can be viewed as a structural property of the underlying MDP, and can be exploited by standard algorithms for sample-efficient exploration, even when the agent does not know said distribution. We complement this result by proving that several weaker notions of coverage, despite being sufficient for offline RL, are insufficient for online RL. We also show that existing complexity measures for online RL, including Bellman rank and Bellman-Eluder dimension, fail to optimally capture coverability, and propose a new complexity measure, the self-normalized coefficient, to provide a unification.",
        "pdf_link": "https://openreview.net/pdf/a2c365918c8b9f3e5b7cd871606f05d90118525a.pdf",
        "forum_url": "https://openreview.net/forum?id=LQIjzPdDt3q",
        "keywords": [
            "reinforcement learning",
            "coverage",
            "sample",
            "online reinforcement learning",
            "online rl",
            "sample complexity",
            "measures"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Training language models to summarize narratives improves brain alignment",
        "authors": [
            "Khai Loong Aw",
            "Mariya Toneva"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Building systems that achieve a deeper understanding of language is one of the central goals of natural language processing (NLP). Towards this goal, recent works have begun to train language models on narrative datasets which require extracting the most critical information by integrating across long contexts. However, it is still an open question whether these models are learning a deeper understanding of the text, or if the models are simply learning a heuristic to complete the task. This work investigates this further by turning to the one language processing system that truly understands complex language: the human brain. We show that training language models for deeper narrative understanding results in richer representations that have improved alignment to human brain activity. We further find that the improvements in brain alignment are larger for character names than for other discourse features, which indicates that these models are learning important narrative elements. Taken together, these results suggest that this type of training can indeed lead to deeper language understanding. These findings have consequences both for cognitive neuroscience by revealing some of the significant factors behind brain-NLP alignment, and for NLP by highlighting that understanding of long-range context can be improved beyond language modeling.",
        "pdf_link": "https://openreview.net/pdf/c67334d169d975ca4c1f56fc722f9eb680ebf5b9.pdf",
        "forum_url": "https://openreview.net/forum?id=KzkLAE49H9b",
        "keywords": [
            "narratives",
            "language",
            "train language",
            "training",
            "brain",
            "learning",
            "brain alignment",
            "language models",
            "natural language processing",
            "brain nlp alignment"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it is still an open question whether these models are learning a deeper understanding of the text, or if the models are simply learning a heuristic to complete the task.\"\n\nThis paper briefly mentions a limitation of LLMs, specifically the uncertainty about whether they truly understand the text or just learn a heuristic to complete the task, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, it is still an open question whether these models are learning a deeper understanding of the text, or if the models are simply learning a heuristic to complete the task.\"\n\nThis paper briefly mentions a limitation of LLMs, specifically the uncertainty about whether they truly understand the text or just learn a heuristic to complete the task, but it is not the primary focus of the paper."
    },
    {
        "title": "PEER: A Collaborative Language Model",
        "authors": [
            "Timo Schick",
            "Jane A. Yu",
            "Zhengbao Jiang",
            "Fabio Petroni",
            "Patrick Lewis",
            "Gautier Izacard",
            "Qingfei You",
            "Christoforos Nalmpantis",
            "Edouard Grave",
            "Sebastian Riedel"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes.\nAgnostic of this process, today’s language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions.\nTo address these shortcomings, we introduce PEER, a collaborative language model that is trained to imitate the entire writing process itself. PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to infill various parts of the writing process, enabling the use of self-training techniques for increasing the quality, amount and diversity of training data. This unlocks PEER's full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that PEER achieves strong performance across various domains and editing tasks.",
        "pdf_link": "https://openreview.net/pdf/e50eaf58c25ddb7ed0ec57bcc796b131b7046154.pdf",
        "forum_url": "https://openreview.net/forum?id=KbYevcLjnc",
        "keywords": [
            "collaborative",
            "collaborative language",
            "peer"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions.\""
    },
    {
        "title": "When and Why Vision-Language Models Behave like Bags-Of-Words, and What to Do About It?",
        "authors": [
            "Mert Yuksekgonul",
            "Federico Bianchi",
            "Pratyusha Kalluri",
            "Dan Jurafsky",
            "James Zou"
        ],
        "published": "ICLR 2023 notable top 5%",
        "summary": "Despite the success of large vision and language models (VLMs) in many downstream applications, it is unclear how well they encode the compositional relationships between objects and attributes. Here, we create the Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order information. ARO consists of \\emph{Visual Genome Attribution}, to test the understanding of objects' properties; \\emph{Visual Genome Relation}, to test for relational understanding; and \\emph{COCO-Order \\& Flickr30k-Order}, to test for order sensitivity in VLMs. ARO is orders of magnitude larger than previous benchmarks of compositionality, with more than 50,000 test cases. We present the settings where  state-of-the-art VLMs behave like bags-of-words---i.e. when they have poor relational understanding, can blunder when linking objects to their attributes, and demonstrate a severe lack of order sensitivity. VLMs are predominantly trained and evaluated on large scale datasets with rich compositional structure in the images and captions. Yet, training on these datasets has not been enough to address the lack of compositional understanding, and evaluating on these datasets has failed to surface this deficiency. To understand why these limitations emerge and are not represented in the standard tests, we zoom into the evaluation and training procedures. We demonstrate that it is possible to perform well on image-text retrieval over existing datasets without using the composition and order information. This further motivates the value of using ARO to benchmark VLMs. Given that contrastive pretraining optimizes for retrieval on large datasets with similar shortcuts, we hypothesize that this can explain why the models do not need to learn to represent compositional information. This finding suggests a natural solution: composition-aware hard negative mining. We show that a simple-to-implement modification of contrastive learning significantly improves the performance on tasks requiring understanding of order and compositionality. ",
        "pdf_link": "https://openreview.net/pdf/ced77554985af011f5544a8798a3035d4b6ab52b.pdf",
        "forum_url": "https://openreview.net/forum?id=KRLUvxh8uaX",
        "keywords": [
            "bags of words",
            "compositionality",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We present the settings where state-of-the-art VLMs behave like bags-of-words---i.e. when they have poor relational understanding, can blunder when linking objects to their attributes, and demonstrate a severe lack of order sensitivity.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We present the settings where state-of-the-art VLMs behave like bags-of-words---i.e. when they have poor relational understanding, can blunder when linking objects to their attributes, and demonstrate a severe lack of order sensitivity.\""
    },
    {
        "title": "A System for Morphology-Task Generalization via Unified Representation and Behavior Distillation",
        "authors": [
            "Hiroki Furuta",
            "Yusuke Iwasawa",
            "Yutaka Matsuo",
            "Shixiang Shane Gu"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "The rise of generalist large-scale models in natural language and vision has made us expect that a massive data-driven approach could achieve broader generalization in other domains such as continuous control. In this work, we explore a method for learning a single policy that manipulates various forms of agents to solve various tasks by distilling a large amount of proficient behavioral data. In order to align input-output (IO) interface among multiple tasks and diverse agent morphologies while preserving essential 3D geometric relations, we introduce morphology-task graph, which treats observations, actions and goals/task in a unified graph representation. We also develop MxT-Bench for fast large-scale behavior generation, which supports procedural generation of diverse morphology-task combinations with a minimal blueprint and hardware-accelerated simulator. Through efficient representation and architecture selection on MxT-Bench, we find out that a morphology-task graph representation coupled with Transformer architecture improves the multi-task performances compared to other baselines including recent discrete tokenization, and provides better prior knowledge for zero-shot transfer or sample efficiency in downstream multi-task imitation learning. Our work suggests large diverse offline datasets, unified IO representation, and policy representation and architecture selection through supervised learning form a promising approach for studying and advancing morphology-task generalization.",
        "pdf_link": "https://openreview.net/pdf/184fcbb9f9a73128759c56558e7ad476b59fa452.pdf",
        "forum_url": "https://openreview.net/forum?id=HcUf-QwZeFh",
        "keywords": [
            "morphology",
            "morphology task graph",
            "behavior distillation",
            "io representation",
            "imitation"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs."
    },
    {
        "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
        "authors": [
            "Andy Zeng",
            "Maria Attarian",
            "brian ichter",
            "Krzysztof Marcin Choromanski",
            "Adrian Wong",
            "Stefan Welker",
            "Federico Tombari",
            "Aveek Purohit",
            "Michael S Ryoo",
            "Vikas Sindhwani",
            "Johnny Lee",
            "Vincent Vanhoucke",
            "Pete Florence"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "We investigate how multimodal prompt engineering can use language as the intermediate representation to combine complementary knowledge from different pretrained (potentially multimodal) language models for a variety of tasks. This approach is both distinct from and complementary to the dominant paradigm of joint multimodal training. It also recalls a traditional systems-building view as in classical NLP pipelines, but with prompting large pretrained multimodal models. We refer to these as Socratic Models (SMs): a modular class of systems in which multiple pretrained models may be composed zero-shot via multimodal-informed prompting to capture new multimodal capabilities, without additional finetuning. We show that these systems provide competitive state-of-the-art performance for zero-shot image captioning and video-to-text retrieval, and also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes), and (iii) robot perception and planning. We hope this work provides (a) results for stronger zero-shot baseline performance with analysis also highlighting their limitations, (b) new perspectives for building multimodal systems powered by large pretrained models, and (c) practical application advantages in certain regimes limited by data scarcity, training compute, or model access.",
        "pdf_link": "https://openreview.net/pdf/92b6e024f8a9e971e8041aa14e06de2802245730.pdf",
        "forum_url": "https://openreview.net/forum?id=G2Q2Mh3avow",
        "keywords": [
            "socratic models",
            "multimodal",
            "multimodal training",
            "zero shot multimodal reasoning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"with analysis also highlighting their limitations\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"with analysis also highlighting their limitations\""
    },
    {
        "title": "Language Modelling with Pixels",
        "authors": [
            "Phillip Rust",
            "Jonas F. Lotz",
            "Emanuele Bugliarello",
            "Elizabeth Salesky",
            "Miryam de Lhoneux",
            "Desmond Elliott"
        ],
        "published": "ICLR 2023 notable top 5%",
        "summary": "Language models are defined over a finite set of inputs, which creates a vocabulary bottleneck when we attempt to scale the number of supported languages. Tackling this bottleneck results in a trade-off between what can be represented in the embedding matrix and computational issues in the output layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which suffers from neither of these issues. PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels. PIXEL is trained to reconstruct the pixels of masked patches instead of predicting a distribution over tokens. We pretrain the 86M parameter PIXEL model on the same English data as BERT and evaluate on syntactic and semantic tasks in typologically diverse languages, including various non-Latin scripts. We find that PIXEL substantially outperforms BERT on syntactic and semantic processing tasks on scripts that are not found in the pretraining data, but PIXEL is slightly weaker than BERT when working with Latin scripts. Furthermore, we find that PIXEL is more robust than BERT to orthographic attacks and linguistic code-switching, further confirming the benefits of modelling language with pixels.",
        "pdf_link": "https://openreview.net/pdf/5ade25a9134d48be86a9acbbebf941357365462c.pdf",
        "forum_url": "https://openreview.net/forum?id=FkSp8VW8RjH",
        "keywords": [
            "language modelling",
            "latin scripts",
            "encoder"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Language models are defined over a finite set of inputs, which creates a vocabulary bottleneck when we attempt to scale the number of supported languages.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Language models are defined over a finite set of inputs, which creates a vocabulary bottleneck when we attempt to scale the number of supported languages.\""
    },
    {
        "title": "Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning",
        "authors": [
            "Anton Bakhtin",
            "David J Wu",
            "Adam Lerer",
            "Jonathan Gray",
            "Athul Paul Jacob",
            "Gabriele Farina",
            "Alexander H Miller",
            "Noam Brown"
        ],
        "published": "ICLR 2023 notable top 5%",
        "summary": "No-press Diplomacy is a complex strategy game involving both cooperation and competition that has served as a benchmark for multi-agent AI research. While self-play reinforcement learning has resulted in numerous successes in purely adversarial games like chess, Go, and poker, self-play alone is insufficient for achieving optimal performance in domains involving cooperation with humans. We address this shortcoming by first introducing a planning algorithm we call DiL-piKL that regularizes a reward-maximizing policy toward a human imitation-learned policy. We prove that this is a no-regret learning algorithm under a modified utility function. We then show that DiL-piKL can be extended into a self-play reinforcement learning algorithm we call RL-DiL-piKL that provides a model of human play while simultaneously training an agent that responds well to this human model. We used RL-DiL-piKL to train an agent we name Diplodocus.\nIn a 200-game no-press Diplomacy tournament involving 62 human participants spanning skill levels from beginner to expert, two Diplodocus agents both achieved a higher average score than all other participants who played more than two games, and ranked first and third according to an Elo ratings model.",
        "pdf_link": "https://openreview.net/pdf/5355b9a9bc1eabd198a78654d7dbfa4e5f1664b0.pdf",
        "forum_url": "https://openreview.net/forum?id=F61FwJTZhb",
        "keywords": [
            "regularized reinforcement learning",
            "reinforcement learning",
            "agents"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task",
        "authors": [
            "Kenneth Li",
            "Aspen K Hopkins",
            "David Bau",
            "Fernanda Viégas",
            "Hanspeter Pfister",
            "Martin Wattenberg"
        ],
        "published": "ICLR 2023 notable top 5%",
        "summary": "Language models show a surprising range of capabilities, but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see? We investigate this question by applying a variant of the GPT model to the task of predicting legal moves in a simple board game, Othello. Although the network has no a priori knowledge of the game or its rules, we uncover evidence of an emergent nonlinear internal representation of the board state. Interventional experiments indicate this representation can be used to control the output of the network and create \"latent saliency maps\" that can help explain predictions in human terms.",
        "pdf_link": "https://openreview.net/pdf/70fb51a26cffdf3304e24f4d2e803b729904fe20.pdf",
        "forum_url": "https://openreview.net/forum?id=DeG07_TcZvT",
        "keywords": [
            "emergent world representations",
            "latent saliency",
            "sequence model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see?\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (the unclear source of their competence) in passing and uses it to motivate the investigation, but does not explore this",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see?\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (the unclear source of their competence) in passing and uses it to motivate the investigation, but does not explore this"
    },
    {
        "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
        "authors": [
            "Daniel Y Fu",
            "Tri Dao",
            "Khaled Kamal Saab",
            "Armin W Thomas",
            "Atri Rudra",
            "Christopher Re"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.",
        "pdf_link": "https://openreview.net/pdf/b3774a7e6b7bda0783528bf1dc8e2600707d797f.pdf",
        "forum_url": "https://openreview.net/forum?id=COZDy0WYGg",
        "keywords": [
            "state space models",
            "language modeling",
            "attention language models",
            "hybrid language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling.\""
    },
    {
        "title": "Multi-lingual Evaluation of Code Generation Models",
        "authors": [
            "Ben Athiwaratkun",
            "Sanjay Krishna Gouda",
            "Zijian Wang",
            "Xiaopeng Li",
            "Yuchen Tian",
            "Ming Tan",
            "Wasi Uddin Ahmad",
            "Shiqi Wang",
            "Qing Sun",
            "Mingyue Shang",
            "Sujan Kumar Gonugondla",
            "Hantian Ding",
            "Varun Kumar",
            "Nathan Fulton",
            "Arash Farahani",
            "Siddhartha Jain",
            "Robert Giaquinto",
            "Haifeng Qian",
            "Murali Krishna Ramanathan",
            "Ramesh Nallapati",
            "Baishakhi Ray",
            "Parminder Bhatia",
            "Sudipta Sengupta",
            "Dan Roth",
            "Bing Xiang"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "We present two new benchmarks, MBXP and Multilingual HumanEval, designed to evaluate code completion models in over 10 programming languages. These datasets are generated using a conversion framework that transpiles prompts and test cases from the original MBPP and HumanEval datasets into the corresponding data in the target language. By using these benchmarks, we are able to assess the performance of code generation models in a multi-lingual fashion, and discovered generalization ability of language models on out-of-domain languages, advantages of multi-lingual models over mono-lingual, the ability of  few-shot prompting to teach the model new languages, and zero-shot translation abilities. In addition, we use our code generation model to perform large-scale bootstrapping to obtain synthetic canonical solutions in several languages, which can be used for other code-related evaluations such as code insertion, robustness, or summarization tasks.",
        "pdf_link": "https://openreview.net/pdf/c2ba4659e44c45ec67969ec9a74097a37184ad62.pdf",
        "forum_url": "https://openreview.net/forum?id=Bo7eeXm6An8",
        "keywords": [
            "code generation",
            "code generation model",
            "multi lingual models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"discovered generalization ability of language models on out-of-domain languages\"\n\nThis paper mentions a limitation of LLMs in passing, specifically their generalization ability on out-of-domain languages, but does not elaborate on it further.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"discovered generalization ability of language models on out-of-domain languages\"\n\nThis paper mentions a limitation of LLMs in passing, specifically their generalization ability on out-of-domain languages, but does not elaborate on it further."
    },
    {
        "title": "Learning Soft Constraints From Constrained Expert Demonstrations",
        "authors": [
            "Ashish Gaurav",
            "Kasra Rezaee",
            "Guiliang Liu",
            "Pascal Poupart"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Inverse reinforcement learning (IRL) methods assume that the expert data is generated by an agent optimizing some reward function. However, in many settings, the agent may optimize a reward function subject to some constraints, where the constraints induce behaviors that may be otherwise difficult to express with just a reward function. We consider the setting where the reward function is given, and the constraints are unknown, and propose a method that is able to recover these constraints satisfactorily from the expert data. While previous work has focused on recovering hard constraints, our method can recover cumulative soft constraints that the agent satisfies on average per episode. In IRL fashion, our method solves this problem by adjusting the constraint function iteratively through a constrained optimization procedure, until the agent behavior matches the expert behavior. We demonstrate our approach on synthetic environments, robotics environments and real world highway driving scenarios.",
        "pdf_link": "https://openreview.net/pdf/8fcf77a080574ee36abb6525663524292f7b5217.pdf",
        "forum_url": "https://openreview.net/forum?id=8sSnD78NqTN",
        "keywords": [
            "learning soft constraints",
            "reinforcement learning",
            "scenarios",
            "constrained optimization"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization",
        "authors": [
            "Rajkumar Ramamurthy",
            "Prithviraj Ammanabrolu",
            "Kianté Brantley",
            "Jack Hessel",
            "Rafet Sifa",
            "Christian Bauckhage",
            "Hannaneh Hajishirzi",
            "Yejin Choi"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "We tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP?\n\nTo help answer this, we first introduce an open-source modular library, $RL4LMs$ (Reinforcement Learning for Language Models), for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al. 2020) with an arbitrary reward function. Next, we present the $GRUE$ (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference.GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, $NLPO$ (Natural Language Policy Optimization)} that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic and human evaluations.",
        "pdf_link": "https://openreview.net/pdf/e7b48c662a15dbddc1a3d5c9a2b338c13189506e.pdf",
        "forum_url": "https://openreview.net/forum?id=8aHzds2uUyB",
        "keywords": [
            "reinforcement learning",
            "policy",
            "natural language policy optimization",
            "natural language processing",
            "optimizing language generators",
            "language generation",
            "benchmarks",
            "lm",
            "preferences"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment.\""
    },
    {
        "title": "Fast and Precise: Adjusting Planning Horizon with Adaptive Subgoal Search",
        "authors": [
            "Michał Zawalski",
            "Michał Tyrolski",
            "Konrad Czechowski",
            "Tomasz Odrzygóźdź",
            "Damian Stachura",
            "Piotr Piękos",
            "Yuhuai Wu",
            "Łukasz Kuciński",
            "Piotr Miłoś"
        ],
        "published": "ICLR 2023 notable top 5%",
        "summary": "Complex reasoning problems contain states that vary in the computational cost required to determine the right action plan. To take advantage of this property, we propose Adaptive Subgoal Search (AdaSubS), a search method that adaptively adjusts the planning horizon. To this end, AdaSubS generates diverse sets of subgoals at different distances. A verification mechanism is employed to filter out unreachable subgoals swiftly, making it possible to focus on feasible further subgoals. In this way, AdaSubS benefits from the efficiency of planning with longer-term subgoals and the fine control with shorter-term ones, and thus scales well to difficult planning problems. We show that AdaSubS significantly surpasses hierarchical planning algorithms on three complex reasoning tasks: Sokoban, the Rubik’s Cube, and the inequality-proving benchmark INT. ",
        "pdf_link": "https://openreview.net/pdf/361fb386c64c303b0467dd1fb8d3946766d58d4c.pdf",
        "forum_url": "https://openreview.net/forum?id=7JsGYvjE88d",
        "keywords": [
            "planning",
            "search",
            "adaptive subgoal search",
            "adaptive subgoal",
            "planning horizon"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Planning Goals for Exploration",
        "authors": [
            "Edward S. Hu",
            "Richard Chang",
            "Oleh Rybkin",
            "Dinesh Jayaraman"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "Dropped into an unknown environment, what should an agent do to quickly learn about the environment and how to accomplish diverse tasks within it? We address this question within the goal-conditioned reinforcement learning paradigm, by identifying how the agent should set its goals at training time to maximize exploration. We propose \"Planning Exploratory Goals\" (PEG), a method that sets goals for each training episode to directly optimize an intrinsic exploration reward. PEG first chooses goal commands such that the agent's goal-conditioned policy, at its current level of training, will end up in states with high exploration potential. It then launches an exploration policy starting at those promising states. To enable this direct optimization, PEG learns world models and adapts sampling-based planning algorithms to \"plan goal commands\". In challenging simulated robotics environments including a multi-legged ant robot in a maze, and a robot arm on a cluttered tabletop, PEG exploration enables more efficient and effective training of goal-conditioned policies relative to baselines and ablations. Our ant successfully navigates a long maze, and the robot arm successfully builds a stack of three blocks upon command. Website: https://sites.google.com/view/exploratory-goals",
        "pdf_link": "https://openreview.net/pdf/b28237bb9e4d96d5f02a9d0639565db68727d08c.pdf",
        "forum_url": "https://openreview.net/forum?id=6qeBuZSo7Pr",
        "keywords": [
            "planning",
            "exploration",
            "peg exploration",
            "planning goals",
            "reinforcement learning"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning",
        "authors": [
            "Antonia Creswell",
            "Murray Shanahan",
            "Irina Higgins"
        ],
        "published": "ICLR 2023 notable top 5%",
        "summary": "Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 46 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.",
        "pdf_link": "https://openreview.net/pdf/4c8f591f9bb58ccd07ed826e0e57885bc4227b12.pdf",
        "forum_url": "https://openreview.net/forum?id=3Pf3Wg6o-A4",
        "keywords": [
            "logical reasoning",
            "selection and inference",
            "selection inference",
            "language models",
            "interpretable logical reasoning",
            "generalisation",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they still tend to perform poorly on multi-step logical reasoning problems.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, they still tend to perform poorly on multi-step logical reasoning problems.\""
    },
    {
        "title": "DaxBench: Benchmarking Deformable Object Manipulation with Differentiable Physics",
        "authors": [
            "Siwei Chen",
            "Yiqing Xu",
            "Cunjun Yu",
            "Linfeng Li",
            "Xiao Ma",
            "Zhongwen Xu",
            "David Hsu"
        ],
        "published": "ICLR 2023 notable top 5%",
        "summary": "Deformable object manipulation (DOM) is a long-standing challenge in robotics and has attracted significant interest recently. This paper presents DaXBench, a differentiable simulation framework for DOM. While existing work often focuses on a specific type of deformable objects, DaXBench supports fluid, rope, cloth ...; it provides a general-purpose benchmark to evaluate widely different DOM methods, including planning, imitation learning, and reinforcement learning. DaXBench combines recent advances in deformable object simulation with JAX, a high-performance computational framework. All DOM tasks in DaXBench are wrapped with the OpenAI Gym API for easy integration with DOM algorithms. We hope that DaXBench provides to the research community a comprehensive, standardized benchmark and a valuable tool to support the development and evaluation of new DOM methods. The code and video are available online.",
        "pdf_link": "https://openreview.net/pdf/3c5184bef72b67b8b06885038e921049f56dc94e.pdf",
        "forum_url": "https://openreview.net/forum?id=1NAzMofMnWl",
        "keywords": [
            "benchmark",
            "simulation",
            "deformable object simulation",
            "benchmarking deformable object manipulation",
            "deformable object manipulation",
            "dom",
            "planning"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "STREET: A MULTI-TASK STRUCTURED REASONING AND EXPLANATION BENCHMARK",
        "authors": [
            "Danilo Neves Ribeiro",
            "Shen Wang",
            "Xiaofei Ma",
            "Henghui Zhu",
            "Rui Dong",
            "Deguang Kong",
            "Juliette Burger",
            "Anjelica Ramos",
            "zhiheng huang",
            "William Yang Wang",
            "George Karypis",
            "Bing Xiang",
            "Dan Roth"
        ],
        "published": "ICLR 2023 notable top 25%",
        "summary": "We introduce STREET, a unified multi-task and multi-domain natural language reasoning and explanation benchmark. Unlike most existing question-answering (QA) datasets, we expect models to not only answer questions, but also produce step-by-step structured explanations describing how premises in the question are used to produce intermediate conclusions that can prove the correctness of a certain answer. We perform extensive evaluation with popular language models such as few-shot prompting GPT-3 and fine-tuned T5. We find that these models still lag behind human performance when producing such structured reasoning steps. We believe this work will provide a way for the community to better train and test systems on multi-step reasoning and explanations in natural language.",
        "pdf_link": "https://openreview.net/pdf/1b74d54ce93b0d4d1558e20806f96d4b743468ea.pdf",
        "forum_url": "https://openreview.net/forum?id=1C_kSW1-k0",
        "keywords": [
            "explanation benchmark",
            "reasoning",
            "structured reasoning",
            "multi task structured reasoning",
            "step reasoning",
            "multi task"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that these models still lag behind human performance when producing such structured reasoning steps.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We find that these models still lag behind human performance when producing such structured reasoning steps.\""
    },
    {
        "title": "​​What learning algorithm is in-context learning? Investigations with linear models",
        "authors": [
            "Ekin Akyürek",
            "Dale Schuurmans",
            "Jacob Andreas",
            "Tengyu Ma",
            "Denny Zhou"
        ],
        "published": "ICLR 2023 notable top 5%",
        "summary": "Neural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. They can construct new predictors from sequences of labeled examples $(x, f(x))$ presented in the input without further parameter updates. We investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding context-specific parametric models in their hidden representations, and updating these implicit models as new examples appear in the context. Using linear regression as a model problem, we offer three sources of evidence for this hypothesis. First, we prove by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form computation of regression parameters. Second, we show that trained in-context learners closely match the predictors computed by gradient descent, ridge regression, and exact least-squares regression, transitioning between different predictors as transformer depth and dataset noise vary. Third, we present preliminary evidence that in-context learners share algorithmic features with these predictors: learners' late layers encode weight vectors and moment matrices.  These results suggest that in-context learning is understandable in algorithmic terms, and that (at least in the linear case) learners may work by rediscovering standard estimation algorithms.",
        "pdf_link": "https://openreview.net/pdf/7295479b5085774245ad66c73c5176e41b868b67.pdf",
        "forum_url": "https://openreview.net/forum?id=0g0X4H8yN4I",
        "keywords": [
            "linear models",
            "learning algorithm",
            "context learning",
            "linear regression",
            "ridge regression",
            "transformers"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Guiding Safe Exploration with Weakest Preconditions",
        "authors": [
            "Greg Anderson",
            "Swarat Chaudhuri",
            "Isil Dillig"
        ],
        "published": "iclr 2023 poster",
        "summary": "In reinforcement learning for safety-critical settings, it is often desirable for the agent to obey safety constraints at all points in time, including during training. We present a novel neurosymbolic approach called SPICE to solve this safe exploration problem. SPICE uses an online shielding layer based on symbolic weakest preconditions to achieve a more precise safety analysis than existing tools without unduly impacting the training process. We evaluate the approach on a suite of continuous control benchmarks and show that it can achieve comparable performance to existing safe learning techniques while incurring fewer safety violations. Additionally, we present theoretical results showing that SPICE converges to the optimal safe policy under reasonable assumptions.",
        "pdf_link": "https://openreview.net/pdf/31fba2ce53b7314f1c3b3ec7719c818d82414a6d.pdf",
        "forum_url": "https://openreview.net/forum?id=zzqBoIFOQ1",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "More Centralized Training, Still Decentralized Execution: Multi-Agent Conditional Policy Factorization",
        "authors": [
            "Jiangxing Wang",
            "Deheng Ye",
            "Zongqing Lu"
        ],
        "published": "iclr 2023 poster",
        "summary": "In cooperative multi-agent reinforcement learning (MARL), combining value decomposition with actor-critic enables agents to learn stochastic policies, which are more suitable for the partially observable environment. Given the goal of learning local policies that enable decentralized execution, agents are commonly assumed to be independent of each other, even in centralized training. However, such an assumption may prohibit agents from learning the optimal joint policy. To address this problem, we explicitly take the dependency among agents into centralized training. Although this leads to the optimal joint policy, it may not be factorized for decentralized execution. Nevertheless, we theoretically show that from such a joint policy, we can always derive another joint policy that achieves the same optimality but can be factorized for decentralized execution. To this end, we propose multi-agent conditional policy factorization (MACPF), which takes more centralized training but still enables decentralized execution. We empirically verify MACPF in various cooperative MARL tasks and demonstrate that MACPF achieves better performance or faster convergence than baselines. Our code is available at https://github.com/PKU-RL/FOP-DMAC-MACPF.",
        "pdf_link": "https://openreview.net/pdf/8258fe1c50fe61494176aa41b2c207716e3d556b.pdf",
        "forum_url": "https://openreview.net/forum?id=znLlSgN-4S0",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "On Pre-training Language Model for Antibody",
        "authors": [
            "Danqing Wang",
            "Fei YE",
            "Hao Zhou"
        ],
        "published": "iclr 2023 poster",
        "summary": "Antibodies are vital proteins offering robust protection for the human body from pathogens. The development of general protein and antibody-specific pre-trained language models both facilitate antibody prediction tasks. However, there have been limited studies that comprehensively explore the representation capability of distinct pre-trained language models on different antibody tasks. To investigate the problem, we aim to answer several key questions in this paper, such as how pre-trained language models perform in antibody tasks with different specificity and how introducing specific biological mechanisms to the pre-training process can benefit the model. Additionally, we evaluate if the learned antibody pre-trained representations can be applied to real-world antibody problems, like drug discovery and immune process understanding. Previously, no benchmark available largely hindered the study to answer these questions. To aid in our investigation, we provide an AnTibody Understanding Evaluation (ATUE) benchmark. We comprehensively evaluate the performance of protein pre-trained language models by empirical study along with conclusions and new insights. Our ATUE and code are released at https://github.com/dqwang122/EATLM.",
        "pdf_link": "https://openreview.net/pdf/c0deacf9e860ac34392346ea2fea727ac90bec4a.pdf",
        "forum_url": "https://openreview.net/forum?id=zaq4LV55xHl",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, there have been limited studies that comprehensively explore the representation capability of distinct pre-trained language models on different antibody tasks.\"\n\nThis rating is given because the abstract mentions a limitation of pre-trained language models (limited studies on their representation capability), but it is not the primary focus of the paper and is not explored in depth.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, there have been limited studies that comprehensively explore the representation capability of distinct pre-trained language models on different antibody tasks.\"\n\nThis rating is given because the abstract mentions a limitation of pre-trained language models (limited studies on their representation capability), but it is not the primary focus of the paper and is not explored in depth."
    },
    {
        "title": "Hybrid RL: Using both offline and online data can make RL efficient",
        "authors": [
            "Yuda Song",
            "Yifei Zhou",
            "Ayush Sekhari",
            "Drew Bagnell",
            "Akshay Krishnamurthy",
            "Wen Sun"
        ],
        "published": "iclr 2023 poster",
        "summary": "We consider a hybrid reinforcement learning setting (Hybrid RL), in which an agent has access to an offline dataset and the ability to collect experience via real-world online interaction. The framework mitigates the challenges that arise in both pure offline and online RL settings, allowing for the design of simple and highly effective algorithms, in both theory and practice. We demonstrate these advantages by adapting the classical Q learning/iteration algorithm to the hybrid setting, which we call Hybrid Q-Learning or Hy-Q. In our theoretical results, we prove that the algorithm is both computationally and statistically efficient whenever the offline dataset supports a high-quality policy and the environment has bounded bilinear rank. Notably, we require no assumptions on the coverage provided by the initial distribution, in contrast with guarantees for policy gradient/iteration methods. In our experimental results, we show that Hy-Q with neural network function approximation outperforms state-of-the-art online, offline, and hybrid RL baselines on challenging benchmarks, including Montezuma’s Revenge.",
        "pdf_link": "https://openreview.net/pdf/dc7cd7793233a77b76033b4b3a179d9a40bc657c.pdf",
        "forum_url": "https://openreview.net/forum?id=yyBis80iUuU",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Complexity-Based Prompting for Multi-step Reasoning",
        "authors": [
            "Yao Fu",
            "Hao Peng",
            "Ashish Sabharwal",
            "Peter Clark",
            "Tushar Khot"
        ],
        "published": "iclr 2023 poster",
        "summary": "We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on math word reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority\nof generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3, our approach substantially improves multi-step reasoning accuracy, with an 8.6% absolute improvement on GSM8K, and 6.4% on MathQA. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.",
        "pdf_link": "https://openreview.net/pdf/45dc479ddf081da97bf30a319e61cd0509d1f701.pdf",
        "forum_url": "https://openreview.net/forum?id=yf1icZHC-l9",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation is mentioned in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation is mentioned in the abstract."
    },
    {
        "title": "Leveraging Large Language Models for Multiple Choice Question Answering",
        "authors": [
            "Joshua Robinson",
            "David Wingate"
        ],
        "published": "iclr 2023 poster",
        "summary": "While large language models (LLMs) like GPT-3 have achieved impressive results on multiple choice question answering (MCQA) tasks in the zero, one, and few-shot settings, they generally lag behind the MCQA state of the art (SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks. An LLM is conditioned on a question (without the associated answer options) and its chosen option is the one assigned the highest probability after normalization (for length, etc.). A more natural prompting approach is to present the question and answer options to the LLM jointly and have it output the symbol (e.g., “A”) associated with its chosen answer option. This approach allows the model to explicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer selection. For the natural approach to be effective, the LLM it is used with must be able to associate answer options with the symbols that represent them. The LLM needs what we term multiple choice symbol binding (MCSB) ability. This ability varies greatly by model. We show that a model with high MCSB ability performs much better with the natural approach than with the traditional approach across 20 diverse datasets and largely closes the gap with the SOTA, suggesting that the MCQA ability of LLMs has been previously underestimated.",
        "pdf_link": "https://openreview.net/pdf/1ae9b0968aa2c0f898d5082be403c3070b0c09fc.pdf",
        "forum_url": "https://openreview.net/forum?id=yKbprarjc5B",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The LLM needs what we term multiple choice symbol binding (MCSB) ability. This ability varies greatly by model.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The LLM needs what we term multiple choice symbol binding (MCSB) ability. This ability varies greatly by model.\""
    },
    {
        "title": "Evaluating Long-Term Memory in 3D Mazes",
        "authors": [
            "Jurgis Pašukonis",
            "Timothy P Lillicrap",
            "Danijar Hafner"
        ],
        "published": "iclr 2023 poster",
        "summary": "Intelligent agents need to remember salient information to reason in partially-observed environments. For example, agents with a first-person view should remember the positions of relevant objects even if they go out of view. Similarly, to effectively navigate through rooms agents need to remember the floor plan of how rooms are connected. However, most benchmark tasks in reinforcement learning do not test long-term memory in agents, slowing down progress in this important research direction. In this paper, we introduce the Memory Maze, a 3D domain of randomized mazes specifically designed for evaluating long-term memory in agents. Unlike existing benchmarks, Memory Maze measures long-term memory separate from confounding agent abilities and requires the agent to localize itself by integrating information over time. With Memory Maze, we propose an online reinforcement learning benchmark, a diverse offline dataset, and an offline probing evaluation. Recording a human player establishes a strong baseline and verifies the need to build up and retain memories, which is reflected in their gradually increasing rewards within each episode. We find that current algorithms benefit from training with truncated backpropagation through time and succeed on small mazes, but fall short of human performance on the large mazes, leaving room for future algorithmic designs to be evaluated on the Memory Maze.",
        "pdf_link": "https://openreview.net/pdf/51ee6a2d289af481a877c54cb8295fa6d6a0fc5f.pdf",
        "forum_url": "https://openreview.net/forum?id=yHLvIlE9RGN",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "BrainBERT: Self-supervised representation learning for intracranial recordings",
        "authors": [
            "Christopher Wang",
            "Vighnesh Subramaniam",
            "Adam Uri Yaari",
            "Gabriel Kreiman",
            "Boris Katz",
            "Ignacio Cases",
            "Andrei Barbu"
        ],
        "published": "iclr 2023 poster",
        "summary": "We create a reusable Transformer, BrainBERT, for intracranial recordings bringing modern representation learning approaches to neuroscience. Much like in NLP and speech recognition, this Transformer enables classifying complex concepts, i.e., decoding neural data, with higher accuracy and with much less data by being pretrained in an unsupervised manner on a large corpus of unannotated neural recordings. Our approach generalizes to new subjects with electrodes in new positions and to unrelated tasks showing that the representations robustly disentangle the neural signal. Just like in NLP where one can study language by investigating what a language model learns, this approach opens the door to investigating the brain by what a model of the brain learns. As a first step along this path, we demonstrate a new analysis of the intrinsic dimensionality of the computations in different areas of the brain. To construct these representations, we combine a technique for producing super-resolution spectrograms of neural data with an approach designed for generating contextual representations of audio by masking. In the future, far more concepts will be decodable from neural recordings by using representation learning, potentially unlocking the brain like language models unlocked language.  ",
        "pdf_link": "https://openreview.net/pdf/0ea1fa510d3c8d4dc63c0ae0775f2c2ea4c765ff.pdf",
        "forum_url": "https://openreview.net/forum?id=xmcYx_reUn6",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"potentially unlocking the brain like language models unlocked language.\"\n\nThis paper mentions LLMs, but does not discuss any limitations of LLMs. It uses the analogy with LLMs to describe the potential of the proposed approach in neuroscience.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"potentially unlocking the brain like language models unlocked language.\"\n\nThis paper mentions LLMs, but does not discuss any limitations of LLMs. It uses the analogy with LLMs to describe the potential of the proposed approach in neuroscience."
    },
    {
        "title": "Toward Adversarial Training on Contextualized Language Representation",
        "authors": [
            "Hongqiu Wu",
            "Yongxiang Liu",
            "Hanwen Shi",
            "hai zhao",
            "Min Zhang"
        ],
        "published": "iclr 2023 poster",
        "summary": "Beyond the success story of adversarial training (AT) in the recent text domain on top of pre-trained language models (PLMs), our empirical study showcases the inconsistent gains from AT on some tasks, e.g. commonsense reasoning, named entity recognition. This paper investigates AT from the perspective of the contextualized language representation outputted by PLM encoders. We find the current AT attacks lean to generate sub-optimal adversarial examples that can fool the decoder part but have a minor effect on the encoder. However, we find it necessary to effectively deviate the latter one to allow AT to gain. Based on the observation, we propose simple yet effective \\textit{Contextualized representation-Adversarial Training} (CreAT), in which the attack is explicitly optimized to deviate the contextualized representation of the encoder. It allows a global optimization of adversarial examples that can fool the entire model. We also find CreAT gives rise to a better direction to optimize the adversarial examples, to let them less sensitive to hyperparameters. Compared to AT, CreAT produces consistent performance gains on a wider range of tasks and is proven to be more effective for language pre-training where only the encoder part is kept for downstream tasks. We achieve the new state-of-the-art performances on a series of challenging benchmarks, e.g. AdvGLUE (59.1 $ \\rightarrow $ 61.1), HellaSWAG (93.0  $ \\rightarrow $ 94.9), ANLI (68.1  $ \\rightarrow $ 69.3).",
        "pdf_link": "https://openreview.net/pdf/f99847b969c62de5c7cf014d27a5a81631b43597.pdf",
        "forum_url": "https://openreview.net/forum?id=xZD10GhCvM",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"our empirical study showcases the inconsistent gains from AT on some tasks, e.g. commonsense reasoning, named entity recognition.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"our empirical study showcases the inconsistent gains from AT on some tasks, e.g. commonsense reasoning, named entity recognition.\""
    },
    {
        "title": "Contrastive Alignment of Vision to Language Through Parameter-Efficient Transfer Learning ",
        "authors": [
            "Zaid Khan",
            "Yun Fu"
        ],
        "published": "iclr 2023 poster",
        "summary": "Contrastive vision-language models (e.g. CLIP) are typically created by updating all the parameters of a vision model and language model through contrastive training. Can such models be created by a small number of parameter updates to an already-trained language model and vision model? The literature describes techniques that can create vision-language models by updating a small number of parameters in a language model, but these require already aligned visual representations and are non-contrastive, hence unusable for latency-sensitive applications such as neural search. We explore the feasibility and benefits of parameter-efficient contrastive vision-language alignment through transfer learning: creating a model such as CLIP by minimally updating an already-trained vision and language model. We find that a minimal set of parameter updates ($<$7\\%) can achieve the same performance as full-model training, and updating specific components ($<$1\\% of parameters) can match 75\\% of full-model training. We describe a series of experiments: we show that existing knowledge is conserved more strongly in parameter-efficient training and that parameter-efficient scaling scales with model and dataset size. Where paired-image text data is scarce but strong multilingual language models exist (e.g. low resource languages), parameter-efficient training is even preferable to full-model training. Given a fixed compute budget, parameter-efficient training allows training larger models on the same hardware, achieving equivalent performance in less time. Parameter-efficient training hence constitutes an energy-efficient and effective training strategy for contrastive vision-language models that may be preferable to the full-model training paradigm for common use cases.\nCode and weights at https://github.com/codezakh/LilT.",
        "pdf_link": "https://openreview.net/pdf/616f79e13971d683f9f48a16c7436a115b065d7f.pdf",
        "forum_url": "https://openreview.net/forum?id=x0BPR9iXc1",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but mentions \"Where paired-image text data is scarce but strong multilingual language models exist (e.g. low resource languages)\" which hints at a limitation of LLMs in low-resource languages, but it is not the main focus.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations, but mentions \"Where paired-image text data is scarce but strong multilingual language models exist (e.g. low resource languages)\" which hints at a limitation of LLMs in low-resource languages, but it is not the main focus."
    },
    {
        "title": "Learning to Decompose Visual Features with Latent Textual Prompts",
        "authors": [
            "Feng Wang",
            "Manling Li",
            "Xudong Lin",
            "Hairong Lv",
            "Alex Schwing",
            "Heng Ji"
        ],
        "published": "iclr 2023 poster",
        "summary": "Recent advances in pre-training vision-language models like CLIP have shown great potential in learning transferable visual representations. Nonetheless, for downstream inference, CLIP-like models suffer from either 1) degraded accuracy and robustness in the case of inaccurate text descriptions during retrieval-based inference (the challenge for zero-shot protocol); or 2) breaking the well-established vision-language alignment (the challenge for linear probing). To address them, we propose Decomposed Feature Prompting (DeFo). DeFo leverages a flexible number of learnable embeddings as textual input while maintaining the vision-language dual-model architecture, which enables the model to learn decomposed visual features with the help of feature-level textual prompts. We further use an additional linear layer to perform classification, allowing a scalable size of language inputs. Our empirical study shows DeFo's significance in improving the vision-language models. For example, DeFo obtains 73.2% test accuracy on ImageNet with a ResNet-50 backbone without tuning any pretrained weights of both the vision and language encoder, outperforming zero-shot CLIP by a large margin of 15.0%, and outperforming state-of-the-art vision-language prompt tuning method by 7.6%.",
        "pdf_link": "https://openreview.net/pdf/113fd3d7efcd01c4d918daa9d8c3b5bfc746b4da.pdf",
        "forum_url": "https://openreview.net/forum?id=wtcud6HroZr",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Safe Exploration Incurs Nearly No Additional Sample Complexity for Reward-Free RL",
        "authors": [
            "Ruiquan Huang",
            "Jing Yang",
            "Yingbin Liang"
        ],
        "published": "iclr 2023 poster",
        "summary": "Reward-free reinforcement learning (RF-RL), a recently introduced RL paradigm, relies on random action-taking to explore the unknown environment without any reward feedback information. While the primary goal of the exploration phase in RF-RL is to reduce the uncertainty in the estimated model with minimum number of trajectories, in practice, the agent often needs to abide by certain safety constraint at the same time. It remains unclear how such safe exploration requirement would affect the corresponding sample complexity in order to achieve the desired optimality of the obtained policy in planning. In this work, we make a first attempt to answer this question. In particular, we consider the scenario where a safe baseline policy is known beforehand, and propose a unified Safe reWard-frEe ExploraTion (SWEET) framework. We then particularize the SWEET framework to the tabular and the low-rank MDP settings, and develop algorithms coined Tabular-SWEET and Low-rank-SWEET, respectively. Both algorithms leverage the concavity and continuity of the newly introduced truncated value functions, and are guaranteed to achieve zero constraint violation during exploration with high probability. Furthermore, both algorithms can provably find a near-optimal policy subject to any constraint in the planning phase. Remarkably, the sample complexities under both algorithms match or even outperform the state of the art in their constraint-free counterparts up to some constant factors, proving that safety constraint hardly increases the sample complexity for RF-RL.",
        "pdf_link": "https://openreview.net/pdf/4d89566649ca26b4992a3fab78d893feb9ca8dc1.pdf",
        "forum_url": "https://openreview.net/forum?id=wNUgn1n6esQ",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Bidirectional Language Models Are Also Few-shot Learners",
        "authors": [
            "Ajay Patel",
            "Bryan Li",
            "Mohammad Sadegh Rasooli",
            "Noah Constant",
            "Colin Raffel",
            "Chris Callison-Burch"
        ],
        "published": "iclr 2023 poster",
        "summary": "Large language models such as GPT-3 (Brown et al., 2020) can perform arbitrary tasks without undergoing fine-tuning after being prompted with only a few labeled examples. An arbitrary task can be reformulated as a natural language prompt, and a language model can be asked to generate the completion, indirectly performing the task in a paradigm known as prompt-based learning. To date, emergent prompt-based learning capabilities have mainly been demonstrated for unidirectional language models. However, bidirectional language models pre-trained on denoising objectives such as masked language modeling produce stronger learned representations for transfer learning. This motivates the possibility of prompting bidirectional models, but their pre-training objectives have made them largely incompatible with the existing prompting paradigm. We present SAP (Sequential Autoregressive Prompting), a technique that enables the prompting of bidirectional models. Utilizing the machine translation task as a case study, we prompt the bidirectional mT5 model (Xue et al., 2021) with SAP and demonstrate its few-shot and zero-shot translations outperform the few-shot translations of unidirectional models like GPT-3 and XGLM (Lin et al., 2021), despite mT5's approximately 50% fewer parameters. We further show SAP is effective on question answering and summarization. For the first time, our results demonstrate prompt-based learning is an emergent property of a broader class of language models, rather than only unidirectional models.",
        "pdf_link": "https://openreview.net/pdf/89bcb6060a2f549ce65ae9acf0cddd043ce5ea1d.pdf",
        "forum_url": "https://openreview.net/forum?id=wCFB37bzud4",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, bidirectional language models pre-trained on denoising objectives such as masked language modeling produce stronger learned representations for transfer learning, but their pre-training objectives have made them largely incompatible with the existing prompting paradigm.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, bidirectional language models pre-trained on denoising objectives such as masked language modeling produce stronger learned representations for transfer learning, but their pre-training objectives have made them largely incompatible with the existing prompting paradigm.\""
    },
    {
        "title": "A Non-monotonic Self-terminating Language Model",
        "authors": [
            "Eugene Choi",
            "Kyunghyun Cho",
            "Cheolhyoung Lee"
        ],
        "published": "iclr 2023 poster",
        "summary": "Recent large-scale neural autoregressive sequence models have shown impressive performances on a variety of natural language generation tasks. However, their generated sequences often exhibit degenerate properties such as non-termination, undesirable repetition, and premature termination, when generated with decoding algorithms such as greedy search, beam search, top-$k$ sampling, and nucleus sampling. In this paper, we focus on the problem of non-terminating sequences resulting from an incomplete decoding algorithm. We first define an incomplete probable decoding algorithm which includes greedy search, top-$k$ sampling, and nucleus sampling, beyond the incomplete decoding algorithm originally put forward by Welleck et al. (2020). We then propose a non-monotonic self-terminating language model, which significantly relaxes the constraint of monotonically increasing termination probability in the originally proposed self-terminating language model by Welleck et al. (2020), to address the issue of non-terminating sequences when using incomplete probable decoding algorithms. We prove that our proposed model prevents non-terminating sequences when using not only incomplete probable decoding algorithms but also beam search. We empirically validate our model on sequence completion tasks with various architectures.",
        "pdf_link": "https://openreview.net/pdf/5af17fbafcba261932381990540df8840ffec3f0.pdf",
        "forum_url": "https://openreview.net/forum?id=vw-5EgYbJZr",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their generated sequences often exhibit degenerate properties such as non-termination, undesirable repetition, and premature termination, when generated with decoding algorithms such as greedy search, beam search, top-$k$ sampling, and nucleus sampling.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, their generated sequences often exhibit degenerate properties such as non-termination, undesirable repetition, and premature termination, when generated with decoding algorithms such as greedy search, beam search, top-$k$ sampling, and nucleus sampling.\""
    },
    {
        "title": "Asynchronous Gradient Play in Zero-Sum Multi-agent Games",
        "authors": [
            "Ruicheng Ao",
            "Shicong Cen",
            "Yuejie Chi"
        ],
        "published": "iclr 2023 poster",
        "summary": "Finding equilibria via gradient play in competitive multi-agent games has been attracting a growing amount of attention in recent years, with emphasis on designing efficient strategies where the agents operate in a decentralized and symmetric manner with guaranteed convergence. While significant efforts have been made in understanding zero-sum two-player matrix games, the performance in zero-sum multi-agent games remains inadequately explored, especially in the presence of delayed feedbacks, leaving the scalability and resiliency of gradient play open to questions. In this paper, we make progress by studying asynchronous gradient plays in zero-sum polymatrix games under delayed feedbacks. We first establish that the last iterate of entropy-regularized optimistic multiplicative weight updates (OMWU) method converges linearly to the quantal response equilibrium (QRE), the solution concept under bounded rationality, in the absence of delays. The linear convergence continues to hold even when the feedbacks are randomly delayed under mild statistical assumptions, albeit at a slower rate. Moving beyond random delays, we further demonstrate entropy-regularized OMWU with two-timescale learning rates enjoys faster last-iterate convergence under fixed delays, and continues to converge provably even when the delays are arbitrarily bounded. Our methods also lead to finite-time guarantees to approximate the Nash equilibrium (NE) by moderating the amount of regularization. To the best of our knowledge, this work is the first that aims to understand asynchronous gradient play in zero-sum polymatrix games under a wide range of delay assumptions.\n ",
        "pdf_link": "https://openreview.net/pdf/e4a53ceb226d81ca0b14144280148b4353ae53d7.pdf",
        "forum_url": "https://openreview.net/forum?id=vPXp7K_Yhre",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Learning to reason over visual objects",
        "authors": [
            "Shanka Subhra Mondal",
            "Taylor Whittington Webb",
            "Jonathan Cohen"
        ],
        "published": "iclr 2023 poster",
        "summary": "A core component of human intelligence is the ability to identify abstract patterns inherent in complex, high-dimensional perceptual data, as exemplified by visual reasoning tasks such as Raven’s Progressive Matrices (RPM). Motivated by the goal of designing AI systems with this capacity, recent work has focused on evaluating whether neural networks can learn to solve RPM-like problems. Previous work has generally found that strong performance on these problems requires the incorporation of inductive biases that are specific to the RPM problem format, raising the question of whether such models might be more broadly useful. Here, we investigated the extent to which a general-purpose mechanism for processing visual scenes in terms of objects might help promote abstract visual reasoning. We found that a simple model, consisting only of an object-centric encoder and a transformer reasoning module, achieved state-of-the-art results on both of two challenging RPM-like benchmarks (PGM and I-RAVEN), as well as a novel benchmark with greater visual complexity (CLEVR-Matrices). These results suggest that an inductive bias for object-centric processing may be a key component of abstract visual reasoning, obviating the need for problem-specific inductive biases.",
        "pdf_link": "https://openreview.net/pdf/a254cbbe48d67b9d3a5f5a0c75a1f6f6d56e99f7.pdf",
        "forum_url": "https://openreview.net/forum?id=uR6x8Be7o_M",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Meta-Learning in Games",
        "authors": [
            "Keegan Harris",
            "Ioannis Anagnostides",
            "Gabriele Farina",
            "Mikhail Khodak",
            "Steven Wu",
            "Tuomas Sandholm"
        ],
        "published": "iclr 2023 poster",
        "summary": "In the literature on game-theoretic equilibrium finding, focus has mainly been on solving a single game in isolation. In practice, however, strategic interactions—ranging from routing problems to online advertising auctions—evolve dynamically, thereby leading to many similar games to be solved. To address this gap, we introduce meta-learning for equilibrium finding and learning to play games. We establish the first meta-learning guarantees for a variety of fundamental and well-studied games, including two-player zero-sum games, general-sum games, Stackelberg games, and multiple extensions thereof. In particular, we obtain rates of convergence to different game-theoretic equilibria that depend on natural notions of similarity between the sequence of games encountered, while at the same time recovering the known single-game guarantees when the sequence of games is arbitrary. Along the way, we prove a number of new results in the single-game regime through a simple and unified framework, which may be of independent interest. Finally, we evaluate our meta-learning algorithms on endgames faced by the poker agent Libratus against top human professionals. The experiments show that games with varying stack sizes can be solved significantly faster using our meta-learning techniques than by solving them separately, often by an order of magnitude.",
        "pdf_link": "https://openreview.net/pdf/f9d6c07c1a127336efae79b0887b97faf1fdae60.pdf",
        "forum_url": "https://openreview.net/forum?id=uHaWaNhCvZD",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "MEDICAL IMAGE UNDERSTANDING WITH PRETRAINED VISION LANGUAGE MODELS: A COMPREHENSIVE STUDY",
        "authors": [
            "Ziyuan Qin",
            "Huahui Yi",
            "Qicheng Lao",
            "Kang Li"
        ],
        "published": "iclr 2023 poster",
        "summary": "The large-scale pre-trained vision language models (VLM) have shown remarkable domain transfer capability on natural images. However, it remains unknown whether this capability can also apply to the medical image domain. This paper thoroughly studies the knowledge transferability of pre-trained VLMs to the medical domain, where we show that well-designed medical prompts are the key to elicit knowledge from pre-trained VLMs. We demonstrate that by prompting with expressive attributes that are shared between domains, the VLM can carry the knowledge across domains and improve its generalization. This mechanism empowers VLMs to recognize novel objects with fewer or without image samples. Furthermore, to avoid the laborious manual designing process, we develop three approaches for automatic generation of medical prompts, which can inject expert-level medical knowledge and image-specific information into the prompts for fine-grained grounding. We conduct extensive experiments on thirteen different medical datasets across various modalities, showing that our well-designed prompts greatly improve the zero-shot performance compared to the default prompts, and our fine-tuned models surpass the supervised models by a significant margin.",
        "pdf_link": "https://openreview.net/pdf/8e53cd494ff16bfef607704574e7a1e2c770f607.pdf",
        "forum_url": "https://openreview.net/forum?id=txlWziuCE5W",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it remains unknown whether this capability can also apply to the medical image domain.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of pre-trained VLMs in the medical domain, but it is not the primary focus of the paper and is only briefly mentioned.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, it remains unknown whether this capability can also apply to the medical image domain.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of pre-trained VLMs in the medical domain, but it is not the primary focus of the paper and is only briefly mentioned."
    },
    {
        "title": "Protein Representation Learning by Geometric Structure Pretraining",
        "authors": [
            "Zuobai Zhang",
            "Minghao Xu",
            "Arian Rokkum Jamasb",
            "Vijil Chenthamarakshan",
            "Aurelie Lozano",
            "Payel Das",
            "Jian Tang"
        ],
        "published": "iclr 2023 poster",
        "summary": "Learning effective protein representations is critical in a variety of tasks in biology such as predicting protein function or structure. Existing approaches usually pretrain protein language models on a large number of unlabeled amino acid sequences and then finetune the models with some labeled data in downstream tasks. Despite the effectiveness of sequence-based approaches, the power of pretraining on known protein structures, which are available in smaller numbers only, has not been explored for protein property prediction, though protein structures are known to be determinants of protein function. In this paper, we propose to pretrain protein representations according to their 3D structures. We first present a simple yet effective encoder to learn the geometric features of a protein. We pretrain the protein graph encoder by leveraging multiview contrastive learning and different self-prediction tasks. Experimental results on both function prediction and fold classification tasks show that our proposed pretraining methods outperform or are on par with the state-of-the-art sequence-based methods, while using much less pretraining data. Our implementation is available at https://github.com/DeepGraphLearning/GearNet.",
        "pdf_link": "https://openreview.net/pdf/fcf1d34c0463c5072ee49ce930224e980675829b.pdf",
        "forum_url": "https://openreview.net/forum?id=to3qCB3tOh9",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "OPTQ: Accurate Quantization for Generative Pre-trained Transformers",
        "authors": [
            "Elias Frantar",
            "Saleh Ashkboos",
            "Torsten Hoefler",
            "Dan Alistarh"
        ],
        "published": "iclr 2023 poster",
        "summary": "Generative Pre-trained Transformer models, known as GPT or OPT, set themselves apart through breakthrough performance across complex language modelling tasks, but also by their extremely high computational and storage costs. Specifically, due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs, which limits the usability of such models. While there is emerging work on relieving this pressure via model compression, the applicability and performance of existing compression techniques is limited by the scale and complexity of GPT models. In this paper, we address this challenge, and propose OPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient. Specifically, OPTQ can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3 or 4 bits per weight, with negligible accuracy degradation relative to the uncompressed baseline. Our method more than doubles the compression gains relative to previously-proposed one-shot quantization methods, preserving accuracy, allowing us for the first time to execute an 175 billion-parameter model inside a single GPU for generative inference. Moreover, we also show that our method can still provide reasonable accuracy in the extreme quantization regime, in which weights are quantized to 2-bit or even ternary quantization levels.\nWe show experimentally that these improvements can be leveraged for end-to-end inference speedups over FP16, of around 3.25x when using high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones (NVIDIA A6000). The implementation is available at https://github.com/IST-DASLab/gptq.",
        "pdf_link": "https://openreview.net/pdf/f99f2d5ea7fda817912034e810f9e385d2add0e1.pdf",
        "forum_url": "https://openreview.net/forum?id=tcbBPnfwxS",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs, which limits the usability of such models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs, which limits the usability of such models.\""
    },
    {
        "title": "Broken Neural Scaling Laws",
        "authors": [
            "Ethan Caballero",
            "Kshitij Gupta",
            "Irina Rish",
            "David Krueger"
        ],
        "published": "iclr 2023 poster",
        "summary": "We present a smoothly broken power law functional form (referred to by us as a broken neural scaling law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning (single agent and multi-agent). When compared to other functional forms for neural scaling behavior, this functional form yields extrapolations of scaling behavior that are considerably more accurate on this set. Moreover, this functional form accurately models and extrapolates scaling behavior that other functional forms are incapable of expressing such as the non-monotonic transitions present in the scaling behavior of phenomena such as double descent and the delayed, sharp inflection points present in the scaling behavior of tasks such as arithmetic. Lastly, we use this functional form to glean insights about the limit of the predictability of scaling behavior. See arXiv for longer version of this paper. Code is available at https://github.com/ethancaballero/broken_neural_scaling_laws",
        "pdf_link": "https://openreview.net/pdf/b9c49ee6a7c5cbe69e7836796acf1b820da86174.pdf",
        "forum_url": "https://openreview.net/forum?id=sckjveqlCZ",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"out-of-distribution (OOD) generalization\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"out-of-distribution (OOD) generalization\""
    },
    {
        "title": "Impossibly Good Experts and How to Follow Them",
        "authors": [
            "Aaron Walsman",
            "Muru Zhang",
            "Sanjiban Choudhury",
            "Dieter Fox",
            "Ali Farhadi"
        ],
        "published": "iclr 2023 poster",
        "summary": "We consider the sequential decision making problem of learning from an expert that has access to more information than the learner.  For many problems this extra information will enable the expert to achieve greater long term reward than any policy without this privileged information access.  We call these experts ``Impossibly Good'' because no learning algorithm will be able to reproduce their behavior.  However, in these settings it is reasonable to attempt to recover the best policy possible given the agent's restricted access to information.  We provide a set of necessary criteria on the expert that will allow a learner to recover the optimal policy in the reduced information space from the expert's advice alone.  We also provide a new approach called Elf Distillation (Explorer Learning from Follower) that can be used in cases where these criteria are not met and environmental rewards must be taken into account.  We show that this algorithm performs better than a variety of strong baselines on a challenging suite of Minigrid and Vizdoom environments.",
        "pdf_link": "https://openreview.net/pdf/b9a9d8247f602605afcc256282e3c3bee4b6efa4.pdf",
        "forum_url": "https://openreview.net/forum?id=sciA_xgYofB",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Can discrete information extraction prompts generalize across language models?",
        "authors": [
            "Nathanaël Carraz Rakotonirina",
            "Roberto Dessi",
            "Fabio Petroni",
            "Sebastian Riedel",
            "Marco Baroni"
        ],
        "published": "iclr 2023 poster",
        "summary": "We study whether automatically-induced prompts that effectively extract information from a language model can also be used, out-of-the-box, to probe other language models for the same information. After confirming that discrete prompts induced with the AutoPrompt algorithm outperform manual and semi-manual prompts on the slot-filling task, we demonstrate a drop in performance for AutoPrompt prompts learned on a model and tested on another. We introduce a way to induce prompts by mixing language models at training time that results in prompts that generalize well across models. We conduct an extensive analysis of the induced prompts, finding that the more general prompts include a larger proportion of existing English words and have a less order-dependent and more uniform distribution of information across their component tokens. Our work provides preliminary evidence that it's possible to generate discrete prompts that can be induced once and used with a number of different models, and gives insights on the properties characterizing such prompts.",
        "pdf_link": "https://openreview.net/pdf/027788e7f8d7f512b53ca6e6935d18aa5150e77f.pdf",
        "forum_url": "https://openreview.net/forum?id=sbWVtxq8-zE",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We demonstrate a drop in performance for AutoPrompt prompts learned on a model and tested on another.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We demonstrate a drop in performance for AutoPrompt prompts learned on a model and tested on another.\""
    },
    {
        "title": "Investigating Multi-task Pretraining and Generalization in Reinforcement Learning",
        "authors": [
            "Adrien Ali Taiga",
            "Rishabh Agarwal",
            "Jesse Farebrother",
            "Aaron Courville",
            "Marc G Bellemare"
        ],
        "published": "iclr 2023 poster",
        "summary": "Deep reinforcement learning~(RL) has achieved remarkable successes in complex single-task settings. However, designing RL agents that can learn multiple tasks and leverage prior experience to quickly adapt to a related new task remains challenging. Despite previous attempts to improve on these areas, our understanding of multi-task training and generalization in RL remains limited. To fill this gap, we investigate the generalization capabilities of a popular actor-critic method, IMPALA. Specifically, we build on previous work that has advocated for the use of modes and difficulties of Atari 2600 games as a challenging benchmark for transfer learning in RL. We do so by pretraining an agent on multiple variants of the same Atari game before fine-tuning on the remaining never-before-seen variants. This protocol simplifies the multi-task pretraining phase by limiting negative interference between tasks and allows us to better understand the dynamics of multi-task training and generalization. We find that, given a fixed amount of pretraining data, agents trained with more variations are able to generalize better. Surprisingly, we also observe that this advantage can still be present after fine-tuning for 200M environment frames than when doing zero-shot transfer. This highlights the potential effect of a good learned representation. We also find that, even though small networks have remained popular to solve Atari 2600 games, increasing the capacity of the value and policy network is critical to achieve good performance as we increase the number of pretraining modes and difficulties. Overall, our findings emphasize key points that are essential for efficient multi-task training and generalization in reinforcement learning.",
        "pdf_link": "https://openreview.net/pdf/dc00572cbf1ba37d6927b5663d6ca68300b6678e.pdf",
        "forum_url": "https://openreview.net/forum?id=sSt9fROSZRO",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Temporal Disentanglement of Representations for Improved Generalisation in Reinforcement Learning",
        "authors": [
            "Mhairi Dunion",
            "Trevor McInroe",
            "Kevin Sebastian Luck",
            "Josiah P. Hanna",
            "Stefano V Albrecht"
        ],
        "published": "iclr 2023 poster",
        "summary": "Reinforcement Learning (RL) agents are often unable to generalise well to environment variations in the state space that were not observed during training. This issue is especially problematic for image-based RL, where a change in just one variable, such as the background colour, can change many pixels in the image. The changed pixels can lead to drastic changes in the agent's latent representation of the image, causing the learned policy to fail. To learn more robust representations, we introduce TEmporal Disentanglement (TED), a self-supervised auxiliary task that leads to disentangled image representations exploiting the sequential nature of RL observations. We find empirically that RL algorithms utilising TED as an auxiliary task adapt more quickly to changes in environment variables with continued training compared to state-of-the-art representation learning methods. Since TED enforces a disentangled structure of the representation, our experiments also show that policies trained with TED generalise better to unseen values of variables irrelevant to the task (e.g. background colour) as well as unseen values of variables that affect the optimal policy (e.g. goal positions).",
        "pdf_link": "https://openreview.net/pdf/01ae4a17b982a43d3a961ee90d4d05b788c87c3b.pdf",
        "forum_url": "https://openreview.net/forum?id=sPgP6aISLTD",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "MAESTRO: Open-Ended Environment Design for Multi-Agent Reinforcement Learning",
        "authors": [
            "Mikayel Samvelyan",
            "Akbir Khan",
            "Michael D Dennis",
            "Minqi Jiang",
            "Jack Parker-Holder",
            "Jakob Nicolaus Foerster",
            "Roberta Raileanu",
            "Tim Rocktäschel"
        ],
        "published": "iclr 2023 poster",
        "summary": "Open-ended learning methods that automatically generate a curriculum of increasingly challenging tasks serve as a promising avenue toward generally capable reinforcement learning agents. Existing methods adapt curricula independently over either environment parameters (in single-agent settings) or co-player policies (in multi-agent settings). However, the strengths and weaknesses of co-players can manifest themselves differently depending on environmental features. It is thus crucial to consider the dependency between the environment and co-player when shaping a curriculum in multi-agent domains. In this work, we use this insight and extend Unsupervised Environment Design (UED) to multi-agent environments. We then introduce Multi-Agent Environment Design Strategist for Open-Ended Learning (MAESTRO), the first multi-agent UED approach for two-player zero-sum settings. MAESTRO efficiently produces adversarial, joint curricula over both environments and co-players and attains minimax-regret guarantees at Nash equilibrium. Our experiments show that MAESTRO outperforms a number of strong baselines on competitive two-player games, spanning discrete and continuous control settings.",
        "pdf_link": "https://openreview.net/pdf/e65b3ee9b4e5db11d48c773dc4e02702868ef8e6.pdf",
        "forum_url": "https://openreview.net/forum?id=sKWlRDzPfd7",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing",
        "authors": [
            "Pengcheng He",
            "Jianfeng Gao",
            "Weizhu Chen"
        ],
        "published": "iclr 2023 poster",
        "summary": "This paper presents a new pre-trained language model, NewModel, which improves the original DeBERTa model by replacing mask language modeling (MLM) with replaced token detection (RTD), a more sample-efficient pre-training task. Our analysis shows that vanilla embedding sharing in ELECTRA hurts training efficiency and model performance. This is because the training losses of the discriminator and the generator pull token embeddings in different directions, creating the “tug-of-war” dynamics. We thus propose a new gradient-disentangled embedding sharing method that avoids the tug-of-war dynamics, improving both training efficiency and the quality of the pre-trained model. We have pre-trained NewModel using the same settings as DeBERTa to demonstrate its exceptional performance on a wide range of downstream natural language understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an example, the NewModel Large model achieves a 91.37% average score, which is 1.37% over DeBERTa and 1.91% over ELECTRA, setting a new state-of-the-art (SOTA) among the models with a similar structure. Furthermore, we have pre-trained a multi-lingual model mNew-Model and observed a larger improvement over strong baselines compared to English models. For example, the mNewModel Base achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6% improvement over XLM-R Base, creating a new SOTA on this benchmark. We will make our model and code publicly available.",
        "pdf_link": "https://openreview.net/pdf/553181e6a53d384858f9fdfabb4dc41b0c245d8e.pdf",
        "forum_url": "https://openreview.net/forum?id=sE7-XhLxHA",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned, but the paper discusses improving the original DeBERTa model, implying that the original model had limitations that are being addressed.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned, but the paper discusses improving the original DeBERTa model, implying that the original model had limitations that are being addressed."
    },
    {
        "title": "SCoMoE: Efficient Mixtures of Experts with Structured Communication",
        "authors": [
            "zhiyuan zeng",
            "Deyi Xiong"
        ],
        "published": "iclr 2023 poster",
        "summary": "  Mixture-of-Experts (MoE) models are promising architectures for massively multilingual neural machine translation and large language models due to the advantage of sublinear scaling. However, the training of large MoE models is usually bottlenecked by the all-to-all communication (Lepikhin et al., 2020). To reduce the communication cost, we propose SCoMoE, an MoE architecture with structured all-to-all communication, inspired by the hierarchical architecture of the communication topology. SCoMoE encourages data to be communicated across devices through fast intra-accelerator/node communication channels, reducing communication throughput in the slow inter-node communication channel. We slice the data on the sequence dimension (SCoMoE-Seq) into three communication groups and project the data on the feature dimension (SCoMoE-Feat) into low-dimensional representations. To compensate the potential performance drop caused by the routing locality in SCoMoE, we further propose a token clustering approach to aggregating related tokens from different devices before the MoE layers. The sigmoid gating in the balanced router used in the token clustering is substituted with the softmax gating with differential sorting. Experiments on bilingual and massively multilingual machine translation demonstrate that SCoMoE achieves a speedup of 1.44x over GShard with comparable performance, and substantially outperforms Gshard (2.8 BLEU) on OPUS-100 with a speedup of 1.25x.",
        "pdf_link": "https://openreview.net/pdf/ac600913a3de976ce9df830677f9ddacb13a4838.pdf",
        "forum_url": "https://openreview.net/forum?id=s-c96mSU0u5",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the training of large MoE models is usually bottlenecked by the all-to-all communication (Lepikhin et al., 2020).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the training of large MoE models is usually bottlenecked by the all-to-all communication (Lepikhin et al., 2020).\""
    },
    {
        "title": "Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models",
        "authors": [
            "Liam H Fowl",
            "Jonas Geiping",
            "Steven Reich",
            "Yuxin Wen",
            "Wojciech Czaja",
            "Micah Goldblum",
            "Tom Goldstein"
        ],
        "published": "iclr 2023 poster",
        "summary": "Privacy is a central tenet of Federated learning (FL), in which a central server trains models without centralizing user data. However, gradient updates used in FL can leak user information.  While the most industrial uses of FL are for text applications (e.g. keystroke prediction), the majority of attacks on user privacy in FL have focused on simple image classifiers and threat models that assume honest execution of the FL protocol from the server. We propose a novel attack that reveals private user text by deploying malicious parameter vectors, and which succeeds even with mini-batches, multiple users, and long sequences. Unlike previous attacks on FL, the attack exploits characteristics of both the Transformer architecture and the token embedding, separately extracting tokens and positional embeddings to retrieve high-fidelity text. We argue that the threat model of malicious server states is highly relevant from a user-centric perspective, and show that in this scenario, text applications using transformer models are much more vulnerable than previously thought. ",
        "pdf_link": "https://openreview.net/pdf/b2cbf4d1dffc2268fe28070b8b6a4e40118f959c.pdf",
        "forum_url": "https://openreview.net/forum?id=r0BrY4BiEXO",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We argue that the threat model of malicious server states is highly relevant from a user-centric perspective, and show that in this scenario, text applications using transformer models are much more vulnerable than previously thought.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We argue that the threat model of malicious server states is highly relevant from a user-centric perspective, and show that in this scenario, text applications using transformer models are much more vulnerable than previously thought.\""
    },
    {
        "title": "Selective Annotation Makes Language Models Better Few-Shot Learners",
        "authors": [
            "Hongjin SU",
            "Jungo Kasai",
            "Chen Henry Wu",
            "Weijia Shi",
            "Tianlu Wang",
            "Jiayi Xin",
            "Rui Zhang",
            "Mari Ostendorf",
            "Luke Zettlemoyer",
            "Noah A. Smith",
            "Tao Yu"
        ],
        "published": "iclr 2023 poster",
        "summary": "Many recent approaches to natural language tasks are built on the remarkable abilities of large language models. Large language models can perform in-context learning, where they learn a new task from a few task demonstrations, without any parameter updates. This work examines the implications of in-context learning for the creation of datasets for new natural language tasks. Departing from recent in-context learning methods, we formulate an annotation-efficient, two-step framework: selective annotation that chooses a pool of examples to annotate from unlabeled data in advance, followed by prompt retrieval that retrieves task examples from the annotated pool at test time. Based on this framework, we propose an unsupervised, graph-based selective annotation method, voke-k, to select diverse, representative examples to annotate. Extensive experiments on 10 datasets (covering classification, commonsense reasoning, dialogue, and text/code generation) demonstrate that our selective annotation method improves the task performance by a large margin. On average, vote-k achieves a 12.9%/11.4% relative gain under an annotation budget of 18/100, as compared to randomly selecting examples to annotate. Compared to state-of-the-art supervised finetuning approaches, it yields similar performance with 10-100x less annotation cost across 10 tasks. We further analyze the effectiveness of our framework in various scenarios: language models with varying sizes, alternative selective annotation methods, and cases where there is a test data domain shift. We hope that our studies will serve as a basis for data annotations as large language models are increasingly applied to new tasks.",
        "pdf_link": "https://openreview.net/pdf/6e58f3c9c108d2d4b15102e79b138f8a4177b0ad.pdf",
        "forum_url": "https://openreview.net/forum?id=qY1hlv7gwg",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract discusses the implications of in-context learning for dataset creation and proposes a framework to improve task performance, but does not mention any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract discusses the implications of in-context learning for dataset creation and proposes a framework to improve task performance, but does not mention any limitations of LLMs."
    },
    {
        "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
        "authors": [
            "Abulhair Saparov",
            "He He"
        ],
        "published": "iclr 2023 poster",
        "summary": "Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",
        "pdf_link": "https://openreview.net/pdf/e73172f359a19430928855ff049b5dd1e7a4d987.pdf",
        "forum_url": "https://openreview.net/forum?id=qFVVBzXxR2V",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.\""
    },
    {
        "title": "Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning",
        "authors": [
            "XIANGYU PENG",
            "Chen Xing",
            "Prafulla Kumar Choubey",
            "Chien-Sheng Wu",
            "Caiming Xiong"
        ],
        "published": "iclr 2023 poster",
        "summary": "Prompt tuning approaches, which learn task-specific soft prompts for a downstream task conditioning on frozen pre-trained models, have attracted growing interest due to its parameter efficiency. With large language models and sufficient training data, prompt tuning performs comparably to full-model tuning. However, with limited training samples in few-shot settings, prompt tuning fails to match the performance of full-model fine-tuning. In this work, we focus on improving the few-shot performance of prompt tuning by transferring knowledge from soft prompts of source tasks with abundant training samples. Recognizing the good generalization capabilities of ensemble methods in low-data regime, we first experiment and show that a simple ensemble of model predictions based on different source prompts, outperforms existing multi-prompt knowledge transfer approaches such as source prompt fusion in the few-shot setting. Motivated by this observation, we further investigate model ensembles and  propose Sample-specific Ensemble of Source Models (SESoM). SESoM learns to adjust the contribution of each source model for each target sample separately when ensembling source model outputs. Through this way, SESoM inherits the superior generalization of ensemble methods and simultaneously captures the sample-specific competence of each source prompt.  We conduct experiments across a diverse set of eight NLP tasks using models of different scales (T5-\\{base, large, XL\\}) and find that SESoM consistently outperforms the existing models of the same as well as larger parametric scale by a large margin.",
        "pdf_link": "https://openreview.net/pdf/0da427b46bc6cf66b7284999d1ecd003cb8d4c3d.pdf",
        "forum_url": "https://openreview.net/forum?id=p0yrSRbN5Bu",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, with limited training samples in few-shot settings, prompt tuning fails to match the performance of full-model fine-tuning.\"\n\nThis paper mentions a limitation of prompt tuning for LLMs in few-shot settings, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, with limited training samples in few-shot settings, prompt tuning fails to match the performance of full-model fine-tuning.\"\n\nThis paper mentions a limitation of prompt tuning for LLMs in few-shot settings, but it is not the primary focus of the paper."
    },
    {
        "title": "KwikBucks: Correlation Clustering with Cheap-Weak and Expensive-Strong Signals",
        "authors": [
            "Sandeep Silwal",
            "Sara Ahmadian",
            "Andrew Nystrom",
            "Andrew McCallum",
            "Deepak Ramachandran",
            "Seyed Mehran Kazemi"
        ],
        "published": "iclr 2023 poster",
        "summary": "The unprecedented rate at which the sizes of machine learning (ML) models are growing necessitates novel approaches to enable efficient and scalable solutions. We contribute to this line of work by studying a novel version of the Budgeted Correlation Clustering problem (\\bcc) where along with a limited number of queries to an expensive oracle for node similarities (e.g. a large ML model), we have unlimited access to a cheaper but less accurate second oracle. Our formulation is inspired by many practical scenarios where coarse approximations of the expensive similarity metric can be efficiently obtained via weaker models. We develop a theoretically motivated algorithm in this setting that leverages the cheap oracle to judiciously query the strong oracle while maintaining high clustering quality. We empirically demonstrate gains in query minimization and clustering metrics on a variety of datasets with diverse strong and cheap oracles. Most notably, we demonstrate a practical application in text clustering based on expensive cross-attention language models by showing that cheaper (but weaker) embedding-based models can be leveraged to substantially reduce the number of inference calls to the former.",
        "pdf_link": "https://openreview.net/pdf/405e4387799886dd6afb53d4b2fb1eaf9fea1ae8.pdf",
        "forum_url": "https://openreview.net/forum?id=p0JSSa1AuV",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Most notably, we demonstrate a practical application in text clustering based on expensive cross-attention language models by showing that cheaper (but weaker) embedding-based models can be leveraged to substantially reduce the number of inference calls to the former.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of expensive cross-attention language models (high inference cost) and proposes a solution to",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Most notably, we demonstrate a practical application in text clustering based on expensive cross-attention language models by showing that cheaper (but weaker) embedding-based models can be leveraged to substantially reduce the number of inference calls to the former.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of expensive cross-attention language models (high inference cost) and proposes a solution to"
    },
    {
        "title": "Exploring the Limits of Differentially Private Deep Learning with Group-wise Clipping",
        "authors": [
            "Jiyan He",
            "Xuechen Li",
            "Da Yu",
            "Huishuai Zhang",
            "Janardhan Kulkarni",
            "Yin Tat Lee",
            "Arturs Backurs",
            "Nenghai Yu",
            "Jiang Bian"
        ],
        "published": "iclr 2023 poster",
        "summary": "Differentially private deep learning has recently witnessed advances in computational efficiency and privacy-utility trade-off. We explore whether further improvements along the two axes are possible and provide affirmative answers leveraging two instantiations of \\emph{group-wise clipping}.  To reduce the compute time overhead of private learning, we show that \\emph{per-layer clipping}, where the gradient of each neural network layer is clipped separately, allows clipping to be performed in conjunction with backpropagation in differentially private optimization. This results in private learning that is as memory-efficient and almost as fast per training update as non-private learning for many workflows of interest.  While per-layer clipping with constant thresholds tends to underperform standard flat clipping, per-layer clipping with adaptive thresholds matches or outperforms flat clipping under given training epoch constraints, hence attaining similar or better task performance within less wall time. To explore the limits of scaling (pretrained) models in differentially private deep learning, we privately fine-tune the 175 billion-parameter GPT-3.  We bypass scaling challenges associated with clipping gradients that are distributed across multiple devices with \\emph{per-device clipping} that clips the gradient of each model piece separately on its host device. Privately fine-tuning GPT-3 with per-device clipping achieves a task performance at $\\epsilon=1$ better than what is attainable by non-privately fine-tuning the largest GPT-2 on a summarization task.",
        "pdf_link": "https://openreview.net/pdf/0090ccfeba79836829be2f524cc06c2d90ef6180.pdf",
        "forum_url": "https://openreview.net/forum?id=oze0clVGPeX",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While per-layer clipping with constant thresholds tends to underperform standard flat clipping, per-layer clipping with adaptive thresholds matches or outperforms flat clipping under given training epoch constraints, hence attaining similar or better task performance within less wall time.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While per-layer clipping with constant thresholds tends to underperform standard flat clipping, per-layer clipping with adaptive thresholds matches or outperforms flat clipping under given training epoch constraints, hence attaining similar or better task performance within less wall time.\""
    },
    {
        "title": "Minimum Description Length Control",
        "authors": [
            "Ted Moskovitz",
            "Ta-Chu Kao",
            "Maneesh Sahani",
            "Matthew Botvinick"
        ],
        "published": "iclr 2023 poster",
        "summary": "We propose a novel framework for multitask reinforcement learning based on the minimum description length (MDL) principle. In this approach, which we term MDL-control (MDL-C), the agent learns the common structure among the tasks with which it is faced and then distills it into a simpler representation which facilitates faster convergence and generalization to new tasks. In doing so, MDL-C naturally balances adaptation to each task with epistemic uncertainty about the task distribution. We motivate MDL-C via formal connections between the MDL principle and Bayesian inference, derive theoretical performance guarantees, and demonstrate MDL-C's empirical effectiveness on both discrete and high-dimensional continuous control tasks.",
        "pdf_link": "https://openreview.net/pdf/5871e6989fbc1c5944dd1b97644f24a796d43f39.pdf",
        "forum_url": "https://openreview.net/forum?id=oX3tGygjW1q",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Proto-Value Networks: Scaling Representation Learning with Auxiliary Tasks",
        "authors": [
            "Jesse Farebrother",
            "Joshua Greaves",
            "Rishabh Agarwal",
            "Charline Le Lan",
            "Ross Goroshin",
            "Pablo Samuel Castro",
            "Marc G Bellemare"
        ],
        "published": "iclr 2023 poster",
        "summary": "Auxiliary tasks improve the representations learned by deep reinforcement learning agents. Analytically, their effect is reasonably well-understood; in practice, how-ever, their primary use remains in support of a main learning objective, rather than as a method for learning representations. This is perhaps surprising given that many auxiliary tasks are defined procedurally, and hence can be treated as an essentially infinite source of information about the environment. Based on this observation, we study the effectiveness of auxiliary tasks for learning rich representations, focusing on the setting where the number of tasks and the size of the agent’s network are simultaneously increased. For this purpose, we derive a new family of auxiliary tasks based on the successor measure. These tasks are easy to implement and have appealing theoretical properties. Combined with a suitable off-policy learning rule, the result is a representation learning algorithm that can be understood as extending Mahadevan & Maggioni (2007)’s proto-value functions to deep reinforcement learning – accordingly, we call the resulting object proto-value networks. Through a series of experiments on the Arcade Learning Environment, we demonstrate that proto-value networks produce rich features that may be used to obtain performance comparable to established algorithms, using only linear approximation and a small number (~4M) of interactions with the environment’s reward function.",
        "pdf_link": "https://openreview.net/pdf/53dd973b091c37ecf6340230fe3534cbd485e01c.pdf",
        "forum_url": "https://openreview.net/forum?id=oGDKSt9JrZi",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs."
    },
    {
        "title": "Prototypical Calibration for Few-shot Learning of Language Models",
        "authors": [
            "Zhixiong Han",
            "Yaru Hao",
            "Li Dong",
            "Yutao Sun",
            "Furu Wei"
        ],
        "published": "iclr 2023 poster",
        "summary": "In-context learning of GPT-like models has been recognized as fragile across different hand-crafted templates, and demonstration permutations. In this work, we propose prototypical calibration to adaptively learn a more robust decision boundary for zero- and few-shot classification, instead of greedy decoding. Concretely, our method first adopts Gaussian mixture distribution to estimate the prototypical clusters for all categories. Then we assign each cluster to the corresponding label by solving a weighted bipartite matching problem. Given an example, its prediction is calibrated by the likelihood of prototypical clusters. Experimental results show that prototypical calibration yields a substantial improvement on a diverse set of tasks. Extensive analysis across different scales also indicates that our method calibrates the decision boundary as expected, greatly improving the robustness of GPT to templates, permutations, and class imbalance.",
        "pdf_link": "https://openreview.net/pdf/a3d3c6d2541dcef2a64adf0dc44b317522722d44.pdf",
        "forum_url": "https://openreview.net/forum?id=nUsP9lFADUF",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"In-context learning of GPT-like models has been recognized as fragile across different hand-crafted templates, and demonstration permutations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"In-context learning of GPT-like models has been recognized as fragile across different hand-crafted templates, and demonstration permutations.\""
    },
    {
        "title": "Revisiting Populations in multi-agent Communication",
        "authors": [
            "Paul Michel",
            "Mathieu Rita",
            "Kory Wallace Mathewson",
            "Olivier Tieleman",
            "Angeliki Lazaridou"
        ],
        "published": "iclr 2023 poster",
        "summary": "Despite evidence from cognitive sciences that larger groups of speakers tend to develop more structured languages in human communication, scaling up to populations has failed to yield significant benefits in emergent multi-agent communication. In this paper we advocate for an alternate population-level training paradigm for referential games based on the idea of \"partitioning\" the agents into sender-receiver pairs and limiting co-adaptation across pairs. We show that this results in optimizing a different objective at the population level, where agents maximize (1) their respective \"internal\" communication accuracy and (2) some measure of alignment between agents. In experiments, we find that this leads to the emergence of languages that are significantly more compositional. Moreover, when agents are trained in populations that are not fully connected (ie. not all agent pairs interact at training time), this approach reduces multi-linguality and improves zero-shot communication with new agents (ie. agents are able to communicate successfully with other agents outside their training partners).",
        "pdf_link": "https://openreview.net/pdf/670a147872e92b070c64ecc75a028560548072a4.pdf",
        "forum_url": "https://openreview.net/forum?id=n-UHRIdPju",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Continual Pre-training of Language Models",
        "authors": [
            "Zixuan Ke",
            "Yijia Shao",
            "Haowei Lin",
            "Tatsuya Konishi",
            "Gyuhak Kim",
            "Bing Liu"
        ],
        "published": "iclr 2023 poster",
        "summary": "Language models (LMs) have been instrumental for the rapid advance of natural language processing. This paper studies continual pre-training of LMs, in particular, continual domain-adaptive pre-training (or continual DAP-training). Existing research has shown that further pre-training an LM using a domain corpus to adapt the LM to the domain can improve the end-task performance in the domain. This paper proposes a novel method to continually DAP-train an LM with a sequence of unlabeled domain corpora to adapt the LM to these domains to improve their end-task performances. The key novelty of our method is a soft-masking mechanism that directly controls the update to the LM. A novel proxy is also proposed to preserve the general knowledge in the original LM. Additionally, it contrasts the representations of the previously learned domain knowledge (including the general knowledge in the pre-trained LM) and the knowledge from the current full network to achieve knowledge integration. The method not only overcomes catastrophic forgetting, but also achieves knowledge transfer to improve end-task performances. Empirical evaluation demonstrates the effectiveness of the proposed method.",
        "pdf_link": "https://openreview.net/pdf/9ac4b567f0aac42656ddf1612df4382c308719f1.pdf",
        "forum_url": "https://openreview.net/forum?id=m_GDIItaI3o",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"overcomes catastrophic forgetting\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"overcomes catastrophic forgetting\""
    },
    {
        "title": "Compositional Prompt Tuning with Motion Cues for Open-vocabulary Video Relation Detection",
        "authors": [
            "Kaifeng Gao",
            "Long Chen",
            "Hanwang Zhang",
            "Jun Xiao",
            "Qianru Sun"
        ],
        "published": "iclr 2023 poster",
        "summary": "Prompt tuning with large-scale pretrained vision-language models empowers open-vocabulary prediction trained on limited base categories, e.g., object classification and detection. In this paper, we propose compositional prompt tuning with motion cues: an extended prompt tuning paradigm for compositional predictions of video data. In particular, we present Relation Prompt (RePro) for Open-vocabulary Video Visual Relation Detection (Open-VidVRD), where conventional prompt tuning is easily biased to certain subject-object combinations and motion patterns. To this end, RePro addresses the two technical challenges of Open-VidVRD: 1) the prompt tokens should respect the two different semantic roles of subject and object, and 2) the tuning should account for the diverse spatiotemporal motion patterns of the subject-object compositions. Our RePro achieves a new state-of-the-art performance on two VidVRD benchmarks of not only the base training object and predicate categories, but also the unseen ones. Extensive ablations also demonstrate the effectiveness of the proposed compositional and multi-mode design of prompt. Code is available at https://github.com/Dawn-LX/OpenVoc-VidVRD.",
        "pdf_link": "https://openreview.net/pdf/c9c64a5b7d6d1b3b977f973ab553b08580576420.pdf",
        "forum_url": "https://openreview.net/forum?id=mE91GkXYipg",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"conventional prompt tuning is easily biased to certain subject-object combinations and motion patterns.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of conventional prompt tuning (bias to certain subject-object combinations and motion patterns) but does not elaborate on it and focuses on the proposed solution (RePro) to address this limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"conventional prompt tuning is easily biased to certain subject-object combinations and motion patterns.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of conventional prompt tuning (bias to certain subject-object combinations and motion patterns) but does not elaborate on it and focuses on the proposed solution (RePro) to address this limitation."
    },
    {
        "title": "Continuous pseudo-labeling from the start",
        "authors": [
            "Dan Berrebbi",
            "Ronan Collobert",
            "Samy Bengio",
            "Navdeep Jaitly",
            "Tatiana Likhomanenko"
        ],
        "published": "iclr 2023 poster",
        "summary": "Self-training (ST), or pseudo-labeling has sparked significant interest in the automatic speech recognition (ASR) community recently because of its success in harnessing unlabeled data. Unlike prior semi-supervised learning approaches that relied on iteratively regenerating pseudo-labels (PLs) from a trained model and using them to train a new model, recent state-of-the-art methods perform `continuous training' where PLs are generated using a very recent version of the model being trained. Nevertheless, these approaches still rely on bootstrapping the ST using an initial supervised learning phase where the model is trained on labeled data alone. We believe this has the potential for over-fitting to the labeled dataset in low resource settings and that ST from the start of training should reduce over-fitting. In this paper we show how we can do this by dynamically controlling the evolution of PLs during the training process in ASR. To the best of our knowledge, this is the first study that shows the feasibility of generating PLs from the very start of the training. We are able to achieve this using two techniques that avoid instabilities which lead to degenerate models that do not generalize. Firstly, we control the evolution of PLs through a curriculum that uses the online changes in PLs to control the membership of the cache of PLs and improve generalization. Secondly, we find that by sampling transcriptions from the predictive distribution, rather than only using the best transcription, we can stabilize training further. With these techniques, our ST models match prior works without an external language model.",
        "pdf_link": "https://openreview.net/pdf/798c3dbf85d7a7bebf220083f508c53e8569a286.pdf",
        "forum_url": "https://openreview.net/forum?id=m3twGT2bAug",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning ",
        "authors": [
            "Qingru Zhang",
            "Minshuo Chen",
            "Alexander Bukharin",
            "Pengcheng He",
            "Yu Cheng",
            "Weizhu Chen",
            "Tuo Zhao"
        ],
        "published": "iclr 2023 poster",
        "summary": "Fine-tuning large pre-trained language models on downstream tasks has become an important paradigm in NLP. However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental updates of pre-trained weights in a parameter efficient way, e.g., low-rank increments. These methods often evenly distribute the budget of incremental updates across all pre-trained weight matrices, and overlook the varying importance of different weight parameters. As a consequence, the fine-tuning performance is suboptimal. To bridge this gap, we propose AdaLoRA, which adaptively allocates the parameter budget among weight matrices according to their importance score. In particular, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition. Such a novel approach allows us to effectively prune the singular values of unimportant updates, which is essentially to reduce their parameter budget but circumvent intensive exact SVD computations. We conduct extensive experiments with several pre-trained models on natural language processing, question answering, and natural language generation to validate the effectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable improvement over baselines, especially in the low budget settings. Our code is publicly available at https://github.com/QingruZhang/AdaLoRA .",
        "pdf_link": "https://openreview.net/pdf/00a236907b21d7118010fb705f96b4a934acff26.pdf",
        "forum_url": "https://openreview.net/forum?id=lq62uWRJjiY",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present.\""
    },
    {
        "title": "Relative Behavioral Attributes: Filling the Gap between Symbolic Goal Specification and Reward Learning from Human Preferences",
        "authors": [
            "Lin Guan",
            "Karthik Valmeekam",
            "Subbarao Kambhampati"
        ],
        "published": "iclr 2023 poster",
        "summary": "Generating complex behaviors that satisfy the preferences of non-expert users is a crucial requirement for AI agents. Interactive reward learning from trajectory comparisons (a.k.a. RLHF) is one way to allow non-expert users to convey complex objectives by expressing preferences over short clips of agent behaviors. Even though this parametric method can encode complex tacit knowledge present in the underlying tasks, it implicitly assumes that the human is unable to provide richer feedback than binary preference labels, leading to intolerably high feedback complexity and poor user experience. While providing a detailed symbolic closed-form specification of the objectives might be tempting, it is not always feasible even for an expert user. However, in most cases, humans are aware of how the agent should change its behavior along meaningful axes to fulfill their underlying purpose, even if they are not able to fully specify task objectives symbolically. Using this as motivation, we introduce the notion of Relative Behavioral Attributes, which allows the users to tweak the agent behavior through symbolic concepts (e.g., increasing the softness or speed of agents' movement). We propose two practical methods that can learn to model any kind of behavioral attributes from ordered behavior clips. We demonstrate the effectiveness of our methods on four tasks with nine different behavioral attributes, showing that once the attributes are learned, end users can produce desirable agent behaviors relatively effortlessly, by providing feedback just around ten times. This is over an order of magnitude less than that required by the popular learning-from-human-preferences baselines. The supplementary video and source code are available at: https://guansuns.github.io/pages/rba.",
        "pdf_link": "https://openreview.net/pdf/8ddd06f0bef80e495ca1650eb74874f576793c38.pdf",
        "forum_url": "https://openreview.net/forum?id=lGz9u1ubUXE",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "CodeT:  Code Generation with Generated Tests",
        "authors": [
            "Bei Chen",
            "Fengji Zhang",
            "Anh Nguyen",
            "Daoguang Zan",
            "Zeqi Lin",
            "Jian-Guang Lou",
            "Weizhu Chen"
        ],
        "published": "iclr 2023 poster",
        "summary": "The task of generating code solutions for a given programming problem can benefit from the use of pre-trained language models such as Codex, which can produce multiple diverse samples. However, a major challenge for this task is to select the most appropriate solution from the multiple samples generated by the pre-trained language models. A natural way to evaluate the quality and correctness of a code solution is to run it against a set of test cases, but the manual creation of such test cases is often costly and time-consuming. In this paper, we propose a novel method, CodeT, that leverages the same pre-trained language models to automatically generate test cases for the code samples, thus reducing the human effort and increasing the coverage of the test scenarios. CodeT then executes the code samples using the generated test cases, and performs a dual execution agreement, which considers both the consistency of the outputs against the generated test cases and the agreement of the outputs with other code samples. We conduct comprehensive experiments on four benchmarks, HumanEval, MBPP, APPS and CodeContests, using five different pre-trained language models with varying sizes and capabilities. Our results show that CodeT can significantly improve the performance of code solution selection over previous methods, achieving remarkable and consistent gains across different models and benchmarks. For instance, CodeT improves the pass@1 metric on HumanEval to 65.8%, which represents an absolute improvement of 18.8% over the code-davinci-002 model, and an absolute improvement of more than 20% over the previous state-of-the-art results.",
        "pdf_link": "https://openreview.net/pdf/39c039dd3e5f58baafde9d304a11e04a606dda0b.pdf",
        "forum_url": "https://openreview.net/forum?id=ktrw68Cmu9c",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, a major challenge for this task is to select the most appropriate solution from the multiple samples generated by the pre-trained language models.\"\n\nThis paper is rated with 1 because it mentions a limitation of pre-trained language models (the challenge of selecting the most appropriate solution from multiple samples) but does not explore it in depth.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, a major challenge for this task is to select the most appropriate solution from the multiple samples generated by the pre-trained language models.\"\n\nThis paper is rated with 1 because it mentions a limitation of pre-trained language models (the challenge of selecting the most appropriate solution from multiple samples) but does not explore it in depth."
    },
    {
        "title": "Discovering Policies with DOMiNO: Diversity Optimization Maintaining Near Optimality",
        "authors": [
            "Tom Zahavy",
            "Yannick Schroecker",
            "Feryal Behbahani",
            "Kate Baumli",
            "Sebastian Flennerhag",
            "Shaobo Hou",
            "Satinder Singh"
        ],
        "published": "iclr 2023 poster",
        "summary": "In this work we propose a Reinforcement Learning (RL) agent that can discover complex behaviours in a rich environment with a simple reward function. We define diversity in terms of state-action occupancy measures, since policies with different occupancy measures visit different states on average. More importantly, defining diversity in this way allows us to derive an intrinsic reward function for maximizing the diversity directly. Our agent, DOMiNO, stands for Diversity Optimization Maintaining Near Optimally. It is based on maximizing a reward function with two components: the extrinsic reward and the diversity intrinsic reward, which are combined with Lagrange multipliers to balance the quality-diversity trade-off. Any RL algorithm can be used to maximize this reward and no other changes are needed. We demonstrate that given a simple reward functions in various control domains, like height (stand) and forward velocity (walk), DOMiNO discovers diverse and meaningful behaviours. We also perform extensive analysis of our approach, compare it with other multi-objective baselines, demonstrate that we can control both the quality and the diversity of the set via interpretable hyperparameters, and show that the set is robust to perturbations of the environment.",
        "pdf_link": "https://openreview.net/pdf/6afcd9948e9a8f36dcac913dfe67d5311fc6c5df.pdf",
        "forum_url": "https://openreview.net/forum?id=kjkdzBW3b8p",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Self-Distillation for Further Pre-training of Transformers",
        "authors": [
            "Seanie Lee",
            "Minki Kang",
            "Juho Lee",
            "Sung Ju Hwang",
            "Kenji Kawaguchi"
        ],
        "published": "iclr 2023 poster",
        "summary": "Pre-training a large transformer model on a massive amount of unlabeled data and fine-tuning it on labeled datasets for diverse downstream tasks has proven to be a successful strategy, for a variety of vision and natural language processing tasks. However, direct fine-tuning of the pre-trained model may be suboptimal if there exist large discrepancies across data domains for pre-training and fine-tuning. To tackle this issue, several previous studies have proposed further pre-training strategies, where we continue to pre-train the model on the target unlabeled dataset before fine-tuning. However, all of them solely focus on language models and we empirically find that a Vision Transformer is vulnerable to overfitting as we continue to pretrain the model on target unlabeled data. In order to tackle this limitation, we propose self-distillation as a regularization for a further pre-training stage. Specifically, we first further pre-train the initial pre-trained model on the target unlabeled data and then consider it as a teacher for self-distillation. Then we take the same initial pre-trained model as a student and enforce its hidden representations to be close to those of the teacher while optimizing the student with a masked auto-encoding objective. We empirically validate the efficacy of self-distillation on a variety of benchmark datasets for image and text classification tasks. Experimentally, we show that our proposed method outperforms all the relevant baselines. Theoretically, we analyze the proposed method with a simplified model to understand how self-distillation for further pre-training can potentially help improve the performance of the downstream tasks.",
        "pdf_link": "https://openreview.net/pdf/008646e566fce92cb8bb6248dcc7c7508818680e.pdf",
        "forum_url": "https://openreview.net/forum?id=kj6oK_Hj40",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, direct fine-tuning of the pre-trained model may be suboptimal if there exist large discrepancies across data domains for pre-training and fine-tuning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, direct fine-tuning of the pre-trained model may be suboptimal if there exist large discrepancies across data domains for pre-training and fine-tuning.\""
    },
    {
        "title": "UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining",
        "authors": [
            "Hyung Won Chung",
            "Xavier Garcia",
            "Adam Roberts",
            "Yi Tay",
            "Orhan Firat",
            "Sharan Narang",
            "Noah Constant"
        ],
        "published": "iclr 2023 poster",
        "summary": "Pretrained multilingual large language models have typically used heuristic temperature-based sampling to balance between different languages. However previous work has not systematically evaluated the efficacy of different pretraining language distributions across model scales. In this paper, we propose a new sampling method, UniMax, that delivers more uniform coverage of head languages while mitigating overfitting on tail languages by explicitly capping the number of repeats over each language's corpus. We perform an extensive series of ablations testing a range of sampling strategies on a suite of multilingual benchmarks, while varying model scale. We find that UniMax outperforms standard temperature-based sampling, and the benefits persist as scale increases. As part of our contribution, we release: (i) an improved and refreshed mC4 multilingual corpus consisting of 29 trillion characters across 107 languages, and (ii) a suite of pretrained umT5 model checkpoints trained with UniMax sampling.",
        "pdf_link": "https://openreview.net/pdf/f9db93a34d7f56ce156cb253feba6c638acc2b21.pdf",
        "forum_url": "https://openreview.net/forum?id=kXwdL1cWOAi",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However previous work has not systematically evaluated the efficacy of different pretraining language distributions across model scales.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However previous work has not systematically evaluated the efficacy of different pretraining language distributions across model scales.\""
    },
    {
        "title": "Systematic Rectification of Language Models via Dead-end Analysis",
        "authors": [
            "Meng Cao",
            "Mehdi Fatemi",
            "Jackie CK Cheung",
            "Samira Shabanian"
        ],
        "published": "iclr 2023 poster",
        "summary": "With adversarial or otherwise normal prompts, existing large language models (LLM) can be pushed to generate toxic discourses. One way to reduce the risk of LLMs generating undesired discourses is to alter the training of the LLM. This can be very restrictive due to demanding computation requirements. Other methods rely on rule-based or prompt-based token elimination, which are limited as they dismiss future tokens and the overall meaning of the complete discourse. Here, we center detoxification on the probability that the finished discourse is ultimately considered toxic. That is, at each point, we advise against token selections proportional to how likely a finished text from this point will be toxic. To this end, we formally extend the dead-end theory from the recent reinforcement learning (RL) literature to also cover uncertain outcomes. Our approach, called rectification, utilizes a separate but significantly smaller model for detoxification, which can be applied to diverse LLMs as long as they share the same vocabulary. Importantly, our method does not require access to the internal representations of the LLM, but only the token probability distribution at each decoding step. We believe this is important since many LLMs today are hosted in servers and only accessible through APIs. When applied to various LLMs, including GPT-3, our approach generates notably better results compared to the base LLMs and other techniques in terms of the overall language and detoxification performance.",
        "pdf_link": "https://openreview.net/pdf/04f652450ee4706ff6565dc9df8580e1881050b2.pdf",
        "forum_url": "https://openreview.net/forum?id=k8_yVW3Wqln",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"With adversarial or otherwise normal prompts, existing large language models (LLM) can be pushed to generate toxic discourses.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"With adversarial or otherwise normal prompts, existing large language models (LLM) can be pushed to generate toxic discourses.\""
    },
    {
        "title": "Improved Sample Complexity for Reward-free Reinforcement Learning under Low-rank MDPs",
        "authors": [
            "Yuan Cheng",
            "Ruiquan Huang",
            "Yingbin Liang",
            "Jing Yang"
        ],
        "published": "iclr 2023 poster",
        "summary": "In reward-free reinforcement learning (RL), an agent explores the environment first without any reward information, in order to achieve certain learning goals afterwards for any given reward. In this paper we focus on reward-free RL under low-rank MDP models, in which both the representation and linear weight vectors are unknown. Although various algorithms have been proposed for reward-free low-rank MDPs, the corresponding sample complexity is still far from being satisfactory. In this work, we first provide the first known sample complexity lower bound that holds for any algorithm under low-rank MDPs. This lower bound implies it is strictly harder to find a near-optimal policy under low-rank MDPs than under linear MDPs. We then propose a novel model-based algorithm, coined RAFFLE, and show it can both find an $\\epsilon$-optimal policy and achieve an $\\epsilon$-accurate system identification via reward-free exploration, with a sample complexity significantly improving the previous results. Such a sample complexity matches our lower bound in the dependence on $\\epsilon$, as well as on $K$ {in the large $d$ regime}, where $d$ and $K$ respectively denote the representation dimension and action space cardinality. Finally, we provide a planning algorithm (without further interaction with true environment) for RAFFLE to learn a near-accurate representation, which is the first known representation learning guarantee under the same setting.\n ",
        "pdf_link": "https://openreview.net/pdf/7e35ca55c657bd4a6cd5f8b5855b554ea5a1b927.pdf",
        "forum_url": "https://openreview.net/forum?id=jpsw-KuOi7r",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models",
        "authors": [
            "Shansan Gong",
            "Mukai Li",
            "Jiangtao Feng",
            "Zhiyong Wu",
            "Lingpeng Kong"
        ],
        "published": "iclr 2023 poster",
        "summary": "Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DiffuSeq: a diffusion model designed for sequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation over a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or even better performance than six established baselines, including a state-of-the-art model that is based on pre-trained language models. Apart from quality, an intriguing property of DiffuSeq is its high diversity during generation, which is desired in many Seq2Seq tasks. We further include a theoretical analysis revealing the connection between DiffuSeq and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks. Code is available at https://github.com/Shark-NLP/DiffuSeq",
        "pdf_link": "https://openreview.net/pdf/60eecf7c181638fdfa60c671ca5d0b67644748cd.pdf",
        "forum_url": "https://openreview.net/forum?id=jQj-_rLVXsj",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Memory Gym: Partially Observable Challenges to Memory-Based Agents",
        "authors": [
            "Marco Pleines",
            "Matthias Pallasch",
            "Frank Zimmer",
            "Mike Preuss"
        ],
        "published": "iclr 2023 poster",
        "summary": "Memory Gym is a novel benchmark for challenging Deep Reinforcement Learning agents to memorize events across long sequences, be robust to noise, and generalize. It consists of the partially observable 2D and discrete control environments Mortar Mayhem, Mystery Path, and Searing Spotlights. These environments are believed to be unsolvable by memory-less agents because they feature strong dependencies on memory and frequent agent-memory interactions. Empirical results based on Proximal Policy Optimization (PPO) and Gated Recurrent Unit (GRU) underline the strong memory dependency of the contributed environments. The hardness of these environments can be smoothly scaled, while different levels of difficulty (some of them unsolved yet) emerge for Mortar Mayhem and Mystery Path. Surprisingly, Searing Spotlights poses a tremendous challenge to GRU-PPO, which remains an open puzzle. Even though the\nrandomly moving spotlights reveal parts of the environment’s ground truth, environmental ablations hint that these pose a severe perturbation to agents that leverage recurrent model architectures as their memory. \nSource Code: https://github.com/MarcoMeter/drl-memory-gym/",
        "pdf_link": "https://openreview.net/pdf/311f37d9f91d2b654e7ef5b66aab43a60b5f0e8b.pdf",
        "forum_url": "https://openreview.net/forum?id=jHc8dCx6DDr",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Causal Representation Learning for Instantaneous and Temporal Effects in Interactive Systems",
        "authors": [
            "Phillip Lippe",
            "Sara Magliacane",
            "Sindy Löwe",
            "Yuki M Asano",
            "Taco Cohen",
            "Efstratios Gavves"
        ],
        "published": "iclr 2023 poster",
        "summary": "Causal representation learning is the task of identifying the underlying causal variables and their relations from high-dimensional observations, such as images. Recent work has shown that one can reconstruct the causal variables from temporal sequences of observations under the assumption that there are no instantaneous causal relations between them. In practical applications, however, our measurement or frame rate might be slower than many of the causal effects. This effectively creates ``instantaneous'' effects and invalidates previous identifiability results. To address this issue, we propose iCITRIS, a causal representation learning method that allows for instantaneous effects in intervened temporal sequences when intervention targets can be observed, e.g., as actions of an agent. iCITRIS identifies the potentially multidimensional causal variables from temporal observations, while simultaneously using a differentiable causal discovery method to learn their causal graph. In experiments on three datasets of interactive systems, iCITRIS accurately identifies the causal variables and their causal graph.",
        "pdf_link": "https://openreview.net/pdf/4fc22cb9d6d8f26d83b6ff14379f6e08f167c4c2.pdf",
        "forum_url": "https://openreview.net/forum?id=itZ6ggvMnzS",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Can Agents Run Relay Race with Strangers? Generalization of RL to Out-of-Distribution Trajectories",
        "authors": [
            "Li-Cheng Lan",
            "Huan Zhang",
            "Cho-Jui Hsieh"
        ],
        "published": "iclr 2023 poster",
        "summary": "In this paper, we evaluate and improve the generalization performance for reinforcement learning (RL) agents on the set of ``controllable'' states, where good policies exist on these states to achieve the goal. An RL agent that generally masters a task should reach its goal starting from any controllable state of the environment instead of memorizing a small set of trajectories. To practically evaluate this type of generalization, we propose relay evaluation, which starts the test agent from the middle of other independently well-trained stranger agents' trajectories. With extensive experimental evaluation, we show the prevalence of generalization failure on controllable states from stranger agents. For example, in the Humanoid environment, we observed that a well-trained Proximal Policy Optimization (PPO) agent, with only 3.9\\% failure rate during regular testing, failed on 81.6\\% of the states generated by well-trained stranger PPO agents. To improve \"relay generalization,\" we propose a novel method called Self-Trajectory Augmentation (STA), which will reset the environment to the agent's old states according to the Q function during training. After applying STA to the Soft Actor Critic's (SAC) training procedure, we reduced the failure rate of SAC under relay-evaluation by more than three times in most settings without impacting agent performance and increasing the needed number of environment interactions. Our code is available at https://github.com/lan-lc/STA.",
        "pdf_link": "https://openreview.net/pdf/80245884d3c21d7b21166281784b35962b9f3e1f.pdf",
        "forum_url": "https://openreview.net/forum?id=ipflrGaf7ry",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "GAMR: A Guided Attention Model for (visual) Reasoning",
        "authors": [
            "Mohit Vaishnav",
            "Thomas Serre"
        ],
        "published": "iclr 2023 poster",
        "summary": "Humans continue to outperform modern AI systems in their ability to flexibly parse and understand complex visual scenes. Here, we present a novel module for visual reasoning, the Guided Attention Model for (visual) Reasoning ($\\textit{GAMR}$), which instantiates an active vision theory -- positing that the brain solves complex visual reasoning problems dynamically -- via sequences of attention shifts to select and route task-relevant visual information into memory. Experiments on an array of visual reasoning tasks and datasets demonstrate GAMR's ability to learn visual routines in a robust and sample-efficient manner. In addition, GAMR is shown to be capable of zero-shot generalization on completely novel reasoning tasks. Overall, our work provides computational support for cognitive theories that postulate the need for a critical interplay between attention and memory to dynamically maintain and manipulate task-relevant visual information to solve complex visual reasoning tasks.",
        "pdf_link": "https://openreview.net/pdf/66fcdeb1236a4c6085e1902756d2129bbdca7f0e.pdf",
        "forum_url": "https://openreview.net/forum?id=iLMgk2IGNyv",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code",
        "authors": [
            "Nadezhda Chirkova",
            "Sergey Troshin"
        ],
        "published": "iclr 2023 poster",
        "summary": "Recent works have widely adopted large language model pretraining for source code, suggested source code-specific pretraining objectives and investigated the applicability of various Transformer-based language model architectures for source code. This work investigates another important aspect of such models, the effect of different subtokenization options, and aims at identifying most effective and length-efficient subtokenizations, taking into account source code specifics. We propose subtokenziation that reduces average length by 17--40% without downstream performance drop, and show that a carefully chosen subtokenization may improve  quality by 0.5-2%, possibly with some length increase.",
        "pdf_link": "https://openreview.net/pdf/c3138a16b1e95192c50eacb849b3a42ecf8a6999.pdf",
        "forum_url": "https://openreview.net/forum?id=htL4UZ344nF",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper implies that subtokenization options might have limitations by investigating the effect of different options and aiming to identify the most effective ones.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None, but the paper implies that subtokenization options might have limitations by investigating the effect of different options and aiming to identify the most effective ones."
    },
    {
        "title": "Behavior Prior Representation learning for Offline Reinforcement Learning",
        "authors": [
            "Hongyu Zang",
            "Xin Li",
            "Jie Yu",
            "Chen Liu",
            "Riashat Islam",
            "Remi Tachet des Combes",
            "Romain Laroche"
        ],
        "published": "iclr 2023 poster",
        "summary": "Offline reinforcement learning (RL) struggles in environments with rich and noisy inputs, where the agent only has access to a fixed dataset without environment interactions. Past works have proposed common workarounds based on the pre-training of state representations, followed by policy training. In this work, we introduce a simple, yet effective approach for learning state representations. Our method, Behavior Prior Representation (BPR), learns state representations with an easy-to-integrate objective based on behavior cloning of the dataset: we first learn a state representation by mimicking actions from the dataset, and then train a policy on top of the fixed representation, using any off-the-shelf Offline RL algorithm. Theoretically, we prove that BPR carries out performance guarantees when integrated into algorithms that have either policy improvement guarantees (conservative algorithms) or produce lower bounds of the policy values (pessimistic algorithms). Empirically, we show that BPR combined with existing state-of-the-art Offline RL algorithms leads to significant improvements across several offline control benchmarks. The code is available at \\url{https://github.com/bit1029public/offline_bpr}",
        "pdf_link": "https://openreview.net/pdf/96eb2ddb9c4aa37f98b044bc2a20d050badf1645.pdf",
        "forum_url": "https://openreview.net/forum?id=hQ4K9Bf4G2B",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Generating Sequences by Learning to Self-Correct",
        "authors": [
            "Sean Welleck",
            "Ximing Lu",
            "Peter West",
            "Faeze Brahman",
            "Tianxiao Shen",
            "Daniel Khashabi",
            "Yejin Choi"
        ],
        "published": "iclr 2023 poster",
        "summary": "Sequence generation applications require satisfying semantic constraints, such as ensuring that programs are correct, using certain keywords, or avoiding undesirable content. Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs. Moreover, some powerful language models are of extreme scale or inaccessible, making it inefficient, if not infeasible, to update their parameters for task-specific adaptation. We present Self-Correction, an approach that decouples an imperfect base generator (an off-the-shelf language model or supervised sequence-to-sequence model) from a separate corrector that learns to iteratively correct imperfect generations. To train the corrector, we propose an online training procedure that can use either scalar or natural language feedback on intermediate imperfect generations. We show that  Self-Correction improves upon the base generator in three diverse generation tasks - mathematical program synthesis, lexically-constrained generation, and toxicity control - even when the corrector is much smaller than the base generator. \n",
        "pdf_link": "https://openreview.net/pdf/492050ca3a1a68a786565a323ad790877b8af5b3.pdf",
        "forum_url": "https://openreview.net/forum?id=hH36JeQZDaO",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs. Moreover, some powerful language models are of extreme scale or inaccessible, making it inefficient, if not infeasible, to update their parameters for task-specific adaptation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs. Moreover, some powerful language models are of extreme scale or inaccessible, making it inefficient, if not infeasible, to update their parameters for task-specific adaptation.\""
    },
    {
        "title": "Composing Ensembles of Pre-trained Models via Iterative Consensus",
        "authors": [
            "Shuang Li",
            "Yilun Du",
            "Joshua B. Tenenbaum",
            "Antonio Torralba",
            "Igor Mordatch"
        ],
        "published": "iclr 2023 poster",
        "summary": "Large pre-trained models exhibit distinct and complementary capabilities dependent on the data they are trained on. Language models such as GPT-3 are capable of textual reasoning but cannot understand visual information, while vision models such as DALL-E can generate photorealistic photos but fail to understand complex language descriptions. In this work, we propose a unified framework for composing ensembles of different pre-trained models -- combining the strengths of each individual model to solve various multimodal problems in a zero-shot manner. We use pre-trained models as \"generators\" or \"scorers\" and compose them via closed-loop iterative consensus optimization. The generator constructs proposals and the scorers iteratively provide feedback to refine the generated result. Such closed-loop communication enables models to correct errors caused by other models, significantly boosting performance on downstream tasks, e.g. improving accuracy on grade school math problems by 7.5%, without requiring any model finetuning. We demonstrate that consensus achieved by an ensemble of scorers outperforms the feedback of a single scorer, by leveraging the strengths of each expert model. Results show that the proposed method can be used as a general purpose framework for a wide range of zero-shot multimodal tasks, such as image generation, video question answering, mathematical reasoning, and robotic manipulation. \n",
        "pdf_link": "https://openreview.net/pdf/3a0f22bd7748e3a5e79b5973a182c5777b2aab02.pdf",
        "forum_url": "https://openreview.net/forum?id=gmwDKo-4cY",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Language models such as GPT-3 are capable of textual reasoning but cannot understand visual information...\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Language models such as GPT-3 are capable of textual reasoning but cannot understand visual information...\""
    },
    {
        "title": "Promptagator: Few-shot Dense Retrieval From 8 Examples",
        "authors": [
            "Zhuyun Dai",
            "Vincent Y Zhao",
            "Ji Ma",
            "Yi Luan",
            "Jianmo Ni",
            "Jing Lu",
            "Anton Bakalov",
            "Kelvin Guu",
            "Keith Hall",
            "Ming-Wei Chang"
        ],
        "published": "iclr 2023 poster",
        "summary": "Much recent research on information retrieval has focused on how to transfer from one task (typically with abundant supervised data) to various other retrieval tasks where supervision is limited, with the implicit assumption that it is possible to generalize from one task to all the rest. However, this overlooks the fact that there are many diverse and unique retrieval problems, each targeting different search intents, queries, and search domains. In this paper, we suggest to work on Few-shot Dense Retrieval, a setting where each task comes with a short description and a few examples. To address this, we introduce Prompt-based Query Generation forRetrieval (Promptagator): for each task, we feed the few-shot examples to a large language model (LLM) and prompt it to behave as a task-specific query generator. Using this, we can synthetically generate a large number of relevant queries for any document, yielding abundant data for training task-specific retrievers --- with no reliance on traditional resources such as Natural Questions (Kwiatkowskiet al., 2019) or MS MARCO (Nguyen et al., 2016). Surprisingly, Promptagator with only 8 annotated examples enables efficient dual encoder retrievers to outperform computationally more expensive models trained on MS MARCO such as ColBERT v2 (Santhanam et al., 2022) by more than 1.2 points nDCG@10 on average on 11 retrieval sets. Further training standard-size re-rankers using the same generated data yields another 5.0 points nDCG@10 improvement. Our studies show that synthetic query generation can be far more effective than previously observed, especially when a small amount of task-specific knowledge is given.",
        "pdf_link": "https://openreview.net/pdf/79a0f9b78ef87a8465c2f60eac8f96b996c84b38.pdf",
        "forum_url": "https://openreview.net/forum?id=gmL46YMpu2J",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation of LLMs is mentioned, but the paper uses LLMs in a novel way to address the challenge of few-shot learning in information retrieval.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation of LLMs is mentioned, but the paper uses LLMs in a novel way to address the challenge of few-shot learning in information retrieval."
    },
    {
        "title": "Compositional Semantic Parsing with Large Language Models",
        "authors": [
            "Andrew Drozdov",
            "Nathanael Schärli",
            "Ekin Akyürek",
            "Nathan Scales",
            "Xinying Song",
            "Xinyun Chen",
            "Olivier Bousquet",
            "Denny Zhou"
        ],
        "published": "iclr 2023 poster",
        "summary": "Humans can reason compositionally when presented with new tasks.  Previous research shows that appropriate prompting techniques enable large language models (LLMs) to solve artificial compositional generalization tasks such as SCAN. In this work, we identify additional challenges in more realistic semantic parsing tasks with larger vocabulary and refine these prompting techniques to address them. Our best method is based on least-to-most prompting: it decomposes the problem using prompting-based syntactic parsing, then uses this decomposition to select appropriate exemplars and to sequentially generate the semantic parse. This method allows us to set a new state of the art for CFQ while requiring only 1% of the training data used by traditional approaches. Due to the general nature of our approach, we expect similar efforts will lead to new results in other tasks and domains, especially for knowledge-intensive applications.",
        "pdf_link": "https://openreview.net/pdf/668ef1e66f349e87c8948f0e5e5984608ebef31d.pdf",
        "forum_url": "https://openreview.net/forum?id=gJW8hSGBys8",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Previous research shows that appropriate prompting techniques enable large language models (LLMs) to solve artificial compositional generalization tasks such as SCAN.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Previous research shows that appropriate prompting techniques enable large language models (LLMs) to solve artificial compositional generalization tasks such as SCAN.\""
    },
    {
        "title": "$k$NN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference",
        "authors": [
            "Benfeng Xu",
            "Quan Wang",
            "Zhendong Mao",
            "Yajuan Lyu",
            "Qiaoqiao She",
            "Yongdong Zhang"
        ],
        "published": "iclr 2023 poster",
        "summary": "In-Context Learning (ICL), which formulates target tasks as prompt completion conditioned on in-context demonstrations, has become the prevailing utilization of LLMs. In this paper, we first disclose an actual predicament for this typical usage that it can not scale up with training data due to context length restriction. Besides, existing works have shown that ICL also suffers from various biases and requires delicate calibration treatment. To address both challenges, we advocate a simple and effective solution, $k$NN Prompting, which first queries LLM with training data for distributed representations, then predicts test instances by simply referring to nearest neighbors. We conduct comprehensive experiments to demonstrate its two-fold superiority: 1) Calibration-Free: $k$NN Prompting does not directly align LLM output distribution with task-specific label space, instead leverages such distribution to align test and training instances. It significantly outperforms state-of-the-art calibration-based methods under comparable few-shot scenario. 2) Beyond-Context: $k$NN Prompting can further scale up effectively with as many training data as are available, continually bringing substantial improvements. The scaling trend holds across 10 orders of magnitude ranging from 2 shots to 1024 shots as well as different LLMs scales ranging from 0.8B to 30B. It successfully bridges data scaling into model scaling, and brings new potentials for the gradient-free paradigm of LLM deployment. Code is publicly available at https://github.com/BenfengXu/KNNPrompting",
        "pdf_link": "https://openreview.net/pdf/feaebae1ecc4dd15ee54a25f37db2412f7e62789.pdf",
        "forum_url": "https://openreview.net/forum?id=fe2S7736sNS",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we first disclose an actual predicament for this typical usage that it can not scale up with training data due to context length restriction.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"we first disclose an actual predicament for this typical usage that it can not scale up with training data due to context length restriction.\""
    },
    {
        "title": "Language models are multilingual chain-of-thought reasoners",
        "authors": [
            "Freda Shi",
            "Mirac Suzgun",
            "Markus Freitag",
            "Xuezhi Wang",
            "Suraj Srivats",
            "Soroush Vosoughi",
            "Hyung Won Chung",
            "Yi Tay",
            "Sebastian Ruder",
            "Denny Zhou",
            "Dipanjan Das",
            "Jason Wei"
        ],
        "published": "iclr 2023 poster",
        "summary": "We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at AnonymousLink and the supplementary material.\n",
        "pdf_link": "https://openreview.net/pdf/972d6eaf77336eece16b7ec5bdb9565b06423b8a.pdf",
        "forum_url": "https://openreview.net/forum?id=fR3wGCk-IXp",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract does not mention any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract does not mention any limitations of LLMs."
    },
    {
        "title": "Hierarchical Abstraction for Combinatorial Generalization in Object Rearrangement",
        "authors": [
            "Michael Chang",
            "Alyssa Li Dayan",
            "Franziska Meier",
            "Thomas L. Griffiths",
            "Sergey Levine",
            "Amy Zhang"
        ],
        "published": "iclr 2023 poster",
        "summary": "Object rearrangement is a challenge for embodied agents because solving these tasks requires generalizing across a combinatorially large set of configurations of entities and their locations. Worse, the representations of these entities are unknown and must be inferred from sensory percepts. We present a hierarchical abstraction approach to uncover these underlying entities and achieve combinatorial generalization from unstructured visual inputs. By constructing a factorized transition graph over clusters of entity representations inferred from pixels, we show how to learn a correspondence between intervening on states of entities in the agent's model and acting on objects in the environment. We use this correspondence to develop a method for control that generalizes to different numbers and configurations of objects, which outperforms current offline deep RL methods when evaluated on simulated rearrangement tasks.",
        "pdf_link": "https://openreview.net/pdf/3fe199764971a79f6ca90f8e713263343dea0eec.pdf",
        "forum_url": "https://openreview.net/forum?id=fGG6vHp3W9W",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Population-size-Aware Policy Optimization for Mean-Field Games",
        "authors": [
            "Pengdeng Li",
            "Xinrun Wang",
            "Shuxin Li",
            "Hau Chan",
            "Bo An"
        ],
        "published": "iclr 2023 poster",
        "summary": "In this work, we attempt to bridge the two fields of finite-agent and infinite-agent games, by studying how the optimal policies of agents evolve with the number of agents (population size) in mean-field games, an agent-centric perspective in contrast to the existing works focusing typically on the convergence of the empirical distribution of the population. To this end, the premise is to obtain the optimal policies of a set of finite-agent games with different population sizes. However, either deriving the closed-form solution for each game is theoretically intractable, training a distinct policy for each game is computationally intensive, or directly applying the policy trained in a game to other games is sub-optimal. We address these challenges through the \\textbf{P}opulation-size-\\textbf{A}ware \\textbf{P}olicy \\textbf{O}ptimization (PAPO). Our contributions are three-fold. First, to efficiently generate efficient policies for games with different population sizes, we propose PAPO, which unifies two natural options (augmentation and hypernetwork) and achieves significantly better performance. PAPO consists of three components: i) the population-size encoding which transforms the original value of population size to an equivalent encoding to avoid training collapse, ii) a hypernetwork to generate a distinct policy for each game conditioned on the population size, and iii) the population size as an additional input to the generated policy. Next, we construct a multi-task-based training procedure to efficiently train the neural networks of PAPO by sampling data from multiple games with different population sizes. Finally, extensive experiments on multiple environments show the significant superiority of PAPO over baselines, and the analysis of the evolution of the generated policies further deepens our understanding of the two fields of finite-agent and infinite-agent games. ",
        "pdf_link": "https://openreview.net/pdf/453e068e0d07905abfd73e6b1e73851d095de2e6.pdf",
        "forum_url": "https://openreview.net/forum?id=fB4V-2QvCEm",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Generate rather than Retrieve: Large Language Models are Strong Context Generators",
        "authors": [
            "Wenhao Yu",
            "Dan Iter",
            "Shuohang Wang",
            "Yichong Xu",
            "Mingxuan Ju",
            "Soumya Sanyal",
            "Chenguang Zhu",
            "Michael Zeng",
            "Meng Jiang"
        ],
        "published": "iclr 2023 poster",
        "summary": "Knowledge-intensive tasks, such as open-domain question answering (QA), require access to a large amount of world or domain knowledge. A common approach for knowledge-intensive tasks is to employ a retrieve-then-read pipeline that first retrieves a handful of relevant contextual documents from an external corpus such as Wikipedia and then predicts an answer conditioned on the retrieved documents. In this paper, we present a novel perspective for solving knowledge-intensive tasks by replacing document retrievers with large language model generators. We call our method generate-then-read (GenRead), which first prompts a large language model to generate contextual documents based on a given question, and then reads the generated documents to produce the final answer. Furthermore, we propose a novel clustering-based prompting method that selects distinct prompts, in order to generate diverse documents that cover different perspectives, leading to better recall over acceptable answers. We conduct extensive experiments on three different knowledge-intensive tasks, including open-domain QA, fact checking, and dialogue system. Notably, GenRead achieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly outperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0 and +3.9, without retrieving any documents from any external knowledge source. Lastly, we demonstrate the model performance can be further improved by combining retrieval and generation. Our code and generated documents can be found at https://github.com/wyu97/GenRead.",
        "pdf_link": "https://openreview.net/pdf/a4086708f91faaf89fdf42039c5895dfd8a6a372.pdf",
        "forum_url": "https://openreview.net/forum?id=fB0hRu9GZUS",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, the paper presents a novel perspective on using LLMs for knowledge-intensive tasks.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, the paper presents a novel perspective on using LLMs for knowledge-intensive tasks."
    },
    {
        "title": "Learning Locality and Isotropy in Dialogue Modeling",
        "authors": [
            "Han Wu",
            "Haochen Tan",
            "Mingjie Zhan",
            "Gangming Zhao",
            "Shaoqing Lu",
            "Ding Liang",
            "Linqi Song"
        ],
        "published": "iclr 2023 poster",
        "summary": "Existing dialogue modeling methods have achieved promising performance on various dialogue tasks with the aid of Transformer and the large-scale pre-trained language models. However, some recent studies revealed that the context representations produced by these methods suffer the problem of anisotropy. In this paper, we find that the generated representations are also not conversational, losing the conversation structure information during the context modeling stage. To this end, we identify two properties in dialogue modeling, i.e., locality and isotropy, and present a simple method for dialogue representation calibration, namely SimDRC, to build isotropic and conversational feature spaces. Experimental results show that our approach significantly outperforms current state-of-the-art models on three open-domain dialogue tasks with eight benchmarks. More in-depth analyses further confirm the effectiveness of our proposed approach. We release the code at https://github.com/hahahawu/SimDRC.",
        "pdf_link": "https://openreview.net/pdf/1bf27b66e1ac5124b7cf6726e833f2902ebec00d.pdf",
        "forum_url": "https://openreview.net/forum?id=dPs6BGO2QT0",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, some recent studies revealed that the context representations produced by these methods suffer the problem of anisotropy. In this paper, we find that the generated representations are also not conversational, losing the conversation structure information during the context modeling stage.\"\n\nThis rating is based on the fact that the abstract mentions specific limitations of the context representations produced by existing dialogue modeling methods, which use",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, some recent studies revealed that the context representations produced by these methods suffer the problem of anisotropy. In this paper, we find that the generated representations are also not conversational, losing the conversation structure information during the context modeling stage.\"\n\nThis rating is based on the fact that the abstract mentions specific limitations of the context representations produced by existing dialogue modeling methods, which use"
    },
    {
        "title": "Certifiably Robust Policy Learning against Adversarial Multi-Agent Communication",
        "authors": [
            "Yanchao Sun",
            "Ruijie Zheng",
            "Parisa Hassanzadeh",
            "Yongyuan Liang",
            "Soheil Feizi",
            "Sumitra Ganesh",
            "Furong Huang"
        ],
        "published": "iclr 2023 poster",
        "summary": "Communication is important in many multi-agent reinforcement learning (MARL) problems for agents to share information and make good decisions. However, when deploying trained communicative agents in a real-world application where noise and potential attackers exist, the safety of communication-based policies becomes a severe issue that is underexplored. Specifically, if communication messages are manipulated by malicious attackers, agents relying on untrustworthy communication may take unsafe actions that lead to catastrophic consequences. Therefore, it is crucial to ensure that agents will not be misled by corrupted communication, while still benefiting from benign communication. In this work, we consider an environment with $N$ agents, where the attacker may arbitrarily change the communication from any $C<\\frac{N-1}{2}$ agents to a victim agent. For this strong threat model, we propose a certifiable defense by constructing a message-ensemble policy that aggregates multiple randomly ablated message sets. Theoretical analysis shows that this message-ensemble policy can utilize benign communication while being certifiably robust to adversarial communication, regardless of the attacking algorithm. Experiments in multiple environments verify that our defense significantly improves the robustness of trained policies against various types of attacks.",
        "pdf_link": "https://openreview.net/pdf/f1c6ea43513dada0ace7e97e3a9c8f26b83250a8.pdf",
        "forum_url": "https://openreview.net/forum?id=dCOL0inGl3e",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "POPGym: Benchmarking Partially Observable Reinforcement Learning",
        "authors": [
            "Steven Morad",
            "Ryan Kortvelesy",
            "Matteo Bettini",
            "Stephan Liwicki",
            "Amanda Prorok"
        ],
        "published": "iclr 2023 poster",
        "summary": "Real world applications of Reinforcement Learning (RL) are often partially observable, thus requiring memory. Despite this, partial observability is still largely ignored by contemporary RL benchmarks and libraries. We introduce Partially Observable Process Gym (POPGym), a two-part library containing (1) a diverse collection of 15 partially observable environments, each with multiple difficulties and (2) implementations of 13 memory model baselines -- the most in a single RL library. Existing partially observable benchmarks tend to fixate on 3D visual navigation, which is computationally expensive and only one type of POMDP. In contrast, POPGym environments are diverse, produce smaller observations, use less memory, and often converge within two hours of training on a consumer-grade GPU. We implement our high-level memory API and memory baselines on top of the popular RLlib framework, providing plug-and-play compatibility with various training algorithms, exploration strategies, and distributed training paradigms. Using POPGym, we execute the largest comparison across RL memory models to date. POPGym is available at https://github.com/proroklab/popgym.",
        "pdf_link": "https://openreview.net/pdf/9dfc2ae9e672bdd44ff589054c88b18924af8483.pdf",
        "forum_url": "https://openreview.net/forum?id=chDrutUTs0K",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Cheap Talk Discovery and Utilization in Multi-Agent Reinforcement Learning",
        "authors": [
            "Yat Long Lo",
            "Christian Schroeder de Witt",
            "Samuel Sokota",
            "Jakob Nicolaus Foerster",
            "Shimon Whiteson"
        ],
        "published": "iclr 2023 poster",
        "summary": "By enabling agents to communicate, recent cooperative multi-agent reinforcement learning (MARL) methods have demonstrated better task performance and more coordinated behavior. Most existing approaches facilitate inter-agent communication by allowing agents to send messages to each other through free communication channels, i.e., \\emph{cheap talk channels}. Current methods require these channels to be constantly accessible and known to the agents a priori. In this work, we lift these requirements such that the agents must discover the cheap talk channels and learn how to use them. Hence, the problem has two main parts: \\emph{cheap talk discovery} (CTD) and \\emph{cheap talk utilization} (CTU). We introduce a novel conceptual framework for both parts and develop a new algorithm based on mutual information maximization that outperforms existing algorithms in CTD/CTU settings. We also release a novel benchmark suite to stimulate future research in CTD/CTU.",
        "pdf_link": "https://openreview.net/pdf/efb6725925bb04b66d9a794a929e5ed57ea8ef69.pdf",
        "forum_url": "https://openreview.net/forum?id=cddbeL1HWaD",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Planning with Sequence Models through Iterative Energy Minimization",
        "authors": [
            "Hongyi Chen",
            "Yilun Du",
            "Yiye Chen",
            "Joshua B. Tenenbaum",
            "Patricio A. Vela"
        ],
        "published": "iclr 2023 poster",
        "summary": "Recent works have shown that language modeling can be effectively used to train reinforcement learning (RL) policies. However, the success of applying existing language models to planning, in which we wish to obtain a trajectory of actions to reach some goal, is less straightforward. The typical autoregressive generation procedures of language models preclude sequential refinement of earlier steps, which limits the effectiveness of a predicted plan. In this paper, we suggest an approach towards integrating planning with language models based on the idea of iterative energy minimization, and illustrate how such a procedure leads to improved RL performance across different tasks. We train a masked language model to capture an implicit energy function over trajectories of actions, and formulate planning as finding a trajectory of actions with minimum energy. We illustrate how this procedure enables improved performance over recent approaches across BabyAI and Atari environments. We further demonstrate unique benefits of our iterative optimization procedure, involving new task generalization, test-time constraints adaptation, and the ability to compose plans together. Project webpage: https://hychen-naza.github.io/projects/LEAP/index.html",
        "pdf_link": "https://openreview.net/pdf/b6ee4b3ab28ce8f9c2f94ec81b64cf338bfdfafe.pdf",
        "forum_url": "https://openreview.net/forum?id=cVFD6qE8gnY",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the success of applying existing language models to planning... is less straightforward. The typical autoregressive generation procedures of language models preclude sequential refinement of earlier steps, which limits the effectiveness of a predicted plan.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the success of applying existing language models to planning... is less straightforward. The typical autoregressive generation procedures of language models preclude sequential refinement of earlier steps, which limits the effectiveness of a predicted plan.\""
    },
    {
        "title": "Language Models are Realistic Tabular Data Generators",
        "authors": [
            "Vadim Borisov",
            "Kathrin Sessler",
            "Tobias Leemann",
            "Martin Pawelczyk",
            "Gjergji Kasneci"
        ],
        "published": "iclr 2023 poster",
        "summary": "Tabular data is among the oldest and most ubiquitous forms of data. However, the generation of synthetic samples with the original data’s characteristics remains a significant challenge for tabular data. While many generative models from the computer vision domain, such as variational autoencoders or generative adversarial networks, have been adapted for tabular data generation, less research has been directed towards recent transformer-based large language models (LLMs), which are also generative in nature. To this end, we propose GReaT (Generation of Realistic Tabular data), which exploits an auto-regressive generative LLM to sample synthetic and yet highly realistic tabular data. Furthermore, GReaT can model tabular data distributions by conditioning on any subset of features; the remaining features are sampled without additional overhead. We demonstrate the effectiveness of the proposed approach in a series of experiments that quantify the validity and quality of the produced data samples from multiple angles. We find that GReaT maintains state-of-the-art performance across numerous real-world and synthetic data sets with heterogeneous feature types coming in various sizes.",
        "pdf_link": "https://openreview.net/pdf/93e938176cd4da2511c79883813c6bb7781f9804.pdf",
        "forum_url": "https://openreview.net/forum?id=cEygmQNOeI",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While many generative models from the computer vision domain, such as variational autoencoders or generative adversarial networks, have been adapted for tabular data generation, less research has been directed towards recent transformer-based large language models (LLMs), which are also generative in nature.\"\n\nNote: The paper mentions LLMs and their generative nature but does not discuss any",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"While many generative models from the computer vision domain, such as variational autoencoders or generative adversarial networks, have been adapted for tabular data generation, less research has been directed towards recent transformer-based large language models (LLMs), which are also generative in nature.\"\n\nNote: The paper mentions LLMs and their generative nature but does not discuss any"
    },
    {
        "title": "Decentralized Optimistic Hyperpolicy Mirror Descent: Provably No-Regret Learning in Markov Games",
        "authors": [
            "Wenhao Zhan",
            "Jason D. Lee",
            "Zhuoran Yang"
        ],
        "published": "iclr 2023 poster",
        "summary": "We study decentralized policy learning in Markov games where we control a single agent to play with nonstationary and possibly adversarial opponents. Our goal is to develop a no-regret online learning algorithm that (i) takes actions based on the local information observed by the agent and (ii) is able to find the best policy in hindsight. For such a problem, the nonstationary state transitions due to the varying opponent pose a significant challenge. In light of a recent hardness result (Liu et al., 2022), we focus on the setting where the opponent's previous policies are revealed to the agent for decision making. With such an information structure, we propose a new algorithm, Decentralized Optimistic hypeRpolicy mIrror deScent (DORIS), which achieves $\\sqrt{K}$-regret in the context of general function approximation, where $K$ is the number of episodes. Moreover, when all the agents adopt DORIS, we prove that their mixture policy constitutes an approximate coarse correlated equilibrium. In particular, DORIS maintains a hyperpolicy which is a distribution over the policy space. The hyperpolicy is updated via mirror descent, where the update direction is obtained by an optimistic variant of least-squares policy evaluation. Furthermore, to illustrate the power of our method, we apply DORIS to constrained and vector-valued MDPs, which can be formulated as zero-sum Markov games with a fictitious opponent. ",
        "pdf_link": "https://openreview.net/pdf/3d25b46ef867366688cc4fabce7f2a3a3c33f9a7.pdf",
        "forum_url": "https://openreview.net/forum?id=bn0GZZdDfI1",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Faster Last-iterate Convergence of Policy Optimization in Zero-Sum Markov Games",
        "authors": [
            "Shicong Cen",
            "Yuejie Chi",
            "Simon Shaolei Du",
            "Lin Xiao"
        ],
        "published": "iclr 2023 poster",
        "summary": "Multi-Agent Reinforcement Learning (MARL)---where multiple agents learn to interact in a shared dynamic environment---permeates across a wide range of critical applications. While there has been substantial progress on understanding the global convergence of policy optimization methods in single-agent RL, designing and analysis of efficient policy optimization algorithms in the MARL setting present significant challenges and new desiderata, which unfortunately, remain highly inadequately addressed by existing theory. In this paper, we focus on the most basic setting of competitive multi-agent RL, namely two-player zero-sum Markov games, and study equilibrium finding algorithms in both the infinite-horizon discounted setting and the finite-horizon episodic setting. We propose a single-loop policy optimization method with symmetric updates from both agents, where the policy is updated via the entropy-regularized optimistic multiplicative weights update (OMWU) method and the value is updated on a slower timescale. We show that, in the full-information tabular setting, the proposed method achieves a finite-time last-iterate linear convergence to the quantal response equilibrium of the regularized problem, which translates to a sublinear convergence to the Nash equilibrium by controlling the amount of regularization. Our convergence results improve upon the best known iteration complexities, and lead to a better understanding of policy optimization in competitive Markov games.",
        "pdf_link": "https://openreview.net/pdf/d08b35cf66974eeaf9743746b69c38f772753301.pdf",
        "forum_url": "https://openreview.net/forum?id=bRwBpKrNzF7",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Safe Reinforcement Learning From Pixels Using a Stochastic Latent Representation",
        "authors": [
            "Yannick Hogewind",
            "Thiago D. Simão",
            "Tal Kachman",
            "Nils Jansen"
        ],
        "published": "iclr 2023 poster",
        "summary": "We address the problem of safe reinforcement learning from pixel observations. Inherent challenges in such settings are (1) a trade-off between reward optimization and adhering to safety constraints, (2) partial observability, and (3) high-dimensional observations. We formalize the problem in a constrained, partially observable Markov decision process framework, where an agent obtains distinct reward and safety signals. To address the curse of dimensionality, we employ a novel safety critic using the stochastic latent actor-critic (SLAC) approach. The latent variable model predicts rewards and safety violations, and we use the safety critic to train safe policies. Using well-known benchmark environments, we demonstrate competitive performance over existing approaches regarding computational requirements, final reward return, and satisfying the safety constraints. ",
        "pdf_link": "https://openreview.net/pdf/9c4b07fd4a61d7c3cbe4abfa26f9ce1b7f127301.pdf",
        "forum_url": "https://openreview.net/forum?id=b39dQt_uffW",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Offline RL for Natural Language Generation with Implicit Language Q Learning",
        "authors": [
            "Charlie Victor Snell",
            "Ilya Kostrikov",
            "Yi Su",
            "Sherry Yang",
            "Sergey Levine"
        ],
        "published": "iclr 2023 poster",
        "summary": "Large language models distill broad knowledge from text corpora. However, they can be inconsistent when it comes to completing user specified tasks. This issue can be addressed by finetuning such models via supervised learning on curated datasets, or via reinforcement learning. In this work, we propose a novel offline RL method, implicit language Q-learning (ILQL), designed for use on language models, that combines both the flexible utility maximization framework of RL algorithms with the ability of supervised learning to leverage previously collected data, as well as its simplicity and stability. Our method employs a combination of value conservatism alongside an implicit dataset support constraint in learning value functions, which are then used to guide language model generations towards maximizing user-specified utility functions. In addition to empirically validating ILQL, we present a detailed empirical analysis of situations where offline RL can be useful in natural language generation settings, demonstrating how it can be a more effective utility optimizer than prior approaches for end-to-end dialogue, and how it can effectively optimize high variance reward functions based on subjective judgement, such as whether to label a comment as toxic or not.",
        "pdf_link": "https://openreview.net/pdf/52d92f1f776e26f5cd3935e847b0f2b77d30b7e9.pdf",
        "forum_url": "https://openreview.net/forum?id=aBH_DydEvoH",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they can be inconsistent when it comes to completing user specified tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, they can be inconsistent when it comes to completing user specified tasks.\""
    },
    {
        "title": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks",
        "authors": [
            "Tushar Khot",
            "Harsh Trivedi",
            "Matthew Finlayson",
            "Yao Fu",
            "Kyle Richardson",
            "Peter Clark",
            "Ashish Sabharwal"
        ],
        "published": "iclr 2023 poster",
        "summary": "Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired.\nWe show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even simpler solvable sub-tasks. When the complexity comes from the input length, we can recursively decompose the task into the same task but with smaller inputs. We also evaluate our approach on textual multi-step reasoning tasks: on long-context multi-hop QA task, we can more effectively teach the sub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA, we can incorporate a symbolic information retrieval within our decomposition framework, leading to improved performance on both tasks. Datasets, Code and Prompts available at https://github.com/allenai/DecomP.",
        "pdf_link": "https://openreview.net/pdf/c0a9bac140b181384176f474f3533e053b8d663d.pdf",
        "forum_url": "https://openreview.net/forum?id=_nGgzQjzaRy",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks.\""
    },
    {
        "title": "Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints",
        "authors": [
            "David Henry Mguni",
            "Aivar Sootla",
            "Juliusz Krzysztof Ziomek",
            "Oliver Slumbers",
            "Zipeng Dai",
            "Kun Shao",
            "Jun Wang"
        ],
        "published": "iclr 2023 poster",
        "summary": "Many real-world settings involve costs for performing actions; transaction costs\nin financial systems and fuel costs being common examples. In these settings,\nperforming actions at each time step quickly accumulates costs leading to vastly\nsuboptimal outcomes. Additionally, repeatedly acting produces wear and tear and\nultimately, damage. Determining when to act is crucial for achieving successful\noutcomes and yet, the challenge of efficiently learning to behave optimally when\nactions incur minimally bounded costs remains unresolved. In this paper, we intro-\nduce a reinforcement learning (RL) framework named Learnable Impulse Control\nReinforcement Algorithm (LICRA), for learning to optimally select both when\nto act and which actions to take when actions incur costs. At the core of LICRA\nis a nested structure that combines RL and a form of policy known as impulse\ncontrol which learns to maximise objectives when actions incur costs. We prove\nthat LICRA, which seamlessly adopts any RL method, converges to policies that\noptimally select when to perform actions and their optimal magnitudes. We then\naugment LICRA to handle problems in which the agent can perform at most k < ∞\nactions and more generally, faces a budget constraint. We show LICRA learns the\noptimal value function and ensures budget constraints are satisfied almost surely.\nWe demonstrate empirically LICRA’s superior performance against benchmark\nRL methods in OpenAI gym’s Lunar Lander and in Highway environments and a\nvariant of the Merton portfolio problem within finance.",
        "pdf_link": "https://openreview.net/pdf/bc9be9e8ebfee6333ee4f81c2a12d4f382d07442.pdf",
        "forum_url": "https://openreview.net/forum?id=_BoPed4tYww",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Scaling Laws for a Multi-Agent Reinforcement Learning Model",
        "authors": [
            "Oren Neumann",
            "Claudius Gros"
        ],
        "published": "iclr 2023 poster",
        "summary": "The recent observation of neural power-law scaling relations has made a significant impact in the field of deep learning. A substantial amount of attention has been dedicated as a consequence to the description of scaling laws, although mostly for supervised learning and only to a reduced extent for reinforcement learning frameworks. In this paper we present an extensive study of performance scaling for a cornerstone reinforcement learning algorithm, AlphaZero. On the basis of a relationship between Elo rating, playing strength and power-law scaling, we train AlphaZero agents on the games Connect Four and Pentago and analyze their performance. We find that player strength scales as a power law in neural network parameter count when not bottlenecked by available compute, and as a power of compute when training optimally sized agents. We observe nearly identical scaling exponents for both games. Combining the two observed scaling laws we obtain a power law relating optimal size to compute similar to the ones observed for language models. We find that the predicted scaling of optimal neural network size fits our data for both games. This scaling law implies that previously published state-of-the-art game-playing models are significantly smaller than their optimal size, given the respective compute budgets. We also show that large AlphaZero models are more sample efficient, performing better than smaller models with the same amount of training data.",
        "pdf_link": "https://openreview.net/pdf/f98ab0085243b503d56efc920c90c19b95dc7ec8.pdf",
        "forum_url": "https://openreview.net/forum?id=ZrEbzL9eQ3W",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Combining the two observed scaling laws we obtain a power law relating optimal size to compute similar to the ones observed for language models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Combining the two observed scaling laws we obtain a power law relating optimal size to compute similar to the ones observed for language models.\""
    },
    {
        "title": "UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph",
        "authors": [
            "Jinhao Jiang",
            "Kun Zhou",
            "Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "iclr 2023 poster",
        "summary": "Multi-hop Question Answering over Knowledge Graph~(KGQA) aims to find the answer entities that are multiple hops away from the topic entities mentioned in a natural language question on a large-scale Knowledge Graph (KG).\nTo cope with the vast search space, existing work usually adopts a two-stage approach: it first retrieves a relatively small subgraph related to the question and then performs the reasoning on the subgraph to find the answer entities accurately.\nAlthough these two stages are highly related, previous work employs very different technical solutions for developing the retrieval and reasoning models, neglecting their relatedness in task essence. \nIn this paper, we propose UniKGQA, a novel approach for multi-hop KGQA task, by unifying  retrieval and reasoning in both model architecture and parameter learning.\nFor model architecture, UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs.\nFor parameter learning, we design a shared pre-training task based on question-relation matching for both retrieval and reasoning models, and then propose retrieval- and reasoning-oriented fine-tuning strategies.\nCompared with previous studies, our approach is more unified, tightly relating the retrieval and reasoning stages. \nExtensive experiments on three benchmark datasets have demonstrated the effectiveness of our method on the multi-hop KGQA task.\nOur codes and data are publicly available at~\\url{https://github.com/RUCAIBox/UniKGQA}.",
        "pdf_link": "https://openreview.net/pdf/6a73e95053a44e33e96d38a7d3fc15c14bbe50d9.pdf",
        "forum_url": "https://openreview.net/forum?id=Z63RvyAZ2Vh",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although these two stages are highly related, previous work employs very different technical solutions for developing the retrieval and reasoning models, neglecting their relatedness in task essence.\"\n\nThis paper does not discuss the limitations of LLMs in detail but mentions a limitation of previous work that employed LLMs, which is the use of different technical solutions for developing the retrieval and reasoning models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Although these two stages are highly related, previous work employs very different technical solutions for developing the retrieval and reasoning models, neglecting their relatedness in task essence.\"\n\nThis paper does not discuss the limitations of LLMs in detail but mentions a limitation of previous work that employed LLMs, which is the use of different technical solutions for developing the retrieval and reasoning models."
    },
    {
        "title": "Parameter-Efficient Fine-Tuning Design Spaces",
        "authors": [
            "Jiaao Chen",
            "Aston Zhang",
            "Xingjian Shi",
            "Mu Li",
            "Alex Smola",
            "Diyi Yang"
        ],
        "published": "iclr 2023 poster",
        "summary": "Parameter-efficient fine-tuning aims to achieve comparable performances of fine-tuning with much fewer trainable parameters. Recently, various tuning strategies (e.g., Adapters, Prefix Tuning, BitFit, and LoRA) have been proposed. However, their designs are hand-crafted separately, and it remains unclear whether certain design patterns exist for parameter-efficient fine-tuning. Thus, we present a parameter-efficient fine-tuning design paradigm and discover design patterns that are applicable to different experimental settings. Instead of focusing on designing another individual tuning strategy, we introduce parameter-efficient fine-tuning design spaces that parameterize tuning structures and tuning strategies. Specifically, any design space is characterized by four components: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. Our comprehensive empirical study leads to the discovery of design patterns: (i) grouping layers in a spindle pattern, (ii) uniformly allocating the number of trainable parameters to layers, (ii) tuning all the groups, and (iv) tuning different groups with proper strategies. Our discovered design patterns result in new parameter-efficient fine-tuning methods. Experiments show that these methods consistently outperform investigated parameter-efficient fine-tuning strategies across different backbone models and different tasks in natural language processing.",
        "pdf_link": "https://openreview.net/pdf/4c4f8f1aff3fb1f79e893b2ca5430c1207b90de4.pdf",
        "forum_url": "https://openreview.net/forum?id=XSRSWxyJIC",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
        "authors": [
            "Denny Zhou",
            "Nathanael Schärli",
            "Le Hou",
            "Jason Wei",
            "Nathan Scales",
            "Xuezhi Wang",
            "Dale Schuurmans",
            "Claire Cui",
            "Olivier Bousquet",
            "Quoc V Le",
            "Ed H. Chi"
        ],
        "published": "iclr 2023 poster",
        "summary": "Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split)  with an accuracy of at least 99\\% using just 14 exemplars, compared to only 16\\% accuracy with chain-of-thought prompting.  This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.",
        "pdf_link": "https://openreview.net/pdf/328fd7b9b742a2398905672f07b91af643001cb5.pdf",
        "forum_url": "https://openreview.net/forum?id=WZH7099tgfM",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts.\""
    },
    {
        "title": "Truthful Self-Play",
        "authors": [
            "Shohei Ohsawa"
        ],
        "published": "iclr 2023 poster",
        "summary": "We present a general framework for evolutionary learning to emergent unbiased state representation without any supervision. Evolutionary frameworks such as self-play converge to bad local optima in case of multi-agent reinforcement learning in non-cooperative partially observable environments with communication due to information asymmetry.  Our proposed framework is a simple modification of self-play inspired by mechanism design, also known as {\\em reverse game theory}, to elicit truthful signals and make the agents cooperative. The key idea is to add imaginary rewards using the peer prediction method, i.e., a mechanism for evaluating the validity of information exchanged between agents in a decentralized environment. Numerical experiments with predator prey, traffic junction and StarCraft tasks demonstrate that the state-of-the-art performance of our framework.",
        "pdf_link": "https://openreview.net/pdf/595f528bafd828c8e449b148ac29a727b9ccaa1d.pdf",
        "forum_url": "https://openreview.net/forum?id=WVRb98rwbv9",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "PINTO: Faithful Language Reasoning Using Prompt-Generated Rationales",
        "authors": [
            "PeiFeng Wang",
            "Aaron Chan",
            "Filip Ilievski",
            "Muhao Chen",
            "Xiang Ren"
        ],
        "published": "iclr 2023 poster",
        "summary": "Neural language models (LMs) have achieved impressive results on various language-based reasoning tasks by utilizing latent knowledge encoded in their own pretrained parameters. To make this reasoning process more explicit, recent works retrieve a rationalizing LM's internal knowledge by training or prompting it to generate free-text rationales, which can be used to guide task predictions made by either the same LM or a separate reasoning LM. However, rationalizing LMs require expensive rationale annotation and/or computation, without any assurance that their generated rationales improve LM task performance or faithfully reflect LM decision-making. In this paper, we propose PINTO, an LM pipeline that rationalizes via prompt-based learning, and learns to faithfully reason over rationales via counterfactual regularization. First, PINTO maps out a suitable reasoning process for the task input by prompting a frozen rationalizing LM to generate a free-text rationale. Second, PINTO's reasoning LM is fine-tuned to solve the task using the generated rationale as context, while regularized to output less confident predictions when the rationale is perturbed. Across four datasets, we show that PINTO significantly improves the generalization ability of the reasoning LM, yielding higher performance on both in-distribution and out-of-distribution test sets. Also, we find that PINTO's rationales are more faithful to its task predictions than those generated by competitive baselines.",
        "pdf_link": "https://openreview.net/pdf/7e3d881a1ec0910d26a1dcbaea914860cb610c81.pdf",
        "forum_url": "https://openreview.net/forum?id=WBXbRs63oVu",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"rationalizing LMs require expensive rationale annotation and/or computation, without any assurance that their generated rationales improve LM task performance or faithfully reflect LM decision-making.\"\n\nThis abstract mentions a limitation of LLMs, specifically the need for expensive annotation and/or computation for rationalizing LMs, and the uncertainty about whether generated rationales improve task performance or reflect decision-making. However, this",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"rationalizing LMs require expensive rationale annotation and/or computation, without any assurance that their generated rationales improve LM task performance or faithfully reflect LM decision-making.\"\n\nThis abstract mentions a limitation of LLMs, specifically the need for expensive annotation and/or computation for rationalizing LMs, and the uncertainty about whether generated rationales improve task performance or reflect decision-making. However, this"
    },
    {
        "title": "Protein Representation Learning via Knowledge Enhanced Primary Structure Reasoning",
        "authors": [
            "Hong-Yu Zhou",
            "Yunxiang Fu",
            "Zhicheng Zhang",
            "Bian Cheng",
            "Yizhou Yu"
        ],
        "published": "iclr 2023 poster",
        "summary": "Protein representation learning has primarily benefited from the remarkable development of language models (LMs). Accordingly, pre-trained protein models also suffer from a problem in LMs: a lack of factual knowledge. The recent solution models the relationships between protein and associated knowledge terms as the knowledge encoding objective. However, it fails to explore the relationships at a more granular level, i.e., the token level. To mitigate this, we propose Knowledge-exploited Auto-encoder for Protein (KeAP), which performs token-level knowledge graph exploration for protein representation learning. In practice, non-masked amino acids iteratively query the associated knowledge tokens to extract and integrate helpful information for restoring masked amino acids via attention. We show that KeAP can consistently outperform the previous counterpart on 9 representative downstream applications, sometimes surpassing it by large margins. These results suggest that KeAP provides an alternative yet effective way to perform knowledge enhanced protein representation learning.",
        "pdf_link": "https://openreview.net/pdf/e4ea10108dbde640764dcde0449ba8a267d56786.pdf",
        "forum_url": "https://openreview.net/forum?id=VbCMhg7MRmj",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"pre-trained protein models also suffer from a problem in LMs: a lack of factual knowledge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"pre-trained protein models also suffer from a problem in LMs: a lack of factual knowledge.\""
    },
    {
        "title": "Self-supervision through Random Segments with Autoregressive Coding (RandSAC)",
        "authors": [
            "Tianyu Hua",
            "Yonglong Tian",
            "Sucheng Ren",
            "Michalis Raptis",
            "Hang Zhao",
            "Leonid Sigal"
        ],
        "published": "iclr 2023 poster",
        "summary": "Inspired by the success of self-supervised autoregressive representation learning in natural language (GPT and its variants), and advances in recent visual architecture design with Vision Transformers (ViTs), in this paper, we explore the effects various design choices have on the success of applying such training strategies for visual feature learning. Specifically, we introduce a novel strategy that we call Random Segments with Autoregressive Coding (RandSAC). In RandSAC, we group patch representations (image tokens) into hierarchically arranged segments; within each segment, tokens are predicted in parallel, similar to BERT, while across segment predictions are sequential, similar to GPT. We illustrate that randomized serialization of the segments significantly improves the performance and results in distribution over spatially-long (across-segments) and -short (within-segment) predictions which are effective for feature learning. We illustrate the pertinence of these design choices and explore alternatives on a number of datasets (e.g., CIFAR10, ImageNet). While our pre-training strategy works with vanilla Transformer, we also propose a conceptually simple, but highly effective, addition to the decoder that allows learnable skip-connections to encoder feature layers, which further improves the performance.",
        "pdf_link": "https://openreview.net/pdf/4e53d24d80bcf1d31e7eea1d99a647f689b2bf87.pdf",
        "forum_url": "https://openreview.net/forum?id=Ubc74gTVo3",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Can BERT Refrain from Forgetting on Sequential Tasks? A Probing Study",
        "authors": [
            "Mingxu Tao",
            "Yansong Feng",
            "Dongyan Zhao"
        ],
        "published": "iclr 2023 poster",
        "summary": "Large pre-trained language models have helped to achieve state of the art on a variety of NLP tasks, nevertheless, they still suffer from forgetting when incrementally learning a series of sequential tasks. To alleviate this problem, recent works propose several models enhanced by sparse experience replay and local adaption, which yield satisfactory performance. However, in this paper we find that pre-trained language models like BERT have a potential ability to learn sequentially, even without any sparse memory replay. To verify the ability of BERT to maintain old knowledge, we adopt and re-finetune single-layer probe networks with the parameters of BERT fixed. We investigate the models on two typical kinds of NLP tasks, text classification and extractive question answering. And our experiments reveal that BERT can actually generate high quality representations for previous tasks in a long term, under extremely sparse replay or even no replay. We further introduce a series of methods to interpret the mechanism of forgetting and how memory rehearsal plays a significant role in task incremental learning, which bridges the gap between our new discovery and previous studies about catastrophic forgetting. Additionally, we provide both quantified and visualized results demonstrating that the representation space of BERT is always topologically organised, which guarantees its performance.",
        "pdf_link": "https://openreview.net/pdf/004c5b63bfdd7dc3e0577f31c9ce5ac302b1bc68.pdf",
        "forum_url": "https://openreview.net/forum?id=UazgYBMS9-W",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large pre-trained language models have helped to achieve state of the art on a variety of NLP tasks, nevertheless, they still suffer from forgetting when incrementally learning a series of sequential tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large pre-trained language models have helped to achieve state of the art on a variety of NLP tasks, nevertheless, they still suffer from forgetting when incrementally learning a series of sequential tasks.\""
    },
    {
        "title": "Nearly Minimax Optimal Offline Reinforcement Learning with Linear Function Approximation: Single-Agent MDP and Markov Game",
        "authors": [
            "Wei Xiong",
            "Han Zhong",
            "Chengshuai Shi",
            "Cong Shen",
            "Liwei Wang",
            "Tong Zhang"
        ],
        "published": "iclr 2023 poster",
        "summary": "Offline reinforcement learning (RL) aims at learning an optimal strategy using a pre-collected dataset without further interactions with the environment. While various algorithms have been proposed for offline RL in the previous literature, the minimax optimality has only been (nearly) established for tabular Markov decision processes (MDPs). In this paper, we focus on offline RL with linear function approximation and propose a new pessimism-based algorithm for offline linear MDP. At the core of our algorithm is the uncertainty decomposition via a reference function, which is new in the literature of offline RL under linear function approximation. Theoretical analysis demonstrates that our algorithm can match the performance lower bound up to logarithmic factors. We also extend our techniques to the two-player zero-sum Markov games (MGs), and establish a new performance lower bound for MGs, which tightens the existing result, and verifies the nearly minimax optimality of the proposed algorithm. To the best of our knowledge, these are the first computationally efficient and nearly minimax optimal algorithms for offline single-agent MDPs and MGs with linear function approximation.",
        "pdf_link": "https://openreview.net/pdf/4ec7b1dd165364db85e719d8b2232962869eb44d.pdf",
        "forum_url": "https://openreview.net/forum?id=UP_GHHPw7rP",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Learning to Jointly Share and Prune Weights for Grounding Based Vision and Language Models",
        "authors": [
            "Shangqian Gao",
            "Burak Uzkent",
            "Yilin Shen",
            "Heng Huang",
            "Hongxia Jin"
        ],
        "published": "iclr 2023 poster",
        "summary": "Transformers have seen growing interest in processing different modalities,  including language and image data. As a result, we can process vision and language data using transformers that are architecturally similar. Leveraging this feature of transformers, we propose weight sharing across two transformer backbones and within the same transformer backbone and pruning across two backbones in a unified framework. More specifically, we investigate weight sharing and pruning for two components of the transformers: (1) Multi-Head Attention (MSA) and (2) Feed-Forward Network (FFN) layers. To jointly perform weight sharing and pruning, we propose to use a regularization term to align model weights and the desired structure during the multimodal pre-training step. The structure vectors of sharing and pruning are generated by using a hypernetwork, which can capture complex interactions between pruning and sharing across layers and modalities. We train the hypernetwork and model weights iteratively so that the learned structure evolves along with model weights. After minimizing the proposed objective in the pre-training step, we perform weight sharing and pruning and fine-tune the compressed model on downstream tasks. Finally, we perform experiments on vision and language tasks, including Referring Expression Comprehension (REC), Visual Question Answering (VQA), and Object Detection using the state-of-the-art grounding based models: MDETR and GLIP. Our experiments show that we can compress these models by $35-40\\%$ by sharing and pruning MSA and FFN weights without almost any loss in accuracy.",
        "pdf_link": "https://openreview.net/pdf/090eb3d3debddf7f3522e5122d1f1f190e4f4082.pdf",
        "forum_url": "https://openreview.net/forum?id=UMERaIHMwB3",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations mentioned, but the need for weight sharing and pruning implies that large models may be inefficient or have redundant weights.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations mentioned, but the need for weight sharing and pruning implies that large models may be inefficient or have redundant weights."
    },
    {
        "title": "Progressive Prompts: Continual Learning for Language Models",
        "authors": [
            "Anastasia Razdaibiedina",
            "Yuning Mao",
            "Rui Hou",
            "Madian Khabsa",
            "Mike Lewis",
            "Amjad Almahairi"
        ],
        "published": "iclr 2023 poster",
        "summary": "We introduce Progressive Prompts – a simple and efficient approach for continual learning in language models. Our method allows forward transfer and resists catastrophic forgetting, without relying on data replay or a large number of task-specific parameters. Progressive Prompts learns a new soft prompt for each task and sequentially concatenates it with the previously learned prompts, while keeping the base model frozen. Experiments on standard continual learning benchmarks show that our approach outperforms state-of-the-art methods, with an improvement >20% in average test accuracy over the previous best-preforming method on T5 model. We also explore a more challenging continual learning setup with longer sequences of tasks and show that Progressive Prompts significantly outperforms prior methods.",
        "pdf_link": "https://openreview.net/pdf/fd24677a6ed80ba3b6c8dbbcacfbeb6bbd507bc5.pdf",
        "forum_url": "https://openreview.net/forum?id=UJTgQBc91_",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "Towards Minimax Optimal Reward-free Reinforcement Learning in Linear MDPs",
        "authors": [
            "Pihe Hu",
            "Yu Chen",
            "Longbo Huang"
        ],
        "published": "iclr 2023 poster",
        "summary": "We study reward-free reinforcement learning with linear function approximation for episodic Markov decision processes (MDPs). In this setting, an agent first interacts with the environment without accessing the reward function in the exploration phase. In the subsequent planning phase, it is given a reward function and asked to output an $\\epsilon$-optimal policy. We propose a novel algorithm LSVI-RFE under the linear MDP setting, where the transition probability and reward functions are linear in a feature mapping. We prove an $\\widetilde{O}(H^{4} d^{2}/\\epsilon^2)$ sample complexity upper bound for LSVI-RFE, where $H$ is the episode length and $d$ is the feature dimension. We also establish a sample complexity lower bound of $\\Omega(H^{3} d^{2}/\\epsilon^2)$. To the best of our knowledge, LSVI-RFE is the first computationally efficient algorithm that achieves the minimax optimal sample complexity in linear MDP settings up to an $H$ and logarithmic factors. Our LSVI-RFE algorithm is based on a novel variance-aware exploration mechanism to avoid overly-conservative exploration in prior works. Our sharp bound relies on the decoupling of UCB bonuses during two phases, and a Bernstein-type self-normalized bound, which remove the extra dependency of sample complexity on $H$ and $d$, respectively.",
        "pdf_link": "https://openreview.net/pdf/98b0ed97b22ff4771f3198ce6446f6efc032fadc.pdf",
        "forum_url": "https://openreview.net/forum?id=U9HW6vyNClg",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Solving Continuous Control via Q-learning",
        "authors": [
            "Tim Seyde",
            "Peter Werner",
            "Wilko Schwarting",
            "Igor Gilitschenski",
            "Martin Riedmiller",
            "Daniela Rus",
            "Markus Wulfmeier"
        ],
        "published": "iclr 2023 poster",
        "summary": "While there has been substantial success for solving continuous control with actor-critic methods, simpler critic-only methods such as Q-learning find limited application in the associated high-dimensional action spaces. However, most actor-critic methods come at the cost of added complexity: heuristics for stabilisation, compute requirements and wider hyperparameter search spaces. We show that a simple modification of deep Q-learning largely alleviates these issues. By combining bang-bang action discretization with value decomposition, framing single-agent control as cooperative multi-agent reinforcement learning (MARL), this simple critic-only approach matches performance of state-of-the-art continuous actor-critic methods when learning from features or pixels. We extend classical bandit examples from cooperative MARL to provide intuition for how decoupled critics leverage state information to coordinate joint optimization, and demonstrate surprisingly strong performance across a variety of continuous control tasks.",
        "pdf_link": "https://openreview.net/pdf/8785841c3d3960cea3b9230ca8db34e70e54e679.pdf",
        "forum_url": "https://openreview.net/forum?id=U5XOGxAgccS",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Learning Zero-Shot Cooperation with Humans, Assuming Humans Are Biased",
        "authors": [
            "Chao Yu",
            "Jiaxuan Gao",
            "Weilin Liu",
            "Botian Xu",
            "Hao Tang",
            "Jiaqi Yang",
            "Yu Wang",
            "Yi Wu"
        ],
        "published": "iclr 2023 poster",
        "summary": "There is a recent trend of applying multi-agent reinforcement learning (MARL) to train an agent that can cooperate with humans in a zero-shot fashion without using any human data. The typical workflow is to first repeatedly run self-play (SP) to build a policy pool and then train the final adaptive policy against this pool. A crucial limitation of this framework is that every policy in the pool is optimized w.r.t. the environment reward function, which implicitly assumes that the testing partners of the adaptive policy will be precisely optimizing the same reward function as well. However, human objectives are often substantially biased according to their own preferences, which can differ greatly from the environment reward. We propose a more general framework, Hidden-Utility Self-Play (HSP), which explicitly models human biases as hidden reward functions in the self-play objective. By approximating the reward space as linear functions, HSP adopts an effective technique to generate an augmented policy pool with biased policies. We evaluate HSP on the Overcooked benchmark. Empirical results show that our HSP method produces higher rewards than baselines when cooperating with learned human models, manually scripted policies, and real humans. The HSP policy is also rated as the most assistive policy based on human feedback.",
        "pdf_link": "https://openreview.net/pdf/009d6d3a093049a11f22f96153117bbec6e44c65.pdf",
        "forum_url": "https://openreview.net/forum?id=TrwE8l9aJzs",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints",
        "authors": [
            "Aran Komatsuzaki",
            "Joan Puigcerver",
            "James Lee-Thorp",
            "Carlos Riquelme Ruiz",
            "Basil Mustafa",
            "Joshua Ainslie",
            "Yi Tay",
            "Mostafa Dehghani",
            "Neil Houlsby"
        ],
        "published": "iclr 2023 poster",
        "summary": "Training large, deep neural networks to convergence can be prohibitively expensive. As a result, often only a small selection of popular, dense models are reused across different contexts and tasks. Increasingly, sparsely activated models, which seek to decouple model size from computation costs, are becoming an attractive alternative to dense models. Although more efficient in terms of quality and computation cost, sparse models remain data-hungry and costly to train from scratch in the large scale regime. In this work, we propose sparse upcycling -- a simple way to reuse sunk training costs by initializing a sparsely activated Mixture-of-Experts model from a dense checkpoint. We show that sparsely upcycled T5 Base, Large, and XL language models and Vision Transformer Base and Large models, respectively, significantly outperform their dense counterparts on SuperGLUE and ImageNet, using only ~50% of the initial dense pretraining sunk cost. The upcycled models also outperform sparse models trained from scratch on 100% of the initial dense pretraining computation budget.",
        "pdf_link": "https://openreview.net/pdf/c037cbccf13c2380ece6d1296d30d8e07d64b943.pdf",
        "forum_url": "https://openreview.net/forum?id=T5nUQDrM4u",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions that \"sparse models remain data-hungry and costly to train from scratch in the large scale regime\", which can be indirectly related to LLMs, however the main focus is on proposing a solution rather than discussing the limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions that \"sparse models remain data-hungry and costly to train from scratch in the large scale regime\", which can be indirectly related to LLMs, however the main focus is on proposing a solution rather than discussing the limitations."
    },
    {
        "title": "In-Situ Text-Only Adaptation of Speech Models with Low-Overhead Speech Imputations",
        "authors": [
            "Ashish Mittal",
            "Sunita Sarawagi",
            "Preethi Jyothi"
        ],
        "published": "iclr 2023 poster",
        "summary": "Fast and accurate adaptation of automatic speech recognition (ASR) systems using only text data in the target domain is a problem of long-standing practical relevance. Text-only adaptation was easy in traditional cascaded ASR systems with completely decoupled acoustic and language models. Recently, the RNNTransducer (RNN-T) has emerged as a default ASR model because of its high accuracy, low latency, and capability of supporting streaming input. However text-only adaptation of the RNN-T model is significantly more challenging due to its tight integration of acoustic and language models and end-to-end training. Existing recent approaches for text-only adaptation of RNN-Ts, either entail significant modification to the network or introduce high latency during decoding. We propose a new approach (TOLSTOI) that imputes speech representations internal to a baseline RNN-T, starting from text-only inputs, and performs in-situ adaptation that results in higher adaptation accuracy without any runtime overheads during decoding. Our imputation model is a function of the labeled data and trained parameters of the ASR model, and that we show, is more effective in controlling catastrophic forgetting compared to existing methods. We establish the effectiveness of TOLSTOI using three target domains and two ASR models of varying complexity. We yield up to 35% relative reduction in word error rate with text-only adaptation while forgetting the least compared to existing adaptation approaches. Our method is easy to implement and can be harnessed on existing RNN-T models without requiring ASR model training from scratch.",
        "pdf_link": "https://openreview.net/pdf/54c647f24fd2b8654ec28bcd6943439a635a84f8.pdf",
        "forum_url": "https://openreview.net/forum?id=T2Ncx_PN2K",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Become a Proficient Player with Limited Data through Watching Pure Videos",
        "authors": [
            "Weirui Ye",
            "Yunsheng Zhang",
            "Pieter Abbeel",
            "Yang Gao"
        ],
        "published": "iclr 2023 poster",
        "summary": "Recently, RL has shown its strong ability for visually complex tasks. However, it suffers from the low sample efficiency and poor generalization ability, which prevent RL from being useful in real-world scenarios. Inspired by the huge success of unsupervised pre-training methods on language and vision domains, we propose to improve the sample efficiency via a novel pre-training method for model-based RL. \nInstead of using pre-recorded agent trajectories that come with their own actions, we consider the setting where the pre-training data are action-free videos, which are more common and available in the real world. We introduce a two-phase training pipeline as follows: for the pre-training phase, we implicitly extract the hidden action embedding from videos and pre-train the visual representation and the environment dynamics network through a novel \\Changes{forward-inverse} cycle consistency \\Changes{(FICC)} objective based on vector quantization; for down-stream tasks, we finetune with small amount of task data based on the learned models. Our framework can significantly improve the sample efficiency on Atari Games with data of only one hour of game playing. We achieve 118.4\\% mean human performance and 36.0\\% median performance with only 50k environment steps, which is 85.6\\% and 65.1\\% better than the scratch EfficientZero model. We believe such pre-training approach can provide an option for solving real-world RL problems. The code is available at \\url{https://github.com/YeWR/FICC.git}.",
        "pdf_link": "https://openreview.net/pdf/b12ad9eb8aa7095d47b4f5b0248dba72a72616bf.pdf",
        "forum_url": "https://openreview.net/forum?id=Sy-o2N0hF4f",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Policy-Based Self-Competition for Planning Problems",
        "authors": [
            "Jonathan Pirnay",
            "Quirin Göttl",
            "Jakob Burger",
            "Dominik Gerhard Grimm"
        ],
        "published": "iclr 2023 poster",
        "summary": "AlphaZero-type algorithms may stop improving on single-player tasks in case the value network guiding the tree search is unable to approximate the outcome of an episode sufficiently well. One technique to address this problem is transforming the single-player task through self-competition. The main idea is to compute a scalar baseline from the agent’s historical performances and to reshape an episode’s reward into a binary output, indicating whether the baseline has been exceeded or not. However, this baseline only carries limited information for the agent about strategies how to improve. We leverage the idea of self-competition and directly incorporate a historical policy into the planning process instead of its scalar performance. Based on the recently introduced Gumbel AlphaZero (GAZ), we propose our algorithm GAZ ‘Play-to-Plan’ (GAZ PTP), in which the agent learns to find strong trajectories by planning against possible strategies of its past self. We show the effectiveness of our approach in two well-known combinatorial optimization problems, the Traveling Salesman Problem and the Job-Shop Scheduling Problem. With only half of the simulation budget for search, GAZ PTP consistently outperforms all selected single-player variants of GAZ.",
        "pdf_link": "https://openreview.net/pdf/e3b41709a4697b503c61af4852343e17df76da28.pdf",
        "forum_url": "https://openreview.net/forum?id=SmufNDN90G",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Language Models Can Teach Themselves to Program Better",
        "authors": [
            "Patrick Haluptzok",
            "Matthew Bowers",
            "Adam Tauman Kalai"
        ],
        "published": "iclr 2023 poster",
        "summary": "Recent Language Models (LMs) achieve breakthrough performance in code generation when trained on human-authored problems, even solving some competitive-programming problems. Self-play has proven useful in games such as Go, and thus it is natural to ask whether LMs can generate their own instructive programming problems to improve their performance. We show that it is possible for an LM to synthesize programming problems and solutions, which are filtered for correctness by a Python interpreter. The LM’s performance is then seen to improve when it is fine-tuned on its own synthetic problems and verified solutions; thus the model “improves itself” using the Python interpreter. Problems are specified formally as programming puzzles [Schuster et al. , 2021], a code-based problem format where solutions can easily be verified for correctness by execution. In experiments on publicly-available LMs, test accuracy more than doubles. This work demonstrates the potential for code LMs, with an interpreter, to generate instructive problems and improve their own performance.",
        "pdf_link": "https://openreview.net/pdf/70f1e70fce89088da12b7493b7d1a8d444a2acec.pdf",
        "forum_url": "https://openreview.net/forum?id=SaRj2ka1XZ3",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract does not mention any limitations of LLMs, but rather focuses on their potential and capabilities.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract does not mention any limitations of LLMs, but rather focuses on their potential and capabilities."
    },
    {
        "title": "Boosting Adversarial Transferability using Dynamic Cues",
        "authors": [
            "Muzammal Naseer",
            "Ahmad Mahmood",
            "Salman Khan",
            "Fahad Khan"
        ],
        "published": "iclr 2023 poster",
        "summary": "The transferability of adversarial perturbations between image models has been extensively studied. In this case, an attack is generated from a known surrogate \\eg, the ImageNet trained model, and transferred to change the decision of an unknown (black-box) model trained on an image dataset. However, attacks generated from image models do not capture the dynamic nature of a moving object or a changing scene due to a lack of temporal cues within image models. This leads to reduced transferability of adversarial attacks from representation-enriched \\emph{image} models such as Supervised Vision Transformers (ViTs), Self-supervised ViTs (\\eg, DINO), and Vision-language models (\\eg, CLIP) to black-box \\emph{video} models. In this work, we induce dynamic cues within the image models without sacrificing their original performance on images. To this end, we optimize \\emph{temporal prompts} through frozen image models to capture motion dynamics. Our temporal prompts are the result of a learnable transformation that allows optimizing for temporal gradients during an adversarial attack to fool the motion dynamics. Specifically, we introduce spatial (image) and temporal (video) cues within the same source model through task-specific prompts. Attacking such prompts maximizes the adversarial transferability from image-to-video and image-to-image models using the attacks designed for image models. As an example, an iterative attack launched from image model Deit-B with temporal prompts reduces generalization (top1 \\% accuracy) of a video model by 35\\% on Kinetics-400. Our approach also improves adversarial transferability to image models by 9\\% on ImageNet w.r.t the current state-of-the-art approach. Our attack results indicate that the attacker does not need specialized architectures, \\eg, divided space-time attention, 3D convolutions, or multi-view convolution networks for different data modalities. Image models are effective surrogates to optimize an adversarial attack to fool black-box models in a changing environment over time. Code is available at \\url{https://bit.ly/3Xd9gRQ}",
        "pdf_link": "https://openreview.net/pdf/9e990c20252d6a4dcc08a88751f2f07536fc4f76.pdf",
        "forum_url": "https://openreview.net/forum?id=SZynfVLGd5",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Learning to Compose Soft Prompts for Compositional Zero-Shot Learning",
        "authors": [
            "Nihal V. Nayak",
            "Peilin Yu",
            "Stephen Bach"
        ],
        "published": "iclr 2023 poster",
        "summary": "We introduce compositional soft prompting (CSP), a parameter-efficient learning technique to improve the zero-shot compositionality of large-scale pretrained vision-language models (VLMs) like CLIP. We develop CSP for compositional zero-shot learning, the task of predicting unseen attribute-object compositions (e.g., old cat and young tiger). VLMs have a flexible text encoder that can represent arbitrary classes as natural language prompts but they often underperform task-specific architectures on the compositional zero-shot benchmark datasets. CSP treats the attributes and objects that define classes as learnable tokens of vocabulary. During training, the vocabulary is tuned to recognize classes that compose tokens in multiple ways (e.g., old cat and white cat). At test time, we recompose the learned attribute-object vocabulary in new combinations to recognize novel classes. We show that CSP outperforms the CLIP on benchmark datasets by an average of 10.9 percentage points on AUC. CSP also outperforms CoOp, a soft prompting method that fine-tunes the prefix context tokens, by an average of 5.8 percentage points on AUC. We perform additional experiments to show that CSP improves generalization to higher-order attribute-attribute-object compositions (e.g., old white cat) and combinations of pretrained attributes and fine-tuned objects. The code is available at https://github.com/BatsResearch/csp.",
        "pdf_link": "https://openreview.net/pdf/0e212bc778de607b8b27f2b040f6b63d6261d59a.pdf",
        "forum_url": "https://openreview.net/forum?id=S8-A2FXnIh",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"VLMs have a flexible text encoder that can represent arbitrary classes as natural language prompts but they often underperform task-specific architectures on the compositional zero-shot benchmark datasets.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of VLMs (a type of LLM) in passing, but does not elaborate on it. The focus of the paper is on proposing",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"VLMs have a flexible text encoder that can represent arbitrary classes as natural language prompts but they often underperform task-specific architectures on the compositional zero-shot benchmark datasets.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of VLMs (a type of LLM) in passing, but does not elaborate on it. The focus of the paper is on proposing"
    },
    {
        "title": "Causal Confusion and Reward Misidentification in Preference-Based Reward Learning",
        "authors": [
            "Jeremy Tien",
            "Jerry Zhi-Yang He",
            "Zackory Erickson",
            "Anca Dragan",
            "Daniel S. Brown"
        ],
        "published": "iclr 2023 poster",
        "summary": "Learning policies via preference-based reward learning is an increasingly popular method for customizing agent behavior, but has been shown anecdotally to be prone to spurious correlations and reward hacking behaviors. While much prior work focuses on causal confusion in reinforcement learning and behavioral cloning, we focus on a systematic study of causal confusion and reward misidentification when learning from preferences. In particular, we perform a series of sensitivity and ablation analyses on several benchmark domains where rewards learned from preferences achieve minimal test error but fail to generalize to out-of-distribution states---resulting in poor policy performance when optimized. We find that the presence of non-causal distractor features, noise in the stated preferences, and partial state observability can all exacerbate reward misidentification. We also identify a set of methods with which to interpret misidentified learned rewards. In general, we observe that optimizing misidentified rewards drives the policy off the reward's training distribution, resulting in high predicted (learned) rewards but low true rewards. These findings illuminate the susceptibility of preference learning to reward misidentification and causal confusion---failure to consider even one of many factors can result in unexpected, undesirable behavior. ",
        "pdf_link": "https://openreview.net/pdf/f41368bc311fd0e894120cf88134acdbc361ec94.pdf",
        "forum_url": "https://openreview.net/forum?id=R0Xxvr_X3ZA",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Task Ambiguity in Humans and Language Models",
        "authors": [
            "Alex Tamkin",
            "Kunal Handa",
            "Avash Shrestha",
            "Noah Goodman"
        ],
        "published": "iclr 2023 poster",
        "summary": "Language models have recently achieved strong performance across a wide range of NLP benchmarks. However, real world tasks are often poorly specified, and agents must deduce the intended behavior from a combination of context, instructions, and examples. We investigate how both humans and models behave in the face of such task ambiguity by proposing AmbiBench, a new benchmark of six ambiguously-specified classification tasks. We evaluate humans and models on AmbiBench by seeing how well they identify the intended task using 1) instructions with varying degrees of ambiguity, and 2) different numbers of labeled examples. We find that the combination of model scaling (to 175B parameters) and reinforcement learning from human feedback (RLHF) enables models to approach or exceed the accuracy of human participants across tasks, but that either one of these alone is not sufficient. In addition, we show how to dramatically improve the accuracy of language models trained without RLHF by finetuning on a small number of ambiguous in-context examples, providing a promising direction for teaching models to generalize well in the face of ambiguity.",
        "pdf_link": "https://openreview.net/pdf/ad665e20278befbeb0b883432e1a85e44109b64d.pdf",
        "forum_url": "https://openreview.net/forum?id=QrnDe_9ZFd8",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"In addition, we show how to dramatically improve the accuracy of language models trained without RLHF by finetuning on a small number of ambiguous in-context examples, providing a promising direction for teaching models to generalize well in the face of ambiguity.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs (difficulty generalizing in the face of ambiguity) but only briefly and",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"In addition, we show how to dramatically improve the accuracy of language models trained without RLHF by finetuning on a small number of ambiguous in-context examples, providing a promising direction for teaching models to generalize well in the face of ambiguity.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs (difficulty generalizing in the face of ambiguity) but only briefly and"
    },
    {
        "title": "Revocable Deep Reinforcement Learning with Affinity Regularization for Outlier-Robust Graph Matching",
        "authors": [
            "Chang Liu",
            "Zetian Jiang",
            "Runzhong Wang",
            "Lingxiao Huang",
            "Pinyan Lu",
            "Junchi Yan"
        ],
        "published": "iclr 2023 poster",
        "summary": "Graph matching (GM) has been a building block in various areas including computer vision and pattern recognition. Despite recent impressive progress, existing deep GM methods often have obvious difficulty in handling outliers, which are ubiquitous in practice. We propose a deep reinforcement learning based approach RGM, whose sequential node matching scheme naturally fits the strategy for selective inlier matching against outliers. A revocable action framework is devised to improve the agent's flexibility against the complex constrained GM. Moreover, we propose a quadratic approximation technique to regularize the affinity score, in the presence of outliers. As such, the agent can finish inlier matching timely when the affinity score stops growing, for which otherwise an additional parameter i.e. the number of inliers is needed to avoid matching outliers. In this paper, we focus on learning the back-end solver under the most general form of GM: the Lawler's QAP, whose input is the affinity matrix. Especially, our approach can also boost existing GM methods that use such input. Experiments on multiple real-world datasets demonstrate its performance regarding both accuracy and robustness.",
        "pdf_link": "https://openreview.net/pdf/11e4101c1705eaa7c0f4626439b493d3b84ed36e.pdf",
        "forum_url": "https://openreview.net/forum?id=QjQibO3scV_",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Provably Efficient Lifelong Reinforcement Learning with Linear Representation",
        "authors": [
            "Sanae Amani",
            "Lin Yang",
            "Ching-An Cheng"
        ],
        "published": "iclr 2023 poster",
        "summary": "We theoretically study lifelong reinforcement learning (RL) with linear representation in a regret minimization setting. The goal of the agent is to learn a multi-task policy based on a linear representation while solving a sequence of tasks that may be adaptively chosen based on the agent's past behaviors. We frame the problem as a linearly parameterized contextual Markov decision process (MDP), where each task is specified by a context and the transition dynamics is context-independent, and we introduce a new completeness-style assumption on the representation which is sufficient to ensure the optimal multi-task policy is realizable under the linear representation. Under this assumption, we propose an algorithm, called UCB Lifelong Value Distillation (UCBlvd), that provably achieves sublinear regret for any sequence of tasks while using only sublinear planning calls. Specifically, for $K$ task episodes of horizon $H$, our algorithm has a regret bound $\\tilde{\\mathcal{O}}(\\sqrt{(d^3+d^\\prime d)H^4K})$ using $\\mathcal{O}(dH\\log(K))$ number of planning calls, where $d$ and $d^\\prime$ are the feature dimensions of the dynamics and rewards, respectively. This theoretical guarantee implies that our algorithm can enable a lifelong learning agent to learn to internalize experiences into a multi-task policy and rapidly solve new tasks.",
        "pdf_link": "https://openreview.net/pdf/2e81dc2a2f2529b307345174708e2ffa49eb464e.pdf",
        "forum_url": "https://openreview.net/forum?id=Qd0p0bl-A9t",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Achieving Near-Optimal Individual Regret & Low Communications in Multi-Agent Bandits",
        "authors": [
            "Xuchuang Wang",
            "Lin Yang",
            "Yu-Zhen Janice Chen",
            "Xutong Liu",
            "Mohammad Hajiesmaili",
            "Don Towsley",
            "John C.S. Lui"
        ],
        "published": "iclr 2023 poster",
        "summary": "Cooperative multi-agent multi-armed bandits (CM2AB) study how distributed agents cooperatively play the same multi-armed bandit game. Most existing CM2AB works focused on maximizing the group performance of all agents---the accumulation of all agents' individual performance (i.e., individual reward). However, in many applications, the performance of the system is more sensitive to the ``bad'' agent---the agent with the worst individual performance. For example, in a drone swarm, a ``bad'' agent may crash into other drones and severely degrade the system performance. In that case, the key of the learning algorithm design is to coordinate computational and communicational resources among agents so to optimize the individual learning performance of the ``bad'' agent. In CM2AB, maximizing the group performance is equivalent to minimizing the group regret of all agents, and minimizing the individual performance can be measured by minimizing the maximum (worst) individual regret among agents. Minimizing the maximum individual regret was largely ignored in prior literature, and currently, there is little work on how to minimize this objective with a low communication overhead. In this paper, we propose a near-optimal algorithm on both individual and group regrets, in addition,  we also propose a novel communication module in the algorithm, which only needs \\(O(\\log (\\log T))\\) communication times where \\(T\\) is the number of decision rounds. We also conduct simulations to illustrate the advantage of our algorithm by comparing it to other known baselines.",
        "pdf_link": "https://openreview.net/pdf/04213bd9368859af7649a1f1b85b691f3a583f8b.pdf",
        "forum_url": "https://openreview.net/forum?id=QTXKTXJKIh",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Does Deep Learning Learn to Abstract? A Systematic Probing Framework",
        "authors": [
            "Shengnan An",
            "Zeqi Lin",
            "Bei Chen",
            "Qiang Fu",
            "Nanning Zheng",
            "Jian-Guang Lou"
        ],
        "published": "iclr 2023 poster",
        "summary": "Abstraction is a desirable capability for deep learning models, which means to induce abstract concepts from concrete instances and flexibly apply them beyond the learning context. At the same time, there is a lack of clear understanding about both the presence and further characteristics of this capability in deep learning models. In this paper, we introduce a systematic probing framework to explore the abstraction capability of deep learning models from a transferability perspective. A set of controlled experiments are conducted based on this framework, providing strong evidence that two probed pre-trained language models (PLMs), T5 and GPT2, have the abstraction capability. We also conduct in-depth analysis, thus shedding further light: (1) the whole training phase exhibits a \"memorize-then-abstract\" two-stage process; (2) the learned abstract concepts are gathered in a few middle-layer attention heads, rather than being evenly distributed throughout the model; (3) the probed abstraction capabilities exhibit robustness against concept mutations, and are more robust to low-level/source-side mutations than high-level/target-side ones; (4) generic pre-training is critical to the emergence of abstraction capability, and PLMs exhibit better abstraction with larger model sizes and data scales.",
        "pdf_link": "https://openreview.net/pdf/b2f9d04c3fb3ccc28534c9012cac99faec1f9aaf.pdf",
        "forum_url": "https://openreview.net/forum?id=QB1dMPEXau5",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"there is a lack of clear understanding about both the presence and further characteristics of this capability in deep learning models.\"\n\nNote: Although the paper discusses limitations in understanding the abstraction capability of deep learning models, it does not specifically focus on limitations of Large Language Models (LLMs) but rather uses pre-trained language models (PLMs) as examples.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"there is a lack of clear understanding about both the presence and further characteristics of this capability in deep learning models.\"\n\nNote: Although the paper discusses limitations in understanding the abstraction capability of deep learning models, it does not specifically focus on limitations of Large Language Models (LLMs) but rather uses pre-trained language models (PLMs) as examples."
    },
    {
        "title": "Order Matters: Agent-by-agent Policy Optimization",
        "authors": [
            "Xihuai Wang",
            "Zheng Tian",
            "Ziyu Wan",
            "Ying Wen",
            "Jun Wang",
            "Weinan Zhang"
        ],
        "published": "iclr 2023 poster",
        "summary": "While multi-agent trust region algorithms have achieved great success empirically in solving coordination tasks, most of them,  however, suffer from a non-stationarity problem since agents update their policies simultaneously. In contrast, a sequential scheme that updates policies agent-by-agent provides another perspective and shows strong performance. However, sample inefficiency and lack of monotonic improvement guarantees for each agent are still the two significant challenges for the sequential scheme. In this paper, we propose the \\textbf{A}gent-by-\\textbf{a}gent \\textbf{P}olicy \\textbf{O}ptimization (A2PO) algorithm to improve the sample efficiency and retain the guarantees of monotonic improvement for each agent during training. We justify the tightness of the monotonic improvement bound compared with other trust region algorithms. From the perspective of sequentially updating agents, we further consider the effect of agent updating order and extend the theory of non-stationarity into the sequential update scheme. To evaluate A2PO, we conduct a comprehensive empirical study on four benchmarks: StarCraftII, Multi-agent MuJoCo, Multi-agent Particle Environment, and Google Research Football full game scenarios. A2PO consistently outperforms strong baselines.",
        "pdf_link": "https://openreview.net/pdf/b7c35e63818d65e4523a6ae4314674a0eeb7bb36.pdf",
        "forum_url": "https://openreview.net/forum?id=Q-neeWNVv1",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "LexMAE: Lexicon-Bottlenecked Pretraining for Large-Scale Retrieval",
        "authors": [
            "Tao Shen",
            "Xiubo Geng",
            "Chongyang Tao",
            "Can Xu",
            "Xiaolong Huang",
            "Binxing Jiao",
            "Linjun Yang",
            "Daxin Jiang"
        ],
        "published": "iclr 2023 poster",
        "summary": "In large-scale retrieval, the lexicon-weighting paradigm, learning weighted sparse representations in vocabulary space, has shown promising results with high quality and low latency. Despite it deeply exploiting the lexicon-representing capability of pre-trained language models, a crucial gap remains between language modeling and lexicon-weighting retrieval -- the former preferring certain or low-entropy words whereas the latter favoring pivot or high-entropy words -- becoming the main barrier to lexicon-weighting performance for large-scale retrieval. To bridge this gap, we propose a brand-new pre-training framework, lexicon-bottlenecked masked autoencoder (LexMAE), to learn importance-aware lexicon representations. Essentially, we present a lexicon-bottlenecked module between a normal language modeling encoder and a weakened decoder, where a continuous bag-of-words bottleneck is constructed to learn a lexicon-importance distribution in an unsupervised fashion. The pre-trained LexMAE is readily transferred to the lexicon-weighting retrieval via fine-tuning. On the ad-hoc retrieval benchmark, MS-Marco, it achieves 42.6% MRR@10 with 45.8 QPS for the passage dataset and 44.4% MRR@100 with 134.8 QPS for the document dataset, by a CPU machine. And LexMAE shows state-of-the-art zero-shot transfer capability on BEIR benchmark with 12 datasets. ",
        "pdf_link": "https://openreview.net/pdf/444a2eefa1d302c6578e50b28a266f21a0f2dd78.pdf",
        "forum_url": "https://openreview.net/forum?id=PfpEtB3-csK",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"a crucial gap remains between language modeling and lexicon-weighting retrieval -- the former preferring certain or low-entropy words whereas the latter favoring pivot or high-entropy words -- becoming the main barrier to lexicon-weighting performance for large-scale retrieval.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"a crucial gap remains between language modeling and lexicon-weighting retrieval -- the former preferring certain or low-entropy words whereas the latter favoring pivot or high-entropy words -- becoming the main barrier to lexicon-weighting performance for large-scale retrieval.\""
    },
    {
        "title": "Offline Congestion Games: How Feedback Type Affects Data Coverage Requirement",
        "authors": [
            "Haozhe Jiang",
            "Qiwen Cui",
            "Zhihan Xiong",
            "Maryam Fazel",
            "Simon Shaolei Du"
        ],
        "published": "iclr 2023 poster",
        "summary": "This paper investigates when one can efficiently recover an approximate Nash Equilibrium (NE) in offline congestion games. The existing dataset coverage assumption in offline general-sum games inevitably incurs a dependency on the number of actions, which can be exponentially large in congestion games. We consider three different types of feedback with decreasing revealed information. Starting from the facility-level (a.k.a., semi-bandit) feedback, we propose a novel one-unit deviation coverage condition and show a pessimism-type algorithm that can recover an approximate NE. For the agent-level (a.k.a., bandit) feedback setting, interestingly, we show the one-unit deviation coverage condition is not sufficient. On the other hand, we convert the game to multi-agent linear bandits and show that with a generalized data coverage assumption in offline linear bandits, we can efficiently recover the approximate NE. Lastly, we consider a novel type of feedback, the game-level feedback where only the total reward from all agents is revealed. Again, we show the coverage assumption for the agent-level feedback setting is insufficient in the game-level feedback setting, and with a stronger version of the data coverage assumption for linear bandits, we can recover an approximate NE. Together, our results constitute the first study of offline congestion games and imply formal separations between different types of feedback.",
        "pdf_link": "https://openreview.net/pdf/5d536b4991d001b1cb1a44b7b6d5785c2f3012d8.pdf",
        "forum_url": "https://openreview.net/forum?id=PXVGer7hmJ",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Compositional Law Parsing with Latent Random Functions",
        "authors": [
            "Fan Shi",
            "Bin Li",
            "Xiangyang Xue"
        ],
        "published": "iclr 2023 poster",
        "summary": "Human cognition has compositionality. We understand a scene by decomposing the scene into different concepts (e.g., shape and position of an object) and learning the respective laws of these concepts, which may be either natural (e.g., laws of motion) or man-made (e.g., laws of a game). The automatic parsing of these laws indicates the model's ability to understand the scene, which makes law parsing play a central role in many visual tasks. This paper proposes a deep latent variable model for Compositional LAw Parsing (CLAP), which achieves the human-like compositionality ability through an encoding-decoding architecture to represent concepts in the scene as latent variables. CLAP employs concept-specific latent random functions instantiated with Neural Processes to capture the law of concepts. Our experimental results demonstrate that CLAP outperforms the baseline methods in multiple visual tasks such as intuitive physics, abstract visual reasoning, and scene representation. The law manipulation experiments illustrate CLAP's interpretability by modifying specific latent random functions on samples. For example, CLAP learns the laws of position-changing and appearance constancy from the moving balls in a scene, making it possible to exchange laws between samples or compose existing laws into novel laws.",
        "pdf_link": "https://openreview.net/pdf/40faa239ba5304c9cc2669fee09427b5d2347d4c.pdf",
        "forum_url": "https://openreview.net/forum?id=PEuxUXIMLlA",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Understanding Zero-shot Adversarial Robustness for Large-Scale Models",
        "authors": [
            "Chengzhi Mao",
            "Scott Geng",
            "Junfeng Yang",
            "Xin Wang",
            "Carl Vondrick"
        ],
        "published": "iclr 2023 poster",
        "summary": "Pretrained large-scale vision-language models like CLIP have exhibited strong generalization over unseen tasks. Yet imperceptible adversarial perturbations can significantly reduce CLIP's performance on new tasks. In this work, we identify and explore the problem of adapting large-scale models for zero-shot adversarial robustness. We first identify two key factors during model adaption--training losses and adaptation methods--that affect the model's zero-shot adversarial robustness. We then propose a text-guided contrastive adversarial training loss, which aligns the text embeddings and the adversarial visual features with contrastive learning on a small set of training data. We apply this training loss to two adaption methods, model finetuning and visual prompt tuning. We find that visual prompt tuning is more effective in the absence of texts, while finetuning wins in the existence of text guidance. Overall, our approach significantly improves the zero-shot adversarial robustness over CLIP, seeing an average improvement of 31 points over ImageNet and 15 zero-shot datasets. We hope this work can shed light on understanding the zero-shot adversarial robustness of large-scale models. ",
        "pdf_link": "https://openreview.net/pdf/3260fd25cd7bfa3805e9cf3a86f91b87e701984d.pdf",
        "forum_url": "https://openreview.net/forum?id=P4bXCawRi5J",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Yet imperceptible adversarial perturbations can significantly reduce CLIP's performance on new tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Yet imperceptible adversarial perturbations can significantly reduce CLIP's performance on new tasks.\""
    },
    {
        "title": "Boosting Multiagent Reinforcement Learning via Permutation Invariant and Permutation Equivariant Networks",
        "authors": [
            "Jianye HAO",
            "Xiaotian Hao",
            "Hangyu Mao",
            "Weixun Wang",
            "Yaodong Yang",
            "Dong Li",
            "YAN ZHENG",
            "Zhen Wang"
        ],
        "published": "iclr 2023 poster",
        "summary": "The state space in Multiagent Reinforcement Learning (MARL) grows exponentially with the agent number. Such a curse of dimensionality results in poor scalability and low sample efficiency, inhibiting MARL for decades. To break this curse, we propose a unified agent permutation framework that exploits the permutation invariance (PI) and permutation equivariance (PE) inductive biases to reduce the multiagent state space. Our insight is that permuting the order of entities in the factored multiagent state space does not change the information. Specifically, we propose two novel implementations: a Dynamic Permutation Network (DPN) and a Hyper Policy Network (HPN). The core idea is to build separate entity-wise PI input and PE output network modules to connect the entity-factored state space and action space in an end-to-end way. DPN achieves such connections by two separate module selection networks, which consistently assign the same input module to the same input entity (guarantee PI) and assign the same output module to the same entity-related output (guarantee PE). To enhance the representation capability, HPN replaces the module selection networks of DPN with hypernetworks to directly generate the corresponding module weights. Extensive experiments in SMAC, Google Research Football and MPE validate that the proposed methods significantly boost the performance and the learning efficiency of existing MARL algorithms. Remarkably, in SMAC, we achieve 100% win rates in almost all hard and super-hard scenarios (never achieved before).",
        "pdf_link": "https://openreview.net/pdf/12d1e22292ed56512748543827e4cf87c98d1534.pdf",
        "forum_url": "https://openreview.net/forum?id=OxNQXyZK-K8",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Rethinking Symbolic Regression: Morphology and Adaptability in the Context of Evolutionary Algorithms",
        "authors": [
            "Kei Sen Fong",
            "Shelvia Wongso",
            "Mehul Motani"
        ],
        "published": "iclr 2023 poster",
        "summary": "Symbolic Regression (SR) is the well-studied problem of finding closed-form analytical expressions that describe the relationship between variables in a measurement dataset. In this paper, we rethink SR from two perspectives: morphology and adaptability. Morphology: Current SR algorithms typically use several man-made heuristics to influence the morphology (or structure) of the expressions in the search space. These man-made heuristics may introduce unintentional bias and data leakage, especially with the relatively few equation-recovery benchmark problems available for evaluating SR approaches. To address this, we formulate a novel minimalistic approach, based on constructing a depth-aware mathematical language model trained on terminal walks of expression trees, as a replacement to these heuristics. Adaptability: Current SR algorithms tend to select expressions based on only a single fitness function (e.g., MSE on the training set). We promote the use of an adaptability framework in evolutionary SR which uses fitness functions that alternate across generations. This leads to robust expressions that perform well on the training set and are close to the true functional form. We demonstrate this by alternating fitness functions that quantify faithfulness to values (via MSE) and empirical derivatives (via a novel theoretically justified fitness metric coined MSEDI). Proof-of-concept: We combine these ideas into a minimalistic evolutionary SR algorithm that outperforms all benchmark and state of-the-art SR algorithms in problems with unknown constants added, which we claim are more reflective of SR performance for real-world applications. Our claim is then strengthened by reproducing the superior performance on real-world regression datasets from SRBench. For researchers interested in equation-recovery problems, we also propose a set of conventions that can be used to promote fairness in comparison across SR methods and to reduce unintentional bias.",
        "pdf_link": "https://openreview.net/pdf/7af58da043c44659d490a741c427285fdc8346d6.pdf",
        "forum_url": "https://openreview.net/forum?id=OPGy07PojsZ",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 Small",
        "authors": [
            "Kevin Ro Wang",
            "Alexandre Variengien",
            "Arthur Conmy",
            "Buck Shlegeris",
            "Jacob Steinhardt"
        ],
        "published": "iclr 2023 poster",
        "summary": "Research in mechanistic interpretability seeks to explain behaviors of ML models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task that requires logical reasoning: indirect object identification (IOI). Our explanation encompasses 28 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches including causal interventions and projections.\nTo our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior \"in the wild\" in a language model.  We evaluate the reliability of our explanation using three quantitative criteria - faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. \nOur work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.",
        "pdf_link": "https://openreview.net/pdf/1e69126a0944a99f44e32245d87e213a30cc2eb2.pdf",
        "forum_url": "https://openreview.net/forum?id=NpsVSN6o4ul",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Though these criteria support our explanation, they also point to remaining gaps in our understanding.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Though these criteria support our explanation, they also point to remaining gaps in our understanding.\""
    },
    {
        "title": "GoBigger: A Scalable Platform for Cooperative-Competitive Multi-Agent Interactive Simulation",
        "authors": [
            "Ming Zhang",
            "Shenghan Zhang",
            "Zhenjie Yang",
            "Lekai Chen",
            "Jinliang Zheng",
            "Chao Yang",
            "Chuming Li",
            "Hang Zhou",
            "Yazhe Niu",
            "Yu Liu"
        ],
        "published": "iclr 2023 poster",
        "summary": "The emergence of various multi-agent environments has motivated powerful algorithms to explore agents' cooperation or competition. Even though this has greatly promoted the development of multi-agent reinforcement learning  (MARL), it is still not enough to support further exploration on the behavior of swarm intelligence between multiple teams, and cooperation between multiple agents due to their limited scalability. To alleviate this, we introduce GoBigger, a scalable platform for cooperative-competition multi-agent interactive simulation. GoBigger is an enhanced environment for the Agar-like game, enabling the simulation of multiple scales of agent intra-team cooperation and inter-team competition. Compared with existing multi-agent simulation environments, our platform supports multi-team games with more than two teams simultaneously, which dramatically expands the diversity of agent cooperation and competition, and can more effectively simulate the swarm intelligent agent behavior. Besides, in GoBigger, the cooperation between the agents in a team can lead to much higher performance. We offer a diverse set of challenging scenarios, built-in bots, and visualization tools for best practices in benchmarking. We evaluate several state-of-the-art algorithms on GoBigger and demonstrate the potential of the environment. We believe this platform can inspire various emerging research directions in MARL, swarm intelligence, and large-scale agent interactive learning. Both GoBigger and its related benchmark are open-sourced. More information could be found at https://github.com/opendilab/GoBigger.",
        "pdf_link": "https://openreview.net/pdf/a9b5c1cac35cce14dad1036f5b0f324ad899d11f.pdf",
        "forum_url": "https://openreview.net/forum?id=NnOZT_CR26Z",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs."
    },
    {
        "title": "Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning",
        "authors": [
            "Zhen Wang",
            "Rameswar Panda",
            "Leonid Karlinsky",
            "Rogerio Feris",
            "Huan Sun",
            "Yoon Kim"
        ],
        "published": "iclr 2023 poster",
        "summary": "Prompt tuning, in which a base pretrained model is adapted to each task via conditioning on learned prompt vectors, has emerged as a promising approach for efficiently adapting large language models to multiple downstream tasks. However, existing methods typically learn soft prompt vectors from scratch, and it has not been clear how to exploit the rich cross-task knowledge with prompt vectors in a multitask learning setting. We propose multitask prompt tuning (MPT), which first learns a single transferable prompt by distilling knowledge from multiple task-specific source prompts. We then learn multiplicative low rank updates to this shared prompt to efficiently adapt it to each downstream target task. Extensive experiments on 23 NLP datasets demonstrate that our proposed approach outperforms the state-of-the-art methods, including the full finetuning baseline in some cases, despite only tuning $0.035\\%$ as many task-specific parameters.",
        "pdf_link": "https://openreview.net/pdf/7506d3076a848ae366fb1e6d0213bf0328ddd8aa.pdf",
        "forum_url": "https://openreview.net/forum?id=Nk2pDtuhTq",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations, but the paper aims to improve the efficiency of adapting large language models to multiple downstream tasks, implying that existing methods may be inefficient.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations, but the paper aims to improve the efficiency of adapting large language models to multiple downstream tasks, implying that existing methods may be inefficient."
    },
    {
        "title": "Simple Emergent Action Representations from Multi-Task Policy Training",
        "authors": [
            "Pu Hua",
            "Yubei Chen",
            "Huazhe Xu"
        ],
        "published": "iclr 2023 poster",
        "summary": "The low-level sensory and motor signals in deep reinforcement learning, which exist in high-dimensional spaces such as image observations or motor torques, are inherently challenging to understand or utilize directly for downstream tasks. While sensory representations have been extensively studied, the representations of motor actions are still an area of active exploration. Our work reveals that a space containing meaningful action representations emerges when a multi-task policy network takes as inputs both states and task embeddings. Moderate constraints are added to improve its representation ability. Therefore, interpolated or composed embeddings can function as a high-level interface within this space, providing instructions to the agent for executing meaningful action sequences. Empirical results demonstrate that the proposed action representations are effective for intra-action interpolation and inter-action composition with limited or no additional learning. Furthermore, our approach exhibits superior task adaptation ability compared to strong baselines in Mujoco locomotion tasks. Our work sheds light on the promising direction of learning action representations for efficient, adaptable, and composable RL, forming the basis of abstract action planning and the understanding of motor signal space. Project page: https://sites.google.com/view/emergent-action-representation/",
        "pdf_link": "https://openreview.net/pdf/af859d60f1af73991c631bf27224a456c44ce94a.pdf",
        "forum_url": "https://openreview.net/forum?id=NUl0ylt7SM",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models",
        "authors": [
            "Guande He",
            "Jianfei Chen",
            "Jun Zhu"
        ],
        "published": "iclr 2023 poster",
        "summary": "Large pre-trained language models (PLMs) have demonstrated strong performance on natural language understanding (NLU) tasks through fine-tuning. However, fine-tuned models still suffer from overconfident predictions, especially in out-of-domain settings. In this paper, we tackle the problem of calibrating fine-tuned language models. We demonstrate that the PLMs are well-calibrated on the masked language modeling task with robust predictive confidence under domain shift, yet the fine-tuned models fail to retain such property due to catastrophic forgetting, which impacts the calibration on the downstream classification task. In light of these observations, we evaluate the calibration of several methods that preserve pre-trained features and show that preserving pre-trained features can improve the calibration of fine-tuned language models. Among these methods, our proposed method that encourages the fine-tuned model to learn generative representations with auxiliary language modeling objective achieves competitive accuracy and the lowest expected calibration error compared to several strong baselines under both in-domain and out-of-domain settings on three downstream NLU tasks.",
        "pdf_link": "https://openreview.net/pdf/032565002e39f747d69e618dba2713aea4a88f39.pdf",
        "forum_url": "https://openreview.net/forum?id=NI7StoWHJPT",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, fine-tuned models still suffer from overconfident predictions, especially in out-of-domain settings.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, fine-tuned models still suffer from overconfident predictions, especially in out-of-domain settings.\""
    },
    {
        "title": "Learning Simultaneous Navigation and Construction in Grid Worlds ",
        "authors": [
            "Wenyu Han",
            "Haoran Wu",
            "Eisuke Hirota",
            "Alexander Gao",
            "Lerrel Pinto",
            "Ludovic Righetti",
            "Chen Feng"
        ],
        "published": "iclr 2023 poster",
        "summary": "We propose to study a new learning task, mobile construction, to enable an agent to build designed structures in 1/2/3D grid worlds while navigating in the same evolving environments. Unlike existing robot learning tasks such as visual navigation and object manipulation, this task is challenging because of the interdependence between accurate localization and strategic construction planning. In pursuit of generic and adaptive solutions to this partially observable Markov decision process (POMDP) based on deep reinforcement learning (RL), we design\na Deep Recurrent Q-Network (DRQN) with explicit recurrent position estimation in this dynamic grid world. Our extensive experiments show that pre-training this position estimation module before Q-learning can significantly improve the construction performance measured by the intersection-over-union score, achieving the best results in our benchmark of various baselines including model-free and model-based RL, a handcrafted SLAM-based policy, and human players. Our code is available at: https://ai4ce.github.io/SNAC/.",
        "pdf_link": "https://openreview.net/pdf/f0b59287a120fe72e3e962c0dd749985ff02ee71.pdf",
        "forum_url": "https://openreview.net/forum?id=NEtep2C7yD",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs."
    },
    {
        "title": "A Multi-Grained Self-Interpretable Symbolic-Neural Model For Single/Multi-Labeled Text Classification",
        "authors": [
            "Xiang Hu",
            "XinYu KONG",
            "Kewei Tu"
        ],
        "published": "iclr 2023 poster",
        "summary": "Deep neural networks based on layer-stacking architectures have historically suffered from poor inherent interpretability. Meanwhile, symbolic probabilistic models function with clear interpretability, but how to combine them with neural networks to enhance their performance remains to be explored. In this paper, we try to marry these two systems for text classification via a structured language model. We propose a Symbolic-Neural model that can learn to explicitly predict class labels of text spans from a constituency tree without requiring any access to span-level gold labels. As the structured language model learns to predict constituency trees in a self-supervised manner, only raw texts and sentence-level labels are required as training data, which makes it essentially a general constituent-level self-interpretable classification model. Our experiments demonstrate that our approach could achieve good prediction accuracy in downstream tasks. Meanwhile, the predicted span labels are consistent with human rationales to a certain degree.",
        "pdf_link": "https://openreview.net/pdf/61a0f38810f4448bf5fb2a81d922d715f92960d9.pdf",
        "forum_url": "https://openreview.net/forum?id=MLJ5TF5FtXH",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Open-Vocabulary Object Detection upon Frozen Vision and Language Models",
        "authors": [
            "Weicheng Kuo",
            "Yin Cui",
            "Xiuye Gu",
            "AJ Piergiovanni",
            "Anelia Angelova"
        ],
        "published": "iclr 2023 poster",
        "summary": "We present F-VLM, a simple open-vocabulary object detection method built uponFrozenVision andLanguageModels.  F-VLM simplifies the current multi-stage training pipeline by eliminating the need for knowledge distillation or detection-tailored pretraining.  Surprisingly, we observe that a frozen VLM: 1) retains the locality-sensitive features necessary for detection, and 2) is a strong region classifier.  We finetune only the detector head and combine the detector and VLM outputs for each region at inference time.  F-VLM shows compelling scaling behavior and achieves +6.5 mask AP improvement over the previous state of theart  on  novel  categories  of  LVIS  open-vocabulary  detection  benchmark. In addition, we demonstrate very competitive results on COCO open-vocabulary detection benchmark and cross-dataset transfer detection, in addition to significant training speed-up and compute savings. Code will be released.\n",
        "pdf_link": "https://openreview.net/pdf/7d9f5b2672aa5a14a75728dc195e5033b0f308aa.pdf",
        "forum_url": "https://openreview.net/forum?id=MIMwy4kh9lf",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned in the abstract, but the method is proposed to simplify the training pipeline and improve performance, implying that existing methods may have limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned in the abstract, but the method is proposed to simplify the training pipeline and improve performance, implying that existing methods may have limitations."
    },
    {
        "title": "DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training",
        "authors": [
            "Wei Li",
            "Linchao Zhu",
            "Longyin Wen",
            "Yi Yang"
        ],
        "published": "iclr 2023 poster",
        "summary": "Large-scale pre-trained multi-modal models (e.g., CLIP) demonstrate strong zero-shot transfer capability in many discriminative tasks, e.g., image classification. Their adaptation to zero-shot image-conditioned text generation tasks has drawn increasing interest. Prior arts approach to zero-shot captioning by either utilizing the existing large language models (e.g., GPT-2) or pre-training the encoder-decoder network in an end-to-end manner. However, the large language models may not generate sensible descriptions due to the task discrepancy between captioning and language modeling, while the end-to-end pre-training requires paired data and extensive computational resources. In this work, we propose a simple framework, named DeCap, for zero-shot captioning. We introduce a lightweight visual-aware language decoder. This decoder is both data-efficient and computation-efficient: 1) it only requires the \\textit{text} data for training, easing the burden on the collection of paired data. 2) it does not require end-to-end training. When trained with text-only data, the decoder takes the text embedding extracted from the off-the-shelf CLIP encoder as a prefix embedding. The challenge is that the decoder is trained on the text corpus but at the inference stage, it needs to generate captions based on visual inputs. Though the CLIP text embedding and the visual embedding are correlated, the \\textit{modality gap} issue is widely observed in multi-modal contrastive models that prevents us from directly taking the visual embedding as the prefix embedding. We propose a training-free mechanism to reduce the modality gap. We project the visual embedding into the CLIP text embedding space, while the projected embedding retains the information of the visual input. Taking the projected embedding as the prefix embedding, the decoder generates high-quality descriptions that match the visual input. The experiments show that DeCap outperforms other zero-shot captioning methods and unpaired captioning methods by a large margin on the typical image captioning benchmarks, i.e., MSCOCO and NoCaps. We apply DeCap to video captioning and achieve state-of-the-art zero-shot performance on MSR-VTT and ActivityNet-Captions. The code is available at https://github.com/dhg-wei/DeCap.",
        "pdf_link": "https://openreview.net/pdf/20281fe81003b21131076887ded62556d1c2dc19.pdf",
        "forum_url": "https://openreview.net/forum?id=Lt8bMlhiwx2",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the large language models may not generate sensible descriptions due to the task discrepancy between captioning and language modeling, while the end-to-end pre-training requires paired data and extensive computational resources.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the large language models may not generate sensible descriptions due to the task discrepancy between captioning and language modeling, while the end-to-end pre-training requires paired data and extensive computational resources.\""
    },
    {
        "title": "Planning with Large Language Models for Code Generation",
        "authors": [
            "Shun Zhang",
            "Zhenfang Chen",
            "Yikang Shen",
            "Mingyu Ding",
            "Joshua B. Tenenbaum",
            "Chuang Gan"
        ],
        "published": "iclr 2023 poster",
        "summary": "Existing large language model-based code generation pipelines typically use beam search or sampling algorithms during the decoding process. Although the programs they generate achieve high token-matching-based scores, they often fail to compile or generate incorrect outputs. The main reason is that conventional Transformer decoding algorithms may not be the best choice for code generation. In this work, we propose a novel Transformer decoding algorithm, Planning-Guided Transformer Decoding (PG-TD), that uses a planning algorithm to do lookahead search and guide the Transformer to generate better programs. Specifically, instead of simply optimizing the likelihood of the generated sequences, the Transformer makes use of a planner that generates candidate programs and tests them on public test cases. The Transformer can therefore make more informed decisions and generate tokens that will eventually lead to higher-quality programs. We also design a mechanism that shares information between the Transformer and the planner to make our algorithm computationally efficient. We empirically evaluate our framework with several large language models as backbones on public coding challenge benchmarks, showing that 1) it can generate programs that consistently achieve higher performance compared with competing baseline methods; 2) it enables controllable code generation, such as concise codes and highly-commented codes by optimizing modified objective.",
        "pdf_link": "https://openreview.net/pdf/5f8b793197851829ddf2e08915b38f1549cb5b9d.pdf",
        "forum_url": "https://openreview.net/forum?id=Lr8cOOtYbfL",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing large language model-based code generation pipelines... often fail to compile or generate incorrect outputs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing large language model-based code generation pipelines... often fail to compile or generate incorrect outputs.\""
    },
    {
        "title": "Scaling Pareto-Efficient Decision Making via Offline Multi-Objective RL",
        "authors": [
            "Baiting Zhu",
            "Meihua Dang",
            "Aditya Grover"
        ],
        "published": "iclr 2023 poster",
        "summary": "The goal of multi-objective reinforcement learning (MORL) is to learn policies that simultaneously optimize multiple competing objectives. In practice, an agent's preferences over the objectives may not be known apriori, and hence, we require policies that can generalize to arbitrary preferences at test time. In this work, we propose a new data-driven setup for offline MORL, where we wish to learn a preference-agnostic policy agent using only a finite dataset of offline demonstrations of other agents and their preferences. The key contributions of this work are two-fold. First, we introduce D4MORL, (D)atasets for MORL that are specifically designed for offline settings. It contains 1.8 million annotated demonstrations obtained by rolling out reference policies that optimize for randomly sampled preferences on 6 MuJoCo environments with 2-3 objectives each. Second, we propose Pareto-Efficient Decision Agents (PEDA), a family of offline MORL algorithms that builds and extends Decision Transformers via a novel preference-and-return-conditioned policy. Empirically, we show that PEDA closely approximates the behavioral policy on the D4MORL benchmark and provides an excellent approximation of the Pareto-front with appropriate conditioning, as measured by the hypervolume and sparsity metrics. ",
        "pdf_link": "https://openreview.net/pdf/3d73c1e257eb3d1dd034f43fe3b51884a6dfade4.pdf",
        "forum_url": "https://openreview.net/forum?id=Ki4ocDm364",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "CANIFE: Crafting Canaries for Empirical Privacy Measurement in Federated Learning",
        "authors": [
            "Samuel Maddock",
            "Alexandre Sablayrolles",
            "Pierre Stock"
        ],
        "published": "iclr 2023 poster",
        "summary": "Federated Learning (FL) is a setting for training machine learning models in distributed environments where the clients do not share their raw data but instead send model updates to a server. However, model updates can be subject to attacks and leak private information. Differential Privacy (DP) is a leading mitigation strategy which involves adding noise to clipped model updates, trading off performance for strong theoretical privacy guarantees. Previous work has shown that the threat model of DP is conservative and that the obtained guarantees may be vacuous or may overestimate information leakage in practice. In this paper, we aim to achieve a tighter measurement of the model exposure by considering a realistic threat model. We propose a novel method, CANIFE, that uses canaries - carefully crafted samples by a strong adversary to evaluate the empirical privacy of a training round. We apply this attack to vision models trained on CIFAR-10 and CelebA and to language models trained on Sent140 and Shakespeare. In particular, in realistic FL scenarios, we demonstrate that the empirical per-round epsilon obtained with CANIFE is 4 -- 5$\\times$ lower than the theoretical bound.",
        "pdf_link": "https://openreview.net/pdf/6b1d947f2e3ef1d00d6505afa1d44a021202d5b6.pdf",
        "forum_url": "https://openreview.net/forum?id=Kf7Yyf4O0u",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Previous work has shown that the threat model of DP is conservative and that the obtained guarantees may be vacuous or may overestimate information leakage in practice.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Previous work has shown that the threat model of DP is conservative and that the obtained guarantees may be vacuous or may overestimate information leakage in practice.\""
    },
    {
        "title": "Human-level Atari 200x faster",
        "authors": [
            "Steven Kapturowski",
            "Víctor Campos",
            "Ray Jiang",
            "Nemanja Rakicevic",
            "Hado van Hasselt",
            "Charles Blundell",
            "Adria Puigdomenech Badia"
        ],
        "published": "iclr 2023 poster",
        "summary": "The task of building general agents that perform well over a wide range of tasks has been an important goal in reinforcement learning since its inception. The problem has been subject of research of a large body of work, with performance frequently measured by observing scores over the wide range of environments contained in the Atari 57 benchmark. Agent57 was the first agent to surpass the human benchmark on all 57 games, but this came at the cost of poor data-efficiency, requiring nearly 80 billion frames of experience to achieve. Taking Agent57 as a starting point, we employ a diverse set of strategies to achieve a 200-fold reduction of experience needed to outperform the human baseline, within our novel agent MEME. We investigate a range of instabilities and bottlenecks we encountered while reducing the data regime, and propose effective solutions to build a more robust and efficient agent. We also demonstrate competitive performance with high-performing methods such as Muesli and MuZero. Our contributions aim to achieve faster propagation of learning signals related to rare events, stabilize learning under differing value scales, improve the neural network architecture, and make updates more robust under a rapidly-changing policy.",
        "pdf_link": "https://openreview.net/pdf/b23bc123e103d66e46f6b7516e3fb6dffd1d2cba.pdf",
        "forum_url": "https://openreview.net/forum?id=JtC6yOHRoJJ",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Wasserstein Auto-encoded MDPs: Formal Verification of Efficiently Distilled RL Policies with Many-sided Guarantees",
        "authors": [
            "Florent Delgrange",
            "Ann Nowe",
            "Guillermo Perez"
        ],
        "published": "iclr 2023 poster",
        "summary": "Although deep reinforcement learning (DRL) has many success stories, the large-scale deployment of policies learned through these advanced techniques in safety-critical scenarios is hindered by their lack of formal guarantees. Variational Markov Decision Processes (VAE-MDPs) are discrete latent space models that provide a reliable framework for distilling formally verifiable controllers from any RL policy. While the related guarantees address relevant practical aspects such as the satisfaction of performance and safety properties, the VAE approach suffers from several learning flaws (posterior collapse, slow learning speed, poor dynamics estimates), primarily due to the absence of abstraction and representation guarantees to support latent optimization. We introduce the Wasserstein auto-encoded MDP (WAE-MDP), a latent space model that fixes those issues by minimizing a penalized form of the optimal transport between the behaviors of the agent executing the original policy and the distilled policy, for which the formal guarantees apply. Our approach yields bisimulation guarantees while learning the distilled policy, allowing concrete optimization of the abstraction and representation model quality. Our experiments show that, besides distilling policies up to 10 times faster, the latent model quality is indeed better in general. Moreover, we present experiments from a simple time-to-failure verification algorithm on the latent space. The fact that our approach enables such simple verification techniques highlights its applicability.",
        "pdf_link": "https://openreview.net/pdf/0f568427fb05b2660614b52c5f8dab551fc4d702.pdf",
        "forum_url": "https://openreview.net/forum?id=JLLTtEdh1ZY",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "A Simple Yet Powerful Deep Active Learning With Snapshots Ensembles",
        "authors": [
            "Seohyeon Jung",
            "Sanghyun Kim",
            "Juho Lee"
        ],
        "published": "iclr 2023 poster",
        "summary": "Given an unlabeled pool of data and the experts who can label them, active learning aims to build an agent that can effectively acquire data to be queried to the experts, maximizing the gain in performance when trained with them. While there are several principles for active learning, a prevailing approach is to estimate uncertainties of predictions for unlabeled samples and use them to define acquisition functions. Active learning with the uncertainty principle works well for deep learning, especially for large-scale image classification tasks with deep neural networks. Still, it is often overlooked how the uncertainty of predictions is estimated, despite the common findings on the difficulty of accurately estimating uncertainties of deep neural networks. In this paper, we highlight the effectiveness of snapshot ensembles for deep active learning. Compared to the previous approaches based on Monte-Carlo dropout or deep ensembles, we show that a simple acquisition strategy based on uncertainties estimated from parameter snapshots gathered from a single optimization path significantly improves the quality of the acquired samples. Based on this observation, we further propose an efficient active learning algorithm that maintains a single learning trajectory throughout the entire active learning episodes, unlike the existing algorithms training models from scratch for every active learning episode. Through the extensive empirical comparison, we demonstrate the effectiveness of snapshot ensembles for deep active learning.",
        "pdf_link": "https://openreview.net/pdf/4b4df3d988ecc07d73d78a6a4063cc0c3153a2aa.pdf",
        "forum_url": "https://openreview.net/forum?id=IVESH65r0Ar",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "SQA3D: Situated Question Answering in 3D Scenes",
        "authors": [
            "Xiaojian Ma",
            "Silong Yong",
            "Zilong Zheng",
            "Qing Li",
            "Yitao Liang",
            "Song-Chun Zhu",
            "Siyuan Huang"
        ],
        "published": "iclr 2023 poster",
        "summary": "We propose a new task to benchmark scene understanding of embodied agents: Situated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g., 3D scan), SQA3D requires the tested agent to first understand its situation (position, orientation, etc.) in the 3D scene as described by text, then reason about its surrounding environment and answer a question under that situation. Based upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k unique situations, along with 20.4k descriptions and 33.4k diverse reasoning questions for these situations. These questions examine a wide spectrum of reasoning capabilities for an intelligent agent, ranging from spatial relation comprehension to commonsense understanding, navigation, and multi-hop reasoning. SQA3D imposes a significant challenge to current multi-modal especially 3D reasoning models. We evaluate various state-of-the-art approaches and find that the best one only achieves an overall score of 47.20%, while amateur human participants can reach 90.06%. We believe SQA3D could facilitate future embodied AI research with stronger situation understanding and reasoning capability.",
        "pdf_link": "https://openreview.net/pdf/7c0e4e7d24c7883389f21b015135c6bfd4f50b62.pdf",
        "forum_url": "https://openreview.net/forum?id=IDJx97BC38",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"SQA3D imposes a significant challenge to current multi-modal especially 3D reasoning models. We evaluate various state-of-the-art approaches and find that the best one only achieves an overall score of 47.20%, while amateur human participants can reach 90.06%.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"SQA3D imposes a significant challenge to current multi-modal especially 3D reasoning models. We evaluate various state-of-the-art approaches and find that the best one only achieves an overall score of 47.20%, while amateur human participants can reach 90.06%.\""
    },
    {
        "title": "RPM: Generalizable Multi-Agent Policies for Multi-Agent Reinforcement Learning",
        "authors": [
            "Wei Qiu",
            "Xiao Ma",
            "Bo An",
            "Svetlana Obraztsova",
            "Shuicheng YAN",
            "Zhongwen Xu"
        ],
        "published": "iclr 2023 poster",
        "summary": "Despite the recent advancement in multi-agent reinforcement learning (MARL), the MARL agents easily overfit the training environment and perform poorly in evaluation scenarios where other agents behave differently. Obtaining generalizable policies for MARL agents is thus necessary but challenging mainly due to complex multi-agent interactions. In this work, we model the MARL problem with Markov Games and propose a simple yet effective method, called ranked policy memory (RPM), i.e., to maintain a look-up memory of policies to achieve good generalizability. The main idea of RPM is to train MARL policies via gathering massive multi-agent interaction data. In particular, we first rank each agent’s policies by its training episode return, i.e., the episode return of each agent in the training environment; we then save the ranked policies in the memory; when an episode starts, each agent can randomly select a policy from the RPM as the behavior policy. Each agent uses the behavior policy to gather multi-agent interaction data for MARL training. This innovative self-play framework guarantees the diversity of multi-agent interaction in the training data. Experimental results on Melting Pot demonstrate that RPM enables MARL agents to interact with unseen agents in multi-agent generalization evaluation scenarios and complete given tasks. It significantly boosts the performance up to 818% on average.",
        "pdf_link": "https://openreview.net/pdf/e0f67e22108de8d6be84c75330ee0de82ed3ae5a.pdf",
        "forum_url": "https://openreview.net/forum?id=HnSceSzlfrY",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs."
    },
    {
        "title": "Learning Rationalizable Equilibria in Multiplayer Games",
        "authors": [
            "Yuanhao Wang",
            "Dingwen Kong",
            "Yu Bai",
            "Chi Jin"
        ],
        "published": "iclr 2023 poster",
        "summary": "A natural goal in multi-agent learning is to learn \\emph{rationalizable} behavior, where players learn to avoid any Iteratively Dominated Action (IDA). However, standard no-regret based equilibria-finding algorithms could take exponential samples to find such rationalizable strategies. In this paper, we first propose a simple yet sample-efficient algorithm for finding a rationalizable action profile in multi-player general-sum games under bandit feedback, which substantially improves over the results of Wu et al. We further develop algorithms with the first efficient guarantees for learning rationalizable Coarse Correlated Equilibria (CCE) and Correlated Equilibria (CE). Our algorithms incorporate several novel techniques to guarantee the elimination of IDA and no (swap-)regret simultaneously, including a correlated exploration scheme and adaptive learning rates, which may be of independent interest. We complement our results with a sample complexity lower bound showing the sharpness of our guarantees.",
        "pdf_link": "https://openreview.net/pdf/d09ac36610fac5f61f64c416d3ac5e4982bb5622.pdf",
        "forum_url": "https://openreview.net/forum?id=HjOo2k8lhFl",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Write and Paint: Generative Vision-Language Models are Unified Modal Learners",
        "authors": [
            "Shizhe Diao",
            "Wangchunshu Zhou",
            "Xinsong Zhang",
            "Jiawei Wang"
        ],
        "published": "iclr 2023 poster",
        "summary": "Recent advances in vision-language pre-training have pushed the state-of-the-art on various vision-language tasks, making machines more capable of multi-modal writing (image-to-text generation) and painting (text-to-image generation). However, few studies investigate if these two essential capabilities can be learned together and boost each other, making a versatile and powerful multi-modal foundation model. In this work, we disclose the potential of symmetric generative vision-language pre-training in learning to write and paint concurrently, and propose a new unified modal model, named DaVinci, trained with prefix language modeling and prefix image modeling, a simple generative self-supervised objective on image-text pairs. Thanks to the proposed prefix multi-modal modeling framework, DaVinci is simple to train, scalable to huge data, adaptable to both writing and painting tasks, and also strong on other vision, text, and multi-modal understanding tasks. DaVinci achieves competitive performance on a wide range of 27 generation/understanding tasks and demonstrates the superiority of combining vision/language generative pre-training. Furthermore, we carefully benchmark the performance of different vision-language pre-training objectives on different scales of pre-training datasets on a heterogeneous and broad distribution coverage. Our results demonstrate the potential of exploiting self-supervision in both language and vision inputs, and establish new, stronger baselines for future comparisons at different data scales. The code and pre-trained models are available at https://github.com/shizhediao/DaVinci.",
        "pdf_link": "https://openreview.net/pdf/782da1a13b9ef3872a63284ecb7f4a21c757982c.pdf",
        "forum_url": "https://openreview.net/forum?id=HgQR0mXQ1_a",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitations of LLMs are mentioned in the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitations of LLMs are mentioned in the abstract."
    },
    {
        "title": "Perfectly Secure Steganography Using Minimum Entropy Coupling",
        "authors": [
            "Christian Schroeder de Witt",
            "Samuel Sokota",
            "J Zico Kolter",
            "Jakob Nicolaus Foerster",
            "Martin Strohmeier"
        ],
        "published": "iclr 2023 poster",
        "summary": "Steganography is the practice of encoding secret information into innocuous content in such a manner that an adversarial third party would not realize that there is hidden meaning. While this problem has classically been studied in security literature, recent advances in generative models have led to a shared interest among security and machine learning researchers in developing scalable steganography techniques. In this work, we show that a steganography procedure is perfectly secure under Cachin (1998)'s information theoretic-model of steganography if and only if it is induced by a coupling. Furthermore, we show that, among perfectly secure procedures, a procedure is maximally efficient if and only if it is induced by a minimum entropy coupling. These insights yield what are, to the best of our knowledge, the first steganography algorithms to achieve perfect security guarantees with non-trivial efficiency; additionally, these algorithms are highly scalable. To provide empirical validation, we compare a minimum entropy coupling-based approach to three modern baselines---arithmetic coding, Meteor, and adaptive dynamic grouping---using GPT-2, WaveRNN, and Image Transformer as communication channels. We find that the minimum entropy coupling-based approach achieves superior encoding efficiency, despite its stronger security constraints. In aggregate, these results suggest that it may be natural to view information-theoretic steganography through the lens of minimum entropy coupling.",
        "pdf_link": "https://openreview.net/pdf/352b361ac5a6926c6bbf58158ea34332012567a4.pdf",
        "forum_url": "https://openreview.net/forum?id=HQ67mj5rJdR",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"To provide empirical validation, we compare a minimum entropy coupling-based approach to three modern baselines---arithmetic coding, Meteor, and adaptive dynamic grouping---using GPT-2, WaveRNN, and Image Transformer as communication channels.\"\n\nNote: Although the paper mentions a specific LLM (GPT-2), it does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"To provide empirical validation, we compare a minimum entropy coupling-based approach to three modern baselines---arithmetic coding, Meteor, and adaptive dynamic grouping---using GPT-2, WaveRNN, and Image Transformer as communication channels.\"\n\nNote: Although the paper mentions a specific LLM (GPT-2), it does not discuss any limitations of LLMs."
    },
    {
        "title": "Teacher Guided Training: An Efficient Framework for Knowledge Transfer",
        "authors": [
            "Manzil Zaheer",
            "Ankit Singh Rawat",
            "Seungyeon Kim",
            "Chong You",
            "Himanshu Jain",
            "Andreas Veit",
            "Rob Fergus",
            "Sanjiv Kumar"
        ],
        "published": "iclr 2023 poster",
        "summary": "The remarkable performance gains realized by large pretrained models, e.g., GPT-3, hinge on the massive amounts of data they are exposed to during training. Analogously, distilling such large models to compact models for efficient deployment also necessitates a large amount of (labeled or unlabeled) training data. In this paper, we propose the teacher-guided training (TGT) framework for training a high-quality compact model that leverages the knowledge acquired by pretrained generative models, while obviating the need to go through a large volume of data. TGT exploits the fact that the teacher has acquired a good representation of the underlying data domain, which typically corresponds to a much lower dimensional manifold than the input space. Furthermore, we can use the teacher to explore input space more efficiently through sampling or gradient-based methods; thus, making TGT especially attractive for limited data or long-tail settings. We formally capture this benefit of proposed data-domain exploration in our generalization bounds. We find that TGT can improve accuracy on several image classification benchmarks as well as a range of text classification and retrieval tasks.",
        "pdf_link": "https://openreview.net/pdf/fc2352cc277d4c634bf33e196742780db863eab2.pdf",
        "forum_url": "https://openreview.net/forum?id=GVSf7Z7DbYL",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"distilling such large models to compact models for efficient deployment also necessitates a large amount of (labeled or unlabeled) training data.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"distilling such large models to compact models for efficient deployment also necessitates a large amount of (labeled or unlabeled) training data.\""
    },
    {
        "title": "Unified Detoxifying and Debiasing in Language Generation via Inference-time Adaptive Optimization",
        "authors": [
            "Zonghan Yang",
            "Xiaoyuan Yi",
            "Peng Li",
            "Yang Liu",
            "Xing Xie"
        ],
        "published": "iclr 2023 poster",
        "summary": "Recently pre-trained language models (PLMs) have prospered in various natural language generation (NLG) tasks due to their ability to generate fairly fluent text. Nevertheless, these models are observed to capture and reproduce harmful contents in training corpora, typically toxic language and social biases, raising severe moral issues. Prior works on ethical NLG tackle detoxifying and debiasing separately, which is problematic since we find debiased models still exhibit toxicity while detoxified ones even exacerbate biases. To address such a challenge, we propose the first unified framework of detoxifying and debiasing called UDDIA, which jointly formalizes these two problems as rectifying the output space. We theoretically interpret our framework as learning a text distribution mixing weighted attributes. Besides, UDDIA conducts adaptive optimization of only a few parameters during decoding based on a parameter-efficient tuning schema without any training data. This leads to minimal generation quality loss and improved rectification performance with acceptable computational cost. Experimental results demonstrate that compared to several strong baselines, UDDIA achieves debiasing and detoxifying simultaneously and better balances efficiency and effectiveness, taking a further step towards practical ethical NLG.",
        "pdf_link": "https://openreview.net/pdf/d5308cde03505a14781fa29264be9ecd9cba9bc4.pdf",
        "forum_url": "https://openreview.net/forum?id=FvevdI0aA_h",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"these models are observed to capture and reproduce harmful contents in training corpora, typically toxic language and social biases, raising severe moral issues.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"these models are observed to capture and reproduce harmful contents in training corpora, typically toxic language and social biases, raising severe moral issues.\""
    },
    {
        "title": "Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners",
        "authors": [
            "Seonghyeon Ye",
            "Doyoung Kim",
            "Joel Jang",
            "Joongbo Shin",
            "Minjoon Seo"
        ],
        "published": "iclr 2023 poster",
        "summary": "Meta-training, which fine-tunes the language model (LM) on various downstream tasks by maximizing the likelihood of the target label given the task instruction and input instance, has improved the zero-shot task generalization performance. However, meta-trained LMs still struggle to generalize to challenging tasks containing novel labels unseen during meta-training. In this paper, we propose Flipped Learning, an alternative method of meta-training which trains the LM to generate the task instruction given the input instance and label. During inference, the LM trained with Flipped Learning, referred to as FLIPPED, selects the label option that is most likely to generate the task instruction. On 14 tasks of the BIG-bench benchmark, the 11B-sized FLIPPED outperforms zero-shot T0-11B (Sanh et al, 2021) and even a 16 times larger 3-shot GPT-3 (175B) (Brown et al, 2020) on average by 8.4% and 9.7% points, respectively. FLIPPED gives particularly large improvements on tasks with unseen labels, outperforming T0-11B by up to +20% average F1 score. This indicates that the strong task generalization of FLIPPED comes from improved generalization to novel labels. We release our code at github.com/seonghyeonye/Flipped-Learning.",
        "pdf_link": "https://openreview.net/pdf/9fc92b9cc405ff3459eff07b3ef415b179428c75.pdf",
        "forum_url": "https://openreview.net/forum?id=FtOxgKe_Zg2",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, meta-trained LMs still struggle to generalize to challenging tasks containing novel labels unseen during meta-training.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, meta-trained LMs still struggle to generalize to challenging tasks containing novel labels unseen during meta-training.\""
    },
    {
        "title": "ERL-Re$^2$: Efficient Evolutionary Reinforcement Learning with Shared State Representation and Individual Policy Representation ",
        "authors": [
            "Jianye HAO",
            "Pengyi Li",
            "Hongyao Tang",
            "YAN ZHENG",
            "Xian Fu",
            "Zhaopeng Meng"
        ],
        "published": "iclr 2023 poster",
        "summary": "Deep Reinforcement Learning (Deep RL) and Evolutionary Algorithm (EA) are two major paradigms of policy optimization with distinct learning principles, i.e., gradient-based v.s. gradient-free. An appealing research direction is integrating Deep RL and EA to devise new methods by fusing their complementary advantages. However, existing works on combining Deep RL and EA have two common drawbacks:1) the RL agent and EA agents learn their policies individually, neglecting efficient sharing of useful common knowledge; 2) parameter-level policy optimization guarantees no semantic level of behavior evolution for the EA side. In this paper, we propose Evolutionary Reinforcement Learning with Two-scale State Representation and Policy Representation (ERL-Re$^2$), a novel solution to the aforementioned two drawbacks. The key idea of ERL-Re$^2$ is two-scale representation: all EA and RL policies share the same nonlinear state representation while maintaining individual linear policy representations. The state representation conveys expressive common features of the environment learned by all the agents collectively; the linear policy representation provides a favorable space for efficient policy optimization, where novel behavior-level crossover and mutation operations can be performed. Moreover, the linear policy representation allows convenient generalization of policy fitness with the help of Policy-extended Value Function Approximator (PeVFA), further improving the sample efficiency of fitness estimation. The experiments on a range of continuous control tasks show that ERL-Re$^2$ consistently outperforms advanced baselines and achieves the State Of The Art (SOTA). Our code is available on  https://github.com/yeshenpy/ERL-Re2.",
        "pdf_link": "https://openreview.net/pdf/9b3a060321395c18a721125540f51db282b2e6a6.pdf",
        "forum_url": "https://openreview.net/forum?id=FYZCHEtt6H0",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Dataless Knowledge Fusion by Merging Weights of Language Models",
        "authors": [
            "Xisen Jin",
            "Xiang Ren",
            "Daniel Preotiuc-Pietro",
            "Pengxiang Cheng"
        ],
        "published": "iclr 2023 poster",
        "summary": "Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a data-less knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-task learning that can preserve or sometimes improve over the individual models without access to the training data. Finally, model merging is more efficient than training a multi-task model, thus making it applicable to a wider set of scenarios.",
        "pdf_link": "https://openreview.net/pdf/21be6dbfbc3e4e121e97e4185dbd267199c7a4a4.pdf",
        "forum_url": "https://openreview.net/forum?id=FCnohuR6AnM",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns.\"\n\nThis evidence suggests that the paper mentions a limitation of LLMs related to data privacy and intellectual property concerns, but it is not the primary focus of the",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns.\"\n\nThis evidence suggests that the paper mentions a limitation of LLMs related to data privacy and intellectual property concerns, but it is not the primary focus of the"
    },
    {
        "title": "Discovering Latent Knowledge in Language Models Without Supervision",
        "authors": [
            "Collin Burns",
            "Haotian Ye",
            "Dan Klein",
            "Jacob Steinhardt"
        ],
        "published": "iclr 2023 poster",
        "summary": "Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4\\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels.",
        "pdf_link": "https://openreview.net/pdf/1a2f757c6a56314972af9e4b40e580d5691e0b55.pdf",
        "forum_url": "https://openreview.net/forum?id=ETKGuby0hcs",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect.\""
    },
    {
        "title": "Composing Task Knowledge With Modular Successor Feature Approximators",
        "authors": [
            "Wilka Torrico Carvalho",
            "Angelos Filos",
            "Richard Lewis",
            "Honglak Lee",
            "Satinder Singh"
        ],
        "published": "iclr 2023 poster",
        "summary": "Recently, the Successor Features and Generalized Policy Improvement (SF&GPI) framework has been proposed as a method for learning, composing and transferring predictive knowledge and behavior. SF&GPI works by having an agent learn predictive representations (SFs) that can be combined for transfer to new tasks with GPI. However, to be effective this approach requires state features that are useful to predict, and these state-features are typically hand-designed. In this work, we present a novel neural network architecture, “Modular Successor Feature Approximators” (MSFA), where modules both discover what is useful to predict, and learn their own predictive representations. We show that MSFA is able to better generalize compared to baseline architectures for learning SFs and a modular network that discovers factored state representations.\n\n",
        "pdf_link": "https://openreview.net/pdf/f6e1ffd51a2415a8b0d1b98c2fabdf7bc677dfeb.pdf",
        "forum_url": "https://openreview.net/forum?id=DrtSx1z40Ib",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning",
        "authors": [
            "Pan Lu",
            "Liang Qiu",
            "Kai-Wei Chang",
            "Ying Nian Wu",
            "Song-Chun Zhu",
            "Tanmay Rajpurohit",
            "Peter Clark",
            "Ashwin Kalyan"
        ],
        "published": "iclr 2023 poster",
        "summary": "Mathematical reasoning, a core ability of human intelligence, presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form, such as math word problems (MWP). However, it is unknown if the models can handle more complex problems that involve math reasoning over heterogeneous information, such as tabular data. To fill the gap, we present Tabular Math Word Problems (TabMWP), a new dataset containing 38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data. Each question in TabMWP is aligned with a tabular context, which is presented as an image, semi-structured text, and a structured table. There are two types of questions: free-text and multi-choice, and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP. To mitigate this, we further propose a novel approach, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by 5.31% on the accuracy metric and reduces the prediction variance significantly compared to random selection, which verifies its effectiveness in selecting in-context examples. The data and code are available at https://promptpg.github.io.",
        "pdf_link": "https://openreview.net/pdf/1f2f51f57875ec48e1bb27c936aa39ee2e65d06e.pdf",
        "forum_url": "https://openreview.net/forum?id=DHyHRBwJUTN",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP.\""
    },
    {
        "title": "HomoDistil: Homotopic Task-Agnostic Distillation of Pre-trained Transformers",
        "authors": [
            "Chen Liang",
            "Haoming Jiang",
            "Zheng Li",
            "Xianfeng Tang",
            "Bing Yin",
            "Tuo Zhao"
        ],
        "published": "iclr 2023 poster",
        "summary": "Knowledge distillation has been shown to be a powerful model compression approach to facilitate the deployment of pre-trained language models in practice. This paper focuses on task-agnostic distillation. It produces a compact pre-trained model that can be easily fine-tuned on various tasks with small computational costs and memory footprints. Despite the practical benefits, task-agnostic distillation is challenging. Since the teacher model has a significantly larger capacity and stronger representation power than the student model, it is very difficult for the student to produce predictions that match the teacher's over a massive amount of open-domain training data. Such a large prediction discrepancy often diminishes the benefits of knowledge distillation. To address this challenge, we propose Homotopic Distillation (HomoDistil), a novel task-agnostic distillation approach equipped with iterative pruning. Specifically, we initialize the student model from the teacher model, and iteratively prune the student's neurons until the target width is reached. Such an approach maintains a small discrepancy between the teacher's and student's predictions throughout the distillation process, which ensures the effectiveness of knowledge transfer. Extensive experiments demonstrate that HomoDistil achieves significant improvements on existing baselines. Our codes will be released.",
        "pdf_link": "https://openreview.net/pdf/c83a0d7736988f2fb7bb42f283697dade74b84f2.pdf",
        "forum_url": "https://openreview.net/forum?id=D7srTrGhAs",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the practical benefits, task-agnostic distillation is challenging. Since the teacher model has a significantly larger capacity and stronger representation power than the student model, it is very difficult for the student to produce predictions that match the teacher's over a massive amount of open-domain training data.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite the practical benefits, task-agnostic distillation is challenging. Since the teacher model has a significantly larger capacity and stronger representation power than the student model, it is very difficult for the student to produce predictions that match the teacher's over a massive amount of open-domain training data.\""
    },
    {
        "title": "Stay Moral and Explore: Learn to Behave Morally in Text-based Games",
        "authors": [
            "Zijing Shi",
            "Meng Fang",
            "Yunqiu Xu",
            "Ling Chen",
            "Yali Du"
        ],
        "published": "iclr 2023 poster",
        "summary": "Reinforcement learning (RL) in text-based games has developed rapidly and achieved promising results. However, little effort has been expended to design agents that pursue objectives while behaving morally, which is a critical issue in the field of autonomous agents. In this paper, we propose a general framework named Moral Awareness Adaptive Learning (MorAL) that enhances the morality capacity of an agent using a plugin moral-aware learning model. The framework allows the agent to execute task learning and morality learning adaptively. The agent selects trajectories from past experiences during task learning. Meanwhile, the trajectories are used to conduct self-imitation learning with a moral-enhanced objective. In order to achieve the trade-off between morality and task progress, the agent uses the combination of task policy and moral policy for action selection. We evaluate on the Jiminy Cricket benchmark, a set of text-based games with various scenes and dense morality annotations. Our experiments demonstrate that, compared with strong contemporary value alignment approaches,  the proposed framework improves task performance while reducing immoral behaviours in various games.",
        "pdf_link": "https://openreview.net/pdf/dad0b424172c8f0ef656ed8e3fc50bf5d323aaa9.pdf",
        "forum_url": "https://openreview.net/forum?id=CtS2Rs_aYk",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs."
    },
    {
        "title": "Proactive Multi-Camera Collaboration for 3D Human Pose Estimation",
        "authors": [
            "Hai Ci",
            "Mickel Liu",
            "Xuehai Pan",
            "fangwei zhong",
            "Yizhou Wang"
        ],
        "published": "iclr 2023 poster",
        "summary": "This paper presents a multi-agent reinforcement learning (MARL) scheme for proactive Multi-Camera Collaboration in 3D Human Pose Estimation in dynamic human crowds. Traditional fixed-viewpoint multi-camera solutions for human motion capture (MoCap) are limited in capture space and susceptible to dynamic occlusions. Active camera approaches proactively control camera poses to find optimal viewpoints for 3D reconstruction. However, current methods still face challenges with credit assignment and environment dynamics. To address these issues, our proposed method introduces a novel Collaborative Triangulation Contribution Reward (CTCR) that improves convergence and alleviates multi-agent credit assignment issues resulting from using 3D reconstruction accuracy as the shared reward. Additionally, we jointly train our model with multiple world dynamics learning tasks to better capture environment dynamics and encourage anticipatory behaviors for occlusion avoidance. We evaluate our proposed method in four photo-realistic UE4 environments to ensure validity and generalizability. Empirical results show that our method outperforms fixed and active baselines in various scenarios with different numbers of cameras and humans.",
        "pdf_link": "https://openreview.net/pdf/c0e6c42114afa223ad1ba354773bb35417716e80.pdf",
        "forum_url": "https://openreview.net/forum?id=CPIy9TWFYBG",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Leveraging Future Relationship Reasoning for Vehicle Trajectory Prediction",
        "authors": [
            "Daehee Park",
            "Hobin Ryu",
            "Yunseo Yang",
            "Jegyeong Cho",
            "Jiwon Kim",
            "Kuk-Jin Yoon"
        ],
        "published": "iclr 2023 poster",
        "summary": "Understanding the interaction between multiple agents is crucial for realistic vehicle trajectory prediction. \nExisting methods have attempted to infer the interaction from the observed past trajectories of agents using pooling, attention, or graph-based methods, which rely on a deterministic approach. \nHowever, these methods can fail under complex road structures, as they cannot predict various interactions that may occur in the future. \nIn this paper, we propose a novel approach that uses lane information to predict a stochastic future relationship among agents. \nTo obtain a coarse future motion of agents, our method first predicts the probability of lane-level waypoint occupancy of vehicles. \nWe then utilize the temporal probability of passing adjacent lanes for each agent pair, assuming that agents passing adjacent lanes will highly interact. \nWe also model the interaction using a probabilistic distribution, which allows for multiple possible future interactions. \nThe distribution is learned from the posterior distribution of interaction obtained from ground truth future trajectories. \nWe validate our method on popular trajectory prediction datasets: nuScenes and Argoverse. \nThe results show that the proposed method brings remarkable performance gain in prediction accuracy, and achieves state-of-the-art performance in long-term prediction benchmark dataset.",
        "pdf_link": "https://openreview.net/pdf/26afa69e0c70599af069b4e1fd67c5256d02890a.pdf",
        "forum_url": "https://openreview.net/forum?id=CGBCTp2M6lA",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "MACTA: A Multi-agent Reinforcement Learning Approach for Cache Timing Attacks and Detection",
        "authors": [
            "Jiaxun Cui",
            "Xiaomeng Yang",
            "Mulong Luo",
            "Geunbae Lee",
            "Peter Stone",
            "Hsien-Hsin S. Lee",
            "Benjamin Lee",
            "G. Edward Suh",
            "Wenjie Xiong",
            "Yuandong Tian"
        ],
        "published": "iclr 2023 poster",
        "summary": "Security vulnerabilities in computer systems raise serious concerns as computers process an unprecedented amount of private and sensitive data today. Cache timing attacks (CTA) pose an important practical threat as they can effectively breach many protection mechanisms in today’s systems. However, the current detection techniques for cache timing attacks heavily rely on heuristics and expert knowledge, which can lead to brittleness and the inability to adapt to new attacks. To mitigate the CTA threat, we propose MACTA, a multi-agent reinforcement learning (MARL) approach that leverages population-based training to train both attackers and detectors. Following best practices, we develop a realistic simulated MARL environment, MA-AUTOCAT, which enables training and evaluation of cache-timing attackers and detectors. Our empirical results suggest that MACTA is an effective solution without any manual input from security experts. MACTA detectors can generalize to a heuristic attack not exposed in training with a 97.8% detection rate and reduce the attack bandwidth of adaptive attackers by 20% on average. In the meantime, MACTA attackers are qualitatively more effective than other attacks studied, and the average evasion rate of MACTA attackers against an unseen state-of-the-art detector can reach up to 99%. Furthermore, we found that agents equipped with a Transformer encoder can learn effective policies in situations when agents with multi-layer perceptron encoders do not in this environment, suggesting the potential of Transformer structures in CTA problems.\n",
        "pdf_link": "https://openreview.net/pdf/8d7f2a48891a6f825223343bc81e59d3c5f6afbe.pdf",
        "forum_url": "https://openreview.net/forum?id=CDlHZ78-Xzi",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Computational Language Acquisition with Theory of Mind",
        "authors": [
            "Andy Liu",
            "Hao Zhu",
            "Emmy Liu",
            "Yonatan Bisk",
            "Graham Neubig"
        ],
        "published": "iclr 2023 poster",
        "summary": "Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack & Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.",
        "pdf_link": "https://openreview.net/pdf/b8215e9ec231405a7f97d58eb05eb515dbef7abe.pdf",
        "forum_url": "https://openreview.net/forum?id=C2ulri4duIs",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks",
        "authors": [
            "Noam Wies",
            "Yoav Levine",
            "Amnon Shashua"
        ],
        "published": "iclr 2023 poster",
        "summary": "The field of Natural Language Processing (NLP) has experienced a dramatic leap in capabilities with the recent introduction of huge Language Models (LMs). Despite this success, natural language problems that involve several compounded steps are still practically unlearnable, even by the largest LMs. This complies with experimental failures for end-to-end learning of composite problems that were demonstrated in a variety of domains. An effective mitigation is to introduce intermediate supervision for solving sub-tasks of the compounded problem. Recently, several works have demonstrated high gains by taking a straightforward approach for incorporating intermediate supervision in compounded natural language problems: the sequence-to-sequence LM is fed with an augmented input, in which the decomposed tasks' labels are simply concatenated to the original input. In this paper, we prove a positive learning result that motivates these recent efforts. We show that when concatenating intermediate supervision to the input and training a sequence-to-sequence model on this modified input, unlearnable composite problems can become learnable. We show that this is true for any family of tasks which on the one hand, are unlearnable, and on the other hand, can be decomposed into a polynomial number of simple sub-tasks, each of which depends only on $O(1)$ previous sub-task results. Beyond motivating contemporary empirical efforts for incorporating intermediate supervision in sequence-to-sequence language models, our positive theoretical result is the first of its kind in the landscape of results on the benefits of intermediate supervision for neural-network learning: Until now, all theoretical results on the subject are negative, i.e., show cases where learning is impossible without intermediate supervision, while our result is positive, showing that learning is facilitated in the presence of intermediate supervision.",
        "pdf_link": "https://openreview.net/pdf/0e2acc3ed9aaaff91e94533aa1eb2cec3a27915b.pdf",
        "forum_url": "https://openreview.net/forum?id=BrJATVZDWEH",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite this success, natural language problems that involve several compounded steps are still practically unlearnable, even by the largest LMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite this success, natural language problems that involve several compounded steps are still practically unlearnable, even by the largest LMs.\""
    },
    {
        "title": "Stateful Active Facilitator: Coordination and Environmental Heterogeneity in Cooperative Multi-Agent Reinforcement Learning",
        "authors": [
            "Dianbo Liu",
            "Vedant Shah",
            "Oussama Boussif",
            "Cristian Meo",
            "Anirudh Goyal",
            "Tianmin Shu",
            "Michael Curtis Mozer",
            "Nicolas Heess",
            "Yoshua Bengio"
        ],
        "published": "iclr 2023 poster",
        "summary": "In cooperative multi-agent reinforcement learning, a team of agents works together\nto achieve a common goal. Different environments or tasks may require varying\ndegrees of coordination among agents in order to achieve the goal in an optimal\nway. The nature of coordination will depend on properties of the environment—its\nspatial layout, distribution of obstacles, dynamics, etc. We term this variation\nof properties within an environment as heterogeneity. Existing literature has not\nsufficiently addressed the fact that different environments may have different levels\nof heterogeneity. We formalize the notions of coordination level and heterogeneity\nlevel of an environment and present HECOGrid, a suite of multi-agent RL\nenvironments that facilitates empirical evaluation of different MARL approaches\nacross different levels of coordination and environmental heterogeneity by providing\na quantitative control over coordination and heterogeneity levels of the\nenvironment. Further, we propose a Centralized Training Decentralized Execution\nlearning approach called Stateful Active Facilitator (SAF) that enables agents to\nwork efficiently in high-coordination and high-heterogeneity environments through\na differentiable and shared knowledge source used during training and dynamic\nselection from a shared pool of policies. We evaluate SAF and compare its performance\nagainst baselines IPPO and MAPPO on HECOGrid. Our results show\nthat SAF consistently outperforms the baselines across different tasks and different\nheterogeneity and coordination levels.",
        "pdf_link": "https://openreview.net/pdf/e91b82d1a670376c4dd37b3ff6ed712eff719b12.pdf",
        "forum_url": "https://openreview.net/forum?id=B4maZQLLW0_",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs or language models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs or language models."
    },
    {
        "title": "Panning for Gold in Federated Learning: Targeted Text Extraction under Arbitrarily Large-Scale Aggregation",
        "authors": [
            "Hong-Min Chu",
            "Jonas Geiping",
            "Liam H Fowl",
            "Micah Goldblum",
            "Tom Goldstein"
        ],
        "published": "iclr 2023 poster",
        "summary": "As federated learning (FL) matures, privacy attacks against FL systems in turn become more numerous and complex. Attacks on language models have progressed from recovering single sentences in simple classification tasks to recovering larger parts of user data. Current attacks against federated language models are sequence-agnostic and aim to extract as much data as possible from an FL update - often at the expense of fidelity for any particular sequence. Because of this, current attacks fail to extract any meaningful data under large-scale aggregation. In realistic settings, an attacker cares most about a small portion of user data that contains sensitive personal information, for example sequences containing the phrase \"my credit card number is ...\". In this work, we propose the first attack on FL that achieves targeted extraction of sequences that contain privacy-critical phrases, whereby we employ maliciously modified parameters to allow the transformer itself to filter relevant sequences from aggregated user data and encode them in the gradient update. Our attack can effectively extract sequences of interest even against extremely large-scale aggregation.",
        "pdf_link": "https://openreview.net/pdf/62d267bced5f7a24db1b239d5ee670b50f2776cd.pdf",
        "forum_url": "https://openreview.net/forum?id=A9WQaxYsfx",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Current attacks against federated language models are sequence-agnostic and aim to extract as much data as possible from an FL update - often at the expense of fidelity for any particular sequence.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Current attacks against federated language models are sequence-agnostic and aim to extract as much data as possible from an FL update - often at the expense of fidelity for any particular sequence.\""
    },
    {
        "title": "Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus",
        "authors": [
            "Gang Li",
            "Yang Li"
        ],
        "published": "iclr 2023 poster",
        "summary": "Mobile UI understanding is important for enabling various interaction tasks such as UI automation and accessibility. Previous mobile UI modeling often depends on the view hierarchy information of a screen, which directly provides the structural data of the UI, with the hope to bypass challenging tasks of visual modeling from screen pixels. However, view hierarchies are not always available, and are often corrupted with missing object descriptions or misaligned structure information. As a result, despite the use of view hierarchies could offer short-term gains, it may ultimately hinder the applicability and performance of the model. In this paper, we propose Spotlight, a vision-only approach for mobile UI understanding. Specifically, we enhance a vision-language model that only takes the screenshot of the UI and a region of interest on the screen---the focus---as the input. This general architecture of Spotlight is easily scalable and capable of performing a range of UI modeling tasks. Our experiments show that our model establishes SoTA results on several representative UI tasks and outperforms previous methods that use both screenshots and view hierarchies as inputs. Furthermore, we explore multi-task learning and few-shot prompting capacities of the proposed models, demonstrating promising results in the multi-task learning direction.",
        "pdf_link": "https://openreview.net/pdf/ad305a1a4c4b0a2571863546dc680f91c8b5b9f1.pdf",
        "forum_url": "https://openreview.net/forum?id=9yE2xEj0BH7",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, view hierarchies are not always available, and are often corrupted with missing object descriptions or misaligned structure information.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, view hierarchies are not always available, and are often corrupted with missing object descriptions or misaligned structure information.\""
    },
    {
        "title": "ImaginaryNet: Learning Object Detectors without Real Images and Annotations",
        "authors": [
            "Minheng Ni",
            "Zitong Huang",
            "Kailai Feng",
            "Wangmeng Zuo"
        ],
        "published": "iclr 2023 poster",
        "summary": "Without the demand of training in reality, humans are able of detecting a new category of object simply based on the language description on its visual characteristics. Empowering deep learning with this ability undoubtedly enables the neural network to handle complex vision tasks, e.g., object detection, without collecting and annotating real images. To this end, this paper introduces a novel challenging learning paradigm Imaginary-Supervised Object Detection (ISOD), where neither real images nor manual annotations are allowed for training object detectors. To resolve this challenge, we propose ImaginaryNet, a framework to synthesize images by combining pretrained language model and text-to-image synthesis model. Given a class label, the language model is used to generate a full description of a scene with a target object, and the text-to-image model is deployed to generate a photo-realistic image. With the synthesized images and class labels, weakly supervised object detection can then be leveraged to accomplish ISOD. By gradually introducing real images and manual annotations, ImaginaryNet can collaborate with other supervision settings to further boost detection performance. Experiments show that ImaginaryNet can (i) obtain about 75% performance in ISOD compared with the weakly supervised counterpart of the same backbone trained on real data, (ii) significantly improve the baseline while achieving state-of-the-art or comparable performance by incorporating ImaginaryNet with other supervision settings.  Our code will be publicly available at https://github.com/kodenii/ImaginaryNet.",
        "pdf_link": "https://openreview.net/pdf/871b8a661c3de03ebac23389dedd92af1f741b4c.pdf",
        "forum_url": "https://openreview.net/forum?id=9MbhFHqrti9",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Given a class label, the language model is used to generate a full description of a scene with a target object...\"\n\nThis paper discusses the use of a pre-trained language model to generate descriptions of scenes with target objects, but it does not mention any limitations of Large Language Models (LLMs).",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Given a class label, the language model is used to generate a full description of a scene with a target object...\"\n\nThis paper discusses the use of a pre-trained language model to generate descriptions of scenes with target objects, but it does not mention any limitations of Large Language Models (LLMs)."
    },
    {
        "title": "Prompting GPT-3 To Be Reliable",
        "authors": [
            "Chenglei Si",
            "Zhe Gan",
            "Zhengyuan Yang",
            "Shuohang Wang",
            "Jianfeng Wang",
            "Jordan Lee Boyd-Graber",
            "Lijuan Wang"
        ],
        "published": "iclr 2023 poster",
        "summary": "Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3’s reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM’s factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions. Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3.",
        "pdf_link": "https://openreview.net/pdf/1545ad3e1d44fe3f8431c30da415e1dd55352da5.pdf",
        "forum_url": "https://openreview.net/forum?id=98p5x51L5af",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored... We decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored... We decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality.\""
    },
    {
        "title": "Large Language Models are Human-Level Prompt Engineers",
        "authors": [
            "Yongchao Zhou",
            "Andrei Ioan Muresanu",
            "Ziwen Han",
            "Keiran Paster",
            "Silviu Pitis",
            "Harris Chan",
            "Jimmy Ba"
        ],
        "published": "iclr 2023 poster",
        "summary": "By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the \"program,\" optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 21/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts.",
        "pdf_link": "https://openreview.net/pdf/e7ec7c76cacea5d0272b93de5a569d872e7344e6.pdf",
        "forum_url": "https://openreview.net/forum?id=92gvk82DE-",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"task performance depends significantly on the quality of the prompt used to steer the model\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"task performance depends significantly on the quality of the prompt used to steer the model\""
    },
    {
        "title": "Linearly Mapping from Image to Text Space",
        "authors": [
            "Jack Merullo",
            "Louis Castricato",
            "Carsten Eickhoff",
            "Ellie Pavlick"
        ],
        "published": "iclr 2023 poster",
        "summary": "The extent to which text-only language models (LMs)  learn to represent the physical, non-linguistic world is an open question. Prior work has shown that pretrained LMs can be taught to ``understand'' visual inputs when the models' parameters are updated on image captioning tasks. We test a stronger hypothesis: that the conceptual representations learned by text-only models are functionally equivalent (up to a linear transformation) to those learned by models trained on vision tasks. Specifically, we show that the image representations from vision models can be transferred as continuous prompts to frozen LMs by training only a single linear projection. Using these to prompt the LM achieves competitive performance on captioning and visual question answering tasks compared to models that tune both the image encoder and text decoder (such as the MAGMA model). We compare three image encoders with increasing amounts of linguistic supervision seen during pretraining: BEIT (no linguistic information), NF-ResNET (lexical category information), and CLIP (full natural language descriptions). We find that all three encoders perform equally well at transferring visual property information to the language model (e.g., whether an animal is large or small), but that image encoders pretrained with linguistic supervision more saliently encode category information (e.g., distinguishing hippo vs.\\ elephant) and thus perform significantly better on benchmark language-and-vision tasks. Our results indicate that LMs encode conceptual information structurally similarly to vision-based models, even those that are solely trained on images.",
        "pdf_link": "https://openreview.net/pdf/bb73f5907bc91ecfb1c8ee44e7e84b62e3f33c49.pdf",
        "forum_url": "https://openreview.net/forum?id=8tYRqb05pVn",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The extent to which text-only language models (LMs) learn to represent the physical, non-linguistic world is an open question.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The extent to which text-only language models (LMs) learn to represent the physical, non-linguistic world is an open question.\""
    },
    {
        "title": "Agent-based Graph Neural Networks",
        "authors": [
            "Karolis Martinkus",
            "Pál András Papp",
            "Benedikt Schesch",
            "Roger Wattenhofer"
        ],
        "published": "iclr 2023 poster",
        "summary": "We present a novel graph neural network we call AgentNet, which is designed specifically for graph-level tasks. AgentNet is inspired by sublinear algorithms, featuring a computational complexity that is independent of the graph size. The architecture of AgentNet differs fundamentally from the architectures of traditional graph neural networks. In AgentNet, some trained \\textit{neural agents} intelligently walk the graph, and then collectively decide on the output. We provide an extensive theoretical analysis of AgentNet: We show that the agents can learn to systematically explore their neighborhood and that AgentNet can distinguish some structures that are even indistinguishable by 2-WL. Moreover, AgentNet is able to separate any two graphs which are sufficiently different in terms of subgraphs. We confirm these theoretical results with synthetic experiments on hard-to-distinguish graphs and real-world graph classification tasks. In both cases, we compare favorably not only to standard GNNs but also to computationally more expensive GNN extensions.",
        "pdf_link": "https://openreview.net/pdf/a33a54dc51ae12d26281cd196933bb3be33a76f3.pdf",
        "forum_url": "https://openreview.net/forum?id=8WTAh0tj2jC",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Visually-Augmented Language Modeling",
        "authors": [
            "Weizhi Wang",
            "Li Dong",
            "Hao Cheng",
            "Haoyu Song",
            "Xiaodong Liu",
            "Xifeng Yan",
            "Jianfeng Gao",
            "Furu Wei"
        ],
        "published": "iclr 2023 poster",
        "summary": "Human language is grounded on multimodal knowledge including visual knowledge like colors, sizes, and shapes. However, current large-scale pre-trained language models rely on the text-only self-supervised training with massive text data, which precludes them from utilizing relevant visual information when necessary. To address this, we propose a novel pre-training framework, named VaLM, to Visually-augment text tokens with retrieved relevant images for Language Modeling. Specifically, VaLM builds on a novel latent text-image alignment method via an image retrieval module to fetch corresponding images given a textual context. With the visually-augmented context, VaLM uses a visual knowledge fusion layer to enable multimodal grounded language modeling by attending on both text context and visual knowledge in images. We evaluate VaLM on various visual knowledge intensive commonsense reasoning tasks, which require visual information to excel. The experimental results illustrate that VaLM outperforms all strong language-only and vision-language baselines with substantial gains on reasoning object commonsense including color, size, and shape.",
        "pdf_link": "https://openreview.net/pdf/c73c81bf4faecceb125dd37e5452d0ba0431a662.pdf",
        "forum_url": "https://openreview.net/forum?id=8IN-qLkl215",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, current large-scale pre-trained language models rely on the text-only self-supervised training with massive text data, which precludes them from utilizing relevant visual information when necessary.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, current large-scale pre-trained language models rely on the text-only self-supervised training with massive text data, which precludes them from utilizing relevant visual information when necessary.\""
    },
    {
        "title": "Representation Learning for Low-rank General-sum Markov Games",
        "authors": [
            "Chengzhuo Ni",
            "Yuda Song",
            "Xuezhou Zhang",
            "Zihan Ding",
            "Chi Jin",
            "Mengdi Wang"
        ],
        "published": "iclr 2023 poster",
        "summary": "We study multi-agent general-sum Markov games with nonlinear function approximation. We focus on low-rank Markov games whose transition matrix admits a hidden low-rank structure on top of an unknown non-linear representation. The goal is to design an algorithm that (1) finds an $\\varepsilon$-equilibrium policy sample efficiently without prior knowledge of the environment or the representation, and (2) permits a deep-learning friendly implementation. We leverage representation learning and present a model-based and a model-free approach to construct an effective representation from collected data. For both approaches, the algorithm achieves a sample complexity of poly$(H,d,A,1/\\varepsilon)$, where $H$ is the game horizon, $d$ is the dimension of the feature vector, $A$ is the size of the joint action space and $\\varepsilon$ is the optimality gap. When the number of players is large, the above sample complexity can scale exponentially with the number of players in the worst case. To address this challenge, we consider Markov Games with a factorized transition structure and present an algorithm that escapes such exponential scaling. To our best knowledge, this is the first sample-efficient algorithm for multi-agent general-sum Markov games that incorporates (non-linear) function approximation. We accompany our theoretical result with a neural network-based implementation of our algorithm and evaluate it against the widely used deep RL baseline, DQN with fictitious play.",
        "pdf_link": "https://openreview.net/pdf/e4a6700bd0867d75ca3bdb47aa252ba2425e580c.pdf",
        "forum_url": "https://openreview.net/forum?id=8FroynZv4C",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Measuring Forgetting of Memorized Training Examples",
        "authors": [
            "Matthew Jagielski",
            "Om Thakkar",
            "Florian Tramer",
            "Daphne Ippolito",
            "Katherine Lee",
            "Nicholas Carlini",
            "Eric Wallace",
            "Shuang Song",
            "Abhradeep Guha Thakurta",
            "Nicolas Papernot",
            "Chiyuan Zhang"
        ],
        "published": "iclr 2023 poster",
        "summary": "Machine learning models exhibit two seemingly contradictory phenomena: training data memorization and various forms of forgetting. In memorization, models overfit specific training examples and become susceptible to privacy attacks. In forgetting, examples which appeared early in training are forgotten by the end. In this work, we connect these phenomena.\nWe propose a technique to measure to what extent models ``forget'' the specifics of training examples, becoming less susceptible to privacy attacks on examples they have not seen recently.\nWe show that, while non-convexity can prevent forgetting from happening in the worst-case, standard image,speech, and language models empirically do forget examples over time.\nWe identify nondeterminism as a potential explanation, showing that deterministically trained models do not forget.\nOur results suggest that examples seen early when training with extremely large datasets---for instance those examples used to pre-train a model---may observe privacy benefits at the expense of examples seen later.",
        "pdf_link": "https://openreview.net/pdf/b47ef8e9b1dd21b0fae0c4bde47a71e27324e333.pdf",
        "forum_url": "https://openreview.net/forum?id=7bJizxLKrR",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "UL2: Unifying Language Learning Paradigms",
        "authors": [
            "Yi Tay",
            "Mostafa Dehghani",
            "Vinh Q. Tran",
            "Xavier Garcia",
            "Jason Wei",
            "Xuezhi Wang",
            "Hyung Won Chung",
            "Dara Bahri",
            "Tal Schuster",
            "Steven Zheng",
            "Denny Zhou",
            "Neil Houlsby",
            "Donald Metzler"
        ],
        "published": "iclr 2023 poster",
        "summary": "Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. Finally, we show that UL2 20B works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. We release Flax-based T5X model checkpoints for the 20B model publicly.\n",
        "pdf_link": "https://openreview.net/pdf/8ac86c3590d8d02420c1aec52a1ee763b2f5166d.pdf",
        "forum_url": "https://openreview.net/forum?id=6ruVLB727MC",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation of LLMs is mentioned, but the paper aims to improve upon existing pre-trained models by proposing a unified framework, implying that current models may not be universally effective.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation of LLMs is mentioned, but the paper aims to improve upon existing pre-trained models by proposing a unified framework, implying that current models may not be universally effective."
    },
    {
        "title": "Thalamus: a brain-inspired algorithm for biologically-plausible continual learning and disentangled representations",
        "authors": [
            "Ali Hummos"
        ],
        "published": "iclr 2023 poster",
        "summary": "Animals thrive in a constantly changing environment and leverage the temporal structure to learn well-factorized causal representations. In contrast, traditional neural networks suffer from forgetting in changing environments and many methods have been proposed to limit forgetting with different trade-offs. Inspired by the brain thalamocortical circuit, we introduce a simple algorithm that uses optimization at inference time to generate internal representations of the current task dynamically. The algorithm alternates between updating the model weights and a latent task embedding, allowing the agent to parse the stream of temporal experience into discrete events and organize learning about them. On a continual learning benchmark, it achieves competitive end average accuracy by mitigating forgetting, but importantly, the interaction between the weights dynamics and the latent dynamics organizes knowledge into flexible structures with a cognitive interface to control them. Tasks later in the sequence can be solved through knowledge transfer as they become reachable within the well-factorized latent space. The algorithm meets many of the desiderata of an ideal continually learning agent in open-ended environments, and its simplicity suggests fundamental computations in circuits with abundant feedback control loops such as the thalamocortical circuits in the brain",
        "pdf_link": "https://openreview.net/pdf/ee4dce013cdb125a610759d64c058b9dd35caa6e.pdf",
        "forum_url": "https://openreview.net/forum?id=6orC5MvgPBK",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Imitating Graph-Based Planning with Goal-Conditioned Policies",
        "authors": [
            "Junsu Kim",
            "Younggyo Seo",
            "Sungsoo Ahn",
            "Kyunghwan Son",
            "Jinwoo Shin"
        ],
        "published": "iclr 2023 poster",
        "summary": "Recently, graph-based planning algorithms have gained much attention to solve goal-conditioned reinforcement learning (RL) tasks: they provide a sequence of subgoals to reach the target-goal, and the agents learn to execute subgoal-conditioned policies. However, the sample-efficiency of such RL schemes still remains a challenge, particularly for long-horizon tasks.  To address this issue, we present a simple yet effective self-imitation scheme which distills a subgoal-conditioned policy into the target-goal-conditioned policy. Our intuition here is that to reach a target-goal, an agent should pass through a subgoal, so target-goal- and subgoal- conditioned policies should be similar to each other. We also propose a novel scheme of stochastically skipping executed subgoals in a planned path, which further improves performance. Unlike prior methods that only utilize graph-based planning in an execution phase, our method transfers knowledge from a planner along with a graph into policy learning. We empirically show that our method can significantly boost the sample-efficiency of the existing goal-conditioned RL methods under various long-horizon control tasks.",
        "pdf_link": "https://openreview.net/pdf/fa60437d007be2312f49bfe0e1aebfab94534575.pdf",
        "forum_url": "https://openreview.net/forum?id=6lUEy1J5R7p",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Contrastive Meta-Learning for Partially Observable Few-Shot Learning",
        "authors": [
            "Adam Jelley",
            "Amos Storkey",
            "Antreas Antoniou",
            "Sam Devlin"
        ],
        "published": "iclr 2023 poster",
        "summary": "Many contrastive and meta-learning approaches learn representations by identifying common features in multiple views. However, the formalism for these approaches generally assumes features to be shared across views to be captured coherently. We consider the problem of learning a unified representation from partial observations, where useful features may be present in only some of the views. We approach this through a probabilistic formalism enabling views to map to representations with different levels of uncertainty in different components; these views can then be integrated with one another through marginalisation over that uncertainty. Our approach, Partial Observation Experts Modelling (POEM), then enables us to meta-learn consistent representations from partial observations. We evaluate our approach on an adaptation of a comprehensive few-shot learning benchmark, Meta-Dataset, and demonstrate the benefits of POEM over other meta-learning methods at representation learning from partial observations. We further demonstrate the utility of POEM by meta-learning to represent an environment from partial views observed by an agent exploring the environment.",
        "pdf_link": "https://openreview.net/pdf/e772b9003ad73dfd1a36126e495b1a62b2495389.pdf",
        "forum_url": "https://openreview.net/forum?id=6iVJOtr2zL2",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Compositional Task Representations for Large Language Models",
        "authors": [
            "NAN SHAO",
            "Zefan Cai",
            "Hanwei xu",
            "Chonghua Liao",
            "Yanan Zheng",
            "Zhilin Yang"
        ],
        "published": "iclr 2023 poster",
        "summary": "Large language models have shown a remarkable cross-task generalization ability. Most prior work assumed that prompts effectively extract knowledge from language models to facilitate generalization to new tasks. This perspective led to numerous studies on improving prompts. In contrast, we introduce a new perspective, compositional generalization, that views each task as a composition of latent codes and generalizes to test tasks by a new composition of seen codes. To this end, we propose a novel prompt-free approach, Compositional Task Representations (CTR), that employs multi-task training to learn a discrete, compositional codebook. Empirically, our CTR substantially outperforms prompt-based methods in zero-label learning on average. According to our analysis, some of the learned CTR codes are interpretable to human and demonstrate a certain degree of controllability.\n",
        "pdf_link": "https://openreview.net/pdf/ef7361f4ac0604d204d4c3f22d833e3e5d4c3163.pdf",
        "forum_url": "https://openreview.net/forum?id=6axIMJA7ME3",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Most prior work assumed that prompts effectively extract knowledge from language models to facilitate generalization to new tasks.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of prior work on LLMs, but it is not the primary focus of the paper. The limitation is mentioned briefly and is used to justify the proposed approach.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Most prior work assumed that prompts effectively extract knowledge from language models to facilitate generalization to new tasks.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of prior work on LLMs, but it is not the primary focus of the paper. The limitation is mentioned briefly and is used to justify the proposed approach."
    },
    {
        "title": "TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization",
        "authors": [
            "Bairu Hou",
            "Jinghan Jia",
            "Yihua Zhang",
            "Guanhua Zhang",
            "Yang Zhang",
            "Sijia Liu",
            "Shiyu Chang"
        ],
        "published": "iclr 2023 poster",
        "summary": "Robustness evaluation against adversarial examples has become increasingly important to unveil the trustworthiness of the prevailing deep models in natural language processing (NLP). However, in contrast to the computer vision domain where the first-order projected gradient descent (PGD) is used as the benchmark approach to generate adversarial examples for robustness evaluation, there lacks a principled first-order gradient-based robustness evaluation framework in NLP. The emerging optimization challenges lie in 1) the discrete nature of textual inputs together with the strong coupling between the perturbation location and the actual content, and 2) the additional constraint that the perturbed text should be fluent and achieve a low perplexity under a language model. These challenges make the development of PGD-like NLP attacks difficult. To bridge the gap, we propose TextGrad, a new attack generator using gradient-driven optimization, supporting high-accuracy and high-quality assessment of adversarial robustness in NLP. Specifically, we address the aforementioned challenges in a unified optimization framework. And we develop an effective convex relaxation method to co-optimize the continuously-relaxed site selection and perturbation variables and leverage an effective sampling method to establish an accurate mapping from the continuous optimization variables to the discrete textual perturbations. Moreover, as a first-order attack generation method, TextGrad can be baked into adversarial training to further improve the robustness of NLP models. Extensive experiments are provided to demonstrate the effectiveness of TextGrad not only in attack generation for robustness evaluation but also in adversarial defense. From the attack perspective, we show that TextGrad achieves remarkable improvements in both the attack success rate and the perplexity score over five state-of-the-art baselines. From the defense perspective, TextGrad-enabled adversarial training yields the most robust NLP model against a wide spectrum of NLP attacks. ",
        "pdf_link": "https://openreview.net/pdf/58c155b8082b3366c06a92d24d638d198650fb53.pdf",
        "forum_url": "https://openreview.net/forum?id=5tKXUZil3X",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "D4AM: A General Denoising Framework for Downstream Acoustic Models",
        "authors": [
            "Chi-Chang Lee",
            "Yu Tsao",
            "Hsin-Min Wang",
            "Chu-Song Chen"
        ],
        "published": "iclr 2023 poster",
        "summary": "The performance of acoustic models degrades notably in noisy environments. Speech enhancement (SE) can be used as a front-end strategy to aid automatic speech recognition (ASR) systems. However, existing training objectives of SE methods are not fully effective at integrating speech-text and noise-clean paired data for training toward unseen ASR systems. In this study, we propose a general denoising framework, D4AM, for various downstream acoustic models. Our framework fine-tunes the SE model with the backward gradient according to a specific acoustic model and the corresponding classification objective. In addition, our method aims to consider the regression objective as an auxiliary loss to make the SE model generalize to other unseen acoustic models. To jointly train an SE unit with regression and classification objectives, D4AM uses an adjustment scheme to directly estimate suitable weighting coefficients rather than undergoing a grid search process with additional training costs. The adjustment scheme consists of two parts: gradient calibration and regression objective weighting. The experimental results show that D4AM can consistently and effectively provide improvements to various unseen acoustic models and outperforms other combination setups. Specifically, when evaluated on the Google ASR API with real noisy data completely unseen during SE training, D4AM achieves a relative WER reduction of 24.65% compared with the direct feeding of noisy input. To our knowledge, this is the first work that deploys an effective combination scheme of regression (denoising) and classification (ASR) objectives to derive a general pre-processor applicable to various unseen ASR systems. Our code is available at https://github.com/ChangLee0903/D4AM.",
        "pdf_link": "https://openreview.net/pdf/520bdf66a9155c04855905babaead2b9aeba0b5d.pdf",
        "forum_url": "https://openreview.net/forum?id=5fvXH49wk2",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "CLARE: Conservative Model-Based Reward Learning for Offline Inverse Reinforcement Learning",
        "authors": [
            "Sheng Yue",
            "Guanbo Wang",
            "Wei Shao",
            "Zhaofeng Zhang",
            "Sen Lin",
            "Ju Ren",
            "Junshan Zhang"
        ],
        "published": "iclr 2023 poster",
        "summary": "This work aims to tackle a major challenge in offline Inverse Reinforcement Learning (IRL), namely the reward extrapolation error, where the learned reward function may fail to explain the task correctly and misguide the agent in unseen environments due to the intrinsic covariate shift. Leveraging both expert data and lower-quality diverse data, we devise a principled algorithm (namely CLARE) that solves offline IRL efficiently via integrating \"conservatism\" into a learned reward function and utilizing an estimated dynamics model. Our theoretical analysis provides an upper bound on the return gap between the learned policy and the expert policy, based on which we characterize the impact of covariate shift by examining subtle two-tier tradeoffs between the exploitation (on both expert and diverse data) and exploration (on the estimated dynamics model). We show that CLARE can provably alleviate the reward extrapolation error by striking the right exploitation-exploration balance therein. Extensive experiments corroborate the significant performance gains of CLARE over existing state-of-the-art algorithms on MuJoCo continuous control tasks (especially with a small offline dataset), and the learned reward is highly instructive for further learning. ",
        "pdf_link": "https://openreview.net/pdf/ab0104788311808f8c526bb4a5471ed9eb68e476.pdf",
        "forum_url": "https://openreview.net/forum?id=5aT4ganOd98",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Automatic Chain of Thought Prompting in Large Language Models",
        "authors": [
            "Zhuosheng Zhang",
            "Aston Zhang",
            "Mu Li",
            "Alex Smola"
        ],
        "published": "iclr 2023 poster",
        "summary": "Large Language Models (LLMs) can carry out complex reasoning tasks by generating intermediate reasoning steps. These steps are triggered by what is called chain-of-thought (CoT) prompting, which comes in two flavors: one leverages a simple prompt like \"Let’s think step by step\" to facilitate step-by-step reasoning before answering a question (Zero-Shot-CoT). The other uses manual demonstrations, each composed of a question and a reasoning chain that leads to an answer (Manual-CoT). Unfortunately, the superior performance of the latter strategy crucially hinges on manually generating task-specific demonstrations. This makes it far less scalable and more dependent on the talent of the CoT engineer. We show that such manual efforts may be eliminated by leveraging LLMs to generate the reasoning chains on its own. Since these generated chains often come with mistakes we propose a number of mitigation strategies. Our proposed Auto-CoT method automaticaly samples diverse questions and we perform post-processing quality control to generate usable reasoning chains from Zero-Shot-CoT. On ten public benchmark reasoning tasks, Auto-CoT performs on par with Manual-CoT without the need for human intervention. Code is available at https://github.com/amazon-research/auto-cot.\n",
        "pdf_link": "https://openreview.net/pdf/f566e3690bb4730c79fee6cd769ca1c7f2dc2bc6.pdf",
        "forum_url": "https://openreview.net/forum?id=5NTt8GFjUHkr",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Unfortunately, the superior performance of the latter strategy crucially hinges on manually generating task-specific demonstrations. This makes it far less scalable and more dependent on the talent of the CoT engineer.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Unfortunately, the superior performance of the latter strategy crucially hinges on manually generating task-specific demonstrations. This makes it far less scalable and more dependent on the talent of the CoT engineer.\""
    },
    {
        "title": "Explaining RL Decisions with Trajectories",
        "authors": [
            "Shripad Vilasrao Deshmukh",
            "Arpan Dasgupta",
            "Balaji Krishnamurthy",
            "Nan Jiang",
            "Chirag Agarwal",
            "Georgios Theocharous",
            "Jayakumar Subramanian"
        ],
        "published": "iclr 2023 poster",
        "summary": "Explanation is a key component for the adoption of reinforcement learning (RL) in many real-world decision-making problems. In the literature,  the explanation is often provided by saliency attribution to the features of the RL agent's state. In this work, we propose a complementary approach to these explanations, particularly for offline RL, where we attribute the policy decisions of a trained RL agent to the trajectories encountered by it during training. To do so, we encode trajectories in offline training data individually as well as collectively (encoding a set of trajectories). We then attribute policy decisions to a set of trajectories in this encoded space by estimating the sensitivity of the decision with respect to that set.  Further, we demonstrate the effectiveness of the proposed approach in terms of quality of attributions as well as practical scalability in diverse environments that involve both discrete and continuous state and action spaces such as grid-worlds, video games (Atari) and continuous control (MuJoCo). We also conduct a human study on a simple navigation task to observe how their understanding of the task compares with data attributed for a trained RL policy.",
        "pdf_link": "https://openreview.net/pdf/8c14263279e4c45dd0a74d7e52a0c6d707338882.pdf",
        "forum_url": "https://openreview.net/forum?id=5Egggz1q575",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Discovering Generalizable Multi-agent Coordination Skills from Multi-task Offline Data",
        "authors": [
            "Fuxiang Zhang",
            "Chengxing Jia",
            "Yi-Chen Li",
            "Lei Yuan",
            "Yang Yu",
            "Zongzhang Zhang"
        ],
        "published": "iclr 2023 poster",
        "summary": "Cooperative multi-agent reinforcement learning (MARL) faces the challenge of adapting to multiple tasks with varying agents and targets. Previous multi-task MARL approaches require costly interactions to simultaneously learn or fine-tune policies in different tasks. However, the situation that an agent should generalize to multiple tasks with only offline data from limited tasks is more in line with the needs of real-world applications. Since offline multi-task data contains a variety of behaviors, an effective data-driven approach is to extract informative latent variables that can represent universal skills for realizing coordination across tasks. In this paper, we propose a novel Offline MARL algorithm to Discover coordInation Skills (ODIS) from multi-task data. ODIS first extracts task-invariant coordination skills from offline multi-task data and learns to delineate different agent behaviors with the discovered coordination skills. Then we train a coordination policy to choose optimal coordination skills with the centralized training and decentralized execution paradigm. We further demonstrate that the discovered coordination skills can assign effective coordinative behaviors, thus significantly enhancing generalization to unseen tasks. Empirical results in cooperative MARL benchmarks, including the StarCraft multi-agent challenge, show that ODIS obtains superior performance in a wide range of tasks only with offline data from limited sources.",
        "pdf_link": "https://openreview.net/pdf/d365ffe4e9b099c3b0b62134ead3eaeba4105768.pdf",
        "forum_url": "https://openreview.net/forum?id=53FyUAdP7d",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Mind's Eye: Grounded Language Model Reasoning through Simulation",
        "authors": [
            "Ruibo Liu",
            "Jason Wei",
            "Shixiang Shane Gu",
            "Te-Yen Wu",
            "Soroush Vosoughi",
            "Claire Cui",
            "Denny Zhou",
            "Andrew M. Dai"
        ],
        "published": "iclr 2023 poster",
        "summary": "Successful and effective communication between humans and AI relies on a shared experience of the world. By training solely on written text, current language models (LMs) miss the grounded experience of humans in the real-world---their failure to relate language to the physical world causes knowledge to be misrepresented and obvious mistakes in their reasoning. We present Mind's Eye, a paradigm to ground language model reasoning in the physical world. Given a physical reasoning question, we use a computational physics engine (DeepMind's MuJoCo) to simulate the possible outcomes, and then use the simulation results as part of the input, which enables language models to perform reasoning. Experiments on 39 tasks in a physics alignment benchmark demonstrate that Mind's Eye can improve reasoning ability by a large margin (27.9% zero-shot, and 46.0% few-shot absolute accuracy improvement on average). Smaller language models armed with Mind's Eye can obtain similar performance to models that are 100x larger. Finally, we confirm the robustness of Mind's Eye through ablation studies.",
        "pdf_link": "https://openreview.net/pdf/0a6dcae7aef4fb3b11746b7175c70cfa12d4c3a3.pdf",
        "forum_url": "https://openreview.net/forum?id=4rXMRuoJlai",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"their failure to relate language to the physical world causes knowledge to be misrepresented and obvious mistakes in their reasoning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"their failure to relate language to the physical world causes knowledge to be misrepresented and obvious mistakes in their reasoning.\""
    },
    {
        "title": "Transformer-Patcher: One Mistake Worth One Neuron",
        "authors": [
            "Zeyu Huang",
            "Yikang Shen",
            "Xiaofeng Zhang",
            "Jie Zhou",
            "Wenge Rong",
            "Zhang Xiong"
        ],
        "published": "iclr 2023 poster",
        "summary": "Large Transformer-based Pretrained Language Models (PLMs) dominate almost all Natural Language Processing (NLP) tasks. Nevertheless, they still make mistakes from time to time. For a model deployed in an industrial environment, fixing these mistakes quickly and robustly is vital to improve user experiences. Previous works formalize such problems as Model Editing (ME) and mostly focus on fixing one mistake. However, the one-mistake-fixing scenario is not an accurate abstraction of the real-world challenge. In the deployment of AI services, there are ever-emerging mistakes, and the same mistake may recur if not corrected in time. Thus a preferable solution is to rectify the mistakes as soon as they appear nonstop. Therefore, we extend the existing ME into the Sequential Model Editing (SME) to help develop more practical editing methods. Our study shows that current ME methods either fail to make a sequence of edits or to remember previous edits. We then introduce Transformer-Patcher, a novel model editor that can shift the behavior of transformer-based models by simply adding and training a few neurons in the last Feed-Forward Network layer. Experimental results on both classification and generation tasks show that Transformer-Patcher can successively correct up to thousands of errors (Reliability) and generalize to their equivalent inputs (Generality) while retaining the model’s accuracy on irrelevant inputs (Locality). Our method outperforms previous fine-tuning and HyperNetwork-based methods and achieves state-of-the-art performance for Sequential Model Editing (SME).",
        "pdf_link": "https://openreview.net/pdf/333a3ae8305ed6fdef22d29e567970ee0060978d.pdf",
        "forum_url": "https://openreview.net/forum?id=4oYUGeGBPm",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large Transformer-based Pretrained Language Models (PLMs) dominate almost all Natural Language Processing (NLP) tasks. Nevertheless, they still make mistakes from time to time.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large Transformer-based Pretrained Language Models (PLMs) dominate almost all Natural Language Processing (NLP) tasks. Nevertheless, they still make mistakes from time to time.\""
    },
    {
        "title": "TypeT5: Seq2seq Type Inference using Static Analysis",
        "authors": [
            "Jiayi Wei",
            "Greg Durrett",
            "Isil Dillig"
        ],
        "published": "iclr 2023 poster",
        "summary": "There has been growing interest in automatically predicting missing type annotations in programs written in Python and JavaScript. While prior methods have achieved impressive accuracy when predicting the most common types, they often perform poorly on rare or complex types. In this paper, we present a new type inference method that treats type prediction as a code infilling task by leveraging CodeT5, a state-of-the-art seq2seq pre-trained language model for code. Our method uses static analysis to construct dynamic contexts for each code element whose type signature is to be predicted by the model.  We also propose an iterative decoding scheme that incorporates previous type predictions in the model's input context, allowing information exchange between related code elements. Our evaluation shows that the proposed approach, TypeT5, not only achieves a higher overall accuracy (particularly on rare and complex types) but also produces more coherent results with fewer type errors---while enabling easy user intervention.",
        "pdf_link": "https://openreview.net/pdf/1db193cae16df420c7376f835dbc310fd7c3d31b.pdf",
        "forum_url": "https://openreview.net/forum?id=4TyNEhI2GdN",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, but the paper mentions \"while prior methods have achieved impressive accuracy when predicting the most common types, they often perform poorly on rare or complex types\" which is related to the performance of the pre-trained language model, but not explicitly stated as a limitation of the LLM.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, but the paper mentions \"while prior methods have achieved impressive accuracy when predicting the most common types, they often perform poorly on rare or complex types\" which is related to the performance of the pre-trained language model, but not explicitly stated as a limitation of the LLM."
    },
    {
        "title": "A Mixture-of-Expert Approach to RL-based Dialogue Management",
        "authors": [
            "Yinlam Chow",
            "Azamat Tulepbergenov",
            "Ofir Nachum",
            "Dhawal Gupta",
            "Moonkyung Ryu",
            "Mohammad Ghavamzadeh",
            "Craig Boutilier"
        ],
        "published": "iclr 2023 poster",
        "summary": "Despite recent advancements in language models (LMs), their application to dialogue management (DM) problems and ability to carry on rich conversations remain a challenge. We use reinforcement learning (RL) to develop a dialogue agent that avoids being short-sighted (outputting generic utterances) and maximizes overall user satisfaction. Most existing RL approaches to DM train the agent at the word-level, and thus, have to deal with a combinatorially complex action space even for a medium-size vocabulary. As a result, they struggle to produce a successful and engaging dialogue even if they are warm-started with a pre-trained LM. To address this issue, we develop a RL-based DM using a novel mixture of expert language model (MoE-LM) that consists of (i) a LM capable of learning diverse semantics for conversation histories, (ii) a number of specialized LMs (or experts) capable of generating utterances corresponding to a particular attribute or personality, and (iii) a RL-based DM that performs dialogue planning with the utterances generated by the experts. Our MoE approach provides greater flexibility to generate sensible utterances with different intents and allows RL to focus on conversational-level DM. We compare it with SOTA baselines on open-domain dialogues and demonstrate its effectiveness both in terms of the diversity and sensibility of the generated utterances and the overall DM performance. ",
        "pdf_link": "https://openreview.net/pdf/ed30d6917d0c6d712d5a89e6216da216ad910d4d.pdf",
        "forum_url": "https://openreview.net/forum?id=4FBUihxz5nm",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite recent advancements in language models (LMs), their application to dialogue management (DM) problems and ability to carry on rich conversations remain a challenge.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite recent advancements in language models (LMs), their application to dialogue management (DM) problems and ability to carry on rich conversations remain a challenge.\""
    },
    {
        "title": "Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions",
        "authors": [
            "Ansong Ni",
            "Jeevana Priya Inala",
            "Chenglong Wang",
            "Alex Polozov",
            "Christopher Meek",
            "Dragomir Radev",
            "Jianfeng Gao"
        ],
        "published": "iclr 2023 poster",
        "summary": "Pretrained language models have shown superior performance on many natural language processing tasks, yet they still struggle at multi-step formal reasoning tasks like grade school math problems. One key challenge of finetuning them to solve such math reasoning problems is that many existing datasets only contain one reference solution for each problem, despite the fact that there are often alternative solutions resembling different reasoning paths to the final answer. This way, the finetuned models are biased towards the limited reference solutions, which limits their generalization to unseen examples. To mitigate this issue, we propose to let the model perform sampling during training and learn from both self-sampled fully-correct solutions, which yield the correct answer upon execution, and partially-correct solutions, whose intermediate state matches an intermediate state of a known correct solution. We show that our use of self-sampled correct and partially-correct solutions can benefit learning and help guide the sampling process, leading to more efficient exploration of the solution space. Additionally, we explore various training objectives to support learning from multiple solutions per example and find they greatly affect the performance. Experiments on two math reasoning datasets show the effectiveness of our method compared to learning from a single reference solution with MLE, where we improve PASS@100 from 35.5% to 44.5% for GSM8K, and 27.6% to 36.2% PASS@80 for MathQA. Such improvements are also consistent across different model sizes.",
        "pdf_link": "https://openreview.net/pdf/cd9398ff5cb3cad0cd4e635371b5bd8915893cd4.pdf",
        "forum_url": "https://openreview.net/forum?id=4D4TSJE6-K",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Pretrained language models have shown superior performance on many natural language processing tasks, yet they still struggle at multi-step formal reasoning tasks like grade school math problems. One key challenge of finetuning them to solve such math reasoning problems is that many existing datasets only contain one reference solution for each problem, despite the fact that there are often alternative solutions resembling different reasoning paths to the final answer",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Pretrained language models have shown superior performance on many natural language processing tasks, yet they still struggle at multi-step formal reasoning tasks like grade school math problems. One key challenge of finetuning them to solve such math reasoning problems is that many existing datasets only contain one reference solution for each problem, despite the fact that there are often alternative solutions resembling different reasoning paths to the final answer"
    },
    {
        "title": "Meta Learning to Bridge Vision and Language Models for Multimodal Few-Shot Learning",
        "authors": [
            "Ivona Najdenkoska",
            "Xiantong Zhen",
            "Marcel Worring"
        ],
        "published": "iclr 2023 poster",
        "summary": "Multimodal few-shot learning is challenging due to the large domain gap between vision and language modalities. Existing methods are trying to communicate visual concepts as prompts to frozen language models, but rely on hand-engineered task induction to reduce the hypothesis space. To make the whole process learnable, we introduce a multimodal meta-learning approach. Specifically, our approach decomposes the training of the model into a set of related multimodal few-shot tasks. We define a meta-mapper network, acting as a meta-learner, to efficiently bridge frozen large-scale vision and language models and leverage their already learned capacity. By updating the learnable parameters only of the meta-mapper, it learns to accrue shared meta-knowledge among these tasks. Thus, it can rapidly adapt to newly presented samples with only a few gradient updates. Importantly, it induces the task in a completely data-driven manner, with no need for a hand-engineered task induction. We evaluate our approach on recently proposed multimodal few-shot benchmarks, measuring how rapidly the model can bind novel visual concepts to words and answer visual questions by observing only a limited set of labeled examples. The experimental results show that our meta-learning approach outperforms the baseline across multiple datasets and various training settings while being computationally more efficient.",
        "pdf_link": "https://openreview.net/pdf/0e9bd6133a3659d2a5883ce2063de3dfff12c275.pdf",
        "forum_url": "https://openreview.net/forum?id=3oWo92cQyxL",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing methods are trying to communicate visual concepts as prompts to frozen language models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Existing methods are trying to communicate visual concepts as prompts to frozen language models\""
    },
    {
        "title": "Federated Neural Bandits",
        "authors": [
            "Zhongxiang Dai",
            "Yao Shu",
            "Arun Verma",
            "Flint Xiaofeng Fan",
            "Bryan Kian Hsiang Low",
            "Patrick Jaillet"
        ],
        "published": "iclr 2023 poster",
        "summary": "Recent works on neural contextual bandits have achieved compelling performances due to their ability to leverage the strong representation power of neural networks (NNs) for reward prediction. Many applications of contextual bandits involve multiple agents who collaborate without sharing raw observations, thus giving rise to the setting of federated contextual bandits}. Existing works on federated contextual bandits rely on linear or kernelized bandits, which may fall short when modeling complex real-world reward functions. So, this paper introduces the federated neural-upper confidence bound (FN-UCB) algorithm. To better exploit the federated setting, FN-UCB adopts a weighted combination of two UCBs: $\\text{UCB}^{a}$ allows every agent to additionally use the observations from the other agents to accelerate exploration (without sharing raw observations), while $\\text{UCB}^{b}$ uses an NN with aggregated parameters for reward prediction in a similar way to federated averaging for supervised learning. Notably, the weight between the two UCBs required by our theoretical analysis is amenable to an interesting interpretation, which emphasizes $\\text{UCB}^{a}$ initially for accelerated exploration and relies more on $\\text{UCB}^{b}$ later after enough observations have been collected to train the NNs for accurate reward prediction (i.e., reliable exploitation). We prove sub-linear upper bounds on both the cumulative regret and the number of communication rounds of FN-UCB, and empirically demonstrate its competitive performance.",
        "pdf_link": "https://openreview.net/pdf/8ac7b7db42b45091cff07d9f5d520796a17d3efa.pdf",
        "forum_url": "https://openreview.net/forum?id=38m4h8HcNRL",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Low-Resource NLP",
        "authors": [
            "Yufei Wang",
            "Jiayi Zheng",
            "Can Xu",
            "Xiubo Geng",
            "Tao Shen",
            "Chongyang Tao",
            "Daxin Jiang"
        ],
        "published": "iclr 2023 poster",
        "summary": "This paper focuses on data augmentation for low-resource NLP tasks where the training set is limited. The existing solutions either leverage task-independent heuristic rules (e.g., Synonym Replacement) or fine-tune general-purpose pre-trained language models (e.g., GPT2) using the limited training instances to produce new synthetic data. Consequently, they have trivial task-specific knowledge and are limited to yielding low-quality synthetic data. To combat this issue, we propose Knowledge Mixture Data Augmentation Model (KnowDA), a Seq2Seq language model pretrained on a mixture of diverse NLP tasks under a novel framework of Knowledge Mixture Training (KoMT). The goal of KoMT is to condense diverse NLP task-specific knowledge into the single KnowDA model\n(i.e., all-in-one). The resulting KnowDA could utilize these knowledge to quickly grasp the inherent synthesis law of the target task through limited training instances. Specifically, KoMT reformulates input examples from various heterogeneous NLP tasks into a unified text-to-text format and employs denoising training objectives in different granularity to learn to reconstruct partial or complete samples. To the best of our knowledge, we are the first to attempt to apply 100+ NLP multi-task training for data augmentation. Extensive experiments show that i) the synthetic data produced by KnowDA successfully improves the performance of the strong pre-trained language\nmodels (i.e., Bert, ALBert and Deberta) by a large margin on the low-resource NLP benchmark FewGLUE, CoNLL’03 and WikiAnn; ii) KnowDA successful transfer the task knowledge to NLP tasks whose types are seen and unseen in KoMT.",
        "pdf_link": "https://openreview.net/pdf/92d40698b0d55d0fbb40c50c4921bf1bb8cd40aa.pdf",
        "forum_url": "https://openreview.net/forum?id=2nocgE1m0A",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The existing solutions... fine-tune general-purpose pre-trained language models (e.g., GPT2)... are limited to yielding low-quality synthetic data.\"\n\nThis paper discusses LLMs, but only mentions limitations of existing solutions that involve fine-tuning pre-trained language models, and does not elaborate on the limitations of LLMs themselves.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"The existing solutions... fine-tune general-purpose pre-trained language models (e.g., GPT2)... are limited to yielding low-quality synthetic data.\"\n\nThis paper discusses LLMs, but only mentions limitations of existing solutions that involve fine-tuning pre-trained language models, and does not elaborate on the limitations of LLMs themselves."
    },
    {
        "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
        "authors": [
            "Xuezhi Wang",
            "Jason Wei",
            "Dale Schuurmans",
            "Quoc V Le",
            "Ed H. Chi",
            "Sharan Narang",
            "Aakanksha Chowdhery",
            "Denny Zhou"
        ],
        "published": "iclr 2023 poster",
        "summary": "Chain-of-thought prompting combined with pretrained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out all possible reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer.  Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
        "pdf_link": "https://openreview.net/pdf/9d06013867701125040af03996c3aefddc8d58d1.pdf",
        "forum_url": "https://openreview.net/forum?id=1PL1NIMMrw",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "A Simple Approach for Visual Room Rearrangement: 3D Mapping and Semantic Search",
        "authors": [
            "Brandon Trabucco",
            "Gunnar A Sigurdsson",
            "Robinson Piramuthu",
            "Gaurav S. Sukhatme",
            "Ruslan Salakhutdinov"
        ],
        "published": "iclr 2023 poster",
        "summary": "Physically rearranging objects is an important capability for embodied agents. Visual room rearrangement evaluates an agent's ability to rearrange objects in a room to a desired goal based solely on visual input. We propose a simple yet effective method for this problem: (1) search for and map which objects need to be rearranged, and (2) rearrange each object until the task is complete. Our approach consists of an off-the-shelf semantic segmentation model, voxel-based semantic map, and semantic search policy to efficiently find objects that need to be rearranged. Our method was the winning submission to the AI2-THOR Rearrangement Challenge in the 2022 Embodied AI Workshop at CVPR 2022, and improves on current state-of-the-art end-to-end reinforcement learning-based methods that learn visual room rearrangement policies from 0.53% correct rearrangement to 16.56%, using only 2.7% as many samples from the environment.",
        "pdf_link": "https://openreview.net/pdf/da9e28116ad9e608e30a012bb8a930a488fd5ff0.pdf",
        "forum_url": "https://openreview.net/forum?id=1C6nCCaRe6p",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Reward Design with Language Models",
        "authors": [
            "Minae Kwon",
            "Sang Michael Xie",
            "Kalesha Bullard",
            "Dorsa Sadigh"
        ],
        "published": "iclr 2023 poster",
        "summary": "Reward design in reinforcement learning (RL) is challenging since specifying human notions of desired behavior may be difficult via reward functions or require many expert demonstrations. Can we instead cheaply design rewards using a natural language interface? This paper explores how to simplify reward design by using a large language model (LLM) such as GPT-3 as a proxy reward function, where the user provides a textual prompt containing a few examples (few-shot) or a description (zero-shot) of desired behavior. Our approach leverages this proxy reward function in an RL framework. Specifically, users specify a prompt once at the beginning of training. During training, the LLM evaluates an RL agent's behavior against the desired behavior described by the prompt and outputs a corresponding reward signal. The RL agent then uses this reward to update its behavior. We evaluate whether our approach can train agents aligned with user objectives in the Ultimatum Game, matrix games, and the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents trained with our framework are well-aligned with the user's objectives and outperforms RL agents trained with reward functions learned via supervised learning. ",
        "pdf_link": "https://openreview.net/pdf/696171827b35dfe4e639dfe0644bf0f279f84c75.pdf",
        "forum_url": "https://openreview.net/forum?id=10uNUgI5Kl",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "Calibrating Sequence likelihood Improves Conditional Language Generation",
        "authors": [
            "Yao Zhao",
            "Mikhail Khalman",
            "Rishabh Joshi",
            "Shashi Narayan",
            "Mohammad Saleh",
            "Peter J Liu"
        ],
        "published": "iclr 2023 poster",
        "summary": "Conditional language models are predominantly trained with maximum likelihood estimation (MLE), giving probability mass to sparsely observed target sequences. While MLE trained models assign high probability to plausible sequences given the context, the model probabilities often do not accurately rank-order generated sequences by quality.  This has been empirically observed in beam search decoding as output quality degrading with large beam sizes,  and decoding strategies benefiting from heuristics such as length normalization and repetition-blocking. In this work, we introduce sequence likelihood calibration (SLiC) where the likelihood of model generated sequences are calibrated to better align with reference sequences in the model’s latent space.  With SLiC, decoding heuristics become unnecessary and decoding candidates’ quality significantly improves regardless of the decoding method. Furthermore, SLiC shows no sign of diminishing returns with model scale, and presents alternative ways to improve quality with limited training and inference budgets. With SLiC, we exceed or match SOTA results on a wide range of generation tasks spanning abstractive summarization, question generation, abstractive question answering and data-to-text generation, even with modest-sized models.",
        "pdf_link": "https://openreview.net/pdf/dfaa19d042f152ef2bcc2b1feef6fb0276c42113.pdf",
        "forum_url": "https://openreview.net/forum?id=0qSOodKmJaN",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "HiCLIP: Contrastive Language-Image Pretraining with Hierarchy-aware Attention",
        "authors": [
            "Shijie Geng",
            "Jianbo Yuan",
            "Yu Tian",
            "Yuxiao Chen",
            "Yongfeng Zhang"
        ],
        "published": "iclr 2023 poster",
        "summary": "The success of large-scale contrastive vision-language pretraining (CLIP) has benefited both visual recognition and multimodal content understanding. The concise design brings CLIP the advantage in inference efficiency against other vision-language models with heavier cross-attention fusion layers, making it a popular choice for a wide spectrum of downstream tasks. However, CLIP does not explicitly capture the hierarchical nature of high-level and fine-grained semantics conveyed in images and texts, which is arguably critical to vision-language understanding and reasoning. To this end, we equip both the visual and language branches in CLIP with hierarchy-aware attentions, namely Hierarchy-aware CLIP (HiCLIP), to progressively discover semantic hierarchies layer-by-layer from both images and texts in an unsupervised manner. As a result, such hierarchical aggregation significantly improves the cross-modal alignment. To demonstrate the advantages of HiCLIP, we conduct qualitative analysis on its unsupervised hierarchy induction during inference, as well as extensive quantitative experiments on both visual recognition and vision-language downstream tasks.",
        "pdf_link": "https://openreview.net/pdf/2e082778e9c948cf856dc93b13cc4d0734583c61.pdf",
        "forum_url": "https://openreview.net/forum?id=0eTTKOOOQkV",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, CLIP does not explicitly capture the hierarchical nature of high-level and fine-grained semantics conveyed in images and texts, which is arguably critical to vision-language understanding and reasoning.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, CLIP does not explicitly capture the hierarchical nature of high-level and fine-grained semantics conveyed in images and texts, which is arguably critical to vision-language understanding and reasoning.\""
    },
    {
        "title": "Fantastic Rewards and How to Tame Them: A Case Study on Reward Learning for Task-oriented Dialogue Systems",
        "authors": [
            "Yihao Feng",
            "Shentao Yang",
            "Shujian Zhang",
            "Jianguo Zhang",
            "Caiming Xiong",
            "Mingyuan Zhou",
            "Huan Wang"
        ],
        "published": "iclr 2023 poster",
        "summary": "When learning task-oriented dialogue (ToD) agents, reinforcement learning (RL) techniques can naturally be utilized to train dialogue strategies to achieve user-specific goals. Prior works mainly focus on adopting advanced RL techniques to train the ToD agents, while the design of the reward function is not well studied. This paper aims at answering the question of how to efficiently learn and leverage a reward function for training end-to-end (E2E) ToD agents. Specifically, we introduce two generalized objectives for reward-function learning, inspired by the classical learning-to-rank literature. Further, we utilize the learned reward function to guide the training of the E2E ToD agent. With the proposed techniques, we achieve competitive results on the E2E response-generation task on the Multiwoz 2.0 dataset. Source code and checkpoints are publicly released at https://github.com/Shentao-YANG/Fantastic_Reward_ICLR2023.",
        "pdf_link": "https://openreview.net/pdf/a1290714106d4bdf27febffa982192b6b9d5819f.pdf",
        "forum_url": "https://openreview.net/forum?id=086pmarAris",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "On the Data-Efficiency with Contrastive Image Transformation in Reinforcement Learning",
        "authors": [
            "Sicong Liu",
            "Xi Sheryl Zhang",
            "Yushuo Li",
            "Yifan Zhang",
            "Jian Cheng"
        ],
        "published": "iclr 2023 poster",
        "summary": "Data-efficiency has always been an essential issue in pixel-based reinforcement learning (RL). As the agent not only learns decision-making but also meaningful representations from images. The line of reinforcement learning with data augmentation shows significant improvements in sample-efficiency. However, it is challenging to guarantee the optimality invariant transformation, that is, the augmented data are readily recognized as a completely different state by the agent. In the end, we propose a contrastive invariant transformation (CoIT), a simple yet promising learnable data augmentation combined with standard model-free algorithms to improve sample-efficiency. Concretely, the differentiable CoIT leverages original samples with augmented samples and hastens the state encoder for a contrastive invariant embedding. We evaluate our approach on DeepMind Control Suite and Atari100K. Empirical results verify advances using CoIT, enabling it to outperform the new state-of-the-art on various tasks. Source code is available at https://github.com/mooricAnna/CoIT.",
        "pdf_link": "https://openreview.net/pdf/42916f46ac828e86998f6f3ae44feae52efdb5ae.pdf",
        "forum_url": "https://openreview.net/forum?id=-nm-rHXi5ga",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Recitation-Augmented Language Models",
        "authors": [
            "Zhiqing Sun",
            "Xuezhi Wang",
            "Yi Tay",
            "Yiming Yang",
            "Denny Zhou"
        ],
        "published": "iclr 2023 poster",
        "summary": "We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrieval-augmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE first recites one or several relevant passages from LLMs’ own memory via sampling, and then produces the final answers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of RECITE on three pre-trained models (In-house LM, UL2, and OPT) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA). Our code is available at \"https://github.com/Edward-Sun/RECITE\".",
        "pdf_link": "https://openreview.net/pdf/693f49dd101c5c13e74972b49546fdff73d91ac4.pdf",
        "forum_url": "https://openreview.net/forum?id=-cqvvvb-NkI",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None"
    },
    {
        "title": "Modeling Sequential Sentence Relation to Improve Cross-lingual Dense Retrieval",
        "authors": [
            "Shunyu Zhang",
            "Yaobo Liang",
            "MING GONG",
            "Daxin Jiang",
            "Nan Duan"
        ],
        "published": "iclr 2023 poster",
        "summary": "Recently multi-lingual pre-trained language models (PLM) such as mBERT and XLM-R have achieved impressive strides in cross-lingual dense retrieval. Despite its successes, they are general-purpose PLM while the multilingual PLM tailored for cross-lingual retrieval is still unexplored. Motivated by an observation that the sentences in parallel documents are approximately in the same order, which is universal across languages, we propose to model this sequential sentence relation to facilitate cross-lingual representation learning. Specifically, we propose a multilingual PLM called masked sentence model (MSM), which consists of a sentence encoder to generate the sentence representations, and a document encoder applied to a sequence of sentence vectors from a document. The document encoder is shared for all languages to model the universal sequential sentence relation across languages. To train the model, we propose a masked sentence prediction task, which masks and predicts the sentence vector via a hierarchical contrastive loss with sampled negatives. Comprehensive experiments on four cross-lingual retrieval tasks show MSM significantly outperforms existing advanced pre-training models, demonstrating the effectiveness and stronger cross-lingual retrieval capabilities of our approach. ",
        "pdf_link": "https://openreview.net/pdf/8649b63e1d2dd8ed68f852a7da6ab541293d32a9.pdf",
        "forum_url": "https://openreview.net/forum?id=-bVsNeR56KS",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite its successes, they are general-purpose PLM while the multilingual PLM tailored for cross-lingual retrieval is still unexplored.\"\n\nThis paper discusses the limitations of general-purpose pre-trained language models (PLM) in cross-lingual retrieval, but the limitation is mentioned briefly and is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Despite its successes, they are general-purpose PLM while the multilingual PLM tailored for cross-lingual retrieval is still unexplored.\"\n\nThis paper discusses the limitations of general-purpose pre-trained language models (PLM) in cross-lingual retrieval, but the limitation is mentioned briefly and is not the primary focus of the paper."
    },
    {
        "title": "TaskPrompter: Spatial-Channel Multi-Task Prompting for Dense Scene Understanding",
        "authors": [
            "Hanrong Ye",
            "Dan Xu"
        ],
        "published": "iclr 2023 poster",
        "summary": "Learning effective representations simultaneously from multiple tasks in a unified network framework is a fundamental paradigm for multi-task dense visual scene understanding. This requires joint modeling (i) task-generic and (ii) task-specific representations, and (iii) cross-task representation interactions. Existing works typically model these three perspectives with separately designed structures, using shared network modules for task-generic learning, different modules for task-specific learning, and establishing connections among these components for cross-task interactions. It is barely explored in the literature to model these three perspectives in each network layer in an end-to-end manner, which can not only minimize the effort of carefully designing empirical structures for the three multi-task representation learning objectives, but also greatly improve the representation learning capability of the multi-task network since all the model capacity will be used to optimize the three objectives together. In this paper, we propose TaskPrompter, a novel spatial-channel multi-task prompting transformer framework to achieve this target. Specifically, we design a set of spatial-channel task prompts and learn their spatial- and channel interactions with the shared image tokens in each transformer layer with attention mechanism, as aggregating spatial and channel information is critical for dense prediction tasks. Each task prompt learns task-specific representation for one task, while all the prompts can jointly contribute to the learning of the shared image token representations, and the interactions between different task prompts model the cross-task relationship. To decode dense predictions for multiple tasks with the learned spatial-channel task prompts from transformer, we accordingly design a dense task prompt decoding mechanism, which queries the shared image tokens using task prompts to obtain spatial- and channel-wise task-specific representations. Extensive experiments on two challenging multi-task dense scene understanding benchmarks (i.e. NYUD-V2 and PASCAL-Context) show the superiority of the proposed framework and TaskPrompter establishes significant state-of-the-art performances on multi-task dense predictions. Codes and models are made publicly available at https://github.com/prismformore/Multi-Task-Transformer.",
        "pdf_link": "https://openreview.net/pdf/ebe8ec2526826ab8aca7189c0b3935036603f11c.pdf",
        "forum_url": "https://openreview.net/forum?id=-CwPopPJda",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam",
        "authors": [
            "Yucheng Lu",
            "Conglong Li",
            "Minjia Zhang",
            "Christopher De Sa",
            "Yuxiong He"
        ],
        "published": "iclr 2023 poster",
        "summary": "1-bit gradient compression and local steps are two representative techniques that enable drastic communication reduction in distributed SGD. Their benefits, however, remain an open question on Adam-based large model pre-training (e.g. BERT and GPT). In this paper, we demonstrate the non-linearity in Adam causes slow convergence even when 1-bit compression or local steps are individually applied. To alleviate this limitation, we propose \\textbf{0/1 Adam} that linearizes each Adam step via approximating its optimizer states using their stale estimates and linear correlation. \\textbf{0/1 Adam} performs an Adam-like step to preserve the adaptivity, while its linearity allows utilizing 1-bit compression and local steps simultaneously for wall-clock time speed up. We provide convergence guarantee for \\textbf{0/1 Adam} on smooth non-convex objectives. On various large-scale benchmarks such as BERT-Base, BERT-Large, GPT-2 pre-training and ImageNet, we demonstrate on up to 128 GPUs that \\textbf{0/1 Adam} is able to reduce up to 87\\% of data volume, 54\\% of communication rounds, and achieve up to 2$\\times$ higher training throughput and end-to-end training time reduction compared to the state-of-the-art baseline 1-bit Adam; while enjoying the same statistical convergence speed and end task model accuracy on GLUE dataset and ImageNet validation set. ",
        "pdf_link": "https://openreview.net/pdf/71e3ec182ebfd2023724f92a59fe1d09a34cb114.pdf",
        "forum_url": "https://openreview.net/forum?id=-CefY2EOupj",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Their benefits, however, remain an open question on Adam-based large model pre-training (e.g. BERT and GPT). In this paper, we demonstrate the non-linearity in Adam causes slow convergence even when 1-bit compression or local steps are individually applied.\"\n\nNote: Although the paper does not directly discuss limitations of LLMs, it talks about a limitation of Adam",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Their benefits, however, remain an open question on Adam-based large model pre-training (e.g. BERT and GPT). In this paper, we demonstrate the non-linearity in Adam causes slow convergence even when 1-bit compression or local steps are individually applied.\"\n\nNote: Although the paper does not directly discuss limitations of LLMs, it talks about a limitation of Adam"
    },
    {
        "title": "GLM-130B: An Open Bilingual Pre-trained Model",
        "authors": [
            "Aohan Zeng",
            "Xiao Liu",
            "Zhengxiao Du",
            "Zihan Wang",
            "Hanyu Lai",
            "Ming Ding",
            "Zhuoyi Yang",
            "Yifan Xu",
            "Wendi Zheng",
            "Xiao Xia",
            "Weng Lam Tam",
            "Zixuan Ma",
            "Yufei Xue",
            "Jidong Zhai",
            "Wenguang Chen",
            "Zhiyuan Liu",
            "Peng Zhang",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "published": "iclr 2023 poster",
        "summary": "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the pre-training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B—the largest Chinese language model—across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization with almost no performance loss, making it the first among 100B-scale models and more importantly, allowing its effective inference on 4×RTX 3090 (24G) or 8×RTX 2080 Ti (11G) GPUs, the most ever affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at https://github.com/THUDM/GLM-130B/.",
        "pdf_link": "https://openreview.net/pdf/8b5b1ff47d47935e500a223aa3c138cb0755002f.pdf",
        "forum_url": "https://openreview.net/forum?id=-Aw0rrrPUF",
        "source": "iclr2023",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence.\""
    }
]