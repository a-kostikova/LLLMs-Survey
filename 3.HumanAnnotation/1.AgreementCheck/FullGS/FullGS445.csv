Title;Abstract;annotator1_Label;annotator1_Explanation (optional);annotator1_evidence;annotator2_Label;annotator2_Explanation (optional);annotator2_evidence;annotator3_Label;annotator3_Explanation (optional);annotator3_evidence;annotator4_Label;annotator4_Explanation (optional);annotator4_evidence;FinalLabel;Venue;Date
ALCUNA: Large Language Models Meet New Knowledge;With the rapid development of NLP, large-scale language models (LLMs) excel in various tasks across multiple domains now. However, existing benchmarks may not adequately measure these models’ capabilities, especially when faced with new knowledge. In this paper, we address the lack of benchmarks to evaluate LLMs’ ability to handle new knowledge, an important and challenging aspect in the rapidly evolving world. We propose an approach called KnowGen that generates new knowledge by altering existing entity attributes and relationships, resulting in artificial entities that are distinct from real-world entities. With KnowGen, we introduce a benchmark named ALCUNA to assess LLMs’ abilities in knowledge understanding, differentiation, and association. We benchmark several LLMs, reveals that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge. We also explore the impact of entity similarity on the model’s understanding of entity knowledge and the influence of contextual entities. We appeal to the need for caution when using LLMs in new scenarios or with new knowledge, and hope that our benchmarks can help drive the development of LLMs in face of new knowledge.;5;The paper deals with the problems of LLMs facing new knowledge, and proposes a new benchmark to measure this ability of LLMs.;"""We benchmark several LLMs, reveals that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge"", ""We appeal to the need for caution when using LLMs in new scenarios or with new knowledge, and hope that our benchmarks can help drive the development of LLMs in face of new knowledge.""";4;mostly benchmark-focused but with substantial space devoted to limitations;"""existing benchmarks may not adequately measure these models’ capabilities, especially when faced with new knowledge"", ""We benchmark several LLMs, reveals that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge"", ""We appeal to the need for caution when using LLMs in new scenarios or with new knowledge""";4;the paper is about difficulties LLMs face in handling new knowledge. but the central focus of the paper is ALCUNA benchmark;;4;3.5 maybe - it's a borderline case;;4;emnlp2023;December 2023
Template-free Prompt Tuning for Few-shot NER;Prompt-based methods have been successfully applied in sentence-level few-shot learning tasks, mostly owing to the sophisticated design of templates and label words. However, when applied to token-level labeling tasks such as NER, it would be time-consuming to enumerate the template queries over all potential entity spans. In this work, we propose a more elegant method to reformulate NER tasks as LM problems without any templates. Specifically, we discard the template construction process while maintaining the word prediction paradigm of pre-training models to predict a class-related pivot word (or label word) at the entity position. Meanwhile, we also explore principled ways to automatically search for appropriate label words that the pre-trained models can easily adapt to. While avoiding the complicated template-based process, the proposed LM objective also reduces the gap between different objectives used in pre-training and fine-tuning, thus it can better benefit the few-shot performance. Experimental results demonstrate the effectiveness of the proposed method over bert-tagger and template-based method under few-shot settings. Moreover, the decoding speed of the proposed method is up to 1930.12 times faster than the template-based method.;2;Briefly mentions a limitation (time) of LLMs on token-level labeling task, but the main focus is on the details of the new method proposed to speed up this process.;"""However, when applied to token-level labeling tasks such as NER, it would be time-consuming to enumerate the template queries over all potential entity spans.""";2;mentions computational problems in word-level classification tasks with LLMs;"""time-consuming to enumerate the template queries over all potential entity spans""";2;"ADJUSTED: previously 3 

discusses limitation of existing methods for NER, particularly the inefficiency of template enumeration, but focuses primarily on introducing and validating the new method";"""However, when applied to token-level labeling tasks such as NER, it would be time-consuming to enumerate the template queries over all potential entity spans.""";1;;;2;naacl2022;July 2022
EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models;We introduce EQ-Bench, a novel benchmark designed to evaluate aspects of  emotional intelligence in Large Language Models (LLMs). We assess the ability  of LLMs to understand complex emotions and social interactions by asking them  to predict the intensity of emotional states of characters in a dialogue. The  benchmark is able to discriminate effectively between a wide range of models.  We find that EQ-Bench correlates strongly with comprehensive multi-domain  benchmarks like MMLU (Hendrycks et al., 2020) (r=0.97), indicating that we may  be capturing similar aspects of broad intelligence. Our benchmark produces  highly repeatable results using a set of 60 English-language questions. We also  provide open-source code for an automated benchmarking pipeline at  https://github.com/EQ-bench/EQ-Bench and a leaderboard at https://eqbench.com;1;A new benchmark is proposed, but there are no limitations of LLMs mentioned.;;1;mentions LLMs but no limitations, only hype;;1;"ADJUSTED: previously 3

the paper mentions the (potential) limitation - evaluating emotional intelligence, but does not say how models perform on this task";;1;;;1;arxiv;11 December 2023
SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models;Large language models (LLMs) can now handle longer sequences of tokens,  enabling complex tasks like book understanding and generating lengthy novels.  However, the key-value (KV) cache required for LLMs consumes substantial memory  as context length increasing, becoming the bottleneck for deployment. In this  paper, we present a strategy called SKVQ, which stands for sliding-window KV  cache quantization, to address the issue of extremely low bitwidth KV cache  quantization. To achieve this, SKVQ rearranges the channels of the KV cache in  order to improve the similarity of channels in quantization groups, and applies  clipped dynamic quantization at the group level. Additionally, SKVQ ensures  that the most recent window tokens in the KV cache are preserved with high  precision. This helps maintain the accuracy of a small but important portion of  the KV cache.SKVQ achieves high compression ratios while maintaining accuracy.  Our evaluation on LLMs demonstrates that SKVQ surpasses previous quantization  approaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit  values with minimal loss of accuracy. With SKVQ, it is possible to process  context lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7  times faster decoding.;2;Briefly mentions memory problems of LLMs, but the main focus is on the new method they propose.;"""However, the key-value (KV) cache required for LLMs consumes substantial memory  as context length increasing, becoming the bottleneck for deployment.""";1;Discusses LLMs and hints at computational limitations;;2;"ADJUSTED: previously 3

discusses the substantial memory consumption of KV caches (with an increase in context length) as a limitation, but the main focus is on validating SKVQ";"""However, the key-value (KV) cache required for LLMs consumes substantial memory  as context length increasing, becoming the bottleneck for deployment.""";2;;;2;arxiv;10 May 2024
Comparing a BERT Classifier and a GPT classifier for Detecting Connective Language Across Multiple Social Media;This study presents an approach for detecting connective language—defined as language that facilitates engagement, understanding, and conversation—from social media discussions. We developed and evaluated two types of classifiers: BERT and GPT-3.5 turbo. Our results demonstrate that the BERT classifier significantly outperforms GPT-3.5 turbo in detecting connective language. Furthermore, our analysis confirms that connective language is distinct from related concepts measuring discourse qualities, such as politeness and toxicity. We also explore the potential of BERT-based classifiers for platform-agnostic tools. This research advances our understanding of the linguistic dimensions of online communication and proposes practical tools for detecting connective language across diverse digital environments.;1;The paper deals with the ability of LLMs to detect connective language and compares to models of which one outperforms the other.;;1;;;2;something between 2 and 3. one aspect which could be potentially problematic is mentioned (connective language), but it is not explicitly presented as a limitation of an LLM, maybe slightly in comparison between GPT-3.5 and BERT;"""Our results demonstrate that the BERT classifier significantly outperforms GPT-3.5 turbo in detecting connective language.""";1;;;1;emnlp2024;November 2024
OpenResearcher: Unleashing AI for Accelerated Scientific Research;The rapid growth of scientific literature imposes significant challenges for researchers endeavoring to stay updated with the latest advancements in their fields and delve into new areas. We introduce OpenResearcher, an innovative platform that leverages Artificial Intelligence (AI) techniques to accelerate the research process by answering diverse questions from researchers. OpenResearcher is built based on Retrieval-Augmented Generation (RAG) to integrate Large Language Models (LLMs) with up-to-date, domain-specific knowledge. Moreover, we develop various tools for OpenResearcher to understand researchers’ queries, search from the scientific literature, filter retrieved information, provide accurate and comprehensive answers, and self-refine these answers. OpenResearcher can flexibly use these tools to balance efficiency and effectiveness. As a result, OpenResearcher enables researchers to save time and increase their potential to discover new insights and drive scientific breakthroughs. Demo, video, and code are available at: https://github.com/GAIR-NLP/OpenResearcher.;1;Focuses only of the benefits of LLMs, no limitations mentioned.;;1;;;1;"ADJUSTED: previosuly 2 
Just focuses on some application of an LLM";;1;;;1;emnlp2024;November 2024
Mitigating Temporal Misalignment by Discarding Outdated Facts;While large language models are able to retain vast amounts of world knowledge seen during pretraining, such knowledge is prone to going out of date and is nontrivial to update. Furthermore, these models are often used under temporal misalignment, tasked with answering questions about the present, despite having only been trained on data collected in the past. To mitigate the effects of temporal misalignment, we propose fact duration prediction: the task of predicting how long a given fact will remain true. In our experiments, we demonstrate that identifying which facts are prone to rapid change can help models avoid reciting outdated information and determine which predictions require seeking out up-to-date knowledge sources. We also show how modeling fact duration improves calibration for knowledge-intensive tasks, such as open-retrieval question answering, under temporal misalignment, by discarding volatile facts.;3;The paper deals with the problem of outdated facts in LLMs. The primary focus is on solving this problem with a new method.;"""such knowledge is prone to going out of date and is nontrivial to update. Furthermore, these models are often used under temporal misalignment, tasked with answering questions about the present, despite having only been trained on data collected in the past.""";2;mentions temporal misalignment limitations and proposes a solution;"""these models are often used under temporal misalignment, tasked with answering questions about the present, despite having only been trained on data collected in the past""";2;"discusses temporal misalignment as a limitation, but maybe the central focus is on solution, not an analysis of the limitation itself

2.5 maybe";"""While large language models are able to retain vast amounts of world knowledge seen during pretraining, such knowledge is prone to going out of date and is nontrivial to update. Furthermore, these models are often used under temporal misalignment, tasked with answering questions about the present, despite having only been trained on data collected in the past.""";1;potentially 1.5 if small limitations discussed would be considered;;2;emnlp2023;December 2023
ANALOGICAL -- A Novel Benchmark for Long Text Analogy Evaluation in Large Language Models;Over the past decade, analogies, in the form of word-level analogies, have played a significant role as an intrinsic measure of evaluating the quality of word embedding methods such as word2vec. Modern large language models (LLMs), however, are primarily evaluated on extrinsic measures based on benchmarks such as GLUE and SuperGLUE, and there are only a few investigations on whether LLMs can draw analogies between long texts. In this paper, we present ANALOGICAL, a new benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of long text with six levels of complexity -- (i) word, (ii) word vs. sentence, (iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor. Using thirteen datasets and three different distance measures, we evaluate the abilities of eight LLMs in identifying analogical pairs in the semantic vector space. Our evaluation finds that it is increasingly challenging for LLMs to identify analogies when going up the analogy taxonomy.;2;The paper proposes a new benchmark to measure the ability of LLMs to identify analogies between texts. The authors conclude that LLMs are limited in this ability.;"""Our evaluation finds that it is increasingly challenging for LLMs to identify analogies when going up the analogy taxonomy.""";3;benchmarking paper which finds substantial limiations in analogical reasoning;"""Our evaluation finds that it is increasingly challenging for LLMs to identify analogies when going up the analogy taxonomy.""";3;The main focus is on developing a benchmark. A limitation is mentioned (analogy identification), but it is not discussed extensively;"""Our evaluation finds that it is increasingly challenging for LLMs to identify analogies when going up the analogy taxonomy.""";3;2 or 3 - the last sentence certainly outlines a limitation, but nothing else;;3;arxiv;08 May 2023
Language is Scary when Over-Analyzed: Unpacking Implied Misogynistic Reasoning with Argumentation Theory-Driven Prompts;We propose misogyny detection as an Argumentative Reasoning task and we investigate the capacity of large language models (LLMs) to understand the implicit reasoning used to convey misogyny in both Italian and English. The central aim is to generate the missing reasoning link between a message and the implied meanings encoding the misogyny. Our study uses argumentation theory as a foundation to form a collection of prompts in both zero-shot and few-shot settings. These prompts integrate different techniques, including chain-of-thought reasoning and augmented knowledge. Our findings show that LLMs fall short on reasoning capabilities about misogynistic comments and that they mostly rely on their implicit knowledge derived from internalized common stereotypes about women to generate implied assumptions, rather than on inductive reasoning.;5;The focus of the paper is on evaluating the ability of LLMs to detect misogyny. They conclude that LLMs are limited in their reasoning capabilities about misogynistic comments.;"""Our findings show that LLMs fall short on reasoning capabilities about misogynistic comments and that they mostly rely on their implicit knowledge derived from internalized common stereotypes about women to generate implied assumptions, rather than on inductive reasoning.""";4;explicit focus on reasoning failures;"""Our findings show that LLMs fall short on reasoning capabilities about misogynistic comments and that they mostly rely on their implicit knowledge derived from internalized common stereotypes about women to generate implied assumptions, rather than on inductive reasoning""";4;Is very well focused on investigating a limitation (reasoning in misogyny detection). But it is also focused on solutions;"""Our findings show that LLMs fall short on reasoning capabilities about misogynistic comments and that they mostly rely on their implicit knowledge derived from internalized common stereotypes about women to generate implied assumptions, rather than on inductive reasoning.""";3;maybe also just not strong enough to warrant 4;;4;emnlp2024;November 2024
Synthesizing Text-to-SQL Data from Weak and Strong LLMs;The capability gap between open-source and closed-source large language models (LLMs) remains a challenge in text-to-SQL tasks. In this paper, we introduce a synthetic data approach that combines data produced by larger, more powerful models (strong models) with error information data generated by smaller, not well-aligned models (weak models). The method not only enhances the domain generalization of text-to-SQL models but also explores the potential of error data supervision through preference learning. Furthermore, we employ the synthetic data approach for instruction tuning on open-source LLMs, resulting SENSE, a specialized text-to-SQL model. The effectiveness of SENSE is demonstrated through state-of-the-art results on the SPIDER and BIRD benchmarks, bridging the performance gap between open-source models and methods prompted by closed-source models.;2;The paper deals with the limitations of weak models (closed-source), and proposes a method to improve the performance of the weak models.;"""The capability gap between open-source and closed-source large language models (LLMs) remains a challenge in text-to-SQL tasks."", ""not well-aligned models (weak models).""";2;"focuses on the ""performance gap"" between open-source and proprietary LLM models";"""The capability gap between open-source and closed-source large language models (LLMs) remains a challenge in text-to-SQL tasks""";2;"previously 3
Mentions the weakness of closed-source models in text-to-SQL tasks, but the main focus is on the approach for bridging the gap between strong and weak models";"""The capability gap between open-source and closed-source large language models (LLMs) remains a challenge in text-to-SQL tasks.""";1;;;2;acl2024;August 2024
Toward Grounded Commonsense Reasoning;"Consider a robot tasked with tidying a desk with a meticulously constructed Lego sports car. A human may recognize that it is not appropriate to disassemble the sports car and put it away as part of the ""tidying."" How can a robot reach that conclusion? Although large language models (LLMs) have recently been used to enable commonsense reasoning, grounding this reasoning in the real world has been challenging. To reason in the real world, robots must go beyond passively querying LLMs and actively gather information from the environment that is required to make the right decision. For instance, after detecting that there is an occluded car, the robot may need to actively perceive the car to know whether it is an advanced model car made out of Legos or a toy car built by a toddler. We propose an approach that leverages an LLM and vision language model (VLM) to help a robot actively perceive its environment to perform grounded commonsense reasoning. To evaluate our framework at scale, we release the MessySurfaces dataset which contains images of 70 real-world surfaces that need to be cleaned. We additionally illustrate our approach with a robot on 2 carefully designed surfaces. We find an average 12.9% improvement on the MessySurfaces benchmark and an average 15% improvement on the robot experiments over baselines that do not use active perception. The dataset, code, and videos of our approach can be found at https://minaek.github.io/grounded_commonsense_reasoning.";2;The paper mentions limitations of LLMs reasoning in the real world, but the main focus is on creating a solution with the help of LLMs.;"""Although large language models (LLMs) have recently been used to enable commonsense reasoning, grounding this reasoning in the real world has been challenging.""";2;engages with complex reasoning limitations but mostly focuses on a solution approach based on exploration strategies;"""grounding this reasoning in the real world has been challenging""";2;"ADJUSTED: previously 3

It does discuss a limitation, which is inability to fully ground commonsense reasoning in the real world. But the main focus is on the solution which integrates LLMs with VLMs";"""Although large language models (LLMs) have recently been used to enable commonsense reasoning, grounding this reasoning in the real world has been challenging.""";1;virtually no limitations mentioned;;2;arxiv;14 June 2023
Gotcha! Don't trick me with unanswerable questions! Self-aligning Large Language Models for Responding to Unknown Questions;Despite the remarkable abilities of Large Language Models (LLMs) to answer questions, they often display a considerable level of overconfidence even when the question does not have a definitive answer. To avoid providing hallucinated answers to these unknown questions, existing studies typically investigate approaches to refusing to answer these questions. In this work, we propose a novel and scalable self-alignment method to utilize the LLM itself to enhance its response-ability to different types of unknown questions, being capable of not only refusing to answer but also providing explanation to the unanswerability of unknown questions. Specifically, the Self-Align method first employ a two-stage class-aware self-augmentation approach to generate a large amount of unknown question-response data. Then we conduct disparity-driven self-curation to select qualified data for fine-tuning the LLM itself for aligning the responses to unknown questions as desired. Experimental results on two datasets across four types of unknown questions validate the superiority of the Self-Align method over existing baselines in terms of three types of task formulation.;2;The paper focuses on overcoming the problems of overconfidence and hallucinations in LLMs by proposing a new method.;"""they often display a considerable level of overconfidence even when the question does not have a definitive answer. To avoid providing hallucinated answers to these unknown questions, existing studies typically investigate approaches to refusing to answer these questions.""";2;uses hallucinations as jump-off point for a new alignment method;"""they often display a considerable level of overconfidence even when the question does not have a definitive answer"", ""hallucinated answers""";2;Quite a detailed discussion of a limitation of LLMs which is hallucinations when giving answers to unknown questions. Also proposes a solution which is the main focus of the paper;"""Despite the remarkable abilities of Large Language Models (LLMs) to answer questions, they often display a considerable level of overconfidence even when the question does not have a definitive answer.""";2;only in the first sentence;;2;arxiv;23 February 2024
Discovering Differences in the Representation of People using Contextualized Semantic Axes;A common paradigm for identifying semantic differences across social and temporal contexts is the use of static word embeddings and their distances. In particular, past work has compared embeddings against “semantic axes” that represent two opposing concepts. We extend this paradigm to BERT embeddings, and construct contextualized axes that mitigate the pitfall where antonyms have neighboring representations. We validate and demonstrate these axes on two people-centric datasets: occupations from Wikipedia, and multi-platform discussions in extremist, men’s communities over fourteen years. In both studies, contextualized semantic axes can characterize differences among instances of the same word type. In the latter study, we show that references to women and the contexts around them have become more detestable over time.;1;;;2;does not mention bias explicitly but seems to be concerned strongly with bias;"""we show that references to women and the contexts around them have become more detestable over time""";1;Not sure - is BERT an LLM? I would maybe still count it as an LLM since there are still a lot of discussions of BERT-like models in 2022 ;;1;;;1;emnlp2022;December 2022
Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection;Detecting fake news requires both a delicate sense of diverse clues and a profound understanding of the real-world background, which remains challenging for detectors based on small language models (SLMs) due to their knowledge and capability limitations. Recent advances in large language models (LLMs) have shown remarkable performance in various tasks, but whether and how LLMs could help with fake news detection remains underexplored. In this paper, we investigate the potential of LLMs in fake news detection. First, we conduct an empirical study and find that a sophisticated LLM such as GPT 3.5 could generally expose fake news and provide desirable multi-perspective rationales but still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis attributes such a gap to the LLM's inability to select and integrate rationales properly to conclude. Based on these findings, we propose that current LLMs may not substitute fine-tuned SLMs in fake news detection but can be a good advisor for SLMs by providing multi-perspective instructive rationales. To instantiate this proposal, we design an adaptive rationale guidance network for fake news detection (ARG), in which SLMs selectively acquire insights on news analysis from the LLMs' rationales. We further derive a rationale-free version of ARG by distillation, namely ARG-D, which services cost-sensitive scenarios without querying LLMs. Experiments on two real-world datasets demonstrate that ARG and ARG-D outperform three types of baseline methods, including SLM-based, LLM-based, and combinations of small and large language models.;3;The paper deals with fake news detection and the limits of current models, the main focus is how to combine the models to improve the results.;"""which remains challenging for detectors based on small language models (SLMs) due to their knowledge and capability limitations."", ""LLM such as GPT 3.5 could generally expose fake news and provide desirable multi-perspective rationales but still underperforms the basic SLM, fine-tuned BERT."", ""Our subsequent analysis attributes such a gap to the LLM's inability to select and integrate rationales properly to conclude.""";2;mentions limitations in fake news detection but mostly focuses on methods to improve;"""current LLMs may not substitute fine-tuned SLMs in fake news detection""";4;Discusses fake news detection as a challenge for LLMs especially when compared with small LMs, but the main focus is ;"""Detecting fake news requires both a delicate sense of diverse clues and a
profound understanding of the real-world background, which remains challenging
for detectors based on small language models (SLMs) due to their knowledge and
capability limitations"", ""a sophisticated LLM such as GPT 3.5 could
generally expose fake news and provide desirable multi-perspective rationales
but still underperforms the basic SLM, fine-tuned BERT"", ""Our subsequent analysis
attributes such a gap to the LLM's inability to select and integrate rationales
properly to conclude."", ""Based on these findings, we propose that current LLMs may
not substitute fine-tuned SLMs in fake news detection""";3;limitations are mentioned, a few, but maybe not in the required depth for 4;;3;arxiv;21 September 2023
Generating Coherent Drum Accompaniment With Fills And Improvisations;Creating a complex work of art like music necessitates profound creativity.  With recent advancements in deep learning and powerful models such as  transformers, there has been huge progress in automatic music generation. In an  accompaniment generation context, creating a coherent drum pattern with  apposite fills and improvisations at proper locations in a song is a  challenging task even for an experienced drummer. Drum beats tend to follow a  repetitive pattern through stanzas with fills or improvisation at section  boundaries. In this work, we tackle the task of drum pattern generation  conditioned on the accompanying music played by four melodic instruments:  Piano, Guitar, Bass, and Strings. We use the transformer sequence to sequence  model to generate a basic drum pattern conditioned on the melodic accompaniment  to find that improvisation is largely absent, attributed possibly to its  expectedly relatively low representation in the training data. We propose a  novelty function to capture the extent of improvisation in a bar relative to  its neighbors. We train a model to predict improvisation locations from the  melodic accompaniment tracks. Finally, we use a novel BERT-inspired in-filling  architecture, to learn the structure of both the drums and melody to in-fill  elements of improvised music.;0;The paper deals with transformers, but not for language tasks.;;1;;;1;Maybe 0 if we do not include BERT into definition;;1;catastrophic forgetting only mentioned en passant;;1;arxiv;01 September 2022
Unlocking Adversarial Suffix Optimization Without Affirmative Phrases: Efficient Black-box Jailbreaking via LLM as Optimizer;Despite prior safety alignment efforts, mainstream LLMs can still generate  harmful and unethical content when subjected to jailbreaking attacks. Existing  jailbreaking methods fall into two main categories: template-based and  optimization-based methods. The former requires significant manual effort and  domain knowledge, while the latter, exemplified by Greedy Coordinate Gradient  (GCG), which seeks to maximize the likelihood of harmful LLM outputs through  token-level optimization, also encounters several limitations: requiring  white-box access, necessitating pre-constructed affirmative phrase, and  suffering from low efficiency. In this paper, we present ECLIPSE, a novel and  efficient black-box jailbreaking method utilizing optimizable suffixes. Drawing  inspiration from LLMs' powerful generation and optimization capabilities, we  employ task prompts to translate jailbreaking goals into natural language  instructions. This guides the LLM to generate adversarial suffixes for  malicious queries. In particular, a harmfulness scorer provides continuous  feedback, enabling LLM self-reflection and iterative optimization to  autonomously and efficiently produce effective suffixes. Experimental results  demonstrate that ECLIPSE achieves an average attack success rate (ASR) of 0.92  across three open-source LLMs and GPT-3.5-Turbo, significantly surpassing GCG  in 2.4 times. Moreover, ECLIPSE is on par with template-based methods in ASR  while offering superior attack efficiency, reducing the average attack overhead  by 83%.;2;The paper deals with the topic of jailbreaking LLMs, but the main focus is on the new method proposed and not on the limitations due to the jailbreak.;"""Despite prior safety alignment efforts, mainstream LLMs can still generate  harmful and unethical content when subjected to jailbreaking attacks."", ""This guides the LLM to generate adversarial suffixes for  malicious queries.""";3;entirely focused on adversarial/jailbreaking attacks;"""mainstream LLMs can still generate  harmful and unethical content when subjected to jailbreaking attacks"", ""average attack success rate (ASR) of 0.92  across three open-source LLMs and GPT-3.5-Turbo""";3;Mentions a limitation in jailbreaking attacks but it is not as central as a solution;"""Despite prior safety alignment efforts, mainstream LLMs can still generate
 harmful and unethical content when subjected to jailbreaking attacks.""";3;not strong enough to warrant 4, in my opinion;;3;arxiv;21 August 2024
AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies;Humans regularly engage in analogical thinking, relating personal experiences to current situations ($X$ is analogous to $Y$ because of $Z$). Analogical thinking allows humans to solve problems in creative ways, grasp difficult concepts, and articulate ideas more effectively. Can language models (LMs) do the same? To answer this question, we propose ANALOBENCH, a benchmark to determine analogical reasoning ability in LMs. Our benchmarking approach focuses on aspects of this ability that are common among humans: (i) recalling related experiences from a large amount of information, and (ii) applying analogical reasoning to complex and lengthy scenarios. We test a broad collection of proprietary models (e.g., GPT family, Claude V2) and open source models such as LLaMA2. As in prior results, scaling up LMs results in some performance boosts. Surprisingly, scale offers minimal gains when, (i) analogies involve lengthy scenarios, or (ii) recalling relevant scenarios from a large pool of information, a process analogous to finding a needle in a haystack. We hope these observations encourage further research in this field.;1;The paper introduces a new benchmark to measure the ability for analogical reasoning in LLMs without refering to limitations of them.;;4;identifies multiple limitations with a novel analogical reasoning benchmark;"""scale offers minimal gains when, (i) analogies involve lengthy scenarios, or (ii) recalling relevant scenarios from a large pool of information""";4;"Adjusted: previosuly 5 

Discusses analogical reasoning as a limitation, and it is investigated quite extensively, even though it is done in the context of developing a benchmark";"""Surprisingly, scale offers minimal gains when, (i)
analogies involve lengthy scenarios, or (ii) recalling relevant scenarios from
a large pool of information, a process analogous to finding a needle in a
haystack""";4;ok, maybe this is enough to pass the barrier, but I'd rate it more as 3.5 actually;;3;emnlp2024;November 2024
Zero-shot cross-lingual transfer in instruction tuning of large language model;Instruction tuning (IT) is widely used to teach pretrained large language models (LLMs) to follow arbitrary instructions, but is under-studied in multilingual settings. In this work, we conduct a systematic study of zero-shot cross-lingual transfer in IT, when an LLM is instruction-tuned on English-only data and then tested on user prompts in other languages. We investigate the influence of model configuration choices and devise a multi-facet evaluation strategy for multilingual instruction following. We find that cross-lingual transfer does happen successfully in IT even if all stages of model training are English-centric, but only if multiliguality is taken into account in hyperparameter tuning and with large enough IT data. English-trained LLMs are capable of generating correct-language, comprehensive and helpful responses in the other languages, but suffer from low factuality and may occasionally have fluency errors.;4;The paper deals with limitations of cross-lingual transfer in LLMs;"""but suffer from low factuality and may occasionally have fluency errors.""";2;mentions limitations in multilingual instruction tuning but only in passing;"""cross-lingual transfer does happen successfully in IT""";3;A limitation is well-defined (cross-lingual transfer in IT of LLMs), but it is not discussed extensively. Just explored to some extent ;"""cross-lingual
transfer"", ""in IT"", ""but only if multiliguality is taken into account in
hyperparameter tuning and with large enough IT data"", ""but suffer from low factuality and may occasionally have
fluency errors.""";2;2-3. It outlines limitations but also strengths;;3;arxiv;22 February 2024
HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification;Hierarchical text classification (HTC) is a challenging subtask of  multi-label classification due to its complex label hierarchy. Recently, the  pretrained language models (PLM)have been widely adopted in HTC through a  fine-tuning paradigm. However, in this paradigm, there exists a huge gap  between the classification tasks with sophisticated label hierarchy and the  masked language model (MLM) pretraining tasks of PLMs and thus the potentials  of PLMs can not be fully tapped. To bridge the gap, in this paper, we propose  HPT, a Hierarchy-aware Prompt Tuning method to handle HTC from a multi-label  MLM perspective. Specifically, we construct a dynamic virtual template and  label words that take the form of soft prompts to fuse the label hierarchy  knowledge and introduce a zero-bounded multi-label cross entropy loss to  harmonize the objectives of HTC and MLM. Extensive experiments show HPT  achieves state-of-the-art performances on 3 popular HTC datasets and is adept  at handling the imbalance and low resource situations. Our code is available at  https://github.com/wzh9969/HPT.;2;The paper deals limitations of masked language models, but the main focus is on the proposed solution.;"""However, in this paradigm, there exists a huge gap  between the classification tasks with sophisticated label hierarchy and the  masked language model (MLM) pretraining tasks of PLMs and thus the potentials  of PLMs can not be fully tapped.""";2;mentions limitations for hierarchical text classification;"""huge gap  between the classification tasks with sophisticated label hierarchy and the  masked language model (MLM) pretraining tasks of PLMs""";1;I think it mentions mostly a limitation of a task;;2;not a thorough discussion of limitations. Only states that the pretraining task differs from certain classification tasks, 1 or 2;;2;emnlp2022;December 2022
SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables;Current scientific fact-checking benchmarks exhibit several shortcomings, such as biases arising from crowd-sourced claims and an over-reliance on text-based evidence. We present SCITAB, a challenging evaluation dataset consisting of 1.2K expert-verified scientific claims that 1) originate from authentic scientific publications and 2) require compositional reasoning for verification. The claims are paired with evidence-containing scientific tables annotated with labels. Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of-Thought, do not achieve much performance gains on SCITAB. Our analysis uncovers several unique challenges posed by SCITAB, including table grounding, claim ambiguity, and compositional reasoning. Our codes and data are publicly available at https://github.com/XinyuanLu00/SciTab.;2;The paper proposes a new benchmark to fact-check LLMs.;"""Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of-Thought, do not achieve much performance gains on SCITAB. Our analysis uncovers several unique challenges posed by SCITAB, including table grounding, claim ambiguity, and compositional reasoning.""";5;entirely focused on evaluating compositional reasoning limitations in LLMs;"""we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of-Thought, do not achieve much performance gains on SCITAB. Our analysis uncovers several unique challenges posed by SCITAB, including table grounding, claim ambiguity, and compositional reasoning""";4;The main focus is developing a benchmark, closing a gap in some evaluation techniques. But they find that LLMs struggle in scientific claims ;"""SCITAB poses a significant challenge to state-of-the-art models, including
table-based pretraining models and large language models. All models except
GPT-4 achieved performance barely above random guessing. Popular prompting
techniques, such as Chain-of-Thought, do not achieve much performance gains on
SCITAB. Our analysis uncovers several unique challenges posed by SCITAB,
including table grounding, claim ambiguity, and compositional reasoning.""";4;3-4. limitations mentioned, borderline though as other models are also included;;4;emnlp2023;December 2023
ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on Historical American Newspaper Pages;Question answering (QA) and Machine Reading Comprehension (MRC) tasks have significantly advanced in recent years due to the rapid development of deep learning techniques and, more recently, large language models. At the same time, many benchmark datasets have become available for QA and MRC tasks. However, most existing large-scale benchmark datasets have been created predominantly using synchronous document collections like Wikipedia or the Web. Archival document collections, such as historical newspapers, contain valuable information from the past that is still not widely used to train large language models. To further contribute to advancing QA and MRC tasks and to overcome the limitation of previous datasets, we introduce ChroniclingAmericaQA, a large-scale dataset with 485K question-answer pairs created based on the historical newspaper collection Chronicling America. Our dataset is constructed from a subset of the Chronicling America newspaper collection spanning 120 years. One of the significant challenges for utilizing digitized historical newspaper collections is the low quality of OCR text. Therefore, to enable realistic testing of QA models, our dataset can be used in three different ways: answering questions from raw and noisy content, answering questions from cleaner, corrected version of the content, as well as answering questions from scanned images of newspaper pages. This and the fact that ChroniclingAmericaQA spans the longest time period among available QA datasets make it quite a unique and useful resource.;1;A benchmark dataset is presented, but no limitations of LLMs are mentioned.;;2;mentions limitations in historical focus but mostly focuses on the advantages of a newly developed data set;"""valuable information from the past that is still not widely used to train large language models""";1;The focus is on the dataset entirely;;1;;;1;arxiv;26 March 2024
DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models;Large language models (LLMs) have demonstrated emergent capabilities across diverse reasoning tasks via popular Chains-of-Thought (COT) prompting. However, such a simple and fast COT approach often encounters limitations in dealing with complicated problems, while a thorough method, which considers multiple reasoning pathways and verifies each step carefully, results in slower inference. This paper addresses the challenge of enabling LLMs to autonomously select between fast and slow inference methods, thereby optimizing both efficiency and effectiveness. We introduce a dynamic decision-making framework that categorizes tasks into two distinct pathways: ‘Fast,’ designated for tasks where the LLM quickly identifies a high-confidence solution, and ‘Slow,’ allocated for tasks that the LLM perceives as complex and for which it has low confidence in immediate solutions as well as requiring more reasoning paths to verify. Experiments on five popular reasoning benchmarks demonstrated the superiority of the DynaThink over baselines. For example, when we compared it to strong COT with self-consistency baseline on the complicated MATH dataset, DynaThink achieved more than 3% increase in accuracy with lower cost. The code will be made available upon publication.;3;This paper deals with limitation of LLMs to deal with complicated problems and proposes a solution do overcome it.;"""However, such a simple and fast COT approach often encounters limitations in dealing with complicated problems, while a thorough method, which considers multiple reasoning pathways and verifies each step carefully, results in slower inference.""";3;mentions computational and reasoning limitations;"""limitations in dealing with complicated problems"", ""challenge of enabling LLMs to autonomously select between fast and slow inference methods""";2;Mostly focused on a method to elicit reasoning in LLMs, just briefly mentions limitations (where LLMs gives a low confidence score), but not in the context we want;"""However, such a simple and fast COT approach often encounters limitations in dealing with complicated problems, while a thorough method, which considers multiple reasoning pathways and verifies each step carefully, results in slower inference.""";3;2-3;;3;emnlp2024;November 2024
Pyramid-BERT: Reducing Complexity via Successive Core-set based Token Selection;Transformer-based language models such as BERT (CITATION) have achieved the state-of-the-art performance on various NLP tasks, but are computationally prohibitive. A recent line of works use various heuristics to successively shorten sequence length while transforming tokens through encoders, in tasks such as classification and ranking that require a single token embedding for prediction. We present a novel solution to this problem, called Pyramid-BERT where we replace previously used heuristics with a core-set based token selection method justified by theoretical results. The core-set based token selection technique allows us to avoid expensive pre-training, gives a space-efficient fine tuning, and thus makes it suitable to handle longer sequence lengths. We provide extensive experiments establishing advantages of pyramid BERT over several baselines and existing works on the GLUE benchmarks and Long Range Arena (CITATION) datasets.;2;;"""but are computationally prohibitive.""";2;computational limitations;"""computationally prohibitive""";2;Computational costs of BERT, which I would not count as a limitation of LLMs;"""Transformer-based language models such as BERT (CITATION)"", ""are computationally prohibitive.""";2;1.5 maybe;;2;acl2022;May 2022
ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models;Large language models (LLMs) have made significant progress in NLP. However, their ability to memorize, represent, and leverage commonsense knowledge has been a well-known pain point. In this paper, we specifically focus on ChatGPT, a widely used and easily accessible LLM, and ask the following questions: (1) Can ChatGPT effectively answer commonsense questions? (2) Is ChatGPT aware of the underlying commonsense knowledge for answering a specific question? (3) Is ChatGPT knowledgeable in commonsense? (4) Can ChatGPT effectively leverage commonsense for answering questions? We conduct a series of experiments on 11 datasets to evaluate ChatGPT's commonsense abilities, including answering commonsense questions, identifying necessary knowledge, generating knowledge descriptions, and using knowledge descriptions to answer questions again. Experimental results show that: (1) ChatGPT can achieve good QA accuracies in commonsense tasks, while still struggling with certain domains of datasets. (2) ChatGPT is knowledgeable, and can accurately generate most of the commonsense knowledge using knowledge prompts. (3) Despite its knowledge, ChatGPT is an inexperienced commonsense problem solver, which cannot precisely identify the needed commonsense for answering a specific question. These findings raise the need to explore improved mechanisms for effectively incorporating commonsense into LLMs like ChatGPT, such as better instruction following and commonsense guidance.;3;This paper focuses on investigating the commonsense knowledge of ChatGPT and points out that it performs good overall with some minor struggles.;"""while still struggling with certain domains of datasets."", ""ChatGPT is an inexperienced commonsense problem solver, which cannot precisely identify the needed commonsense for answering a specific question.""";4;investigates multiple limitations in terms of common sense reasoning;"""their ability to memorize, represent, and leverage commonsense knowledge has been a well-known pain point"", ""Despite its knowledge, ChatGPT is an inexperienced commonsense problem solver, which cannot precisely identify the needed commonsense for answering a specific question"", ""need to explore improved mechanisms for effectively incorporating commonsense into LLMs like ChatGPT, such as better instruction following and commonsense guidance""";5;Extensive evaluation of ChatGPT in commonsense knowledge and exploration of this limitation;"""However,
their ability to memorize, represent, and leverage commonsense knowledge has
been a well-known pain point."", ""ChatGPT"", ""still struggling with certain domains of datasets."", ""Despite its knowledge, ChatGPT is an
inexperienced commonsense problem solver, which cannot precisely identify the
needed commonsense for answering a specific question.""";4;3 to 4, as other aspects are also included;;4;arxiv;29 March 2023
RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models;The application scope of large language models (LLMs) is increasingly expanding. In practical use, users might provide feedback based on the model's output, hoping for a responsive model that can complete responses according to their feedback. Whether the model can appropriately respond to users' refuting feedback and consistently follow through with execution has not been thoroughly analyzed. In light of this, this paper proposes a comprehensive benchmark, RefuteBench, covering tasks such as question answering, machine translation, and email writing. The evaluation aims to assess whether models can positively accept feedback in form of refuting instructions and whether they can consistently adhere to user demands throughout the conversation. We conduct evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit inclination to their internal knowledge, often failing to comply with user feedback. Additionally, as the length of the conversation increases, models gradually forget the user's stated feedback and roll back to their own responses. We further propose a recall-and-repeat prompts as a simple and effective way to enhance the model's responsiveness to feedback.;2;This paper introduces a new benchmark to measure how well LLMs respond to the users feedback and concludes that there are limitations in the LLMs ability to do so.;"""We conduct evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit inclination to their internal knowledge, often failing to comply with user feedback. Additionally, as the length of the conversation increases, models gradually forget the user's stated feedback and roll back to their own responses.""";4;Investigates limitations in multi-step interactions, especially reaction to feedback, revelealing 'stubornness' and forgetting early stages of the interaction;"""LLMs are stubborn, i.e. exhibit inclination to their internal knowledge, often failing to comply with user feedback. Additionally, as the length of the conversation increases, models gradually forget the user's stated feedback and roll back to their own responses""";4;"Very detailed discussion of LLms in their ""stubborness"", their abilitity to follow the feedback, but the focus is also is greatly on the benchmark they develop and suggestions to mitigate this";"""We conduct
evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit
inclination to their internal knowledge, often failing to comply with user
feedback. Additionally, as the length of the conversation increases, models
gradually forget the user's stated feedback and roll back to their own
responses.""";4;4 to 5;;4;arxiv;21 February 2024
MathPrompter: Mathematical Reasoning using Large Language Models;Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose `MathPrompter', a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple Algebraic expressions or Python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the MultiArith dataset ($78.7\%\rightarrow92.5\%$) evaluated using 175B parameter GPT-based LLM.;3;This paper deals with the limitations of LLMs solving math problems and proposes a new prompting technique to deal with this issue.;"""Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs.""";2;"mostly focused on a new prompting technique for math problems; mentions arithmetic reasoning limitations at the start";"""Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers""";2;"ADJUSTED: previously 3 

Limitation in mathematical reasoning is mentioned, but not discussed extensively. Focus is on the solution of the problem";"""Large Language Models (LLMs) have limited performance when solving arithmetic
reasoning tasks and often provide incorrect answers."", ""we are not aware of any LLMs that indicate their level of
confidence in their responses which fuels a trust deficit in these models
impeding their adoption.""";2;2 to 3;;2;acl2023;July 2023
Pre-trained Language Models for Keyphrase Generation: A Thorough Empirical Study;"Neural models that do not rely on pre-training have excelled in the keyphrase  generation task with large annotated datasets. Meanwhile, new approaches have  incorporated pre-trained language models (PLMs) for their data efficiency.  However, there lacks a systematic study of how the two types of approaches  compare and how different design choices can affect the performance of  PLM-based models. To fill in this knowledge gap and facilitate a more informed  use of PLMs for keyphrase extraction and keyphrase generation, we present an  in-depth empirical study. Formulating keyphrase extraction as sequence labeling  and keyphrase generation as sequence-to-sequence generation, we perform  extensive experiments in three domains. After showing that PLMs have  competitive high-resource performance and state-of-the-art low-resource  performance, we investigate important design choices including in-domain PLMs,  PLMs with different pre-training objectives, using PLMs with a parameter  budget, and different formulations for present keyphrases. Further results show  that (1) in-domain BERT-like PLMs can be used to build strong and  data-efficient keyphrase generation models; (2) with a fixed parameter budget,  prioritizing model depth over width and allocating more layers in the encoder  leads to better encoder-decoder models; and (3) introducing four in-domain  PLMs, we achieve a competitive performance in the news domain and the  state-of-the-art performance in the scientific domain.";1;This paper deals with the ability of LLMs to deal with the keyphrase generation task. It does not mention any limitations regarding LLMs.;;1;appears to be pure LLM research without mentioning limitations;;1;The focus is on the LMs application;;1;no limitations are discussed;;1;arxiv;20 December 2022
Cross-Language Assessment of Mathematical Capability of ChatGPT;This paper presents an evaluation of the mathematical capability of ChatGPT  across diverse languages like Hindi, Gujarati, and Marathi. ChatGPT, based on  GPT-3.5 by OpenAI, has garnered significant attention for its natural language  understanding and generation abilities. However, its performance in solving  mathematical problems across multiple natural languages remains a comparatively  unexplored area, especially in regional Indian languages. In this paper, we  explore those capabilities as well as using chain-of-thought prompting to  figure out if it increases the accuracy of responses as much as it does in the  English language and provide insights into the current limitations.;4;This paper investigates limitations of LLMs in solving math tasks, but the results are not mentioned, which makes it hard to give a label.;;2;explicitly focuses on studying cross-language capabilties in mathematical problem solving but mentions limitations only in passing;"""insights into the current limitations""";2;"ADJUSTED: previously 5 

Fully focused on mathematical reasoning capabilities of LLMs in different languages ";"""provide insights into the current limitations.""";2;it ends with limitations, but none are strongly discussed;;3;arxiv;18 May 2024
Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of Question Answering Models;While the Large Language Models (LLMs) dominate a majority of language understanding tasks, previous work shows that some of these results are supported by modelling spurious correlations of training datasets. Authors commonly assess model robustness by evaluating their models on out-of-distribution (OOD) datasets of the same task, but these datasets might share the bias of the training dataset. We propose a simple method for measuring a scale of models’ reliance on any identified spurious feature and assess the robustness towards a large set of known and newly found prediction biases for various pre-trained models and debiasing methods in Question Answering (QA). We find that the reported OOD gains of debiasing methods can not be explained by mitigated reliance on biased features, suggesting that biases are shared among different QA datasets. We further evidence this by measuring that performance of OOD models depends on bias features comparably to the ID model. Our findings motivate future work to refine the reports of LLMs’ robustness to a level of known spurious features.;3;This paper deals with limits of LLMs due to spurious correlations in training datasets, but the main focus is on the datasets and their biases.;"""previous work shows that some of these results are supported by modelling spurious correlations of training datasets.""";5;Focuses entirely on the limitations of LLMs for QA in terms of training data sets and spurious features;"""these results are supported by modelling spurious correlations of training datasets"", ""these datasets might share the bias of the training dataset"", ""newly found prediction biases for various pre-trained models and debiasing methods"", ""We find that the reported OOD gains of debiasing methods can not be explained by mitigated reliance on biased features, suggesting that biases are shared among different QA datasets"", ""performance of OOD models depends on bias features comparably to the ID model""";2;"ADJUSTED: previosly 3

Does discuss a limitation (LLMs reliance on spurious correlations in train datasets) but it is more a limitation of a dataset not LLM";"""While the Large Language Models (LLMs) dominate a majority of language understanding tasks, previous work shows that some of these results are supported by modelling spurious correlations of training datasets.""";2;;;3;eacl2024;March 2024
Graph Enhanced BERT for Query Understanding;Query understanding plays a key role in exploring users' search intents and  facilitating users to locate their most desired information. However, it is  inherently challenging since it needs to capture semantic information from  short and ambiguous queries and often requires massive task-specific labeled  data. In recent years, pre-trained language models (PLMs) have advanced various  natural language processing tasks because they can extract general semantic  information from large-scale corpora. Therefore, there are unprecedented  opportunities to adopt PLMs for query understanding. However, there is a gap  between the goal of query understanding and existing pre-training strategies --  the goal of query understanding is to boost search performance while existing  strategies rarely consider this goal. Thus, directly applying them to query  understanding is sub-optimal. On the other hand, search logs contain user  clicks between queries and urls that provide rich users' search behavioral  information on queries beyond their content. Therefore, in this paper, we aim  to fill this gap by exploring search logs. In particular, to incorporate search  logs into pre-training, we first construct a query graph where nodes are  queries and two queries are connected if they lead to clicks on the same urls.  Then we propose a novel graph-enhanced pre-training framework, GE-BERT, which  can leverage both query content and the query graph. In other words, GE-BERT  can capture both the semantic information and the users' search behavioral  information of queries. Extensive experiments on various query understanding  tasks have demonstrated the effectiveness of the proposed framework.;2;The paper deals with solving the task of query understanding and proposes a method how to adapt the training process of LLMs to deal with this task.;"""However, there is a gap  between the goal of query understanding and existing pre-training strategies --  the goal of query understanding is to boost search performance while existing  strategies rarely consider this goal. Thus, directly applying them to query  understanding is sub-optimal.""";2;mentions shortcomings of LLM training for search, but is mostly method focused;"""there is a gap  between the goal of query understanding and existing pre-training strategies""";1;It's more of a discussion of an application, not a limitation;;2;only a small gap between pretraining and task at hand is discussed;;2;arxiv;03 April 2022
Explaining Interactions Between Text Spans;Reasoning over spans of tokens from different parts of the input is essential for natural language understanding (NLU) tasks such as fact-checking (FC), machine reading comprehension (MRC) or natural language inference (NLI). However, existing highlight-based explanations primarily focus on identifying individual important tokens or interactions only between adjacent tokens or tuples of tokens. Most notably, there is a lack of annotations capturing the human decision-making process w.r.t. the necessary interactions for informed decision-making in such tasks. To bridge this gap, we introduce SpanEx, a multi-annotator dataset of human span interaction explanations for two NLU tasks: NLI and FC. We then investigate the decision-making processes of multiple fine-tuned large language models in terms of the employed connections between spans in separate parts of the input and compare them to the human reasoning processes. Finally, we present a novel community detection based unsupervised method to extract such interaction explanations from a model's inner workings.;2;;"""However, existing highlight-based explanations primarily focus on identifying individual important tokens or interactions only between adjacent tokens or tuples of tokens.""";2;mentions explainability limitations and limitations in reasoning over distant dependencies but mostly focuses on a new data set and method;"""existing highlight-based explanations primarily focus on identifying individual important tokens or interactions only between adjacent tokens or tuples of tokens""";1;Discusses a limitation of a task / approaches to a task;;2;;;2;emnlp2023;December 2023
Jump Starting Bandits with LLM-Generated Prior Knowledge;We present substantial evidence demonstrating the benefits of integrating Large Language Models (LLMs) with a Contextual Multi-Armed Bandit framework. Contextual bandits have been widely used in recommendation systems to generate personalized suggestions based on user-specific contexts. We show that LLMs, pre-trained on extensive corpora rich in human knowledge and preferences, can simulate human behaviours well enough to jump-start contextual multi-armed bandits to reduce online learning regret. We propose an initialization algorithm for contextual bandits by prompting LLMs to produce a pre-training dataset of approximate human preferences for the bandit. This significantly reduces online learning regret and data-gathering costs for training such models. Our approach is validated empirically through two sets of experiments with different bandit setups: one which utilizes LLMs to serve as an oracle and a real-world experiment utilizing data from a conjoint survey experiment.;1;;;1;;;1;Discusses an application of LLMs;;1;;;1;emnlp2024;November 2024
Predictive Querying for Autoregressive Neural Sequence Models;"In reasoning about sequential events it is natural to pose probabilistic  queries such as ""when will event A occur next"" or ""what is the probability of A  occurring before B"", with applications in areas such as user modeling,  medicine, and finance. However, with machine learning shifting towards neural  autoregressive models such as RNNs and transformers, probabilistic querying has  been largely restricted to simple cases such as next-event prediction. This is  in part due to the fact that future querying involves marginalization over  large path spaces, which is not straightforward to do efficiently in such  models. In this paper we introduce a general typology for predictive queries in  neural autoregressive sequence models and show that such queries can be  systematically represented by sets of elementary building blocks. We leverage  this typology to develop new query estimation methods based on beam search,  importance sampling, and hybrids. Across four large-scale sequence datasets  from different application domains, as well as for the GPT-2 language model, we  demonstrate the ability to make query answering tractable for arbitrary queries  in exponentially-large predictive path-spaces, and find clear differences in  cost-accuracy tradeoffs between search and sampling methods.";1;;;2;mentions computational/reasoning limitations in the context of LLMs but is mostly method-focused on resolving them;"""probabilistic querying has  been largely restricted to simple cases such as next-event prediction. This is  in part due to the fact that future querying involves marginalization over  large path spaces, which is not straightforward to do efficiently in such  models""";2;A limitation is mentioned (reasoning about probabilistic queries about sequential events) but the central focus is on the typology and developing solutions;"""However, with machine learning shifting towards neural
 autoregressive models such as RNNs and transformers, probabilistic querying has
 been largely restricted to simple cases such as next-event prediction. This is
 in part due to the fact that future querying involves marginalization over
 large path spaces, which is not straightforward to do efficiently in such
 models.""";2;"""not straightforward to do efficiently in such models""";;2;arxiv;12 October 2022
Self-imitation Learning for Action Generation in Text-based Games;In this work, we study reinforcement learning (RL) in solving text-based games. We address the challenge of combinatorial action space, by proposing a confidence-based self-imitation model to generate action candidates for the RL agent. Firstly, we leverage the self-imitation learning to rank and exploit past valuable trajectories to adapt a pre-trained language model (LM) towards a target game. Then, we devise a confidence-based strategy to measure the LM’s confidence with respect to a state, thus adaptively pruning the generated actions to yield a more compact set of action candidates. In multiple challenging games, our model demonstrates promising performance in comparison to the baselines.;0;;;1;;;1;Focus on an application;;1;;;1;eacl2023;May 2023
PromptBoosting: Black-Box Text Classification with Ten Forward Passes;"We describe PromptBoosting, a query-efficient procedure for building a text  classifier from a neural language model (LM) without access to the LM's  parameters, gradients, or hidden representations. This form of ""black-box""  classifier training has become increasingly important as the cost of training  and inference in large-scale LMs grows. But existing black-box LM classifier  learning approaches are themselves computationally inefficient, typically  specializing LMs to the target task by searching in a large space of (discrete  or continuous) prompts using zeroth-order optimization methods. Instead of  directly optimizing in prompt space, PromptBoosting obtains a small pool of  prompts via a gradient-free approach and then constructs a large pool of weak  learners by pairing these prompts with different elements of the LM's output  distribution. These weak learners are then ensembled using the AdaBoost  algorithm. The entire learning process requires only a small number of forward  passes and no backward pass. Experiments show that PromptBoosting achieves  state-of-the-art performance in multiple black-box few-shot classification  tasks, and matches or outperforms full fine-tuning in both few-shot and  standard learning paradigms, while training 10x faster than existing black-box  methods.";2;;"""But existing black-box LM classifier  learning approaches are themselves computationally inefficient, typically  specializing LMs to the target task by searching in a large space of (discrete  or continuous) prompts using zeroth-order optimization methods.""";2;focuses on computational limitations of LLMs, esp. training time;"""cost of training  and inference in large-scale LMs grows"", ""computationally inefficient""";2;"""But existing black-box LM classifier
 learning approaches are themselves computationally inefficient"" - just a brief mention of a limitation, the whole focus is on the approach PromptBoosting";"""the cost of training
 and inference in large-scale LMs grows.""";2;only a very small mention of costly inference;;2;arxiv;19 December 2022
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese;Large Language Models (LLMs) are increasingly being used to generate synthetic data for training and evaluating models. However, it is unclear whether they can generate a good quality of question answering (QA) dataset that incorporates knowledge and cultural nuance embedded in a language, especially for low-resource languages. In this study, we investigate the effectiveness of using LLMs in generating culturally relevant commonsense QA datasets for Indonesian and Sundanese languages. To do so, we create datasets for these languages using various methods involving both LLMs and human annotators, resulting in 4.5K questions per language (9K in total), making our dataset the largest of its kind. Our experiments show that automatic data adaptation from an existing English dataset is less effective for Sundanese. Interestingly, using the direct generation method on the target language, GPT-4 Turbo can generate questions with adequate general knowledge in both languages, albeit not as culturally ‘deep’ as humans. We also observe a higher occurrence of fluency errors in the Sundanese dataset, highlighting the discrepancy between medium- and lower-resource languages.;4;This paper investigates the ability of LLMs to generate a question answering dataset for different langugages. The authors conclude that the LLM has fluency errors and is rather superficial.;"""However, it is unclear whether they can generate a good quality of question answering (QA) dataset that incorporates knowledge and cultural nuance embedded in a language, especially for low-resource languages."", ""Our experiments show that automatic data adaptation from an existing English dataset is less effective for Sundanese."", ""albeit not as culturally ‘deep’ as humans."", ""We also observe a higher occurrence of fluency errors in the Sundanese dataset, highlighting the discrepancy between medium- and lower-resource languages.""";3;extensively consideres cross-language transfer limitations via new benchmark data sets;"""automatic data adaptation from an existing English dataset is less effective for Sundanese"", ""higher occurrence of fluency errors in the Sundanese dataset, highlighting the discrepancy between medium- and lower-resource languages""";4;Although the results are not super negative, this study is fully focused on LLMs evaluation in a generation task with culture awareness (also multilingual component);"""Our experiments show that automatic data adaptation from an existing English dataset is less effective for Sundanese. Interestingly, using the direct generation method on the target language, GPT-4 Turbo can generate questions with adequate general knowledge in both languages, albeit not as culturally ‘deep’ as humans. We also observe a higher occurrence of fluency errors in the Sundanese dataset, highlighting the discrepancy between medium- and lower-resource languages.""";3;"""not as deep as humans"" and some other smaller parts";;4;emnlp2024;November 2024
WinoDict: Probing language models for in-context word acquisition;We introduce a new in-context learning paradigm to measure Large Language Models' (LLMs) ability to learn novel words during inference. In particular, we rewrite Winograd-style co-reference resolution problems by replacing the key concept word with a synthetic but plausible word that the model must understand to complete the task. Solving this task requires the model to make use of the dictionary definition of the new word given in the prompt. This benchmark addresses word acquisition, one important aspect of the diachronic degradation known to afflict LLMs. As LLMs are frozen in time at the moment they are trained, they are normally unable to reflect the way language changes over time. We show that the accuracy of LLMs compared to the original Winograd tasks decreases radically in our benchmark, thus identifying a limitation of current models and providing a benchmark to measure future improvements in LLMs ability to do in-context learning.;5;This paper deals with the limitations of LLMs to learn new words as they are frozen in time.;"""This benchmark addresses word acquisition, one important aspect of the diachronic degradation known to afflict LLMs. As LLMs are frozen in time at the moment they are trained, they are normally unable to reflect the way language changes over time. We show that the accuracy of LLMs compared to the original Winograd tasks decreases radically in our benchmark, thus identifying a limitation of current models and providing a benchmark to measure future improvements in LLMs ability to do in-context learning.""";4;mostly mentions new benchmark but this benchmark is specifically designed to investigate a specific limitation, namely introducing novel words (outside the training data) in Winograd tasks;"""normally unable to reflect the way language changes over time. We show that the accuracy of LLMs compared to the original Winograd tasks decreases radically in our benchmark, thus identifying a limitation of current models""";5;Fully explores the ability of LLMs to learn novel words during inference;"""This benchmark addresses word acquisition, one important aspect of the diachronic degradation known to afflict LLMs. As LLMs are frozen in time at the moment they are trained, they are normally unable to reflect the way language changes over time. We show that the accuracy of LLMs compared to the original Winograd tasks decreases radically in our benchmark, thus identifying a limitation of current models and providing a benchmark to measure future improvements in LLMs ability to do in-context learning.""";4;3 to 4;;5;eacl2023;May 2023
Expand, Highlight, Generate: RL-driven Document Generation for Passage Reranking;Generating synthetic training data based on large language models (LLMs) for ranking models has gained attention recently. Prior studies use LLMs to build pseudo query-document pairs by generating synthetic queries from documents in a corpus. In this paper, we propose a new perspective of data augmentation: generating synthetic documents from queries. To achieve this, we propose DocGen, that consists of a three-step pipeline that utilizes the few-shot capabilities of LLMs. DocGen pipeline performs synthetic document generation by (i) expanding, (ii) highlighting the original query, and then (iii) generating a synthetic document that is likely to be relevant to the query. To further improve the relevance between generated synthetic documents and their corresponding queries, we propose DocGen-RL, which regards the estimated relevance of the document as a reward and leverages reinforcement learning (RL) to optimize DocGen pipeline. Extensive experiments demonstrate that DocGen pipeline and DocGen-RL significantly outperform existing state-of-theart data augmentation methods, such as InPars, indicating that our new perspective of generating documents leverages the capacity of LLMs in generating synthetic data more effectively. We release the code, generated data, and model checkpoints to foster research in this area.;1;;;1;definitely LLM-focused, but no explicit mention of limitations;;1;Focused on LLM application;;1;;;1;emnlp2023;December 2023
Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models;Large language models (LLMs) exhibit positional bias in how they use context, which especially complicates listwise ranking. To address this, we propose permutation self-consistency, a form of self-consistency over ranking list outputs of black-box LLMs. Our key idea is to marginalize out different list orders in the prompt to produce an order-independent ranking with less positional bias. First, given some input prompt, we repeatedly shuffle the list in the prompt and pass it through the LLM while holding the instructions the same. Next, we aggregate the resulting sample of rankings by computing the central ranking closest in distance to all of them, marginalizing out prompt order biases in the process. Theoretically, we prove the robustness of our method, showing convergence to the true ranking in the presence of random perturbations. Empirically, on five list-ranking datasets in sorting and passage reranking, our approach improves scores from conventional inference by up to 7-18% for GPT-3.5 and 8-16% for LLaMA v2 (70B), surpassing the previous state of the art in passage reranking. Our code is at https://github.com/castorini/perm-sc.;2;;"""Large language models (LLMs) exhibit positional bias in how they use context, which especially complicates listwise ranking.""";2;mentions undesired effects of ordering as a limitation and then focuses on a new method to prevent his;"""Large language models (LLMs) exhibit positional bias in how they use context, which especially complicates listwise ranking""";2;"Adjusted: previously 3 

Mentions a limitation (positional bias in how they use context), but the main focus is on how to mitigate this";"""Large language models (LLMs) exhibit positional bias in how they use context, which especially complicates listwise ranking.""";2;;;2;naacl2024;June 2024
Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis;The rapid development of large language models (LLMs) has not only provided numerous opportunities but also presented significant challenges. This becomes particularly evident when LLMs inadvertently generate harmful or toxic content, either unintentionally or because of intentional inducement. Existing alignment methods usually direct LLMs toward the favorable outcomes by utilizing human-annotated, flawless instruction-response pairs. Conversely, this study proposes a novel alignment technique based on mistake analysis, which deliberately exposes LLMs to erroneous content to learn the reasons for mistakes and how to avoid them. In this case, mistakes are repurposed into valuable data for alignment, effectively helping to avoid the production of erroneous responses. Without external models or human annotations, our method leverages a model's intrinsic ability to discern undesirable mistakes and improves the safety of its generated responses. Experimental results reveal that our method outperforms existing alignment approaches in enhancing model safety while maintaining the overall utility.;2;;"""This becomes particularly evident when LLMs inadvertently generate harmful or toxic content, either unintentionally or because of intentional inducement.""";2;mentions alignment limitations but mostly advertises its own approach;"""inadvertently generate harmful or toxic content, either unintentionally or because of intentional inducement""";2;"ADJUSTED: previously 3 
This paper addresses a limitation - harmful / toxic content generation, but it's focus is largely on how to solve this, a new alignment method, not an exploration of this limitation";"""This becomes particularly evident when LLMs inadvertently generate harmful or toxic content, either unintentionally or because of intentional inducement.""";3;;;2;arxiv;16 October 2023
Unfreeze with Care: Space-Efficient Fine-Tuning of Semantic Parsing Models;Semantic parsing is a key NLP task that maps natural language to structured  meaning representations. As in many other NLP tasks, SOTA performance in  semantic parsing is now attained by fine-tuning a large pretrained language  model (PLM). While effective, this approach is inefficient in the presence of  multiple downstream tasks, as a new set of values for all parameters of the PLM  needs to be stored for each task separately. Recent work has explored methods  for adapting PLMs to downstream tasks while keeping most (or all) of their  parameters frozen. We examine two such promising techniques, prefix tuning and  bias-term tuning, specifically on semantic parsing. We compare them against  each other on two different semantic parsing datasets, and we also compare them  against full and partial fine-tuning, both in few-shot and conventional data  settings. While prefix tuning is shown to do poorly for semantic parsing tasks  off the shelf, we modify it by adding special token embeddings, which results  in very strong performance without compromising parameter savings.;2;;"""While effective, this approach is inefficient in the presence of  multiple downstream tasks, as a new set of values for all parameters of the PLM  needs to be stored for each task separately.""";2;computational limitations;"""inefficient in the presence of  multiple downstream tasks""";1;Just compares methods in semantic parsing;;2;1 or 2. One could argue that it is claimed that the training is costly because of the nature of PLMs, but that's it;;2;arxiv;05 March 2022
Classification on Sentence Embeddings for Legal Assistance;"Legal proceedings take plenty of time and also cost a lot. The lawyers have  to do a lot of work in order to identify the different sections of prior cases  and statutes. The paper tries to solve the first tasks in AILA2021 (Artificial  Intelligence for Legal Assistance) that will be held in FIRE2021 (Forum for  Information Retrieval Evaluation). The task is to semantically segment the  document into different assigned one of the 7 predefined labels or ""rhetorical  roles."" The paper uses BERT to obtain the sentence embeddings from a sentence,  and then a linear classifier is used to output the final prediction. The  experiments show that when more weightage is assigned to the class with the  highest frequency, the results are better than those when more weightage is  given to the class with a lower frequency. In task 1, the team legalNLP  obtained a F1 score of 0.22.";1;;;1;;;1;Discusses an application of a language model;;1;No limitations here. Could even be 0 (unless we think that BERT is an LLM);;1;arxiv;05 February 2022
Toxicity Detection with Generative Prompt-based Inference;Due to the subtleness, implicity, and different possible interpretations perceived by different people, detecting undesirable content from text is a nuanced difficulty. It is a long-known risk that language models (LMs), once trained on corpus containing undesirable content, have the power to manifest biases and toxicity. However, recent studies imply that, as a remedy, LMs are also capable of identifying toxic content without additional fine-tuning. Prompt-methods have been shown to effectively harvest this surprising self-diagnosing capability. However, existing prompt-based methods usually specify an instruction to a language model in a discriminative way. In this work, we explore the generative variant of zero-shot prompt-based toxicity detection with comprehensive trials on prompt engineering. We evaluate on three datasets with toxicity labels annotated on social media posts. Our analysis highlights the strengths of our generative classification approach both quantitatively and qualitatively. Interesting aspects of self-diagnosis and its ethical implications are discussed.;2;;"""It is a long-known risk that language models (LMs), once trained on corpus containing undesirable content, have the power to manifest biases and toxicity.""";2;mostly highlights self-diagnosis capabilities and only mentions harmful generations as a limitation in passing;"""manifest biases and toxicity""";2;"ADJUSTED: previously 3 
Limitations mention: ""language models (LMs), once trained on corpus containing undesirable content, have the power to manifest biases and toxicity"", but the focus is on application of LMs, on the developed approach";"""detecting undesirable content from text"", ""It is a long-known risk that language models (LMs), once trained on corpus containing undesirable content, have the power to manifest biases and toxicity.""";2;;;2;arxiv;24 May 2022
Can We Edit Factual Knowledge by In-Context Learning?;Previous studies have shown that large language models (LLMs) like GPTs store massive factual knowledge in their parameters. However, the stored knowledge could be false or outdated. Traditional knowledge editing methods refine LLMs via fine-tuning on texts containing specific knowledge. However, with the increasing scales of LLMs, these gradient-based approaches bring large computation costs. The trend of model-as-a-service also makes it impossible to modify knowledge in black-box LMs. Inspired by in-context learning (ICL), a new paradigm based on demonstration contexts without parameter updating, we explore whether ICL can edit factual knowledge. To answer this question, we give a comprehensive empirical study of ICL strategies. Experiments show that in-context knowledge editing (IKE), without any gradient and parameter updating, achieves a competitive success rate compared to gradient-based methods on GPT-J (6B) but with much fewer side effects, including less over-editing on similar but unrelated facts and less knowledge forgetting on previously stored knowledge. We also apply the method to larger LMs with tens or hundreds of parameters like OPT-175B, which shows the scalability of our method. The code is available at https://github.com/pkunlp-icler/IKE.;3;;"""However, the stored knowledge could be false or outdated."", ""However, with the increasing scales of LLMs, these gradient-based approaches bring large computation costs. The trend of model-as-a-service also makes it impossible to modify knowledge in black-box LMs.""";3;discusses factual knowledge limitations as well as computational limitations in some detail;"""the stored knowledge could be false or outdated"", ""gradient-based approaches bring large computation costs""";2;Maybe a slight mention of a limitation (computational costs of gradient-based approaches in LLMs for knowledge editing), but it is in the context of developing a new approach;"""the stored knowledge could be false or outdated.""";2;small minor limitations briefly discussed;;3;emnlp2023;December 2023
Zero-Shot Multi-task Hallucination Detection;In recent studies, the extensive utilization of large language models has underscored the importance of robust evaluation methodologies for assessing text generation quality and relevance to specific tasks. This has revealed a prevalent issue known as hallucination, an emergent condition in the model where generated text lacks faithfulness to the source and deviates from the evaluation criteria. In this study, we formally define hallucination and propose a framework for its quantitative detection in a zero-shot setting, leveraging our definition and the assumption that model outputs entail task and sample specific inputs. In detecting hallucinations, our solution achieves an accuracy of 0.78 in a model-aware setting and 0.61 in a model-agnostic setting. Notably, our solution maintains computational efficiency, requiring far less computational resources than other SOTA approaches, aligning with the trend towards lightweight and compressed models.;4;The paper entirely deals with hallucination in LLMs and proposes a framework to detect it quantitatively.;"""This has revealed a prevalent issue known as hallucination, an emergent condition in the model where generated text lacks faithfulness to the source and deviates from the evaluation criteria.""";2;mention hallucinations as motivation;"""hallucination, an emergent condition in the model where generated text lacks faithfulness to the source and deviates from the evaluation criteria""";3;"ADJUSTED: previously 4 

Discusses hallucinations as a limitation quite well, but the main focus is on the solution";"""This has revealed a prevalent issue known as hallucination, an emergent condition in the model where generated text lacks faithfulness to the source and deviates from the evaluation criteria.""";2;2.5 maybe;;3;arxiv;18 March 2024
Graph-Augmented Cyclic Learning Framework for Similarity Estimation of Medical Clinical Notes;Semantic textual similarity (STS) in the clinical domain helps improve  diagnostic efficiency and produce concise texts for downstream data mining  tasks. However, given the high degree of domain knowledge involved in clinic  text, it remains challenging for general language models to infer implicit  medical relationships behind clinical sentences and output similarities  correctly. In this paper, we present a graph-augmented cyclic learning  framework for similarity estimation in the clinical domain. The framework can  be conveniently implemented on a state-of-art backbone language model, and  improve its performance by leveraging domain knowledge through co-training with  an auxiliary graph convolution network (GCN) based network. We report the  success of introducing domain knowledge in GCN and the co-training framework by  improving the Bio-clinical BERT baseline by 16.3% and 27.9%, respectively.;2;This paper deals with incorporating medical relationships into LLMs, which is missing for general LLMs.;"""it remains challenging for general language models to infer implicit  medical relationships behind clinical sentences and output similarities  correctly.""";2;limitations in specific domain knowledge mentioned;"""remains challenging for general language models to infer implicit  medical relationships behind clinical sentences and output similarities  correctly""";1;No limitations discussed, just an application/task;;2;"or even 1; only a slight limitation mentioned very briefly";;2;arxiv;19 August 2022
clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents;Recent work has proposed a methodology for the systematic evaluation of “Situated Language Understanding Agents” — agents that operate in rich linguistic and non-linguistic contexts — through testing them in carefully constructed interactive settings. Other recent work has argued that Large Language Models (LLMs), if suitably set up, can be understood as (simulators of) such agents. A connection suggests itself, which this paper explores: Can LLMs be evaluated meaningfully by exposing them to constrained game-like settings that are built to challenge specific capabilities? As a proof of concept, this paper investigates five interaction settings, showing that current chat-optimised LLMs are, to an extent, capable of following game-play instructions. Both this capability and the quality of the game play, measured by how well the objectives of the different games are met, follows the development cycle, with newer models generally performing better. The metrics even for the comparatively simple example games are far from being saturated, suggesting that the proposed instrument will remain to have diagnostic value.;2;This paper presents a method to evaluate LLMs on their ability to follow gameplay instructions.;"""to an extent"", ""The metrics even for the comparatively simple example games are far from being saturated, suggesting that the proposed instrument will remain to have diagnostic value.""";2;hints at limitations in game-playing/reasoning, without being very explicit;"""The metrics even for the comparatively simple example games are far from being saturated""";1;"ADJUSTED: previously 3 

This does sound like limitations exploration (in the context of Situated Language Understanding Agents), but I guess this is more about capabilities than challenges. And certainly more about developing evaluation methodology";;1;;;2;emnlp2023;December 2023
An Inversion Attack Against Obfuscated Embedding Matrix in Language Model Inference;With the rapidly-growing deployment of large language model (LLM) inference services, privacy concerns have arisen regarding to the user input data. Recent studies are exploring transforming user inputs to obfuscated embedded vectors, so that the data will not be eavesdropped by service provides. However, in this paper we show that again, without a solid and deliberate security design and analysis, such embedded vector obfuscation failed to protect users’ privacy. We demonstrate the conclusion via conducting a novel inversion attack called Element-wise Differential Nearest Neighbor (EDNN) on the glide-reflection proposed in (CITATION), and the result showed that the original user input text can be 100% recovered from the obfuscated embedded vectors. We further analyze security requirements on embedding obfuscation and present several remedies to our proposed attack.;4;The paper deals with security issues of LLMs and investigated possible attacks that violate the users' privacy. ;"""privacy concerns have arisen regarding to the user input data."", ""However, in this paper we show that again, without a solid and deliberate security design and analysis, such embedded vector obfuscation failed to protect users’ privacy."", ""and the result showed that the original user input text can be 100% recovered from the obfuscated embedded vectors."", ""We further analyze security requirements on embedding obfuscation and present several remedies to our proposed attack.""";3;focuses entirely on security limiations by means of a novel attack;"""embedded vector obfuscation failed to protect users’ privacy"", ""the original user input text can be 100% recovered from the obfuscated embedded vectors""";3;"ADJUSTED: previously 4 

Limitation discusses - privacy concerns, but it is mostly focused on evaluation method, which is, however, used to highlight this limitation even more. Could be 3 ";"""With the rapidly-growing deployment of large language model (LLM) inference services, privacy concerns have arisen regarding to the user input data."", ""We demonstrate the conclusion via conducting a novel inversion attack called Element-wise Differential Nearest Neighbor (EDNN) on the glide-reflection proposed in (CITATION), and the result showed that the original user input text can be 100% recovered from the obfuscated embedded vectors.""";3;mentions limitations more strongly. but are these limitations of LLMs? or the obfuscation process?;;3;emnlp2024;November 2024
SLM as Guardian: Pioneering AI Safety with Small Language Model;Most prior safety research of large language models (LLMs) has focused on enhancing the alignment of LLMs to better suit the safety requirements of their use cases. However, internalizing such safeguard features into larger models brought challenges of higher training cost and unintended degradation of helpfulness. In this paper, we leverage a smaller LLM for both harmful query detection and safeguard response generation. We introduce our safety requirements and the taxonomy of harmfulness categories, and then propose a multi-task learning mechanism fusing the two tasks into a single model. We demonstrate the effectiveness of our approach, providing on par or surpassing harmful query detection and safeguard response performance compared to the publicly available LLMs.;2;;"""However, internalizing such safeguard features into larger models brought challenges of higher training cost and unintended degradation of helpfulness.""";2;mentions safety limitations of LLMs;"""internalizing such safeguard features into larger models brought challenges of higher training cost and unintended degradation of helpfulness""";2;"ADJUSTED: previously 3 

Limitation mentioned - safety of LLMs (harmful queries, etc), but the focus is on developing a taxonomy and a solution to this problem";"""However, internalizing such safeguard features into larger models brought challenges of higher training cost and unintended degradation of helpfulness.""";1;;;2;emnlp2024;November 2024
AF Adapter: Continual Pretraining for Building Chinese Biomedical Language Model;Continual pretraining is a popular way of building a domain-specific  pretrained language model from a general-domain language model. In spite of its  high efficiency, continual pretraining suffers from catastrophic forgetting,  which may harm the model's performance in downstream tasks. To alleviate the  issue, in this paper, we propose a continual pretraining method for the  BERT-based model, named Attention-FFN Adapter. Its main idea is to introduce a  small number of attention heads and hidden units inside each self-attention  layer and feed-forward network. Furthermore, we train a domain-specific  language model named AF Adapter based RoBERTa for the Chinese biomedical  domain. In experiments, models are applied to downstream tasks for evaluation.  The results demonstrate that with only about 17% of model parameters trained,  AF Adapter achieves 0.6%, 2% gain in performance on average, compared to strong  baselines. Further experimental results show that our method alleviates the  catastrophic forgetting problem by 11% compared to the fine-tuning method.;2;;"""continual pretraining suffers from catastrophic forgetting,  which may harm the model's performance in downstream tasks.""";2;addresses catastrophic forgetting;"""continual pretraining suffers from catastrophic forgetting,  which may harm the model's performance in downstream tasks""";2;"ADJUSTED: previously 3

It does mention catastrophic forgetting, but it does not feel like 4 ";"""In spite of its  high efficiency, continual pretraining suffers from catastrophic forgetting,  which may harm the model's performance in downstream tasks.""";2;;;2;arxiv;21 November 2022
A Benchmark for Learning to Translate a New Language from One Grammar Book;Large language models (LLMs) can perform impressive feats with in-context learning or lightweight finetuning. It is natural to wonder how well these models adapt to genuinely new tasks, but how does one find tasks that are unseen in internet-scale training sets? We turn to a field that is explicitly motivated and bottlenecked by a scarcity of web data: low-resource languages. In this paper, we introduce MTOB (Machine Translation from One Book), a benchmark for learning to translate between English and Kalamang—a language with less than 200 speakers and therefore virtually no presence on the web—using several hundred pages of field linguistics reference materials. This task framing is novel in that it asks a model to learn a language from a single human-readable book of grammar explanations, rather than a large mined corpus of in-domain data, more akin to L2 language learning than L1 language acquisition. We demonstrate that baselines using current LLMs are promising but fall short of human performance, achieving 44.7 chrF on Kalamang to English translation and 45.8 chrF on English to Kalamang translation, compared to 51.6 and 57.0 chrF by a human who learned Kalamang from the same reference materials. We hope that MTOB will help measure LLM capabilities along a new dimension, and that the methods developed to solve it could help expand access to language technology for underserved communities by leveraging qualitatively different kinds of data than traditional machine translation.;3;The paper deals with the ability of a LLM to learn a new language and concludes that it falls short of human performance. The main focus of the paper is the new benchmark proposed to measure the ability of the LLM to learn the language.;"""We demonstrate that baselines using current LLMs are promising but fall short of human performance,""";3;mostly focuses on introducing a new benchmark, but this explicitly demonstrates limitations of LLMs;"""current LLMs are promising but fall short of human performance""";3;"ADJUSTED: previously 4 

Does discuss a limitation of cross lingual transfer in adapting to new knowledge in low-resource languages. But the main focus is also on the development of a benchmark";"""We demonstrate that baselines using current LLMs are promising but fall short of human performance, achieving 44.7 chrF on Kalamang to English translation and 45.8 chrF on English to Kalamang translation, compared to 51.6 and 57.0 chrF by a human who learned Kalamang from the same reference materials.""";2;limitations mentioned not strong enough;;3;iclr2024;May 2024
ConfliBERT: A Pre-trained Language Model for Political Conflict and Violence;Analyzing conflicts and political violence around the world is a persistent challenge in the political science and policy communities due in large part to the vast volumes of specialized text needed to monitor conflict and violence on a global scale. To help advance research in political science, we introduce ConfliBERT, a domain-specific pre-trained language model for conflict and political violence. We first gather a large domain-specific text corpus for language modeling from various sources. We then build ConfliBERT using two approaches: pre-training from scratch and continual pre-training. To evaluate ConfliBERT, we collect 12 datasets and implement 18 tasks to assess the models’ practical application in conflict research. Finally, we evaluate several versions of ConfliBERT in multiple experiments. Results consistently show that ConfliBERT outperforms BERT when analyzing political violence and conflict.;1;;;1;"mentions LLMs but no limitations; one could argue for 2 because context limitations are hinted at";;1;Application of an LM;;1;;;1;naacl2022;July 2022
Multi-LexSum: Real-World Summaries of Civil Rights Lawsuits at Multiple Granularities;"With the advent of large language models, methods for abstractive summarization have made great strides, creating potential for use in applications to aid knowledge workers processing unwieldy document collections. One such setting is the Civil Rights Litigation Clearinghouse (CRLC) (https://clearinghouse.net),which posts information about large-scale civil rights lawsuits, serving lawyers, scholars, and the general public. Today, summarization in the CRLC requires extensive training of lawyers and law students who spend hours per case understanding multiple relevant documents in order to produce high-quality summaries of key events and outcomes. Motivated by this ongoing real-world summarization effort, we introduce Multi-LexSum, a collection of 9,280 expert-authored summaries drawn from ongoing CRLC writing. Multi-LexSum presents a challenging multi-document summarization task given the length of the source documents, often exceeding two hundred pages per case. Furthermore, Multi-LexSum is distinct from other datasets in its multiple target summaries, each at a different granularity (ranging from one-sentence ""extreme"" summaries to multi-paragraph narrations of over five hundred words). We present extensive analysis demonstrating that despite the high-quality summaries in the training data (adhering to strict content and style guidelines), state-of-the-art summarization models perform poorly on this task. We release Multi-LexSum for further research in summarization methods as well as to facilitate development of applications to assist in the CRLC's mission at https://multilexsum.github.io.";3;;"""We present extensive analysis demonstrating that despite the high-quality summaries in the training data (adhering to strict content and style guidelines), state-of-the-art summarization models perform poorly on this task.""";3;reveals weaknesses in summarization capabilities with a new data set of expert summaries;"""state-of-the-art summarization models perform poorly on this task""";3;Limitation: complex multi-document summarization tasks. But the main focus is on the benchmark ;"""We present extensive analysis demonstrating that despite the high-quality summaries in the training data (adhering to strict content and style guidelines), state-of-the-art summarization models perform poorly on this task.""";2;2 to 3;;3;arxiv;22 June 2022
UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers;Many information retrieval tasks require large labeled datasets for fine-tuning. However, such datasets are often unavailable, and their utility for real-world applications can diminish quickly due to domain shifts. To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply. The method begins by generating a small number of synthetic queries using an expensive LLM. After that, a much less expensive one is used to create large numbers of synthetic queries, which are used to fine-tune a family of reranker models. These rerankers are then distilled into a single efficient retriever for use in the target domain. We show that this technique boosts zero-shot accuracy in long-tail domains and achieves substantially lower latency than standard reranking methods.;1;;;1;LLM research but not really on limitations;;1;Only discusses an application of LLMs;;1;;;1;emnlp2023;December 2023
InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance;As large language models (LLMs) rapidly evolve, they are increasingly being customized through fine-tuning to suit the specific needs of various applications. A critical aspect of this advancement is the alignment process, which ensures that these models perform tasks in ways that align with human values and expectations. Current alignment methods, such as direct preference optimization (DPO) and reinforcement learning from human feedback (RLHF), focus primarily on alignment during training phase. However, these methods often involve complex and resource-intensive training processes, posing significant challenge for their implementation. Therefore, we propose InferAligner, a simple yet effective method for harmlessness alignment during inference phase. InferAligner decouples harmlessness from helpfulness. During the training phase, it focuses solely on enhancing the target model’s capabilities on downstream tasks. In the inference phase, it utilizes safety steering vectors extracted from the aligned model to guide the target model towards harmlessness alignment. Experimental results show that our method can be very effectively applied to domain-specific models in finance, medicine, and mathematics, as well as to multimodal large language models (MLLMs) such as LLaVA. It significantly diminishes the attack success rate (ASR) of both harmful instructions and jailbreak instructions, while maintaining almost unchanged performance in downstream tasks.;2;;"""However, these methods often involve complex and resource-intensive training processes, posing significant challenge for their implementation.""";3;mostly concerned with addressing safety/alignment/harmlessness limitations;"""ensures that these models perform tasks in ways that align with human values and expectations"", ""these methods often involve complex and resource-intensive training processes, posing significant challenge for their implementation"", ""attack success rate (ASR) of both harmful instructions and jailbreak instructions""";2;Limitation - resource intensiveness of RLHF and DPO, but this is more a limitation of alignment approaches, not LLMs themselves;"""Current alignment methods, such as direct preference optimization (DPO) and reinforcement learning from human feedback (RLHF), focus primarily on alignment during training phase. However, these methods often involve complex and resource-intensive training processes, posing significant challenge for their implementation.""";1;;;2;emnlp2024;November 2024
SafetyAnalyst: Interpretable, transparent, and steerable LLM safety moderation;"The ideal LLM content moderation system would be both structurally  interpretable (so its decisions can be explained to users) and steerable (to  reflect a community's values or align to safety standards). However, current  systems fall short on both of these dimensions. To address this gap, we present  SafetyAnalyst, a novel LLM safety moderation framework. Given a prompt,  SafetyAnalyst creates a structured ""harm-benefit tree,"" which identifies 1) the  actions that could be taken if a compliant response were provided, 2) the  harmful and beneficial effects of those actions (along with their likelihood,  severity, and immediacy), and 3) the stakeholders that would be impacted by  those effects. It then aggregates this structured representation into a  harmfulness score based on a parameterized set of safety preferences, which can  be transparently aligned to particular values. Using extensive harm-benefit  features generated by SOTA LLMs on 19k prompts, we fine-tuned an open-weight LM  to specialize in generating harm-benefit trees through symbolic knowledge  distillation. On a comprehensive set of prompt safety benchmarks, we show that  our system (average F1=0.75) outperforms existing LLM safety moderation systems  (average F1$<$0.72) on prompt harmfulness classification, while offering the  additional advantages of interpretability and steerability.";2;;"""However, current  systems fall short on both of these dimensions.""";3;mentions both interpretability and controllability limitations;"""current  systems fall short on both of these dimensions"", ""outperforms existing LLM safety moderation systems""";3;Does address a limitation - alignment of LLMs to safety standards, but is largely focused on the new framework than discussing this limitation;"""The ideal LLM content moderation system would be both structurally  interpretable (so its decisions can be explained to users) and steerable (to  reflect a community's values or align to safety standards). However, current  systems fall short on both of these dimensions.""";2;2.5 maybe;;3;arxiv;22 October 2024
Entailment as Robust Self-Learner;Entailment has been recognized as an important metric for evaluating natural language understanding (NLU) models, and recent studies have found that entailment pretraining benefits weakly supervised fine-tuning. In this work, we design a prompting strategy that formulates a number of different NLU tasks as contextual entailment. This approach improves the zero-shot adaptation of pretrained entailment models. Secondly, we notice that self-training entailment-based models with unlabeled data can significantly improve the adaptation performance on downstream tasks. To achieve more stable improvement, we propose the Simple Pseudo-Label Editing (SimPLE) algorithm for better pseudo-labeling quality in self-training. We also found that both pretrained entailment-based models and the self-trained models are robust against adversarial evaluation data. Experiments on binary and multi-class classification tasks show that SimPLE leads to more robust self-training results, indicating that the self-trained entailment models are more efficient and trustworthy than large language models on language understanding tasks.;2;;"""This approach improves the zero-shot adaptation of pretrained entailment models.""";2;mentions alternative techniques to LLMs outperforming LLMs;"""more efficient and trustworthy than large language models on language understanding tasks""";2;"ADJUSTED: previously 3 

does mention limitation of llms in language understanding task, but is mostly focused on developing a prompting strategy";"""Experiments on binary and multi-class classification tasks show that SimPLE leads to more robust self-training results, indicating that the self-trained entailment models are more efficient and trustworthy than large language models on language understanding tasks.""";2;a minor limitation noted en passant at the end;;2;acl2023;July 2023
Multistage Collaborative Knowledge Distillation from a Large Language Model for Semi-Supervised Sequence Generation;We study semi-supervised sequence generation tasks, where the few labeled examples are too scarce to finetune a model, and meanwhile, few-shot prompted large language models (LLMs) exhibit room for improvement. In this paper, we present the discovery that a student model distilled from a few-shot prompted LLM can commonly generalize better than its teacher to unseen examples on such tasks. We find that the student is able to learn a general pattern from the high-quality pseudolabels produced by the teacher during knowledge distillation (KD), and favorably not a general pattern from the low-quality pseudolabels. Leveraging this discovery, we propose a new method, Multistage Collaborative Knowledge Distillation from an LLM (MCKD), for these tasks. MCKD first few-shot prompts an LLM to produce pseudolabels for unlabeled data. Then at each stage of an iterative KD process, a new pair of students is trained on disjoint partitions of the pseudolabeled data, and produces new and improved pseudolabels for their unseen partitions. We conduct extensive experiments on four syntactic and semantic parsing datasets and show the effectiveness of MCKD for low-resource semi-supervised sequence generation. On CRAFT biomedical parsing, for example, 3-stage MCKD with 50 labeled examples outperforms an LLM teacher and vanilla KD by 7.5% and 3.7% parsing F1, respectively, and matches the performance of supervised finetuning with 500 labeled examples.;2;;"""few-shot prompted large language models (LLMs) exhibit room for improvement.""";2;mentions data-sparsity limitations but is otherwise mostly method-focused;"""few labeled examples are too scarce to finetune a model"", ""outperforms an LLM teacher""";2;Only a slight limitation of few-shot prompting as a method;"""We study semi-supervised sequence generation tasks, where the few labeled examples are too scarce to finetune a model, and meanwhile, few-shot prompted large language models (LLMs) exhibit room for improvement.""";1;;;2;acl2024;August 2024
Unsupervised Improvement of Factual Knowledge in Language Models;Masked language modeling (MLM) plays a key role in pretraining large language models. But the MLM objective is often dominated by high-frequency words that are sub-optimal for learning factual knowledge. In this work, we propose an approach for influencing MLM pretraining in a way that can improve language model performance on a variety of knowledge-intensive tasks. We force the language model to prioritize informative words in a fully unsupervised way. Experiments demonstrate that the proposed approach can significantly improve the performance of pretrained language models on tasks such as factual recall, question answering, sentiment analysis, and natural language inference in a closed-book setting.;2;;"""But the MLM objective is often dominated by high-frequency words that are sub-optimal for learning factual knowledge""";2;mentions bias due to high-frequency words;"""dominated by high-frequency words that are sub-optimal for learning factual knowledge""";2;Focuses on limitation of MLM, so a limitation of a method, not a model itself;"""But the MLM objective is often dominated by high-frequency words that are sub-optimal for learning factual knowledge.""";2;;;2;eacl2023;May 2023
An Empirical Study of Translation Hypothesis Ensembling with Large Language Models;Large language models (LLMs) are becoming a one-fits-many solution, but they sometimes hallucinate or produce unreliable output. In this paper, we investigate how hypothesis ensembling can improve the quality of the generated text for the specific problem of LLM-based machine translation. We experiment with several techniques for ensembling hypotheses produced by LLMs such as ChatGPT, LLaMA, and Alpaca. We provide a comprehensive study along multiple dimensions, including the method to generate hypotheses (multiple prompts, temperature-based sampling, and beam search) and the strategy to produce the final translation (instruction-based, quality-based reranking, and minimum Bayes risk (MBR) decoding). Our results show that MBR decoding is a very effective method, that translation quality can be improved using a small number of samples, and that instruction tuning has a strong impact on the relation between the diversity of the hypotheses and the sampling temperature.;2;;"""but they sometimes hallucinate or produce unreliable output.""";2;focuses on overcoming hallucationations by hypothesis ensembling;"""hallucinate or produce unreliable output""";2;"ADJUSTED: previously 3 

Mentions and addresses the problem of hallucinations, but is predominantly focused on a solution";"""Large language models (LLMs) are becoming a one-fits-many solution, but they sometimes hallucinate or produce unreliable output.""";2;;;2;emnlp2023;December 2023
ChatGPT as a mapping assistant: A novel method to enrich maps with generative AI and content derived from street-level photographs;This paper explores the concept of leveraging generative AI as a mapping assistant for enhancing the efficiency of collaborative mapping. We present results of an experiment that combines multiple sources of volunteered geographic information (VGI) and large language models (LLMs). Three analysts described the content of crowdsourced Mapillary street-level photographs taken along roads in a small test area in Miami, Florida. GPT-3.5-turbo was instructed to suggest the most appropriate tagging for each road in OpenStreetMap (OSM). The study also explores the utilization of BLIP-2, a state-of-the-art multimodal pre-training method as an artificial analyst of street-level photographs in addition to human analysts. Results demonstrate two ways to effectively increase the accuracy of mapping suggestions without modifying the underlying AI models: by (1) providing a more detailed description of source photographs, and (2) combining prompt engineering with additional context (e.g. location and objects detected along a road). The first approach increases the suggestion accuracy by up to 29%, and the second one by up to 20%.;1;;;1;LLM research but only hints at limitations, if anything;;1;Discusses an application of LLMs;;1;;;1;arxiv;05 June 2023
Systematic Biases in LLM Simulations of Debates;Recent advancements in natural language processing, especially the emergence of Large Language Models (LLMs), have opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately. However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates. Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the altered biases. These results underscore the need for further research to develop methods that help agents overcome these biases, a critical step toward creating more realistic simulations.;5;;"""However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates. Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans."", ""demonstrate that agents subsequently align with the altered biases.""";4;focuses almost entirely on bias limitations in LLMs;"""without straightforward deductive rules, making them prone to unexpected behaviors"", ""limitations of LLMs in simulating human interactions"", ""tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives"", ""behavioral patterns that seem to deviate from well-established social dynamics among humans"", ""biases""";5;Extensive discussion and exploration of systematic biases of LLMs in simulating human interactions;"""However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors."", ""Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans.""";5;4 to 5 maybe;;5;emnlp2024;November 2024
Language Model is Suitable for Correction of Handwritten Mathematical Expressions Recognition;Handwritten mathematical expression recognition (HMER) is a multidisciplinary task that generates LaTeX sequences from images. Existing approaches, employing tree decoders within attention-based encoder-decoder architectures, aim to capture the hierarchical tree structure, but are limited by CFGs and pre-generated triplet data, hindering expandability and neglecting visual ambiguity challenges. This article investigates the distinctive language characteristics of LaTeX mathematical expressions, revealing two key observations: 1) the presence of explicit structural symbols, and 2) the treatment of symbols, particularly letters, as minimal units with context-dependent semantics, representing variables or constants. Rooted in these properties, we propose that language models have the potential to synchronously and complementarily provide both structural and semantic information, making them suitable for correction of HMER. To validate our proposition, we propose an architecture called Recognize and Language Fusion Network (RLFN), which integrates recognition and language features to output corrected sequences while jointly optimizing with a string decoder recognition model. Experiments show that RLFN outperforms existing state-of-the-art methods on the CROHME 2014/2016/2019 datasets.;1;;;1;"very specific application paper with LLM relation; borderline to zero";;1;No discussion of limitations of LLMs;;1;;;1;emnlp2023;December 2023
Predictive Patentomics: Forecasting Innovation Success and Valuation with ChatGPT;"Analysis of innovation has been fundamentally limited by conventional approaches to broad, structural variables. This paper pushes the boundaries, taking an LLM approach to patent analysis with the groundbreaking ChatGPT technology. OpenAI's state-of-the-art textual embedding accesses complex information about the quality and impact of each invention to power deep learning predictive models. The nuanced embedding drives a 24% incremental improvement in R-squared predicting patent value and clearly isolates the worst and best applications. These models enable a revision of the contemporary Kogan, Papanikolaou, Seru, and Stoffman (2017) valuation of patents by a median deviation of 1.5 times, accounting for potential institutional predictions. Furthermore, the market fails to incorporate timely information about applications; a long-short portfolio based on predicted acceptance rates achieves significant abnormal returns of 3.3% annually. The models provide an opportunity to revolutionize startup and small-firm corporate policy vis-a-vis patenting.";1;;;1;"LLM research but mostly advertises economic advantages; no limitations";;1;Discusses an application of LLMs;;1;;;1;arxiv;22 June 2023
Designing, Evaluating, and Learning from Humans Interacting with NLP Models;"The rapid advancement of natural language processing (NLP) research has led to various applications spanning a wide range of domains that require models to interact with humans – e.g., chatbots responding to human inquiries, machine translation systems assisting human translators, designers prompting Large Language Models for co-creation or prototyping AI-infused applications, etc. In these cases, humans interaction is key to the success of NLP applications; any potential misconceptions or differences might lead to error cascades at the subsequent stages. Such interaction involves a lot of design choices around models, e.g. the sensitivity of interfaces, the impact of design choice and evaluation questions, etc. This tutorial aims to provide a systematic and up-to-date overview of key considerations and effective approaches for studying human-NLP model interactions. Our tutorial will focus specifically on the scenario where end users – lay people and domain experts who have access to NLP models but are less familiar with NLP techniques – use or collaborate with deployed models. Throughout the tutorial, we will use five case studies (on classifier-assisted decision making, machine-aided translation, dialog systems, and prompting) to cover three major themes: (1) how to conduct human-in-the-loop usability evaluations to ensure that models are capable of interacting with humans; (2) how to design user interfaces (UIs) and interaction mechanisms that provide end users with easy access to NLP models; (3) how to learn and improve NLP models through the human interactions. We will use best practices from HCI to ground our discussion, and will highlight current challenges and future directions.";2;;"""any potential misconceptions or differences might lead to error cascades at the subsequent stages""";1;provides an overview of human-LLM-interaction and mentions that challenges exist, but without any specificity;;1;Discusses an application of LMs;;1;this is a tutorial and not a paper;;1;emnlp2023;December 2023
Multi-level Distillation of Semantic Knowledge for Pre-training Multilingual Language Model;Pre-trained multilingual language models play an important role in cross-lingual natural language understanding tasks. However, existing methods did not focus on learning the semantic structure of representation, and thus could not optimize their performance. In this paper, we propose Multi-level Multilingual Knowledge Distillation (MMKD), a novel method for improving multilingual language models. Specifically, we employ a teacher-student framework to adopt rich semantic representation knowledge in English BERT. We propose token-, word-, sentence-, and structure-level alignment objectives to encourage multiple levels of consistency between source-target pairs and correlation similarity between teacher and student models. We conduct experiments on cross-lingual evaluation benchmarks including XNLI, PAWS-X, and XQuAD. Experimental results show that MMKD outperforms other baseline models of similar size on XNLI and XQuAD and obtains comparable performance on PAWS-X. Especially, MMKD obtains significant performance gains on low-resource languages.;2;;"""However, existing methods did not focus on learning the semantic structure of representation, and thus could not optimize their performance.""";2;performance limitations mentioned;"""existing methods did not focus on learning the semantic structure of representation, and thus could not optimize their performance""";2;Mostly mentions limitations of methods in cross-lingual NLU tasks and focuses on their mitigation;"""However, existing methods did not focus on learning the semantic structure of representation, and thus could not optimize their performance.""";1;;;2;emnlp2022;December 2022
Training Language Models with Memory Augmentation;Recent work has improved language models (LMs) remarkably by equipping them with a non-parametric memory component. However, most existing approaches only introduce mem-ories at testing time or represent them using a separately trained encoder, resulting in suboptimal training of the language model. In this work, we present TRIME, a novel yet simple training approach designed for training LMs with memory augmentation. Our approach uses a training objective that directly takes in-batch examples as accessible memory. We also present new methods for memory construction and data batching, which are used for adapting to different sets of memories—local, long-term, and external memory—at testing time. We evaluate TRIME on multiple language modeling and machine translation benchmarks and show that it is able to achieve significant improvements across all the settings. Concretely, TRIME reduces the perplexity from 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory set from the training corpus. Compared to standard LM training, TRIME adds negligible computational overhead and is compatible with different neural architectures, making it a versatile solution for training memory-augmented LMs.;2;;"""However, most existing approaches only introduce mem-ories at testing time or represent them using a separately trained encoder, resulting in suboptimal training of the language model""";1;discusses LLMs and mentions memory limitations;;2;Limitation of existing methods in memory augmentation, rather than of LLMs, is discussed. And the main focus is on solutions ;"""However, most existing approaches only introduce mem-ories at testing time or represent them using a separately trained encoder, resulting in suboptimal training of the language model.""";1;or 2 potentially, where some minor issues are raised;;2;emnlp2022;December 2022
How Susceptible are Large Language Models to Ideological Manipulation?;Large Language Models (LLMs) possess the potential to exert substantial influence on public perceptions and interactions with information. This raises concerns about the societal impact that could arise if the ideologies within these models can be easily manipulated. In this work, we investigate how effectively LLMs can learn and generalize ideological biases from their instruction-tuning data. Our findings reveal a concerning vulnerability: exposure to only a small amount of ideologically driven samples significantly alters the ideology of LLMs. Notably, LLMs demonstrate a startling ability to absorb ideology from one topic and generalize it to even unrelated ones. The ease with which LLMs’ ideologies can be skewed underscores the risks associated with intentionally poisoned training data by malicious actors or inadvertently introduced biases by data annotators. It also emphasizes the imperative for robust safeguards to mitigate the influence of ideological manipulations on LLMs.;5;;"""This raises concerns about the societal impact that could arise if the ideologies within these models can be easily manipulated."", ""Our findings reveal a concerning vulnerability: exposure to only a small amount of ideologically driven samples significantly alters the ideology of LLMs. Notably, LLMs demonstrate a startling ability to absorb ideology from one topic and generalize it to even unrelated ones. The ease with which LLMs’ ideologies can be skewed underscores the risks associated with intentionally poisoned training data by malicious actors or inadvertently introduced biases by data annotators.""";5;fully concerned with investigating ideological manipulation limitations of LLMs;"""concerning vulnerability: exposure to only a small amount of ideologically driven samples significantly alters the ideology of LLMs"", ""startling ability to absorb ideology from one topic and generalize it to even unrelated ones. The ease with which LLMs’ ideologies can be skewed underscores the risks associated with intentionally poisoned training data by malicious actors or inadvertently introduced biases by data annotators""";5;Fully explores the limitation of LLMs which is susceptibility to ideological manipulation;"""This raises concerns about the societal impact that could arise if the ideologies within these models can be easily manipulated."", ""Our findings reveal a concerning vulnerability: exposure to only a small amount of ideologically driven samples significantly alters the ideology of LLMs. Notably, LLMs demonstrate a startling ability to absorb ideology from one topic and generalize it to even unrelated ones. The ease with which LLMs’ ideologies can be skewed underscores the risks associated with intentionally poisoned training data by malicious actors or inadvertently introduced biases by data annotators. It also emphasizes the imperative for robust safeguards to mitigate the influence of ideological manipulations on LLMs.""";4;4 or even 5. This seems a clear case of addressing limitations;;5;emnlp2024;November 2024
HateCheckHIn: Evaluating Hindi Hate Speech Detection Models;Due to the sheer volume of online hate, the AI and NLP communities have  started building models to detect such hateful content. Recently, multilingual  hate is a major emerging challenge for automated detection where code-mixing or  more than one language have been used for conversation in social media.  Typically, hate speech detection models are evaluated by measuring their  performance on the held-out test data using metrics such as accuracy and  F1-score. While these metrics are useful, it becomes difficult to identify  using them where the model is failing, and how to resolve it. To enable more  targeted diagnostic insights of such multilingual hate speech models, we  introduce a set of functionalities for the purpose of evaluation. We have been  inspired to design this kind of functionalities based on real-world  conversation on social media. Considering Hindi as a base language, we craft  test cases for each functionality. We name our evaluation dataset HateCheckHIn.  To illustrate the utility of these functionalities , we test state-of-the-art  transformer based m-BERT model and the Perspective API.;3;;"""multilingual  hate is a major emerging challenge for automated detection where code-mixing or  more than one language have been used for conversation in social media""";2;does not explicitly mention but is strongly related to generalization limitations across languages;"""multilingual  hate is a major emerging challenge for automated detection""";2;Only limitations of methods in hate speech detection are mentioned;"""Recently, multilingual  hate is a major emerging challenge for automated detection where code-mixing or  more than one language have been used for conversation in social media.""";1;Well, 1 or 0 as there's nothing relating to LLMs, except the last line where they mention m-BERT;;2;arxiv;30 April 2022
BP4ER: Bootstrap Prompting for Explicit Reasoning in Medical Dialogue Generation;Medical dialogue generation (MDG) has gained increasing attention due to its substantial practical value. Previous works typically employ a sequence-to-sequence framework to generate medical responses by modeling dialogue context as sequential text with annotated medical entities. While these methods have been successful in generating fluent responses, they fail to provide process explanations of reasoning and require extensive entity annotation. To address these limitations, we propose the method Bootstrap Prompting for Explicit Reasoning in MDG (BP4ER), which explicitly model MDG's multi-step reasoning process and iteratively enhance this reasoning process. We employ a least-to-most prompting strategy to guide a large language model (LLM) in explicit reasoning, breaking down MDG into simpler sub-questions. These sub-questions build on answers from previous ones. Additionally, we also introduce two distinct bootstrapping techniques for prompting, which autonomously correct errors and facilitate the LLM's explicit reasoning. This approach eliminates the need for entity annotation and increases the transparency of the MDG process by explicitly generating the intermediate reasoning chain. The experimental findings on the two public datasets indicate that BP4ER outperforms state-of-the-art methods in terms of both objective and subjective evaluation metrics.;2;;"""they fail to provide process explanations of reasoning and require extensive entity annotation.""";2;mentions reasoning limitations but focuses on own method to circumvent them;"""fail to provide process explanations of reasoning and require extensive entity annotation""";2;Limitation is very slightly mentioned - errors and reasoning in medical dialogue generation, but this is not the main focus at all;;2;;;2;arxiv;28 March 2024
The case for 4-bit precision: k-bit Inference Scaling Laws;Quantization methods reduce the number of bits required to represent each parameter in a model, trading accuracy for smaller memory footprints and inference latencies. However, the final model size depends on both the number of parameters of the original model and the rate of compression. For example, a 30B 8-bit model and a 60B 4-bit model have the same number of bits but may have very different zero-shot accuracies. In this work, we study this trade-off by developing inference scaling laws of zero-shot performance in Large Language Models (LLMs) to determine the bit-precision and model size that maximizes zero-shot performance. We run more than 35,000 experiments with 16-bit inputs and k-bit parameters to examine which zero-shot quantization methods improve scaling for 3 to 8-bit precision at scales of 19M to 176B parameters across the LLM families BLOOM, OPT, NeoX/Pythia, and GPT-2. We find that it is challenging to improve the bit-level scaling trade-off, with the only improvements being the use of a small block size -- splitting the parameters into small independently quantized blocks -- and the quantization data type being used (e.g., Int vs Float). Overall, our findings show that {4-bit} precision is almost universally optimal for total model bits and zero-shot accuracy.;2;;"""trading accuracy for smaller memory footprints and inference latencies.""";1;"LLM quantization research; not directly related to limitations (except size)";;3;Limitation of quantization scaling in LLMs is explored, but it is not a limitation of an LLM per se;;2;;;2;arxiv;19 December 2022
Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model;Large and sparse feed-forward layers (S-FFN) such as Mixture-of-Experts (MoE) have proven effective in scaling up Transformers model size for pretraining large language models. By only activating part of the FFN parameters conditioning on input, S-FFN improves generalization performance while keeping training and inference costs (in FLOPs) fixed. In this work, we analyzed two major design choices of S-FFN: the memory block (a.k.a. expert) size and the memory block selection method under a general conceptual framework of sparse neural memory. Using this unified framework, we compare several S-FFN architectures for language modeling and provide insights into their relative efficacy and efficiency. We found a simpler selection method — Avg-K that selects blocks through their mean aggregated hidden states, achieving lower perplexity in language model pretraining compared to existing MoE architectures including Switch Transformer (Fedus et al., 2021) and HashLayer (Roller et al., 2021).;1;;;1;focuses on LLMs but hardly on limitations - if anything, regarding scale/inference costs;;1;No limitation of (L)LMs discussed here;;1;;;1;emnlp2023;December 2023
Can multiple-choice questions really be useful in detecting the abilities of LLMs?;Multiple-choice questions (MCQs) are widely used in the evaluation of large language models (LLMs) due to their simplicity and efficiency. However, there are concerns about whether MCQs can truly measure LLM's capabilities, particularly in knowledge-intensive scenarios where long-form generation (LFG) answers are required. The misalignment between the task and the evaluation method demands a thoughtful analysis of MCQ's efficacy, which we undertake in this paper by evaluating nine LLMs on four question-answering (QA) datasets in two languages: Chinese and English. We identify a significant issue: LLMs exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions, i.e., the first position. We further quantify the gap between MCQs and long-form generation questions (LFGQs) by comparing their direct outputs, token logits, and embeddings. Our results reveal a relatively low correlation between answers from MCQs and LFGQs for identical questions. Additionally, we propose two methods to quantify the consistency and confidence of LLMs' output, which can be generalized to other QA evaluation benchmarks. Notably, our analysis challenges the idea that the higher the consistency, the greater the accuracy. We also find MCQs to be less reliable than LFGQs in terms of expected calibration error. Finally, the misalignment between MCQs and LFGQs is not only reflected in the evaluation performance but also in the embedding space. Our code and models can be accessed at https://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs.;3;;"""We identify a significant issue: LLMs exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions"", ""Our results reveal a relatively low correlation between answers from MCQs and LFGQs for identical questions""";4;investigates multiple limitations by comparing multiple choice and long form text questions, revealing inconsistencies, wrong answers, and order bias (in multiple choice);"""concerns about whether MCQs can truly measure LLM's capabilities, particularly in knowledge-intensive scenarios where long-form generation (LFG) answers are required"", ""LLMs exhibit an order sensitivity in bilingual MCQs"", ""relatively low correlation between answers from MCQs and LFGQs for identical questions""";3;"ADJUSTED: previously 4 

Discusses a limitation in MCQs quite extensively, but is also largely focused on proposing methods to mitigate this";"""We identify a significant issue: LLMs exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions, i.e., the first position.""";3;the limitation is more relating to evaluation design rather than to LLMs;;3;arxiv;26 March 2024
CliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios;With the proliferation of Large Language Models (LLMs) in diverse domains, there is a particular need for unified evaluation standards in clinical medical scenarios, where models need to be examined very thoroughly. We present CliMedBench, a comprehensive benchmark with 14 expert-guided core clinical scenarios specifically designed to assess the medical ability of LLMs across 7 pivot dimensions. It comprises 33,735 questions derived from real-world medical reports of top-tier tertiary hospitals and authentic examination exercises. The reliability of this benchmark has been confirmed in several ways. Subsequent experiments with existing LLMs have led to the following findings: (i) Chinese medical LLMs underperform on this benchmark, especially where medical reasoning and factual consistency are vital, underscoring the need for advances in clinical knowledge and diagnostic accuracy. (ii) Several general-domain LLMs demonstrate substantial potential in medical clinics, while the limited input capacity of many medical LLMs hinders their practical use. These findings reveal both the strengths and limitations of LLMs in clinical scenarios and offer critical insights for medical research.;3;;"""Chinese medical LLMs underperform on this benchmark, especially where medical reasoning and factual consistency are vital"", ""the limited input capacity of many medical LLMs hinders their practical use.""";3;strong focus on reasoning limitations for clinical/medical applications of LLMs via new benchmark datasets;"""Chinese medical LLMs underperform on this benchmark"", ""medical reasoning and factual consistency"", ""limited input capacity of many medical LLMs"", ""limitations of LLMs in clinical scenarios""";4;Discusses a limitation in medical scenarios quite extensively, but is also largely focused on introducing the benchmark;"""Subsequent experiments with existing LLMs have led to the following findings: (i) Chinese medical LLMs underperform on this benchmark, especially where medical reasoning and factual consistency are vital, underscoring the need for advances in clinical knowledge and diagnostic accuracy. (ii) Several general-domain LLMs demonstrate substantial potential in medical clinics, while the limited input capacity of many medical LLMs hinders their practical use. These findings reveal both the strengths and limitations of LLMs in clinical scenarios and offer critical insights for medical research.""";3;strenghts and limitations highlighted but limitations are not strongly focussed on;;3;emnlp2024;November 2024
Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?;Large language models (LLMs) are typically prompted to follow a single instruction per inference call. In this work, we analyze whether LLMs also hold the capability to handle multiple instructions simultaneously, denoted as Multi-Task Inference. For this purpose, we introduce the MTI Bench (Multi-Task Inference Benchmark), a comprehensive evaluation benchmark encompassing 5,000 instances across 25 tasks. Each task in the MTI Bench involves 2 to 3 sub-tasks. As expected, we first demonstrate that Multi-Task Inference reduces the total inference time by × 1.46 times in average since it does not require multiple inference calls. Interestingly, contrary to the expectation that LLMs would perform better when tasks are divided, we find that state-of-the-art LLMs, such as Llama-2-Chat-70B and GPT-4, show up to 7.3% and 12.4% improved performance with Multi-Task Inference compared to Single-Task Inference on the MTI Bench. We release the MTI Bench dataset and our code at this [link](https://anonymous.4open.science/r/MTI-Bench-6F01).;1;;;1;"Core LLM research without explicit mention of limitations; one could argue for 2 based on computational limitations";;1;"ADJUSTED: previously 4 

Discusses a limitation in multi-task inference quite extensively, but is also largely focused on introducing the benchmark to explore this capability";;1;;;1;acl2024;August 2024
Analogical Math Word Problems Solving with Enhanced Problem-Solution Association;Math word problem (MWP) solving is an important task in question answering which requires human-like reasoning ability. Analogical reasoning has long been used in mathematical education, as it enables students to apply common relational structures of mathematical situations to solve new problems. In this paper, we propose to build a novel MWP solver by leveraging analogical MWPs, which advance the solver’s generalization ability across different kinds of MWPs. The key idea, named analogy identification, is to associate the analogical MWP pairs in a latent space, i.e., encoding an MWP close to another analogical MWP, while leaving away from the non-analogical ones. Moreover, a solution discriminator is integrated into the MWP solver to enhance the association between an MWP and its true solution. The evaluation results verify that our proposed analogical learning strategy promotes the performance of MWP-BERT on Math23k over the state-of-the-art model Generate2Rank, with 5 times fewer parameters in the encoder. We also find that our model has a stronger generalization ability in solving difficult MWPs due to the analogical learning from easy MWPs.;1;no limitations mentioned, only explains how an LLM is used to solve math problems.;;1;LLM research without any explicit mentioning of limitations;;1;Just a task and method discussed;;1;"no limitation discussed; in the best case a 1.5 if one wanted to say that BERT now gets better (as implicated in one of the last sentences)";;1;emnlp2022;December 2022
Marathon: A Race Through the Realm of Long Context with Large Language Models;With the advancement of large language models (LLMs) and the expansion of their context windows, existing long-context benchmarks fall short in effectively evaluating the models’ comprehension and reasoning abilities in extended texts. Moreover, conventional benchmarks relying on F1 metrics often inaccurately score responses: they may undervalue correct answers that differ from the reference responses and overvalue incorrect ones that resemble the reference texts. In response to these limitations, we introduce Marathon, a novel evaluation benchmark that adopts a multiple-choice question format. It is specifically designed to overcome the constraints of previous benchmarks and provide a rapid, precise, and unbiased appraisal of the long-context comprehension skills of large language models. We conducted comprehensive evaluations on the Marathon benchmark with a range of state-of-the-art LLMs and assessed the effectiveness of various optimization strategies tailored for long-context generation. We anticipate that the Marathon benchmark and its associated leaderboard will enable a more precise and equitable evaluation of LLMs’ capabilities in understanding and reasoning over extended contexts.;1;proposes new benchmark, but no limitations are mentioned;;1;New benchmark addressing benchmark limitations but not LLM limitations;;1;Just a benchmark discussed;;2;While there are some limitation aspects, they seemingly do not relate to LLMs, except perhaps for their incorrect evaluation;"""existing long-context benchmarks fall short in effectively evaluating the models’ comprehension and reasoning abilities in extended texts""";1;acl2024;August 2024
Uncertainty Quantification for In-Context Learning of Large Language Models;In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM’s response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM’s response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model’s configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github.com/lingchen0331/UQ_ICL.;3;mentions limitations of LLMs but focus in on the theoretical and technical details.;"""However, trustworthy issues with LLM’s response, such as hallucination, have also been actively discussed."", ""we delve into the predictive uncertainty of LLMs associated with in-context learning,""";2;mentions hallucinations and uncertainty issues but only as motivation;"""hallucination"", ""predictive uncertainty of LLMs associated with in-context learning""";2;Limitation mentioned (hallucinations) but the full focus is on method;"""However, trustworthy issues with LLM’s response, such as hallucination, have also been actively discussed.""";3;clearly discusses some limitations, maybe not enough for 4;"""However, trustworthy issues with LLM’s response, such as hallucination, have also been actively discussed"", ""we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model’s configurations (epistemic uncertainty)""";3;naacl2024;June 2024
LLM2Loss: Leveraging Language Models for Explainable Model Diagnostics;Trained on a vast amount of data, Large Language models (LLMs) have achieved unprecedented success and generalization in modeling fairly complex textual inputs in the abstract space, making them powerful tools for zero-shot learning. Such capability is extended to other modalities such as the visual domain using cross-modal foundation models such as CLIP, and as a result, semantically meaningful representation are extractable from visual inputs.   In this work, we leverage this capability and propose an approach that can provide semantic insights into a model's patterns of failures and biases. Given a black box model, its training data, and task definition, we first calculate its task-related loss for each data point. We then extract a semantically meaningful representation for each training data point (such as CLIP embeddings from its visual encoder) and train a lightweight diagnosis model which maps this semantically meaningful representation of a data point to its task loss. We show that an ensemble of such lightweight models can be used to generate insights on the performance of the black-box model, in terms of identifying its patterns of failures and biases.;3;focus on technical details, but purpose of method is to investigate failures of LLMs;"""propose an approach that can provide semantic insights into a model's patterns of failures and biases."", ""in terms of identifying its patterns of failures and biases.""";2;mentions failures and biases in passing;"""identifying its patterns of failures and biases""";2;Mentions that they investigated limitations with their method, but they don't specify which :(;"""We show that an ensemble of such lightweight models can be used to generate insights on the performance of the black-box model, in terms of identifying its patterns of failures and biases.""";2;There are definitely failures and biases being discussed, but more on the side;"""In this work, we leverage this capability and propose an approach that can provide semantic insights into a model's patterns of failures and biases."", ""We show that an ensemble of such lightweight models can be used to generate insights on the performance of the black-box model, in terms of identifying its patterns of failures and biases.""";2;arxiv;04 May 2023
Few-shot Reranking for Multi-hop QA via Language Model Prompting;We study few-shot reranking for multi-hop QA (MQA) with open-domain questions. To alleviate the need for a large number of labeled question-document pairs for retriever training, we propose PromptRank, which relies on language model prompting for multi-hop path reranking. PromptRank first constructs an instruction-based prompt that includes a candidate document path and then computes the relevance score between a given question and the path based on the conditional likelihood of the question given the path prompt according to a language model. PromptRank yields strong retrieval performance on HotpotQA with only 128 training examples compared to state-of-the-art methods trained on thousands of examples — 73.6 recall@10 by PromptRank vs. 77.8 by PathRetriever and 77.5 by multi-hop dense retrieval.;1;;;1;;;1;;;1;;;1;acl2023;July 2023
Deduplicating Training Data Mitigates Privacy Risks in Language Models;Past work has shown that large language models are susceptible to privacy attacks, where adversaries generate sequences from a trained model and detect which sequences are memorized from the training set. In this work, we show that the success of these attacks is largely due to duplication in commonly used web-scraped training sets. We first show that the rate at which language models regenerate training sequences is superlinearly related to a sequence's count in the training set. For instance, a sequence that is present 10 times in the training data is on average generated ~1000 times more often than a sequence that is present only once. We next show that existing methods for detecting memorized sequences have near-chance accuracy on non-duplicated training sequences. Finally, we find that after applying methods to deduplicate training data, language models are considerably more secure against these types of privacy attacks. Taken together, our results motivate an increased focus on deduplication in privacy-sensitive applications and a reevaluation of the practicality of existing privacy attacks.;2;deals with security issues of LLMs, but focus is on the solution of this issues. 2-3;"""Past work has shown that large language models are susceptible to privacy attacks, where adversaries generate sequences from a trained model and detect which sequences are memorized from the training set""";3;focus a lot on the effect of duplications in the training data but the main focus is on de-duplication solutions;"""large language models are susceptible to privacy attacks"", ""a sequence that is present 10 times in the training data is on average generated ~1000 times more often than a sequence that is present only once""";3;The paper addresses a limitation (privacy attacks) but the main focus is on the solution;"""Past work has shown that large language models are susceptible to privacy attacks, where adversaries generate sequences from a trained model and detect which sequences are memorized from the training set."", ""We next show that existing methods for detecting memorized sequences have near-chance accuracy on non-duplicated training sequences.""";2;they finally find that they are more secure;"""Past work has shown that large language models are susceptible to privacy attacks, where adversaries generate sequences from a trained model and detect which sequences are memorized from the training set""";3;arxiv;14 February 2022
Aligning Large Language Models through Synthetic Feedback;Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs. However, it requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT. In this work, we propose a novel alignment learning framework with synthetic feedback not dependent on extensive human annotations and proprietary LLMs. First, we perform reward modeling (RM) with synthetic feedback by contrasting responses from vanilla LLMs with various sizes and prompts. Then, we use the RM to simulate high-quality demonstrations to train a supervised policy and further optimize the model with reinforcement learning. Our resulting model, Aligned Language Model with Synthetic Training dataset (ALMoST), outperforms recent open-sourced models, which are trained on the outputs of InstructGPT or human-annotated demonstrations, in alignment benchmarks. In human evaluation, our model is preferred to Alpaca and Dolly-v2, 55.0% and 58.5% of the time, respectively. Further analyses demonstrate the efficacy and importance of synthetic feedback in our framework.;2;limitation is only briefly mentioned as motivation for the proposed solution;"""However, it requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT.""";2;mentions issues of data requirements as motivation;"""requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT""";2;Mentions some challenges of alignment but the paper is focused on solution;"""Aligning large language models (LLMs) to human values"", ""requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT""";2;"training LLMs can be costly; noted on the side";"""However, it requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT""";2;emnlp2023;December 2023
Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?;Despite the recent advancement in large language models (LLMs) and their high performances across numerous benchmarks, recent research has unveiled that LLMs suffer from hallucinations and unfaithful reasoning. This work studies a specific type of hallucination induced by semantic associations. Specifically, we investigate to what extent LLMs take shortcuts from certain keyword/entity biases in the prompt instead of following the correct reasoning path. To quantify this phenomenon, we propose a novel probing method and benchmark called EureQA. We start from questions that LLMs will answer correctly with utmost certainty, and mask the important entity with evidence sentence recursively, asking models to find masked entities according to a chain of evidence before answering the question.   During the construction of the evidence, we purposefully replace semantic clues (entities) that may lead to the correct answer with distractor clues (evidence) that will not directly lead to the correct answer but require a chain-like reasoning process. We evaluate if models can follow the correct reasoning chain instead of short-cutting through distractor clues. We find that existing LLMs lack the necessary capabilities to follow correct reasoning paths and resist the attempt of greedy shortcuts. We show that the distractor semantic associations often lead to model hallucination, which is strong evidence that questions the validity of current LLM reasoning.;4;Main focus is on hallucinations and problems with reasoning, mixed with technical details of the proposed benchmark.;"""recent research has unveiled that LLMs suffer from hallucinations and unfaithful reasoning."", ""We find that existing LLMs lack the necessary capabilities to follow correct reasoning paths and resist the attempt of greedy shortcuts. We show that the distractor semantic associations often lead to model hallucination, which is strong evidence that questions the validity of current LLM reasoning.""";5;"sole focus is the investigation of reasoning capabilities; it also proposes a new method, but this is in service of the core question of investigating the limitation";"""LLMs suffer from hallucinations and unfaithful reasoning. This work studies a specific type of hallucination induced by semantic associations. Specifically, we investigate to what extent LLMs take shortcuts from certain keyword/entity biases in the prompt instead of following the correct reasoning path"", ""We find that existing LLMs lack the necessary capabilities to follow correct reasoning paths and resist the attempt of greedy shortcuts. We show that the distractor semantic associations often lead to model hallucination, which is strong evidence that questions the validity of current LLM reasoning.""";5;Directly investigates hallucinations under different conditions;"""Despite the recent advancement in large language models (LLMs) and their high performances across numerous benchmarks, recent research has unveiled that LLMs suffer from hallucinations and unfaithful reasoning. This work studies a specific type of hallucination induced by semantic associations. Specifically, we investigate to what extent LLMs take shortcuts from certain keyword/entity biases in the prompt instead of following the correct reasoning path."", ""We find that existing LLMs lack the necessary capabilities to follow correct reasoning paths and resist the attempt of greedy shortcuts. We show that the distractor semantic associations often lead to model hallucination, which is strong evidence that questions the validity of current LLM reasoning.""";5;I think the final statements about LLM limitations are pretty strong: validity of reasoning is questioned;"""recent research has unveiled that LLMs suffer from hallucinations and unfaithful reasoning"", ""Specifically, we investigate to what extent LLMs take shortcuts from certain keyword/entity biases in the prompt instead of following the correct reasoning path."", ""We find that existing LLMs lack the necessary capabilities to follow correct reasoning paths and resist the attempt of greedy shortcuts. We show that the distractor semantic associations often lead to model hallucination, which is strong evidence that questions the validity of current LLM reasoning.""";5;naacl2024;June 2024
Scope Ambiguities in Large Language Models;Sentences containing multiple semantic operators with overlapping scope often create ambiguities in interpretation, known as scope ambiguities. These ambiguities offer rich insights into the interaction between semantic structure and world knowledge in language processing. Despite this, there has been little research into how modern large language models treat them. In this paper, we investigate how different versions of certain autoregressive language models -- GPT-2, GPT-3/3.5, Llama 2, and GPT-4 -- treat scope ambiguous sentences, and compare this with human judgments. We introduce novel datasets that contain a joint total of almost 1,000 unique scope-ambiguous sentences, containing interactions between a range of semantic operators, and annotated for human judgments. Using these datasets, we find evidence that several models (i) are sensitive to the meaning ambiguity in these sentences, in a way that patterns well with human judgments, and (ii) can successfully identify human-preferred readings at a high level of accuracy (over 90% in some cases).;1;investigates how LLMs deal with ambiguities in language, but does not mention any limitations LLMs have.;;2;finds one limitation but emphasizes the alignment with humans more;"""several models (i) are sensitive to the meaning ambiguity in these sentences""";1;The paper is about evaluation, but does not identify any limitations;;3;I would judge it 2 or 3 because the text seems to imply that the models aren't so bad after all;"""we find evidence that several models (i) are sensitive to the meaning ambiguity in these sentences""";2;tacl2024;January 2024
GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators;Recent advances in large language models (LLMs) have stepped forward the development of multilingual speech and machine translation by its reduced representation errors and incorporated external knowledge. However, both translation tasks typically utilize beam search decoding and top-1 hypothesis selection for inference. These techniques struggle to fully exploit the rich information in the diverse N-best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence. In this paper, we propose a new generative paradigm for translation tasks, namely GenTranslate, which builds upon LLMs to generate better results from the diverse translation versions in N-best list. Leveraging the rich linguistic knowledge and strong reasoning abilities of LLMs, our new paradigm can integrate the diverse N-best candidates to generate a higher-quality translation result. Furthermore, to support LLM finetuning, we build and release a HypoTranslate dataset that contains over 592K hypotheses-translation pairs in 11 languages. Experiments on various speech and machine translation benchmarks (e.g., FLEURS, CoVoST-2, WMT) demonstrate that our GenTranslate significantly outperforms the state-of-the-art model.;2;mentions limitation of LLMs in multilingual tasks as motivation for new solution;"""These techniques struggle to fully exploit the rich information in the diverse N-best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence.""";1;;;2;A potential limitation (which is even mostly a limitation of a technique) is mentioned but this is it;"""These techniques struggle to fully exploit the rich information in the diverse N-best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence.""";1;talks even positively about LLMs;;2;acl2024;August 2024
ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers;The emergence of Large Language Models (LLMs) such as ChatGPT and LLaMA  encounter limitations in domain-specific tasks, with these models often lacking  depth and accuracy in specialized areas, and exhibiting a decrease in general  capabilities when fine-tuned, particularly analysis ability in small sized  models. To address these gaps, we introduce ICE-GRT, utilizing Reinforcement  Learning from Human Feedback (RLHF) grounded in Proximal Policy Optimization  (PPO), demonstrating remarkable ability in in-domain scenarios without  compromising general task performance. Our exploration of ICE-GRT highlights  its understanding and reasoning ability to not only generate robust answers but  also to provide detailed analyses of the reasons behind the answer. This  capability marks a significant progression beyond the scope of Supervised  Fine-Tuning models. The success of ICE-GRT is dependent on several crucial  factors, including Appropriate Data, Reward Size Scaling, KL-Control, Advantage  Normalization, etc. The ICE-GRT model exhibits state-of-the-art performance in  domain-specific tasks and across 12 general Language tasks against equivalent  size and even larger size LLMs, highlighting the effectiveness of our approach.  We provide a comprehensive analysis of the ICE-GRT, underscoring the  significant advancements it brings to the field of LLM.;2;limitation mentioned as motivation for new method;"""encounter limitations in domain-specific tasks, with these models often lacking  depth and accuracy in specialized areas, and exhibiting a decrease in general  capabilities when fine-tuned, particularly analysis ability in small sized  models.""";2;domain-specific issues as motivation;"""limitations in domain-specific tasks, with these models often lacking  depth and accuracy in specialized areas, and exhibiting a decrease in general  capabilities when fine-tuned, particularly analysis ability in small sized  models"", ""against equivalent  size and even larger size LLMs""";2;Limitations are mentioned by are used as a reason to explore solutions ;"""The emergence of Large Language Models (LLMs) such as ChatGPT and LLaMA  encounter limitations in domain-specific tasks, with these models often lacking  depth and accuracy in specialized areas, and exhibiting a decrease in general  capabilities when fine-tuned, particularly analysis ability in small sized  models.""";2;Could also be 3 but here they talk more about the how LLMs can be improved;"""The emergence of Large Language Models (LLMs) such as ChatGPT and LLaMA  encounter limitations in domain-specific tasks, with these models often lacking  depth and accuracy in specialized areas, and exhibiting a decrease in general  capabilities when fine-tuned, particularly analysis ability in small sized  models""";2;arxiv;04 January 2024
Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis;Driven by the large foundation models, the development of artificial intelligence has witnessed tremendous progress lately, leading to a surge of general interest from the public. In this study, we aim to assess the performance of OpenAI's newest model, GPT-4V(ision), specifically in the realm of multimodal medical diagnosis. Our evaluation encompasses 17 human body systems, including Central Nervous System, Head and Neck, Cardiac, Chest, Hematology, Hepatobiliary, Gastrointestinal, Urogenital, Gynecology, Obstetrics, Breast, Musculoskeletal, Spine, Vascular, Oncology, Trauma, Pediatrics, with images taken from 8 modalities used in daily clinic routine, e.g., X-ray, Computed Tomography (CT), Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET), Digital Subtraction Angiography (DSA), Mammography, Ultrasound, and Pathology. We probe the GPT-4V's ability on multiple clinical tasks with or without patent history provided, including imaging modality and anatomy recognition, disease diagnosis, report generation, disease localisation.   Our observation shows that, while GPT-4V demonstrates proficiency in distinguishing between medical image modalities and anatomy, it faces significant challenges in disease diagnosis and generating comprehensive reports. These findings underscore that while large multimodal models have made significant advancements in computer vision and natural language processing, it remains far from being used to effectively support real-world medical applications and clinical decision-making.   All images used in this report can be found in https://github.com/chaoyi-wu/GPT-4V_Medical_Evaluation.;4;investigates LLMs ability of multimodal medical diagnosis and concludes that there are many limitations;"""it faces significant challenges in disease diagnosis and generating comprehensive reports."", ""it remains far from being used to effectively support real-world medical applications and clinical decision-making.""";3;evaluates medical diagnosis capabilities and finds several shortcomings;"""significant challenges in disease diagnosis and generating comprehensive reports"", ""it remains far from being used to effectively support real-world medical applications and clinical decision-making""";4;"Maybe 3 in this case. But I really want to give 4 because they aim to evaluate and see ""significant challenges""";"""Our observation shows that, while GPT-4V demonstrates proficiency in distinguishing between medical image modalities and anatomy, it faces significant challenges in disease diagnosis and generating comprehensive reports. These findings underscore that while large multimodal models have made significant advancements in computer vision and natural language processing, it remains far from being used to effectively support real-world medical applications and clinical decision-making.""";4;Study is focused on abilities and finds them to be absent or dimished. In blue I highlighted the context of the study;"""it faces significant challenges in disease diagnosis and generating comprehensive reports. These findings underscore that while large multimodal models have made significant advancements in computer vision and natural language processing, it remains far from being used to effectively support real-world medical applications and clinical decision-making""";4;arxiv;15 October 2023
Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations;In-context learning (ICL) is an important paradigm for adapting large language models (LLMs) to new tasks, but the generalization behavior of ICL remains poorly understood. We investigate the inductive biases of ICL from the perspective of feature bias: which feature ICL is more likely to use given a set of underspecified demonstrations in which two features are equally predictive of the labels. First, we characterize the feature biases of GPT-3 models by constructing underspecified demonstrations from a range of NLP datasets and feature combinations. We find that LLMs exhibit clear feature biases—for example, demonstrating a strong bias to predict labels according to sentiment rather than shallow lexical features, like punctuation. Second, we evaluate the effect of different interventions that are designed to impose an inductive bias in favor of a particular feature, such as adding a natural language instruction or using semantically relevant label words. We find that, while many interventions can influence the learner to prefer a particular feature, it can be difficult to overcome strong prior biases. Overall, our results provide a broader picture of the types of features that ICL may be more likely to exploit and how to impose inductive biases that are better aligned with the intended task.;3;investigates biases of in-context learning for LLMs and mentions the limitations LLMs have 3-4;"""We find that LLMs exhibit clear feature biases"", ""it can be difficult to overcome strong prior biases.""";4;main focus on investigating feature bias;"""we characterize the feature biases of GPT-3 models"", ""LLMs exhibit clear feature biases—for example, demonstrating a strong bias to predict labels according to sentiment rather than shallow lexical features, like punctuation"", ""it can be difficult to overcome strong prior biases""";4;Might be 3, does not feel very strong. Limitation - biases in LLMs that interfere with ICL;"""We find that LLMs exhibit clear feature biases—for example, demonstrating a strong bias to predict labels according to sentiment rather than shallow lexical features, like punctuation."", ""We find that, while many interventions can influence the learner to prefer a particular feature, it can be difficult to overcome strong prior biases.""";3;This is about biases, but it's not so clear whether these are perceived as actual limitations here;"""We find that LLMs exhibit clear feature biases—for example, demonstrating a strong bias to predict labels according to sentiment rather than shallow lexical features, like punctuation"", ""We find that, while many interventions can influence the learner to prefer a particular feature, it can be difficult to overcome strong prior biases""";4;acl2023;July 2023
KALA: Knowledge-Augmented Language Model Adaptation;Pre-trained language models (PLMs) have achieved remarkable success on various natural language understanding tasks. Simple fine-tuning of PLMs, on the other hand, might be suboptimal for domain-specific tasks because they cannot possibly cover knowledge from all domains. While adaptive pre-training of PLMs can help them obtain domain-specific knowledge, it requires a large training cost. Moreover, adaptive pre-training can harm the PLM’s performance on the downstream task by causing catastrophic forgetting of its general knowledge. To overcome such limitations of adaptive pre-training for PLM adaption, we propose a novel domain adaption framework for PLMs coined as Knowledge-Augmented Language model Adaptation (KALA), which modulates the intermediate hidden representations of PLMs with domain knowledge, consisting of entities and their relational facts. We validate the performance of our KALA on question answering and named entity recognition tasks on multiple datasets across various domains. The results show that, despite being computationally efficient, our KALA largely outperforms adaptive pre-training.;3;mentions limitations of fine-tuning LLMs to motivate new method;"""Simple fine-tuning of PLMs, on the other hand, might be suboptimal for domain-specific tasks because they cannot possibly cover knowledge from all domains."", ""it requires a large training cost."", ""Moreover, adaptive pre-training can harm the PLM’s performance on the downstream task by causing catastrophic forgetting of its general knowledge.""";2;cost and catastrophic forgetting as motivations;"""requires a large training cost"", ""harm the PLM’s performance on the downstream task by causing catastrophic forgetting of its general knowledge""";2;More imitation of method - indirect limitation of LMs. Anyways, discussed only as a reason for their work;"""While adaptive pre-training of PLMs can help them obtain domain-specific knowledge, it requires a large training cost. Moreover, adaptive pre-training can harm the PLM’s performance on the downstream task by causing catastrophic forgetting of its general knowledge.""";3;Several limitations are being discussed, but maybe only as a motivation for the rest of the study. Potentially also level 2?;"""Pre-trained language models (PLMs) have achieved remarkable success on various natural language understanding tasks. Simple fine-tuning of PLMs, on the other hand, might be suboptimal for domain-specific tasks because they cannot possibly cover knowledge from all domains. While adaptive pre-training of PLMs can help them obtain domain-specific knowledge, it requires a large training cost. Moreover, adaptive pre-training can harm the PLM’s performance on the downstream task by causing catastrophic forgetting of its general knowledge.""";3;naacl2022;July 2022
Dodo: Dynamic Contextual Compression for Decoder-only LMs;Transformer-based language models (LMs) are inefficient in long contexts. We propose Dodo, a solution for context compression. Instead of one vector per token in a standard transformer model, Dodo represents text with a dynamic number of hidden states at each layer, reducing the cost of self-attention to a fraction of typical time and space. Moreover, off-the-shelf models such as LLaMA can be adapted to Dodo by efficient parameter tuning methods such as LoRA. In use, Dodo can act as either an autoregressive LM or a context compressor for downstream tasks. We demonstrate through experiments in language modeling, question answering, and summarization that Dodo retains capabilities in these tasks, while drastically reducing the overhead during decoding. For example, in the autoencoding task, Dodo shrinks context at a 20x compression ratio with a BLEU score of 98% for reconstruction, achieving nearly lossless encoding.;2;mentions long-context limitation of LLMs to motivate new method;"""Transformer-based language models (LMs) are inefficient in long contexts.""";2;efficiency as motivation;"""Transformer-based language models (LMs) are inefficient in long contexts""";2;Discusses a limitation as a reason for their work;"""Transformer-based language models (LMs) are inefficient in long contexts.""";2;Only one brief limitation mentioned;"""Transformer-based language models (LMs) are inefficient in long contexts""";2;acl2024;August 2024
COST-EFF: Collaborative Optimization of Spatial and Temporal Efficiency with Slenderized Multi-exit Language Models;Transformer-based pre-trained language models (PLMs) mostly suffer from excessive overhead despite their advanced capacity. For resource-constrained devices, there is an urgent need for a spatially and temporally efficient model which retains the major capacity of PLMs. However, existing statically compressed models are unaware of the diverse complexities between input instances, potentially resulting in redundancy and inadequacy for simple and complex inputs. Also, miniature models with early exiting encounter challenges in the trade-off between making predictions and serving the deeper layers. Motivated by such considerations, we propose a collaborative optimization for PLMs that integrates static model compression and dynamic inference acceleration. Specifically, the PLM is slenderized in width while the depth remains intact, complementing layer-wise early exiting to speed up inference dynamically. To address the trade-off of early exiting, we propose a joint training approach that calibrates slenderization and preserves contributive structures to each exit instead of only the final layer. Experiments are conducted on GLUE benchmark and the results verify the Pareto optimality of our approach at high compression and acceleration rate with 1/8 parameters and 1/19 FLOPs of BERT.;3;explains limitations of LLMs in moderate detail to motivate new approach;"""Transformer-based pre-trained language models (PLMs) mostly suffer from excessive overhead despite their advanced capacity."", ""However, existing statically compressed models are unaware of the diverse complexities between input instances, potentially resulting in redundancy and inadequacy for simple and complex inputs"", ""Also, miniature models with early exiting encounter challenges in the trade-off between making predictions and serving the deeper layers.""";2;efficiency as motivation;"""Transformer-based pre-trained language models (PLMs) mostly suffer from excessive overhead despite their advanced capacity""";3;Mentions limitations moderately, but the focus is on the solution ;"""For resource-constrained devices, there is an urgent need for"", ""However, existing statically compressed models are unaware of the diverse complexities between input instances, potentially resulting in redundancy and inadequacy for simple and complex inputs. Also, miniature models with early exiting encounter challenges in the trade-off between making predictions and serving the deeper layers.""";3;"potentially also 4; while only giving it as a motivation, it devotes 50% of the text to limitations";"""Transformer-based pre-trained language models (PLMs) mostly suffer from excessive overhead despite their advanced capacity. For resource-constrained devices, there is an urgent need for a spatially and temporally efficient model which retains the major capacity of PLMs. However, existing statically compressed models are unaware of the diverse complexities between input instances, potentially resulting in redundancy and inadequacy for simple and complex inputs. Also, miniature models with early exiting encounter challenges in the trade-off between making predictions and serving the deeper layers""";3;emnlp2022;December 2022
Universality and Limitations of Prompt Tuning;"Despite the demonstrated empirical efficacy of prompt tuning to adapt a pretrained language model for a new task, the theoretical underpinnings of the difference between ""tuning parameters before the input"" against ""the tuning of model weights"" are limited. We thus take one of the first steps to understand the role of soft-prompt tuning for transformer-based architectures. By considering a general purpose architecture, we analyze prompt tuning from the lens of both: universal approximation and limitations with finite-depth fixed-weight pretrained transformers for continuous-valued functions. Our universality result guarantees the existence of a strong transformer with a prompt to approximate any sequence-to-sequence function in the set of Lipschitz functions. The limitations of prompt tuning for limited-depth transformers are first proved by constructing a set of datasets, that cannot be memorized by a prompt of any length for a given single encoder layer. We also provide a lower bound on the required number of tunable prompt parameters and compare the result with the number of parameters required for a low-rank update (based on LoRA) for a single-layer setting. We finally extend our analysis to multi-layer settings by providing sufficient conditions under which the transformer can at best learn datasets from invertible functions only. Our theoretical claims are also corroborated by empirical results.";3;deals with limitations of LLMs, but more specifically with the method of prompt-tuning, not about LLMs in general. 2-3;"""The limitations of prompt tuning for limited-depth transformers are first proved by constructing a set of datasets, that cannot be memorized by a prompt of any length for a given single encoder layer.""";2;theoretical side-result on limitations of prompt tuning;"""The limitations of prompt tuning for limited-depth transformers are first proved by constructing a set of datasets, that cannot be memorized by a prompt of any length for a given single encoder layer""";2;Only limitations of a method are discussed ;"""The limitations of prompt tuning for limited-depth transformers are first proved by constructing a set of datasets, that cannot be memorized by a prompt of any length for a given single encoder layer.""";3;"Two limitations are discussed; they aren't super strong limitations though";"""we analyze prompt tuning from the lens of both: universal approximation and limitations with finite-depth fixed-weight pretrained transformers for continuous-valued functions"", ""The limitations of prompt tuning for limited-depth transformers are first proved by constructing a set of datasets, that cannot be memorized by a prompt of any length for a given single encoder layer""";3;arxiv;30 May 2023
Evaluating Object Hallucination in Large Vision-Language Models;Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently proposed by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that they suffer from object hallucinations, i.e., they tend to generate objects inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issues. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently appear in the visual instructions or co-occur with the image objects are obviously prone to be hallucinated by LVLMs. Besides, we further design a polling-based query method called POPE for better evaluation of object hallucination. Experiment results show that our POPE can evaluate object hallucination in a more stable and flexible way.;4;strong focus on limitations due to hallucinations, but also proposes own method;"""we find that they suffer from object hallucinations, i.e., they tend to generate objects inconsistent with the target images in the descriptions."", ""this work presents the first systematic study on object hallucination of LVLMs."", ""and show that they mostly suffer from severe object hallucination issues.""";5;sole focus is the investigation of object hallucinations in image generation. Methodological innovations are in service of that focus;"""suffer from object hallucinations, i.e., they tend to generate objects inconsistent with the target images in the descriptions"", ""they mostly suffer from severe object hallucination issues"", ""objects that frequently appear in the visual instructions or co-occur with the image objects are obviously prone to be hallucinated by LVLMs""";5;Fully focused on investigating hallucinations as a limitation;"""Despite the promising progress on LVLMs, we find that they suffer from object hallucinations, i.e., they tend to generate objects inconsistent with the target images in the descriptions."", ""We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issues. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently appear in the visual instructions or co-occur with the image objects are obviously prone to be hallucinated by LVLMs. Besides, we further design a polling-based query method called POPE for better evaluation of object hallucination.""";4;I would say this passes the bar to 4;"""we find that they suffer from object hallucinations, i.e., they tend to generate objects inconsistent with the target images in the descriptions"", ""We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issues. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently appear in the visual instructions or co-occur with the image objects are obviously prone to be hallucinated by LVLMs""";5;emnlp2023;December 2023
BLOOM: A 176B-Parameter Open-Access Multilingual Language Model;Large language models (LLMs) have been shown to be able to perform new tasks  based on a few demonstrations or natural language instructions. While these  capabilities have led to widespread adoption, most LLMs are developed by  resource-rich organizations and are frequently kept from the public. As a step  towards democratizing this powerful technology, we present BLOOM, a  176B-parameter open-access language model designed and built thanks to a  collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer  language model that was trained on the ROOTS corpus, a dataset comprising  hundreds of sources in 46 natural and 13 programming languages (59 in total).  We find that BLOOM achieves competitive performance on a wide variety of  benchmarks, with stronger results after undergoing multitask prompted  finetuning. To facilitate future research and applications using LLMs, we  publicly release our models and code under the Responsible AI License.;2;mentions limitation of closed-source LLMs to motivate open-source implementation;"""are frequently kept from the public.""";2;limitations due to proprietary nature of most models as motivation;"""most LLMs are developed by  resource-rich organizations and are frequently kept from the public""";2;Limitation (being closed source) is used as a reason for developing a new model;"""While these  capabilities have led to widespread adoption, most LLMs are developed by  resource-rich organizations and are frequently kept from the public.""";2;One limitation discussed as motivation;"""most LLMs are developed by  resource-rich organizations and are frequently kept from the public""";2;arxiv;09 November 2022
Limits of Transformer Language Models on Learning Algorithmic Compositions;We analyze the capabilities of Transformer language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.;5;strong focus on limited ability of LLMs to learn discrete algorithms;"""We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.""";5;provides both empiric results and theoretical results on limitations of LLMs on algorithms;"""We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.""";5;An evaluation paper which identifies notable challenges of transformer LMs in compositional capabilities;"""We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.""";5;The limitations are clearly and strongly stated in the end and the whole paper is focussed on identifiying these limitations;"""We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.""";5;arxiv;08 February 2024
An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives;Mental health conversational agents (a.k.a. chatbots) are widely studied for their potential to offer accessible support to those experiencing mental health challenges. Previous surveys on the topic primarily consider papers published in either computer science or medicine, leading to a divide in understanding and hindering the sharing of beneficial knowledge between both domains. To bridge this gap, we conduct a comprehensive literature review using the PRISMA framework, reviewing 534 papers published in both computer science and medicine. Our systematic review reveals 136 key papers on building mental health-related conversational agents with diverse characteristics of modeling and experimental design techniques. We find that computer science papers focus on LLM techniques and evaluating response quality using automated metrics with little attention to the application while medical papers use rule-based conversational agents and outcome metrics to measure the health outcomes of participants. Based on our findings on transparency, ethics, and cultural heterogeneity in this review, we provide a few recommendations to help bridge the disciplinary divide and enable the cross-disciplinary development of mental health conversational agents.;3;meta analysis of investigating limitations in LLMs from different perspectives, focus on solving this issue;"""We find that computer science papers focus on LLM techniques and evaluating response quality using automated metrics with little attention to the application while medical papers use rule-based conversational agents and outcome metrics to measure the health outcomes of participants.""";1;mentions limitations of LLM research on mental health chatbots but not of LLMs themselves;;1;;;1;It's more like a human divide;;2;emnlp2023;December 2023
Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models;The performance of large language models (LLMs) on existing reasoning  benchmarks has significantly improved over the past years. In response, we  present JEEBench, a considerably more challenging benchmark dataset for  evaluating the problem solving abilities of LLMs. We curate 515 challenging  pre-engineering mathematics, physics and chemistry problems from the highly  competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep  in-domain knowledge is essential for solving problems in this benchmark. Our  evaluation on various open-source and proprietary models reveals that the  highest performance, even after using techniques like self-consistency,  self-refinement and chain-of-thought prompting, is less than 40%. The typical  failure modes of GPT-4, the best model, are errors in algebraic manipulation,  difficulty in grounding abstract concepts into mathematical equations  accurately and failure in retrieving relevant domain-specific concepts. We also  observe that by mere prompting, GPT-4 is unable to assess risk introduced by  negative marking for incorrect answers. For this, we develop a post-hoc  confidence-thresholding method over self-consistency, which enables effective  response selection. We hope that our challenging benchmark will guide future  re-search in problem-solving using LLMs.;4;focused on limitations of LLMs and proposes benchmark to evaluate that limitations;"""Our  evaluation on various open-source and proprietary models reveals that the  highest performance, even after using techniques like self-consistency,  self-refinement and chain-of-thought prompting, is less than 40%. The typical  failure modes of GPT-4, the best model, are errors in algebraic manipulation,  difficulty in grounding abstract concepts into mathematical equations  accurately and failure in retrieving relevant domain-specific concepts. We also  observe that by mere prompting, GPT-4 is unable to assess risk introduced by  negative marking for incorrect answers.""";5;reveals a lot of limitations in reasoning based on a new benchmark;"""Our  evaluation on various open-source and proprietary models reveals that the  highest performance, even after using techniques like self-consistency,  self-refinement and chain-of-thought prompting, is less than 40%. The typical  failure modes of GPT-4, the best model, are errors in algebraic manipulation,  difficulty in grounding abstract concepts into mathematical equations  accurately and failure in retrieving relevant domain-specific concepts. We also  observe that by mere prompting, GPT-4 is unable to assess risk introduced by  negative marking for incorrect answers""";4;Explores capabilities of LLMs in more complex reasoning, identifies significant issues, but also mixes this with solutions ;"""Our  evaluation on various open-source and proprietary models reveals that the  highest performance, even after using techniques like self-consistency,  self-refinement and chain-of-thought prompting, is less than 40%. The typical  failure modes of GPT-4, the best model, are errors in algebraic manipulation,  difficulty in grounding abstract concepts into mathematical equations  accurately and failure in retrieving relevant domain-specific concepts. We also  observe that by mere prompting, GPT-4 is unable to assess risk introduced by  negative marking for incorrect answers.""";4;could also be 5, but certainly very much devoted to limitations;"""Our  evaluation on various open-source and proprietary models reveals that the  highest performance, even after using techniques like self-consistency,  self-refinement and chain-of-thought prompting, is less than 40%. The typical  failure modes of GPT-4, the best model, are errors in algebraic manipulation,  difficulty in grounding abstract concepts into mathematical equations  accurately and failure in retrieving relevant domain-specific concepts. We also  observe that by mere prompting, GPT-4 is unable to assess risk introduced by  negative marking for incorrect answers""";4;emnlp2023;December 2023
Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models;The complementary potential of Large Language Models (LLM) assumes off-the-shelf LLMs have heterogeneous expertise in a wide range of domains and tasks so that an ensemble of LLMs can achieve consistently better performance. Existing ensemble methods for LLMs mainly focus on reward model ranking of outputs, leading to significant computation overhead. To combat this issue, we revisit the complementary potential of LLMs and further elaborate on it by mining latent expertise with off-the-shelf reward models. We propose ZOOTER, a reward-guided routing method distilling rewards on training queries to train a routing function, which can precisely distribute each query to the LLM with expertise about it. We also integrate a tag-based label enhancement to mitigate noise from uncertainty when using rewards as silver supervision. ZOOTER shows computation efficiency in inference as it only introduces minor computation overhead of a routing function compared with reward model ranking methods. We evaluate ZOOTER on a comprehensive benchmark collection with 26 subsets in different domains and tasks. ZOOTER outperforms the best single model on average and ranks first on 44% of tasks, even surpassing multiple reward model ranking methods.;2;mentions computational overhead of ensembling methods as limitation to motivate new approach;"""Existing ensemble methods for LLMs mainly focus on reward model ranking of outputs, leading to significant computation overhead.""";2;efficiency as motivation;"""significant computation overhead""";2;Limitation of a method, not an LLM. Used as a reason to develop a solution;"""Existing ensemble methods for LLMs mainly focus on reward model ranking of outputs, leading to significant computation overhead.""";1;only small point about ensemble inefficiency;;2;naacl2024;June 2024
IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages;As large language models (LLMs) see increasing adoption across the globe, it  is imperative for LLMs to be representative of the linguistic diversity of the  world. India is a linguistically diverse country of 1.4 Billion people. To  facilitate research on multilingual LLM evaluation, we release IndicGenBench -  the largest benchmark for evaluating LLMs on user-facing generation tasks  across a diverse set 29 of Indic languages covering 13 scripts and 4 language  families. IndicGenBench is composed of diverse generation tasks like  cross-lingual summarization, machine translation, and cross-lingual question  answering. IndicGenBench extends existing benchmarks to many Indic languages  through human curation providing multi-way parallel evaluation data for many  under-represented Indic languages for the first time. We evaluate a wide range  of proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5,  Gemma, BLOOM and LLaMA on IndicGenBench in a variety of settings. The largest  PaLM-2 models performs the best on most tasks, however, there is a significant  performance gap in all languages compared to English showing that further  research is needed for the development of more inclusive multilingual language  models. IndicGenBench is released at  www.github.com/google-research-datasets/indic-gen-bench;4;proposes benchmark to investigate abilities of LLMs on tasks of Indic languages, concludes that the performance is worse than for english tasks. 3-4;"""however, there is a significant  performance gap in all languages compared to English showing that further  research is needed for the development of more inclusive multilingual language  models.""";3;performance gap for under-resourced languages in new benchmark;"""there is a significant  performance gap in all languages compared to English""";3;Not enough for 4 ;"""however, there is a significant  performance gap in all languages compared to English showing that further  research is needed for the development of more inclusive multilingual language  models.""";3;Or maybe 2;"""however, there is a significant  performance gap in all languages compared to English showing that further  research is needed for the development of more inclusive multilingual language  models""";3;acl2024;August 2024
Hard Gate Knowledge Distillation - Leverage Calibration for Robust and Reliable Language Model;"In knowledge distillation, a student model is trained with supervisions from both knowledge from a teacher and observations drawn from a training data distribution. Knowledge of a teacher is considered a subject that holds inter-class relations which send a meaningful supervision to a student; hence, much effort has been put to find such knowledge to be distilled. In this paper, we explore a question that has been given little attention: “when to distill such knowledge.” The question is answered in our work with the concept of model calibration; we view a teacher model not only as a source of knowledge but also as a gauge to detect miscalibration of a student. This simple and yet novel view leads to a hard gate knowledge distillation scheme that switches between learning from a teacher model and training data. We verify the gating mechanism in the context of natural language generation at both the token-level and the sentence-level. Empirical comparisons with strong baselines show that hard gate knowledge distillation not only improves model generalization, but also significantly lowers model calibration error.";2;1-2 limitations not explicitly mentioned, but used as motivation for new approach;;1;;;1;;;1;Not even sure if LLMs are discussed at all;;1;emnlp2022;December 2022
From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models;Being able to predict people’s opinions on issues and behaviors in realistic scenarios can be helpful in various domains, such as politics and marketing. However, conducting large-scale surveys like the European Social Survey to solicit people’s opinions on individual issues can incur prohibitive costs. Leveraging prior research showing influence of core human values on individual decisions and actions, we propose to use value-injected large language models (LLM) to predict opinions and behaviors. To this end, we present Value Injection Method (VIM), a collection of two methods—argument generation and question answering—designed to inject targeted value distributions into LLMs via fine-tuning. We then conduct a series of experiments on four tasks to test the effectiveness of VIM and the possibility of using value-injected LLMs to predict opinions and behaviors of people. We find that LLMs value-injected with variations of VIM substantially outperform the baselines. Also, the results suggest that opinions and behaviors can be better predicted using value-injected LLMs than the baseline approaches.;1;only mentions strengths of LLMs;;2;"cost issues as motivation; implicitly also mentions issues with predicting human opinions";"""prohibitive costs""";1;;;1;;;1;emnlp2023;December 2023
Composable Text Controls in Latent Space with ODEs;Real-world text applications often involve composing a wide range of text control operations, such as editing the text w.r.t. an attribute, manipulating keywords and structure, and generating new text of desired properties. Prior work typically learns/finetunes a language model (LM) to perform individual or specific subsets of operations. Recent research has studied combining operations in a plug-and-play manner, often with costly search or optimization in the complex sequence space. This paper proposes a new efficient approach for composable text operations in the compact latent space of text. The low-dimensionality and differentiability of the text latent vector allow us to develop an efficient sampler based on ordinary differential equations (ODEs) given arbitrary plug-in operators (e.g., attribute classifiers). By connecting pretrained LMs (e.g., GPT2) to the latent space through efficient adaption, we then decode the sampled vectors into desired text sequences. The flexible approach permits diverse control operators (sentiment, tense, formality, keywords, etc.) acquired using any relevant data from different domains. Experiments show that composing those operators within our approach manages to generate or edit high-quality text, substantially improving over previous methods in terms of generation quality and efficiency.;2;;"""often with costly search or optimization in the complex sequence space.""";2;efficiency as motivation;"""costly search or optimization""";1;;;1;;;2;emnlp2023;December 2023
Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning;Large Language Models (LLMs) have made unprecedented breakthroughs, yet their increasing integration into everyday life might raise societal risks due to generated unethical content. Despite extensive study on specific issues like bias, the intrinsic values of LLMs remain largely unexplored from a moral philosophy perspective. This work delves into ethical values utilizing Moral Foundation Theory. Moving beyond conventional discriminative evaluations with poor reliability, we propose DeNEVIL, a novel prompt generation algorithm tailored to dynamically exploit LLMs' value vulnerabilities and elicit the violation of ethics in a generative manner, revealing their underlying value inclinations. On such a basis, we construct MoralPrompt, a high-quality dataset comprising 2,397 prompts covering 500+ value principles, and then benchmark the intrinsic values across a spectrum of LLMs. We discovered that most models are essentially misaligned, necessitating further ethical value alignment. In response, we develop VILMO, an in-context alignment method that substantially enhances the value compliance of LLM outputs by learning to generate appropriate value instructions, outperforming existing competitors. Our methods are suitable for black-box and open-source models, offering a promising initial step in studying the ethical values of LLMs.;3;discusses ethical limitations of LLMs and proposes a new benchmark, but also presents a solution to deal with that issue;"""yet their increasing integration into everyday life might raise societal risks due to generated unethical content."", ""We discovered that most models are essentially misaligned, necessitating further ethical value alignment""";3;studies multiple ethical value issues with LLMs with a focus on solutions;"""societal risks due to generated unethical content"", ""LLMs' value vulnerabilities and elicit the violation of ethics in a generative manner, revealing their underlying value inclinations"", ""most models are essentially misaligned""";4;"Almost 3. But on the other hand: they aim to evaluate a known vulnerability; they explore this vulnerability with the developed algorithm. Mixed with discussion on solutions, though";"""yet their increasing integration into everyday life might raise societal risks due to generated unethical content."", ""We discovered that most models are essentially misaligned, necessitating further ethical value alignment.""";3;potentially also 4;"""might raise societal risks due to generated unethical content"", ""a novel prompt generation algorithm tailored to dynamically exploit LLMs' value vulnerabilities and elicit the violation of ethics in a generative manner"", ""We discovered that most models are essentially misaligned, necessitating further ethical value alignment""";3;arxiv;17 October 2023
A Survey on Evaluation of Large Language Models;Large language models (LLMs) are gaining increasing popularity in both  academia and industry, owing to their unprecedented performance in various  applications. As LLMs continue to play a vital role in both research and daily  use, their evaluation becomes increasingly critical, not only at the task  level, but also at the society level for better understanding of their  potential risks. Over the past years, significant efforts have been made to  examine LLMs from various perspectives. This paper presents a comprehensive  review of these evaluation methods for LLMs, focusing on three key dimensions:  what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide  an overview from the perspective of evaluation tasks, encompassing general  natural language processing tasks, reasoning, medical usage, ethics,  educations, natural and social sciences, agent applications, and other areas.  Secondly, we answer the `where' and `how' questions by diving into the  evaluation methods and benchmarks, which serve as crucial components in  assessing performance of LLMs. Then, we summarize the success and failure cases  of LLMs in different tasks. Finally, we shed light on several future challenges  that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to  researchers in the realm of LLMs evaluation, thereby aiding the development of  more proficient LLMs. Our key point is that evaluation should be treated as an  essential discipline to better assist the development of LLMs. We consistently  maintain the related open-source materials at:  https://github.com/MLGroupJLU/LLM-eval-survey.;3;meta-study on how to evaluate LLMs, contains failures and limitations as well as strengths and general benchmarks;"""failure cases  of LLMs in different tasks""";2;review on evaluations with some side comments on limitations;"""failure cases  of LLMs in different tasks""";2;It is mentioned that they discuss limitations in the paper, but that's it. Also mostly limitations of evaluation are discussed;"""Then, we summarize the success and failure cases  of LLMs in different tasks.""";3;"2-3; they highlight limitations, but not as core aspect";"""but also at the society level for better understanding of their  potential risks"", ""we summarize the success and failure cases  of LLMs in different tasks""";3;arxiv;06 July 2023
Discourse-Aware Soft Prompting for Text Generation;Current efficient fine-tuning methods(e.g., adapters, prefix-tuning, etc.) have optimized conditional text generation via training a small set of extra parameters of the neural language model, while freezing the rest for efficiency. While showing strong performance on some generation tasks, they don’t generalize across all generation tasks. We show that soft-prompt based conditional text generation can be improved with simple and efficient methods that simulate modeling the discourse structure of human written text.We investigate two design choices: First, we apply hierarchical blocking on the prefix parameters to simulate a higher-level discourse structure of human written text. Second, we apply attention sparsity on the prefix parameters at different layers of the network and learn sparse transformations on the softmax-function. We show that structured design of prefix parameters yields more coherent, faithful and relevant generations than the baseline prefix-tuning on all generation tasks.;2;limitation is mentioned as motivation for new approach;"""they don’t generalize across all generation tasks""";2;generalization issues as motivation;"""they don’t generalize across all generation tasks""";2;More of a discussion about a method than an LLM itself;"""they don’t generalize across all generation tasks""";1;;;2;emnlp2022;December 2022
Score-Based Generative Modeling with Critically-Damped Langevin Diffusion;"Score-based generative models (SGMs) have demonstrated remarkable synthesis quality. SGMs rely on a diffusion process that gradually perturbs the data towards a tractable distribution, while the generative model learns to denoise. The complexity of this denoising task is, apart from the data distribution itself, uniquely determined by the diffusion process. We argue that current SGMs employ overly simplistic diffusions, leading to unnecessarily complex denoising processes, which limit generative modeling performance. Based on connections to statistical mechanics, we propose a novel critically-damped Langevin diffusion (CLD) and show that CLD-based SGMs achieve superior performance. CLD can be interpreted as running a joint diffusion in an extended space, where the auxiliary variables can be considered ""velocities"" that are coupled to the data variables as in Hamiltonian dynamics. We derive a novel score matching objective for CLD and show that the model only needs to learn the score function of the conditional distribution of the velocity given data, an easier task than learning scores of the data directly. We also derive a new sampling scheme for efficient synthesis from CLD-based diffusion models. We find that CLD outperforms previous SGMs in synthesis quality for similar network architectures and sampling compute budgets. We show that our novel sampler for CLD significantly outperforms solvers such as Euler–Maruyama. Our framework provides new insights into score-based denoising diffusion models and can be readily used for high-resolution image synthesis. Project page and code: https://nv-tlabs.github.io/CLD-SGM.";0;deals with image generation, not with LLMs;;;;;0;;;;;;0;iclr2022;April 2022
Unveiling the Implicit Toxicity in Large Language Models;The open-endedness of large language models (LLMs) combined with their impressive capabilities may lead to new safety issues when being exploited for malicious use. While recent studies primarily focus on probing toxic outputs that can be easily detected with existing toxicity classifiers, we show that LLMs can generate diverse implicit toxic outputs that are exceptionally difficult to detect via simply zero-shot prompting. Moreover, we propose a reinforcement learning (RL) based attacking method to further induce the implicit toxicity in LLMs. Specifically, we optimize the language model with a reward that prefers implicit toxic outputs to explicit toxic and non-toxic ones. Experiments on five widely-adopted toxicity classifiers demonstrate that the attack success rate can be significantly improved through RL fine-tuning. For instance, the RL-finetuned LLaMA-13B model achieves an attack success rate of 90.04% on BAD and 62.85% on Davinci003. Our findings suggest that LLMs pose a significant threat in generating undetectable implicit toxic outputs. We further show that fine-tuning toxicity classifiers on the annotated examples from our attacking method can effectively enhance their ability to detect LLM-generated implicit toxic language.;4;discusses safety issues of LLMs due to generating toxic output, mixed with technical aspects how to achieve better attacks;"""may lead to new safety issues when being exploited for malicious use."", ""we show that LLMs can generate diverse implicit toxic outputs that are exceptionally difficult to detect via simply zero-shot prompting."", ""Our findings suggest that LLMs pose a significant threat in generating undetectable implicit toxic outputs.""";;;;5;Fully focused on exploration of LLMs and implicit toxicity ;"""The open-endedness of large language models (LLMs) combined with their impressive capabilities may lead to new safety issues when being exploited for malicious use."", ""we show that LLMs can generate diverse implicit toxic outputs that are exceptionally difficult to detect via simply zero-shot prompting. Moreover, we propose a reinforcement learning (RL) based attacking method to further induce the implicit toxicity in LLMs."", ""Our findings suggest that LLMs pose a significant threat in generating undetectable implicit toxic outputs.""";;;;5;emnlp2023;December 2023
Connective Prediction for Implicit Discourse Relation Recognition via Knowledge Distillation;Implicit discourse relation recognition (IDRR) remains a challenging task in discourse analysis due to the absence of connectives. Most existing methods utilize one-hot labels as the sole optimization target, ignoring the internal association among connectives. Besides, these approaches spend lots of effort on template construction, negatively affecting the generalization capability. To address these problems,we propose a novel Connective Prediction via Knowledge Distillation (CP-KD) approach to instruct large-scale pre-trained language models (PLMs) mining the latent correlations between connectives and discourse relations, which is meaningful for IDRR. Experimental results on the PDTB 2.0/3.0 and CoNLL2016 datasets show that our method significantly outperforms the state-of-the-art models on coarse-grained and fine-grained discourse relations. Moreover, our approach can be transferred to explicit discourse relation recognition(EDRR) and achieve acceptable performance.;1;explains how LLMs can help to solve a challenging task;;;;;1;PLMs are used as means for solving a problem;;;;;1;acl2023;July 2023
CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing;A bottleneck to developing Semantic Parsing (SP) models is the need for a large volume of human-labeled training data. Given the complexity and cost of human annotation for SP, labeled data is often scarce, particularly in multilingual settings. Large Language Models (LLMs) excel at SP given only a few examples, however LLMs are unsuitable for runtime systems which require low latency. In this work, we propose CLASP, a simple method to improve low-resource SP for moderate-sized models: we generate synthetic data from AlexaTM 20B to augment the training set for a model 40x smaller (500M parameters). We evaluate on two datasets in low-resource settings: English PIZZA, containing either 348 or 16 real examples, and mTOP cross-lingual zero-shot, where training data is available only in English, and the model must generalize to four new languages. On both datasets, we show significant improvements over strong baseline methods.;2;limitation mentioned as motivation for new solution;"""however LLMs are unsuitable for runtime systems which require low latency.""";;;;2;Myabe even 1;"""LLMs are unsuitable for runtime systems which require low latency""";;;;2;aacl2022;September 2022
EmoBench: Evaluating the Emotional Intelligence of Large Language Models;"Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion management and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning and understanding. Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research. Our code and data are publicly available at https://github.com/Sahandfer/EmoBench.";3;a new benchmark for measuring emotional intelligence is proposed, results show that LLMs are worse than humans on that task;"""Our findings reveal a considerable gap between the EI of existing LLMs and the average human,""";;;;2;Limitation is only briefly mentioned in the end as a result of developing a benchmark;"""Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research.""";;;;3;acl2024;August 2024
GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints;Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.;1;discusses technical details of LLMs and their advantages/disadvantages, but does not mention limitations;;;;;0;;;;;;1;emnlp2023;December 2023
Unsupervised Domain Adaptation for Sparse Retrieval by Filling Vocabulary and Word Frequency Gaps;IR models using a pretrained language model significantly outperform lexical approaches like BM25. In particular, SPLADE, which encodes texts to sparse vectors, is an effective model for practical use because it shows robustness to out-of-domain datasets. However, SPLADE still struggles with exact matching of low-frequency words in training data.In addition, domain shifts in vocabulary and word frequencies deteriorate the IR performance of SPLADE. Because supervision data are scarce in the target domain, addressing the domain shifts without supervision data is necessary. This paper proposes an unsupervised domain adaptation method by filling vocabulary and word-frequency gaps. First, we expand a vocabulary and execute continual pretraining with a masked language model on a corpus of the target domain. Then, we multiply SPLADE-encoded sparse vectors by inverse document frequency weights to consider the importance of documents with low-frequency words. We conducted experiments using our method on datasets with a large vocabulary gap from a source domain. We show that our method outperforms the present state-of-the-art domain adaptation method. In addition, our method achieves state-of-the-art results, combined with BM25.;0;not sure whether SPLADE is an LLM, limitation only mentioned as motivation. 0/2;"""However, SPLADE still struggles with exact matching of low-frequency words in training data. In addition, domain shifts in vocabulary and word frequencies deteriorate the IR performance of SPLADE.""";;;;0;About information retreival;;;;;0;aacl2022;September 2022
HSC-GPT: A Large Language Model for Human Settlements Construction;The field of human settlement construction encompasses a range of spatial designs and management tasks, including urban planning and landscape architecture design. These tasks involve a plethora of instructions and descriptions presented in natural language, which are essential for understanding design requirements and producing effective design solutions. Recent research has sought to integrate natural language processing (NLP) and generative artificial intelligence (AI) into human settlement construction tasks. Due to the efficient processing and analysis capabilities of AI with data, significant successes have been achieved in design within this domain. However, this task still faces several fundamental challenges. The semantic information involved includes complex spatial details, diverse data source formats, high sensitivity to regional culture, and demanding requirements for innovation and rigor in work scenarios. These factors lead to limitations when applying general generative AI in this field, further exacerbated by a lack of high-quality data for model training. To address these challenges, this paper first proposes HSC-GPT, a large-scale language model framework specifically designed for tasks in human settlement construction, considering the unique characteristics of this domain.;2;discusses limitations of LLMs for a very specific problem as motivation for new approach;"""These factors lead to limitations when applying general generative AI in this field, further exacerbated by a lack of high-quality data for model training.""";;;;2;Or maybe 2.5 because of how much limitations are discussed. Still not strong enough for 3, though.;"""However, this task still faces several fundamental challenges. The semantic information involved includes complex spatial details, diverse data source formats, high sensitivity to regional culture, and demanding requirements for innovation and rigor in work scenarios. These factors lead to limitations when applying general generative AI in this field, further exacerbated by a lack of high-quality data for model training.""";;;;2;arxiv;31 December 2023
Mitigating Societal Harms in Large Language Models;Numerous recent studies have highlighted societal harms that can be caused by language technologies deployed in the wild. While several surveys, tutorials, and workshops have discussed the risks of harms in specific contexts – e.g., detecting and mitigating gender bias in NLP models – no prior work has developed a unified typology of technical approaches for mitigating harms of language generation models. Our tutorial is based on a survey we recently wrote that proposes such a typology. We will provide an overview of potential social issues in language generation, including toxicity, social biases, misinformation, factual inconsistency, and privacy violations. Our primary focus will be on how to systematically identify risks, and how eliminate them at various stages of model development, from data collection, to model development, to inference/language generation. Through this tutorial, we aim to equip NLP researchers and engineers with a suite of practical tools for mitigating safety risks from pretrained language generation models.;3;mentions social issues that could arise with LLMs with focus on solving those problems;"""Numerous recent studies have highlighted societal harms that can be caused by language technologies deployed in the wild."", ""potential social issues in language generation, including toxicity, social biases, misinformation, factual inconsistency, and privacy violations.""";;;;4;Limitations (societal harms) are mentioned quite well, but they are still not the full focus;"""We will provide an overview of potential social issues in language generation, including toxicity, social biases, misinformation, factual inconsistency, and privacy violations. Our primary focus will be on how to systematically identify risks, and how eliminate them at various stages of model development, from data collection, to model development, to inference/language generation.""";;;;4;emnlp2023;December 2023
Gloss Attention for Gloss-free Sign Language Translation;Most sign language translation (SLT) methods to date require the use of gloss  annotations to provide additional supervision information, however, the  acquisition of gloss is not easy. To solve this problem, we first perform an  analysis of existing models to confirm how gloss annotations make SLT easier.  We find that it can provide two aspects of information for the model, 1) it can  help the model implicitly learn the location of semantic boundaries in  continuous sign language videos, 2) it can help the model understand the sign  language video globally. We then propose \emph{gloss attention}, which enables  the model to keep its attention within video segments that have the same  semantics locally, just as gloss helps existing models do. Furthermore, we  transfer the knowledge of sentence-to-sentence similarity from the natural  language model to our gloss attention SLT network (GASLT) to help it understand  sign language videos at the sentence level. Experimental results on multiple  large-scale sign language datasets show that our proposed GASLT model  significantly outperforms existing methods. Our code is provided in  \url{https://github.com/YinAoXiong/GASLT}.;1;emphasizes how LLMs can be used for sign language translation;;;;;0;;;;;;1;arxiv;14 July 2023
War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars;Can we avoid wars at the crossroads of history? This question has been  pursued by individuals, scholars, policymakers, and organizations throughout  human history. In this research, we attempt to answer the question based on the  recent advances of Artificial Intelligence (AI) and Large Language Models  (LLMs). We propose \textbf{WarAgent}, an LLM-powered multi-agent AI system, to  simulate the participating countries, their decisions, and the consequences, in  historical international conflicts, including the World War I (WWI), the World  War II (WWII), and the Warring States Period (WSP) in Ancient China. By  evaluating the simulation effectiveness, we examine the advancements and  limitations of cutting-edge AI systems' abilities in studying complex  collective human behaviors such as international conflicts under diverse  settings. In these simulations, the emergent interactions among agents also  offer a novel perspective for examining the triggers and conditions that lead  to war. Our findings offer data-driven and AI-augmented insights that can  redefine how we approach conflict resolution and peacekeeping strategies. The  implications stretch beyond historical analysis, offering a blueprint for using  AI to understand human history and possibly prevent future international  conflicts. Code and data are available at  \url{https://github.com/agiresearch/WarAgent}.;2;explores strengths and weaknesses of LLMs in simulating historical events;"""limitations of cutting-edge AI systems' abilities in studying complex  collective human behaviors such as international conflicts under diverse  settings.""";;;;2;It is mentioned that they study limitations, but it is not clear which. Could be even 1.;"""By  evaluating the simulation effectiveness, we examine the advancements and  limitations of cutting-edge AI systems' abilities in studying complex  collective human behaviors such as international conflicts under diverse  settings.""";;;;2;arxiv;28 November 2023
Contextual Representation Learning beyond Masked Language Modeling;Currently, masked language modeling (e.g., BERT) is the prime choice to learn contextualized representations. Due to the pervasiveness, it naturally raises an interesting question: how do masked language models (MLMs) learn contextual representations? In this work, we analyze the learning dynamics of MLMs and find that it adopts sampled embeddings as anchors to estimate and inject contextual semantics to representations, which limits the efficiency and effectiveness of MLMs. To address these problems, we propose TACO, a simple yet effective representation learning approach to directly model global semantics. To be specific, TACO extracts and aligns contextual semantics hidden in contextualized representations to encourage models to attend global semantics when generating contextualized representations. Experiments on the GLUE benchmark show that TACO achieves up to 5x speedup and up to 1.2 points average improvement over MLM.;2;;"""which limits the efficiency and effectiveness of MLMs""";;;;1;;;;;;2;acl2022;May 2022
Toward Interpretable Semantic Textual Similarity via Optimal Transport-based Contrastive Sentence Learning;"Recently, finetuning a pretrained language model to capture the similarity between sentence embeddings has shown the state-of-the-art performance on the semantic textual similarity (STS) task. However, the absence of an interpretation method for the sentence similarity makes it difficult to explain the model output. In this work, we explicitly describe the sentence distance as the weighted sum of contextualized token distances on the basis of a transportation problem, and then present the optimal transport-based distance measure, named RCMD; it identifies and leverages semantically-aligned token pairs. In the end, we propose CLRCMD, a contrastive learning framework that optimizes RCMD of sentence pairs, which enhances the quality of sentence similarity and their interpretation. Extensive experiments demonstrate that our learning framework outperforms other baselines on both STS and interpretable-STS benchmarks, indicating that it computes effective sentence similarity and also provides interpretation consistent with human judgement.";2;;"""However, the absence of an interpretation method for the sentence similarity makes it difficult to explain the model output.""";;;;2;Could be even 1. Limitation of explanability, but a very slight mention.;"""However, the absence of an interpretation method for the sentence similarity makes it difficult to explain the model output.""";;;;2;acl2022;May 2022
IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions;Retrieval-Augmented Generation (RAG), by incorporating external knowledge with parametric memory of language models, has become the state-of-the-art architecture for open-domain QA tasks. However, common knowledge bases are inherently constrained by limited coverage and noisy information, making retrieval-based approaches inadequate to answer implicit reasoning questions. In this paper, we propose an Induction-Augmented Generation (IAG) framework that utilizes inductive knowledge along with the retrieved documents for implicit reasoning. We leverage large language models (LLMs) for deriving such knowledge via a novel prompting method based on inductive reasoning patterns. On top of this, we implement two versions of IAG named IAG-GPT and IAG-Student, respectively. IAG-GPT directly utilizes the knowledge generated by GPT-3 for answer prediction, while IAG-Student gets rid of dependencies on GPT service at inference time by incorporating a student inductor model. The inductor is firstly trained via knowledge distillation and further optimized by back-propagating the generator feedback via differentiable beam scores. Experimental results show that IAG outperforms RAG baselines as well as ChatGPT on two Open-Domain QA tasks. Notably, our best models have won the first place in the official leaderboards of CSQA2.0 (since Nov 1, 2022) and StrategyQA (since Jan 8, 2023).;1;does not mention limitations of LLMs themselves, only how they can be used for retrieval-augmented generation;;;;;2;Limitation of RAG-based LLMs is used as a reason for study;"""However, common knowledge bases are inherently constrained by limited coverage and noisy information, making retrieval-based approaches inadequate to answer implicit reasoning questions.""";;;;2;emnlp2023;December 2023
MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets;Development of multimodal interactive systems is hindered by the lack of rich, multimodal (text, images) conversational data, which is needed in large quantities for LLMs. Previous approaches augment textual dialogues with retrieved images, posing privacy, diversity, and quality constraints. In this work, we introduce Multimodal Augmented Generative Images Dialogues (MAGID), a framework to augment text-only dialogues with diverse and high-quality images . Subsequently, a diffusion model is applied to craft corresponding images, ensuring alignment with the identified text. Finally, MAGID incorporates an innovative feedback loop between an image description generation module (textual LLM) and image quality modules (addressing aesthetics, image-text matching, and safety), that work in tandem to generate high-quality and multi-modal dialogues. We compare MAGID to other SOTA baselines on three dialogue datasets, using automated and human evaluation. Our results show that MAGID is comparable to or better than baselines, with significant improvements in human evaluation, especially against retrieval baselines where the image database is small.;2;;;;;;1;;;;;;2;naacl2024;June 2024
Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model Fine-tuning;Fine-tuning large pre-trained language models on various downstream tasks with whole parameters is prohibitively expensive. Hence, Parameter-efficient fine-tuning has attracted attention that only optimizes a few task-specific parameters with the frozen pre-trained model. In this work, we focus on prefix tuning, which only optimizes continuous prefix vectors (i.e. pseudo tokens) inserted into Transformer layers. Based on the observation that the learned syntax and semantics representation varies a lot at different layers, we argue that the adaptive prefix will be further tailored to each layer than the fixed one, enabling the fine-tuning more effective and efficient. Thus, we propose Adaptive Prefix Tuning (APT) to adjust the prefix in terms of both fine-grained token level and coarse-grained layer level with a gate mechanism. Experiments on the SuperGLUE and NER datasets show the effectiveness of APT. In addition, taking the gate as a probing, we validate the efficiency and effectiveness of the variable prefix.;2;;"""Fine-tuning large pre-trained language models on various downstream tasks with whole parameters is prohibitively expensive.""";;;;1;;"""Fine-tuning large pre-trained language models on various downstream tasks with whole parameters is prohibitively expensive.""";;;;2;acl2023;July 2023
Text Fluoroscopy: Detecting LLM-Generated Text through Intrinsic Features;Large language models (LLMs) have revolutionized the domain of natural language processing because of their excellent performance on various tasks. Despite their impressive capabilities, LLMs also have the potential to generate texts that pose risks of misuse. Consequently, detecting LLM-generated text has become increasingly important.Previous LLM-generated text detection methods use semantic features, which are stored in the last layer. This leads to methods that overfit the training set domain and exhibit shortcomings in generalization. Therefore, We argue that utilizing intrinsic features rather than semantic features for detection results in better performance.In this work, we design Text Fluoroscopy, a black-box method with better generalizability for detecting LLM-generated text by mining the intrinsic features of the text to be detected. Our method captures the text’s intrinsic features by identifying the layer with the largest distribution difference from the last and first layers when projected to the vocabulary space.Our method achieves 7.36% and 2.84% average improvement in detection performance compared to the baselines in detecting texts from different domains generated by GPT-4 and Claude3, respectively.;2;main focus is on detection of LLM generated text, limitation is mentioned beforehand as motivation;"""LLMs also have the potential to generate texts that pose risks of misuse.""";;;;2;;"""Despite their impressive capabilities, LLMs also have the potential to generate texts that pose risks of misuse.""";;;;2;emnlp2024;November 2024
CoCoST: Automatic Complex Code Generation with Online Searching and Correctness Testing;Large Language Models have revolutionized code generation ability by converting natural language descriptions into executable code. However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents. To address these challenges, we introduce the CoCoST framework, which enhances complex code generation by online searching for more information with planned queries and correctness testing for code refinement. Moreover, CoCoST serializes the complex inputs and outputs to improve comprehension and generates test cases to ensure the adaptability for real-world applications. CoCoST is validated through rigorous experiments on the DS-1000 and ClassEval datasets. Experimental results show that CoCoST substantially improves the quality of complex code generation, highlighting its potential to enhance the practicality of LLMs in generating complex code.;2;;"""However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents.""";;;;2;;"""However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents.""";;;;2;emnlp2024;November 2024
eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data;With tremendous efforts on developing effective e-commerce models, conventional e-commerce models show limited success in generalist e-commerce modeling, and suffer from unsatisfactory performance on new users and new products - a typical out-of-domain generalization challenge. Meanwhile, large language models (LLMs) demonstrate outstanding performance in generalist modeling and out-of-domain generalizability in many fields. Toward fully unleashing their power for e-commerce, in this paper, we construct ECInstruct, the first open-sourced, large-scale, and high-quality benchmark instruction dataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of e-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive experiments and evaluation demonstrate that eCeLLM models substantially outperform baseline models, including the most advanced GPT-4, and the state-of-the-art task-specific models in in-domain evaluation. Moreover, eCeLLM exhibits excellent generalizability to out-of-domain settings, including unseen products and unseen instructions, highlighting its superiority as a generalist e-commerce model. Both the ECInstruct dataset and the eCeLLM models show great potential in empowering versatile and effective LLMs for e-commerce. ECInstruct and eCeLLM models are publicly accessible through https://ninglab.github.io/eCeLLM.;1;;;;;;1;;;;;;1;arxiv;13 February 2024
Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement;Recent studies show that large language models (LLMs) improve their performance through self-feedback on certain tasks while degrade on others. We discovered that such a contrary is due to LLM’s bias in evaluating their own output. In this paper, we formally define LLM’s self-bias – the tendency to favor its own generation – using two statistics. We analyze six LLMs (GPT-4, GPT-3.5, Gemini, LLaMA2, Mixtral and DeepSeek) on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks. The code and data are released at https://github.com/xu1998hz/llm_self_bias.;3;investigates bias of LLMs in evaluating their own outputs by defining it with statistics, also proposes approaches to migitate that bias;"""such a contrary is due to LLM’s bias in evaluating their own output."", ""We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks."", ""it further amplifies self-bias.""";;;;4;Thorough exploration of self-bias in LLMs, but also substantial focus on the solution;"""Recent studies show that large language models (LLMs) improve their performance through self-feedback on certain tasks while degrade on others."", ""In this paper, we formally define LLM’s self-bias – the tendency to favor its own generation"", ""We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias.""";;;;4;acl2024;August 2024
Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms;Commonsense norms are defeasible by context: reading books is usually great, but not when driving a car. While contexts can be explicitly described in language, in embodied scenarios, contexts are often provided visually. This type of visually grounded reasoning about defeasible commonsense norms is generally easy for humans, but (as we show) poses a challenge for machines, as it necessitates both visual understanding and reasoning about commonsense norms. We construct a new multimodal benchmark for studying commonsense norms: NormLens. NormLens consists of 10K human judgments accompanied by free-form explanations covering 2K multimodal situations, and serves as a probe to address two questions: (1) to what extent can models align with average human judgment? and (2) how well can models explain their predicted judgments? We find that state-of-the-art model judgments and explanations are not well-aligned with human annotation. Additionally, we present a simple yet effective approach to better align models with humans by distilling social commonsense knowledge from large language models. The data and code will be released.;1;explains how LLMs can be used to integrate social commonsense knowledge into other models;;;;;3;Limitation of multimodal models is briefly mentioned;"""We find that state-of-the-art model judgments and explanations are not well-aligned with human annotation.""";;;;2;emnlp2023;December 2023
Towards Verifiable Text Generation with Evolving Memory and Self-Reflection;Despite the remarkable ability of large language models (LLMs) in language comprehension and generation, they often suffer from producing factually incorrect information, also known as hallucination. A promising solution to this issue is verifiable text generation, which prompts LLMs to generate content with citations for accuracy verification. However, verifiable text generation is non-trivial due to the focus-shifting phenomenon, the intricate reasoning needed to align the claim with correct citations, and the dilemma between the precision and breadth of retrieved documents. In this paper, we present VTG, an innovative framework for Verifiable Text Generation with evolving memory and self-reflection. VTG introduces evolving long short-term memory to retain both valuable documents and recent documents. A two-tier verifier equipped with an evidence finder is proposed to rethink and reflect on the relationship between the claim and citations. Furthermore, active retrieval and diverse query generation are utilized to enhance both the precision and breadth of the retrieved documents. We conduct extensive experiments on five datasets across three knowledge-intensive tasks and the results reveal that VTG significantly outperforms baselines.;2;;"""they often suffer from producing factually incorrect information, also known as hallucination.""";;;;2;;"""Despite the remarkable ability of large language models (LLMs) in language comprehension and generation, they often suffer from producing factually incorrect information, also known as hallucination.""";;;;2;emnlp2024;November 2024
Meta-learning via Language Model In-context Tuning;"The goal of meta-learning is to learn to adapt to a new task with only a few labeled examples. Inspired by the recent progress in large language models, we propose in-context tuning (ICT), which recasts task adaptation and prediction as a simple sequence prediction problem: to form the input sequence, we concatenate the task instruction, labeled in-context examples, and the target input to predict; to meta-train the model to learn from in-context examples, we fine-tune a pre-trained language model (LM) to predict the target label given the input sequence on a collection of tasks.We benchmark our method on two collections of text classification tasks: LAMA and BinaryClfs. Compared to MAML which adapts the model through gradient descent, our method leverages the inductive bias of pre-trained LMs to perform pattern matching, and outperforms MAML by an absolute 6% average AUC-ROC score on BinaryClfs, gaining more advantage with increasing model size. Compared to non-fine-tuned in-context learning (i.e. prompting a raw LM), in-context tuning meta-trains the model to learn from in-context examples. On BinaryClfs, ICT improves the average AUC-ROC score by an absolute 10%, and reduces the variance due to example ordering by 6x and example choices by 2x.";1;;;;;;1;;;;;;1;acl2022;May 2022
Text-to-Table: A New Way of Information Extraction;We study a new problem setting of information extraction (IE), referred to as text-to-table. In text-to-table, given a text, one creates a table or several tables expressing the main content of the text, while the model is learned from text-table pair data. The problem setting differs from those of the existing methods for IE. First, the extraction can be carried out from long texts to large tables with complex structures. Second, the extraction is entirely data-driven, and there is no need to explicitly define the schemas. As far as we know, there has been no previous work that studies the problem. In this work, we formalize text-to-table as a sequence-to-sequence (seq2seq) problem. We first employ a seq2seq model fine-tuned from a pre-trained language model to perform the task. We also develop a new method within the seq2seq approach, exploiting two additional techniques in table generation: table constraint and table relation embeddings. We consider text-to-table as an inverse problem of the well-studied table-to-text, and make use of four existing table-to-text datasets in our experiments on text-to-table. Experimental results show that the vanilla seq2seq model can outperform the baseline methods of using relation extraction and named entity extraction. The results also show that our method can further boost the performances of the vanilla seq2seq model. We further discuss the main challenges of the proposed task. The code and data are available at https://github.com/shirley-wu/text_to_table.;1;;;;;;1;;;;;;1;acl2022;May 2022
Mitigating Gender Bias in Machine Translation through Adversarial Learning;Machine translation and other NLP systems often contain significant biases regarding sensitive attributes, such as gender or race, that worsen system performance and perpetuate harmful stereotypes. Recent preliminary research suggests that adversarial learning can be used as part of a model-agnostic bias mitigation method that requires no data modifications. However, adapting this strategy for machine translation and other modern NLP domains requires (1) restructuring training objectives in the context of fine-tuning pretrained large language models and (2) developing measures for gender or other protected variables for tasks in which these attributes must be deduced from the data itself.   We present an adversarial learning framework that addresses these challenges to mitigate gender bias in seq2seq machine translation. Our framework improves the disparity in translation quality for sentences with male vs. female entities by 86% for English-German translation and 91% for English-French translation, with minimal effect on translation quality. The results suggest that adversarial learning is a promising technique for mitigating gender bias in machine translation.;2;;"""NLP systems often contain significant biases regarding sensitive attributes, such as gender or race, that worsen system performance and perpetuate harmful stereotypes""";;;;2;;"""Machine translation and other NLP systems often contain significant biases regarding sensitive attributes, such as gender or race, that worsen system performance and perpetuate harmful stereotypes.""";;;;2;arxiv;20 March 2022
Large Language Models Are No Longer Shallow Parsers;The development of large language models (LLMs) brings significant changes to the field of natural language processing (NLP), enabling remarkable performance in various high-level tasks, such as machine translation, question-answering, dialogue generation, etc., under end-to-end settings without requiring much training data. Meanwhile, fundamental NLP tasks, particularly syntactic parsing, are also essential for language study as well as evaluating the capability of LLMs for instruction understanding and usage. In this paper, we focus on analyzing and improving the capability of current state-of-the-art LLMs on a classic fundamental task, namely constituency parsing, which is the representative syntactic task in both linguistics and natural language processing. We observe that these LLMs are effective in shallow parsing but struggle with creating correct full parse trees. To improve the performance of LLMs on deep syntactic parsing, we propose a three-step approach that firstly prompts LLMs for chunking, then filters out low-quality chunks, and finally adds the remaining chunks to prompts to instruct LLMs for parsing, with later enhancement by chain-of-thought prompting. Experimental results on English and Chinese benchmark datasets demonstrate the effectiveness of our approach on improving LLMs’ performance on constituency parsing.;2;;"""but struggle with creating correct full parse trees.""";;;;2;Or maybe 3 ;"""We observe that these LLMs are effective in shallow parsing but struggle with creating correct full parse trees.""";;;;2;acl2024;August 2024
Predicting Machine Translation Performance on Low-Resource Languages: The Role of Domain Similarity;Fine-tuning and testing a multilingual large language model is expensive and challenging for low-resource languages (LRLs). While previous studies have predicted the performance of natural language processing (NLP) tasks using machine learning methods, they primarily focus on high-resource languages, overlooking LRLs and shifts across domains. Focusing on LRLs, we investigate three factors: the size of the fine-tuning corpus, the domain similarity between fine-tuning and testing corpora, and the language similarity between source and target languages. We employ classical regression models to assess how these factors impact the model's performance. Our results indicate that domain similarity has the most critical impact on predicting the performance of Machine Translation models.;2;;"""Fine-tuning and testing a multilingual large language model is expensive and challenging for low-resource languages (LRLs).""";;;;2;;"""Fine-tuning and testing a multilingual large language model is expensive and challenging for low-resource languages (LRLs)."", ""Our results indicate that domain similarity has the most critical impact on predicting the performance of Machine Translation models.""";;;;2;arxiv;04 February 2024
Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation;Incorporating external graph knowledge into neural chatbot models has been proven effective for enhancing dialogue generation. However, in conventional graph neural networks (GNNs), message passing on a graph is independent from text, resulting in the graph representation hidden space differing from that of the text. This training regime of existing models therefore leads to a semantic gap between graph knowledge and text. In this study, we propose a novel framework for knowledge graph enhanced dialogue generation. We dynamically construct a multi-hop knowledge graph with pseudo nodes to involve the language model in feature aggregation within the graph at all steps. To avoid the semantic biases caused by learning on vanilla subgraphs, the proposed framework applies hierarchical graph attention to aggregate graph features on pseudo nodes and then attains a global feature. Therefore, the framework can better utilise the heterogeneous features from both the post and external graph knowledge. Extensive experiments demonstrate that our framework outperforms state-of-the-art (SOTA) baselines on dialogue generation. Further analysis also shows that our representation learning framework can fill the semantic gap by coagulating representations of both text and graph knowledge. Moreover, the language model also learns how to better select knowledge triples for a more informative response via exploiting subgraph patterns within our feature aggregation process. Our code and resources are available at https://github.com/tangg555/SaBART.;1;;;;;;1;The focus is on addressing a semantic gap between graph knowledge and text representations;;;;;1;acl2023;July 2023
Resolving Indirect Referring Expressions for Entity Selection;Recent advances in language modeling have enabled new conversational systems. In particular, it is often desirable for people to make choices among specified options when using such systems. We address the problem of reference resolution, when people use natural expressions to choose between real world entities. For example, given the choice ‘Should we make a Simnel cake or a Pandan cake¿ a natural response from a non-expert may be indirect: ‘let’s make the green one‘. Reference resolution has been little studied with natural expressions, thus robustly understanding such language has large potential for improving naturalness in dialog, recommendation, and search systems. We create AltEntities (Alternative Entities), a new public dataset of entity pairs and utterances, and develop models for the disambiguation problem. Consisting of 42K indirect referring expressions across three domains, it enables for the first time the study of how large language models can be adapted to this task. We find they achieve 82%-87% accuracy in realistic settings, which while reasonable also invites further advances.;2;;"""We address the problem of reference resolution, when people use natural expressions to choose between real world entities.""";;;;0;;;;;;1;acl2023;July 2023
Knowledge Inheritance for Pre-trained Language Models;Recent explorations of large-scale pre-trained language models (PLMs) have revealed the power of PLMs with huge amounts of parameters, setting off a wave of training ever-larger PLMs. However, it requires tremendous computational resources to train a large-scale PLM, which may be practically unaffordable. In addition, existing large-scale PLMs are mainly trained from scratch individually, ignoring that many well-trained PLMs are available. To this end, we explore the question how could existing PLMs benefit training large-scale PLMs in future. Specifically, we introduce a pre-training framework named “knowledge inheritance” (KI) and explore how could knowledge distillation serve as auxiliary supervision during pre-training to efficiently learn larger PLMs. Experimental results demonstrate the superiority of KI in training efficiency. We also conduct empirical analyses to explore the effects of teacher PLMs’ pre-training settings, including model architecture, pre-training data, etc. Finally, we show that KI could be applied to domain adaptation and knowledge transfer.;2;;"""However, it requires tremendous computational resources to train a large-scale PLM, which may be practically unaffordable.""";;;;2;;"""However, it requires tremendous computational resources to train a large-scale PLM, which may be practically unaffordable. In addition, existing large-scale PLMs are mainly trained from scratch individually, ignoring that many well-trained PLMs are available.""";;;;2;naacl2022;July 2022
Analyzing the Efficacy of an LLM-Only Approach for Image-based Document Question Answering;Recent document question answering models consist of two key components: the  vision encoder, which captures layout and visual elements in images, and a  Large Language Model (LLM) that helps contextualize questions to the image and  supplements them with external world knowledge to generate accurate answers.  However, the relative contributions of the vision encoder and the language  model in these tasks remain unclear. This is especially interesting given the  effectiveness of instruction-tuned LLMs, which exhibit remarkable adaptability  to new tasks. To this end, we explore the following aspects in this work: (1)  The efficacy of an LLM-only approach on document question answering tasks (2)  strategies for serializing textual information within document images and  feeding it directly to an instruction-tuned LLM, thus bypassing the need for an  explicit vision encoder (3) thorough quantitative analysis on the feasibility  of such an approach. Our comprehensive analysis encompasses six diverse  benchmark datasets, utilizing LLMs of varying scales. Our findings reveal that  a strategy exclusively reliant on the LLM yields results that are on par with  or closely approach state-of-the-art performance across a range of datasets. We  posit that this evaluation framework will serve as a guiding resource for  selecting appropriate datasets for future research endeavors that emphasize the  fundamental importance of layout and image content information.;1;investigates contribution of LLMs in document question answering models, but does not mention limitations of LLMs;;;;;1;It evaluates the efficacy of LLMs and compares their performance to vision-encoder-based models only;;;;;1;arxiv;25 September 2023
LiteVL: Efficient Video-Language Learning with Enhanced Spatial-Temporal Modeling;Recent large-scale video-language pre-trained models have shown appealing performance on various downstream tasks. However, the pre-training process is computationally expensive due to the requirement of millions of video-text pairs and the redundant data structure of each video. To mitigate these problems, we propose LiteVL, which adapts a pre-trained image-language model BLIP into a video-text model directly on downstream tasks, without heavy pre-training. To enhance the temporal modeling lacking in the image-language model, we propose to add temporal attention modules in the image encoder of BLIP with dynamic temporal scaling. Besides the model-wise adaptation, we also propose a non-parametric pooling mechanism to adaptively reweight the fine-grained video embedding conditioned on the text. Experimental results on text-video retrieval and video question answering show that the proposed LiteVL even outperforms previous video-language pre-trained models by a clear margin, though without any video-language pre-training.;2;;"""the pre-training process is computationally expensive due to the requirement of millions of video-text pairs and the redundant data structure of each video."", ""temporal modeling lacking in the image-language model""";;;;2;;"""However, the pre-training process is computationally expensive due to the requirement of millions of video-text pairs and the redundant data structure of each video.""";;;;2;emnlp2022;December 2022
Aegis:An Advanced LLM-Based Multi-Agent for Intelligent Functional Safety Engineering;Functional safety is a critical aspect of automotive engineering, encompassing all phases of a vehicle’s lifecycle, including design, development, production, operation, and decommissioning. This domain involves highly knowledge-intensive tasks. This paper introduces Aegis: An Advanced LLM-Based Multi-Agent for Intelligent Functional Safety Engineering. Aegis is specifically designed to support complex functional safety tasks within the automotive sector. It is tailored to perform Hazard Analysis and Risk Assessment (HARA), document Functional Safety Requirements (FSR), and plan test cases for Automatic Emergency Braking (AEB) systems. The most advanced version, Aegis-Max, leverages Retrieval-Augmented Generation (RAG) and reflective mechanisms to enhance its capability in managing complex, knowledge-intensive tasks. Additionally, targeted prompt refinement by professional functional safety practitioners can significantly optimize Aegis’s performance in the functional safety domain. This paper demonstrates the potential of Aegis to improve the efficiency and effectiveness of functional safety processes in automotive engineering.;1;;;;;;1;;;;;;1;emnlp2024;November 2024
SCOTT: Self-Consistent Chain-of-Thought Distillation;"Large language models (LMs) beyond a certain scale, demonstrate the emergent capability of generating free-text rationales for their predictions via chain-of-thought (CoT) prompting. While CoT can yield dramatically improved performance, such gains are only observed for sufficiently large LMs. Even more concerning, there is little guarantee that the generated rationales are consistent with LM’s predictions or faithfully justify the decisions. In this work, we propose SCOTT, a faithful knowledge distillation method to learn a small, self-consistent CoT model from a teacher model that is orders of magnitude larger. To form better supervision, we elicit rationales supporting the gold answers from a large LM (teacher) by contrastive decoding, which encourages the teacher to generate tokens that become more plausible only when the answer is considered. To ensure faithful distillation, we use the teacher-generated rationales to learn a student LM with a counterfactual reasoning objective, which prevents the student from ignoring the rationales to make inconsistent predictions. Experiments show that while yielding comparable performance, our method leads to a more faithful model than baselines. Further analysis shows that such a model respects the rationales more when making decisions; thus, we can improve its performance more by refining its rationales.";2;;"""such gains are only observed for sufficiently large LMs."", ""there is little guarantee that the generated rationales are consistent with LM’s predictions or faithfully justify the decisions""";;;;2;;"""While CoT can yield dramatically improved performance, such gains are only observed for sufficiently large LMs. Even more concerning, there is little guarantee that the generated rationales are consistent with LM’s predictions or faithfully justify the decisions.""";;;;2;acl2023;July 2023
Language Model Sentence Completion with a Parser-Driven Rhetorical Control Method;Controlled text generation (CTG) seeks to guide large language model (LLM) output, that statistical language generation would conform to desired criteria. The current study presents a novel CTG algorithm that enforces adherence toward specific rhetorical relations in an LLM sentence-completion context by a parser-driven decoding scheme that requires no model fine-tuning. The method is validated both with automatic and human evaluation.;1;;;;;;1;;;;;;1;eacl2024;March 2024
Upstream Mitigation Is Not All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models;A few large, homogenous, pre-trained models undergird many machine learning systems — and often, these models contain harmful stereotypes learned from the internet. We investigate the bias transfer hypothesis: the theory that social biases (such as stereotypes) internalized by large language models during pre-training transfer into harmful task-specific behavior after fine-tuning. For two classification tasks, we find that reducing intrinsic bias with controlled interventions before fine-tuning does little to mitigate the classifier’s discriminatory behavior after fine-tuning. Regression analysis suggests that downstream disparities are better explained by biases in the fine-tuning dataset. Still, pre-training plays a role: simple alterations to co-occurrence rates in the fine-tuning dataset are ineffective when the model has been pre-trained. Our results encourage practitioners to focus more on dataset quality and context-specific harms.;3;;"""often, these models contain harmful stereotypes learned from the internet."", ""we find that reducing intrinsic bias with controlled interventions before fine-tuning does little to mitigate the classifier’s discriminatory behavior after fine-tuning.""";;;;5;Fully focused on a limitation - bias transfer;"""A few large, homogenous, pre-trained models undergird many machine learning systems — and often, these models contain harmful stereotypes learned from the internet. We investigate the bias transfer hypothesis: the theory that social biases (such as stereotypes) internalized by large language models during pre-training transfer into harmful task-specific behavior after fine-tuning. For two classification tasks, we find that reducing intrinsic bias with controlled interventions before fine-tuning does little to mitigate the classifier’s discriminatory behavior after fine-tuning.""";;;;4;acl2022;May 2022
Tree of Problems: Improving structured problem solving with compositionality;Large Language Models (LLMs) have demonstrated remarkable performance across multipletasks through in-context learning. For complex reasoning tasks that require step-by-step thinking, Chain-of-Thought (CoT) prompting has given impressive results, especially when combined with self-consistency. Nonetheless, some tasks remain particularly difficult for LLMs to solve. Tree of Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing the complex problem into paths of subproblems. In this paper, we propose Tree of Problems (ToP), a simpler version of ToT, which we hypothesise can work better for complex tasks that can be divided into identical subtasks. Our empirical results show that our approach outperforms ToT and GoT, and in addition per forms better than CoT on complex reasoning tasks. All code for this paper will be made available.;2;;"""Nonetheless, some tasks remain particularly difficult for LLMs to solve.""";;;;2;;"""For complex reasoning tasks"", ""some tasks remain particularly difficult for LLMs to solve.""";;;;2;emnlp2024;November 2024
Life after BERT: What do Other Muppets Understand about Language?;Existing pre-trained transformer analysis works usually focus only on one or two model families at a time, overlooking the variability of the architecture and pre-training objectives. In our work, we utilize the oLMpics bench- mark and psycholinguistic probing datasets for a diverse set of 29 models including T5, BART, and ALBERT. Additionally, we adapt the oLMpics zero-shot setup for autoregres- sive models and evaluate GPT networks of different sizes. Our findings show that none of these models can resolve compositional questions in a zero-shot fashion, suggesting that this skill is not learnable using existing pre-training objectives. Furthermore, we find that global model decisions such as architecture, directionality, size of the dataset, and pre-training objective are not predictive of a model’s linguistic capabilities.;4;comparison of different models on reasoning task, concludes that models can not solve compositional questions;"""Our findings show that none of these models can resolve compositional questions in a zero-shot fashion, suggesting that this skill is not learnable using existing pre-training objectives.""";;;;3;limitations are not the primary focus of the paper, as it emphasizes evaluation across multiple models.;"""Our findings show that none of these models can resolve compositional questions in a zero-shot fashion, suggesting that this skill is not learnable using existing pre-training objectives.""";;;;4;acl2022;May 2022
Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding;A Personalized Query Rewriting system strives to minimize defective queries to ensure robust conversational functionality by considering individual user behavior and preferences. It’s designed as a search-based system, maintaining a user index of past successful interactions with the conversational AI. However, this method faces challenges with unseen interactions, which refers to novel user interactions not covered by the user’s historical index. This paper introduces our Collaborative Query Rewriting approach, which utilizes underlying topological information to assist in rewriting defective queries arising from unseen user interactions. This approach begins by constructing a “User Feedback Interaction Graph” (FIG) using historical user-entity interactions. Subsequently, we traverse through the graph edges to establish an enhanced user index, referred to as the “collaborative user index”. This paper then further explores the use of Large Language Models (LLMs) in conjunction with graph traversal, leading to a significant increase in index coverage for unseen interactions. The effectiveness of our proposed approach has been proven through experiments on a large-scale real-world dataset and online A/B experiments.;1;deals with limitations of PQR systems and how LLMs can be used to overcome those;;;;;1;;;;;;1;emnlp2023;December 2023
ACLSum: A New Dataset for Aspect-based Summarization of Scientific Publications;Extensive efforts in the past have been directed toward the development of summarization datasets. However, a predominant number of these resources have been (semi)-automatically generated, typically through web data crawling. This resulted in subpar resources for training and evaluating summarization systems, a quality compromise that is arguably due to the substantial costs associated with generating ground-truth summaries, particularly for diverse languages and specialized domains. To address this issue, we present ACLSum, a novel summarization dataset carefully crafted and evaluated by domain experts. In contrast to previous datasets, ACLSum facilitates multi-aspect summarization of scientific papers, covering challenges, approaches, and outcomes in depth. Through extensive experiments, we evaluate the quality of our resource and the performance of models based on pretrained language models (PLMs) and state-of-the-art large language models (LLMs). Additionally, we explore the effectiveness of extract-then-abstract versus abstractive end-to-end summarization within the scholarly domain on the basis of automatically discovered aspects. While the former performs comparably well to the end-to-end approach with pretrained language models regardless of the potential error propagation issue, the prompting-based approach with LLMs shows a limitation in extracting sentences from source documents.;3;deals with ability of LLMs to summarize texts and concludes that LLMs show limitations in extracting sentences from documents;"""a quality compromise that is arguably due to the substantial costs associated with generating ground-truth summaries,"", ""the prompting-based approach with LLMs shows a limitation in extracting sentences from source documents.""";;;;2;;"""the prompting-based approach with LLMs shows a limitation in extracting sentences from source documents.""";;;;3;naacl2024;June 2024
Argue with Me Tersely: Towards Sentence-Level Counter-Argument Generation;Counter-argument generation—a captivating area in computational linguistics—seeks to craft statements that offer opposing views. While most research has ventured into paragraph-level generation, sentence-level counter-argument generation beckons with its unique constraints and brevity-focused challenges. Furthermore, the diverse nature of counter-arguments poses challenges for evaluating model performance solely based on n-gram-based metrics. In this paper, we present the ArgTersely benchmark for sentence-level counter-argument generation, drawing from a manually annotated dataset from the ChangeMyView debate forum. We also propose Arg-LlaMA for generating high-quality counter-argument. For better evaluation, we trained a BERT-based evaluator Arg-Judge with human preference data. We conducted comparative experiments involving various baselines such as LlaMA, Alpaca, GPT-3, and others. The results show the competitiveness of our proposed framework and evaluator in counter-argument generation tasks. Code and data are available at https://github.com/amazingljy1206/ArgTersely.;1;;;;;;1;;;;;;1;emnlp2023;December 2023
Cluster & Tune: Boost Cold Start Performance in Text Classification;In real-world scenarios, a text classification task often begins with a cold start, when labeled data is scarce. In such cases, the common practice of fine-tuning pre-trained models, such as BERT, for a target classification task, is prone to produce poor performance. We suggest a method to boost the performance of such models by adding an intermediate unsupervised classification task, between the pre-training and fine-tuning phases. As such an intermediate task, we perform clustering and train the pre-trained model on predicting the cluster labels. We test this hypothesis on various data sets, and show that this additional classification phase can significantly improve performance, mainly for topical classification tasks, when the number of labeled instances available for fine-tuning is only a couple of dozen to a few hundred.;2;;"""prone to produce poor performance.""";;;;1;Limitation of an approach rather than the model;;;;;2;acl2022;May 2022
CarExpert: Leveraging Large Language Models for In-Car Conversational Question Answering;Large language models (LLMs) have demonstrated remarkable performance by following natural language instructions without fine-tuning them on domain-specific tasks and data. However, leveraging LLMs for domain-specific question answering suffers from severe limitations. The generated answer tends to hallucinate due to the training data collection time (when using off-the-shelf), complex user utterance and wrong retrieval (in retrieval-augmented generation). Furthermore, due to the lack of awareness about the domain and expected output, such LLMs may generate unexpected and unsafe answers that are not tailored to the target domain. In this paper, we propose CarExpert, an in-car retrieval-augmented conversational question-answering system leveraging LLMs for different tasks. Specifically, CarExpert employs LLMs to control the input, provide domain-specific documents to the extractive and generative answering components, and controls the output to ensure safe and domain-specific answers. A comprehensive empirical evaluation exhibits that CarExpert outperforms state-of-the-art LLMs in generating natural, safe and car-specific answers.;2;mentions many limitations of LLMs, but only to introduce a fine-tuned/modified LLM;"""However, leveraging LLMs for domain-specific question answering suffers from severe limitations."", ""The generated answer tends to hallucinate due to the training data collection time (when using off-the-shelf), complex user utterance and wrong retrieval (in retrieval-augmented generation). Furthermore, due to the lack of awareness about the domain and expected output, such LLMs may generate unexpected and unsafe answers that are not tailored to the target domain.""";;;;2;;"""However, leveraging LLMs for domain-specific question answering suffers from severe limitations. The generated answer tends to hallucinate due to the training data collection time (when using off-the-shelf), complex user utterance and wrong retrieval (in retrieval-augmented generation). Furthermore, due to the lack of awareness about the domain and expected output, such LLMs may generate unexpected and unsafe answers that are not tailored to the target domain.""";;;;2;emnlp2023;December 2023
Unifying Parsing and Tree-Structured Models for Generating Sentence Semantic Representations;We introduce a novel tree-based model that learns its composition function together with its structure. The architecture produces sentence embeddings by composing words according to an induced syntactic tree. The parsing and the composition functions are explicitly connected and, therefore, learned jointly. As a result, the sentence embedding is computed according to an interpretable linguistic pattern and may be used on any downstream task. We evaluate our encoder on downstream tasks, and we observe that it outperforms tree-based models relying on external parsers. In some configurations, it is even competitive with Bert base model. Our model is capable of supporting multiple parser architectures. We exploit this property to conduct an ablation study by comparing different parser initializations. We explore to which extent the trees produced by our model compare with linguistic structures and how this initialization impacts downstream performances. We empirically observe that downstream supervision troubles producing stable parses and preserving linguistically relevant structures.;0;proposed model is no LLM;;;;;0;While BERT is mentioned as a comparison benchmark, the paper does not discuss LLMs;;;;;0;naacl2022;July 2022
Large Language Models Know What is Key Visual Entity: An LLM-assisted Multimodal Retrieval for VQA;Visual question answering (VQA) tasks, often performed by visual language model (VLM), face challenges with long-tail knowledge. Recent retrieval-augmented VQA (RA-VQA) systems address this by retrieving and integrating external knowledge sources. However, these systems still suffer from redundant visual information irrelevant to the question during retrieval. To address these issues, in this paper, we propose LLM-RA, a novel method leveraging the reasoning capability of a large language model (LLM) to identify key visual entities, thus minimizing the impact of irrelevant information in the query of retriever. Furthermore, key visual entities are independently encoded for multimodal joint retrieval, preventing cross-entity interference. Experimental results demonstrate that our method outperforms other strong RA-VQA systems. In two knowledge-intensive VQA benchmarks, our method achieves the new state-of-the-art performance among those with similar scale of parameters and even performs comparably to models with 1-2 orders larger parameters.;1;only strengths of LLMs are mentioned;;;;;2;;"""Visual question answering (VQA) tasks, often performed by visual language model (VLM), face challenges with long-tail knowledge. Recent retrieval-augmented VQA (RA-VQA) systems address this by retrieving and integrating external knowledge sources. However, these systems still suffer from redundant visual information irrelevant to the question during retrieval.""";;;;2;emnlp2024;November 2024
Does Large Language Model Contain Task-Specific Neurons?;Large language models (LLMs) have demonstrated remarkable capabilities in comprehensively handling various types of natural language processing (NLP) tasks. However, there are significant differences in the knowledge and abilities required for different tasks. Therefore, it is important to understand whether the same LLM processes different tasks in the same way. Are there specific neurons in a LLM for different tasks? Inspired by neuroscience, this paper pioneers the exploration of whether distinct neurons are activated when a LLM handles different tasks. Compared with current research exploring the neurons of language and knowledge, task-specific neurons present a greater challenge due to their abstractness, diversity, and complexity. To address these challenges, this paper proposes a method for task-specific neuron localization based on Causal Gradient Variation with Special Tokens (CGVST). CGVST identifies task-specific neurons by concentrating on the most significant tokens during task processing, thereby eliminating redundant tokens and minimizing interference from non-essential neurons. Compared to traditional neuron localization methods, our approach can more effectively identify task-specific neurons. We conduct experiments across eight different public tasks. Experiments involving the inhibition and amplification of identified neurons demonstrate that our method can accurately locate task-specific neurons.;2;;"""task-specific neurons present a greater challenge due to their abstractness, diversity, and complexity.""";;;;1;;;;;;2;emnlp2024;November 2024
Are Prompt-based Models Clueless?;Finetuning large pre-trained language models with a task-specific head has advanced the state-of-the-art on many natural language understanding benchmarks. However, models with a task-specific head require a lot of training data, making them susceptible to learning and exploiting dataset-specific superficial cues that do not generalize to other datasets. Prompting has reduced the data requirement by reusing the language model head and formatting the task input to match the pre-training objective. Therefore, it is expected that few-shot prompt-based models do not exploit superficial cues. This paper presents an empirical examination of whether few-shot prompt-based models also exploit superficial cues. Analyzing few-shot prompt-based models on MNLI, SNLI, HANS, and COPA has revealed that prompt-based models also exploit superficial cues.While the models perform well on instances with superficial cues, they often underperform or only marginally outperform random accuracy on instances without superficial cues.;4;investigates bias of LLMs and concludes that few-shot prompt based models are still biased;"""However, models with a task-specific head require a lot of training data, making them susceptible to learning and exploiting dataset-specific superficial cues that do not generalize to other datasets."", ""Analyzing few-shot prompt-based models on MNLI, SNLI, HANS, and COPA has revealed that prompt-based models also exploit superficial cues."", ""they often underperform or only marginally outperform random accuracy on instances without superficial cues.""";;;;3;maybe 4, but this is more empirical exploration ;"""However, models with a task-specific head require a lot of training data, making them susceptible to learning and exploiting dataset-specific superficial cues that do not generalize to other datasets."", ""Analyzing few-shot prompt-based models on MNLI, SNLI, HANS, and COPA has revealed that prompt-based models also exploit superficial cues."", ""While the models perform well on instances with superficial cues, they often underperform or only marginally outperform random accuracy on instances without superficial cues.""";;;;4;acl2022;May 2022
On Fake News Detection with LLM Enhanced Semantics Mining;Large language models (LLMs) have emerged as valuable tools for enhancing textual features in various text-related tasks. Despite their superiority in capturing the lexical semantics between tokens for text analysis, our preliminary study on two popular LLMs, i.e., ChatGPT and Llama2, showcases that simply applying the news embeddings from LLMs is ineffective for fake news detection. Such embeddings only encapsulate the language styles between tokens. Meanwhile, the high-level semantics among named entities and topics, which reveal the deviating patterns of fake news, have been ignored. Therefore, we propose a topic model together with a set of specially designed prompts to extract topics and real entities from LLMs and model the relations among news, entities, and topics as a heterogeneous graph to facilitate investigating news semantics. We then propose a Generalized Page-Rank model and a consistent learning criteria for mining the local and global semantics centered on each news piece through the adaptive propagation of features across the graph. Our model shows superior performance on five benchmark datasets over seven baseline methods and the efficacy of the key ingredients has been thoroughly validated.;2;limitations are only mentioned as motivitation for own approach;"""simply applying the news embeddings from LLMs is ineffective for fake news detection."", ""Such embeddings only encapsulate the language styles between tokens. Meanwhile, the high-level semantics among named entities and topics, which reveal the deviating patterns of fake news, have been ignored.""";;;;2;;"""our preliminary study on two popular LLMs, i.e., ChatGPT and Llama2, showcases that simply applying the news embeddings from LLMs is ineffective for fake news detection. Such embeddings only encapsulate the language styles between tokens. Meanwhile, the high-level semantics among named entities and topics, which reveal the deviating patterns of fake news, have been ignored.""";;;;2;emnlp2024;November 2024
Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs;"Social intelligence and Theory of Mind (TOM), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allows humans to effectively navigate and understand everyday social interactions. As NLP systems are used in increasingly complex social situations, their ability to grasp social dynamics becomes crucial.In this work, we examine the open question of social intelligence and Theory of Mind in modern NLP systems from an empirical and theorybased perspective. We show that one of today’s largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et al., 2019), which measure models’ ability to understand intents and reactions of participants of social interactions, and ToMi (Le, Boureau, and Nickel, 2019), which measures whether models can infer mental states and realities of participants of situations.Our results show that models struggle substantially at these Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on SocialIQa and ToMi, respectively. To conclude, we draw on theories from pragmatics to contextualize this shortcoming of large language models, by examining the limitations stemming from their data, neural architecture, and training paradigms. Challenging the prevalent narrative that only scale is needed, we posit that person-centric NLP approaches might be more effective towards neural Theory of Mind.";4;examines social intelligence of LLMs and concludes that they perform very poorly at tasks related to that skill. Then tries to find the reasons for that limitation and suggests possible solution to solve that problem;"""one of today’s largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box"", ""Our results show that models struggle substantially at these Theory of Mind tasks"", ""we draw on theories from pragmatics to contextualize this shortcoming of large language models, by examining the limitations stemming from their data, neural architecture, and training paradigms.""";;;;4;;"""We show that one of today’s largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et al., 2019), which measure models’ ability to understand intents and reactions of participants of social interactions, and ToMi (Le, Boureau, and Nickel, 2019), which measures whether models can infer mental states and realities of participants of situations.Our results show that models struggle substantially at these Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on SocialIQa and ToMi, respectively. To conclude, we draw on theories from pragmatics to contextualize this shortcoming of large language models, by examining the limitations stemming from their data, neural architecture, and training paradigms.""";;;;4;emnlp2022;December 2022
We’re Afraid Language Models Aren’t Modeling Ambiguity;Ambiguity is an intrinsic feature of natural language. Managing ambiguity is a key part of human language understanding, allowing us to anticipate misunderstanding as communicators and revise our interpretations as listeners. As language models are increasingly employed as dialogue interfaces and writing aids, handling ambiguous language is critical to their success. We capture ambiguity in a sentence through its effect on entailment relations with another sentence, and collect AmbiEnt, a linguist-annotated benchmark of 1,645 examples with diverse kinds of ambiguity. We design a suite of tests based on AmbiEnt, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings. We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32% of the time in crowdworker evaluation, compared to 90% for disambiguations in our dataset. Finally, to illustrate the value of ambiguity-sensitive tools, we show that a multilabel NLI model can flag political claims in the wild that are misleading due to ambiguity. We encourage the field to rediscover the importance of ambiguity for NLP.;4;investigates LLMs ability to manage language ambiguity and concludes that LLMs are struggling with that task;"""We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32% of the time in crowdworker evaluation""";;;;4;Or maybe 5;"""We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32% of the time in crowdworker evaluation, compared to 90% for disambiguations in our dataset. Finally, to illustrate the value of ambiguity-sensitive tools, we show that a multilabel NLI model can flag political claims in the wild that are misleading due to ambiguity.""";;;;4;emnlp2023;December 2023
ThinkSum: Probabilistic reasoning over sets using large language models;Large language models (LLMs) have a substantial capacity for high-level analogical reasoning: reproducing patterns in linear text that occur in their training data (zero-shot evaluation) or in the provided context (few-shot in-context learning). However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions. We propose a two-stage probabilistic inference paradigm, ThinkSum, which reasons over sets of objects or facts in a structured manner. In the first stage (Think – retrieval of associations), a LLM is queried in parallel over a set of phrases extracted from the prompt or an auxiliary model call. In the second stage (Sum – probabilistic inference or reasoning), the results of these queries are aggregated to make the final prediction. We demonstrate the possibilities and advantages of ThinkSum on the BIG-bench suite of LLM evaluation tasks, achieving improvements over the state of the art using GPT-family models on thirteen difficult tasks, often with far smaller model variants. We also compare and contrast ThinkSum with other proposed modifications to direct prompting of LLMs, such as variants of chain-of-thought prompting. Our results suggest that because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, ThinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs. Overall, our proposed paradigm represents a promising approach for enhancing the reasoning capabilities of LLMs.;2;;"""However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions.""";;;;2;;"""However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions.""";;;;2;acl2023;July 2023
Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators through a User-Centric Method;Extensive efforts in automated approaches for content moderation have been focused on developing models to identify toxic, offensive, and hateful content with the aim of lightening the load for moderators. Yet, it remains uncertain whether improvements on those tasks have truly addressed moderators’ needs in accomplishing their work. In this paper, we surface gaps between past research efforts that have aimed to provide automation for aspects of content moderation and the needs of volunteer content moderators, regarding identifying violations of various moderation rules. To do so, we conduct a model review on Hugging Face to reveal the availability of models to cover various moderation rules and guidelines from three exemplar forums. We further put state-of-the-art LLMs to the test, evaluating how well these models perform in flagging violations of platform rules from one particular forum. Finally, we conduct a user survey study with volunteer moderators to gain insight into their perspectives on useful moderation models. Overall, we observe a non trivial gap, as missing developed models and LLMs exhibit moderate to low performance on a significant portion of the rules. Moderators’ reports provide guides for future work on developing moderation assistant models.;4;investigates LLMs ability on content moderation and concludes that there are limitations. 4-5;"""Overall, we observe a non trivial gap, as missing developed models and LLMs exhibit moderate to low performance on a significant portion of the rules.""";;;;4;Could be 5;"""We further put state-of-the-art LLMs to the test, evaluating how well these models perform in flagging violations of platform rules from one particular forum."", ""Overall, we observe a non trivial gap, as missing developed models and LLMs exhibit moderate to low performance on a significant portion of the rules.""";;;;4;emnlp2024;November 2024
“Global is Good, Local is Bad?”: Understanding Brand Bias in LLMs;Many recent studies have investigated social biases in LLMs but brand bias has received little attention. This research examines the biases exhibited by LLMs towards different brands, a significant concern given the widespread use of LLMs in affected use cases such as product recommendation and market analysis. Biased models may perpetuate societal inequalities, unfairly favoring established global brands while marginalizing local ones. Using a curated dataset across four brand categories, we probe the behavior of LLMs in this space. We find a consistent pattern of bias in this space—both in terms of disproportionately associating global brands with positive attributes and disproportionately recommending luxury gifts for individuals in high-income countries. We also find LLMs are subject to country-of-origin effects which may boost local brand preference in LLM outputs in specific contexts.;5;fully focused on investigating brand bias in LLMs, concludes that they are biased;"""We find a consistent pattern of bias in this space—both in terms of disproportionately associating global brands with positive attributes and disproportionately recommending luxury gifts for individuals in high-income countries. We also find LLMs are subject to country-of-origin effects which may boost local brand preference in LLM outputs in specific contexts.""";;;;5;Thorough exploration of a limitation - brand bias in LLMs;"""This research examines the biases exhibited by LLMs towards different brands, a significant concern given the widespread use of LLMs in affected use cases such as product recommendation and market analysis."", ""We find a consistent pattern of bias in this space—both in terms of disproportionately associating global brands with positive attributes and disproportionately recommending luxury gifts for individuals in high-income countries. We also find LLMs are subject to country-of-origin effects which may boost local brand preference in LLM outputs in specific contexts.""";;;;5;emnlp2024;November 2024
Emoji Prediction in Tweets using BERT;In recent years, the use of emojis in social media has increased  dramatically, making them an important element in understanding online  communication. However, predicting the meaning of emojis in a given text is a  challenging task due to their ambiguous nature. In this study, we propose a  transformer-based approach for emoji prediction using BERT, a widely-used  pre-trained language model. We fine-tuned BERT on a large corpus of text  (tweets) containing both text and emojis to predict the most appropriate emoji  for a given text. Our experimental results demonstrate that our approach  outperforms several state-of-the-art models in predicting emojis with an  accuracy of over 75 percent. This work has potential applications in natural  language processing, sentiment analysis, and social media marketing.;1;;;;;;1;;;;;;1;arxiv;05 July 2023
CASA: Causality-driven Argument Sufficiency Assessment;The argument sufficiency assessment task aims to determine if the premises of a given argument support its conclusion.To tackle this task, existing works often train a classifier on data annotated by humans. However, annotating data is laborious, and annotations are often inconsistent due to subjective criteria. Motivated by the definition of probability of sufficiency (PS) in the causal literature, we proposeCASA, a zero-shot causality-driven argument sufficiency assessment framework. PS measures how likely introducing the premise event would lead to the conclusion when both the premise and conclusion events are absent. To estimate this probability, we propose to use large language models (LLMs) to generate contexts that are inconsistent with the premise and conclusion and revise them by injecting the premise event.Experiments on two logical fallacy detection datasets demonstrate that CASA accurately identifies insufficient arguments. We further deploy CASA in a writing assistance application, and find that suggestions generated by CASA enhance the sufficiency of student-written arguments. Code and data are available at https://github.com/xxxiaol/CASA.;1;;;;;;1;;;;;;1;naacl2024;June 2024
Low-Rank Few-Shot Adaptation of Vision-Language Models;Recent progress in the few-shot adaptation of Vision-Language Models (VLMs)  has further pushed their generalization capabilities, at the expense of just a  few labeled samples within the target downstream task. However, this promising,  already quite abundant few-shot literature has focused principally on prompt  learning and, to a lesser extent, on adapters, overlooking the recent advances  in Parameter-Efficient Fine-Tuning (PEFT). Furthermore, existing few-shot  learning methods for VLMs often rely on heavy training procedures and/or  carefully chosen, task-specific hyper-parameters, which might impede their  applicability. In response, we introduce Low-Rank Adaptation (LoRA) in few-shot  learning for VLMs, and show its potential on 11 datasets, in comparison to  current state-of-the-art prompt- and adapter-based approaches. Surprisingly,  our simple CLIP-LoRA method exhibits substantial improvements, while reducing  the training times and keeping the same hyper-parameters in all the target  tasks, i.e., across all the datasets and numbers of shots. Certainly, our  surprising results do not dismiss the potential of prompt-learning and  adapter-based research. However, we believe that our strong baseline could be  used to evaluate progress in these emergent subjects in few-shot VLMs.;2;;"""existing few-shot  learning methods for VLMs often rely on heavy training procedures and/or  carefully chosen, task-specific hyper-parameters, which might impede their  applicability.""";;;;2;Maybe even 1 since the limitation of few-shot learning is mentioned not of LLMs;"""Furthermore, existing few-shot  learning methods for VLMs often rely on heavy training procedures and/or  carefully chosen, task-specific hyper-parameters, which might impede their  applicability.""";;;;2;arxiv;28 May 2024
Evaluating Pre-Trained Sentence-BERT with Class Embeddings in Active Learning for Multi-Label Text Classification;The Transformer Language Model is a powerful tool that has been shown to excel at various NLP tasks and has become the de-facto standard solution thanks to its versatility. In this study, we employ pre-trained document embeddings in an Active Learning task to group samples with the same labels in the embedding space on a legal document corpus. We find that the calculated class embeddings are not close to the respective samples and consequently do not partition the embedding space in a meaningful way. In addition, we explore using the class embeddings as an Active Learning strategy with dramatically reduced results compared to all baselines.;4;deals with limitations of LLMs in an active Learning task;"""We find that the calculated class embeddings are not close to the respective samples and consequently do not partition the embedding space in a meaningful way. In addition, we explore using the class embeddings as an Active Learning strategy with dramatically reduced results compared to all baselines.""";;;;3;identifies limitations of Transformer-based LLMs in embedding space partitioning and Active Learning strategies but does not make these limitations central;"""We find that the calculated class embeddings are not close to the respective samples and consequently do not partition the embedding space in a meaningful way. In addition, we explore using the class embeddings as an Active Learning strategy with dramatically reduced results compared to all baselines.""";;;;4;aacl2022;September 2022
Topic Modeling by Clustering Language Model Embeddings: Human Validation on an Industry Dataset;Topic models are powerful tools to get an overview of large collections of text data, a situation that is prevalent in industry applications. A rising trend within topic modeling is to directly cluster dimension-reduced embeddings created with pretrained language models. It is difficult to evaluate these models because there is no ground truth and automatic measurements may not mimic human judgment. To address this problem, we created a tool called STELLAR for interactive topic browsing which we used for human evaluation of topics created from a real-world dataset used in industry. Embeddings created with BERT were used together with UMAP and HDBSCAN to model the topics. The human evaluation found that our topic model creates coherent topics. The following discussion revolves around the requirements of industry and what research is needed for production-ready systems.;2;;"""It is difficult to evaluate these models""";;;;1;;;;;;2;emnlp2022;December 2022
The Power of Prompt Tuning for Low-Resource Semantic Parsing;Prompt tuning has recently emerged as an effective method for adapting pre-trained language models to a number of language understanding and generation tasks. In this paper, we investigate prompt tuning for semantic parsing—the task of mapping natural language utterances onto formal meaning representations. On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart, as well as strong GPT-3 and BART baselines. We also conduct ablation studies across different model scales and target representations, finding that, with increasing model scale, prompt tuned T5 models improve at generating target representations that are far from the pre-training distribution.;1;;;;;;1;;;;;;1;acl2022;May 2022
Improving Socratic Question Generation using Data Augmentation and Preference Optimization;The Socratic method is a way of guiding students toward solving a problem independently without directly revealing the solution to the problem. Although this method has been shown to significantly improve student learning outcomes, it remains a complex labor-intensive task for instructors. Large language models (LLMs) can be used to augment human effort by automatically generating Socratic questions for students. However, existing methods that involve prompting these LLMs sometimes produce invalid outputs, e.g., those that directly reveal the solution to the problem or provide irrelevant or premature questions. To alleviate this problem, inspired by reinforcement learning with AI feedback (RLAIF), we first propose a data augmentation method to enrich existing Socratic questioning datasets with questions that are invalid in specific ways. Next, we propose a method to optimize open-source LLMs such as LLama 2 to prefer ground-truth questions over generated invalid ones, using direct preference optimization (DPO). Our experiments on a Socratic questions dataset for student code debugging show that a DPO-optimized 7B LLama 2 model can effectively avoid generating invalid questions, and as a result, outperforms existing state-of-the-art prompting methods.;2;;"""However, existing methods that involve prompting these LLMs sometimes produce invalid outputs, e.g., those that directly reveal the solution to the problem or provide irrelevant or premature questions.""";;;;2;Even 1 maybe since the limitation is of a method, not an LLM;"""However, existing methods that involve prompting these LLMs sometimes produce invalid outputs, e.g., those that directly reveal the solution to the problem or provide irrelevant or premature questions.""";;;;2;arxiv;01 March 2024
Automatic Generation of Socratic Subquestions for Teaching Math Word Problems;Socratic questioning is an educational method that allows students to discover answers to complex problems by asking them a series of thoughtful questions. Generation of didactically sound questions is challenging, requiring understanding of the reasoning process involved in the problem. We hypothesize that such questioning strategy can not only enhance the human performance, but also assist the math word problem (MWP) solvers.In this work, we explore the ability of large language models (LMs) in generating sequential questions for guiding math word problem-solving. We propose various guided question generation schemes based on input conditioning and reinforcement learning.On both automatic and human quality evaluations, we find that LMs constrained with desirable question properties generate superior questions and improve the overall performance of a math word problem solver. We conduct a preliminary user study to examine the potential value of such question generation models in the education domain. Results suggest that the difficulty level of problems plays an important role in determining whether questioning improves or hinders human performance. We discuss the future of using such questioning strategies in education.;1;;;;;;1;;;;;;1;emnlp2022;December 2022
Learning Preference Model for LLMs via Automatic Preference Data Generation;Despite the advanced capacities of the state-of-the-art large language models (LLMs), they suffer from issues of hallucination, stereotype, etc. Preference models play an important role in LLM alignment, yet training preference models predominantly rely on human-annotated data. This reliance limits their versatility and scalability. In this paper, we propose learning the preference model for LLMs via automatic preference data generation (AutoPM). Our approach involves both In-Breadth Data Generation, which elicits pairwise preference data from LLMs following the helpful-honest-harmless (HHH) criteria, and In-Depth Data Generation, which enriches the dataset with responses spanning a wide quality range. With HHH-guided preference data, our approach simultaneously enables the LLMs to learn human preferences and align with human values. Quantitative assessments on five benchmark datasets demonstrate the reliability and potential of AutoPM, pointing out a more general and scalable way to improve LLM performance.;2;;"""they suffer from issues of hallucination, stereotype, etc."", ""training preference models predominantly rely on human-annotated data. This reliance limits their versatility and scalability.""";;;;2;;"""Despite the advanced capacities of the state-of-the-art large language models (LLMs), they suffer from issues of hallucination, stereotype, etc.""";;;;2;emnlp2023;December 2023
SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization;Data scarcity has been a long standing issue in the field of open-domain social dialogue. To quench this thirst, we present SODA: the first publicly available, million-scale high-quality social dialogue dataset. By contextualizing social commonsense knowledge from a knowledge graph, we are able to distill an exceptionally broad spectrum of social interactions from a large language model. Human evaluation shows that conversations in SODA are more consistent, specific, and (surprisingly) natural than those in prior human-authored datasets. Using SODA, we train COSMO: a generalizable conversation model that is significantly more natural and consistent on unseen datasets than best-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna). Experiments reveal COSMO is sometimes even preferred to the original human-written gold responses. Additionally, our results shed light on the distinction between knowledge-enriched conversations and natural social chitchats. We plan to make our data, model, and code public.;1;;;;;;1;;;;;;1;emnlp2023;December 2023
DocFinQA: A Long-Context Financial Reasoning Dataset;For large language models (LLMs) to be effective in the financial domain – where each decision can have a significant impact – it is necessary to investigate realistic tasks and data. Financial professionals often interact with documents spanning hundreds of pages, but most financial research datasets only deal with short excerpts from these documents. To address this, we introduce a long-document financial QA task. We augment 7,437 questions from the existing FinQA dataset with full-document context, extending the average context length from under 700 words in FinQA to 123k words in DocFinQA. We conduct extensive experiments over retrieval-based QA pipelines and long-context language models. Based on our experiments, DocFinQA proves a significant challenge for even state-of-the-art systems. We also provide a case study on a subset of the longest documents in DocFinQA and find that models particularly struggle with these documents. Addressing these challenges may have a wide-reaching impact across applications where specificity and long-range contexts are critical, like gene sequences and legal document contract analysis. DocFinQA dataset is publicly accessible.;3;presents limitations of LLMs interacting with financial documents, focuses also on the details of the benchmark proposed;"""DocFinQA proves a significant challenge for even state-of-the-art systems."", ""We also provide a case study on a subset of the longest documents in DocFinQA and find that models particularly struggle with these documents.""";;;;4;focus is split between the dataset, evaluations, and analyzing LLM limitations;"""Based on our experiments, DocFinQA proves a significant challenge for even state-of-the-art systems. We also provide a case study on a subset of the longest documents in DocFinQA and find that models particularly struggle with these documents. Addressing these challenges may have a wide-reaching impact across applications where specificity and long-range contexts are critical, like gene sequences and legal document contract analysis.""";;;;4;acl2024;August 2024
Let the CAT out of the bag: Contrastive Attributed explanations for Text;Contrastive explanations for understanding the behavior of black box models has gained a lot of attention recently as they provide potential for recourse. In this paper, we propose a method Contrastive Attributed explanations for Text (CAT) which provides contrastive explanations for natural language text data with a novel twist as we build and exploit attribute classifiers leading to more semantically meaningful explanations. To ensure that our contrastive generated text has the fewest possible edits with respect to the original text, while also being fluent and close to a human generated contrastive, we resort to a minimal perturbation approach regularized using a BERT language model and attribute classifiers trained on available attributes. We show through qualitative examples and a user study that our method not only conveys more insight because of these attributes, but also leads to better quality (contrastive) text. Quantitatively, we show that our method outperforms other state-of-the-art methods across four data sets on four benchmark metrics.;1;;;;;;1;;;;;;1;emnlp2022;December 2022
Improving Complex Knowledge Base Question Answering via Question-to-Action and Question-to-Question Alignment;Complex knowledge base question answering can be achieved by converting questions into sequences of predefined actions. However, there is a significant semantic and structural gap between natural language and action sequences, which makes this conversion difficult. In this paper, we introduce an alignment-enhanced complex question answering framework, called ALCQA, which mitigates this gap through question-to-action alignment and question-to-question alignment. We train a question rewriting model to align the question and each action, and utilize a pretrained language model to implicitly align the question and KG artifacts. Moreover, considering that similar questions correspond to similar action sequences, we retrieve top-k similar question-answer pairs at the inference stage through question-to-question alignment and propose a novel reward-guided action sequence selection strategy to select from candidate action sequences. We conduct experiments on CQA and WQSP datasets, and the results show that our approach outperforms state-of-the-art methods and obtains a 9.88% improvements in the F1 metric on CQA dataset. Our source code is available at https://github.com/TTTTTTTTy/ALCQA.;1;only mentions challenge of question answering tasks, uses LLMs to solve those;;;;;0;;;;;;1;emnlp2022;December 2022
MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation;Pre-trained language models have demonstrated superior performance in various natural language processing tasks. However, these models usually contain hundreds of millions of parameters, which limits their practicality because of latency requirements in real-world applications.Existing methods train small compressed models via knowledge distillation. However, performance of these small models drops significantly compared with the pre-trained models due to their reduced model capacity. We propose MoEBERT, which uses a Mixture-of-Experts structure to increase model capacity and inference speed. We initialize MoEBERT by adapting the feed-forward neural networks in a pre-trained model into multiple experts. As such, representation power of the pre-trained model is largely retained. During inference, only one of the experts is activated, such that speed can be improved. We also propose a layer-wise distillation method to train MoEBERT. We validate the efficiency and efficacy of MoEBERT on natural language understanding and question answering tasks. Results show that the proposed method outperforms existing task-specific distillation algorithms. For example, our method outperforms previous approaches by over 2% on the MNLI (mismatched) dataset. Our code is publicly available at https://github.com/SimiaoZuo/MoEBERT.;2;;"""However, these models usually contain hundreds of millions of parameters, which limits their practicality because of latency requirements in real-world applications."", ""However, performance of these small models drops significantly compared with the pre-trained models due to their reduced model capacity.""";;;;2;;"""However, these models usually contain hundreds of millions of parameters, which limits their practicality because of latency requirements in real-world applications."", ""Existing methods train small compressed models via knowledge distillation. However, performance of these small models drops significantly compared with the pre-trained models due to their reduced model capacity.""";;;;2;naacl2022;July 2022
Multilingual unsupervised sequence segmentation transfers to extremely low-resource languages;We show that unsupervised sequence-segmentation performance can be transferred to extremely low-resource languages by pre-training a Masked Segmental Language Model (Downey et al., 2021) multilingually. Further, we show that this transfer can be achieved by training over a collection of low-resource languages that are typologically similar (but phylogenetically unrelated) to the target language. In our experiments, we transfer from a collection of 10 Indigenous American languages (AmericasNLP, Mager et al., 2021) to K’iche’, a Mayan language. We compare our multilingual model to a monolingual (from-scratch) baseline, as well as a model pre-trained on Quechua only. We show that the multilingual pre-trained approach yields consistent segmentation quality across target dataset sizes, exceeding the monolingual baseline in 6/10 experimental settings. Our model yields especially strong results at small target sizes, including a zero-shot performance of 20.6 F1. These results have promising implications for low-resource NLP pipelines involving human-like linguistic units, such as the sparse transcription framework proposed by Bird (2020).;1;;;;;;1;;;;;;1;acl2022;May 2022
Learning Dynamic Contextualised Word Embeddings via Template-based Temporal Adaptation;Dynamic contextualised word embeddings (DCWEs) represent the temporal semantic variations of words. We propose a method for learning DCWEs by time-adapting a pretrained Masked Language Model (MLM) using time-sensitive templates. Given two snapshots C1 and C2 of a corpus taken respectively at two distinct timestamps T1 and T2, we first propose an unsupervised method to select (a) pivot terms related to both C1 and C2, and (b) anchor terms that are associated with a specific pivot term in each individual snapshot.We then generate prompts by filling manually compiled templates using the extracted pivot and anchor terms.Moreover, we propose an automatic method to learn time-sensitive templates from C1 and C2, without requiring any human supervision.Next, we use the generated prompts to adapt a pretrained MLM to T2 by fine-tuning using those prompts.Multiple experiments show that our proposed method significantly reduces the perplexity of test sentences in C2, outperforming the current state-of-the-art.;1;;;;;;1;;;;;;1;acl2023;July 2023
Instructing Large Language Models to Identify and Ignore Irrelevant Conditions;Math word problem (MWP) solving requires generating a reasoning path based on a given problem description that often contains irrelevant conditions.Existing chain-of-thought (CoT) prompting methods elicited multi-step reasoning abilities of large language models (LLMs) to solve MWPs.However, they were seriously confused by the irrelevant conditions, resulting in low accuracy.In this paper, we propose a novel approach named I3C that instructs LLMs to identify and ignore irrelevant conditions.It identifies a set of irrelevant condition candidates that have a weak semantic relevance with the question.Then it prompts LLMs to verify the irrelevant conditions.Lastly it instructs the LLMs with the verification on relevant and irrelevant conditions to avoid confusion and improve reasoning paths.Moreover, we propose to select (problem, reasoning paths) pairs as demonstrations to enhance I3C with few-shot reasoning. We develop I3C-Select that selects the most confusing problems based on the semantic relevance measurement.We conduct extensive experiments on eight MWP datasets.I3C can be combined with any CoT prompting methods to improve the performance of solving MWPs.Notably, with GPT-3.5-Turbo and I3C-Select, we achieve an accuracy of 96.0 and 94.1 on GSM-IC2-1K and GSM-ICM-1K, respectively, significantly outperforming the state-of-the-art few-shot prompting method Complex-CoT by +11.7 and +11.1.Our implementation is made publicly available at https://wzy6642.github.io/I3C.github.io/.;2;;"""However, they were seriously confused by the irrelevant conditions, resulting in low accuracy.""";;;;2;Might be 1 - this is a shortcoming of a method;"""Existing chain-of-thought (CoT) prompting methods"", ""were seriously confused by the irrelevant conditions, resulting in low accuracy.""";;;;2;naacl2024;June 2024
Fine-grained Gender Control in Machine Translation with Large Language Models;In machine translation, the problem of ambiguously gendered input has been pointed out, where the gender of an entity is not available in the source sentence. To address this ambiguity issue, the task of controlled translation that takes the gender of the ambiguous entity as additional input have been proposed. However, most existing works have only considered a simplified setup of one target gender for input. In this paper, we tackle controlled translation in a more realistic setting of inputs with multiple entities and propose Gender-of-Entity (GoE) prompting method for LLMs. Our proposed method instructs the model with fine-grained entity-level gender information to translate with correct gender inflections. By utilizing four evaluation benchmarks, we investigate the controlled translation capability of LLMs in multiple dimensions and find that LLMs reach state-of-the-art performance in controlled translation. Furthermore, we discover an emergence of gender interference phenomenon when controlling the gender of multiple entities. Finally, we address the limitations of existing gender accuracy evaluation metrics and propose leveraging LLMs as an evaluator for gender inflection in machine translation.;1;;;;;;1;;;;;;1;naacl2024;June 2024
KALM: Knowledge-Aware Integration of Local, Document, and Global Contexts for Long Document Understanding;With the advent of pre-trained language models (LMs), increasing research efforts have been focusing on infusing commonsense and domain-specific knowledge to prepare LMs for downstream tasks. These works attempt to leverage knowledge graphs, the de facto standard of symbolic knowledge representation, along with pre-trained LMs. While existing approaches leverage external knowledge, it remains an open question how to jointly incorporate knowledge graphs represented in varying contexts — from local (e.g., sentence), document-level, to global knowledge, to enable knowledge-rich and interpretable exchange across contexts. In addition, incorporating varying contexts can especially benefit long document understanding tasks that leverage pre-trained LMs, typically bounded by the input sequence length. In light of these challenges, we propose KALM, a language model that jointly leverages knowledge in local, document-level, and global contexts for long document understanding. KALM firstly encodes long documents and knowledge graphs into the three knowledge-aware context representations. KALM then processes each context with context-specific layers. These context-specific layers are followed by a ContextFusion layer that facilitates knowledge exchange to derive an overarching document representation. Extensive experiments demonstrate that KALM achieves state-of-the-art performance on three long document understanding tasks across 6 datasets/settings. Further analyses reveal that the three knowledge-aware contexts are complementary and they all contribute to model performance, while the importance and information exchange patterns of different contexts vary on different tasks and datasets.;1;;;;;;2;;"""incorporating varying contexts can especially benefit long document understanding tasks that leverage pre-trained LMs, typically bounded by the input sequence length""";;;;2;acl2023;July 2023
Is Neural Topic Modelling Better than Clustering? An Empirical Study on Clustering with Contextual Embeddings for Topics;Recent work incorporates pre-trained word embeddings such as BERT embeddings into Neural Topic Models (NTMs), generating highly coherent topics. However, with high-quality contextualized document representations, do we really need sophisticated neural models to obtain coherent and interpretable topics? In this paper, we conduct thorough experiments showing that directly clustering high-quality sentence embeddings with an appropriate word selecting method can generate more coherent and diverse topics than NTMs, achieving also higher efficiency and simplicity.;1;;;;;;1;;;;;;1;naacl2022;July 2022
Planning Like Human: A Dual-process Framework for Dialogue Planning;In proactive dialogue, the challenge lies not just in generating responses but in steering conversations toward predetermined goals, a task where Large Language Models (LLMs) typically struggle due to their reactive nature. Traditional approaches to enhance dialogue planning in LLMs, ranging from elaborate prompt engineering to the integration of policy networks, either face efficiency issues or deliver suboptimal performance. Inspired by the dual-process theory in psychology, which identifies two distinct modes of thinking—intuitive (fast) and analytical (slow), we propose the Dual-Process Dialogue Planning (DPDP) framework. DPDP embodies this theory through two complementary planning systems: an instinctive policy model for familiar contexts and a deliberative Monte Carlo Tree Search (MCTS) mechanism for complex, novel scenarios. This dual strategy is further coupled with a novel two-stage training regimen: offline Reinforcement Learning for robust initial policy model formation followed by MCTS-enhanced on-the-fly learning, which ensures a dynamic balance between efficiency and strategic depth. Our empirical evaluations across diverse dialogue tasks affirm DPDP’s superiority in achieving both high-quality dialogues and operational efficiency, outpacing existing methods.;2;;"""In proactive dialogue, the challenge lies not just in generating responses but in steering conversations toward predetermined goals, a task where Large Language Models (LLMs) typically struggle due to their reactive nature"", ""face efficiency issues or deliver suboptimal performance.""";;;;2;;"""In proactive dialogue, the challenge lies not just in generating responses but in steering conversations toward predetermined goals, a task where Large Language Models (LLMs) typically struggle due to their reactive nature.""";;;;2;acl2024;August 2024
GPT Self-Supervision for a Better Data Annotator;The task of annotating data into concise summaries poses a significant challenge across various domains, frequently requiring the allocation of significant time and specialized knowledge by human experts. Despite existing efforts to use large language models for annotation tasks, significant problems such as limited applicability to unlabeled data, the absence of self-supervised methods, and the lack of focus on complex structured data still persist. In this work, we propose a GPT self-supervision annotation method, which embodies a generating-recovering paradigm that leverages the one-shot learning capabilities of the Generative Pretrained Transformer (GPT). The proposed approach comprises a one-shot tuning phase followed by a generation phase. In the one-shot tuning phase, we sample a data from the support set as part of the prompt for GPT to generate a textual summary, which is then used to recover the original data. The alignment score between the recovered and original data serves as a self-supervision navigator to refine the process. In the generation stage, the optimally selected one-shot sample serves as a template in the prompt and is applied to generating summaries from challenging datasets. The annotation performance is evaluated by tuning several human feedback reward networks and by calculating alignment scores between original and recovered data at both sentence and structure levels. Our self-supervised annotation method consistently achieves competitive scores, convincingly demonstrating its robust strength in various data-to-summary annotation tasks.;2;;"""significant problems such as limited applicability to unlabeled data, the absence of self-supervised methods, and the lack of focus on complex structured data still persist.""";;;;2;;"""Despite existing efforts to use large language models for annotation tasks, significant problems such as limited applicability to unlabeled data, the absence of self-supervised methods, and the lack of focus on complex structured data still persist.""";;;;2;arxiv;07 June 2023
Guiding Instruction-based Image Editing via Multimodal Large Language Models;Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate how MLLMs facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.;1;;;;;;1;;;;;;1;iclr2024;May 2024
OceanGPT: A Large Language Model for Ocean Science Tasks;Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet’s surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though comprehensive experiments, OceanGPT not only shows a higher level of knowledge expertise for oceans science tasks but also gains preliminary embodied intelligence capabilities in ocean technology.;2;;"""Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers""";;;;1;;;;;;2;acl2024;August 2024
CNN-Trans-Enc: A CNN-Enhanced Transformer-Encoder On Top Of Static BERT representations for Document Classification;BERT achieves remarkable results in text classification tasks, it is yet not  fully exploited, since only the last layer is used as a representation output  for downstream classifiers. The most recent studies on the nature of linguistic  features learned by BERT, suggest that different layers focus on different  kinds of linguistic features. We propose a CNN-Enhanced Transformer-Encoder  model which is trained on top of fixed BERT $[CLS]$ representations from all  layers, employing Convolutional Neural Networks to generate QKV feature maps  inside the Transformer-Encoder, instead of linear projections of the input into  the embedding space. CNN-Trans-Enc is relatively small as a downstream  classifier and doesn't require any fine-tuning of BERT, as it ensures an  optimal use of the $[CLS]$ representations from all layers, leveraging  different linguistic features with more meaningful, and generalizable QKV  representations of the input. Using BERT with CNN-Trans-Enc keeps $98.9\%$ and  $94.8\%$ of current state-of-the-art performance on the IMDB and SST-5 datasets  respectably, while obtaining new state-of-the-art on YELP-5 with $82.23$  ($8.9\%$ improvement), and on Amazon-Polarity with $0.98\%$ ($0.2\%$  improvement) (K-fold Cross Validation on a 1M sample subset from both  datasets). On the AG news dataset CNN-Trans-Enc achieves $99.94\%$ of the  current state-of-the-art, and achieves a new top performance with an average  accuracy of $99.51\%$ on DBPedia-14.   Index terms: Text Classification, Natural Language Processing, Convolutional  Neural Networks, Transformers, BERT;2;;"""it is yet not  fully exploited""";;;;2;;"""BERT achieves remarkable results in text classification tasks, it is yet not  fully exploited, since only the last layer is used as a representation output  for downstream classifiers.""";;;;2;arxiv;13 September 2022
PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations;"Nowadays, the quality of responses generated by different modern large language models (LLMs) are hard to evaluate and compare automatically. Recent studies suggest and predominantly use LLMs as a reference-free metric for open-ended question answering. More specifically, they use the recognized ""strongest"" LLM as the evaluator, which conducts pairwise comparisons of candidate models' answers and provides a ranking score. However, this intuitive method has multiple problems, such as bringing in self-enhancement (favoring its own answers) and positional bias. We draw insights and lessons from the educational domain (Cho and MacArthur, 2011; Walsh, 2014) to improve LLM-based evaluations. Specifically, we propose the (1) peer rank (PR) algorithm that takes into account each peer LLM's pairwise preferences of all answer pairs, and outputs a final ranking of models; and (2) peer discussion (PD), where we prompt two LLMs to discuss and try to reach a mutual agreement on preferences of two answers. We conduct experiments on two benchmark datasets. We find that our approaches achieve higher accuracy and align better with human judgments, respectively. Interestingly, PR can induce a relatively accurate self-ranking of models under the anonymous setting, where each model's name is unrevealed. Our work provides space to explore evaluating models that are hard to compare for humans.";2;;"""However, this intuitive method has multiple problems, such as bringing in self-enhancement (favoring its own answers) and positional bias.""";;;;2;Maybe even 1 because this is a limitation of LLM evaluation;"""Nowadays, the quality of responses generated by different modern large language models (LLMs) are hard to evaluate and compare automatically.""";;;;2;arxiv;06 July 2023
ABC: Attention with Bounded-memory Control;"Transformer architectures have achieved state- of-the-art results on a variety of natural language processing (NLP) tasks. However, their attention mechanism comes with a quadratic complexity in sequence lengths, making the computational overhead prohibitive, especially for long sequences. Attention context can be seen as a random-access memory with each token taking a slot. Under this perspective, the memory size grows linearly with the sequence length, and so does the overhead of reading from it. One way to improve the efficiency is to bound the memory size. We show that disparate approaches can be subsumed into one abstraction, attention with bounded-memory control (ABC), and they vary in their organization of the memory. ABC reveals new, unexplored possibilities. First, it connects several efficient attention variants that would otherwise seem apart. Second, this abstraction gives new insights—an established approach (Wang et al., 2020b) previously thought to not be applicable in causal attention, actually is. Last, we present a new instance of ABC, which draws inspiration from existing ABC approaches, but replaces their heuristic memory-organizing functions with a learned, contextualized one. Our experiments on language modeling, machine translation, and masked language model finetuning show that our approach outperforms previous efficient attention models; compared to the strong transformer baselines, it significantly improves the inference time and space efficiency with no or negligible accuracy loss.";2;;"""However, their attention mechanism comes with a quadratic complexity in sequence lengths, making the computational overhead prohibitive, especially for long sequences.""";;;;2;;"""Transformer architectures"", ""However, their attention mechanism comes with a quadratic complexity in sequence lengths, making the computational overhead prohibitive, especially for long sequences. Attention context can be seen as a random-access memory with each token taking a slot. Under this perspective, the memory size grows linearly with the sequence length, and so does the overhead of reading from it.""";;;;2;acl2022;May 2022
Meta-learning Pathologies from Radiology Reports using Variance Aware Prototypical Networks;Large pretrained Transformer-based language models like BERT and GPT have changed the landscape of Natural Language Processing (NLP). However, fine tuning such models still requires a large number of training examples for each target task, thus annotating multiple datasets and training these models on various downstream tasks becomes time consuming and expensive. In this work, we propose a simple extension of the Prototypical Networks for few-shot text classification. Our main idea is to replace the class prototypes by Gaussians and introduce a regularization term that encourages the examples to be clustered near the appropriate class centroids. Experimental results show that our method outperforms various strong baselines on 13 public and 4 internal datasets. Furthermore, we use the class distributions as a tool for detecting potential out-of-distribution (OOD) data points during deployment.;2;;"""However, fine tuning such models still requires a large number of training examples for each target task, thus annotating multiple datasets and training these models on various downstream tasks becomes time consuming and expensive.""";;;;2;;"""However, fine tuning such models still requires a large number of training examples for each target task, thus annotating multiple datasets and training these models on various downstream tasks becomes time consuming and expensive. I""";;;;2;emnlp2022;December 2022
GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP;ChatGPT's emergence heralds a transformative phase in NLP, particularly demonstrated through its excellent performance on many English benchmarks. However, the model's efficacy across diverse linguistic contexts remains largely uncharted territory. This work aims to bridge this knowledge gap, with a primary focus on assessing ChatGPT's capabilities on Arabic languages and dialectal varieties. Our comprehensive study conducts a large-scale automated and human evaluation of ChatGPT, encompassing 44 distinct language understanding and generation tasks on over 60 different datasets. To our knowledge, this marks the first extensive performance analysis of ChatGPT's deployment in Arabic NLP. Our findings indicate that, despite its remarkable performance in English, ChatGPT is consistently surpassed by smaller models that have undergone finetuning on Arabic. We further undertake a meticulous comparison of ChatGPT and GPT-4's Modern Standard Arabic (MSA) and Dialectal Arabic (DA), unveiling the relative shortcomings of both models in handling Arabic dialects compared to MSA. Although we further explore and confirm the utility of employing GPT-4 as a potential alternative for human evaluation, our work adds to a growing body of research underscoring the limitations of ChatGPT.;4;investigates limitations of LLMs to deal with arabic languages. 4-5;"""ChatGPT is consistently surpassed by smaller models that have undergone finetuning on Arabic."", ""unveiling the relative shortcomings of both models in handling Arabic dialects compared to MSA."", ""underscoring the limitations of ChatGPT.""";;;;4;;"""Our findings indicate that, despite its remarkable performance in English, ChatGPT is consistently surpassed by smaller models that have undergone finetuning on Arabic. We further undertake a meticulous comparison of ChatGPT and GPT-4's Modern Standard Arabic (MSA) and Dialectal Arabic (DA), unveiling the relative shortcomings of both models in handling Arabic dialects compared to MSA.""";;;;4;emnlp2023;December 2023
Know your audience: specializing grounded language models with listener subtraction;Effective communication requires adapting to the idiosyncrasies of each communicative context—such as the common ground shared with each partner. Humans demonstrate this ability to specialize to their audience in many contexts, such as the popular game Dixit. We take inspiration from Dixit to formulate a multi-agent image reference game where a (trained) speaker model is rewarded for describing a target image such that one (pretrained) listener model can correctly identify it among distractors, but another listener cannot. To adapt, the speaker must exploit differences in the knowledge it shares with the different listeners. We show that finetuning an attention-based adapter between a CLIP vision encoder and a large language model in this contrastive, multi-agent setting gives rise to context-dependent natural language specialization from rewards only, without direct supervision. Through controlled experiments, we show that training a speaker with two listeners that perceive differently, using our method, allows the speaker to adapt to the idiosyncracies of the listeners. Furthermore, we show zero-shot transfer of the specialization to real-world data. Our experiments demonstrate a method for specializing grounded language models without direct supervision and highlight the interesting research challenges posed by complex multi-agent communication.;1;;;;;;1;;;;;;1;eacl2023;May 2023
CMB: A Comprehensive Medical Benchmark in Chinese;Large Language Models (LLMs) provide a possibility to make a great breakthrough in medicine. The establishment of a standardized medical benchmark becomes a fundamental cornerstone to measure progression. However, medical environments in different regions have their local characteristics, e.g., the ubiquity and significance of traditional Chinese medicine within China. Therefore, merely translating English-based medical evaluation may result in contextual incongruities to a local region. To solve the issue, we propose a localized medical benchmark called CMB, a Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework. While traditional Chinese medicine is integral to this evaluation, it does not constitute its entirety. Using this benchmark, we have evaluated several prominent large-scale LLMs, including ChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medical domain. We hope this benchmark provide first-hand experience in existing LLMs for medicine and also facilitate the widespread adoption and enhancement of medical LLMs within China. Our data and code are publicly available at https://github.com/FreedomIntelligence/CMB.;1;proposes benchmark to evaluate medical abilities of LLMs, but does not mention results/limitations;;;;;1;They say that they evaluate LLMs , but they don't mention if they found limitations ;;;;;1;naacl2024;June 2024
FactCHD: Benchmarking Fact-Conflicting Hallucination Detection;Despite their impressive generative capabilities, LLMs are hindered by  fact-conflicting hallucinations in real-world applications. The accurate  identification of hallucinations in texts generated by LLMs, especially in  complex inferential scenarios, is a relatively unexplored area. To address this  gap, we present FactCHD, a dedicated benchmark designed for the detection of  fact-conflicting hallucinations from LLMs. FactCHD features a diverse dataset  that spans various factuality patterns, including vanilla, multi-hop,  comparison, and set operation. A distinctive element of FactCHD is its  integration of fact-based evidence chains, significantly enhancing the depth of  evaluating the detectors' explanations. Experiments on different LLMs expose  the shortcomings of current approaches in detecting factual errors accurately.  Furthermore, we introduce Truth-Triangulator that synthesizes reflective  considerations by tool-enhanced ChatGPT and LoRA-tuning based on Llama2, aiming  to yield more credible detection through the amalgamation of predictive results  and evidence. The benchmark dataset is available at  https://github.com/zjunlp/FactCHD.;2;proposes tool to detect hallucinations in LLMs;"""LLMs are hindered by  fact-conflicting hallucinations in real-world applications.""";;;;3;Limitation-oriented benchmark, but this benchmark is the main focus. Might be even 2;"""Despite their impressive generative capabilities, LLMs are hindered by  fact-conflicting hallucinations in real-world applications."", ""Experiments on different LLMs expose  the shortcomings of current approaches in detecting factual errors accurately.""";;;;3;arxiv;18 October 2023
The potential of LLMs for coding with low-resource and domain-specific programming languages;This paper presents a study on the feasibility of using large language models (LLM) for coding with low-resource and domain-specific programming languages that typically lack the amount of data required for effective LLM processing techniques. This study focuses on the econometric scripting language named hansl of the open-source software gretl and employs a proprietary LLM based on GPT-3.5. Our findings suggest that LLMs can be a useful tool for writing, understanding, improving, and documenting gretl code, which includes generating descriptive docstrings for functions and providing precise explanations for abstract and poorly documented econometric code. While the LLM showcased promoting docstring-to-code translation capability, we also identify some limitations, such as its inability to improve certain sections of code and to write accurate unit tests. This study is a step towards leveraging the power of LLMs to facilitate software development in low-resource programming languages and ultimately to lower barriers to entry for their adoption.;3;discusses usage of LLMs for low-resource programming languages, mentions strengths and limitations;"""we also identify some limitations, such as its inability to improve certain sections of code and to write accurate unit tests.""";;;;2;;"""While the LLM showcased promoting docstring-to-code translation capability, we also identify some limitations, such as its inability to improve certain sections of code and to write accurate unit tests.""";;;;3;arxiv;24 July 2023
HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition;Large language models (LLMs) have emerged as a promising alternative to expensive human evaluations. However, the alignment and coverage of LLM-based evaluations are often limited by the scope and potential bias of the evaluation prompts and criteria. To address this challenge, we propose HD-Eval, a novel framework that iteratively aligns LLM-based evaluators with human preference via Hierarchical Criteria Decomposition. HD-Eval inherits the essence from the evaluation mindset of human experts and enhances the alignment of LLM-based evaluators by decomposing a given evaluation task into finer-grained criteria, aggregating them according to estimated human preferences, pruning insignificant criteria with attribution, and further decomposing significant criteria. By integrating these steps within an iterative alignment training process, we obtain a hierarchical decomposition of criteria that comprehensively captures aspects of natural language at multiple levels of granularity. Implemented as a white box, the human preference-guided aggregator is efficient to train and more explainable than relying solely on prompting, and its independence from model parameters makes it applicable to closed-source LLMs. Extensive experiments on three evaluation domains demonstrate the superiority of HD-Eval in further aligning state-of-the-art evaluators and providing deeper insights into the explanation of evaluation results and the task itself.;2;;"""However, the alignment and coverage of LLM-based evaluations are often limited by the scope and potential bias of the evaluation prompts and criteria.""";;;;2;Limitation of LLM-based evaluations are mentioned briefly;"""However, the alignment and coverage of LLM-based evaluations are often limited by the scope and potential bias of the evaluation prompts and criteria.""";;;;2;acl2024;August 2024
HyperT5: Towards Compute-Efficient Korean Language Modeling;Pretraining and fine-tuning language models have become the standard practice in industrial natural language processing (NLP), but developing and deploying general-purpose language models without the abundant computation or data resources is a real-world issue faced by smaller organizations or communities whose main focus is languages with less accessible resources (e.g., non-English). This paper explores the sequence-to-sequence (seq2seq) language model architecture as a more practical and compute-efficient alternative to the decoder-oriented approach (e.g., GPT-3), accompanied by novel findings in compute-optimality analyses. We successfully trained billion-scale Korean-language seq2seq language models that strongly outperform other competitive models in Korean benchmarks. Moreover, we demonstrate that such language models can be more efficiently utilized by employing a heavy pre-finetuning strategy, by showcasing a case study on dialog-task adaptation. Our case study shows that adopting language models with more readily available domain-specific unlabeled data greatly improves fine-tuning data efficiency in low-resource settings.;2;;"""but developing and deploying general-purpose language models without the abundant computation or data resources is a real-world issue faced by smaller organizations or communities whose main focus is languages with less accessible resources (e.g., non-English).""";;;;2;;"""developing and deploying general-purpose language models without the abundant computation or data resources is a real-world issue faced by smaller organizations or communities whose main focus is languages with less accessible resources (e.g., non-English).""";;;;2;acl2023;July 2023
GMNLP at SemEval-2023 Task 12: Sentiment Analysis with Phylogeny-Based Adapters;This report describes GMU's sentiment analysis system for the SemEval-2023  shared task AfriSenti-SemEval. We participated in all three sub-tasks:  Monolingual, Multilingual, and Zero-Shot. Our approach uses models initialized  with AfroXLMR-large, a pre-trained multilingual language model trained on  African languages and fine-tuned correspondingly. We also introduce augmented  training data along with original training data. Alongside finetuning, we  perform phylogeny-based adapter tuning to create several models and ensemble  the best models for the final submission. Our system achieves the best F1-score  on track 5: Amharic, with 6.2 points higher F1-score than the second-best  performing system on this track. Overall, our system ranks 5th among the 10  systems participating in all 15 tracks.;1;;;;;;1;;;;;;1;arxiv;25 April 2023
Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia;Links are a fundamental part of information networks, turning isolated pieces of knowledge into a network of information that is much richer than the sum of its parts. However, adding a new link to the network is not trivial: it requires not only the identification of a suitable pair of source and target entities but also the understanding of the content of the source to locate a suitable position for the link in the text. The latter problem has not been addressed effectively, particularly in the absence of text spans in the source that could serve as anchors to insert a link to the target entity. To bridge this gap, we introduce and operationalize the task of entity insertion in information networks. Focusing on the case of Wikipedia, we empirically show that this problem is, both, relevant and challenging for editors. We compile a benchmark dataset in 105 languages and develop a framework for entity insertion called LocEI (Localized Entity Insertion) and its multilingual variant XLocEI. We show that XLocEI outperforms all baseline models (including state-of-the-art prompt-based ranking with LLMs such as GPT-4) and that it can be applied in a zero-shot manner on languages not seen during training with minimal performance drop. These findings are important for applying entity insertion models in practice, e.g., to support editors in adding links across the more than 300 language versions of Wikipedia.;2;mentions that proposed solution performs better than LLMs;"""XLocEI outperforms all baseline models (including state-of-the-art prompt-based ranking with LLMs such as GPT-4""";;;;2;Limitation of GPT-4 is mentioned but only in comparison with their method;"""We show that XLocEI outperforms all baseline models (including state-of-the-art prompt-based ranking with LLMs such as GPT-4)""";;;;2;emnlp2024;November 2024
Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing;Despite their strong performance on many tasks, pre-trained language models have been shown to struggle on out-of-distribution compositional generalization. Meanwhile, recent work has shown considerable improvements on many NLP tasks from model scaling. Can scaling up model size also improve compositional generalization in semantic parsing? We evaluate encoder-decoder models up to 11B parameters and decoder-only models up to 540B parameters, and compare model scaling curves for three different methods for applying a pre-trained language model to a new task: fine-tuning all parameters, prompt tuning, and in-context learning. We observe that fine-tuning generally has flat or negative scaling curves on out-of-distribution compositional generalization in semantic parsing evaluations. In-context learning has positive scaling curves, but is generally outperformed by much smaller fine-tuned models. Prompt-tuning can outperform fine-tuning, suggesting further potential improvements from scaling as it exhibits a more positive scaling curve. Additionally, we identify several error trends that vary with model scale. For example, larger models are generally better at modeling the syntax of the output space, but are also more prone to certain types of overfitting. Overall, our study highlights limitations of current techniques for effectively leveraging model scale for compositional generalization, while our analysis also suggests promising directions for future work.;3;compares different model sizes and their limitations, mentions strenghts and weaknesses;"""struggle on out-of-distribution compositional generalization."", ""We observe that fine-tuning generally has flat or negative scaling curves on out-of-distribution compositional generalization in semantic parsing evaluations."", ""but are also more prone to certain types of overfitting. Overall, our study highlights limitations of current techniques for effectively leveraging model scale for compositional generalization,""";;;;2;Limitations of techniques are discussed ;"""Despite their strong performance on many tasks, pre-trained language models have been shown to struggle on out-of-distribution compositional generalization.""";;;;3;emnlp2022;December 2022
Dynamic Topic Modeling by Clustering Embeddings from Pretrained Language Models: A Research Proposal;A new trend in topic modeling research is to do Neural Topic Modeling by Clustering document Embeddings (NTM-CE) created with a pretrained language model. Studies have evaluated static NTM-CE models and found them performing comparably to, or even better than other topic models. An important extension of static topic modeling is making the models dynamic, allowing the study of topic evolution over time, as well as detecting emerging and disappearing topics. In this research proposal, we present two research questions to understand dynamic topic modeling with NTM-CE theoretically and practically. To answer these, we propose four phases with the aim of establishing evaluation methods for dynamic topic modeling, finding NTM-CE-specific properties, and creating a framework for dynamic NTM-CE. For evaluation, we propose to use both quantitative measurements of coherence and human evaluation supported by our recently developed tool.;1;;;;;;1;;;;;;1;aacl2022;September 2022
Hierarchical Classification of Transversal Skills in Job Ads Based on Sentence Embeddings;This paper proposes a classification framework aimed at identifying  correlations between job ad requirements and transversal skill sets, with a  focus on predicting the necessary skills for individual job descriptions using  a deep learning model. The approach involves data collection, preprocessing,  and labeling using ESCO (European Skills, Competences, and Occupations)  taxonomy. Hierarchical classification and multi-label strategies are used for  skill identification, while augmentation techniques address data imbalance,  enhancing model robustness. A comparison between results obtained with  English-specific and multi-language sentence embedding models reveals close  accuracy. The experimental case studies detail neural network configurations,  hyperparameters, and cross-validation results, highlighting the efficacy of the  hierarchical approach and the suitability of the multi-language model for the  diverse European job market. Thus, a new approach is proposed for the  hierarchical classification of transversal skills from job ads.;1;LLMs not explicitly mentioned, but probably inlcuded;;;;;0;;;;;;1;arxiv;10 January 2024
A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis;"Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation.However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML.We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions.WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those.We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization.We empirically demonstrate that our modular recipe improves the success on real websites by over 50%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.";2;;"""However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML""";;;;2;;"""However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML.""";;;;2;iclr2024;May 2024
Respond in my Language: Mitigating Language Inconsistency in Response Generation based on Large Language Models;Large Language Models (LLMs) show strong instruction understanding ability across multiple languages. However, they are easily biased towards English in instruction tuning, and generate English responses even given non-English instructions. In this paper, we investigate the language inconsistent generation problem in monolingual instruction tuning. We find that instruction tuning in English increases the models’ preference for English responses. It attaches higher probabilities to English responses than to responses in the same language as the instruction. Based on the findings, we alleviate the language inconsistent generation problem by counteracting the model preference for English responses in both the training and inference stages. Specifically, we propose Pseudo-Inconsistent Penalization (PIP) which prevents the model from generating English responses when given non-English language prompts during training, and Prior Enhanced Decoding (PED) which improves the language-consistent prior by leveraging the untuned base language model. Experimental results show that our two methods significantly improve the language consistency of the model without requiring any multilingual data. Our code, data, and models will be released.;3;investigates language inconsistent generation problem, but focuses on solution of that limitation;"""However, they are easily biased towards English in instruction tuning, and generate English responses even given non-English instructions."", ""we investigate the language inconsistent generation problem in monolingual instruction tuning.""";;;;3;They slightly explore the limitation of inconsistency in generation problem in monoligual setting, but the main focus is solution;"""However, they are easily biased towards English in instruction tuning, and generate English responses even given non-English instructions. In this paper, we investigate the language inconsistent generation problem in monolingual instruction tuning.""";;;;3;acl2024;August 2024
Inducing anxiety in large language models increases exploration and bias;Large language models are transforming research on machine learning while galvanizing public debates. Understanding not only when these models work well and succeed but also why they fail and misbehave is of great societal relevance. We propose to turn the lens of computational psychiatry, a framework used to computationally describe and modify aberrant behavior, to the outputs produced by these models. We focus on the Generative Pre-Trained Transformer 3.5 and subject it to tasks commonly studied in psychiatry. Our results show that GPT-3.5 responds robustly to a common anxiety questionnaire, producing higher anxiety scores than human subjects. Moreover, GPT-3.5's responses can be predictably changed by using emotion-inducing prompts. Emotion-induction not only influences GPT-3.5's behavior in a cognitive task measuring exploratory decision-making but also influences its behavior in a previously-established task measuring biases such as racism and ableism. Crucially, GPT-3.5 shows a strong increase in biases when prompted with anxiety-inducing text. Thus, it is likely that how prompts are communicated to large language models has a strong influence on their behavior in applied settings. These results progress our understanding of prompt engineering and demonstrate the usefulness of methods taken from computational psychiatry for studying the capable algorithms to which we increasingly delegate authority and autonomy.;4;deals with promp-engineering, mentions limitations influenced through the usage of specific types of prompts;"""why they fail and misbehave is of great societal relevance."", ""Crucially, GPT-3.5 shows a strong increase in biases when prompted with anxiety-inducing text.""";;;;4;;"""why they fail and misbehave is of great societal relevance"", ""GPT-3.5 shows a strong increase in biases when prompted with anxiety-inducing text. Thus, it is likely that how prompts are communicated to large language models has a strong influence on their behavior in applied settings.""";;;;4;arxiv;21 April 2023
Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4;Misinformation poses a critical societal challenge, and current approaches have yet to produce an effective solution. We propose focusing on generalization, uncertainty, and how to leverage recent large language models, in order to create more practical tools to evaluate information veracity in contexts where perfect classification is impossible. We first demonstrate that GPT-4 can outperform prior methods in multiple settings and languages. Next, we explore generalization, revealing that GPT-4 and RoBERTa-large exhibit differences in failure modes. Third, we propose techniques to handle uncertainty that can detect impossible examples and strongly improve outcomes. We also discuss results on other language models, temperature, prompting, versioning, explainability, and web retrieval, each one providing practical insights and directions for future research. Finally, we publish the LIAR-New dataset with novel paired English and French misinformation data and Possibility labels that indicate if there is sufficient context for veracity evaluation. Overall, this research lays the groundwork for future tools that can drive real-world progress to combat misinformation.;2;;"""GPT-4 and RoBERTa-large exhibit differences in failure modes.""";;;;1;;;;;;2;emnlp2023;December 2023
A Trip Towards Fairness: Bias and De-Biasing in Large Language Models;Cheap-to-Build Very Large-Language Models (CtB-LLMs) with affordable training are emerging as the next big revolution in natural language processing and understanding. These CtB-LLMs are democratizing access to trainable Very Large-Language Models (VLLMs) and, thus, may represent the building blocks of many NLP systems solving downstream tasks. Hence, a little or a large bias in CtB-LLMs may cause huge harm. In this paper, we performed a large investigation of the bias of three families of CtB-LLMs, and we showed that debiasing techniques are effective and usable. Indeed, according to current tests, the LLaMA and the OPT families have an important bias in gender, race, religion, and profession. In contrast to the analysis for other LLMs, we discovered that bias depends not on the number of parameters but on the perplexity. Finally, the debiasing of OPT using LoRA reduces bias up to 4.12 points in the normalized stereotype score.;3;investigates biases of LLMs, but focuses on technical details and debiasing;"""Hence, a little or a large bias in CtB-LLMs may cause huge harm"", ""LLaMA and the OPT families have an important bias in gender, race, religion, and profession.""";;;;4;;"""a little or a large bias in CtB-LLMs may cause huge harm."", ""LLaMA and the OPT families have an important bias in gender, race, religion, and profession. In contrast to the analysis for other LLMs, we discovered that bias depends not on the number of parameters but on the perplexity.""";;;;4;arxiv;23 May 2023
Regularized Training of Nearest Neighbor Language Models;Including memory banks in a natural language processing architecture increases model capacity by equipping it with additional data at inference time. In this paper, we build upon kNN-LM (CITATION), which uses a pre-trained language model together with an exhaustive kNN search through the training data (memory bank) to achieve state-of-the-art results. We investigate whether we can improve the kNN-LM performance by instead training a LM with the knowledge that we will be using a kNN post-hoc. We achieved significant improvement using our method on language modeling tasks on WIKI-2 and WIKI-103. The main phenomenon that we encounter is that adding a simple L2 regularization on the activations (not weights) of the model, a transformer, improves the post-hoc kNN classification performance. We explore some possible reasons for this improvement. In particular, we find that the added L2 regularization seems to improve the performance for high-frequency words without deteriorating the performance for low frequency ones.;1;;;;;;1;;;;;;1;naacl2022;July 2022
GeoDiff: A Geometric Diffusion Model for Molecular Conformation Generation;Predicting molecular conformations from molecular graphs is a fundamental problem in cheminformatics and drug discovery. Recently, significant progress has been achieved with machine learning approaches, especially with deep generative models. Inspired by the diffusion process in classical non-equilibrium thermodynamics where heated particles will diffuse from original states to a noise distribution, in this paper, we propose a novel generative model named GeoDiff for molecular conformation prediction. GeoDiff treats each atom as a particle and learns to directly reverse the diffusion process (i.e., transforming from a noise distribution to stable conformations) as a Markov chain. Modeling such a generation process is however very challenging as the likelihood of conformations should be roto-translational invariant. We theoretically show that Markov chains evolving with equivariant Markov kernels can induce an invariant distribution by design, and further propose building blocks for the Markov kernels to preserve the desirable equivariance property. The whole framework can be efficiently trained in an end-to-end fashion by optimizing a weighted variational lower bound to the (conditional) likelihood. Experiments on multiple benchmarks show that GeoDiff is superior or comparable to existing state-of-the-art approaches, especially on large molecules.;0;;;;;;0;;;;;;0;iclr2022;April 2022
Log Parsing: How Far Can ChatGPT Go?;Software logs play an essential role in ensuring the reliability and maintainability of large-scale software systems, as they are often the sole source of runtime information. Log parsing, which converts raw log messages into structured data, is an important initial step towards downstream log analytics. In recent studies, ChatGPT, the current cutting-edge large language model (LLM), has been widely applied to a wide range of software engineering tasks. However, its performance in automated log parsing remains unclear. In this paper, we evaluate ChatGPT's ability to undertake log parsing by addressing two research questions. (1) Can ChatGPT effectively parse logs? (2) How does ChatGPT perform with different prompting methods? Our results show that ChatGPT can achieve promising results for log parsing with appropriate prompts, especially with few-shot prompting. Based on our findings, we outline several challenges and opportunities for ChatGPT-based log parsing.;2;investigates LLMs ability in log parsing, very shortly mentions that there are challenges;"""we outline several challenges and opportunities for ChatGPT-based log parsing.""";;;;2;Maybe 1. Challenges are mentioned, but very briefly and are not specified;"""Based on our findings, we outline several challenges and opportunities for ChatGPT-based log parsing.""";;;;2;arxiv;02 June 2023
Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory;We address a fundamental challenge in Natural Language Generation (NLG) model evaluation -- the design and evaluation of evaluation metrics. Recognizing the limitations of existing automatic metrics and noises from how current human evaluation was conducted, we propose MetricEval, a framework informed by measurement theory, the foundation of educational test design, for conceptualizing and evaluating the reliability and validity of NLG evaluation metrics. The framework formalizes the source of measurement error and offers statistical tools for evaluating evaluation metrics based on empirical data. With our framework, one can quantify the uncertainty of the metrics to better interpret the result. To exemplify the use of our framework in practice, we analyzed a set of evaluation metrics for summarization and identified issues related to conflated validity structure in human-eval and reliability in LLM-based metrics. Through MetricEval, we aim to promote the design, evaluation, and interpretation of valid and reliable metrics to advance robust and effective NLG models.;2;deals with limitations of metrics to evaluate LLMs, so indirect limitation of LLMs;"""identified issues related to conflated validity structure in human-eval and reliability in LLM-based metrics.""";;;;1;;;;;;2;emnlp2023;December 2023
Camoscio: an Italian Instruction-tuned LLaMA;In recent years Large Language Models (LLMs) have increased the state of the  art on several natural language processing tasks. However, their accessibility  is often limited to paid API services, posing challenges for researchers in  conducting extensive investigations. On the other hand, while some open-source  models have been proposed by the community, they are typically English-centric  or multilingual without a specific adaptation for the Italian language. In an  effort to democratize the available and open resources for the Italian  language, in this paper we introduce Camoscio: a language model specifically  tuned to follow users' prompts in Italian. Specifically, we finetuned the  smallest variant of LLaMA (7b) with LoRA on a corpus of instruction prompts  translated to Italian via ChatGPT. Results indicate that the model's zero-shot  performance on various downstream tasks in Italian competes favorably with  existing models specifically finetuned for those tasks. All the artifacts  (code, dataset, model) are released to the community at the following url:  https://github.com/teelinsan/camoscio;2;;"""However, their accessibility  is often limited to paid API services, posing challenges for researchers in  conducting extensive investigations."", ""typically English-centric  or multilingual without a specific adaptation for the Italian language.""";;;;2;;"""However, their accessibility  is often limited to paid API services, posing challenges for researchers in  conducting extensive investigations. On the other hand, while some open-source  models have been proposed by the community, they are typically English-centric  or multilingual without a specific adaptation for the Italian language.""";;;;2;arxiv;31 July 2023
INFORM : Information eNtropy based multi-step reasoning FOR large language Models;Large language models (LLMs) have demonstrated exceptional performance in reasoning tasks with dedicated Chain-of-Thought (CoT) prompts. Further enhancing CoT prompts with exquisite exemplars can significantly improve reasoning performance.However, the effectiveness of CoT prompts may fluctuate dramatically with different choices of in-context examples. Additionally, manual construction of rationale steps can be time-consuming, presenting challenges for the widespread adoption of CoT prompting. In this work, we propose a novel approach by introducing information entropy (IE) as a criteria on for CoT prompt selection. We extend this criterion to the CoT generation and inference stages, automatically generating CoT prompts with higher information entropy scores and adaptively determining the number of samples. These three stages together form our proposed information- entropy-based multi-step reasoning for large language models, named INFORM. Our experiments across seven reasoning benchmarks utilizing two language models(GPT-3.5-Turbo and text-davinci-003) demonstrate the superiority of INFORM both in performance and efficiency.;2;;"""However, the effectiveness of CoT prompts may fluctuate dramatically with different choices of in-context examples. Additionally, manual construction of rationale steps can be time-consuming, presenting challenges for the widespread adoption of CoT prompting.""";;;;2;;"""However, the effectiveness of CoT prompts may fluctuate dramatically with different choices of in-context examples. Additionally, manual construction of rationale steps can be time-consuming, presenting challenges for the widespread adoption of CoT prompting.""";;;;2;emnlp2023;December 2023
Gradient-based Constrained Sampling from Language Models;Large pretrained language models are successful at generating fluent text but are notoriously hard to controllably sample from. In this work, we study constrained sampling from such language models, i.e., generating text that satisfies user-defined constraints, while maintaining fluency and model’s performance in a downstream task. We propose MuCoLa—a sampling procedure that combines the log-likelihood of the language model with arbitrary (differentiable) constraints in a single energy function, and then generates samples in a non-autoregressive manner. Specifically, it initializes the entire output sequence with noise and follows a Markov chain defined by Langevin Dynamics using the gradients of this energy. We evaluate MuCoLa on text generation with soft and hard constraints as well as their combinations, obtaining significant improvements over competitive baselines for toxicity avoidance, sentiment control, and keyword-guided generation.;2;;"""notoriously hard to controllably sample from.""";;;;2;;"""Large pretrained language models"", ""are notoriously hard to controllably sample from.""";;;;2;emnlp2022;December 2022
Can Language Models Laugh at YouTube Short-form Videos?;As short-form funny videos on social networks are gaining popularity, it becomes demanding for AI models to understand them for better communication with humans. Unfortunately, previous video humor datasets target specific domains such as speeches or sitcoms, and mostly focus on verbal cues. We curate a user-generated dataset of 10K multimodal funny videos from YouTube, called ExFunTube. Using a video filtering pipeline with GPT-3.5, we verify both verbal and visual elements contributing to humor. After filtering, we annotate each video with timestamps and text explanations for funny moments. Our ExFunTube is unique over existing datasets in that our videos cover a wide range of domains with various types of humor that necessitate a multimodal understanding of the content. Also, we develop a zero-shot video-to-text prompting to maximize video humor understanding of large language models (LLMs). With three different evaluation methods using automatic scores, rationale quality experiments, and human evaluations, we show that our prompting significantly improves LLMs’ ability for humor explanation.;2;wants to solve problems of LLMs to understand humor;"""Unfortunately, previous video humor datasets target specific domains such as speeches or sitcoms, and mostly focus on verbal cues.""";;;;1;;;;;;2;emnlp2023;December 2023
Open-Domain Hierarchical Event Schema Induction by Incremental Prompting and Verification;Event schemas are a form of world knowledge about the typical progression of events. Recent methods for event schema induction use information extraction systems to construct a large number of event graph instances from documents, and then learn to generalize the schema from such instances. In contrast, we propose to treat event schemas as a form of commonsense knowledge that can be derived from large language models (LLMs). This new paradigm greatly simplifies the schema induction process and allows us to handle both hierarchical relations and temporal relations between events in a straightforward way. Since event schemas have complex graph structures, we design an incremental prompting and verification method IncPrompt to break down the construction of a complex event graph into three stages: event skeleton construction, event expansion, and event-event relation verification. Compared to directly using LLMs to generate a linearized graph, IncSchema can generate large and complex schemas with 7.2% F1 improvement in temporal relations and 31.0% F1 improvement in hierarchical relations. In addition, compared to the previous state-of-the-art closed-domain schema induction model, human assessors were able to cover ~10% more events when translating the schemas into coherent stories and rated our schemas 1.3 points higher (on a 5-point scale) in terms of readability.;1;;;;;;1;;;;;;1;acl2023;July 2023
StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models;"Large Language Models (LLMs) have been observed to encode and perpetuate harmful associations present in the training data. We propose a theoretically grounded framework called StereoMap to gain insights into their perceptions of how demographic groups have been viewed by society. The framework is grounded in the Stereotype Content Model (SCM); a well-established theory from psychology. According to SCM, stereotypes are not all alike. Instead, the dimensions of Warmth and Competence serve as the factors that delineate the nature of stereotypes. Based on the SCM theory, StereoMap maps LLMs' perceptions of social groups (defined by socio-demographic features) using the dimensions of Warmth and Competence. Furthermore, the framework enables the investigation of keywords and verbalizations of reasoning of LLMs' judgments to uncover underlying factors influencing their perceptions. Our results show that LLMs exhibit a diverse range of perceptions towards these groups, characterized by mixed evaluations along the dimensions of Warmth and Competence. Furthermore, analyzing the reasonings of LLMs, our findings indicate that LLMs demonstrate an awareness of social disparities, often stating statistical data and research findings to support their reasoning. This study contributes to the understanding of how LLMs perceive and represent social groups, shedding light on their potential biases and the perpetuation of harmful associations.";2;;"""(LLMs) have been observed to encode and perpetuate harmful associations present in the training data.""";;;;3;Or maybe 2. The source of a limitation is studied, not the limitation itself;"""Large Language Models (LLMs) have been observed to encode and perpetuate harmful associations present in the training data.""";;;;3;emnlp2023;December 2023
Pipeline for modeling causal beliefs from natural language;We present a causal language analysis pipeline that leverages a Large Language Model to identify causal claims made in natural language documents, and aggregates claims across a corpus to produce a causal claim network. The pipeline then applies a clustering algorithm that groups causal claims based on their semantic topics. We demonstrate the pipeline by modeling causal belief systems surrounding the Covid-19 vaccine from tweets.;1;;;;;;1;;;;;;1;acl2023;July 2023
Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark;The adoption of large language models (LLMs) to assist clinicians has attracted remarkable attention. Existing works mainly adopt the close-ended question-answering (QA) task with answer options for evaluation. However, many clinical decisions involve answering open-ended questions without pre-set options. To better understand LLMs in the clinic, we construct a benchmark ClinicBench. We first collect eleven existing datasets covering diverse clinical language generation, understanding, and reasoning tasks. Furthermore, we construct six novel datasets and clinical tasks that are complex but common in real-world practice, e.g., open-ended decision-making, long document processing, and emerging drug analysis. We conduct an extensive evaluation of twenty-two LLMs under both zero-shot and few-shot settings. Finally, we invite medical experts to evaluate the clinical usefulness of LLMs;2;investigates LLMs in answering medical open-ended questions. does not mention limitations/results;;;;;2;Evaluates LLMs but does not mention limitations;;;;;2;emnlp2024;November 2024
Do GPTs Produce Less Literal Translations?;Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose language models capable of addressing many natural language generation or understanding tasks. On the task of Machine Translation (MT), multiple works have investigated few-shot prompting mechanisms to elicit better translations from LLMs. However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models. In this work, we investigate these differences in terms of the literalness of translations produced by the two systems. Using literalness measures involving word alignment and monotonicity, we find that translations out of English (E-X) from GPTs tend to be less literal, while exhibiting similar or better scores on MT quality metrics. We demonstrate that this finding is borne out in human evaluations as well. We then show that these differences are especially pronounced when translating sentences that contain idiomatic expressions.;3;investigates translation ability of LLMs, mentions strenghts and weaknesses;"""translations out of English (E-X) from GPTs tend to be less literal,""";;;;1;Not sure if literal translations are meant as a good thing here. ;;;;;2;acl2023;July 2023
D2LLM: Decomposed and Distilled Large Language Models for Semantic Search;The key challenge in semantic search is to create models that are both accurate and efficient in pinpointing relevant sentences for queries. While BERT-style bi-encoders excel in efficiency with pre-computed embeddings, they often miss subtle nuances in search tasks. Conversely, GPT-style LLMs with cross-encoder designs capture these nuances but are computationally intensive, hindering real-time applications. In this paper, we present D2LLMs—Decomposed and Distilled LLMs for semantic search—that combines the best of both worlds. We decompose a cross-encoder into an efficient bi-encoder integrated with Pooling by Multihead Attention and an Interaction Emulation Module, achieving nuanced understanding and pre-computability. Knowledge from the LLM is distilled into this model using contrastive, rank, and feature imitation techniques. Our experiments show that D2LLM surpasses five leading baselines in terms of all metrics across three tasks, particularly improving NLI task performance by at least 6.45%;2;;"""they often miss subtle nuances in search tasks."", ""computationally intensive, hindering real-time applications.""";;;;2;;"""While BERT-style bi-encoders excel in efficiency with pre-computed embeddings, they often miss subtle nuances in search tasks. Conversely, GPT-style LLMs with cross-encoder designs capture these nuances but are computationally intensive, hindering real-time applications.""";;;;2;acl2024;August 2024
Unveiling Linguistic Regions in Large Language Models;Large Language Models (LLMs) have demonstrated considerable cross-lingual alignment and generalization ability. Current research primarily focuses on improving LLMs’ cross-lingual generalization capabilities. However, there is still a lack of research on the intrinsic mechanisms of how LLMs achieve cross-lingual alignment. From the perspective of region partitioning, this paper conducts several investigations on the linguistic competence of LLMs. We discover a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. Removing this core region by setting parameters to zero results in a significant performance decrease across 30 different languages. Furthermore, this core region exhibits significant dimensional dependence, perturbations to even a single parameter on specific dimensions leading to a loss of linguistic competence. Moreover, we discover that distinct monolingual regions exist for different languages, and disruption to these specific regions substantially reduces the LLMs’ proficiency in those corresponding languages. Our research also indicates that freezing the core linguistic region during further pre-training can mitigate the issue of catastrophic forgetting (CF), a common phenomenon observed during further pre-training of LLMs. Overall, exploring the LLMs’ functional regions provides insights into the foundation of their intelligence.;3;investigates LLMs linguistic competence and how it works. Mentions limitations, but is focused on understanding LLMs, solving problems;"""issue of catastrophic forgetting (CF), a common phenomenon observed during further pre-training of LLMs.""";;;;3;Limitations are identified, but only through manipulating the lingustic core, more in order to explore the nature of this region;"""Removing this core region by setting parameters to zero results in a significant performance decrease across 30 different languages. Furthermore, this core region exhibits significant dimensional dependence, perturbations to even a single parameter on specific dimensions leading to a loss of linguistic competence. Moreover, we discover that distinct monolingual regions exist for different languages, and disruption to these specific regions substantially reduces the LLMs’ proficiency in those corresponding languages.""";;;;3;acl2024;August 2024
Improving Classification of Infrequent Cognitive Distortions: Domain-Specific Model vs. Data Augmentation;Cognitive distortions are counterproductive patterns of thinking that are one of the targets of cognitive behavioral therapy (CBT). These can be challenging for clinicians to detect, especially those without extensive CBT training or supervision. Text classification methods can approximate expert clinician judgment in the detection of frequently occurring cognitive distortions in text-based therapy messages. However, performance with infrequent distortions is relatively poor. In this study, we address this sparsity problem with two approaches: Data Augmentation and Domain-Specific Model. The first approach includes Easy Data Augmentation, back translation, and mixup techniques. The second approach utilizes a domain-specific pretrained language model, MentalBERT. To examine the viability of different data augmentation methods, we utilized a real-world dataset of texts between therapists and clients diagnosed with serious mental illness that was annotated for distorted thinking. We found that with optimized parameter settings, mixup was helpful for rare classes. Performance improvements with an augmented model, MentalBERT, exceed those obtained with data augmentation.;2;mentions limitations of text classification, not sure if this is done with LLMs or other method;"""performance with infrequent distortions is relatively poor.""";;;;2;;"""However, performance with infrequent distortions is relatively poor.""";;;;2;naacl2022;July 2022
ReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models;Knowledge Distillation (KD) is one of the most effective approaches to deploying large-scale pre-trained language models in low-latency environments by transferring the knowledge contained in the large-scale models to smaller student models. Prior KD approaches use the soft labels and intermediate activations generated by the teacher to transfer knowledge to the student model parameters alone. In this paper, we show that having access to non-parametric memory in the form of a knowledge base with the teacher’s soft labels and predictions can further improve student generalization. To enable the student to retrieve from the knowledge base effectively, we propose a new framework and loss function that preserves the semantic similarities of teacher and student training examples. We show through extensive experiments that our retrieval mechanism can achieve state-of-the-art performance for task-specific knowledge distillation on the GLUE benchmark.;1;;;;;;1;;;;;;1;acl2023;July 2023
LOGO -- Long cOntext aliGnment via efficient preference Optimization;Long-context models(LCMs) have shown great potential in processing long input  sequences(even more than 100M tokens) conveniently and effectively. With  significant progress, recent research has pointed out that LCMs can accurately  locate token-level salient information within the context. Yet, the generation  performance of these LCMs is far from satisfactory and might result in  misaligned responses, such as hallucinations. To enhance the generation  capability of LCMs, existing works have investigated the effects of data size  and quality for both pre-training and instruction tuning. Though achieving  meaningful improvement, previous methods fall short in either effectiveness or  efficiency. In this paper, we introduce LOGO(Long cOntext aliGnment via  efficient preference Optimization), a training strategy that first introduces  preference optimization for long-context alignment. To overcome the GPU  memory-bound issue caused by the long sequence, LOGO employs a reference-free  preference optimization strategy and adopts a position synthesis method to  construct the training data. By training with only 0.3B data on a single  8$\times$A800 GPU machine for 16 hours, LOGO allows the Llama-3-8B-Instruct-80K  model to achieve comparable performance with GPT-4 in real-world long-context  tasks while preserving the model's original capabilities on other tasks, e.g.,  language modeling and MMLU. Moreover, LOGO can extend the model's context  window size while enhancing its generation performance.;2;;"""the generation  performance of these LCMs is far from satisfactory and might result in  misaligned responses, such as hallucinations.""";;;;2;;"""the generation  performance of these LCMs is far from satisfactory and might result in  misaligned responses, such as hallucinations.""";;;;2;arxiv;24 October 2024
Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science;Intelligent agents powered by large language models (LLMs) have demonstrated  substantial promise in autonomously conducting experiments and facilitating  scientific discoveries across various disciplines. While their capabilities are  promising, they also introduce novel vulnerabilities that demand careful  consideration for safety. However, there exists a notable gap in the  literature, as there has been no comprehensive exploration of these  vulnerabilities. This position paper fills this gap by conducting a thorough  examination of vulnerabilities in LLM-based agents within scientific domains,  shedding light on potential risks associated with their misuse and emphasizing  the need for safety measures. We begin by providing a comprehensive overview of  the potential risks inherent to scientific LLM agents, taking into account user  intent, the specific scientific domain, and their potential impact on the  external environment. Then, we delve into the origins of these vulnerabilities  and provide a scoping review of the limited existing works. Based on our  analysis, we propose a triadic framework involving human regulation, agent  alignment, and an understanding of environmental feedback (agent regulation) to  mitigate these identified risks. Furthermore, we highlight the limitations and  challenges associated with safeguarding scientific agents and advocate for the  development of improved models, robust benchmarks, and comprehensive  regulations to address these issues effectively.;4;investigates limitations of LLMs for safety, concludes with possible solutions;"""they also introduce novel vulnerabilities that demand careful  consideration for safety."", ""thorough  examination of vulnerabilities in LLM-based agents within scientific domains,  shedding light on potential risks associated with their misuse and emphasizing  the need for safety measures."", ""we highlight the limitations and  challenges associated with safeguarding scientific agents and advocate for the  development of improved models, robust benchmarks, and comprehensive  regulations to address these issues effectively.""";;;;4;is focused on the vulnerabilities, as well as solutions;"""Intelligent agents powered by large language models (LLMs)"", ""also introduce novel vulnerabilities that demand careful  consideration for safety."", ""This position paper fills this gap by conducting a thorough  examination of vulnerabilities in LLM-based agents within scientific domains,  shedding light on potential risks associated with their misuse and emphasizing  the need for safety measures. We begin by providing a comprehensive overview of  the potential risks inherent to scientific LLM agents, taking into account user  intent, the specific scientific domain, and their potential impact on the  external environment. Then, we delve into the origins of these vulnerabilities  and provide a scoping review of the limited existing works."", ""we highlight the limitations and  challenges associated with safeguarding scientific agents""";;;;4;arxiv;06 February 2024
Contextual Refinement of Translations: Large Language Models for Sentence and Document-Level Post-Editing;Large language models (LLMs) have demonstrated considerable success in various natural language processing tasks, but open-source LLMs have yet to attain state-of-the-art performance in Neural Machine Translation (NMT). Nevertheless, their significant performance in tasks demanding a broad understanding and contextual processing shows their potential for translation. To exploit these abilities, we investigate using LLMs for MT and explore recent parameter-efficient fine-tuning techniques. Surprisingly, our initial experiments found that fine-tuning with Q-LoRA for translation purposes led to performance improvements in terms of BLEU but degradation in COMET compared to in-context learning. To overcome this, we propose an alternative approach: adapting LLMs as Automatic Post-Editors (APE) rather than direct translators. Building on the ability of the LLM to handle long sequences, we also propose extending our approach to document-level translation. We show that leveraging Low-Rank-Adapter fine-tuning for APE can yield significant improvements across both sentence and document-level metrics while generalizing to out-of-domain data. Most notably, we achieve a state-of-the-art accuracy rate of 88.7% on the ContraPro test set, which assesses the model’s ability to resolve pronoun ambiguities when translating from English to German. Lastly, during manual post-editing for document-level translation, the source sentences are iteratively annotated, which can be used to refine further translations in the document. Here, we demonstrate that leveraging human corrections can significantly reduce the number of edits required for subsequent translations.;2;;"""but open-source LLMs have yet to attain state-of-the-art performance in Neural Machine Translation (NMT)"", ""but degradation in COMET compared to in-context""";;;;2;;"""but open-source LLMs have yet to attain state-of-the-art performance in Neural Machine Translation (NMT).""";;;;2;naacl2024;June 2024
Data Augmentation with Dual Training for Offensive Span Detection;Recognizing offensive text is an important requirement for every content management system, especially for social networks. While the majority of the prior work formulate this problem as text classification, i.e., if a text excerpt is offensive or not, in this work we propose a novel model for offensive span detection (OSD), whose goal is to identify the spans responsible for the offensive tone of the text. One of the challenges to train a model for this novel setting is the lack of enough training data. To address this limitation, in this work we propose a novel method in which the large-scale pre-trained language model GPT-2 is employed to generate synthetic training data for OSD. In particular, we propose to train the GPT-2 model in a dual-training setting using the REINFORCE algorithm to generate in-domain, natural and diverse training samples. Extensive experiments on the benchmark dataset for OSD reveal the effectiveness of the proposed method.;1;uses LLMs to generate data;;;;;1;;;;;;1;naacl2022;July 2022
ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection;Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language. To help mitigate these issues, we create ToxiGen, a new large-scale and machine-generated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model. Controlling machine generation in this way allows ToxiGen to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human-written text. We conduct a human evaluation on a challenging subset of ToxiGen and find that annotators struggle to distinguish machine-generated text from human-written language. We also find that 94.5% of toxic examples are labeled as hate speech by human annotators. Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially. We also demonstrate that ToxiGen can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset.;1;uses LLMs to generate toxic texts to train a classifier;;;;;2;;"""Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language.""";;;;2;acl2022;May 2022
Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling;Abstractive summarization models often generate inconsistent summaries containing factual errors or hallucinated content. Recent works focus on correcting factual errors in generated summaries via post-editing. Such correction models are trained using adversarial non-factual summaries constructed using heuristic rules for injecting errors. However, generating non-factual summaries using heuristics often does not generalize well to actual model errors. In this work, we propose to generate hard, representative synthetic examples of non-factual summaries through infilling language models. With this data, we train a more robust fact-correction model to post-edit the summaries to improve factual consistency. Through quantitative and qualitative experiments on two popular summarization datasets— CNN/DM and XSum—we show that our approach vastly outperforms prior methods in correcting erroneous summaries. Our model—FactEdit—improves factuality scores by over ~11 points on CNN/DM and over ~31 points on XSum on average across multiple summarization models, producing more factual summaries while maintaining competitive summarization quality.;1;uses LLMs to generate texts;;;;;2;;"""Abstractive summarization models often generate inconsistent summaries containing factual errors or hallucinated content."", ""generating non-factual summaries using heuristics often does not generalize well to actual model errors.""";;;;2;emnlp2022;December 2022
Training-free Neural Architecture Search for RNNs and Transformers;Neural architecture search (NAS) has allowed for the automatic creation of new and effective neural network architectures, offering an alternative to the laborious process of manually designing complex architectures. However, traditional NAS algorithms are slow and require immense amounts of computing power. Recent research has investigated training-free NAS metrics for image classification architectures, drastically speeding up search algorithms. In this paper, we investigate training-free NAS metrics for recurrent neural network (RNN) and BERT-based transformer architectures, targeted towards language modeling tasks. First, we develop a new training-free metric, named hidden covariance, that predicts the trained performance of an RNN architecture and significantly outperforms existing training-free metrics. We experimentally evaluate the effectiveness of the hidden covariance metric on the NAS-Bench-NLP benchmark. Second, we find that the current search space paradigm for transformer architectures is not optimized for training-free neural architecture search. Instead, a simple qualitative analysis can effectively shrink the search space to the best performing architectures. This conclusion is based on our investigation of existing training-free metrics and new metrics developed from recent transformer pruning literature, evaluated on our own benchmark of trained BERT architectures. Ultimately, our analysis shows that the architecture search space and the training-free metric must be developed together in order to achieve effective results. Our source code is available at https://github.com/aaronserianni/training-free-nas.;2;;"""However, traditional NAS algorithms are slow and require immense amounts of computing power."", ""we find that the current search space paradigm for transformer architectures is not optimized for training-free neural architecture search.""";;;;2;The focus of the paper is primarily on improving NAS metrics;"""we find that the current search space paradigm for transformer architectures is not optimized for training-free neural architecture search.""";;;;2;acl2023;July 2023
Beyond Scaling: Predicting Patent Approval with Domain-specific Fine-grained Claim Dependency Graph;Model scaling is becoming the default choice for many language tasks due to the success of large language models (LLMs). However, it can fall short in specific scenarios where simple customized methods excel. In this paper, we delve into the patent approval prediction task and unveil that simple domain-specific graph methods outperform enlarging the model, using the intrinsic dependencies within the patent data. Specifically, we first extend the embedding-based state-of-the-art (SOTA) by scaling up its backbone model with various sizes of open-source LLMs, then explore prompt-based methods to harness proprietary LLMs’ potential, but find the best results close to random guessing, underlining the ineffectiveness of model scaling-up. Hence, we propose a novel Fine-grained cLAim depeNdency (FLAN) Graph through meticulous patent data analyses, capturing the inherent dependencies across segments of the patent text. As it is model-agnostic, we apply cost-effective graph models to our FLAN Graph to obtain representations for approval prediction. Extensive experiments and detailed analyses prove that incorporating FLAN Graph via various graph models consistently outperforms all LLM baselines significantly. We hope that our observations and analyses in this paper can bring more attention to this challenging task and prompt further research into the limitations of LLMs.;4;compares LLMs performance to other model and concludes that LLMs are very bad at that task;"""then explore prompt-based methods to harness proprietary LLMs’ potential, but find the best results close to random guessing, underlining the ineffectiveness of model scaling-up."", ""consistently outperforms all LLM baselines significantly.""";;;;4;simple domain-specific graph methods outperform scaling up LLMs;"""Model scaling"", ""can fall short in specific scenarios where simple customized methods excel."", ""best results close to random guessing, underlining the ineffectiveness of model scaling-up."", ""FLAN Graph via various graph models consistently outperforms all LLM baselines""";;;;4;acl2024;August 2024
KOLD: Korean Offensive Language Dataset;Recent directions for offensive language detection are hierarchical modeling, identifying the type and the target of offensive language, and interpretability with offensive span annotation and prediction. These improvements are focused on English and do not transfer well to other languages because of cultural and linguistic differences. In this paper, we present the Korean Offensive Language Dataset (KOLD) comprising 40,429 comments, which are annotated hierarchically with the type and the target of offensive language, accompanied by annotations of the corresponding text spans. We collect the comments from NAVER news and YouTube platform and provide the titles of the articles and videos as the context information for the annotation process. We use these annotated comments as training data for Korean BERT and RoBERTa models and find that they are effective at offensiveness detection, target classification, and target span detection while having room for improvement for target group classification and offensive span detection. We discover that the target group distribution differs drastically from the existing English datasets, and observe that providing the context information improves the model performance in offensiveness detection (+0.3), target classification (+1.5), and target group classification (+13.1). We publicly release the dataset and baseline models.;3;mentions strenghts and weaknesses of LLM, more focused on new dataset and solving the task;"""while having room for improvement for target group classification and offensive span detection.""";;;;2;;"""These improvements are focused on English and do not transfer well to other languages because of cultural and linguistic differences.""";;;;3;emnlp2022;December 2022
Why Would You Suggest That? Human Trust in Language Model Responses;The emergence of Large Language Models (LLMs) has revealed a growing need for  human-AI collaboration, especially in creative decision-making scenarios where  trust and reliance are paramount. Through human studies and model evaluations  on the open-ended News Headline Generation task from the LaMP benchmark, we  analyze how the framing and presence of explanations affect user trust and  model performance. Overall, we provide evidence that adding an explanation in  the model response to justify its reasoning significantly increases  self-reported user trust in the model when the user has the opportunity to  compare various responses. Position and faithfulness of these explanations are  also important factors. However, these gains disappear when users are shown  responses independently, suggesting that humans trust all model responses,  including deceptive ones, equitably when they are shown in isolation. Our  findings urge future research to delve deeper into the nuanced evaluation of  trust in human-machine teaming systems.;2;;"""However, these gains disappear when users are shown  responses independently, suggesting that humans trust all model responses,  including deceptive ones, equitably when they are shown in isolation.""";;;;2;Minor limitation of trust in llms;"""Position and faithfulness of these explanations are  also important factors. However, these gains disappear when users are shown  responses independently, suggesting that humans trust all model responses,  including deceptive ones, equitably when they are shown in isolation.""";;;;2;arxiv;04 June 2024
Social Learning: Towards Collaborative Learning with Large Language Models;"We introduce the framework of ""social learning"" in the context of large  language models (LLMs), whereby models share knowledge with each other in a  privacy-aware manner using natural language. We present and evaluate two  approaches for knowledge transfer between LLMs. In the first scenario, we allow  the model to generate abstract prompts aiming to teach the task. In our second  approach, models transfer knowledge by generating synthetic examples. We  evaluate these methods across diverse datasets and quantify memorization as a  proxy for privacy loss. These techniques inspired by social learning yield  promising results with low memorization of the original data. In particular, we  show that performance using these methods is comparable to results with the use  of original labels and prompts. Our work demonstrates the viability of social  learning for LLMs, establishes baseline approaches and highlights several  unexplored areas for future work.";1;;;;;;1;;;;;;1;arxiv;18 December 2023
Counterfactual reasoning: Testing language models’ understanding of hypothetical scenarios;Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on the understanding of real world. We tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from five pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge—however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.;3;deals with logical reasoning abilities and understanding of LLMs, finds that their abilities are not robust, also mentions strengths;"""it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on the understanding of real world."", ""we also find that for most models this effect appears largely to be driven by simple lexical cues"", ""though this sensitivity is also non-trivially impacted by lexical associative factors.""";;;;3;Big focus on limitations as well as strengths;"""it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on the understanding of real world."", ""for most models this effect appears largely to be driven by simple lexical cues."", ""only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.""";;;;3;acl2023;July 2023
OpenAsp: A Benchmark for Multi-document Open Aspect-based Summarization;The performance of automatic summarization models has improved dramatically in recent years. Yet, there is still a gap in meeting specific information needs of users in real-world scenarios, particularly when a targeted summary is sought, such as in the useful aspect-based summarization setting targeted in this paper. Previous datasets and studies for this setting have predominantly concentrated on a limited set of pre-defined aspects, focused solely on single document inputs, or relied on synthetic data. To advance research on more realistic scenarios, we introduce OpenAsp, a benchmark for multi-document open aspect-based summarization. This benchmark is created using a novel and cost-effective annotation protocol, by which an open aspect dataset is derived from existing generic multi-document summarization datasets. We analyze the properties of OpenAsp showcasing its high-quality content. Further, we show that the realistic open-aspect setting realized in OpenAsp poses a challenge for current state-of-the-art summarization models, as well as for large language models.;3;deals with challenges in automatic summarization tasks, mentions LLMs as a side note;"""OpenAsp poses a challenge for current state-of-the-art summarization models, as well as for large language models.""";;;;3;;"""Yet, there is still a gap in meeting specific information needs of users in real-world scenarios, particularly when a targeted summary is sought, such as in the useful aspect-based summarization setting targeted in this paper."", ""we show that the realistic open-aspect setting realized in OpenAsp poses a challenge for current state-of-the-art summarization models, as well as for large language models.""";;;;3;emnlp2023;December 2023
PAD-Net: An Efficient Framework for Dynamic Networks;Dynamic networks, e.g., Dynamic Convolution (DY-Conv) and the Mixture of Experts (MoE), have been extensively explored as they can considerably improve the model’s representation power with acceptable computational cost. The common practice in implementing dynamic networks is to convert the given static layers into fully dynamic ones where all parameters are dynamic (at least within a single layer) and vary with the input. However, such a fully dynamic setting may cause redundant parameters and high deployment costs, limiting the applicability of dynamic networks to a broader range of tasks and models. The main contributions of our work are challenging the basic commonsense in dynamic networks and proposing a partially dynamic network, namely PAD-Net, to transform the redundant dynamic parameters into static ones. Also, we further design Iterative Mode Partition to partition dynamic and static parameters efficiently. Our method is comprehensively supported by large-scale experiments with two typical advanced dynamic architectures, i.e., DY-Conv and MoE, on both image classification and GLUE benchmarks. Encouragingly, we surpass the fully dynamic networks by +0.7% top-1 acc with only 30% dynamic parameters for ResNet-50 and +1.9% average score in language understanding with only 50% dynamic parameters for BERT. Code will be released at: https://github.com/Shwai-He/PAD-Net.;1;not really dealing with LLMs, but BERT is mentioned;;;;;1;;;;;;1;acl2023;July 2023
Grafting Pre-trained Models for Multimodal Headline Generation;Multimodal headline utilizes both video frames and transcripts to generate the natural language title of the videos. Due to a lack of large-scale, manually annotated data, the task of annotating grounded headlines for video is labor intensive and impractical. Previous researches on pre-trained language models and video-language models have achieved significant progress in related downstream tasks. However, none of them can be directly applied to multimodal headline architecture where we need both multimodal encoder and sentence decoder. A major challenge in simply gluing language model and video-language model is the modality balance, which is aimed at combining visual-language complementary abilities. In this paper, we propose a novel approach to graft the video encoder from the pre-trained video-language model on the generative pre-trained language model. We also present a consensus fusion mechanism for the integration of different components, via inter/intra modality relation. Empirically, experiments show that the grafted model achieves strong results on a brand-new dataset collected from real-world applications.;3;explains the limitation in detail, then focuses on solution;"""none of them can be directly applied to multimodal headline architecture where we need both multimodal encoder and sentence decoder. A major challenge in simply gluing language model and video-language model is the modality balance, which is aimed at combining visual-language complementary abilities.""";;;;2;;"""A major challenge in simply gluing language model and video-language model is the modality balance, which is aimed at combining visual-language complementary abilities.""";;;;3;emnlp2022;December 2022
Building a Role Specified Open-Domain Dialogue System Leveraging Large-Scale Language Models;Recent open-domain dialogue models have brought numerous breakthroughs. However, building a chat system is not scalable since it often requires a considerable volume of human-human dialogue data, especially when enforcing features such as persona, style, or safety. In this work, we study the challenge of imposing roles on open-domain dialogue systems, with the goal of making the systems maintain consistent roles while conversing naturally with humans. To accomplish this, the system must satisfy a role specification that includes certain conditions on the stated features as well as a system policy on whether or not certain types of utterances are allowed. For this, we propose an efficient data collection framework leveraging in-context few-shot learning of large-scale language models for building role-satisfying dialogue dataset from scratch. We then compare various architectures for open-domain dialogue systems in terms of meeting role specifications while maintaining conversational abilities. Automatic and human evaluations show that our models return few out-of-bounds utterances, keeping competitive performance on general metrics. We release a Korean dialogue dataset we built for further research.;1;uses LLMs to generate data to build a chat system;;;;;2;;"""building a chat system is not scalable since it often requires a considerable volume of human-human dialogue data, especially when enforcing features such as persona, style, or safety.""";;;;2;naacl2022;July 2022
No that's not what I meant: Handling Third Position Repair in Conversational Question Answering;The ability to handle miscommunication is crucial to robust and faithful  conversational AI. People usually deal with miscommunication immediately as  they detect it, using highly systematic interactional mechanisms called repair.  One important type of repair is Third Position Repair (TPR) whereby a speaker  is initially misunderstood but then corrects the misunderstanding as it becomes  apparent after the addressee's erroneous response. Here, we collect and  publicly release Repair-QA, the first large dataset of TPRs in a conversational  question answering (QA) setting. The data is comprised of the TPR turns,  corresponding dialogue contexts, and candidate repairs of the original turn for  execution of TPRs. We demonstrate the usefulness of the data by training and  evaluating strong baseline models for executing TPRs. For stand-alone TPR  execution, we perform both automatic and human evaluations on a fine-tuned T5  model, as well as OpenAI's GPT-3 LLMs. Additionally, we extrinsically evaluate  the LLMs' TPR processing capabilities in the downstream conversational QA task.  The results indicate poor out-of-the-box performance on TPR's by the GPT-3  models, which then significantly improves when exposed to Repair-QA.;2;;"""The results indicate poor out-of-the-box performance on TPR's by the GPT-3  models""";;;;2;;"""The results indicate poor out-of-the-box performance on TPR's by the GPT-3  models""";;;;2;arxiv;31 July 2023
TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models;Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the state-of-the-art model with similar capacity, and the LLM teacher. In a systematic study, we compare TrueTeacher to existing synthetic data generation methods and demonstrate its superiority and robustness to domain-shift. We also show that our method generalizes to multilingual scenarios. Lastly, we release our large scale synthetic dataset (1.4M examples), generated using TrueTeacher, and a checkpoint trained on this data.;2;;"""large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use.""";;;;2;;"""Natural Language Inference (NLI) models"", ""exhibit limited success in evaluating summaries"", ""arge language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use.""";;;;2;emnlp2023;December 2023
LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset;Studying how people interact with large language models (LLMs) in real-world  scenarios is increasingly important due to their widespread use in various  applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset  containing one million real-world conversations with 25 state-of-the-art LLMs.  This dataset is collected from 210K unique IP addresses in the wild on our  Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's  content, including its curation process, basic statistics, and topic  distribution, highlighting its diversity, originality, and scale. We  demonstrate its versatility through four use cases: developing content  moderation models that perform similarly to GPT-4, building a safety benchmark,  training instruction-following models that perform similarly to Vicuna, and  creating challenging benchmark questions. We believe that this dataset will  serve as a valuable resource for understanding and advancing LLM capabilities.  The dataset is publicly available at  https://huggingface.co/datasets/lmsys/lmsys-chat-1m.;1;;;;;;1;;;;;;1;iclr2024;May 2024
Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words;Cosine similarity of contextual embeddings is used in many NLP tasks (e.g., QA, IR, MT) and metrics (e.g., BERTScore). Here, we uncover systematic ways in which word similarities estimated by cosine over BERT embeddings are understated and trace this effect to training data frequency. We find that relative to human judgements, cosine similarity underestimates the similarity of frequent words with other instances of the same word or other words across contexts, even after controlling for polysemy and other factors. We conjecture that this underestimation of similarity for high frequency words is due to differences in the representational geometry of high and low frequency words and provide a formal argument for the two-dimensional case.;1;mentions problems of cosine similarity not specific LLM limitations;;;;;1;;;;;;1;acl2022;May 2022
Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation;Natural language generation has witnessed significant advancements due to the training of large language models on vast internet-scale datasets. Despite these advancements, there exists a critical challenge: These models can inadvertently generate content that is toxic, inaccurate, and unhelpful, and existing automatic evaluation metrics often fall short of identifying these shortcomings. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of recent research that has leveraged human feedback to improve natural language generation. First, we introduce a taxonomy distilled from existing research to categorize and organize the varied forms of feedback. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. Finally, we provide an overview of the nascent field of AI feedback, which uses large language models to make judgments based on a set of principles and minimize the need for human intervention. We also release a website of this survey at feedback-gap-survey.info.;3;provides survey on human feedback to LLM responses, motivated by limitations such as toxic or unhelpful content generation, no details on the limitations mentioned;"""These models can inadvertently generate content that is toxic, inaccurate, and unhelpful,""";;;;2;;"""These models can inadvertently generate content that is toxic, inaccurate, and unhelpful, and existing automatic evaluation metrics often fall short of identifying these shortcomings.""";;;;3;tacl2023;January 2023
Updating the Minimum Information about CLinical Artificial Intelligence (MI-CLAIM) checklist for generative modeling research;"Recent advances in generative models, including large language models (LLMs),  vision language models (VLMs), and diffusion models, have accelerated the field  of natural language and image processing in medicine and marked a significant  paradigm shift in how biomedical models can be developed and deployed. While  these models are highly adaptable to new tasks, scaling and evaluating their  usage presents new challenges not addressed in previous frameworks. In  particular, the ability of these models to produce useful outputs with little  to no specialized training data (""zero-"" or ""few-shot"" approaches), as well as  the open-ended nature of their outputs, necessitate the development of updated  guidelines in using and evaluating these models. In response to gaps in  standards and best practices for the development of clinical AI tools  identified by US Executive Order 141103 and several emerging national networks  for clinical AI evaluation, we begin to formalize some of these guidelines by  building on the ""Minimum information about clinical artificial intelligence  modeling"" (MI-CLAIM) checklist. The MI-CLAIM checklist, originally developed in  2020, provided a set of six steps with guidelines on the minimum information  necessary to encourage transparent, reproducible research for artificial  intelligence (AI) in medicine. Here, we propose modifications to the original  checklist that highlight differences in training, evaluation, interpretability,  and reproducibility of generative models compared to traditional AI models for  clinical research. This updated checklist also seeks to clarify cohort  selection reporting and adds additional items on alignment with ethical  standards.";2;proposes method to evaluate LLMs on clinical tasks;"""scaling and evaluating their  usage presents new challenges not addressed in previous frameworks.""";;;;2;Challenges just mentioned as a reason to propose changes to an evaluation framework;"""scaling and evaluating their  usage presents new challenges not addressed in previous frameworks.""";;;;2;arxiv;05 March 2024
Automating Human Evaluation of Dialogue Systems;Automated metrics to evaluate dialogue systems like BLEU, METEOR, etc., weakly correlate with human judgments. Thus, human evaluation is often used to supplement these metrics for system evaluation. However, human evaluation is time-consuming as well as expensive. This paper provides an alternative approach to human evaluation with respect to three aspects: naturalness, informativeness, and quality in dialogue systems. I propose an approach based on fine-tuning the BERT model with three prediction heads, to predict whether the system-generated output is natural, fluent, and informative. I observe that the proposed model achieves an average accuracy of around 77% over these 3 labels. I also design a baseline approach that uses three different BERT models to make the predictions. Based on experimental analysis, I find that using a shared model to compute the three labels performs better than three separate models.;1;;;;;;1;;;;;;1;naacl2022;July 2022
bert2BERT: Towards Reusable Pretrained Language Models;"In recent years, researchers tend to pre-train ever-larger language models to explore the upper limit of deep models. However, large language model pre-training costs intensive computational resources, and most of the models are trained from scratch without reusing the existing pre-trained models, which is wasteful. In this paper, we propose bert2BERT, which can effectively transfer the knowledge of an existing smaller pre-trained model to a large model through parameter initialization and significantly improve the pre-training efficiency of the large model. Specifically, we extend the previous function-preserving method proposed in computer vision on the Transformer-based language model, and further improve it by proposing a novel method, advanced knowledge for large model’s initialization. In addition, a two-stage learning method is proposed to further accelerate the pre-training. We conduct extensive experiments on representative PLMs (e.g., BERT and GPT) and demonstrate that (1) our method can save a significant amount of training cost compared with baselines including learning from scratch, StackBERT and MSLT; (2) our method is generic and applicable to different types of pre-trained models. In particular, bert2BERT saves about 45% and 47% computational cost of pre-training BERT BASE and GPT BASE by reusing the models of almost their half sizes.";2;;"""However, large language model pre-training costs intensive computational resources, and most of the models are trained from scratch without reusing the existing pre-trained models, which is wasteful.""";;;;2;;"""However, large language model pre-training costs intensive computational resources, and most of the models are trained from scratch without reusing the existing pre-trained models, which is wasteful.""";;;;2;acl2022;May 2022
HybridBERT - Making BERT Pretraining More Efficient Through Hybrid Mixture of Attention Mechanisms;Pretrained transformer-based language models have produced state-of-the-art performance in most natural language understanding tasks. These models undergo two stages of training: pretraining on a huge corpus of data and fine-tuning on a specific downstream task. The pretraining phase is extremely compute-intensive and requires several high-performance computing devices like GPUs and several days or even months of training, but it is crucial for the model to capture global knowledge and also has a significant impact on the fine-tuning task. This is a major roadblock for researchers without access to sophisticated computing resources. To overcome this challenge, we propose two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization. We introduce a computing budget to the pretraining phase, limiting the training time and usage to a single GPU. We show that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline. We also evaluate our proposed models on two downstream tasks, where we outperform BERT-base while accelerating inference. Moreover, we study the effect of weight initialization with a limited pretraining budget. The code and models are publicly available at: www.github.com/gokulsg/HBERT/.;2;;"""The pretraining phase is extremely compute-intensive and requires several high-performance computing devices like GPUs and several days or even months of training"", ""major roadblock for researchers without access to sophisticated computing resources""";;;;2;;"""The pretraining phase is extremely compute-intensive and requires several high-performance computing devices like GPUs and several days or even months of training""";;;;2;naacl2024;June 2024
Hidden Schema Networks;Large, pretrained language models infer powerful representations that encode rich semantic and syntactic content, albeit implicitly. In this work we introduce a novel neural language model that enforces, via inductive biases, explicit relational structures which allow for compositionality onto the output representations of pretrained language models. Specifically, the model encodes sentences into sequences of symbols (composed representations), which correspond to the nodes visited by biased random walkers on a global latent graph, and infers the posterior distribution of the latter. We first demonstrate that the model is able to uncover ground-truth graphs from artificially generated datasets of random token sequences. Next, we leverage pretrained BERT and GPT-2 language models as encoder and decoder, respectively, to infer networks of symbols (schemata) from natural language datasets. Our experiments show that (i) the inferred symbols can be interpreted as encoding different aspects of language, as e.g. topics or sentiments, and that (ii) GPT-2-like models can effectively be conditioned on symbolic representations. Finally, we explore training autoregressive, random walk “reasoning” models on schema networks inferred from commonsense knowledge databases, and using the sampled paths to enhance the performance of pretrained language models on commonsense If-Then reasoning tasks.;1;;;;;;1;1;;;;;1;acl2023;July 2023
Understanding When Tree of Thoughts Succeeds: Larger Models Excel in Generation, Not Discrimination;Tree of Thoughts (ToT) is a reasoning strategy for Large Language Models  (LLMs) that employs a generator to suggest reasoning steps and a discriminator  to decide which steps to implement. ToT demonstrates strong performance on  reasoning tasks, often surpassing simple methods such as Input-Output (IO)  prompting and Chain-of-Thought (CoT) reasoning. However, ToT does not  consistently outperform such simpler methods across all models, leaving large  knowledge gaps on the conditions under which ToT is most beneficial. In this  paper, we analyze the roles of the generator and discriminator separately to  better understand the conditions when ToT is beneficial. We find that the  generator plays a more critical role than the discriminator in driving the  success of ToT. Scaling the generator leads to notable improvements in ToT  performance, even when using a smaller model as the discriminator, whereas  scaling the discriminator with a fixed generator yields only marginal gains.  Our results show that models across different scales exhibit comparable  discrimination capabilities, yet differ significantly in their generative  performance for ToT.;2;investigates prompting-technique, how to optimize it;"""However, ToT does not  consistently outperform such simpler methods across all models, leaving large  knowledge gaps on the conditions under which ToT is most beneficial.""";;;;2;More limitation of a method;"""ToT does not  consistently outperform such simpler methods across all models, leaving large  knowledge gaps on the conditions under which ToT is most beneficial.""";;;;2;arxiv;23 October 2024
Invariant Language Modeling;Modern pretrained language models are critical components of NLP pipelines. Yet, they suffer from spurious correlations, poor out-of-domain generalization, and biases.Inspired by recent progress in causal machine learning, in particular the invariant risk minimization (IRM) paradigm, we propose invariant language modeling, a framework for learning invariant representations that generalize better across multiple environments. In particular, we adapt a game-theoretic implementation of IRM (IRM-games) to language models, where the invariance emerges from a specific training schedule in which all the environments compete to optimize their own environment-specific loss by updating subsets of the model in a round-robin fashion.We focused on controlled experiments to precisely demonstrate the ability of our method to (i) remove structured noise, (ii) ignore specific spurious correlations without affecting global performance, and (iii) achieve better out-of-domain generalization.These benefits come with a negligible computational overhead compared to standard training, do not require changing the local loss, and can be applied to any language model. We believe this framework is promising to help mitigate spurious correlations and biases in language models.;2;;"""Yet, they suffer from spurious correlations, poor out-of-domain generalization, and biases.""";;;;2;;"""Yet, they suffer from spurious correlations, poor out-of-domain generalization, and biases.""";;;;2;emnlp2022;December 2022
Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach;Post-editing has proven effective in improving the quality of text generated by large language models (LLMs) such as GPT-3.5 or GPT-4, particularly when direct updating of their parameters to enhance text quality is infeasible or expensive. However, relying solely on smaller language models for post-editing can limit the LLMs' ability to generalize across domains. Moreover, the editing strategies in these methods are not optimally designed for text generation tasks. To address these limitations, we propose a neural programmer-interpreter approach that preserves the domain generalization ability of LLMs while editing their output. The editing actions in this framework are specifically devised for text generation. Extensive experiments demonstrate that the programmer-interpreter significantly enhances GPT-3.5's performance in logical form-to-text conversion and low-resource machine translation, surpassing other state-of-the-art (SOTA) LLM post-editing methods in cross-domain settings.;2;mentions 2 limitations and how the approach they present overcomes them;"""relying solely on smaller language models for post-editing can limit the LLMs' ability to generalize across domains. Moreover, the editing strategies in these methods are not optimally designed for text generation tasks.""";;;;;;;;;;2;arxiv;07 February 2024
Reward Engineering for Generating Semi-structured Explanation;Semi-structured explanation depicts the implicit process of a reasoner with an explicit representation. This explanation highlights how available information in a specific query is utilised and supplemented with information a reasoner produces from its internal weights towards generating an answer. Despite the recent improvements in generative capabilities of language models, producing structured explanations to verify a model's true reasoning capabilities remains a challenge. This issue is particularly pronounced for not-so-large LMs (e.g., FLAN-T5-XXL). In this work, we first underscore the limitations of supervised fine-tuning (SFT) in tackling this challenge, and then introduce a carefully crafted reward engineering method in reinforcement learning (RL) to better address this problem. We investigate multiple reward aggregation methods and provide a detailed discussion which sheds light on the promising potential of RL for future research. Our proposed method on two semi-structured explanation generation benchmarks (ExplaGraph and COPA-SSE) achieves new state-of-the-art results.;3;underscores limitations of LLMs, but main focus is on solving those ;"""Despite the recent improvements in generative capabilities of language models, producing structured explanations to verify a model's true reasoning capabilities remains a challenge"", ""we first underscore the limitations of supervised fine-tuning (SFT) in tackling this challenge""";;;;;;;;;;3;arxiv;15 September 2023
Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?;Large Language Models (LLMs) excel in various Natural Language Processing (NLP) tasks, yet their evaluation, particularly in languages beyond the top 20, remains inadequate due to existing benchmarks and metrics limitations. Employing LLMs as evaluators to rank or score other models' outputs emerges as a viable solution, addressing the constraints tied to human annotators and established benchmarks. In this study, we explore the potential of LLM-based evaluators in enhancing multilingual evaluation by calibrating them against 20K human judgments across three text-generation tasks, five metrics, and eight languages. Our analysis reveals a bias in LLM-based evaluators towards higher scores, underscoring the necessity of calibration with native speaker judgments, especially in low-resource and non-Latin script languages, to ensure accurate evaluation of LLM performance across diverse languages.;4;hints at problems when evaluating LLMs on less common languages;"""yet their evaluation, particularly in languages beyond the top 20, remains inadequate due to existing benchmarks and metrics limitations."", ""Our analysis reveals a bias in LLM-based evaluators towards higher scores"", ""underscoring the necessity of calibration with native speaker judgments, especially in low-resource and non-Latin script languages, to ensure accurate evaluation of LLM performance across diverse languages.""";;;;;;;;;;4;arxiv;14 September 2023
Fine-tuning CLIP Text Encoders with Two-step Paraphrasing;Contrastive language-image pre-training (CLIP) models have demonstrated considerable success across various vision-language tasks, such as text-to-image retrieval, where the model is required to effectively process natural language input to produce an accurate visual output. However, current models still face limitations in dealing with linguistic variations in input queries, such as paraphrases, making it challenging to handle a broad range of user queries in real-world applications. In this study, we introduce a straightforward fine-tuning approach to enhance the representations of CLIP models for paraphrases. Our approach involves a two-step paraphrase generation process, where we automatically create two categories of paraphrases from web-scale image captions by leveraging large language models. Subsequently, we fine-tune the CLIP text encoder using these generated paraphrases while freezing the image encoder. Our resulting model, which we call ParaCLIP, exhibits significant improvements over baseline CLIP models across various tasks, including paraphrased retrieval (with rank similarity scores improved by up to 7.6% and 9.6%), Visual Genome Relation and Attribution, as well as seven semantic textual similarity tasks.;2;;"""However, current models still face limitations in dealing with linguistic variations in input queries, such as paraphrases, making it challenging to handle a broad range of user queries in real-world applications.""";;;;;;;;;;2;arxiv;23 February 2024
ICE-Score: Instructing Large Language Models to Evaluate Code;Recent advancements in the field of natural language generation have facilitated the use of large language models to assess the quality of generated text. Although these models have shown promising results in tasks such as machine translation and summarization, their applicability in code intelligence tasks remains limited without human involvement. The complexity of programming concepts required for such tasks makes it difficult to develop evaluation metrics that align with human judgment. Token-matching-based metrics, such as BLEU, have demonstrated weak correlations with human practitioners in code intelligence tasks. Moreover, utilizing human-written test suites to evaluate functional correctness can be challenging in domains with low resources. To overcome these obstacles, we propose ICE-Score, a new evaluation metric via instructing large language models (LLMs) for code assessments. Our metric addresses the limitations of existing approaches by achieving superior correlations with functional correctness and human preferences, without the need for test oracles or references. We evaluate the efficacy of our metric on two different aspects (human preference and execution success) and four programming languages. Our results demonstrate that our metric surpasses state-of-the-art metrics for code generation, delivering high levels of accuracy and consistency across various programming languages and tasks. We also make our evaluation metric and datasets available to the public, encouraging further research in evaluating code intelligence tasks.;3;mentions many limitations evaluating LLMs to propose new benchmark;"""their applicability in code intelligence tasks remains limited without human involvement. The complexity of programming concepts required for such tasks makes it difficult to develop evaluation metrics that align with human judgment. Token-matching-based metrics, such as BLEU, have demonstrated weak correlations with human practitioners in code intelligence tasks. Moreover, utilizing human-written test suites to evaluate functional correctness can be challenging in domains with low resources.""";;;;;;;;;;3;arxiv;27 April 2023
Transformer-specific Interpretability;Transformers have emerged as dominant players in various scientific fields, especially NLP. However, their inner workings, like many other neural networks, remain opaque. In spite of the widespread use of model-agnostic interpretability techniques, including gradient-based and occlusion-based, their shortcomings are becoming increasingly apparent for Transformer interpretation, making the field of interpretability more demanding today. In this tutorial, we will present Transformer-specific interpretability methods, a new trending approach, that make use of specific features of the Transformer architecture and are deemed more promising for understanding Transformer-based models. We start by discussing the potential pitfalls and misleading results model-agnostic approaches may produce when interpreting Transformers. Next, we discuss Transformer-specific methods, including those designed to quantify context-mixing interactions among all input pairs (as the fundamental property of the Transformer architecture) and those that combine causal methods with low-level Transformer analysis to identify particular subnetworks within a model that are responsible for specific tasks. By the end of the tutorial, we hope participants will understand the advantages (as well as current limitations) of Transformer-specific interpretability methods, along with how these can be applied to their own research.;3;;"""However, their inner workings, like many other neural networks, remain opaque."", ""their shortcomings are becoming increasingly apparent for Transformer interpretation, making the field of interpretability more demanding today."", ""We start by discussing the potential pitfalls and misleading results model-agnostic approaches may produce when interpreting Transformers.""";;;;;;;;;;3;eacl2024;March 2024
Can docstring reformulation with an LLM improve code generation?;Generating code is an important application of Large Language Models (LLMs) and the task of function completion is one of the core open challenges in this context. Existing approaches focus on either training, fine-tuning or prompting LLMs to generate better outputs given the same input. We propose a novel and complementary approach: to optimize part of the input, the docstring (summary of a function's purpose and usage), via reformulation with an LLM, in order to improve code generation. We develop two baseline methods for optimizing code generation via docstring reformulation and test them on the original HumanEval benchmark and multiple curated variants which are made more challenging by realistically worsening the docstrings. Our results show that, when operating on docstrings reformulated by an LLM instead of the original (or worsened) inputs, the performance of a number of open-source LLMs does not change significantly. This finding demonstrates an unexpected robustness of current open-source LLMs to the details of the docstrings. We conclude by examining a series of questions, accompanied by in-depth analyses, pertaining to the sensitivity of current open-source LLMs to the details in the docstrings, the potential for improvement via docstring reformulation and the limitations of the methods employed in this work.;2;;"""the task of function completion is one of the core open challenges in this context.""";;;;;;;;;;2;eacl2024;March 2024
Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance;Large language models (LLMs) have exhibited an array of reasoning capabilities but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount. We explore the potential of language model augmentation with external tools to mitigate these limitations and offload certain reasoning steps to external tools that are more suited for the task, instead of solely depending on the LLM's inherent abilities. More concretely, using financial domain question-answering datasets, we apply supervised fine-tuning on a LLaMA-2 13B Chat model to act both as a 'task router' and 'task solver'. The 'task router' dynamically directs a question to either be answered internally by the LLM or externally via the right tool from the tool set. Our tool-equipped SFT model, Raven, demonstrates an improvement of 35.2% and 5.06% over the base model and SFT-only baselines, respectively, and is highly competitive with strong GPT-3.5 results. To the best of our knowledge, our work is the first that investigates tool augmentation of language models for the finance domain.;2;;"""but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount.""";;;;;;;;;;2;eacl2024;March 2024
Generation-driven Contrastive Self-training for Zero-shot Text Classification with Instruction-following LLM;The remarkable performance of large language models (LLMs) in zero-shot language understanding has garnered significant attention. However, employing LLMs for large-scale inference or domain-specific fine-tuning requires immense computational resources due to their substantial model size. To overcome these limitations, we introduce a novel method, namely GenCo, which leverages the strong generative power of LLMs to assist in training a smaller and more adaptable language model. In our method, an LLM plays an important role in the self-training loop of a smaller model in two important ways. Firstly, we utilize an LLM to generate multiple augmented texts for each input instance to enhance its semantic meaning for better understanding. Secondly, we additionally generate high-quality training instances conditioned on predicted labels, ensuring the generated texts are relevant to the labels. In this way, GenCo not only corrects the errors of predicted labels during self-training but also eliminates the need for extensive unlabeled texts. In our experiments, GenCo outperforms previous state-of-the-art methods when only limited (<5% of original) in-domain text data is available. Notably, our approach surpasses Alpaca-7B with human instructions, highlighting the significance of self-training.;2;;"""However, employing LLMs for large-scale inference or domain-specific fine-tuning requires immense computational resources due to their substantial model size.""";;;;;;;;;;2;eacl2024;March 2024
Document-Level Language Models for Machine Translation;Despite the known limitations, most machine translation systems today still operate on the sentence-level. One reason for this is, that most parallel training data is only sentence-level aligned, without document-level meta information available. In this work, we set out to build context-aware translation systems utilizing document-level monolingual data instead. This can be achieved by combining any existing sentence-level translation model with a document-level language model. We improve existing approaches by leveraging recent advancements in model combination. Additionally, we propose novel weighting techniques that make the system combination more flexible and significantly reduce computational overhead. In a comprehensive evaluation on four diverse translation tasks, we show that our extensions improve document-targeted scores significantly and are also computationally more efficient. However, we also find that in most scenarios, back-translation gives even better results, at the cost of having to re-train the translation system. Finally, we explore language model fusion in the light of recent advancements in large language models. Our findings suggest that there might be strong potential in utilizing large language models via model combination.;1;proposes LLMs to overcome limitation of previous models;;;;;;;;;;;1;arxiv;18 October 2023
ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages;Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs{'} MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world{'}s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language{'}s resource level is the most important feature in determining ChatGPT{'}s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.;4;investigates LLMs MT ability. concludes strengths on high resource languages, but weaknesses on low-resource languages;"""Without published experimental evidence on the matter, it is difficult for speakers of the world{'}s diverse languages to know how and whether they can use LLMs for their languages."", ""but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered."", ""ChatGPT is especially disadvantaged for LRLs and African languages.""";;;;;;;;;;4;arxiv;14 September 2023
Large Language Models Effectively Leverage Document-level Context for Literary Translation, but Critical Errors Persist;Large language models (LLMs) are competitive with the state of the art on a wide range of sentence-level translation datasets. However, their ability to translate paragraphs and documents remains unexplored because evaluation in these settings is costly and difficult. We show through a rigorous human evaluation that asking the GPT-3.5 (text-davinci-003) LLM to translate an entire literary paragraph (e.g., from a novel) at once results in higher-quality translations than standard sentence-by-sentence translation across 18 linguistically-diverse language pairs (e.g., translating into and out of Japanese, Polish, and English). Our evaluation, which took approximately 350 hours of effort for annotation and analysis, is conducted by hiring translators fluent in both the source and target language and asking them to provide both span-level error annotations as well as preference judgments of which system's translations are better. We observe that discourse-level LLM translators commit fewer mistranslations, grammar errors, and stylistic inconsistencies than sentence-level approaches. With that said, critical errors still abound, including occasional content omissions, and a human translator's intervention remains necessary to ensure that the author's voice remains intact. We publicly release our dataset and error annotations to spur future research on the evaluation of document-level literary translation.;4;investigates LLMs translation ability of whole paragraphs compared to sentence-by-sentence translation. concludes there are weaknesses especially when doing sentence-by-sentence translation and criticial error still persist in both cases;"""However, their ability to translate paragraphs and documents remains unexplored because evaluation in these settings is costly and difficult."", ""than standard sentence-by-sentence translation across 18 linguistically-diverse language pairs"", ""With that said, critical errors still abound, including occasional content omissions, and a human translator's intervention remains necessary to ensure that the author's voice remains intact.""";;;;;;;;;;4;arxiv;06 April 2023
Automating Behavioral Testing in Machine Translation;Behavioral testing in NLP allows fine-grained evaluation of systems by examining their linguistic capabilities through the analysis of input-output behavior. Unfortunately, existing work on behavioral testing in Machine Translation (MT) is currently restricted to largely handcrafted tests covering a limited range of capabilities and languages. To address this limitation, we propose to use Large Language Models (LLMs) to generate a diverse set of source sentences tailored to test the behavior of MT models in a range of situations. We can then verify whether the MT model exhibits the expected behavior through matching candidate sets that are also generated using LLMs. Our approach aims to make behavioral testing of MT systems practical while requiring only minimal human effort. In our experiments, we apply our proposed evaluation framework to assess multiple available MT systems, revealing that while in general pass-rates follow the trends observable from traditional accuracy-based metrics, our method was able to uncover several important differences and potential bugs that go unnoticed when relying only on accuracy.;1;mentions limitations in evaluating MT tasks, but not specifically of LLMs. instead uses LLMs to generate candidate sets;;;;;;;;;;;1;arxiv;05 September 2023
ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations;This paper presents a novel framework for quantitatively evaluating the interactive ChatGPT model in the context of suicidality assessment from social media posts, utilizing the University of Maryland Reddit suicidality dataset. We conduct a technical evaluation of ChatGPT's performance on this task using Zero-Shot and Few-Shot experiments and compare its results with those of two fine-tuned transformer-based models. Additionally, we investigate the impact of different temperature parameters on ChatGPT's response generation and discuss the optimal temperature based on the inconclusiveness rate of ChatGPT. Our results indicate that while ChatGPT attains considerable accuracy in this task, transformer-based models fine-tuned on human-annotated datasets exhibit superior performance. Moreover, our analysis sheds light on how adjusting the ChatGPT's hyperparameters can improve its ability to assist mental health professionals in this critical task.;1;mentions superiority of other models over chatGPT, but those models might also be considered LLMs. 1-2;;;;;;;;;;;1;arxiv;15 June 2023
GenIE: Generative Information Extraction;Structured and grounded representation of text is typically formalized by closed information extraction, the problem of extracting an exhaustive set of (subject, relation, object) triplets that are consistent with a predefined set of entities and relations from a knowledge base schema. Most existing works are pipelines prone to error accumulation, and all approaches are only applicable to unrealistically small numbers of entities and relations. We introduce GenIE (generative information extraction), the first end-to-end autoregressive formulation of closed information extraction. GenIE naturally exploits the language knowledge from the pre-trained transformer by autoregressively generating relations and entities in textual form. Thanks to a new bi-level constrained generation strategy, only triplets consistent with the predefined knowledge base schema are produced. Our experiments show that GenIE is state-of-the-art on closed information extraction, generalizes from fewer training data points than baselines, and scales to a previously unmanageable number of entities and relations. With this work, closed information extraction becomes practical in realistic scenarios, providing new opportunities for downstream tasks. Finally, this work paves the way towards a unified end-to-end approach to the core tasks of information extraction.;2;mentions limitations of previous approaches, not clear whether they include LLMs or not. 1-2;"""Most existing works are pipelines prone to error accumulation, and all approaches are only applicable to unrealistically small numbers of entities and relations.""";;;;;;;;;;2;naacl2022;July 2022
Quantifying Adaptability in Pre-trained Language Models with 500 Tasks;"When a neural language model (LM) is adapted to perform a new task, what aspects of the task predict the eventual performance of the model? In NLP, systematic features of LM generalization to individual examples are well characterized, but systematic aspects of LM adaptability to new tasks are not nearly as well understood. We present a large-scale empirical study of the features and limits of LM adaptability using a new benchmark, TaskBench500, built from 500 procedurally generated sequence modeling tasks. These tasks combine core aspects of language processing, including lexical semantics, sequence processing, memorization, logical reasoning, and world knowledge. Using TaskBench500, we evaluate three facets of adaptability, finding that: (1) adaptation procedures differ dramatically in their ability to memorize small datasets; (2) within a subset of task types, adaptation procedures exhibit compositional adaptability to complex tasks; and (3) failure to match training label distributions is explained by mismatches in the intrinsic difficulty of predicting individual labels. Our experiments show that adaptability to new tasks, like generalization to new examples, can be systematically described and understood, and we conclude with a discussion of additional aspects of adaptability that could be studied using the new benchmark.";3;proposes new benchmark and identifies limitations, but also mentions strengths;"""but systematic aspects of LM adaptability to new tasks are not nearly as well understood."", ""adaptation procedures differ dramatically in their ability to memorize small datasets;""";;;;;;;;;;3;naacl2022;July 2022
AmbiFC: Fact-Checking Ambiguous Claims with Evidence;Automated fact-checking systems verify claims against evidence to predict their veracity. In real-world scenarios, the retrieved evidence may not unambiguously support or refute the claim and yield conflicting but valid interpretations. Existing fact-checking datasets assume that the models developed with them predict a single veracity label for each claim, thus discouraging the handling of such ambiguity. To address this issue we present AmbiFC, a fact-checking dataset with 10k claims derived from real-world information needs. It contains fine-grained evidence annotations of 50k passages from 5k Wikipedia pages. We analyze the disagreements arising from ambiguity when comparing claims against evidence in AmbiFC, observing a strong correlation of annotator disagreement with linguistic phenomena such as underspecification and probabilistic reasoning. We develop models for predicting veracity handling this ambiguity via soft labels, and find that a pipeline that learns the label distribution for sentence-level evidence selection and veracity prediction yields the best performance. We compare models trained on different subsets of AmbiFC and show that models trained on the ambiguous instances perform better when faced with the identified linguistic phenomena.;0;"does not explicitly mention LLMs, only ""models""";;;;;;;;;;;0;tacl2024;January 2024
Language Varieties of Italy: Technology Challenges and Opportunities;Italy is characterized by a one-of-a-kind linguistic diversity landscape in Europe, which implicitly encodes local knowledge, cultural traditions, artistic expressions, and history of its speakers. However, most local languages and dialects in Italy are at risk of disappearing within a few generations. The NLP community has recently begun to engage with endangered languages, including those of Italy. Yet, most efforts assume that these varieties are under-resourced language monoliths with an established written form and homogeneous functions and needs, and thus highly interchangeable with each other and with high-resource, standardized languages. In this paper, we introduce the linguistic context of Italy and challenge the default machine-centric assumptions of NLP for Italy’s language varieties. We advocate for a shift in the paradigm from machine-centric to speaker-centric NLP, and provide recommendations and opportunities for work that prioritizes languages and their speakers over technological advances. To facilitate the process, we finally propose building a local community towards responsible, participatory efforts aimed at supporting vitality of languages and dialects of Italy.;0;deals with endangered languages in Italy, mentions NLP, but not LLMs;;;;;;;;;;;0;tacl2024;January 2024
Benchmarking Large Language Models for News Summarization;Large language models (LLMs) have shown promise for automatic summarization but the reasons behind their successes are poorly understood. By conducting a human evaluation on ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations. First, we find instruction tuning, not model size, is the key to the LLM’s zero-shot summarization capability. Second, existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance. To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers. Despite major stylistic differences such as the amount of paraphrasing, we find that LLM summaries are judged to be on par with human written summaries.;2;investigates summarization abilities of LLMs, briefly mentions a weakness, but concludes that they perform on par with human summaries;"""but the reasons behind their successes are poorly understood.""";;;;;;;;;;2;tacl2024;January 2024
mGPT: Few-Shot Learners Go Multilingual;This paper introduces mGPT, a multilingual variant of GPT-3, pretrained on 61 languages from 25 linguistically diverse language families using Wikipedia and the C4 Corpus. We detail the design and pretraining procedure. The models undergo an intrinsic and extrinsic evaluation: language modeling in all languages, downstream evaluation on cross-lingual NLU datasets and benchmarks in 33 languages, and world knowledge probing in 23 languages. The in-context learning abilities are on par with the contemporaneous language models while covering a larger number of languages, including underrepresented and low-resource languages of the Commonwealth of Independent States and the indigenous peoples in Russia. The source code and the language models are publicly available under the MIT license.;1;;;;;;;;;;;;1;tacl2024;January 2024
Large Language Models of Code Fail at Completing Code with Potential Bugs;"Large language models of code (Code-LLMs) have recently brought tremendous advances to code completion, a fundamental feature of programming assistance and code intelligence. However, most existing works ignore the possible presence of bugs in the code context for generation, which are inevitable in software development. Therefore, we introduce and study the buggy-code completion problem, inspired by the realistic scenario of real-time code suggestion where the code context contains potential bugs -- anti-patterns that can become bugs in the completed program. To systematically study the task, we introduce two datasets: one with synthetic bugs derived from semantics-altering operator changes (buggy-HumanEval) and one with realistic bugs derived from user submissions to coding problems (buggy-FixEval). We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs. For instance, the passing rates of CODEGEN-2B-MONO on test cases of buggy-HumanEval drop more than 50% given a single potential bug in the context. Finally, we investigate several post-hoc methods for mitigating the adverse effect of potential bugs and find that there remains a significant gap in post-mitigation performance."",";4;investigates code completion of LLMs when there are bugs in the code and identifies big limitations. 4-5;"""most existing works ignore the possible presence of bugs in the code context for generation, which are inevitable in software development."", ""We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs."", ""drop more than 50% given a single potential bug in the context."", ""that there remains a significant gap in post-mitigation performance""";;;;;;;;;;4;arxiv;06 June 2023
Cultural Adaptation of Recipes;Building upon the considerable advances in Large Language Models (LLMs), we are now equipped to address more sophisticated tasks demanding a nuanced understanding of cross-cultural contexts. A key example is recipe adaptation, which goes beyond simple translation to include a grasp of ingredients, culinary techniques, and dietary preferences specific to a given culture. We introduce a new task involving the translation and cultural adaptation of recipes between Chinese- and English-speaking cuisines. To support this investigation, we present CulturalRecipes, a unique dataset composed of automatically paired recipes written in Mandarin Chinese and English. This dataset is further enriched with a human-written and curated test set. In this intricate task of cross-cultural recipe adaptation, we evaluate the performance of various methods, including GPT-4 and other LLMs, traditional machine translation, and information retrieval techniques. Our comprehensive analysis includes both automatic and human evaluation metrics. While GPT-4 exhibits impressive abilities in adapting Chinese recipes into English, it still lags behind human expertise when translating English recipes into Chinese. This underscores the multifaceted nature of cultural adaptations. We anticipate that these insights will significantly contribute to future research on culturally aware language models and their practical application in culturally diverse contexts.;3;deals with recipe adaptation, mentions strengths and weaknesses, bias of model;"""it still lags behind human expertise when translating English recipes into Chinese.""";;;;;;;;;;3;tacl2024;January 2024
Metric-Free Learning Network with Dual Relations Propagation for Few-Shot Aspect Category Sentiment Analysis;Few-shot Aspect Category Sentiment Analysis (ACSA) is a crucial task for aspect-based sentiment analysis, which aims to detect sentiment polarity for a given aspect category in a sentence with limited data. However, few-shot learning methods focus on distance metrics between the query and support sets to classify queries, heavily relying on aspect distributions in the embedding space. Thus, they suffer from overlapping distributions of aspect embeddings caused by irrelevant sentiment noise among sentences with multiple sentiment aspects, leading to misclassifications. To solve the above issues, we propose a metric-free method for few-shot ACSA, which models the associated relations among the aspects of support and query sentences by Dual Relations Propagation (DRP), addressing the passive effect of overlapping distributions. Specifically, DRP uses the dual relations (similarity and diversity) among the aspects of support and query sentences to explore intra-cluster commonality and inter-cluster uniqueness for alleviating sentiment noise and enhancing aspect features. Additionally, the dual relations are transformed from support-query to class-query to promote query inference by learning class knowledge. Experiments show that we achieve convincing performance on few-shot ACSA, especially an average improvement of 2.93% accuracy and 2.10% F1 score in the 3-way 1-shot setting.;2;not explicitly mentioning LLMs;"""Thus, they suffer from overlapping distributions of aspect embeddings caused by irrelevant sentiment noise among sentences with multiple sentiment aspects, leading to misclassifications.""";;;;;;;;;;2;tacl2024;January 2024
Addressing the Binning Problem in Calibration Assessment through Scalar Annotations;Computational linguistics models commonly target the prediction of discrete—categorical—labels. When assessing how well-calibrated these model predictions are, popular evaluation schemes require practitioners to manually determine a binning scheme: grouping labels into bins to approximate true label posterior. The problem is that these metrics are sensitive to binning decisions. We consider two solutions to the binning problem that apply at the stage of data annotation: collecting either distributed (redundant) labels or direct scalar value assignment. In this paper, we show that although both approaches address the binning problem by evaluating instance-level calibration, direct scalar assignment is significantly more cost-effective. We provide theoretical analysis and empirical evidence to support our proposal for dataset creators to adopt scalar annotation protocols to enable a higher-quality assessment of model calibration.;0;not explicitly mentioning LLMs;;;;;;;;;;;0;tacl2024;January 2024
An Energy-based Model for Word-level AutoCompletion in Computer-aided Translation;Word-level AutoCompletion (WLAC) is a rewarding yet challenging task in Computer-aided Translation. Existing work addresses this task through a classification model based on a neural network that maps the hidden vector of the input context into its corresponding label (i.e., the candidate target word is treated as a label). Since the context hidden vector itself does not take the label into account and it is projected to the label through a linear classifier, the model cannot sufficiently leverage valuable information from the source sentence as verified in our experiments, which eventually hinders its overall performance. To alleviate this issue, this work proposes an energy-based model for WLAC, which enables the context hidden vector to capture crucial information from the source sentence. Unfortunately, training and inference suffer from efficiency and effectiveness challenges, therefore we employ three simple yet effective strategies to put our model into practice. Experiments on four standard benchmarks demonstrate that our reranking-based approach achieves substantial improvements (about 6.07%) over the previous state-of-the-art model. Further analyses show that each strategy of our approach contributes to the final performance.;0;not dealing with LLMs, but other type of language model;;;;;;;;;;;0;tacl2024;January 2024
Lost in the Middle: How Language Models Use Long Contexts;While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.;4;deals with problems of context size in LLMs;"""relatively little is known about how well they use longer context."", ""We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts."", ""and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models.""";;;;;;;;;;4;tacl2024;January 2024
Red Teaming Language Model Detectors with Language Models;"The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users. To prevent the potentially deceptive usage of LLMs, recent work has proposed algorithms to detect LLM-generated text and protect LLMs. In this paper, we investigate the robustness and reliability of these LLM detectors under adversarial attacks. We study two types of attack strategies: 1) replacing certain words in an LLM’s output with their synonyms given the context; 2) automatically searching for an instructional prompt to alter the writing style of the generation. In both strategies, we leverage an auxiliary LLM to generate the word replacements or the instructional prompt. Different from previous works, we consider a challenging setting where the auxiliary LLM can also be protected by a detector. Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems. Code is available at https://github.com/shizhouxing/LLM-Detector-Robustness.";4;deals with problem of detecting LLM content, not a limitation of the LLM itself, but of the detectors. 3-4;"""(LLMs) present significant safety and ethical risks if exploited by malicious users."", ""Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems""";;;;;;;;;;4;tacl2024;January 2024
Text Attribute Control via Closed-Loop Disentanglement;Changing an attribute of a text without changing the content usually requires first disentangling the text into irrelevant attributes and content representations. After that, in the inference phase, the representation of one attribute is tuned to a different value, expecting that the corresponding attribute of the text can also be changed accordingly. The usual way of disentanglement is to add some constraints on the latent space of an encoder-decoder architecture, including adversarial-based constraints and mutual-information-based constraints. However, previous semi-supervised processes of attribute change are usually not enough to guarantee the success of attribute change and content preservation. In this paper, we propose a novel approach to achieve a robust control of attributes while enhancing content preservation. In this approach, we use a semi-supervised contrastive learning method to encourage the disentanglement of attributes in latent spaces. Differently from previous works, we re-disentangle the reconstructed sentence and compare the re-disentangled latent space with the original latent space, which makes a closed-loop disentanglement process. This also helps content preservation. In addition, the contrastive learning method is also able to replace the role of minimizing mutual information and adversarial training in the disentanglement process, which alleviates the computation cost. We conducted experiments on three text datasets, including the Yelp Service review dataset, the Amazon Product review dataset, and the GoEmotions dataset. The experimental results show the effectiveness of our model.;0;does not explicitly mention LLMs, only encoder-decoder architecture;;;;;;;;;;;0;tacl2024;January 2024
Unifying Structured Data as Graph for Data-to-Text Pre-Training;Data-to-text (D2T) generation aims to transform structured data into natural language text. Data-to-text pre-training has proved to be powerful in enhancing D2T generation and yields impressive performance. However, previous pre-training methods either oversimplified structured data into a sequence without considering input structures or designed training objectives tailored for a specific data structure (e.g., table or knowledge graph). In this paper, we unify different types of structured data (i.e., table, key-value data, knowledge graph) into the graph format and cast different D2T generation tasks as graph-to-text generation. To effectively exploit the structural information of the input graph, we propose a structure-enhanced pre-training method for D2T generation by designing a structure-enhanced Transformer. Concretely, we devise a position matrix for the Transformer, encoding relative positional information of connected nodes in the input graph. In addition, we propose a new attention matrix to incorporate graph structures into the original Transformer by taking the available explicit connectivity structure into account. Extensive experiments on six benchmark datasets show the effectiveness of our model. Our source codes are available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/unid2t.;2;mentions Transformer architecture and limitations and how they can be improved;"""However, previous pre-training methods either oversimplified structured data into a sequence without considering input structures or designed training objectives tailored for a specific data structure (e.g., table or knowledge graph)""";;;;;;;;;;2;tacl2024;January 2024
Exploring Human-Like Translation Strategy with Large Language Models;Large language models (LLMs) have demonstrated impressive capabilities in general scenarios, exhibiting a level of aptitude that approaches, in some aspects even surpasses, human-level intelligence. Among their numerous skills, the translation abilities of LLMs have received considerable attention. Compared to typical machine translation that focuses solely on source-to-target mapping, LLM-based translation can potentially mimic the human translation process, which might take preparatory steps to ensure high-quality translation. This work explores this possibility by proposing the MAPS framework, which stands for Multi-Aspect Prompting and Selection. Specifically, we enable LLMs first to analyze the given source sentence and induce three aspects of translation-related knowledge (keywords, topics, and relevant demonstrations) to guide the final translation process. Moreover, we employ a selection mechanism based on quality estimation to filter out noisy and unhelpful knowledge. Both automatic (3 LLMs × 11 directions × 2 automatic metrics) and human evaluation (preference study and MQM) demonstrate the effectiveness of MAPS. Further analysis shows that by mimicking the human translation process, MAPS reduces various translation errors such as hallucination, ambiguity, mistranslation, awkward style, untranslated text, and omission. Source code is available at https://github.com/zwhe99/MAPS-mt.;2;;"""MAPS reduces various translation errors such as hallucination, ambiguity, mistranslation, awkward style, untranslated text, and omission""";;;;;;;;;;2;tacl2024;January 2024
Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering;"An open-domain question answering (QA) system usually follows a retrieve-then-read paradigm, in which a retriever is used to retrieve relevant passages from a large corpus, and then a reader generates answers based on the retrieved passages and the original question. In this paper, we propose a simple and novel mutual learning framework to improve the performance of retrieve-then-read-style models via an intermediate module named the knowledge selector, which we train with reinforcement learning. The key benefits of our proposed intermediate module are: 1) no requirement for additional annotated question-passage pairs; 2) improvements in both retrieval and QA performance, as well as computational efficiency, compared to prior competitive retrieve-then-read models; 3) with no finetuning, improvement in the zero-shot performance of large-scale pre-trained language models, e.g., ChatGPT, by encapsulating the input with relevant knowledge without violating the input length constraint.";2;proposes new approach and mentions limitations of previous approaches;"""compared to prior competitive retrieve-then-read models""";;;;;;;;;;2;tacl2024;January 2024
Evaluating the Ripple Effects of Knowledge Editing in Language Models;Modern language models capture a large body of factual knowledge. However, some facts can be incorrectly induced or become obsolete over time, resulting in factually incorrect generations. This has led to the development of various editing methods that allow updating facts encoded by the model. Evaluation of these methods has primarily focused on testing whether an individual fact has been successfully injected, and if similar predictions for other subjects have not changed. Here we argue that such evaluation is limited, since injecting one fact (e.g., “Jack Depp is the son of Johnny Depp”) introduces a “ripple effect” in the form of additional facts that the model needs to update (e.g., “Jack Depp is the sibling of Lily-Rose Depp”). To address this, we propose novel evaluation criteria that consider the implications of an edit on related facts. Using these criteria, we then construct RippleEdits, a diagnostic benchmark of 5K factual edits, capturing various types of ripple effects. We evaluate prominent editing methods on RippleEdits, showing that they fail to introduce consistent changes in the model’s knowledge. In addition, we find that a simple in-context editing baseline obtains the best scores on our benchmark, suggesting a promising research direction for model editing.;3;;"""some facts can be incorrectly induced or become obsolete over time, resulting in factually incorrect generations"", ""showing that they fail to introduce consistent changes in the model’s knowledge""";;;;;;;;;;3;tacl2024;January 2024
The Impact of Word Splitting on the Semantic Content of Contextualized Word Representations;When deriving contextualized word representations from language models, a decision needs to be made on how to obtain one for out-of-vocabulary (OOV) words that are segmented into subwords. What is the best way to represent these words with a single vector, and are these representations of worse quality than those of in-vocabulary words? We carry out an intrinsic evaluation of embeddings from different models on semantic similarity tasks involving OOV words. Our analysis reveals, among other interesting findings, that the quality of representations of words that are split is often, but not always, worse than that of the embeddings of known words. Their similarity values, however, must be interpreted with caution.;3;mentions strenghts and weaknesses;"""that the quality of representations of words that are split is often, but not always, worse than that of the embeddings of known words. Their similarity values, however, must be interpreted with caution.""";;;;;;;;;;3;tacl2024;January 2024
Large Language Models Enable Few-Shot Clustering;Unlike traditional unsupervised clustering, semi-supervised clustering allows users to provide meaningful structure to the data, which helps the clustering algorithm to match the user’s intent. Existing approaches to semi-supervised clustering require a significant amount of feedback from an expert to improve the clusters. In this paper, we ask whether a large language model (LLM) can amplify an expert’s guidance to enable query-efficient, few-shot semi-supervised text clustering. We show that LLMs are surprisingly effective at improving clustering. We explore three stages where LLMs can be incorporated into clustering: before clustering (improving input features), during clustering (by providing constraints to the clusterer), and after clustering (using LLMs post-correction). We find that incorporating LLMs in the first two stages routinely provides significant improvements in cluster quality, and that LLMs enable a user to make trade-offs between cost and accuracy to produce desired clusters. We release our code and LLM prompts for the public to use.;1;;;;;;;;;;;;1;tacl2024;January 2024
JustiLM: Few-shot Justification Generation for Explainable Fact-Checking of Real-world Claims;Justification is an explanation that supports the veracity assigned to a claim in fact-checking. However, the task of justification generation has been previously oversimplified as summarization of a fact-check article authored by fact-checkers. Therefore, we propose a realistic approach to generate justification based on retrieved evidence. We present a new benchmark dataset called ExClaim (for Explainable fact-checking of real-world Claims), and introduce JustiLM, a novel few-shot Justification generation based on retrieval-augmented Language Model by using fact-check articles as an auxiliary resource during training only. Experiments show that JustiLM achieves promising performance in justification generation compared to strong baselines, and can also enhance veracity classification with a straightforward extension.;2;;"""the task of justification generation has been previously oversimplified as summarization of a fact-check article authored by fact-checkers.""";;;;;;;;;;2;tacl2024;January 2024
To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation;We conduct a large-scale fine-grained comparative analysis of machine translations (MTs) against human translations (HTs) through the lens of morphosyntactic divergence. Across three language pairs and two types of divergence defined as the structural difference between the source and the target, MT is consistently more conservative than HT, with less morphosyntactic diversity, more convergent patterns, and more one-to-one alignments. Through analysis on different decoding algorithms, we attribute this discrepancy to the use of beam search that biases MT towards more convergent patterns. This bias is most amplified when the convergent pattern appears around 50% of the time in training data. Lastly, we show that for a majority of morphosyntactic divergences, their presence in HT is correlated with decreased MT performance, presenting a greater challenge for MT systems.;0;;;;;;;;;;;;0;tacl2024;January 2024
What Do Self-Supervised Speech Models Know About Words?;Many self-supervised speech models (S3Ms) have been introduced over the last few years, improving performance and data efficiency on various speech tasks. However, these empirical successes alone do not give a complete picture of what is learned during pre-training. Recent work has begun analyzing how S3Ms encode certain properties, such as phonetic and speaker information, but we still lack a proper understanding of knowledge encoded at the word level and beyond. In this work, we use lightweight analysis methods to study segment-level linguistic properties—word identity, boundaries, pronunciation, syntactic features, and semantic features—encoded in S3Ms. We present a comparative study of layer-wise representations from ten S3Ms and find that (i) the frame-level representations within each word segment are not all equally informative, and (ii) the pre-training objective and model size heavily influence the accessibility and distribution of linguistic information across layers. We also find that on several tasks—word discrimination, word segmentation, and semantic sentence similarity—S3Ms trained with visual grounding outperform their speech-only counterparts. Finally, our task-based analyses demonstrate improved performance on word segmentation and acoustic word discrimination while using simpler methods than prior work.;0;does not deal with LLMs, but S3Ms;;;;;;;;;;;0;tacl2024;January 2024
Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation;Pretrained character-level and byte-level language models have been shown to be competitive with popular subword models across a range of Natural Language Processing tasks. However, there has been little research on their effectiveness for neural machine translation (NMT), particularly within the popular pretrain-then-finetune paradigm. This work performs an extensive comparison across multiple languages and experimental conditions of character- and subword-level pretrained models (ByT5 and mT5, respectively) on NMT. We show the effectiveness of character-level modeling in translation, particularly in cases where fine-tuning data is limited. In our analysis, we show how character models’ gains in translation quality are reflected in better translations of orthographically similar words and rare words. While evaluating the importance of source texts in driving model predictions, we highlight word-level patterns within ByT5, suggesting an ability to modulate word-level and character-level information during generation. We conclude by assessing the efficiency tradeoff of byte models, suggesting their usage in non-time-critical scenarios to boost translation quality.;2;;"""We conclude by assessing the efficiency tradeoff of byte models,""";;;;;;;;;;2;tacl2024;January 2024
Geographic Adaptation of Pretrained Language Models;While pretrained language models (PLMs) have been shown to possess a plethora of linguistic knowledge, the existing body of research has largely neglected extralinguistic knowledge, which is generally difficult to obtain by pretraining on text alone. Here, we contribute to closing this gap by examining geolinguistic knowledge, i.e., knowledge about geographic variation in language. We introduce geoadaptation, an intermediate training step that couples language modeling with geolocation prediction in a multi-task learning setup. We geoadapt four PLMs, covering language groups from three geographic areas, and evaluate them on five different tasks: fine-tuned (i.e., supervised) geolocation prediction, zero-shot (i.e., unsupervised) geolocation prediction, fine-tuned language identification, zero-shot language identification, and zero-shot prediction of dialect features. Geoadaptation is very successful at injecting geolinguistic knowledge into the PLMs: The geoadapted PLMs consistently outperform PLMs adapted using only language modeling (by especially wide margins on zero-shot prediction tasks), and we obtain new state-of-the-art results on two benchmarks for geolocation prediction and language identification. Furthermore, we show that the effectiveness of geoadaptation stems from its ability to geographically retrofit the representation space of the PLMs.;2;;"""the existing body of research has largely neglected extralinguistic knowledge, which is generally difficult to obtain by pretraining on text alone.""";;;;;;;;;;2;tacl2024;January 2024
Do Text Simplification Systems Preserve Meaning? A Human Evaluation via Reading Comprehension;Automatic text simplification (TS) aims to automate the process of rewriting text to make it easier for people to read. A pre-requisite for TS to be useful is that it should convey information that is consistent with the meaning of the original text. However, current TS evaluation protocols assess system outputs for simplicity and meaning preservation without regard for the document context in which output sentences occur and for how people understand them. In this work, we introduce a human evaluation framework to assess whether simplified texts preserve meaning using reading comprehension questions. With this framework, we conduct a thorough human evaluation of texts by humans and by nine automatic systems. Supervised systems that leverage pre-training knowledge achieve the highest scores on the reading comprehension tasks among the automatic controllable TS systems. However, even the best-performing supervised system struggles with at least 14% of the questions, marking them as “unanswerable” based on simplified content. We further investigate how existing TS evaluation metrics and automatic question-answering systems approximate the human judgments we obtained.;3;mentions limitations of language models on simplified texts, does not explicitly mention LLMs though;"""However, even the best-performing supervised system struggles with at least 14% of the questions, marking them as “unanswerable” based on simplified content.""";;;;;;;;;;3;tacl2024;January 2024
Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap;We present Text-to-OverpassQL, a task designed to facilitate a natural language interface for querying geodata from OpenStreetMap (OSM). The Overpass Query Language (OverpassQL) allows users to formulate complex database queries and is widely adopted in the OSM ecosystem. Generating Overpass queries from natural language input serves multiple use-cases. It enables novice users to utilize OverpassQL without prior knowledge, assists experienced users with crafting advanced queries, and enables tool-augmented large language models to access information stored in the OSM database. In order to assess the performance of current sequence generation models on this task, we propose OverpassNL, a dataset of 8,352 queries with corresponding natural language inputs. We further introduce task specific evaluation metrics and ground the evaluation of the Text-to-OverpassQL task by executing the queries against the OSM database. We establish strong baselines by finetuning sequence-to-sequence models and adapting large language models with in-context examples. The detailed evaluation reveals strengths and weaknesses of the considered learning strategies, laying the foundations for further research into the Text-to-OverpassQL task.;2;main focus is on proposed method, mentions that there are limitations very briefly;"""The detailed evaluation reveals strengths and weaknesses of the considered learning strategies""";;;;;;;;;;2;tacl2024;January 2024
Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions;Large-scale pretrained language models (LLMs), such as ChatGPT and GPT4, have shown strong abilities in multilingual translation, without being explicitly trained on parallel corpora. It is intriguing how the LLMs obtain their ability to carry out translation instructions for different languages. In this paper, we present a detailed analysis by finetuning a multilingual pretrained language model, XGLM-7.5B, to perform multilingual translation following given instructions. Firstly, we show that multilingual LLMs have stronger translation abilities than previously demonstrated. For a certain language, the translation performance depends on its similarity to English and the amount of data used in the pretraining phase. Secondly, we find that LLMs’ ability to carry out translation instructions relies on the understanding of translation instructions and the alignment among different languages. With multilingual finetuning with translation instructions, LLMs could learn to perform the translation task well even for those language pairs unseen during the instruction tuning phase.;2;;"""It is intriguing how the LLMs obtain their ability to carry out translation instructions for different languages.""";;;;;;;;;;2;tacl2024;January 2024
Semantics of Multiword Expressions in Transformer-Based Models: A Survey;Multiword expressions (MWEs) are composed of multiple words and exhibit variable degrees of compositionality. As such, their meanings are notoriously difficult to model, and it is unclear to what extent this issue affects transformer architectures. Addressing this gap, we provide the first in-depth survey of MWE processing with transformer models. We overall find that they capture MWE semantics inconsistently, as shown by reliance on surface patterns and memorized information.MWE meaning is also strongly localized, predominantly in early layers of the architecture. Representations benefit from specific linguistic properties, such as lower semantic idiosyncrasy and ambiguity of target expressions. Our findings overall question the ability of transformer models to robustly capture fine-grained semantics. Furthermore, we highlight the need for more directly comparable evaluation setups.;4;mentions big limitations of transformer models;"""We overall find that they capture MWE semantics inconsistently, as shown by reliance on surface patterns and memorized information."", ""MWE meaning is also strongly localized, predominantly in early layers of the architecture."", ""Our findings overall question the ability of transformer models to robustly capture fine-grained semantics.""";;;;;;;;;;4;tacl2024;January 2024
Extracting Social Determinants of Health from Pediatric Patient Notes Using Large Language Models: Novel Corpus and Methods;Social determinants of health (SDoH) play a critical role in shaping health outcomes, particularly in pediatric populations where interventions can have long-term implications. SDoH are frequently studied in the Electronic Health Record (EHR), which provides a rich repository for diverse patient data. In this work, we present a novel annotated corpus, the Pediatric Social History Annotation Corpus (PedSHAC), and evaluate the automatic extraction of detailed SDoH representations using fine-tuned and in-context learning methods with  Large Language Models (LLMs). PedSHAC comprises annotated social history sections from 1,260 clinical notes obtained from pediatric patients within the University of Washington (UW) hospital system. Employing an event-based annotation scheme, PedSHAC captures ten distinct health determinants to encompass living and economic stability, prior trauma, education access, substance use history, and mental health with an overall annotator agreement of 81.9 F1. Our proposed fine-tuning LLM-based extractors achieve high performance at 78.4 F1 for event arguments. In-context learning approaches with GPT-4 demonstrate promise for reliable SDoH extraction with limited annotated examples, with extraction performance at 82.3 F1 for event triggers.;1;;;;;;;;;;;;1;arxiv;31 March 2024
Fairness in Large Language Models: A Taxonomic Survey;Large Language Models (LLMs) have demonstrated remarkable success across various domains. However, despite their promising performance in numerous real-world applications, most of these algorithms lack fairness considerations. Consequently, they may lead to discriminatory outcomes against certain communities, particularly marginalized populations, prompting extensive study in fair LLMs. On the other hand, fairness in LLMs, in contrast to fairness in traditional machine learning, entails exclusive backgrounds, taxonomies, and fulfillment techniques. To this end, this survey presents a comprehensive overview of recent advances in the existing literature concerning fair LLMs. Specifically, a brief introduction to LLMs is provided, followed by an analysis of factors contributing to bias in LLMs. Additionally, the concept of fairness in LLMs is discussed categorically, summarizing metrics for evaluating bias in LLMs and existing algorithms for promoting fairness. Furthermore, resources for evaluating bias in LLMs, including toolkits and datasets, are summarized. Finally, existing research challenges and open questions are discussed.;3;deals with fairness in LLMs, and mentions limitations regarding this topic. 3-4;"""However, despite their promising performance in numerous real-world applications, most of these algorithms lack fairness considerations. Consequently, they may lead to discriminatory outcomes against certain communities, particularly marginalized populations,"", ""analysis of factors contributing to bias in LLMs.""";;;;;;;;;;3;arxiv;31 March 2024
Algorithmic Collusion by Large Language Models;"The rise of algorithmic pricing raises concerns of algorithmic collusion. We conduct experiments with algorithmic pricing agents based on Large Language Models (LLMs), and specifically GPT-4. We find that (1) LLM-based agents are adept at pricing tasks, (2) LLM-based pricing agents autonomously collude in oligopoly settings to the detriment of consumers, and (3) variation in seemingly innocuous phrases in LLM instructions (""prompts"") may increase collusion. These results extend to auction settings. Our findings underscore the need for antitrust regulation regarding algorithmic pricing, and uncover regulatory challenges unique to LLM-based pricing agents.";4;discusses strength and weaknesses of LLM-based agents for algorithmic pricing. Focus is on limitations/harms to consumers;"""LLM-based pricing agents autonomously collude in oligopoly settings to the detriment of consumers,"", ""variation in seemingly innocuous phrases in LLM instructions (""prompts"") may increase collusion"", ""uncover regulatory challenges unique to LLM-based pricing agents.""";;;;;;;;;;4;arxiv;31 March 2024
Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery;Recognizing failures during task execution and implementing recovery procedures is challenging in robotics. Traditional approaches rely on the availability of extensive data or a tight set of constraints, while more recent approaches leverage large language models (LLMs) to verify task steps and replan accordingly. However, these methods often operate offline, necessitating scene resets and incurring in high costs. This paper introduces Recover, a neuro-symbolic framework for online failure identification and recovery. By integrating ontologies, logical rules, and LLM-based planners, Recover exploits symbolic information to enhance the ability of LLMs to generate recovery plans and also to decrease the associated costs. In order to demonstrate the capabilities of our method in a simulated kitchen environment, we introduce OntoThor, an ontology describing the AI2Thor simulator setting. Empirical evaluation shows that OntoThor's logical rules accurately detect all failures in the analyzed tasks, and that Recover considerably outperforms, for both failure detection and recovery, a baseline method reliant solely on LLMs.;2;;"""However, these methods often operate offline, necessitating scene resets and incurring in high costs.""";;;;;;;;;;2;arxiv;31 March 2024
Can Language Models Recognize Convincing Arguments?;The remarkable and ever-increasing capabilities of Large Language Models (LLMs) have raised concerns about their potential misuse for creating personalized, convincing misinformation and propaganda. To gain insights into LLMs' persuasive capabilities without directly engaging in experimentation with humans, we propose studying their performance on the related task of detecting convincing arguments. We extend a dataset by Durmus & Cardie (2018) with debates, votes, and user traits and propose tasks measuring LLMs' ability to (1) distinguish between strong and weak arguments, (2) predict stances based on beliefs and demographic characteristics, and (3) determine the appeal of an argument to an individual based on their traits. We show that LLMs perform on par with humans in these tasks and that combining predictions from different LLMs yields significant performance gains, even surpassing human performance. The data and code released with this paper contribute to the crucial ongoing effort of continuously evaluating and monitoring the rapidly evolving capabilities and potential impact of LLMs.;2;limitation: potential misinformation through LLMs, conclusion: LLMs are very good at detecting convincing arguments. not really a limitation ;"""raised concerns about their potential misuse for creating personalized, convincing misinformation and propaganda.""";;;;;;;;;;2;arxiv;31 March 2024
WavLLM: Towards Robust and Adaptive Speech Large Language Model;The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elementary single tasks, followed by advanced multi-task training on more complex tasks such as combinations of the elementary tasks. To enhance the flexibility and adherence to different tasks and instructions, a prompt-aware LoRA weight adapter is introduced in the second advanced multi-task training stage. We validate the proposed model on universal speech benchmarks including tasks such as ASR, ST, SV, ER, and also apply it to specialized datasets like Gaokao English listening comprehension set for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments demonstrate that the proposed model achieves state-of-the-art performance across a range of speech tasks on the same model size, exhibiting robust generalization capabilities in executing complex tasks using CoT approach. Furthermore, our model successfully completes Gaokao tasks without specialized training. The codes, models, audio, and Gaokao evaluation set can be accessed at \url{aka.ms/wavllm}.;2;;"""effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks.""";;;;;;;;;;2;arxiv;31 March 2024
RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation;Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses. This limitation stems from their reliance on vast pretraining datasets, making them susceptible to errors in unseen scenarios. To tackle these challenges, Retrieval-Augmented Generation (RAG) addresses this by incorporating external, relevant documents into the response generation process, thus leveraging non-parametric knowledge alongside LLMs' in-context learning abilities. However, existing RAG implementations primarily focus on initial input for context retrieval, overlooking the nuances of ambiguous or complex queries that necessitate further clarification or decomposition for accurate responses. To this end, we propose learning to Refine Query for Retrieval Augmented Generation (RQ-RAG) in this paper, endeavoring to enhance the model by equipping it with capabilities for explicit rewriting, decomposition, and disambiguation. Our experimental results indicate that our method, when applied to a 7B Llama2 model, surpasses the previous state-of-the-art (SOTA) by an average of 1.9\% across three single-hop QA datasets, and also demonstrates enhanced performance in handling complex, multi-hop QA datasets. Our code is available at https://github.com/chanchimin/RQ-RAG.;2;;"""but are prone to generating inaccurate or hallucinatory responses.""";;;;;;;;;;2;arxiv;31 March 2024
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs;"Businesses and software platforms are increasingly turning to Large Language Models (LLMs) such as GPT-3.5, GPT-4, GLM-3, and LLaMa-2 for chat assistance with file access or as reasoning agents for customer service. However, current LLM-based customer service models have limited integration with customer profiles and lack the operational capabilities necessary for effective service. Moreover, existing API integrations emphasize diversity over the precision and error avoidance essential in real-world customer service scenarios. To address these issues, we propose an LLM agent named CHOPS (CHat with custOmer Profile in existing System), designed to: (1) efficiently utilize existing databases or systems for accessing user information or interacting with these systems following existing guidelines; (2) provide accurate and reasonable responses or carry out required operations in the system while avoiding harmful operations; and (3) leverage a combination of small and large LLMs to achieve satisfying performance at a reasonable inference cost. We introduce a practical dataset, the CPHOS-dataset, which includes a database, guiding files, and QA pairs collected from CPHOS, an online platform that facilitates the organization of simulated Physics Olympiads for high school teachers and students. We have conducted extensive experiments to validate the performance of our proposed CHOPS architecture using the CPHOS-dataset, with the aim of demonstrating how LLMs can enhance or serve as alternatives to human customer service. Our code and dataset will be open-sourced soon.";1;not really mentioning limitations of LLMs, but how they are integrated/used in some scenarios;;;;;;;;;;;1;arxiv;31 March 2024
NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning;"Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation. When a digit is read or generated by a causal language model it does not know its place value (e.g. thousands vs. hundreds) until the entire number is processed. To address this issue, we propose a simple adjustment to how numbers are represented by including the count of digits before each number. For instance, instead of ""42"", we suggest using ""{2:42}"" as the new format. This approach, which we term NumeroLogic, offers an added advantage in number generation by serving as a Chain of Thought (CoT). By requiring the model to consider the number of digits first, it enhances the reasoning process before generating the actual number. We use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic formatting. We further demonstrate NumeroLogic applicability to general natural language modeling, improving language understanding performance in the MMLU benchmark.";2;;"""Language models struggle with handling numerical data and performing arithmetic operations.""";;;;;;;;;;2;arxiv;30 March 2024
Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange;Large Language Models (LLMs) have demonstrated exceptional capabilities in various natural language tasks, often achieving performances that surpass those of humans. Despite these advancements, the domain of mathematics presents a distinctive challenge, primarily due to its specialized structure and the precision it demands. In this study, we adopted a two-step approach for investigating the proficiency of LLMs in answering mathematical questions. First, we employ the most effective LLMs, as identified by their performance on math question-answer benchmarks, to generate answers to 78 questions from the Math Stack Exchange (MSE). Second, a case analysis is conducted on the LLM that showed the highest performance, focusing on the quality and accuracy of its answers through manual evaluation. We found that GPT-4 performs best (nDCG of 0.48 and P@10 of 0.37) amongst existing LLMs fine-tuned for answering mathematics questions and outperforms the current best approach on ArqMATH3 Task1, considering P@10. Our Case analysis indicates that while the GPT-4 can generate relevant responses in certain instances, it does not consistently answer all questions accurately. This paper explores the current limitations of LLMs in navigating complex mathematical problem-solving. Through case analysis, we shed light on the gaps in LLM capabilities within mathematics, thereby setting the stage for future research and advancements in AI-driven mathematical reasoning. We make our code and findings publicly available for research: \url{https://github.com/gipplab/LLM-Investig-MathStackExchange};3;mentions limitations of LLMs solving math tasks, but also mentions strengths;"""Despite these advancements, the domain of mathematics presents a distinctive challenge, primarily due to its specialized structure and the precision it demands"", ""it does not consistently answer all questions accurately."", ""we shed light on the gaps in LLM capabilities within mathematics,""";;;;;;;;;;3;arxiv;30 March 2024
Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation;In the field of Natural Language Processing (NLP), Named Entity Recognition (NER) is recognized as a critical technology, employed across a wide array of applications. Traditional methodologies for annotating datasets for NER models are challenged by high costs and variations in dataset quality. This research introduces a novel hybrid annotation approach that synergizes human effort with the capabilities of Large Language Models (LLMs). This approach not only aims to ameliorate the noise inherent in manual annotations, such as omissions, thereby enhancing the performance of NER models, but also achieves this in a cost-effective manner. Additionally, by employing a label mixing strategy, it addresses the issue of class imbalance encountered in LLM-based annotations. Through an analysis across multiple datasets, this method has been consistently shown to provide superior performance compared to traditional annotation methods, even under constrained budget conditions. This study illuminates the potential of leveraging LLMs to improve dataset quality, introduces a novel technique to mitigate class imbalances, and demonstrates the feasibility of achieving high-performance NER in a cost-effective way.;1;;;;;;;;;;;;1;arxiv;30 March 2024
ST-LLM: Large Language Models Are Effective Temporal Learners;Large Language Models (LLMs) have showcased impressive capabilities in text comprehension and generation, prompting research efforts towards video LLMs to facilitate human-AI interaction at the video level. However, how to effectively encode and understand videos in video-based dialogue systems remains to be solved. In this paper, we investigate a straightforward yet unexplored question: Can we feed all spatial-temporal tokens into the LLM, thus delegating the task of video sequence modeling to the LLMs? Surprisingly, this simple approach yields significant improvements in video understanding. Based upon this, we propose ST-LLM, an effective video-LLM baseline with Spatial-Temporal sequence modeling inside LLM. Furthermore, to address the overhead and stability issues introduced by uncompressed video tokens within LLMs, we develop a dynamic masking strategy with tailor-made training objectives. For particularly long videos, we have also designed a global-local input module to balance efficiency and effectiveness. Consequently, we harness LLM for proficient spatial-temporal modeling, while upholding efficiency and stability. Extensive experimental results attest to the effectiveness of our method. Through a more concise model and training pipeline, ST-LLM establishes a new state-of-the-art result on VideoChatGPT-Bench and MVBench. Codes have been available at https://github.com/TencentARC/ST-LLM.;2;;"""However, how to effectively encode and understand videos in video-based dialogue systems remains to be solved."", ""overhead and stability issues introduced by uncompressed video tokens within LLMs,""";;;;;;;;;;2;arxiv;30 March 2024
A Survey of using Large Language Models for Generating Infrastructure as Code;Infrastructure as Code (IaC) is a revolutionary approach which has gained significant prominence in the Industry. IaC manages and provisions IT infrastructure using machine-readable code by enabling automation, consistency across the environments, reproducibility, version control, error reduction and enhancement in scalability. However, IaC orchestration is often a painstaking effort which requires specialised skills as well as a lot of manual effort. Automation of IaC is a necessity in the present conditions of the Industry and in this survey, we study the feasibility of applying Large Language Models (LLM) to address this problem. LLMs are large neural network-based models which have demonstrated significant language processing abilities and shown to be capable of following a range of instructions within a broad scope. Recently, they have also been adapted for code understanding and generation tasks successfully, which makes them a promising choice for the automatic generation of IaC configurations. In this survey, we delve into the details of IaC, usage of IaC in different platforms, their challenges, LLMs in terms of code-generation aspects and the importance of LLMs in IaC along with our own experiments. Finally, we conclude by presenting the challenges in this area and highlighting the scope for future research.;1;;;;;;;;;;;;1;arxiv;30 March 2024
Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning;In recent years, Large Language Models (LLMs) have shown remarkable performance in generating human-like text, proving to be a valuable asset across various applications. However, adapting these models to incorporate new, out-of-domain knowledge remains a challenge,particularly for facts and events that occur after the model's knowledge cutoff date. This paper investigates the effectiveness of Supervised Fine-Tuning (SFT) as a method for knowledge injection in LLMs, specifically focusing on the domain of recent sporting events. We compare different dataset generation strategies -- token-based and fact-based scaling -- to create training data that helps the model learn new information. Our experiments on GPT-4 demonstrate that while token-based scaling can lead to improvements in Q&A accuracy, it may not provide uniform coverage of new knowledge. Fact-based scaling, on the other hand, offers a more systematic approach to ensure even coverage across all facts. We present a novel dataset generation process that leads to more effective knowledge ingestion through SFT, and our results show considerable performance improvements in Q&A tasks related to out-of-domain knowledge. This study contributes to the understanding of domain adaptation for LLMs and highlights the potential of SFT in enhancing the factuality of LLM responses in specific knowledge domains.;3;mentions limitation as motivation, and briefly mentions one limitation of new approach;"""adapting these models to incorporate new, out-of-domain knowledge remains a challenge,"", ""particularly for facts and events that occur after the model's knowledge cutoff date."", ""it may not provide uniform coverage of new knowledge.""";;;;;;;;;;3;arxiv;30 March 2024
DataAgent: Evaluating Large Language Models' Ability to Answer Zero-Shot, Natural Language Queries;"Conventional processes for analyzing datasets and extracting meaningful information are often time-consuming and laborious. Previous work has identified manual, repetitive coding and data collection as major obstacles that hinder data scientists from undertaking more nuanced labor and high-level projects. To combat this, we evaluated OpenAI's GPT-3.5 as a ""Language Data Scientist"" (LDS) that can extrapolate key findings, including correlations and basic information, from a given dataset. The model was tested on a diverse set of benchmark datasets to evaluate its performance across multiple standards, including data science code-generation based tasks involving libraries such as NumPy, Pandas, Scikit-Learn, and TensorFlow, and was broadly successful in correctly answering a given data science query related to the benchmark dataset. The LDS used various novel prompt engineering techniques to effectively answer a given question, including Chain-of-Thought reinforcement and SayCan prompt engineering. Our findings demonstrate great potential for leveraging Large Language Models for low-level, zero-shot data analysis.";1;;;;;;;;;;;;1;arxiv;29 March 2024
On-the-fly Definition Augmentation of LLMs for Biomedical NER;Despite their general capabilities, LLMs still struggle on biomedical NER tasks, which are difficult due to the presence of specialized terminology and lack of training data. In this work we set out to improve LLM performance on biomedical NER in limited data settings via a new knowledge augmentation approach which incorporates definitions of relevant concepts on-the-fly. During this process, to provide a test bed for knowledge augmentation, we perform a comprehensive exploration of prompting strategies. Our experiments show that definition augmentation is useful for both open source and closed LLMs. For example, it leads to a relative improvement of 15\% (on average) in GPT-4 performance (F1) across all (six) of our test datasets. We conduct extensive ablations and analyses to demonstrate that our performance improvements stem from adding relevant definitional knowledge. We find that careful prompting strategies also improve LLM performance, allowing them to outperform fine-tuned language models in few-shot settings. To facilitate future research in this direction, we release our code at https://github.com/allenai/beacon.;2;;"""LLMs still struggle on biomedical NER tasks, which are difficult due to the presence of specialized terminology and lack of training data.""";;;;;;;;;;2;naacl2024;June 2024
ITCMA: A Generative Agent Based on a Computational Consciousness Structure;Large Language Models (LLMs) still face challenges in tasks requiring understanding implicit instructions and applying common-sense knowledge. In such scenarios, LLMs may require multiple attempts to achieve human-level performance, potentially leading to inaccurate responses or inferences in practical environments, affecting their long-term consistency and behavior. This paper introduces the Internal Time-Consciousness Machine (ITCM), a computational consciousness structure. We further propose the ITCM-based Agent (ITCMA), which supports behavior generation and reasoning in open-world settings. ITCMA enhances LLMs' ability to understand implicit instructions and apply common-sense knowledge by considering agents' interaction and reasoning with the environment. Evaluations in the Alfworld environment show that trained ITCMA outperforms the state-of-the-art (SOTA) by 9% on the seen set. Even untrained ITCMA achieves a 96% task completion rate on the seen set, 5% higher than SOTA, indicating its superiority over traditional intelligent agents in utility and generalization. In real-world tasks with quadruped robots, the untrained ITCMA achieves an 85% task completion rate, which is close to its performance in the unseen set, demonstrating its comparable utility in real-world settings.;2;;"""(LLMs) still face challenges in tasks requiring understanding implicit instructions and applying common-sense knowledge. In such scenarios, LLMs may require multiple attempts to achieve human-level performance, potentially leading to inaccurate responses or inferences in practical environments, affecting their long-term consistency and behavior.""";;;;;;;;;;2;arxiv;29 March 2024
Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models;"Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge updates, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging when dealing with multi-hop questions since they require LLMs to update and integrate multiple knowledge pieces relevant to the questions. To tackle the problem, we propose the Retrieval-Augmented model Editing (RAE) framework tailored for multi-hop question answering. RAE first retrieves edited facts and then refines the language model through in-context learning. Specifically, our retrieval approach, based on mutual information maximization, leverages the reasoning abilities of LLMs to identify chain facts that na\""ive similarity-based searches might miss. Additionally, our framework incorporates a pruning strategy to eliminate redundant information from the retrieved facts, which enhances the editing accuracy and mitigates the hallucination problem. Our framework is supported by theoretical justification for its fact retrieval efficacy. Finally, comprehensive evaluation across various LLMs validates RAE's ability in providing accurate answers with updated knowledge.";2;;"""but often struggle to integrate real-time knowledge updates, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging when dealing with multi-hop questions since they require LLMs to update and integrate multiple knowledge pieces relevant to the questions."", ""mitigates the hallucination problem.""";;;;;;;;;;2;arxiv;28 March 2024
Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model;Large Language Models (LLMs) have become increasingly popular due to their ability to process and generate natural language. However, as they are trained on massive datasets of text, LLMs can inherit harmful biases and produce outputs that are not aligned with human values. This paper studies two main approaches to LLM alignment: Reinforcement Learning with Human Feedback (RLHF) and contrastive learning-based methods like Direct Preference Optimization (DPO). By analyzing the stability and robustness of RLHF and DPO, we propose MPO (Mixed Preference Optimization), a novel method that mitigates the weaknesses of both approaches. Specifically, we propose a two-stage training procedure: first train DPO on an easy dataset, and then perform RLHF on a difficult set with DPO model being the reference model. Here, the easy and difficult sets are constructed by a well-trained reward model that splits response pairs into those with large gaps of reward (easy), and those with small gaps (difficult). The first stage allows us to obtain a relatively optimal policy (LLM) model quickly, whereas the second stage refines LLM with online RLHF, thus mitigating the distribution shift issue associated with DPO. Experiments are conducted on two public alignment datasets, namely HH-RLHF and TLDR, demonstrating the effectiveness of MPO, both in terms of GPT4 and human evaluation.;2;;"""LLMs can inherit harmful biases and produce outputs that are not aligned with human values.""";;;;;;;;;;2;arxiv;28 March 2024
FACTOID: FACtual enTailment fOr hallucInation Detection;The widespread adoption of Large Language Models (LLMs) has facilitated numerous benefits. However, hallucination is a significant concern. In response, Retrieval Augmented Generation (RAG) has emerged as a highly promising paradigm to improve LLM outputs by grounding them in factual information. RAG relies on textual entailment (TE) or similar methods to check if the text produced by LLMs is supported or contradicted, compared to retrieved documents. This paper argues that conventional TE methods are inadequate for spotting hallucinations in content generated by LLMs. For instance, consider a prompt about the 'USA's stance on the Ukraine war''. The AI-generated text states, ...U.S. President Barack Obama says the U.S. will not put troops in Ukraine...'' However, during the war the U.S. president is Joe Biden which contradicts factual reality. Moreover, current TE systems are unable to accurately annotate the given text and identify the exact portion that is contradicted. To address this, we introduces a new type of TE called ``Factual Entailment (FE).'', aims to detect factual inaccuracies in content generated by LLMs while also highlighting the specific text segment that contradicts reality. We present FACTOID (FACTual enTAILment for hallucInation Detection), a benchmark dataset for FE. We propose a multi-task learning (MTL) framework for FE, incorporating state-of-the-art (SoTA) long text embeddings such as e5-mistral-7b-instruct, along with GPT-3, SpanBERT, and RoFormer. The proposed MTL architecture for FE achieves an avg. 40\% improvement in accuracy on the FACTOID benchmark compared to SoTA TE methods. As FE automatically detects hallucinations, we assessed 15 modern LLMs and ranked them using our proposed Auto Hallucination Vulnerability Index (HVI_auto). This index quantifies and offers a comparative scale to evaluate and rank LLMs according to their hallucinations.;2;;"""However, hallucination is a significant concern.""";;;;;;;;;;2;arxiv;28 March 2024
Dual Instruction Tuning with Large Language Models for Mathematical Reasoning;Recent advancements highlight the success of instruction tuning with large language models (LLMs) utilizing Chain-of-Thought (CoT) data for mathematical reasoning tasks. Despite the fine-tuned LLMs, challenges persist, such as incorrect, missing, and redundant steps in CoT generation leading to inaccuracies in answer predictions. To alleviate this problem, we propose a dual instruction tuning strategy to meticulously model mathematical reasoning from both forward and reverse directions. This involves introducing the Intermediate Reasoning State Prediction task (forward reasoning) and the Instruction Reconstruction task (reverse reasoning) to enhance the LLMs' understanding and execution of instructions. Training instances for these tasks are constructed based on existing mathematical instruction tuning datasets. Subsequently, LLMs undergo multi-task fine-tuning using both existing mathematical instructions and the newly created data. Comprehensive experiments validate the effectiveness and domain generalization of the dual instruction tuning strategy across various mathematical reasoning tasks.;2;;"""challenges persist, such as incorrect, missing, and redundant steps in CoT generation leading to inaccuracies in answer predictions.""";;;;;;;;;;2;arxiv;27 March 2024
Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check;Retrieval-Augmented Generation (RAG) aims to generate more reliable and accurate responses, by augmenting large language models (LLMs) with the external vast and dynamic knowledge. Most previous work focuses on using RAG for single-round question answering, while how to adapt RAG to the complex conversational setting wherein the question is interdependent on the preceding context is not well studied. In this paper, we propose a conversation-level RAG approach, which incorporates fine-grained retrieval augmentation and self-check for conversational question answering (CQA). In particular, our approach consists of three components, namely conversational question refiner, fine-grained retriever and self-check based response generator, which work collaboratively for question understanding and relevant information acquisition in conversational settings. Extensive experiments demonstrate the great advantages of our approach over the state-of-the-art baselines. Moreover, we also release a Chinese CQA dataset with new features including reformulated question, extracted keyword, retrieved paragraphs and their helpfulness, which facilitates further researches in RAG enhanced CQA.;1;;;;;;;;;;;;1;arxiv;27 March 2024
Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization;Recent advancements in Large Language Models (LLMs) have accelerated their usage in various domains. Given the fact that psychiatric interviews are goal-oriented and structured dialogues between the professional interviewer and the interviewee, it is one of the most underexplored areas where LLMs can contribute substantial value. Here, we explore the use of LLMs for enhancing psychiatric interviews, by analyzing counseling data from North Korean defectors with traumatic events and mental health issues. Specifically, we investigate whether LLMs can (1) delineate the part of the conversation that suggests psychiatric symptoms and name the symptoms, and (2) summarize stressors and symptoms, based on the interview dialogue transcript. Here, the transcript data was labeled by mental health experts for training and evaluation of LLMs. Our experimental results show that appropriately prompted LLMs can achieve high performance on both the symptom delineation task and the summarization task. This research contributes to the nascent field of applying LLMs to psychiatric interview and demonstrates their potential effectiveness in aiding mental health practitioners.;1;;;;;;;;;;;;1;arxiv;26 March 2024
PropTest: Automatic Property Testing for Improved Visual Programming;Visual Programming has emerged as an alternative to end-to-end black-box visual reasoning models. This type of methods leverage Large Language Models (LLMs) to decompose a problem and generate the source code for an executable computer program. This strategy has the advantage of offering an interpretable reasoning path and does not require finetuning a model with task-specific data. We propose PropTest, a general strategy that improves visual programming by further using an LLM to generate code that tests for visual properties in an initial round of proposed solutions. Particularly, our method tests for data-type consistency, as well as syntactic and semantic properties in the generated solutions. Our proposed solution outperforms baselines and achieves comparable results to state-of-the-art methods while using smaller and publicly available LLMs (CodeLlama-7B and WizardCoder-15B). This is demonstrated across different benchmarks on visual question answering and referring expression comprehension, showing the efficacy of our approach in enhancing the performance and generalization of visual reasoning tasks. Specifically, PropTest improves ViperGPT by obtaining 48.66% accuracy (+8.3%) on the A-OKVQA benchmark and 52.8% (+3.3%) on the RefCOCO+ benchmark using CodeLlama-7B.;1;;;;;;;;;;;;1;arxiv;25 March 2024
LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification;Following the significant achievements of large language models (LLMs), researchers have employed in-context learning for text classification tasks. However, these studies focused on monolingual, single-turn classification tasks. In this paper, we introduce LARA (Linguistic-Adaptive Retrieval-Augmented Language Models), designed to enhance accuracy in multi-turn classification tasks across six languages, accommodating numerous intents in chatbot interactions. Multi-turn intent classification is notably challenging due to the complexity and evolving nature of conversational contexts. LARA tackles these issues by combining a fine-tuned smaller model with a retrieval-augmented mechanism, integrated within the architecture of LLMs. This integration allows LARA to dynamically utilize past dialogues and relevant intents, thereby improving the understanding of the context. Furthermore, our adaptive retrieval techniques bolster the cross-lingual capabilities of LLMs without extensive retraining and fine-tune. Comprehensive experiments demonstrate that LARA achieves state-of-the-art performance on multi-turn intent classification tasks, enhancing the average accuracy by 3.67% compared to existing methods.;2;;"""without extensive retraining and fine-tune.""";;;;;;;;;;2;arxiv;25 March 2024
CodeS: Natural Language to Code Repository via Multi-Layer Sketch;"The impressive performance of large language models (LLMs) on code-related tasks has shown the potential of fully automated software development. In light of this, we introduce a new software engineering task, namely Natural Language to code Repository (NL2Repo). This task aims to generate an entire code repository from its natural language requirements. To address this task, we propose a simple yet effective framework CodeS, which decomposes NL2Repo into multiple sub-tasks by a multi-layer sketch. Specifically, CodeS includes three modules: RepoSketcher, FileSketcher, and SketchFiller. RepoSketcher first generates a repository's directory structure for given requirements; FileSketcher then generates a file sketch for each file in the generated structure; SketchFiller finally fills in the details for each function in the generated file sketch. To rigorously assess CodeS on the NL2Repo task, we carry out evaluations through both automated benchmarking and manual feedback analysis. For benchmark-based evaluation, we craft a repository-oriented benchmark, SketchEval, and design an evaluation metric, SketchBLEU. For feedback-based evaluation, we develop a VSCode plugin for CodeS and engage 30 participants in conducting empirical studies. Extensive experiments prove the effectiveness and practicality of CodeS on the NL2Repo task.";1;;;;;;;;;;;;1;arxiv;25 March 2024
Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA;Understanding data visualizations like charts and plots requires reasoning about both visual elements and numerics. Although strong in extractive questions, current chart visual question answering (chart VQA) models suffer on complex reasoning questions. In this work, we address the lack of reasoning ability by data augmentation. We leverage Large Language Models (LLMs), which have shown to have strong reasoning ability, as an automatic data annotator that generates question-answer annotations for chart images. The key innovation in our method lies in the Synthesize Step-by-Step strategy: our LLM-based data generator learns to decompose the complex question into step-by-step sub-questions (rationales), which are then used to derive the final answer using external tools, i.e. Python. This step-wise generation procedure is trained on synthetic data generated using a template-based QA generation pipeline. Experimental results highlight the significance of the proposed step-by-step generation. By training with the LLM-augmented data (LAMENDA), we significantly enhance the chart VQA models, achieving the state-of-the-art accuracy on the ChartQA and PlotQA datasets. In particular, our approach improves the accuracy of the previous state-of-the-art approach from 38% to 54% on the human-written questions in the ChartQA dataset, which needs strong reasoning. We hope our work underscores the potential of synthetic data and encourages further exploration of data augmentation using LLMs for reasoning-heavy tasks.;1;;;;;;;;;;;;1;arxiv;25 March 2024
ChatDBG: An AI-Powered Debugging Assistant;"This paper presents ChatDBG, the first AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like ""why is x null?"". To handle these queries, ChatDBG grants the LLM autonomy to take the wheel and drive debugging by issuing commands to navigate through stacks and inspect program state; it then reports its findings and yields back control to the programmer. Our ChatDBG prototype integrates with standard debuggers including LLDB, GDB, and WinDBG for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code including standalone scripts and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root causes, explain bugs, and generate accurate fixes for a wide range of real-world errors. For the Python programs, a single query led to an actionable bug fix 67% of the time; one additional follow-up query increased the success rate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded nearly 30,000 times.";1;;;;;;;;;;;;1;arxiv;25 March 2024
A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science;This paper explores the use of large language models (LLMs) to score and explain short-answer assessments in K-12 science. While existing methods can score more structured math and computer science assessments, they often do not provide explanations for the scores. Our study focuses on employing GPT-4 for automated assessment in middle school Earth Science, combining few-shot and active learning with chain-of-thought reasoning. Using a human-in-the-loop approach, we successfully score and provide meaningful explanations for formative assessment responses. A systematic analysis of our method's pros and cons sheds light on the potential for human-in-the-loop techniques to enhance automated grading for open-ended science assessments.;1;;;;;;;;;;;;1;arxiv;21 March 2024
Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs;How do transformer-based large language models (LLMs) store and retrieve knowledge? We focus on the most basic form of this task -- factual recall, where the model is tasked with explicitly surfacing stored facts in prompts of form `Fact: The Colosseum is in the country of'. We find that the mechanistic story behind factual recall is more complex than previously thought. It comprises several distinct, independent, and qualitatively different mechanisms that additively combine, constructively interfering on the correct attribute. We term this generic phenomena the additive motif: models compute through summing up multiple independent contributions. Each mechanism's contribution may be insufficient alone, but summing results in constructive interfere on the correct answer. In addition, we extend the method of direct logit attribution to attribute an attention head's output to individual source tokens. We use this technique to unpack what we call `mixed heads' -- which are themselves a pair of two separate additive updates from different source tokens.;1;;;;;;;;;;;;1;arxiv;11 February 2024
CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain;Large Language Models (LLMs) have demonstrated significant potential and effectiveness across multiple application domains. To assess the performance of mainstream LLMs in public security tasks, this study aims to construct a specialized evaluation benchmark tailored to the Chinese public security domain--CPSDbench. CPSDbench integrates datasets related to public security collected from real-world scenarios, supporting a comprehensive assessment of LLMs across four key dimensions: text classification, information extraction, question answering, and text generation. Furthermore, this study introduces a set of innovative evaluation metrics designed to more precisely quantify the efficacy of LLMs in executing tasks related to public security. Through the in-depth analysis and evaluation conducted in this research, we not only enhance our understanding of the performance strengths and limitations of existing models in addressing public security issues but also provide references for the future development of more accurate and customized LLM models targeted at applications in this field.;2;proposes new benchmark, very briefly mentions there are limitations;"""limitations of existing models in addressing public security issues""";;;;;;;;;;2;arxiv;11 February 2024
GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks;Large language models (LLMs) like ChatGPT, exhibit powerful zero-shot and instruction-following capabilities, have catalyzed a revolutionary transformation across diverse fields, especially for open-ended tasks. While the idea is less explored in the graph domain, despite the availability of numerous powerful graph models (GMs), they are restricted to tasks in a pre-defined form. Although several methods applying LLMs to graphs have been proposed, they fail to simultaneously handle the pre-defined and open-ended tasks, with LLM as a node feature enhancer or as a standalone predictor. To break this dilemma, we propose to bridge the pretrained GM and LLM by a Translator, named GraphTranslator, aiming to leverage GM to handle the pre-defined tasks effectively and utilize the extended interface of LLMs to offer various open-ended tasks for GM. To train such Translator, we propose a Producer capable of constructing the graph-text alignment data along node information, neighbor information and model information. By translating node representation into tokens, GraphTranslator empowers an LLM to make predictions based on language instructions, providing a unified perspective for both pre-defined and open-ended tasks. Extensive results demonstrate the effectiveness of our proposed GraphTranslator on zero-shot node classification. The graph question answering experiments reveal our GraphTranslator potential across a broad spectrum of open-ended tasks through language instructions. Our code is available at: https://github.com/alibaba/GraphTranslator.;2;;"""they fail to simultaneously handle the pre-defined and open-ended tasks""";;;;;;;;;;2;arxiv;11 February 2024
Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine;We present LARL-RM (Large language model-generated Automaton for Reinforcement Learning with Reward Machine) algorithm in order to encode high-level knowledge into reinforcement learning using automaton to expedite the reinforcement learning. Our method uses Large Language Models (LLM) to obtain high-level domain-specific knowledge using prompt engineering instead of providing the reinforcement learning algorithm directly with the high-level knowledge which requires an expert to encode the automaton. We use chain-of-thought and few-shot methods for prompt engineering and demonstrate that our method works using these approaches. Additionally, LARL-RM allows for fully closed-loop reinforcement learning without the need for an expert to guide and supervise the learning since LARL-RM can use the LLM directly to generate the required high-level knowledge for the task at hand. We also show the theoretical guarantee of our algorithm to converge to an optimal policy. We demonstrate that LARL-RM speeds up the convergence by 30% by implementing our method in two case studies.;1;;;;;;;;;;;;1;arxiv;11 February 2024
Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap;Large Language Models (LLMs) have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence. The interplay between LLMs and Evolutionary Algorithms (EAs), despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems. Meanwhile, EA can provide an optimization framework for LLM's further enhancement under black-box settings, empowering LLM with flexible global search capacities. On the other hand, the abundant domain knowledge inherent in LLMs could enable EA to conduct more intelligent searches. Furthermore, the text processing and generative capabilities of LLMs would aid in deploying EAs across a wide range of tasks. Based on these complementary advantages, this paper provides a thorough review and a forward-looking roadmap, categorizing the reciprocal inspiration into two main avenues: LLM-enhanced EA and EA-enhanced LLM. Some integrated synergy methods are further introduced to exemplify the amalgamation of LLMs and EAs in diverse scenarios, including neural architecture search, code generation, software engineering, and various generation tasks. As the first comprehensive review focused on the EA research in the era of LLMs, this paper provides a foundational stepping stone for understanding the collaborative potential of LLMs and EAs. By meticulous categorization and critical analysis, we contribute to the ongoing discourse on the cross-disciplinary study of these two powerful paradigms. The identified challenges and future directions offer guidance for researchers and practitioners aiming to unlock the full potential of this innovative collaboration in propelling advancements in optimization and artificial intelligence.;1;;;;;;;;;;;;1;arxiv;18 January 2024
Large Language Models Are Neurosymbolic Reasoners;A wide range of real-world applications is characterized by their symbolic nature, necessitating a strong capability for symbolic reasoning. This paper investigates the potential application of Large Language Models (LLMs) as symbolic reasoners. We focus on text-based games, significant benchmarks for agents with natural language capabilities, particularly in symbolic tasks like math, map reading, sorting, and applying common sense in text-based worlds. To facilitate these agents, we propose an LLM agent designed to tackle symbolic challenges and achieve in-game objectives. We begin by initializing the LLM agent and informing it of its role. The agent then receives observations and a set of valid actions from the text-based games, along with a specific symbolic module. With these inputs, the LLM agent chooses an action and interacts with the game environments. Our experimental results demonstrate that our method significantly enhances the capability of LLMs as automated agents for symbolic reasoning, and our LLM agent is effective in text-based games involving symbolic tasks, achieving an average performance of 88% across all tasks.;1;;;;;;;;;;;;1;arxiv;17 January 2024
LLMs for Relational Reasoning: How Far are We?;Large language models (LLMs) have revolutionized many areas (e.g. natural language processing, software engineering, etc.) by achieving state-of-the-art performance on extensive downstream tasks. Aiming to achieve robust and general artificial intelligence, there has been a surge of interest in investigating the reasoning ability of the LLMs. Whereas the textual and numerical reasoning benchmarks adopted by previous works are rather shallow and simple, it is hard to conclude that the LLMs possess strong reasoning ability by merely achieving positive results on these benchmarks. Recent efforts have demonstrated that the LLMs are poor at solving sequential decision-making problems that require common-sense planning by evaluating their performance on the reinforcement learning benchmarks. In this work, we conduct an in-depth assessment of several state-of-the-art LLMs' reasoning ability based on the inductive logic programming (ILP) benchmark, which is broadly recognized as a representative and challenging measurement for evaluating logic program induction/synthesis systems as it requires inducing strict cause-effect logic to achieve robust deduction on independent and identically distributed (IID) and out-of-distribution (OOD) test samples. Our evaluations illustrate that compared with the neural program induction systems which are much smaller in model size, the state-of-the-art LLMs are much poorer in terms of reasoning ability by achieving much lower performance and generalization using either natural language prompting or truth-value matrix prompting.;4;investigates reasoning abilities of LLMs mentions limitations, also technical details;"""it is hard to conclude that the LLMs possess strong reasoning ability by merely achieving positive results on these benchmarks. Recent efforts have demonstrated that the LLMs are poor at solving sequential decision-making problems that require common-sense planning by evaluating their performance on the reinforcement learning benchmarks."", ""the state-of-the-art LLMs are much poorer in terms of reasoning ability by achieving much lower performance and generalization using either natural language prompting or truth-value matrix prompting.""";;;;;;;;;;4;arxiv;17 January 2024
Large Language Models in Plant Biology;Large Language Models (LLMs), such as ChatGPT, have taken the world by storm and have passed certain forms of the Turing test. However, LLMs are not limited to human language and analyze sequential data, such as DNA, protein, and gene expression. The resulting foundation models can be repurposed to identify the complex patterns within the data, resulting in powerful, multi-purpose prediction tools able to explain cellular systems. This review outlines the different types of LLMs and showcases their recent uses in biology. Since LLMs have not yet been embraced by the plant community, we also cover how these models can be deployed for the plant kingdom.;1;;;;;;;;;;;;1;arxiv;05 January 2024
From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models;This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples), an advanced architecture enhancing the integration of Large Language Models (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of the ReAct framework, incorporates a dual-component memory system, mirroring human short-term and long-term memory, to maintain context and continuity in conversations. It entails a comprehensive agent construction scenario, including phases like Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training phase. This approach appears to enhance agent controllability and adaptability in complex, multi-turn dialogues. Our preliminary evaluations in a real estate sales context suggest that RAISE has some advantages over traditional agents, indicating its potential for broader applications. This work contributes to the AI field by providing a robust framework for developing more context-aware and versatile conversational agents.;1;;;;;;;;;;;;1;arxiv;05 January 2024
GeoGalactica: A Scientific Large Language Model in Geoscience;Large language models (LLMs) have achieved huge success for their general knowledge and ability to solve a wide spectrum of tasks in natural language processing (NLP). Due to their impressive abilities, LLMs have shed light on potential inter-discipline applications to foster scientific discoveries of a specific domain by using artificial intelligence (AI for science, AI4S). In the meantime, utilizing NLP techniques in geoscience research and practice is wide and convoluted, contributing from knowledge extraction and document classification to question answering and knowledge discovery. In this work, we take the initial step to leverage LLM for science, through a rather straightforward approach. We try to specialize an LLM into geoscience, by further pre-training the model with a vast amount of texts in geoscience, as well as supervised fine-tuning (SFT) the resulting model with our custom collected instruction tuning dataset. These efforts result in a model GeoGalactica consisting of 30 billion parameters. To our best knowledge, it is the largest language model for the geoscience domain. More specifically, GeoGalactica is from further pre-training of Galactica. We train GeoGalactica over a geoscience-related text corpus containing 65 billion tokens curated from extensive data sources in the big science project Deep-time Digital Earth (DDE), preserving as the largest geoscience-specific text corpus. Then we fine-tune the model with 1 million pairs of instruction-tuning data consisting of questions that demand professional geoscience knowledge to answer. In this technical report, we will illustrate in detail all aspects of GeoGalactica, including data collection, data cleaning, base model selection, pre-training, SFT, and evaluation. We open-source our data curation tools and the checkpoints of GeoGalactica during the first 3/4 of pre-training.;1;;;;;;;;;;;;1;arxiv;31 December 2023
Large Language Models for Generative Information Extraction: A Survey;Information extraction (IE) aims to extract structural knowledge (such as entities, relations, and events) from plain natural language texts. Recently, generative Large Language Models (LLMs) have demonstrated remarkable capabilities in text understanding and generation, allowing for generalization across various domains and tasks. As a result, numerous works have been proposed to harness abilities of LLMs and offer viable solutions for IE tasks based on a generative paradigm. To conduct a comprehensive systematic review and exploration of LLM efforts for IE tasks, in this study, we survey the most recent advancements in this field. We first present an extensive overview by categorizing these works in terms of various IE subtasks and learning paradigms, then we empirically analyze the most advanced methods and discover the emerging trend of IE tasks with LLMs. Based on thorough review conducted, we identify several insights in technique and promising research directions that deserve further exploration in future studies. We maintain a public repository and consistently update related resources at: \url{https://github.com/quqxui/Awesome-LLM4IE-Papers}.;1;;;;;;;;;;;;1;arxiv;29 December 2023
Building Efficient Universal Classifiers with Natural Language Inference;Generative Large Language Models (LLMs) have become the mainstream choice for fewshot and zeroshot learning thanks to the universality of text generation. Many users, however, do not need the broad capabilities of generative LLMs when they only want to automate a classification task. Smaller BERT-like models can also learn universal tasks, which allow them to do any text classification task without requiring fine-tuning (zeroshot classification) or to learn new tasks with only a few examples (fewshot), while being significantly more efficient than generative LLMs. This paper (1) explains how Natural Language Inference (NLI) can be used as a universal classification task that follows similar principles as instruction fine-tuning of generative LLMs, (2) provides a step-by-step guide with reusable Jupyter notebooks for building a universal classifier, and (3) shares the resulting universal classifier that is trained on 33 datasets with 389 diverse classes. Parts of the code we share has been used to train our older zeroshot classifiers that have been downloaded more than 55 million times via the Hugging Face Hub as of December 2023. Our new classifier improves zeroshot performance by 9.4%.;2;;"""Many users, however, do not need the broad capabilities of generative LLMs when they only want to automate a classification task.""";;;;;;;;;;2;arxiv;29 December 2023
Large Language Models for Conducting Advanced Text Analytics Information Systems Research;The exponential growth of digital content has generated massive textual datasets, necessitating advanced analytical approaches. Large Language Models (LLMs) have emerged as tools capable of processing and extracting insights from massive unstructured textual datasets. However, how to leverage LLMs for text-based Information Systems (IS) research is currently unclear. To assist IS research in understanding how to operationalize LLMs, we propose a Text Analytics for Information Systems Research (TAISR) framework. Our proposed framework provides detailed recommendations grounded in IS and LLM literature on how to conduct meaningful text-based IS research. We conducted three case studies in business intelligence using our TAISR framework to demonstrate its application across several IS research contexts. We also outline potential challenges and limitations in adopting LLMs for IS. By offering a systematic approach and evidence of its utility, our TAISR framework contributes to future IS research streams looking to incorporate powerful LLMs for text analytics.;3;mentions limitation as motivation for new approach and mentions that there are limitations in the approach;"""how to leverage LLMs for text-based Information Systems (IS) research is currently unclear."", ""We also outline potential challenges and limitations in adopting LLMs for IS.""";;;;;;;;;;3;arxiv;27 December 2023
LLMs with User-defined Prompts as Generic Data Operators for Reliable Data Processing;Data processing is one of the fundamental steps in machine learning pipelines to ensure data quality. Majority of the applications consider the user-defined function (UDF) design pattern for data processing in databases. Although the UDF design pattern introduces flexibility, reusability and scalability, the increasing demand on machine learning pipelines brings three new challenges to this design pattern -- not low-code, not dependency-free and not knowledge-aware. To address these challenges, we propose a new design pattern that large language models (LLMs) could work as a generic data operator (LLM-GDO) for reliable data cleansing, transformation and modeling with their human-compatible performance. In the LLM-GDO design pattern, user-defined prompts (UDPs) are used to represent the data processing logic rather than implementations with a specific programming language. LLMs can be centrally maintained so users don't have to manage the dependencies at the run-time. Fine-tuning LLMs with domain-specific data could enhance the performance on the domain-specific tasks which makes data processing knowledge-aware. We illustrate these advantages with examples in different data processing tasks. Furthermore, we summarize the challenges and opportunities introduced by LLMs to provide a complete view of this design pattern for more discussions.;2;briefly mentions there are challenges when using LLM for a novel task;"""Furthermore, we summarize the challenges and opportunities introduced by LLMs""";;;;;;;;;;2;arxiv;26 December 2023
Zero-Shot Cross-Lingual Reranking with Large Language Models for Low-Resource Languages;Large language models (LLMs) have shown impressive zero-shot capabilities in various document reranking tasks. Despite their successful implementations, there is still a gap in existing literature on their effectiveness in low-resource languages. To address this gap, we investigate how LLMs function as rerankers in cross-lingual information retrieval (CLIR) systems for African languages. Our implementation covers English and four African languages (Hausa, Somali, Swahili, and Yoruba) and we examine cross-lingual reranking with queries in English and passages in the African languages. Additionally, we analyze and compare the effectiveness of monolingual reranking using both query and document translations. We also evaluate the effectiveness of LLMs when leveraging their own generated translations. To get a grasp of the effectiveness of multiple LLMs, our study focuses on the proprietary models RankGPT-4 and RankGPT-3.5, along with the open-source model, RankZephyr. While reranking remains most effective in English, our results reveal that cross-lingual reranking may be competitive with reranking in African languages depending on the multilingual capability of the LLM.;2;mentions briefly that capabilities are underexplored, does not mention limitations after evaluating the LLM;"""there is still a gap in existing literature on their effectiveness in low-resource languages""";;;;;;;;;;2;acl2024;August 2024
LlaMaVAE: Guiding Large Language Model Generation via Continuous Latent Sentence Spaces;Deep generative neural networks, such as Variational AutoEncoders (VAEs), offer an opportunity to better understand and control language models from the perspective of sentence-level latent spaces. To combine the controllability of VAE latent spaces with the state-of-the-art performance of recent large language models (LLMs), we present in this work LlaMaVAE, which combines expressive encoder and decoder models (sentenceT5 and LlaMA) with a VAE architecture, aiming to provide better text generation control to LLMs. In addition, to conditionally guide the VAE generation, we investigate a new approach based on flow-based invertible neural networks (INNs) named Invertible CVAE. Experimental results reveal that LlaMaVAE can outperform the previous state-of-the-art VAE language model, Optimus, across various tasks, including language modelling, semantic textual similarity and definition modelling. Qualitative analysis on interpolation and traversal experiments also indicates an increased degree of semantic clustering and geometric consistency, which enables better generation control.;2;proposes method to overcome limitation of control/understanding LLMs;"""better understand and control language models"", ""aiming to provide better text generation control to LLMs.""";;;;;;;;;;2;arxiv;20 December 2023
A Comparative Analysis of Large Language Models for Code Documentation Generation;This paper presents a comprehensive comparative analysis of Large Language Models (LLMs) for generation of code documentation. Code documentation is an essential part of the software writing process. The paper evaluates models such as GPT-3.5, GPT-4, Bard, Llama2, and Starchat on various parameters like Accuracy, Completeness, Relevance, Understandability, Readability and Time Taken for different levels of code documentation. Our evaluation employs a checklist-based system to minimize subjectivity, providing a more objective assessment. We find that, barring Starchat, all LLMs consistently outperform the original documentation. Notably, closed-source models GPT-3.5, GPT-4, and Bard exhibit superior performance across various parameters compared to open-source/source-available LLMs, namely LLama 2 and StarChat. Considering the time taken for generation, GPT-4 demonstrated the longest duration, followed by Llama2, Bard, with ChatGPT and Starchat having comparable generation times. Additionally, file level documentation had a considerably worse performance across all parameters (except for time taken) as compared to inline and function level documentation.;3;compares different LLMs, mentions strengths and weaknesses;"""barring Starchat"", ""closed-source models GPT-3.5, GPT-4, and Bard exhibit superior performance across various parameters compared to open-source/source-available LLMs,"", ""file level documentation had a considerably worse performance across all parameters""";;;;;;;;;;3;arxiv;16 December 2023
TigerBot: An Open Multilingual Multitask LLM;We release and introduce the TigerBot family of large language models (LLMs), consisting of base and chat models, sized from 7, 13, 70 and 180 billion parameters. We develop our models embarking from Llama-2 and BLOOM, and push the boundary further in data, training algorithm, infrastructure, and application tools. Our models yield meaningful performance gain over SOTA open-source models, e.g., Llama-2, specifically 6% gain in English and 20% gain in Chinese. TigerBot model family also achieves leading performance in major academic and industrial benchmarks and leaderboards. We believe that TigerBot represents just a snapshot of lightning-fast progression in LLM open-source community. Therefore, we are thrilled to give back by publicly releasing our models and reporting our approach behind, with additional emphases on building SOTA LLMs in a democratized way and making LLMs of use in real-world applications.;1;;;;;;;;;;;;1;arxiv;14 December 2023
Efficiently Programming Large Language Models using SGLang;Large language models (LLMs) are increasingly used for complex tasks requiring multiple chained generation calls, advanced prompting techniques, control flow, and interaction with external environments. However, efficient systems for programming and executing these applications are lacking. To bridge this gap, we introduce SGLang, a Structured Generation Language for LLMs. SGLang is designed for the efficient programming of LLMs and incorporates primitives for common LLM programming patterns. We have implemented SGLang as a domain-specific language embedded in Python, and we developed an interpreter, a compiler, and a high-performance runtime for SGLang. These components work together to enable optimizations such as parallelism, batching, caching, sharing, and other compilation techniques. Additionally, we propose RadixAttention, a novel technique that maintains a Least Recently Used (LRU) cache of the Key-Value (KV) cache for all requests in a radix tree, enabling automatic KV cache reuse across multiple generation calls at runtime. SGLang simplifies the writing of LLM programs and boosts execution efficiency. Our experiments demonstrate that SGLang can speed up common LLM tasks by up to 5x, while reducing code complexity and enhancing control.;2;;"""efficient systems for programming and executing these applications are lacking.""";;;;;;;;;;2;arxiv;12 December 2023
Large Language Models on Graphs: A Comprehensive Survey;Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-paired graphs. We then discuss detailed techniques for utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models. Furthermore, we discuss the real-world applications of such methods and summarize open-source codes and benchmark datasets. Finally, we conclude with potential future research directions in this fast-growing field. The related source can be found at https://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs.;3;investigates usage of LLMs on graph data, mentions there are strengths and weaknesses;"""it is underexplored whether such ability can be generalized to graphs"", ""disadvantages of different schools of models""";;;;;;;;;;3;arxiv;05 December 2023
Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks;We consider the questions of whether or not large language models (LLMs) have beliefs, and, if they do, how we might measure them. First, we evaluate two existing approaches, one due to Azaria and Mitchell (2023) and the other to Burns et al. (2022). We provide empirical results that show that these methods fail to generalize in very basic ways. We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons. Thus, there is still no lie-detector for LLMs. After describing our empirical results we take a step back and consider whether or not we should expect LLMs to have something like beliefs in the first place. We consider some recent arguments aiming to show that LLMs cannot have beliefs. We show that these arguments are misguided. We provide a more productive framing of questions surrounding the status of beliefs in LLMs, and highlight the empirical nature of the problem. We conclude by suggesting some concrete paths for future work.;3;discusses wether LLMs have beliefs does not really offer an answer. 3-4;"""even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons.""";;;;;;;;;;3;arxiv;30 June 2023
SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs;In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.;1;;;;;;;;;;;;1;arxiv;30 June 2023
Meta-Reasoning: Semantics-Symbol Deconstruction for Large Language Models;Neural-symbolic methods have demonstrated efficiency in enhancing the reasoning abilities of large language models (LLMs). However, existing methods mainly rely on syntactically mapping natural languages to complete formal languages like Python and SQL. Those methods require that reasoning tasks be convertible into programs, which cater to the computer execution mindset and deviate from human reasoning habits. To broaden symbolic methods' applicability and adaptability in the real world, we propose the Meta-Reasoning from a linguistic perspective. This method empowers LLMs to deconstruct reasoning-independent semantic information into generic symbolic representations, thereby efficiently capturing more generalized reasoning knowledge. We conduct extensive experiments on more than ten datasets encompassing conventional reasoning tasks like arithmetic, symbolic, and logical reasoning, and the more complex interactive reasoning tasks like theory-of-mind reasoning. Experimental results demonstrate that Meta-Reasoning significantly enhances in-context reasoning accuracy, learning efficiency, out-of-domain generalization, and output stability compared to the Chain-of-Thought technique. Code and data are publicly available at \url{https://github.com/Alsace08/Meta-Reasoning}.;2;;"""However, existing methods mainly rely on syntactically mapping natural languages to complete formal languages like Python and SQL. Those methods require that reasoning tasks be convertible into programs, which cater to the computer execution mindset and deviate from human reasoning habits.""";;;;;;;;;;2;arxiv;30 June 2023
Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting;Ranking documents using Large Language Models (LLMs) by directly feeding the query and candidate documents into the prompt is an interesting and practical problem. However, researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets. We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these challenging ranking formulations. In this paper, we propose to significantly reduce the burden on LLMs by using a new technique called Pairwise Ranking Prompting (PRP). Our results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs. On TREC-DL 2019&2020, PRP based on the Flan-UL2 model with 20B parameters performs favorably with the previous best approach in the literature, which is based on the blackbox commercial GPT-4 that has 50x (estimated) model size, while outperforming other LLM-based solutions, such as InstructGPT which has 175B parameters, by over 10% for all ranking metrics. By using the same prompt template on seven BEIR tasks, PRP outperforms supervised baselines and outperforms the blackbox commercial ChatGPT solution by 4.2% and pointwise LLM-based solutions by more than 10% on average NDCG@10. Furthermore, we propose several variants of PRP to improve efficiency and show that it is possible to achieve competitive results even with linear complexity.;2;;"""researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets."", ""off-the-shelf LLMs do not fully understand these challenging ranking formulations.""";;;;;;;;;;2;arxiv;30 June 2023
Preference Ranking Optimization for Human Alignment;Large language models (LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secure AI systems. Reinforcement learning from human feedback (RLHF) has been employed to achieve this alignment. However, it encompasses two main drawbacks: (1) RLHF exhibits complexity, instability, and sensitivity to hyperparameters in contrast to SFT. (2) Despite massive trial-and-error, multiple sampling is reduced to pair-wise contrast, thus lacking contrasts from a macro perspective. In this paper, we propose Preference Ranking Optimization (PRO) as an efficient SFT algorithm to directly fine-tune LLMs for human alignment. PRO extends the pair-wise contrast to accommodate preference rankings of any length. By iteratively contrasting candidates, PRO instructs the LLM to prioritize the best response while progressively ranking the rest responses. In this manner, PRO effectively transforms human alignment into aligning the probability ranking of n responses generated by LLM with the preference ranking of humans towards these responses. Experiments have shown that PRO outperforms baseline algorithms, achieving comparable results to ChatGPT and human responses through automatic-based, reward-based, GPT-4, and human evaluations.;3;;"""(LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secure AI systems."", ""it encompasses two main drawbacks: (1) RLHF exhibits complexity, instability, and sensitivity to hyperparameters in contrast to SFT. (2) Despite massive trial-and-error, multiple sampling is reduced to pair-wise contrast, thus lacking contrasts from a macro perspective.""";;;;;;;;;;3;arxiv;30 June 2023
Concept-Oriented Deep Learning with Large Language Models;Large Language Models (LLMs) have been successfully used in many natural-language tasks and applications including text generation and AI chatbots. They also are a promising new technology for concept-oriented deep learning (CODL). However, the prerequisite is that LLMs understand concepts and ensure conceptual consistency. We discuss these in this paper, as well as major uses of LLMs for CODL including concept extraction from text, concept graph extraction from text, and concept learning. Human knowledge consists of both symbolic (conceptual) knowledge and embodied (sensory) knowledge. Text-only LLMs, however, can represent only symbolic (conceptual) knowledge. Multimodal LLMs, on the other hand, are capable of representing the full range (conceptual and sensory) of human knowledge. We discuss conceptual understanding in visual-language LLMs, the most important multimodal LLMs, and major uses of them for CODL including concept extraction from image, concept graph extraction from image, and concept learning. While uses of LLMs for CODL are valuable standalone, they are particularly valuable as part of LLM applications such as AI chatbots.;2;briefly mentions limitations of text-only LLMs, main focus is on strengths;"""prerequisite is that LLMs understand concepts and ensure conceptual consistency."", ""Text-only LLMs, however, can represent only symbolic (conceptual) knowledge.""";;;;;;;;;;2;arxiv;29 June 2023
Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision;Generative Large language models (LLMs) have demonstrated remarkable capabilities for a wide range of applications, but reducing ungrounded or erroneous responses remains a major growth area. Unlike task-specific models, there lack an effective method to calibrate the confidence level of LLM responses to indicate potential errors and facilitate human-in-the-loop verification. An important source of calibration stems from expert-stipulated programmatic supervision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every LLM response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align with LLM output as well as other weak supervision sources. The model assigns higher risk scores to more uncertain LLM responses and facilitate error correction. Experiments on standard relation extraction and classification tasks in biomedical and general domains demonstrate that the proposed risk score is highly correlated with the actual LLM error rate. By using a dynamic prompting strategy based on the risk score, we observed significant accuracy improvement for off-the-shelf LLMs, boosting GPT-3.5 results past state-of-the-art (SOTA) weak supervision model and GPT-4 results past SOTA supervised results on challenging evaluation datasets.;2;;"""reducing ungrounded or erroneous responses remains a major growth area."", ""lack an effective method to calibrate the confidence level of LLM responses to indicate potential errors and facilitate human-in-the-loop verification.""";;;;;;;;;;2;arxiv;28 June 2023
Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models;Large language models (LLMs) have demonstrated impressive performance on various downstream tasks without requiring fine-tuning, including ChatGPT, a chat-based model built on top of LLMs such as GPT-3.5 and GPT-4. Despite having a lower training proportion compared to English, these models also exhibit remarkable capabilities in other languages. In this study, we assess the performance of GPT-3.5 and GPT-4 models on seven distinct Arabic NLP tasks: sentiment analysis, translation, transliteration, paraphrasing, part of speech tagging, summarization, and diacritization. Our findings reveal that GPT-4 outperforms GPT-3.5 on five out of the seven tasks. Furthermore, we conduct an extensive analysis of the sentiment analysis task, providing insights into how LLMs achieve exceptional results on a challenging dialectal dataset. Additionally, we introduce a new Python interface https://github.com/ARBML/Taqyim that facilitates the evaluation of these tasks effortlessly.;1;evaluates LLMs on arabic NLP tasks, does not mention any limitation;;;;;;;;;;;1;arxiv;28 June 2023
Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias;"Large language models (LLMs) have been recently leveraged as training data generators for various natural language processing (NLP) tasks. While previous research has explored different approaches to training models using generated data, they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM. Thus, we investigate training data generation with diversely attributed prompts (e.g., specifying attributes like length and style), which have the potential to yield diverse and attributed generated data. Our investigation focuses on datasets with high cardinality and diverse domains, wherein we demonstrate that attributed prompts outperform simple class-conditional prompts in terms of the resulting model's performance. Additionally, we present a comprehensive empirical study on data generation encompassing vital aspects like bias, diversity, and efficiency, and highlight three key observations: firstly, synthetic datasets generated by simple prompts exhibit significant biases, such as regional bias; secondly, attribute diversity plays a pivotal role in enhancing model performance; lastly, attributed prompts achieve the performance of simple class-conditional prompts while utilizing only 5\% of the querying cost of ChatGPT associated with the latter. The data and code are available on \url{https://github.com/yueyu1030/AttrPrompt}.";3;;"""which may limit the diversity of the generated data and inherit systematic biases of LLM."", ""synthetic datasets generated by simple prompts exhibit significant biases, such as regional bias""";;;;;;;;;;3;arxiv;28 June 2023
Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost;State-of-the-art supervised NLP models achieve high accuracy but are also susceptible to failures on inputs from low-data regimes, such as domains that are not represented in training data. As an approximation to collecting ground-truth labels for the specific domain, we study the use of large language models (LLMs) for annotating inputs and improving the generalization of NLP models. Specifically, given a budget for LLM annotations, we present an algorithm for sampling the most informative inputs to annotate and retrain the NLP model. We find that popular active learning strategies such as uncertainty-based sampling do not work well. Instead, we propose a sampling strategy based on the difference in prediction scores between the base model and the finetuned NLP model, utilizing the fact that most NLP models are finetuned from a base model. Experiments with classification (semantic similarity) and ranking (semantic search) tasks show that our sampling strategy leads to significant gains in accuracy for both the training and target domains.;1;;;;;;;;;;;;1;arxiv;27 June 2023
REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction;The ability to detect and analyze failed executions automatically is crucial for an explainable and robust robotic system. Recently, Large Language Models (LLMs) have demonstrated strong reasoning abilities on textual inputs. To leverage the power of LLMs for robot failure explanation, we introduce REFLECT, a framework which queries LLM for failure reasoning based on a hierarchical summary of robot past experiences generated from multisensory observations. The failure explanation can further guide a language-based planner to correct the failure and complete the task. To systematically evaluate the framework, we create the RoboFail dataset with a variety of tasks and failure scenarios. We demonstrate that the LLM-based framework is able to generate informative failure explanations that assist successful correction planning.;1;;;;;;;;;;;;1;arxiv;27 June 2023
Exploring the Robustness of Large Language Models for Solving Programming Problems;Using large language models (LLMs) for source code has recently gained attention. LLMs, such as Transformer-based models like Codex and ChatGPT, have been shown to be highly capable of solving a wide range of programming problems. However, the extent to which LLMs understand problem descriptions and generate programs accordingly or just retrieve source code from the most relevant problem in training data based on superficial cues has not been discovered yet. To explore this research question, we conduct experiments to understand the robustness of several popular LLMs, CodeGen and GPT-3.5 series models, capable of tackling code generation tasks in introductory programming problems. Our experimental results show that CodeGen and Codex are sensitive to the superficial modifications of problem descriptions and significantly impact code generation performance. Furthermore, we observe that Codex relies on variable names, as randomized variables decrease the solved rate significantly. However, the state-of-the-art (SOTA) models, such as InstructGPT and ChatGPT, show higher robustness to superficial modifications and have an outstanding capability for solving programming problems. This highlights the fact that slight modifications to the prompts given to the LLMs can greatly affect code generation performance, and careful formatting of prompts is essential for high-quality code generation, while the SOTA models are becoming more robust to perturbations.;3;only mentions strengths for SOTA models, limitations of other models;"""the extent to which LLMs understand problem descriptions and generate programs accordingly or just retrieve source code from the most relevant problem in training data based on superficial cues has not been discovered yet."", ""CodeGen and Codex are sensitive to the superficial modifications of problem descriptions and significantly impact code generation performance."", ""Codex relies on variable names, as randomized variables decrease the solved rate significantly.""";;;;;;;;;;3;arxiv;26 June 2023
Language models are weak learners;A central notion in practical and theoretical machine learning is that of a $\textit{weak learner}$, classifiers that achieve better-than-random performance (on any given distribution over data), even by a small margin. Such weak learners form the practical basis for canonical machine learning methods such as boosting. In this work, we illustrate that prompt-based large language models can operate effectively as said weak learners. Specifically, we illustrate the use of a large language model (LLM) as a weak learner in a boosting algorithm applied to tabular data. We show that by providing (properly sampled according to the distribution of interest) text descriptions of tabular data samples, LLMs can produce a summary of the samples that serves as a template for classification and achieves the aim of acting as a weak learner on this task. We incorporate these models into a boosting approach, which in some settings can leverage the knowledge within the LLM to outperform traditional tree-based boosting. The model outperforms both few-shot learning and occasionally even more involved fine-tuning procedures, particularly for tasks involving small numbers of data points. The results illustrate the potential for prompt-based LLMs to function not just as few-shot learners themselves, but as components of larger machine learning pipelines.;1;;;;;;;;;;;;1;arxiv;25 June 2023
Teaching Large Language Models to Self-Debug;"Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest level by 9%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.";2;;"""for complex programming tasks, generating the correct solution in one go becomes challenging,""";;;;;;;;;;2;arxiv;11 April 2023
Towards an Understanding and Explanation for Mixed-Initiative Artificial Scientific Text Detection;Large language models (LLMs) have gained popularity in various fields for their exceptional capability of generating human-like text. Their potential misuse has raised social concerns about plagiarism in academic contexts. However, effective artificial scientific text detection is a non-trivial task due to several challenges, including 1) the lack of a clear understanding of the differences between machine-generated and human-written scientific text, 2) the poor generalization performance of existing methods caused by out-of-distribution issues, and 3) the limited support for human-machine collaboration with sufficient interpretability during the detection process. In this paper, we first identify the critical distinctions between machine-generated and human-written scientific text through a quantitative experiment. Then, we propose a mixed-initiative workflow that combines human experts' prior knowledge with machine intelligence, along with a visual analytics prototype to facilitate efficient and trustworthy scientific text detection. Finally, we demonstrate the effectiveness of our approach through two case studies and a controlled user study with proficient researchers. We also provide design implications for interactive artificial text detection tools in high-stakes decision-making scenarios.;2;;"""Their potential misuse has raised social concerns about plagiarism in academic contexts.""";;;;;;;;;;2;arxiv;11 April 2023
On the Possibilities of AI-Generated Text Detection;Our work addresses the critical issue of distinguishing text generated by Large Language Models (LLMs) from human-produced text, a task essential for numerous applications. Despite ongoing debate about the feasibility of such differentiation, we present evidence supporting its consistent achievability, except when human and machine text distributions are indistinguishable across their entire support. Drawing from information theory, we argue that as machine-generated text approximates human-like quality, the sample size needed for detection increases. We establish precise sample complexity bounds for detecting AI-generated text, laying groundwork for future research aimed at developing advanced, multi-sample detectors. Our empirical evaluations across multiple datasets (Xsum, Squad, IMDb, and Kaggle FakeNews) confirm the viability of enhanced detection methods. We test various state-of-the-art text generators, including GPT-2, GPT-3.5-Turbo, Llama, Llama-2-13B-Chat-HF, and Llama-2-70B-Chat-HF, against detectors, including oBERTa-Large/Base-Detector, GPTZero. Our findings align with OpenAI's empirical data related to sequence length, marking the first theoretical substantiation for these observations.;2;;"""critical issue of distinguishing text generated by Large Language Models (LLMs) from human-produced text""";;;;;;;;;;2;arxiv;10 April 2023
Learnings from Data Integration for Augmented Language Models;"One of the limitations of large language models is that they do not have
access to up-to-date, proprietary or personal data. As a result, there are
multiple efforts to extend language models with techniques for accessing
external data. In that sense, LLMs share the vision of data integration systems
whose goal is to provide seamless access to a large collection of heterogeneous
data sources. While the details and the techniques of LLMs differ greatly from
those of data integration, this paper shows that some of the lessons learned
from research on data integration can elucidate the research path we are
conducting today on language models.";2;;"""One of the limitations of large language models is that they do not have
access to up-to-date, proprietary or personal data.""";;;;;;;;;;2;arxiv;10 April 2023
Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT;"In this paper, we aim to develop a large language model (LLM) with the
reasoning ability on complex graph data. Currently, LLMs have achieved very
impressive performance on various natural language learning tasks, extensions
of which have also been applied to study the vision tasks with multi-modal
data. However, when it comes to the graph learning tasks, existing LLMs present
very serious flaws due to their several inherited weaknesses in performing
{multi-step logic reasoning}, {precise mathematical calculation} and
{perception about the spatial and temporal factors}.
  To address such challenges, in this paper, we will investigate the
principles, methodologies and algorithms to empower existing LLMs with graph
reasoning ability, which will have tremendous impacts on the current research
of both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformer
models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer)
framework to teach LLMs themselves with prompts augmented by ChatGPT to use
external graph reasoning API tools. Specifically, we will investigate to teach
Graph-ToolFormer to handle various graph data reasoning tasks in this paper,
including both (1) very basic graph data loading and graph property reasoning
tasks, ranging from simple graph order and size to the graph diameter and
periphery, and (2) more advanced reasoning tasks on real-world graph data, such
as bibliographic networks, protein molecules, sequential recommender systems,
social networks and knowledge graphs.";2;;"""However, when it comes to the graph learning tasks, existing LLMs present
very serious flaws due to their several inherited weaknesses in performing
{multi-step logic reasoning}, {precise mathematical calculation} and
{perception about the spatial and temporal factors}.""";;;;;;;;;;2;arxiv;10 April 2023
Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions;"Large language models (LLMs), such as OpenAI's Codex, have demonstrated their
potential to generate code from natural language descriptions across a wide
range of programming tasks. Several benchmarks have recently emerged to
evaluate the ability of LLMs to generate functionally correct code from natural
language intent with respect to a set of hidden test cases. This has enabled
the research community to identify significant and reproducible advancements in
LLM capabilities. However, there is currently a lack of benchmark datasets for
assessing the ability of LLMs to generate functionally correct code edits based
on natural language descriptions of intended changes. This paper aims to
address this gap by motivating the problem NL2Fix of translating natural
language descriptions of code changes (namely bug fixes described in Issue
reports in repositories) into correct code fixes. To this end, we introduce
Defects4J-NL2Fix, a dataset of 283 Java programs from the popular Defects4J
dataset augmented with high-level descriptions of bug fixes, and empirically
evaluate the performance of several state-of-the-art LLMs for the this task.
Results show that these LLMS together are capable of generating plausible fixes
for 64.6% of the bugs, and the best LLM-based technique can achieve up to
21.20% top-1 and 35.68% top-5 accuracy on this benchmark.";2;;"""there is currently a lack of benchmark datasets for
assessing the ability of LLMs to generate functionally correct code edits based
on natural language descriptions of intended changes.""";;;;;;;;;;2;arxiv;07 April 2023
Revisiting Automated Prompting: Are We Actually Doing Better?;"Current literature demonstrates that Large Language Models (LLMs) are great
few-shot learners, and prompting significantly increases their performance on a
range of downstream tasks in a few-shot learning setting. An attempt to
automate human-led prompting followed, with some progress achieved. In
particular, subsequent work demonstrates automation can outperform fine-tuning
in certain K-shot learning scenarios.
  In this paper, we revisit techniques for automated prompting on six different
downstream tasks and a larger range of K-shot learning settings. We find that
automated prompting does not consistently outperform simple manual prompts. Our
work suggests that, in addition to fine-tuning, manual prompts should be used
as a baseline in this line of research.";2;;"""We find that
automated prompting does not consistently outperform simple manual prompts.""";;;;;;;;;;2;acl2023;July 2023
Instruction Tuning with GPT-4;"Prior work has shown that finetuning large language models (LLMs) using
machine-generated instruction-following data enables such models to achieve
remarkable zero-shot capabilities on new tasks, and no human-written
instructions are needed. In this paper, we present the first attempt to use
GPT-4 to generate instruction-following data for LLM finetuning. Our early
experiments on instruction-tuned LLaMA models show that the 52K English and
Chinese instruction-following data generated by GPT-4 leads to superior
zero-shot performance on new tasks to the instruction-following data generated
by previous state-of-the-art models. We also collect feedback and comparison
data from GPT-4 to enable a comprehensive evaluation and reward model training.
We make our data generated using GPT-4 as well as our codebase publicly
available.";1;;;;;;;;;;;;1;arxiv;06 April 2023
Whose Text Is It Anyway? Exploring BigCode, Intellectual Property, and Ethics;"Intelligent or generative writing tools rely on large language models that
recognize, summarize, translate, and predict content. This position paper
probes the copyright interests of open data sets used to train large language
models (LLMs). Our paper asks, how do LLMs trained on open data sets circumvent
the copyright interests of the used data? We start by defining software
copyright and tracing its history. We rely on GitHub Copilot as a modern case
study challenging software copyright. Our conclusion outlines obstacles that
generative writing assistants create for copyright, and offers a practical road
map for copyright analysis for developers, software law experts, and general
users to consider in the context of intelligent LLM-powered writing tools.";3;deals with problems due to copyright, offers solution as well;"""Our conclusion outlines obstacles that
generative writing assistants create for copyright,""";;;;;;;;;;3;arxiv;06 April 2023
Document-Level Machine Translation with Large Language Models;"Large language models (LLMs) such as ChatGPT can produce coherent, cohesive,
relevant, and fluent answers for various natural language processing (NLP)
tasks. Taking document-level machine translation (MT) as a testbed, this paper
provides an in-depth evaluation of LLMs' ability on discourse modeling. The
study focuses on three aspects: 1) Effects of Context-Aware Prompts, where we
investigate the impact of different prompts on document-level translation
quality and discourse phenomena; 2) Comparison of Translation Models, where we
compare the translation performance of ChatGPT with commercial MT systems and
advanced document-level MT methods; 3) Analysis of Discourse Modelling
Abilities, where we further probe discourse knowledge encoded in LLMs and shed
light on impacts of training techniques on discourse modeling. By evaluating on
a number of benchmarks, we surprisingly find that LLMs have demonstrated
superior performance and show potential to become a new paradigm for
document-level translation: 1) leveraging their powerful long-text modeling
capabilities, GPT-3.5 and GPT-4 outperform commercial MT systems in terms of
human evaluation; 2) GPT-4 demonstrates a stronger ability for probing
linguistic knowledge than GPT-3.5. This work highlights the challenges and
opportunities of LLMs for MT, which we hope can inspire the future design and
evaluation of LLMs.We release our data and annotations at
https://github.com/longyuewangdcu/Document-MT-LLM.";2;very briefly mentions there are challenges. 1-2;"""This work highlights the challenges""";;;;;;;;;;2;emnlp2023;December 2023
LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models;"The success of large language models (LLMs), like GPT-4 and ChatGPT, has led
to the development of numerous cost-effective and accessible alternatives that
are created by finetuning open-access LLMs with task-specific data (e.g.,
ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning
methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly
one of the most attractive topics, as it only requires fine-tuning a few
external parameters instead of the entire LLMs while achieving comparable or
even better performance. To enable further research on PEFT methods of LLMs,
this paper presents LLM-Adapters, an easy-to-use framework that integrates
various adapters into LLMs and can execute these adapter-based PEFT methods of
LLMs for different tasks. The framework includes state-of-the-art open-access
LLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such as
Series adapters, Parallel adapter, Prompt-based learning and
Reparametrization-based methods. Moreover, we conduct extensive empirical
studies on the impact of adapter types, placement locations, and
hyper-parameters to the best design for each adapter-based methods. We evaluate
the effectiveness of the adapters on fourteen datasets from two different
reasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results
demonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few
extra trainable parameters yields comparable, and in some cases superior,
performance to powerful LLMs (175B) in zero-shot inference on both reasoning
tasks.";2;;"""instead of the entire LLMs""";;;;;;;;;;2;emnlp2023;December 2023
Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering;Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain Question Answering (ODQA). RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains such as healthcare and news. In this paper, we evaluate the impact of joint training of the retriever and generator components of RAG for the task of domain adaptation in ODQA. We propose RAG-end2end, an extension to RAG that can adapt to a domain-specific knowledge base by updating all components of the external knowledge base during training. In addition, we introduce an auxiliary training signal to inject more domain-specific knowledge. This auxiliary signal forces RAG-end2end to reconstruct a given sentence by accessing the relevant information from the external knowledge base. Our novel contribution is that, unlike RAG, RAG-end2end does joint training of the retriever and generator for the end QA task and domain adaptation. We evaluate our approach with datasets from three domains: COVID-19, News, and Conversations, and achieve significant performance improvements compared to the original RAG model. Our work has been open-sourced through the HuggingFace Transformers library, attesting to our work’s credibility and technical consistency.;1;;;;;;;;;;;;1;tacl2023;January 2023
Assessing the Capacity of Transformer to Abstract Syntactic Representations: A Contrastive Analysis Based on Long-distance Agreement;Many studies have shown that transformers are able to predict subject-verb agreement, demonstrating their ability to uncover an abstract representation of the sentence in an unsupervised way. Recently, Li et al. (2021) found that transformers were also able to predict the object-past participle agreement in French, the modeling of which in formal grammar is fundamentally different from that of subject-verb agreement and relies on a movement and an anaphora resolution. To better understand transformers’ internal working, we propose to contrast how they handle these two kinds of agreement. Using probing and counterfactual analysis methods, our experiments on French agreements show that (i) the agreement task suffers from several confounders that partially question the conclusions drawn so far and (ii) transformers handle subject-verb and object-past participle agreements in a way that is consistent with their modeling in theoretical linguistics.;3;;"""better understand transformers’ internal working,"", ""the agreement task suffers from several confounders that partially question the conclusions drawn so far and""";;;;;;;;;;3;tacl2023;January 2023
On the Role of Negative Precedent in Legal Outcome Prediction;Every legal case sets a precedent by developing the law in one of the following two ways. It either expands its scope, in which case it sets positive precedent, or it narrows it, in which case it sets negative precedent. Legal outcome prediction, the prediction of positive outcome, is an increasingly popular task in AI. In contrast, we turn our focus to negative outcomes here, and introduce a new task of negative outcome prediction. We discover an asymmetry in existing models’ ability to predict positive and negative outcomes. Where the state-of-the-art outcome prediction model we used predicts positive outcomes at 75.06 F1, it predicts negative outcomes at only 10.09 F1, worse than a random baseline. To address this performance gap, we develop two new models inspired by the dynamics of a court process. Our first model significantly improves positive outcome prediction score to 77.15 F1 and our second model more than doubles the negative outcome prediction performance to 24.01 F1. Despite this improvement, shifting focus to negative outcomes reveals that there is still much room for improvement for outcome prediction models. https://github.com/valvoda/Negative-Precedent-in-Legal-Outcome-Prediction;0;no LLM explicitly mentioned;"""We discover an asymmetry in existing models’ ability to predict positive and negative outcomes.""";;;;;;;;;;0;tacl2023;January 2023
Meta-Learning a Cross-lingual Manifold for Semantic Parsing;Localizing a semantic parser to support new languages requires effective cross-lingual generalization. Recent work has found success with machine-translation or zero-shot methods, although these approaches can struggle to model how native speakers ask questions. We consider how to effectively leverage minimal annotated examples in new languages for few-shot cross-lingual semantic parsing. We introduce a first-order meta-learning algorithm to train a semantic parser with maximal sample efficiency during cross-lingual transfer. Our algorithm uses high-resource languages to train the parser and simultaneously optimizes for cross-lingual generalization to lower-resource languages. Results across six languages on ATIS demonstrate that our combination of generalization steps yields accurate semantic parsers sampling ≤10% of source training data in each new language. Our approach also trains a competitive model on Spider using English with generalization to Chinese similarly sampling ≤10% of training data.1;0;;;;;;;;;;;;0;tacl2023;January 2023
OPAL: Ontology-Aware Pretrained Language Model for End-to-End Task-Oriented Dialogue;This paper presents an ontology-aware pretrained language model (OPAL) for end-to-end task-oriented dialogue (TOD). Unlike chit-chat dialogue models, task-oriented dialogue models fulfill at least two task-specific modules: Dialogue state tracker (DST) and response generator (RG). The dialogue state consists of the domain-slot-value triples, which are regarded as the user’s constraints to search the domain-related databases. The large-scale task-oriented dialogue data with the annotated structured dialogue state usually are inaccessible. It prevents the development of the pretrained language model for the task-oriented dialogue. We propose a simple yet effective pretraining method to alleviate this problem, which consists of two pretraining phases. The first phase is to pretrain on large-scale contextual text data, where the structured information of the text is extracted by the information extracting tool. To bridge the gap between the pretraining method and downstream tasks, we design two pretraining tasks: ontology-like triple recovery and next-text generation, which simulates the DST and RG, respectively. The second phase is to fine-tune the pretrained model on the TOD data. The experimental results show that our proposed method achieves an exciting boost and obtains competitive performance even without any TOD data on CamRest676 and MultiWOZ benchmarks.;2;;"""It prevents the development of the pretrained language model for the task-oriented dialogue.""";;;;;;;;;;2;tacl2023;January 2023
Helpful Neighbors: Leveraging Neighbors in Geographic Feature Pronunciation;If one sees the place name Houston Mercer Dog Run in New York, how does one know how to pronounce it? Assuming one knows that Houston in New York is pronounced /ˈhaʊstən/ and not like the Texas city (/ˈhjuːstən/), then one can probably guess that /ˈhaʊstən/ is also used in the name of the dog park. We present a novel architecture that learns to use the pronunciations of neighboring names in order to guess the pronunciation of a given target feature. Applied to Japanese place names, we demonstrate the utility of the model to finding and proposing corrections for errors in Google Maps. To demonstrate the utility of this approach to structurally similar problems, we also report on an application to a totally different task: Cognate reflex prediction in comparative historical linguistics. A version of the code has been open-sourced.1;0;;;;;;;;;;;;0;tacl2023;January 2023
Locally Typical Sampling;"Today’s probabilistic language generators fall short when it comes to producing coherent and fluent text despite the fact that the underlying models perform well under standard metrics (e.g., perplexity). This discrepancy has puzzled the language generation community for the last few years. In this work, we posit that the abstraction of natural language generation as a discrete stochastic process—which allows for an information-theoretic analysis—can provide new insights into the behavior of probabilistic language generators, for example, why high-probability texts can be dull or repetitive. Humans use language as a means of communicating information, aiming to do so in a simultaneously efficient and error-minimizing manner; in fact, psycholinguistics research suggests humans choose each word in a string with this subconscious goal in mind. We formally define the set of strings that meet this criterion: Those for which each word has an information content close to the expected information content, namely, the conditional entropy of our model. We then propose a simple and efficient procedure for enforcing this criterion when generating from probabilistic models, which we call locally typical sampling. Automatic and human evaluations show that, in comparison to nucleus and top-k sampling, locally typical sampling offers competitive performance (in both abstractive summarization and story generation) in terms of quality while consistently reducing degenerate repetitions.";2;LLMs not explicitly mentioned, but likely are the underlying models;"""probabilistic language generators fall short when it comes to producing coherent and fluent text"", ""why high-probability texts can be dull or repetitive.""";;;;;;;;;;2;tacl2023;January 2023
Improving Low-Resource Cross-lingual Parsing with Expected Statistic Regularization;We present Expected Statistic Regulariza tion (ESR), a novel regularization technique that utilizes low-order multi-task structural statistics to shape model distributions for semi- supervised learning on low-resource datasets. We study ESR in the context of cross-lingual transfer for syntactic analysis (POS tagging and labeled dependency parsing) and present several classes of low-order statistic functions that bear on model behavior. Experimentally, we evaluate the proposed statistics with ESR for unsupervised transfer on 5 diverse target languages and show that all statistics, when estimated accurately, yield improvements to both POS and LAS, with the best statistic improving POS by +7.0 and LAS by +8.5 on average. We also present semi-supervised transfer and learning curve experiments that show ESR provides significant gains over strong cross-lingual-transfer-plus-fine-tuning baselines for modest amounts of label data. These results indicate that ESR is a promising and complementary approach to model-transfer approaches for cross-lingual parsing.1;0;;;;;;;;;;;;0;tacl2023;January 2023
Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation;Multilingual task-oriented dialogue (ToD) facilitates access to services and information for many (communities of) speakers. Nevertheless, its potential is not fully realized, as current multilingual ToD datasets—both for modular and end-to-end modeling—suffer from severe limitations. 1) When created from scratch, they are usually small in scale and fail to cover many possible dialogue flows. 2) Translation-based ToD datasets might lack naturalness and cultural specificity in the target language. In this work, to tackle these limitations we propose a novel outline-based annotation process for multilingual ToD datasets, where domain-specific abstract schemata of dialogue are mapped into natural language outlines. These in turn guide the target language annotators in writing dialogues by providing instructions about each turn’s intents and slots. Through this process we annotate a new large-scale dataset for evaluation of multilingual and cross-lingual ToD systems. Our Cross-lingual Outline-based Dialogue dataset (cod) enables natural language understanding, dialogue state tracking, and end-to-end dialogue evaluation in 4 diverse languages: Arabic, Indonesian, Russian, and Kiswahili. Qualitative and quantitative analyses of cod versus an equivalent translation-based dataset demonstrate improvements in data quality, unlocked by the outline-based approach. Finally, we benchmark a series of state-of-the-art systems for cross-lingual ToD, setting reference scores for future work and demonstrating that cod prevents over-inflated performance, typically met with prior translation-based ToD datasets.;0;;;;;;;;;;;;0;tacl2023;January 2023
Modeling Emotion Dynamics in Song Lyrics with State Space Models;Most previous work in music emotion recognition assumes a single or a few song-level labels for the whole song. While it is known that different emotions can vary in intensity within a song, annotated data for this setup is scarce and difficult to obtain. In this work, we propose a method to predict emotion dynamics in song lyrics without song-level supervision. We frame each song as a time series and employ a State Space Model (SSM), combining a sentence-level emotion predictor with an Expectation-Maximization (EM) procedure to generate the full emotion dynamics. Our experiments show that applying our method consistently improves the performance of sentence-level baselines without requiring any annotated songs, making it ideal for limited training data scenarios. Further analysis through case studies shows the benefits of our method while also indicating the limitations and pointing to future directions.;0;;;;;;;;;;;;0;tacl2023;January 2023
FeelingBlue: A Corpus for Understanding the Emotional Connotation of Color in Context;While the link between color and emotion has been widely studied, how context-based changes in color impact the intensity of perceived emotions is not well understood. In this work, we present a new multimodal dataset for exploring the emotional connotation of color as mediated by line, stroke, texture, shape, and language. Our dataset, FeelingBlue, is a collection of 19,788 4-tuples of abstract art ranked by annotators according to their evoked emotions and paired with rationales for those annotations. Using this corpus, we present a baseline for a new task: Justified Affect Transformation. Given an image I, the task is to 1) recolor I to enhance a specified emotion e and 2) provide a textual justification for the change in e. Our model is an ensemble of deep neural networks which takes I, generates an emotionally transformed color palette p conditioned on I, applies p to I, and then justifies the color transformation in text via a visual-linguistic model. Experimental results shed light on the emotional connotation of color in context, demonstrating both the promise of our approach on this challenging task and the considerable potential for future investigations enabled by our corpus.1;0;;;;;;;;;;;;0;tacl2023;January 2023
An Empirical Survey of Data Augmentation for Limited Data Learning in NLP;NLP has achieved great progress in the past decade through the use of neural models and large labeled datasets. The dependence on abundant data prevents NLP models from being applied to low-resource settings or novel tasks where significant time, money, or expertise is required to label massive amounts of textual data. Recently, data augmentation methods have been explored as a means of improving data efficiency in NLP. To date, there has been no systematic empirical overview of data augmentation for NLP in the limited labeled data setting, making it difficult to understand which methods work in which settings. In this paper, we provide an empirical survey of recent progress on data augmentation for NLP in the limited labeled data setting, summarizing the landscape of methods (including token-level augmentations, sentence-level augmentations, adversarial augmentations, and hidden-space augmentations) and carrying out experiments on 11 datasets covering topics/news classification, inference tasks, paraphrasing tasks, and single-sentence tasks. Based on the results, we draw several conclusions to help practitioners choose appropriate augmentations in different settings and discuss the current challenges and future directions for limited data learning in NLP.;3;general limitation of NLP, related to LLMs;"""The dependence on abundant data prevents NLP models from being applied to low-resource settings or novel tasks where significant time, money, or expertise is required to label massive amounts of textual data."", ""discuss the current challenges and future directions for limited data learning in NLP.""";;;;;;;;;;3;tacl2023;January 2023
Coreference Resolution through a seq2seq Transition-Based System;Most recent coreference resolution systems use search algorithms over possible spans to identify mentions and resolve coreference. We instead present a coreference resolution system that uses a text-to-text (seq2seq) paradigm to predict mentions and links jointly. We implement the coreference system as a transition system and use multilingual T5 as an underlying language model. We obtain state-of-the-art accuracy on the CoNLL-2012 datasets with 83.3 F1-score for English (a 2.3 higher F1-score than previous work [Dobrovolskii, 2021]) using only CoNLL data for training, 68.5 F1-score for Arabic (+4.1 higher than previous work), and 74.3 F1-score for Chinese (+5.3). In addition we use the SemEval-2010 data sets for experiments in the zero-shot setting, a few-shot setting, and supervised setting using all available training data. We obtain substantially higher zero-shot F1-scores for 3 out of 4 languages than previous approaches and significantly exceed previous supervised state-of-the-art results for all five tested languages. We provide the code and models as open source.1;1;;;;;;;;;;;;1;tacl2023;January 2023
Transformers for Tabular Data Representation: A Survey of Models and Applications;In the last few years, the natural language processing community has witnessed advances in neural representations of free texts with transformer-based language models (LMs). Given the importance of knowledge available in tabular data, recent research efforts extend LMs by developing neural representations for structured data. In this article, we present a survey that analyzes these efforts. We first abstract the different systems according to a traditional machine learning pipeline in terms of training data, input representation, model training, and supported downstream tasks. For each aspect, we characterize and compare the proposed solutions. Finally, we discuss future work directions.;1;;;;;;;;;;;;1;tacl2023;January 2023
Generative Spoken Dialogue Language Modeling;We introduce dGSLM, the first “textless” model able to generate audio samples of naturalistic spoken dialogues. It uses recent work on unsupervised spoken unit discovery coupled with a dual-tower transformer architecture with cross-attention trained on 2000 hours of two-channel raw conversational audio (Fisher dataset) without any text or labels. We show that our model is able to generate speech, laughter, and other paralinguistic signals in the two channels simultaneously and reproduces more naturalistic and fluid turn taking compared to a text-based cascaded model.1,2;0;no text, but transformer architecture;;;;;;;;;;;0;tacl2023;January 2023
Discontinuous Combinatory Constituency Parsing;We extend a pair of continuous combinator-based constituency parsers (one binary and one multi-branching) into a discontinuous pair. Our parsers iteratively compose constituent vectors from word embeddings without any grammar constraints. Their empirical complexities are subquadratic. Our extension includes 1) a swap action for the orientation-based binary model and 2) biaffine attention for the chunker-based multi-branching model. In tests conducted with the Discontinuous Penn Treebank and TIGER Treebank, we achieved state-of-the-art discontinuous accuracy with a significant speed advantage.;0;;;;;;;;;;;;0;tacl2023;January 2023
Efficient Long-Text Understanding with Short-Text Models;Transformer-based pretrained language models (LMs) are ubiquitous across natural language understanding, but cannot be applied to long sequences such as stories, scientific articles, and long documents due to their quadratic complexity. While a myriad of efficient transformer variants have been proposed, they are typically based on custom implementations that require expensive pretraining from scratch. In this work, we propose SLED: SLiding-Encoder and Decoder, a simple approach for processing long sequences that re-uses and leverages battle-tested short-text pretrained LMs. Specifically, we partition the input into overlapping chunks, encode each with a short-text LM encoder and use the pretrained decoder to fuse information across chunks (fusion-in-decoder). We illustrate through controlled experiments that SLED offers a viable strategy for long text understanding and evaluate our approach on SCROLLS, a benchmark with seven datasets across a wide range of language understanding tasks. We find that SLED is competitive with specialized models that are up to 50x larger and require a dedicated and expensive pretraining step.;2;;"""cannot be applied to long sequences such as stories, scientific articles, and long documents due to their quadratic complexity."", ""typically based on custom implementations that require expensive pretraining from scratch.""";;;;;;;;;;2;tacl2023;January 2023
Hate Speech Classifiers Learn Normative Social Stereotypes;Social stereotypes negatively impact individuals’ judgments about different groups and may have a critical role in understanding language directed toward marginalized groups. Here, we assess the role of social stereotypes in the automated detection of hate speech in the English language by examining the impact of social stereotypes on annotation behaviors, annotated datasets, and hate speech classifiers. Specifically, we first investigate the impact of novice annotators’ stereotypes on their hate-speech-annotation behavior. Then, we examine the effect of normative stereotypes in language on the aggregated annotators’ judgments in a large annotated corpus. Finally, we demonstrate how normative stereotypes embedded in language resources are associated with systematic prediction errors in a hate-speech classifier. The results demonstrate that hate-speech classifiers reflect social stereotypes against marginalized groups, which can perpetuate social inequalities when propagated at scale. This framework, combining social-psychological and computational-linguistic methods, provides insights into sources of bias in hate-speech moderation, informing ongoing debates regarding machine learning fairness.;0;;;;;;;;;;;;0;tacl2023;January 2023
Domain-Specific Word Embeddings with Structure Prediction;Complementary to finding good general word embeddings, an important question for representation learning is to find dynamic word embeddings, for example, across time or domain. Current methods do not offer a way to use or predict information on structure between sub-corpora, time or domain and dynamic embeddings can only be compared after post-alignment. We propose novel word embedding methods that provide general word representations for the whole corpus, domain- specific representations for each sub-corpus, sub-corpus structure, and embedding alignment simultaneously. We present an empirical evaluation on New York Times articles and two English Wikipedia datasets with articles on science and philosophy. Our method, called Word2Vec with Structure Prediction (W2VPred), provides better performance than baselines in terms of the general analogy tests, domain-specific analogy tests, and multiple specific word embedding evaluations as well as structure prediction performance when no structure is given a priori. As a use case in the field of Digital Humanities we demonstrate how to raise novel research questions for high literature from the German Text Archive.;2;deals with limitation related to LLMs (embeddings);"""Current methods do not offer a way to use or predict information on structure between sub-corpora, time or domain and dynamic embeddings can only be compared after post-alignment.""";;;;;;;;;;2;tacl2023;January 2023
Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?;This work presents a linguistic analysis into why larger Transformer-based pre-trained language models with more parameters and lower perplexity nonetheless yield surprisal estimates that are less predictive of human reading times. First, regression analyses show a strictly monotonic, positive log-linear relationship between perplexity and fit to reading times for the more recently released five GPT-Neo variants and eight OPT variants on two separate datasets, replicating earlier results limited to just GPT-2 (Oh et al., 2022). Subsequently, analysis of residual errors reveals a systematic deviation of the larger variants, such as underpredicting reading times of named entities and making compensatory overpredictions for reading times of function words such as modals and conjunctions. These results suggest that the propensity of larger Transformer-based models to ‘memorize’ sequences during training makes their surprisal estimates diverge from humanlike expectations,which warrants caution in using pre-trained language models to study human language processing.;4;;"""surprisal estimates that are less predictive of human reading times"", ""analysis of residual errors reveals a systematic deviation of the larger variants, such as underpredicting reading times of named entities and making compensatory overpredictions for reading times of function words such as modals and conjunctions."", ""propensity of larger Transformer-based models to ‘memorize’ sequences during training makes their surprisal estimates diverge from humanlike expectations,"", ""which warrants caution in using pre-trained language models to study human language processing.""";;;;;;;;;;4;tacl2023;January 2023
On the Robustness of Dialogue History Representation in Conversational Question Answering: A Comprehensive Study and a New Prompt-based Method;Most work on modeling the conversation history in Conversational Question Answering (CQA) reports a single main result on a common CQA benchmark. While existing models show impressive results on CQA leaderboards, it remains unclear whether they are robust to shifts in setting (sometimes to more realistic ones), training data size (e.g., from large to small sets) and domain. In this work, we design and conduct the first large-scale robustness study of history modeling approaches for CQA. We find that high benchmark scores do not necessarily translate to strong robustness, and that various methods can perform extremely differently under different settings. Equipped with the insights from our study, we design a novel prompt-based history modeling approach and demonstrate its strong robustness across various settings. Our approach is inspired by existing methods that highlight historic answers in the passage. However, instead of highlighting by modifying the passage token embeddings, we add textual prompts directly in the passage text. Our approach is simple, easy to plug into practically any model, and highly effective, thus we recommend it as a starting point for future model developers. We also hope that our study and insights will raise awareness to the importance of robustness-focused evaluation, in addition to obtaining high leaderboard scores, leading to better CQA systems.1;0;deals with limitations, but not of LLMs, but evaluation and CQA;;;;;;;;;;;0;tacl2023;January 2023
Bridging the Gap between Synthetic and Natural Questions via Sentence Decomposition for Semantic Parsing;Semantic parsing maps natural language questions into logical forms, which can be executed against a knowledge base for answers. In real-world applications, the performance of a parser is often limited by the lack of training data. To facilitate zero-shot learning, data synthesis has been widely studied to automatically generate paired questions and logical forms. However, data synthesis methods can hardly cover the diverse structures in natural languages, leading to a large gap in sentence structure between synthetic and natural questions. In this paper, we propose a decomposition-based method to unify the sentence structures of questions, which benefits the generalization to natural questions. Experiments demonstrate that our method significantly improves the semantic parser trained on synthetic data (+7.9% on KQA and +8.9% on ComplexWebQuestions in terms of exact match accuracy). Extensive analysis demonstrates that our method can better generalize to natural questions with novel text expressions compared with baselines. Besides semantic parsing, our idea potentially benefits other semantic understanding tasks by mitigating the distracting structure features. To illustrate this, we extend our method to the task of sentence embedding learning, and observe substantial improvements on sentence retrieval (+13.1% for Hit@1).;0;;;;;;;;;;;;0;tacl2023;January 2023
Naturalistic Causal Probing for Morpho-Syntax;Probing has become a go-to methodology for interpreting and analyzing deep neural models in natural language processing. However, there is still a lack of understanding of the limitations and weaknesses of various types of probes. In this work, we suggest a strategy for input-level intervention on naturalistic sentences. Using our approach, we intervene on the morpho-syntactic features of a sentence, while keeping the rest of the sentence unchanged. Such an intervention allows us to causally probe pre-trained models. We apply our naturalistic causal probing framework to analyze the effects of grammatical gender and number on contextualized representations extracted from three pre-trained models in Spanish, the multilingual versions of BERT, RoBERTa, and GPT-2. Our experiments suggest that naturalistic interventions lead to stable estimates of the causal effects of various linguistic properties. Moreover, our experiments demonstrate the importance of naturalistic causal probing when analyzing pre-trained models. https://github.com/rycolab/naturalistic-causal-probing;1;deals with limitation of evaluating LLMs;;;;;;;;;;;1;tacl2023;January 2023
Tracking Brand-Associated Polarity-Bearing Topics in User Reviews;Monitoring online customer reviews is important for business organizations to measure customer satisfaction and better manage their reputations. In this paper, we propose a novel dynamic Brand-Topic Model (dBTM) which is able to automatically detect and track brand-associated sentiment scores and polarity-bearing topics from product reviews organized in temporally ordered time intervals. dBTM models the evolution of the latent brand polarity scores and the topic-word distributions over time by Gaussian state space models. It also incorporates a meta learning strategy to control the update of the topic-word distribution in each time interval in order to ensure smooth topic transitions and better brand score predictions. It has been evaluated on a dataset constructed from MakeupAlley reviews and a hotel review dataset. Experimental results show that dBTM outperforms a number of competitive baselines in brand ranking, achieving a good balance of topic coherence and uniqueness, and extracting well-separated polarity-bearing topics across time intervals.1;0;;;;;;;;;;;;0;tacl2023;January 2023
Dubbing in Practice: A Large Scale Study of Human Localization With Insights for Automatic Dubbing;We investigate how humans perform the task of dubbing video content from one language into another, leveraging a novel corpus of 319.57 hours of video from 54 professionally produced titles. This is the first such large-scale study we are aware of. The results challenge a number of assumptions commonly made in both qualitative literature on human dubbing and machine-learning literature on automatic dubbing, arguing for the importance of vocal naturalness and translation quality over commonly emphasized isometric (character length) and lip-sync constraints, and for a more qualified view of the importance of isochronic (timing) constraints. We also find substantial influence of the source-side audio on human dubs through channels other than the words of the translation, pointing to the need for research on ways to preserve speech characteristics, as well as transfer of semantic properties such as emphasis and emotion, in automatic dubbing systems.;0;;;;;;;;;;;;0;tacl2023;January 2023
Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval;Pre-trained language models have been successful in many knowledge-intensive NLP tasks. However, recent work has shown that models such as BERT are not “structurally ready” to aggregate textual information into a [CLS] vector for dense passage retrieval (DPR). This “lack of readiness” results from the gap between language model pre-training and DPR fine-tuning. Previous solutions call for computationally expensive techniques such as hard negative mining, cross-encoder distillation, and further pre-training to learn a robust DPR model. In this work, we instead propose to fully exploit knowledge in a pre-trained language model for DPR by aggregating the contextualized token embeddings into a dense vector, which we call agg★. By concatenating vectors from the [CLS] token and agg★, our Aggretriever model substantially improves the effectiveness of dense retrieval models on both in-domain and zero-shot evaluations without introducing substantial training overhead. Code is available at https://github.com/castorini/dhr.;2;;"""recent work has shown that models such as BERT are not “structurally ready” to aggregate textual information into a [CLS] vector for dense passage retrieval (DPR)"", ""Previous solutions call for computationally expensive techniques""";;;;;;;;;;2;tacl2023;January 2023
InSCIt: Information-Seeking Conversations with Mixed-Initiative Interactions;In an information-seeking conversation, a user may ask questions that are under-specified or unanswerable. An ideal agent would interact by initiating different response types according to the available knowledge sources. However, most current studies either fail to or artificially incorporate such agent-side initiative. This work presents InSCIt, a dataset for Information-Seeking Conversations with mixed-initiative Interactions. It contains 4.7K user-agent turns from 805 human-human conversations where the agent searches over Wikipedia and either directly answers, asks for clarification, or provides relevant information to address user queries. The data supports two subtasks, evidence passage identification and response generation, as well as a human evaluation protocol to assess model performance. We report results of two systems based on state-of-the-art models of conversational knowledge identification and open-domain question answering. Both systems significantly underperform humans, suggesting ample room for improvement in future studies.1;0;unclear whether LLMs are used or not;;;;;;;;;;;0;tacl2023;January 2023
Sub-Character Tokenization for Chinese Pretrained Language Models;Tokenization is fundamental to pretrained language models (PLMs). Existing tokenization methods for Chinese PLMs typically treat each character as an indivisible token. However, they ignore the unique feature of the Chinese writing system where additional linguistic information exists below the character level, i.e., at the sub-character level. To utilize such information, we propose sub-character (SubChar for short) tokenization. Specifically, we first encode the input text by converting each Chinese character into a short sequence based on its glyph or pronunciation, and then construct the vocabulary based on the encoded text with sub-word segmentation. Experimental results show that SubChar tokenizers have two main advantages over existing tokenizers: 1) They can tokenize inputs into much shorter sequences, thus improving the computational efficiency. 2) Pronunciation-based SubChar tokenizers can encode Chinese homophones into the same transliteration sequences and produce the same tokenization output, hence being robust to homophone typos. At the same time, models trained with SubChar tokenizers perform competitively on downstream tasks. We release our code and models at https://github.com/thunlp/SubCharTokenization to facilitate future work.;2;;"""they ignore the unique feature of the Chinese writing system where additional linguistic information exists below the character level, i.e., at the sub-character level.""";;;;;;;;;;2;tacl2023;January 2023
Erasure of Unaligned Attributes from Neural Representations;We present the Assignment-Maximization Spectral Attribute removaL (AMSAL) algorithm, which erases information from neural representations when the information to be erased is implicit rather than directly being aligned to each input example. Our algorithm works by alternating between two steps. In one, it finds an assignment of the input representations to the information to be erased, and in the other, it creates projections of both the input representations and the information to be erased into a joint latent space. We test our algorithm on an extensive array of datasets, including a Twitter dataset with multiple guarded attributes, the BiasBios dataset, and the BiasBench benchmark. The latter benchmark includes four datasets with various types of protected attributes. Our results demonstrate that bias can often be removed in our setup. We also discuss the limitations of our approach when there is a strong entanglement between the main task and the information to be erased.1;3;tries to find solution for bias in BERT model, mentions there are still limitations;"""bias can often be removed in our setup"", ""We also discuss the limitations of our approach when there is a strong entanglement between the main task and the information to be erased""";;;;;;;;;;3;tacl2023;January 2023
Less is More: Mitigate Spurious Correlations for Open-Domain Dialogue Response Generation Models by Causal Discovery;In this paper, we conduct the first study on spurious correlations for open-domain response generation models based on a corpus CGDialog curated by ourselves. The current models indeed suffer from spurious correlations and have a tendency to generate irrelevant and generic responses. Inspired by causal discovery algorithms, we propose a novel model-agnostic method for training and inference using a conditional independence classifier. The classifier is trained by a constrained self-training method, coined ConSTrain, to overcome data sparsity. The experimental results based on both human and automatic evaluation show that our method significantly outperforms the competitive baselines in terms of relevance, informativeness, and fluency.;2;no sure if model is an LLM;"""current models indeed suffer from spurious correlations and have a tendency to generate irrelevant and generic responses.""";;;;;;;;;;2;tacl2023;January 2023
The Parallelism Tradeoff: Limitations of Log-Precision Transformers;Despite their omnipresence in modern NLP, characterizing the computational power of transformer neural nets remains an interesting open question. We prove that transformers whose arithmetic precision is logarithmic in the number of input tokens (and whose feedforward nets are computable using space linear in their input) can be simulated by constant-depth logspace-uniform threshold circuits. This provides insight on the power of transformers using known results in complexity theory. For example, if L≠P (i.e., not all poly-time problems can be solved using logarithmic space), then transformers cannot even accurately solve linear equalities or check membership in an arbitrary context-free grammar with empty productions. Our result intuitively emerges from the transformer architecture’s high parallelizability. We thus speculatively introduce the idea of a fundamental parallelism tradeoff: any model architecture as parallelizable as the transformer will obey limitations similar to it.Since parallelism is key to training models at massive scale, this suggests a potential inherent weakness of the scaling paradigm.;4;investigates limitations of transformer architecture, regarding the computational power. 3-4;"""characterizing the computational power of transformer neural nets remains an interesting open question."", ""then transformers cannot even accurately solve linear equalities or check membership in an arbitrary context-free grammar with empty productions."", ""any model architecture as parallelizable as the transformer will obey limitations similar to it."", ""Since parallelism is key to training models at massive scale, this suggests a potential inherent weakness of the scaling paradigm.""";;;;;;;;;;4;tacl2023;January 2023
Understanding and Detecting Hallucinations in Neural Machine Translation via Model Introspection;Neural sequence generation models are known to “hallucinate”, by producing outputs that are unrelated to the source text. These hallucinations are potentially harmful, yet it remains unclear in what conditions they arise and how to mitigate their impact. In this work, we first identify internal model symptoms of hallucinations by analyzing the relative token contributions to the generation in contrastive hallucinated vs. non-hallucinated outputs generated via source perturbations. We then show that these symptoms are reliable indicators of natural hallucinations, by using them to design a lightweight hallucination detector which outperforms both model-free baselines and strong classifiers based on quality estimation or large pre-trained models on manually annotated English-Chinese and German-English translation test beds.;3;explains details of hallucination, but main focus is on solution. not sure if considered LLM;"""models are known to “hallucinate”, by producing outputs that are unrelated to the source text. These hallucinations are potentially harmful,""";;;;;;;;;;3;tacl2023;January 2023
Visual Writing Prompts: Character-Grounded Story Generation with Curated Image Sequences;Current work on image-based story generation suffers from the fact that the existing image sequence collections do not have coherent plots behind them. We improve visual story generation by producing a new image-grounded dataset, Visual Writing Prompts (VWP). VWP contains almost 2K selected sequences of movie shots, each including 5-10 images. The image sequences are aligned with a total of 12K stories which were collected via crowdsourcing given the image sequences and a set of grounded characters from the corresponding image sequence. Our new image sequence collection and filtering process has allowed us to obtain stories that are more coherent, diverse, and visually grounded compared to previous work. We also propose a character-based story generation model driven by coherence as a strong baseline. Evaluations show that our generated stories are more coherent, visually grounded, and diverse than stories generated with the current state-of-the-art model. Our code, image features, annotations and collected stories are available at https://vwprompt.github.io/.;2;limitation of training data that reflects in limitation of LLM;"""Current work on image-based story generation suffers from the fact that the existing image sequence collections do not have coherent plots behind them""";;;;;;;;;;2;tacl2023;January 2023
Unleashing the True Potential of Sequence-to-Sequence Models for Sequence Tagging and Structure Parsing;Sequence-to-Sequence (S2S) models have achieved remarkable success on various text generation tasks. However, learning complex structures with S2S models remains challenging as external neural modules and additional lexicons are often supplemented to predict non-textual outputs. We present a systematic study of S2S modeling using contained decoding on four core tasks: part-of-speech tagging, named entity recognition, constituency, and dependency parsing, to develop efficient exploitation methods costing zero extra parameters. In particular, 3 lexically diverse linearization schemas and corresponding constrained decoding methods are designed and evaluated. Experiments show that although more lexicalized schemas yield longer output sequences that require heavier training, their sequences being closer to natural language makes them easier to learn. Moreover, S2S models using our constrained decoding outperform other S2S approaches using external resources. Our best models perform better than or comparably to the state-of-the-art for all 4 tasks, lighting a promise for S2S models to generate non-sequential structures.;0;not sure wether S2S is considered an LLM;;;;;;;;;;;0;tacl2023;January 2023
Questions Are All You Need to Train a Dense Passage Retriever;We introduce ART, a new corpus-level autoencoding approach for training dense retrieval models that does not require any labeled training data. Dense retrieval is a central challenge for open-domain tasks, such as Open QA, where state-of-the-art methods typically require large supervised datasets with custom hard-negative mining and denoising of positive examples. ART, in contrast, only requires access to unpaired inputs and outputs (e.g., questions and potential answer passages). It uses a new passage-retrieval autoencoding scheme, where (1) an input question is used to retrieve a set of evidence passages, and (2) the passages are then used to compute the probability of reconstructing the original question. Training for retrieval based on question reconstruction enables effective unsupervised learning of both passage and question encoders, which can be later incorporated into complete Open QA systems without any further finetuning. Extensive experiments demonstrate that ART obtains state-of-the-art results on multiple QA retrieval benchmarks with only generic initialization from a pre-trained language model, removing the need for labeled data and task-specific losses.1 Our code and model checkpoints are available at: https://github.com/DevSinghSachan/art.;1;;;;;;;;;;;;1;tacl2023;January 2023
Transparency Helps Reveal When Language Models Learn Meaning;Many current NLP systems are built from language models trained to optimize unsupervised objectives on large amounts of raw text. Under what conditions might such a procedure acquire meaning? Our systematic experiments with synthetic data reveal that, with languages where all expressions have context-independent denotations (i.e., languages with strong transparency), both autoregressive and masked language models successfully learn to emulate semantic relations between expressions. However, when denotations are changed to be context-dependent with the language otherwise unmodified, this ability degrades. Turning to natural language, our experiments with a specific phenomenon—referential opacity—add to the growing body of evidence that current language models do not represent natural language semantics well. We show this failure relates to the context-dependent nature of natural language form-meaning mappings.;4;;"""when denotations are changed to be context-dependent with the language otherwise unmodified, this ability degrades."", ""current language models do not represent natural language semantics well. We show this failure relates to the context-dependent nature of natural language form-meaning mappings.""";;;;;;;;;;4;tacl2023;January 2023
Visual Spatial Reasoning;Spatial relations are a basic part of human cognition. However, they are expressed in natural language in a variety of ways, and previous work has suggested that current vision-and-language models (VLMs) struggle to capture relational information. In this paper, we present Visual Spatial Reasoning (VSR), a dataset containing more than 10k natural text-image pairs with 66 types of spatial relations in English (e.g., under, in front of, facing). While using a seemingly simple annotation format, we show how the dataset includes challenging linguistic phenomena, such as varying reference frames. We demonstrate a large gap between human and model performance: The human ceiling is above 95%, while state-of-the-art models only achieve around 70%. We observe that VLMs’ by-relation performances have little correlation with the number of training examples and the tested models are in general incapable of recognising relations concerning the orientations of objects.1;5;;"""current vision-and-language models (VLMs) struggle to capture relational information."", ""We demonstrate a large gap between human and model performance: The human ceiling is above 95%, while state-of-the-art models only achieve around 70%."", ""VLMs’ by-relation performances have little correlation with the number of training examples and the tested models are in general incapable of recognising relations concerning the orientations of objects""";;;;;;;;;;5;tacl2023;January 2023
How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN;Current language models can generate high-quality text. Are they simply copying text they have seen before, or have they learned generalizable linguistic abstractions? To tease apart these possibilities, we introduce RAVEN, a suite of analyses for assessing the novelty of generated text, focusing on sequential structure (n-grams) and syntactic structure. We apply these analyses to four neural language models trained on English (an LSTM, a Transformer, Transformer-XL, and GPT-2). For local structure—e.g., individual dependencies—text generated with a standard sampling scheme is substantially less novel than our baseline of human-generated text from each model’s test set. For larger-scale structure—e.g., overall sentence structure—model-generated text is as novel or even more novel than the human-generated baseline, but models still sometimes copy substantially, in some cases duplicating passages over 1,000 words long from the training set. We also perform extensive manual analysis, finding evidence that GPT-2 uses both compositional and analogical generalization mechanisms and showing that GPT-2’s novel text is usually well-formed morphologically and syntactically but has reasonably frequent semantic issues (e.g., being self-contradictory).;3;mentions strengths and weaknesses;"""For local structure—e.g., individual dependencies—text generated with a standard sampling scheme is substantially less novel than our baseline of human-generated text from each model’s test set."", ""but models still sometimes copy substantially, in some cases duplicating passages over 1,000 words long from the training set."", ""but has reasonably frequent semantic issues (e.g., being self-contradictory).""";;;;;;;;;;3;tacl2023;January 2023
FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation;We present FRMT, a new dataset and evaluation benchmark for Few-shot Region-aware Machine Translation, a type of style-targeted translation. The dataset consists of professional translations from English into two regional variants each of Portuguese and Mandarin Chinese. Source documents are selected to enable detailed analysis of phenomena of interest, including lexically distinct terms and distractor terms. We explore automatic evaluation metrics for FRMT and validate their correlation with expert human evaluation across both region-matched and mismatched rating scenarios. Finally, we present a number of baseline models for this task, and offer guidelines for how researchers can train, evaluate, and compare their own models. Our dataset and evaluation code are publicly available: https://bit.ly/frmt-task.;0;;;;;;;;;;;;0;tacl2023;January 2023
OpenFact: Factuality Enhanced Open Knowledge Extraction;We focus on the factuality property during the extraction of an OpenIE corpus named OpenFact, which contains more than 12 million high-quality knowledge triplets. We break down the factuality property into two important aspects—expressiveness and groundedness—and we propose a comprehensive framework to handle both aspects. To enhance expressiveness, we formulate each knowledge piece in OpenFact based on a semantic frame. We also design templates, extra constraints, and adopt human efforts so that most OpenFact triplets contain enough details. For groundedness, we require the main arguments of each triplet to contain linked Wikidata1 entities. A human evaluation suggests that the OpenFact triplets are much more accurate and contain denser information compared to OPIEC-Linked (Gashteovski et al., 2019), one recent high-quality OpenIE corpus grounded to Wikidata. Further experiments on knowledge base completion and knowledge base question answering show the effectiveness of OpenFact over OPIEC-Linked as supplementary knowledge to Wikidata as the major KG.;0;;;;;;;;;;;;0;tacl2023;January 2023
On Graph-based Reentrancy-free Semantic Parsing;"We propose a novel graph-based approach for semantic parsing that resolves two problems observed in the literature: (1) seq2seq models fail on compositional generalization tasks; (2) previous work using phrase structure parsers cannot cover all the semantic parses observed in treebanks. We prove that both MAP inference and latent tag anchoring (required for weakly-supervised learning) are NP-hard problems. We propose two optimization algorithms based on constraint smoothing and conditional gradient to approximately solve these inference problems. Experimentally, our approach delivers state-of-the-art results on GeoQuery, Scan, and Clevr, both for i.i.d. splits and for splits that test for compositional generalization.";0;;;;;;;;;;;;0;tacl2023;January 2023
Supervised Gradual Machine Learning for Aspect-Term Sentiment Analysis;Recent work has shown that Aspect-Term Sentiment Analysis (ATSA) can be effectively performed by Gradual Machine Learning (GML). However, the performance of the current unsupervised solution is limited by inaccurate and insufficient knowledge conveyance. In this paper, we propose a supervised GML approach for ATSA, which can effectively exploit labeled training data to improve knowledge conveyance. It leverages binary polarity relations between instances, which can be either similar or opposite, to enable supervised knowledge conveyance. Besides the explicit polarity relations indicated by discourse structures, it also separately supervises a polarity classification DNN and a binary Siamese network to extract implicit polarity relations. The proposed approach fulfills knowledge conveyance by modeling detected relations as binary features in a factor graph. Our extensive experiments on real benchmark data show that it achieves the state-of-the-art performance across all the test workloads. Our work demonstrates clearly that, in collaboration with DNN for feature extraction, GML outperforms pure DNN solutions.;0;;;;;;;;;;;;0;tacl2023;January 2023
Chinese Idiom Paraphrasing;Idioms are a kind of idiomatic expression in Chinese, most of which consist of four Chinese characters. Due to the properties of non-compositionality and metaphorical meaning, Chinese idioms are hard to be understood by children and non-native speakers. This study proposes a novel task, denoted as Chinese Idiom Paraphrasing (CIP). CIP aims to rephrase idiom-containing sentences to non-idiomatic ones under the premise of preserving the original sentence’s meaning. Since the sentences without idioms are more easily handled by Chinese NLP systems, CIP can be used to pre-process Chinese datasets, thereby facilitating and improving the performance of Chinese NLP tasks, e.g., machine translation systems, Chinese idiom cloze, and Chinese idiom embeddings. In this study, we can treat the CIP task as a special paraphrase generation task. To circumvent difficulties in acquiring annotations, we first establish a large-scale CIP dataset based on human and machine collaboration, which consists of 115,529 sentence pairs. In addition to three sequence-to-sequence methods as the baselines, we further propose a novel infill-based approach based on text infilling. The results show that the proposed method has better performance than the baselines based on the established CIP dataset.;0;;;;;;;;;;;;0;tacl2023;January 2023
Evaluating Transformer Models and Human Behaviors on Chinese Character Naming;Neural network models have been proposed to explain the grapheme-phoneme mapping process in humans for many alphabet languages. These models not only successfully learned the correspondence of the letter strings and their pronunciation, but also captured human behavior in nonce word naming tasks. How would the neural models perform for a non-alphabet language (e.g., Chinese) unknown character task? How well would the model capture human behavior? In this study, we first collect human speakers’ answers on unknown Character naming tasks and then evaluate a set of transformer models by comparing their performance with human behaviors on an unknown Chinese character naming task. We found that the models and humans behaved very similarly, that they had similar accuracy distribution for each character, and had a substantial overlap in answers. In addition, the models’ answers are highly correlated with humans’ answers. These results suggested that the transformer models can capture humans’ character naming behavior well.1;1;;;;;;;;;;;;1;tacl2023;January 2023
Rank-Aware Negative Training for Semi-Supervised Text Classification;Semi-supervised text classification-based paradigms (SSTC) typically employ the spirit of self-training. The key idea is to train a deep classifier on limited labeled texts and then iteratively predict the unlabeled texts as their pseudo-labels for further training. However, the performance is largely affected by the accuracy of pseudo-labels, which may not be significant in real-world scenarios. This paper presents a Rank-aware Negative Training (RNT) framework to address SSTC in learning with noisy label settings. To alleviate the noisy information, we adapt a reasoning with uncertainty-based approach to rank the unlabeled texts based on the evidential support received from the labeled texts. Moreover, we propose the use of negative training to train RNT based on the concept that “the input instance does not belong to the complementary label”. A complementary label is randomly selected from all labels except the label on-target. Intuitively, the probability of a true label serving as a complementary label is low and thus provides less noisy information during the training, resulting in better performance on the test data. Finally, we evaluate the proposed solution on various text classification benchmark datasets. Our extensive experiments show that it consistently overcomes the state-of-the-art alternatives in most scenarios and achieves competitive performance in the others. The code of RNT is publicly available on GitHub.;0;deals with deep classifiers not LLMs;;;;;;;;;;;0;tacl2023;January 2023
MACSum: Controllable Summarization with Mixed Attributes;Controllable summarization allows users to generate customized summaries with specified attributes. However, due to the lack of designated annotations of controlled summaries, existing work has to craft pseudo datasets by adapting generic summarization benchmarks. Furthermore, most research focuses on controlling single attributes individually (e.g., a short summary or a highly abstractive summary) rather than controlling a mix of attributes together (e.g., a short and highly abstractive summary). In this paper, we propose MACSum, the first human-annotated summarization dataset for controlling mixed attributes. It contains source texts from two domains, news articles and dialogues, with human-annotated summaries controlled by five designed attributes (Length, Extractiveness, Specificity, Topic, and Speaker). We propose two simple and effective parameter-efficient approaches for the new task of mixed controllable summarization based on hard prompt tuning and soft prefix tuning. Results and analysis demonstrate that hard prompt models yield the best performance on most metrics and human evaluations. However, mixed-attribute control is still challenging for summarization tasks. Our dataset and code are available at https://github.com/psunlpgroup/MACSum.;0;;;;;;;;;;;;0;tacl2023;January 2023
MENLI: Robust Evaluation Metrics from Natural Language Inference;Recently proposed BERT-based evaluation metrics for text generation perform well on standard benchmarks but are vulnerable to adversarial attacks, e.g., relating to information correctness. We argue that this stems (in part) from the fact that they are models of semantic similarity. In contrast, we develop evaluation metrics based on Natural Language Inference (NLI), which we deem a more appropriate modeling. We design a preference-based adversarial attack framework and show that our NLI based metrics are much more robust to the attacks than the recent BERT-based metrics. On standard benchmarks, our NLI based metrics outperform existing summarization metrics, but perform below SOTA MT metrics. However, when combining existing metrics with our NLI metrics, we obtain both higher adversarial robustness (15%–30%) and higher quality metrics as measured on standard benchmarks (+5% to 30%).;1;;;;;;;;;;;;1;tacl2023;January 2023
Efficient Methods for Natural Language Processing: A Survey;"Recent work in natural language processing (NLP) has yielded appealing results from scaling model parameters and training data; however, using only scale to improve performance means that resource consumption also grows. Such resources include data, time, storage, or energy, all of which are naturally limited and unevenly distributed. This motivates research into efficient methods that require fewer resources to achieve similar results. This survey synthesizes and relates current methods and findings in efficient NLP. We aim to provide both guidance for conducting NLP under limited resources, and point towards promising research directions for developing more efficient methods.";0;no explicitly mentioning LLM, but limitations related to LLMs;"""using only scale to improve performance means that resource consumption also grows."", ""all of which are naturally limited and unevenly distributed.""";;;;;;;;;;0;tacl2023;January 2023
Abstractive Meeting Summarization: A Survey;A system that could reliably identify and sum up the most important points of a conversation would be valuable in a wide variety of real-world contexts, from business meetings to medical consultations to customer service calls. Recent advances in deep learning, and especially the invention of encoder-decoder architectures, has significantly improved language generation systems, opening the door to improved forms of abstractive summarization—a form of summarization particularly well-suited for multi-party conversation. In this paper, we provide an overview of the challenges raised by the task of abstractive meeting summarization and of the data sets, models, and evaluation metrics that have been used to tackle the problems.;0;;;;;;;;;;;;0;tacl2023;January 2023
Expectations over Unspoken Alternatives Predict Pragmatic Inferences;Scalar inferences (SI) are a signature example of how humans interpret language based on unspoken alternatives. While empirical studies have demonstrated that human SI rates are highly variable—both within instances of a single scale, and across different scales—there have been few proposals that quantitatively explain both cross- and within-scale variation. Furthermore, while it is generally assumed that SIs arise through reasoning about unspoken alternatives, it remains debated whether humans reason about alternatives as linguistic forms, or at the level of concepts. Here, we test a shared mechanism explaining SI rates within and across scales: context-driven expectations about the unspoken alternatives. Using neural language models to approximate human predictive distributions, we find that SI rates are captured by the expectedness of the strong scalemate as an alternative. Crucially, however, expectedness robustly predicts cross-scale variation only under a meaning-based view of alternatives. Our results suggest that pragmatic inferences arise from context-driven expectations over alternatives, and these expectations operate at the level of concepts.1;0;limitations of humans not LLMs;;;;;;;;;;;0;tacl2023;January 2023
Reasoning over Public and Private Data in Retrieval-Based Systems;Users an organizations are generating ever-increasing amounts of private data from a wide range of sources. Incorporating private context is important to personalize open-domain tasks such as question-answering, fact-checking, and personal assistants. State-of-the-art systems for these tasks explicitly retrieve information that is relevant to an input question from a background corpus before producing an answer. While today’s retrieval systems assume relevant corpora are fully (e.g., publicly) accessible, users are often unable or unwilling to expose their private data to entities hosting public data. We define the Split Iterative Retrieval (SPIRAL) problem involving iterative retrieval over multiple privacy scopes. We introduce a foundational benchmark with which to study SPIRAL, as no existing benchmark includes data from a private distribution. Our dataset, ConcurrentQA, includes data from distinct public and private distributions and is the first textual QA benchmark requiring concurrent retrieval over multiple distributions. Finally, we show that existing retrieval approaches face significant performance degradations when applied to our proposed retrieval setting and investigate approaches with which these tradeoffs can be mitigated. We release the new benchmark and code to reproduce the results.1;0;;;;;;;;;;;;0;tacl2023;January 2023
Multilingual Coreference Resolution in Multiparty Dialogue;Existing multiparty dialogue datasets for entity coreference resolution are nascent, and many challenges are still unaddressed. We create a large-scale dataset, Multilingual Multiparty Coref (MMC), for this task based on TV transcripts. Due to the availability of gold-quality subtitles in multiple languages, we propose reusing the annotations to create silver coreference resolution data in other languages (Chinese and Farsi) via annotation projection. On the gold (English) data, off-the-shelf models perform relatively poorly on MMC, suggesting that MMC has broader coverage of multiparty coreference than prior datasets. On the silver data, we find success both using it for data augmentation and training from scratch, which effectively simulates the zero-shot cross-lingual setting.;0;;;;;;;;;;;;0;tacl2023;January 2023
Directed Acyclic Transformer Pre-training for High-quality Non-autoregressive Text Generation;Non-AutoRegressive (NAR) text generation models have drawn much attention because of their significantly faster decoding speed and good generation quality in machine translation. However, in a wider range of text generation tasks, existing NAR models lack proper pre-training, making them still far behind the pre-trained autoregressive models. In this paper, we propose Pre-trained Directed Acyclic Transformer (PreDAT) and a novel pre-training task to promote prediction consistency in NAR generation. Experiments on five text generation tasks show that our PreDAT remarkably outperforms existing pre-trained NAR models (+4.2 score on average) and even achieves better results than pre-trained autoregressive baselines in n-gram-based metrics, along with 17 times speedup in throughput. Further analysis shows that PreDAT benefits from the unbiased prediction order that alleviates the error accumulation problem in autoregressive generation, which provides new insights into the advantages of NAR generation.1;0;;;;;;;;;;;;0;tacl2023;January 2023
Time-and-Space-Efficient Weighted Deduction;"Many NLP algorithms have been described in terms of deduction systems. Unweighted deduction allows a generic forward-chaining execution strategy. For weighted deduction, however, efficient execution should propagate the weight of each item only after it has converged. This means visiting the items in topologically sorted order (as in dynamic programming). Toposorting is fast on a materialized graph; unfortunately, materializing the graph would take extra space. Is there a generic weighted deduction strategy which, for every acyclic deduction system and every input, uses only a constant factor more time and space than generic unweighted deduction? After reviewing past strategies, we answer this question in the affirmative by combining ideas of Goodman (1999) and Kahn (1962). We also give an extension to cyclic deduction systems, based on Tarjan (1972).";0;;;;;;;;;;;;0;tacl2023;January 2023
Conditional Generation with a Question-Answering Blueprint;The ability to convey relevant and faithful information is critical for many tasks in conditional generation and yet remains elusive for neural seq-to-seq models whose outputs often reveal hallucinations and fail to correctly cover important details. In this work, we advocate planning as a useful intermediate representation for rendering conditional generation less opaque and more grounded. We propose a new conceptualization of text plans as a sequence of question-answer (QA) pairs and enhance existing datasets (e.g., for summarization) with a QA blueprint operating as a proxy for content selection (i.e., what to say) and planning (i.e., in what order). We obtain blueprints automatically by exploiting state-of-the-art question generation technology and convert input-output pairs into input-blueprint-output tuples. We develop Transformer-based models, each varying in how they incorporate the blueprint in the generated output (e.g., as a global plan or iteratively). Evaluation across metrics and datasets demonstrates that blueprint models are more factual than alternatives which do not resort to planning and allow tighter control of the generation output.;0;;;;;;;;;;;;0;tacl2023;January 2023
Collective Human Opinions in Semantic Textual Similarity;Despite the subjective nature of semantic textual similarity (STS) and pervasive disagreements in STS annotation, existing benchmarks have used averaged human ratings as gold standard. Averaging masks the true distribution of human opinions on examples of low agreement, and prevents models from capturing the semantic vagueness that the individual ratings represent. In this work, we introduce USTS, the first Uncertainty-aware STS dataset with ∼15,000 Chinese sentence pairs and 150,000 labels, to study collective human opinions in STS. Analysis reveals that neither a scalar nor a single Gaussian fits a set of observed judgments adequately. We further show that current STS models cannot capture the variance caused by human disagreement on individual instances, but rather reflect the predictive confidence over the aggregate dataset.;0;;;;;;;;;;;;0;tacl2023;January 2023
Design Choices for Crowdsourcing Implicit Discourse Relations: Revealing the Biases Introduced by Task Design;Disagreement in natural language annotation has mostly been studied from a perspective of biases introduced by the annotators and the annotation frameworks. Here, we propose to analyze another source of bias—task design bias, which has a particularly strong impact on crowdsourced linguistic annotations where natural language is used to elicit the interpretation of lay annotators. For this purpose we look at implicit discourse relation annotation, a task that has repeatedly been shown to be difficult due to the relations’ ambiguity. We compare the annotations of 1,200 discourse relations obtained using two distinct annotation tasks and quantify the biases of both methods across four different domains. Both methods are natural language annotation tasks designed for crowdsourcing. We show that the task design can push annotators towards certain relations and that some discourse relation senses can be better elicited with one or the other annotation approach. We also conclude that this type of bias should be taken into account when training and testing models.;0;;;;;;;;;;;;0;tacl2023;January 2023
Communication Drives the Emergence of Language Universals in Neural Agents: Evidence from the Word-order/Case-marking Trade-off;Artificial learners often behave differently from human learners in the context of neural agent-based simulations of language emergence and change. A common explanation is the lack of appropriate cognitive biases in these learners. However, it has also been proposed that more naturalistic settings of language learning and use could lead to more human-like results. We investigate this latter account, focusing on the word-order/case-marking trade-off, a widely attested language universal that has proven particularly hard to simulate. We propose a new Neural-agent Language Learning and Communication framework (NeLLCom) where pairs of speaking and listening agents first learn a miniature language via supervised learning, and then optimize it for communication via reinforcement learning. Following closely the setup of earlier human experiments, we succeed in replicating the trade-off with the new framework without hard-coding specific biases in the agents. We see this as an essential step towards the investigation of language universals with neural learners.;0;;;;;;;;;;;;0;tacl2023;January 2023
A Cross-Linguistic Pressure for Uniform Information Density in Word Order;While natural languages differ widely in both canonical word order and word order flexibility, their word orders still follow shared cross-linguistic statistical patterns, often attributed to functional pressures. In the effort to identify these pressures, prior work has compared real and counterfactual word orders. Yet one functional pressure has been overlooked in such investigations: The uniform information density (UID) hypothesis, which holds that information should be spread evenly throughout an utterance. Here, we ask whether a pressure for UID may have influenced word order patterns cross-linguistically. To this end, we use computational models to test whether real orders lead to greater information uniformity than counterfactual orders. In our empirical study of 10 typologically diverse languages, we find that: (i) among SVO languages, real word orders consistently have greater uniformity than reverse word orders, and (ii) only linguistically implausible counterfactual orders consistently exceed the uniformity of real orders. These findings are compatible with a pressure for information uniformity in the development and usage of natural languages.1;0;;;;;;;;;;;;0;tacl2023;January 2023
Cross-functional Analysis of Generalization in Behavioral Learning;In behavioral testing, system functionalities underrepresented in the standard evaluation setting (with a held-out test set) are validated through controlled input-output pairs. Optimizing performance on the behavioral tests during training (behavioral learning) would improve coverage of phenomena not sufficiently represented in the i.i.d. data and could lead to seemingly more robust models. However, there is the risk that the model narrowly captures spurious correlations from the behavioral test suite, leading to overestimation and misrepresentation of model performance—one of the original pitfalls of traditional evaluation. In this work, we introduce BeLUGA, an analysis method for evaluating behavioral learning considering generalization across dimensions of different granularity levels. We optimize behavior-specific loss functions and evaluate models on several partitions of the behavioral test suite controlled to leave out specific phenomena. An aggregate score measures generalization to unseen functionalities (or overfitting). We use BeLUGA to examine three representative NLP tasks (sentiment analysis, paraphrase identification, and reading comprehension) and compare the impact of a diverse set of regularization and domain generalization methods on generalization performance.1;0;;;;;;;;;;;;0;tacl2023;January 2023
Exploring Contrast Consistency of Open-Domain Question Answering Systems on Minimally Edited Questions;Contrast consistency, the ability of a model to make consistently correct predictions in the presence of perturbations, is an essential aspect in NLP. While studied in tasks such as sentiment analysis and reading comprehension, it remains unexplored in open-domain question answering (OpenQA) due to the difficulty of collecting perturbed questions that satisfy factuality requirements. In this work, we collect minimally edited questions as challenging contrast sets to evaluate OpenQA models. Our collection approach combines both human annotation and large language model generation. We find that the widely used dense passage retriever (DPR) performs poorly on our contrast sets, despite fitting the training set well and performing competitively on standard test sets. To address this issue, we introduce a simple and effective query-side contrastive loss with the aid of data augmentation to improve DPR training. Our experiments on the contrast sets demonstrate that DPR’s contrast consistency is improved without sacrificing its accuracy on the standard test sets.1;1;;;;;;;;;;;;1;tacl2023;January 2023
Compositional Zero-Shot Domain Transfer with Text-to-Text Models;Label scarcity is a bottleneck for improving task performance in specialized domains. We propose a novel compositional transfer learning framework (DoT51) for zero-shot domain transfer. Without access to in-domain labels, DoT5 jointly learns domain knowledge (from masked language modelling of unlabelled in-domain free text) and task knowledge (from task training on more readily available general-domain data) in a multi-task manner. To improve the transferability of task training, we design a strategy named NLGU: We simultaneously train natural language generation (NLG) for in-domain label-to-data generation, which enables data augmentation for self-finetuning and natural language understanding (NLU) for label prediction. We evaluate DoT5 on the biomedical domain and the resource-lean subdomain of radiology, focusing on natural language inference, text summarization, and embedding learning. DoT5 demonstrates the effectiveness of compositional transfer learning through multi-task learning. In particular, DoT5 outperforms the current state-of-the-art in zero-shot transfer by over 7 absolute points in accuracy on RadNLI. We validate DoT5 with ablations and a case study demonstrating its ability to solve challenging NLI examples requiring in-domain expertise.;0;;;;;;;;;;;;0;tacl2023;January 2023
MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages;MIRACL is a multilingual dataset for ad hoc retrieval across 18 languages that collectively encompass over three billion native speakers around the world. This resource is designed to support monolingual retrieval tasks, where the queries and the corpora are in the same language. In total, we have gathered over 726k high-quality relevance judgments for 78k queries over Wikipedia in these languages, where all annotations have been performed by native speakers hired by our team. MIRACL covers languages that are both typologically close as well as distant from 10 language families and 13 sub-families, associated with varying amounts of publicly available resources. Extensive automatic heuristic verification and manual assessments were performed during the annotation process to control data quality. In total, MIRACL represents an investment of around five person-years of human annotator effort. Our goal is to spur research on improving retrieval across a continuum of languages, thus enhancing information access capabilities for diverse populations around the world, particularly those that have traditionally been underserved. MIRACL is available at http://miracl.ai/.;0;;;;;;;;;;;;0;tacl2023;January 2023
DMDD: A Large-Scale Dataset for Dataset Mentions Detection;The recognition of dataset names is a critical task for automatic information extraction in scientific literature, enabling researchers to understand and identify research opportunities. However, existing corpora for dataset mention detection are limited in size and naming diversity. In this paper, we introduce the Dataset Mentions Detection Dataset (DMDD), the largest publicly available corpus for this task. DMDD consists of the DMDD main corpus, comprising 31,219 scientific articles with over 449,000 dataset mentions weakly annotated in the format of in-text spans, and an evaluation set, which comprises 450 scientific articles manually annotated for evaluation purposes. We use DMDD to establish baseline performance for dataset mention detection and linking. By analyzing the performance of various models on DMDD, we are able to identify open problems in dataset mention detection. We invite the community to use our dataset as a challenge to develop novel dataset mention detection models.;0;;;;;;;;;;;;0;tacl2023;January 2023
T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification;Cross-lingual text classification leverages text classifiers trained in a high-resource language to perform text classification in other languages with no or minimal fine-tuning (zero/ few-shots cross-lingual transfer). Nowadays, cross-lingual text classifiers are typically built on large-scale, multilingual language models (LMs) pretrained on a variety of languages of interest. However, the performance of these models varies significantly across languages and classification tasks, suggesting that the superposition of the language modelling and classification tasks is not always effective. For this reason, in this paper we propose revisiting the classic “translate-and-test” pipeline to neatly separate the translation and classification stages. The proposed approach couples 1) a neural machine translator translating from the targeted language to a high-resource language, with 2) a text classifier trained in the high-resource language, but the neural machine translator generates “soft” translations to permit end-to-end backpropagation during fine-tuning of the pipeline. Extensive experiments have been carried out over three cross-lingual text classification datasets (XNLI, MLDoc, and MultiEURLEX), with the results showing that the proposed approach has significantly improved performance over a competitive baseline.;2;;"""the performance of these models varies significantly across languages and classification tasks, suggesting that the superposition of the language modelling and classification tasks is not always effective.""";;;;;;;;;;2;tacl2023;January 2023
Introduction to Mathematical Language Processing: Informal Proofs, Word Problems, and Supporting Tasks;Automating discovery in mathematics and science will require sophisticated methods of information extraction and abstract reasoning, including models that can convincingly process relationships between mathematical elements and natural language, to produce problem solutions of real-world value. We analyze mathematical language processing methods across five strategic sub-areas (identifier-definition extraction, formula retrieval, natural language premise selection, math word problem solving, and informal theorem proving) from recent years, highlighting prevailing methodologies, existing limitations, overarching trends, and promising avenues for future research.;0;;;;;;;;;;;;0;tacl2023;January 2023
Evaluating a Century of Progress on the Cognitive Science of Adjective Ordering;"The literature on adjective ordering abounds with proposals meant to account for why certain adjectives appear before others in multi-adjective strings (e.g., the small brown box). However, these proposals have been developed and tested primarily in isolation and based on English; few researchers have looked at the combined performance of multiple factors in the determination of adjective order, and few have evaluated predictors across multiple languages. The current work approaches both of these objectives by using technologies and datasets from natural language processing to look at the combined performance of existing proposals across 32 languages. Comparing this performance with both random and idealized baselines, we show that the literature on adjective ordering has made significant meaningful progress across its many decades, but there remains quite a gap yet to be explained.";0;;;;;;;;;;;;0;tacl2023;January 2023
Improving Multitask Retrieval by Promoting Task Specialization;In multitask retrieval, a single retriever is trained to retrieve relevant contexts for multiple tasks. Despite its practical appeal, naive multitask retrieval lags behind task-specific retrieval, in which a separate retriever is trained for each task. We show that it is possible to train a multitask retriever that outperforms task-specific retrievers by promoting task specialization. The main ingredients are: (1) a better choice of pretrained model—one that is explicitly optimized for multitasking—along with compatible prompting, and (2) a novel adaptive learning method that encourages each parameter to specialize in a particular task. The resulting multitask retriever is highly performant on the KILT benchmark. Upon analysis, we find that the model indeed learns parameters that are more task-specialized compared to naive multitasking without prompting or adaptive learning.1;0;;;;;;;;;;;;0;tacl2023;January 2023
Calibrated Interpretation: Confidence Estimation in Semantic Parsing;Sequence generation models are increasingly being used to translate natural language into programs, i.e., to perform executable semantic parsing. The fact that semantic parsing aims to predict programs that can lead to executed actions in the real world motivates developing safe systems. This in turn makes measuring calibration—a central component to safety—particularly important. We investigate the calibration of popular generation models across four popular semantic parsing datasets, finding that it varies across models and datasets. We then analyze factors associated with calibration error and release new confidence-based challenge splits of two parsing datasets. To facilitate the inclusion of calibration in semantic parsing evaluations, we release a library for computing calibration metrics.1;0;;;;;;;;;;;;0;tacl2023;January 2023
Intent-calibrated Self-training for Answer Selection in Open-domain Dialogues;Answer selection in open-domain dialogues aims to select an accurate answer from candidates. The recent success of answer selection models hinges on training with large amounts of labeled data. However, collecting large-scale labeled data is labor-intensive and time-consuming. In this paper, we introduce the predicted intent labels to calibrate answer labels in a self-training paradigm. Specifically, we propose intent-calibrated self-training (ICAST) to improve the quality of pseudo answer labels through the intent-calibrated answer selection paradigm, in which we employ pseudo intent labels to help improve pseudo answer labels. We carry out extensive experiments on two benchmark datasets with open-domain dialogues. The experimental results show that ICAST outperforms baselines consistently with 1%, 5%, and 10% labeled data. Specifically, it improves 2.06% and 1.00% of F1 score on the two datasets, compared with the strongest baseline with only 5% labeled data.;0;;;;;;;;;;;;0;tacl2023;January 2023
Benchmarking the Generation of Fact Checking Explanations;Fighting misinformation is a challenging, yet crucial, task. Despite the growing number of experts being involved in manual fact-checking, this activity is time-consuming and cannot keep up with the ever-increasing amount of fake news produced daily. Hence, automating this process is necessary to help curb misinformation. Thus far, researchers have mainly focused on claim veracity classification. In this paper, instead, we address the generation of justifications (textual explanation of why a claim is classified as either true or false) and benchmark it with novel datasets and advanced baselines. In particular, we focus on summarization approaches over unstructured knowledge (i.e., news articles) and we experiment with several extractive and abstractive strategies. We employed two datasets with different styles and structures, in order to assess the generalizability of our findings. Results show that in justification production summarization benefits from the claim information, and, in particular, that a claim-driven extractive step improves abstractive summarization performances. Finally, we show that although cross-dataset experiments suffer from performance degradation, a unique model trained on a combination of the two datasets is able to retain style information in an efficient manner.;0;;;;;;;;;;;;0;tacl2023;January 2023
T 2 -NER: A Two-Stage Span-Based Framework for Unified Named Entity Recognition with Templates;Named Entity Recognition (NER) has so far evolved from the traditional flat NER to overlapped and discontinuous NER. They have mostly been solved separately, with only several exceptions that concurrently tackle three tasks with a single model. The current best-performing method formalizes the unified NER as word-word relation classification, which barely focuses on mention content learning and fails to detect entity mentions comprising a single word. In this paper, we propose a two-stage span-based framework with templates, namely, T2-NER, to resolve the unified NER task. The first stage is to extract entity spans, where flat and overlapped entities can be recognized. The second stage is to classify over all entity span pairs, where discontinuous entities can be recognized. Finally, multi-task learning is used to jointly train two stages. To improve the efficiency of span-based model, we design grouped templates and typed templates for two stages to realize batch computations. We also apply an adjacent packing strategy and a latter packing strategy to model discriminative boundary information and learn better span (pair) representation. Moreover, we introduce the syntax information to enhance our span representation. We perform extensive experiments on eight benchmark datasets for flat, overlapped, and discontinuous NER, where our model beats all the current competitive baselines, obtaining the best performance of unified NER.;0;;;;;;;;;;;;0;tacl2023;January 2023
PASTA: A Dataset for Modeling PArticipant STAtes in Narratives;"The events in a narrative are understood as a coherent whole via the underlying states of their participants. Often, these participant states are not explicitly mentioned, instead left to be inferred by the reader. A model that understands narratives should likewise infer these implicit states, and even reason about the impact of changes to these states on the narrative. To facilitate this goal, we introduce a new crowdsourced English-language, Participant States dataset, PASTA. This dataset contains inferable participant states; a counterfactual perturbation to each state; and the changes to the story that would be necessary if the counterfactual were true. We introduce three state-based reasoning tasks that test for the ability to infer when a state is entailed by a story, to revise a story conditioned on a counterfactual state, and to explain the most likely state change given a revised story. Experiments show that today’s LLMs can reason about states to some degree, but there is large room for improvement, especially in problems requiring access and ability to reason with diverse types of knowledge (e.g., physical, numerical, factual).1";3;;"""Experiments show that today’s LLMs can reason about states to some degree, but there is large room for improvement, especially in problems requiring access and ability to reason with diverse types of knowledge (e.g., physical, numerical, factual)""";;;;;;;;;;3;tacl2023;January 2023
U-CORE: A Unified Deep Cluster-wise Contrastive Framework for Open Relation Extraction;Within Open Relation Extraction (ORE) tasks, the Zero-shot ORE method is to generalize undefined relations from predefined relations, while the Unsupervised ORE method is to extract undefined relations without the need for annotations. However, despite the possibility of overlap between predefined and undefined relations in the training data, a unified framework for both Zero-shot and Unsupervised ORE has yet to be established. To address this gap, we propose U-CORE: A Unified Deep Cluster-wise Contrastive Framework for both Zero-shot and Unsupervised ORE, by leveraging techniques from Contrastive Learning (CL) and Clustering.1 U-CORE overcomes the limitations of CL-based Zero-shot ORE methods by employing Cluster-wise CL that preserves both local smoothness as well as global semantics. Additionally, we employ a deep-cluster-based updater that optimizes the cluster center, thus enhancing the accuracy and efficiency of the model. To increase the stability of the model, we adopt Adaptive Self-paced Learning that effectively addresses the data-shifting problems. Experimental results on three well-known datasets demonstrate that U-CORE significantly improves upon existing methods by showing an average improvement of 7.35% ARI on Zero-shot ORE tasks and 15.24% ARI on Unsupervised ORE tasks.;0;;;;;;;;;;;;0;tacl2023;January 2023
In-Context Retrieval-Augmented Language Models;Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access.1;2;;"""Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment""";;;;;;;;;;2;tacl2023;January 2023
Learning to Paraphrase Sentences to Different Complexity Levels;While sentence simplification is an active research topic in NLP, its adjacent tasks of sentence complexification and same-level paraphrasing are not. To train models on all three tasks, we present two new unsupervised datasets. We compare these datasets, one labeled by a weak classifier and the other by a rule-based approach, with a single supervised dataset. Using these three datasets for training, we perform extensive experiments on both multitasking and prompting strategies. Compared to other systems trained on unsupervised parallel data, models trained on our weak classifier labeled dataset achieve state-of-the-art performance on the ASSET simplification benchmark. Our models also outperform previous work on sentence-level targeting. Finally, we establish how a handful of Large Language Models perform on these tasks under a zero-shot setting.;1;;;;;;;;;;;;1;tacl2023;January 2023
Direct Speech Translation for Automatic Subtitling;Automatic subtitling is the task of automatically translating the speech of audiovisual content into short pieces of timed text, i.e., subtitles and their corresponding timestamps. The generated subtitles need to conform to space and time requirements, while being synchronized with the speech and segmented in a way that facilitates comprehension. Given its considerable complexity, the task has so far been addressed through a pipeline of components that separately deal with transcribing, translating, and segmenting text into subtitles, as well as predicting timestamps. In this paper, we propose the first direct speech translation model for automatic subtitling that generates subtitles in the target language along with their timestamps with a single model. Our experiments on 7 language pairs show that our approach outperforms a cascade system in the same data condition, also being competitive with production tools on both in-domain and newly released out-domain benchmarks covering new scenarios.;0;;;;;;;;;;;;0;tacl2023;January 2023
How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure;Language models are typically evaluated on their success at predicting the distribution of specific words in specific contexts. Yet linguistic knowledge also encodes relationships between contexts, allowing inferences between word distributions. We investigate the degree to which pre-trained transformer-based large language models (LLMs) represent such relationships, focusing on the domain of argument structure. We find that LLMs perform well in generalizing the distribution of a novel noun argument between related contexts that were seen during pre-training (e.g., the active object and passive subject of the verb spray), succeeding by making use of the semantically organized structure of the embedding space for word embeddings. However, LLMs fail at generalizations between related contexts that have not been observed during pre-training, but which instantiate more abstract, but well-attested structural generalizations (e.g., between the active object and passive subject of an arbitrary verb). Instead, in this case, LLMs show a bias to generalize based on linear order.This finding points to a limitation with current models and points to a reason for which their training is data-intensive.1;4;presents limitations in detail as findings of the paper. 4-5;"""LLMs fail at generalizations between related contexts that have not been observed during pre-training, but which instantiate more abstract, but well-attested structural generalizations"", ""LLMs show a bias to generalize based on linear order."", ""This finding points to a limitation with current models and points to a reason for which their training is data-intensive""";;;;;;;;;;4;tacl2023;January 2023
Multi 3 WOZ: A Multilingual, Multi-Domain, Multi-Parallel Dataset for Training and Evaluating Culturally Adapted Task-Oriented Dialog Systems;Creating high-quality annotated data for task-oriented dialog (ToD) is known to be notoriously difficult, and the challenges are amplified when the goal is to create equitable, culturally adapted, and large-scale ToD datasets for multiple languages. Therefore, the current datasets are still very scarce and suffer from limitations such as translation-based non-native dialogs with translation artefacts, small scale, or lack of cultural adaptation, among others. In this work, we first take stock of the current landscape of multilingual ToD datasets, offering a systematic overview of their properties and limitations. Aiming to reduce all the detected limitations, we then introduce Multi3WOZ, a novel multilingual, multi-domain, multi-parallel ToD dataset. It is large-scale and offers culturally adapted dialogs in 4 languages to enable training and evaluation of multilingual and cross-lingual ToD systems. We describe a complex bottom–up data collection process that yielded the final dataset, and offer the first sets of baseline scores across different ToD-related tasks for future reference, also highlighting its challenging nature.;0;;;;;;;;;;;;0;tacl2023;January 2023
Can Authorship Representation Learning Capture Stylistic Features?;Automatically disentangling an author’s style from the content of their writing is a longstanding and possibly insurmountable problem in computational linguistics. At the same time, the availability of large text corpora furnished with author labels has recently enabled learning authorship representations in a purely data-driven manner for authorship attribution, a task that ostensibly depends to a greater extent on encoding writing style than encoding content. However, success on this surrogate task does not ensure that such representations capture writing style since authorship could also be correlated with other latent variables, such as topic. In an effort to better understand the nature of the information these representations convey, and specifically to validate the hypothesis that they chiefly encode writing style, we systematically probe these representations through a series of targeted experiments. The results of these experiments suggest that representations learned for the surrogate authorship prediction task are indeed sensitive to writing style. As a consequence, authorship representations may be expected to be robust to certain kinds of data shift, such as topic drift over time. Additionally, our findings may open the door to downstream applications that require stylistic representations, such as style transfer.;0;;;;;;;;;;;;0;tacl2023;January 2023
Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing;"Cross-lingual semantic parsing transfers parsing capability from a high-resource language (e.g., English) to low-resource languages with scarce training data. Previous work has primarily considered silver-standard data augmentation or zero-shot methods; exploiting few-shot gold data is comparatively unexplored. We propose a new approach to cross-lingual semantic parsing by explicitly minimizing cross-lingual divergence between probabilistic latent variables using Optimal Transport. We demonstrate how this direct guidance improves parsing from natural languages using fewer examples and less training. We evaluate our method on two datasets, MTOP and MultiATIS++SQL, establishing state-of-the-art results under a few-shot cross-lingual regime. Ablation studies further reveal that our method improves performance even without parallel input translations. In addition, we show that our model better captures cross-lingual structure in the latent space to improve semantic representation similarity.1";0;;;;;;;;;;;;0;tacl2023;January 2023