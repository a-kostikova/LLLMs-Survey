<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Augmentation Improves Zero-Shot Cross-Lingual Transfer</title>
				<funder>
					<orgName type="full">Cisco Research Award</orgName>
				</funder>
				<funder ref="#_E4KchGX">
					<orgName type="full">Air Force Research Laboratory</orgName>
				</funder>
				<funder ref="#_cdWSSKB">
					<orgName type="full">Annenberg Fellowship at USC</orgName>
				</funder>
				<funder>
					<orgName type="full">Amazon Research Awards</orgName>
				</funder>
				<funder ref="#_fw6RTbz #_WYfA8zA">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California ‡ University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kuan-Hao</forename><surname>Huang</surname></persName>
							<email>khhuang@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California ‡ University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
							<email>kwchang@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California ‡ University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
							<email>muhaoche@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California ‡ University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Augmentation Improves Zero-Shot Cross-Lingual Transfer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">966A86504B0C9871ECC4A0284CD3205E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Zero-shot cross-lingual transfer is a central task in multilingual NLP, allowing models trained in languages with more sufficient training resources to generalize to other low-resource languages. Earlier efforts on this task use parallel corpora, bilingual dictionaries, or other annotated alignment data to improve cross-lingual transferability, which are typically expensive to obtain. In this paper, we propose a simple yet effective method, SALT, to improve the zero-shot cross-lingual transfer of the multilingual pretrained language models without the help of such external data. By incorporating code-switching and embedding mixup with self-augmentation, SALT effectively distills cross-lingual knowledge from the multilingual PLM and enhances its transferability on downstream tasks. Experimental results on XNLI and PAWS-X show that our method is able to improve zero-shot cross-lingual transferability without external data. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Zero-shot cross-lingual transfer is integral to many multilingual NLP tasks <ref type="bibr" target="#b29">(Ma and Xia, 2014;</ref><ref type="bibr" target="#b5">Artetxe and Schwenk, 2019;</ref><ref type="bibr" target="#b0">Ahmad et al., 2019)</ref>. For some NLP tasks, the task-specific training data are often not evenly provided in terms of quantity and quality for distinct languages, and may even be unavailable for particularly low-resource languages. Zero-shot cross-lingual transfer allows models trained in languages with sufficient training resources to generalize to other low-resource languages. Earlier efforts on this task use parallel corpora, bilingual dictionaries, or other annotated alignment data to improve cross-lingual transferability, which are typically expensive to obtain <ref type="bibr" target="#b10">(Chi et al., 2021;</ref><ref type="bibr" target="#b40">Yang et al., 2022;</ref><ref type="bibr" target="#b33">Qin et al., 2020;</ref><ref type="bibr">Lee et al., 2021;</ref><ref type="bibr" target="#b23">Krishnan et al., 2021)</ref>. 1 Our code is available at https://github.com/ luka-group/SALT.  Recent analyses show that multilingual pretrained language models (PLMs) possess rich crosslingual knowledge to facilitate the transfer <ref type="bibr" target="#b32">(Pires et al., 2019;</ref><ref type="bibr">Conneau et al., 2020a;</ref><ref type="bibr" target="#b21">Huang et al., 2021)</ref>. However, when being finetuned on a specific task in the source language, multilingual PLMs may catastrophically forget cross-lingual knowledge <ref type="bibr" target="#b28">(Liu et al., 2021;</ref><ref type="bibr" target="#b8">Chalkidis et al., 2021)</ref>. We argue that cross-lingual knowledge possessed by multilingual PLMs can be distilled and incorporated into task training data to preserve and improve models' cross-lingual transferability when fine-tuning on downstream tasks.</p><p>In this paper, we focus on the setting where no external cross-lingual alignment data are available and propose a simple yet effective Self-Augmented Language Transfer (SALT) method for multilingual PLMs. SALT introduces two self-augmentation techniques on monolingual task-specific training data as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Before task training, an offline technique based on cross-lingual codeswitching first uses the PLM to predict a crosslingual counterpart of each token in the given training sample. Accordingly, this technique generates a cross-lingual augmentation of the training sample by substituting the tokens with their crosslingual counterparts. During task training, an on-line self-augmentation technique based on embedding mixup <ref type="bibr" target="#b41">(Zhang et al., 2018)</ref> randomly perturbs the representation of the training sample between the source and target languages. SALT allows the information about context-specific alignment of tokens to be distilled from the multilingual PLM, and to be further enhanced through perturbed training.</p><p>Experimental results on XNLI and PAWS-X benchmarks demonstrate that SALT achieves more improvements on zero-shot transferability than previous SOTA methods <ref type="bibr">(Lewis et al., 2020;</ref><ref type="bibr" target="#b21">Huang et al., 2021)</ref>. With self-augmentation on three target languages, SALT achieves 1.4% improvement on 15 languages of XNLI and 4.1% on 6 languages of PAWS-X in terms of average accuracy comparing with the base model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Self-Augmentation</head><p>Our method distills cross-lingual token-level alignment from the multilingual PLM and incorporates the knowledge to task-specific data through codeswitching. To further improve the diversity of augmentation for better generalization, we also apply a random embedding mixup. Following are the details of these two techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Self-Augmentation with Code-Switching</head><p>SALT adopts a modified masked language modeling (MLM) method to distill cross-lingual token translation pairs. As multilingual PLMs, such as mBERT <ref type="bibr" target="#b14">(Devlin et al., 2019)</ref> and XLM-R <ref type="bibr">(Conneau et al., 2020a)</ref>, are pre-trained with MLM, we can directly mask these models to predict crosslingual tokens. Specifically, SALT makes two modifications to the original MLM learning objective. First, we let the model predict tokens only in a specific target language, and disregard all tokens not belonging to this language. Tokens in each target language are collected from a monolingual vocabulary list.<ref type="foot" target="#foot_0">2</ref> Second, to ensure that the semantics of predictions are similar to original tokens, we do not mask the original tokens when inputting the sentence to the multilingual PLM. This seeks to help the PLM better infer cross-lingual counterpart tokens given the references of original tokens in the input context.</p><p>Generated token-level cross-lingual substitutions with high enough predicted probabilities are used for code-switching. In this work, we adopt a fixed probability threshold for all target languages. We also predict synonyms in the source language with SALT. Since predicted probability in the source language and target languages are of different scales, we assign a different threshold for synonym prediction. This self-augmentation process is done offline before task training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-Augmentation with Embedding Mixup</head><p>To further improve the diversity of self-augmented features and reduce the noise introduced by codeswitching through smoothing, we propose an online self-augmentation technique based on crosslingual embedding mixup <ref type="bibr" target="#b19">(Guo et al., 2019)</ref>. Specifically, for each token t i in the generated sentence from code-switching, we interpolate its embedding with that of the original token s i before code-switching. In detail, given the embedding of original token h s i and embedding of the substituted token h t i , the mixed up token embedding is generated as</p><formula xml:id="formula_0">h i = r • h s i + (1 -r) • h t i ,</formula><p>where r = {r j }, r j ∈ [0, 1] is a random vector. In the embedding mixup, the interpolation coefficient of each embedding dimension j is independently generated from a uniform distribution to allow for more diverse combinations of embeddings in two languages. Applying this self-augmentation technique allows for randomly perturbing the training instance between the source and target language representation, leading to further improved transferability of the obtained task model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training</head><p>In this work, we use English as the source language and consider three target languages for codeswitching, i.e. French, German and Spanish<ref type="foot" target="#foot_1">3</ref> . Before training the task model, on each original training sample in the source language, we generate an augmented sample in each target language offline with code-switching, where all predicted highprobability token substitutions are applied. to these augmented samples where the interpolation coefficient vector r is dynamically sampled in each step of training for each instance. While both self-augmentation techniques automatically switch and perturb the original training samples towards the target language(s), the labels on those training samples remain unchanged. Hence, the final task model is trained directly on the self-augmented training samples by optimizing the original learning objective of the task. This allows for robust zero-shot transfer of the task model by using only monolingual training data in the source language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head><p>In this section, we evaluate SALT to demonstrate that self-augmentation methods improve zero-shot cross-lingual transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>Following <ref type="bibr" target="#b21">Huang et al. (2021)</ref>, we consider two cross-lingual datasets. XNLI is a natural language inference (NLI) dataset, including premisehypothesis pairs in 15 languages labeled as entailment/neutral/contradiction. PAWS-X is a paraphrase identification dataset, including sentence pairs in six languages with binary labels. On both datasets, we train and validate models with English data, and test them with data in all languages. We report the average accuracy of five-run experiment.</p><p>Baseline. We compare SALT with two SOTA zero-shot cross-lingual transfer methods without any external data. RS-ADV and RS-RP <ref type="bibr" target="#b21">(Huang et al., 2021)</ref> enhance transferability by respectively adding adversarial and random embedding pertur-bation during task training on English data. We also provide results of three methods that use external data. RS-DA <ref type="bibr" target="#b21">(Huang et al., 2021)</ref> augments training data by replacing words with predefined synonyms <ref type="bibr" target="#b3">(Alzantot et al., 2018)</ref>. CoSDA-ML <ref type="bibr" target="#b33">(Qin et al., 2020)</ref> creates code-switching data with bilingual dictionaries. SCOPA <ref type="bibr">(Lee et al., 2021)</ref> extends CoSDA-ML by mixing the hidden states of original and code-switched data with a fixed ratio.</p><p>Implementation Details. We evaluate the proposed method based on mBERT. For both sentence pair classification tasks, self-augmentation is conducted separately on each sentence. We augment one code-switched instance per language (including English, French, Spanish, and German) for each original instance. For other hyper-parameters, we follow the training scripts by <ref type="bibr" target="#b21">Huang et al. (2021)</ref>. More details are in Appx. §A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Tab. 1 shows the results on XNLI, where we observe that previous methods without external data have a positive influence on a few languages but have a negative influence on other languages. Their average improvement is however lesser in comparison to SALT which leads to an average improvement in accuracy by 1% as well as significant improvements in 9 out of 15 languages over mBERT. Our method also outperforms other baselines by at least 0.8% in terms of average accuracy (both w/ and w/o en). The results indicate that the model can benefit from cross-lingual knowledge distilled from itself. Moreover, augmenting the data to three target languages can bring improvements to all 14 target languages. For example, the improvements on ar, bg, el, hi, tr and zh are 1.2%, 2.6%, 1.7%, 1.4%, 1.2% and 1.0%. Experiment on PAWS-X also shows that SALT can improve model performance in comparison with the vanilla setting (Tab. 2). However, RS-RP is also effective on this task and achieves comparable results. Considering that NLI requires inferring the logical consequence that can be dependent on various components of the two sentences, this complex reasoning process benefits more from the robust training of SALT. On the other hand, as a simpler task based on sentence similarity, paraphrase identification can be sufficiently improved based on random perturbation.</p><p>Generalized Setting. We further evaluate SALT in a generalized setting <ref type="bibr">(Lewis et al., 2020;</ref><ref type="bibr" target="#b21">Huang et al., 2021)</ref> on XNLI. The new setting pairs up sentences from two different languages as the premise and the hypothesis, converting the original test data from 15 languages to 225 language pairs. SALT achieves 0.5% of improvement over the best baseline and 2.2% over the vanilla PLM in average. Full results on all language pairs are in Appx. §B. Despite the vocabulary gap between training and inference for baselines, SALT reduces this gap by code-switching and mixup.</p><p>Ablation Study. To further investigate the incorporated techniques in SALT, we conduct an ablation study on PAWS-X as shown in Tab. 3. Offline codeswitching solely improves the average accuracy by 1.7%, while online embedding mixup further improves it by 0.8%. We also evaluate the influence of involved target languages in SALT. Code-switching with only English synonyms distilled from PLMs can bring an improvement of 1.8%, while involving three target languages further improves the performance by 0.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Zero-shot cross-lingual transfer has become an emerging research topic since it potentially reduces the effort of collecting labeled data for lowresource languages <ref type="bibr" target="#b0">(Ahmad et al., 2019;</ref><ref type="bibr" target="#b20">Hu et al., 2020;</ref><ref type="bibr" target="#b16">Dufter and Schütze, 2020;</ref><ref type="bibr" target="#b34">Ruder et al., 2021;</ref><ref type="bibr" target="#b7">Chai et al., 2022;</ref><ref type="bibr" target="#b22">Huang et al., 2022)</ref>. Earlier works directly apply multilingual PLMs, such as multilingual BERT <ref type="bibr" target="#b14">(Devlin et al., 2019)</ref>, XLM <ref type="bibr" target="#b12">(Conneau and Lample, 2019)</ref>, and XLM-R <ref type="bibr">(Conneau et al., 2020a)</ref>, and achieve surprisingly well performance on this setting. Recently, the performance is further improved with additional auxiliary data, such as parallel translation pairs <ref type="bibr" target="#b10">(Chi et al., 2021;</ref><ref type="bibr" target="#b36">Wei et al., 2021;</ref><ref type="bibr" target="#b40">Yang et al., 2022;</ref><ref type="bibr" target="#b17">Feng et al., 2022)</ref>, bilingual dictionaries <ref type="bibr" target="#b6">(Cao et al., 2020;</ref><ref type="bibr" target="#b33">Qin et al., 2020;</ref><ref type="bibr">Liu et al., 2020b;</ref><ref type="bibr" target="#b23">Krishnan et al., 2021;</ref><ref type="bibr">Lee et al., 2021)</ref>, and syntactic features <ref type="bibr" target="#b35">(Subburathinam et al., 2019;</ref><ref type="bibr" target="#b30">Meng et al., 2019;</ref><ref type="bibr">Ahmad et al., 2021a,b)</ref>.</p><p>Our work aligns more with another line of research that studies zero-shot cross-lingual transfer without using additional annotations. This includes unsupervised embedding alignment <ref type="bibr" target="#b4">(Artetxe et al., 2020;</ref><ref type="bibr">Conneau et al., 2020b)</ref>, robust training <ref type="bibr" target="#b21">(Huang et al., 2021)</ref>, and meta-learning <ref type="bibr" target="#b31">(Nooralahzadeh et al., 2020)</ref>. Our idea is motivated by the self-augmentation techniques <ref type="bibr" target="#b18">(Feng et al., 2021;</ref><ref type="bibr" target="#b38">Xu et al., 2021)</ref> that are mostly explored for monolingual tasks, and the mixup techniques <ref type="bibr" target="#b41">(Zhang et al., 2018;</ref><ref type="bibr">Lee et al., 2021;</ref><ref type="bibr" target="#b40">Yang et al., 2022)</ref> which seeks to smooth the embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose SALT, a self-augmention method for zero-shot cross-lingual transfer of PLMs. SALT distills cross-lingual knowledge from PLMs and incorporates them into task training data through an offline code-switching technique, and an online embedding mixup technique to improve transferability with a smoothed representation space. Experiments and analyses based on XNLI and PAWS-X demonstrate promising improvement to SALT in terms of cross-lingual transfer without using external data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>We implement our model based on Huggingface Transformers <ref type="bibr" target="#b37">(Wolf et al., 2019)</ref>. We apply the uncased base version of mBERT model consisting of 110M parameters. We set the probability threshold for token substitution to 1e-3 for English synonym replacement and 1e-7 for code-switching in other languages. We run experiments with a NVIDIA GeForce RTX 2080 GPU. It takes about 1 hour for training on PAWS-X and 5 hours on XNLI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Results of Generalized Setting</head><p>Results for SALT, RS-RP and vanilla mBERT on XNLI under generalized setting are shown in Tab. 4, Tab. 5 and Tab. 6 respectively. Baseline results of RS-RP and vanilla mBERT are copied from <ref type="bibr" target="#b21">Huang et al. (2021)</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: SALT distills the cross-lingual counterpart of each token from the multilingual PLM for codeswitching and applies embedding mixup to improve the diversity of self-augmented features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>You see that on television also.</figDesc><table><row><cell>Offline Code-Switching</cell><cell cols="2">Online Embedding Mixup</cell></row><row><cell>You see that on tele también.</cell><cell cols="2">outputs</cell></row><row><cell>mBERT</cell><cell cols="2">mBERT</cell></row><row><cell>(pretrained)</cell><cell cols="2">(finetuning)</cell></row><row><cell></cell><cell>⨁</cell><cell>random mixup ratio</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Average accuracy of zero-shot cross-lingual transfer on XNLI with 5 different random seeds. We provide detailed results of 15 languages and their average (w/ and w/o English). The highest scores are in bold. Significant improvements in comparison with mBERT baseline by t-test (p ≤ 0.05) are underlined. We also provide results of methods with external data as the referenced upper bound. * We reproduce mBERT with our code base. Other baseline results are from previous papers.</figDesc><table><row><cell>Then</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Average accuracy of zero-shot cross-lingual transfer on PAWS-X.</figDesc><table><row><cell>Model</cell><cell></cell><cell>en</cell><cell>de</cell><cell>es</cell><cell>fr</cell><cell>ja</cell><cell>ko</cell><cell>zh</cell><cell>avg.</cell><cell>w/ en</cell></row><row><cell cols="3">without external data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mBERT</cell><cell></cell><cell>94.0</cell><cell>85.7</cell><cell>87.4</cell><cell>87.0</cell><cell>73.0</cell><cell>69.6</cell><cell>77.0</cell><cell>80.0</cell><cell>82.0</cell></row><row><cell cols="2">mBERT*</cell><cell>93.9</cell><cell>84.5</cell><cell>87.9</cell><cell>87.2</cell><cell>74.2</cell><cell>76.1</cell><cell>79.8</cell><cell>81.6</cell><cell>83.4</cell></row><row><cell cols="2">+RS-ADV</cell><cell>93.7</cell><cell>86.5</cell><cell>88.5</cell><cell>87.8</cell><cell>76.1</cell><cell>75.3</cell><cell>80.4</cell><cell>82.4</cell><cell>84.0</cell></row><row><cell>+RS-RP</cell><cell></cell><cell>94.5</cell><cell>87.4</cell><cell>90.0</cell><cell>89.5</cell><cell>77.9</cell><cell>77.5</cell><cell>82.0</cell><cell>84.1</cell><cell>85.5</cell></row><row><cell>+SALT</cell><cell></cell><cell>94.2</cell><cell>87.9</cell><cell>89.9</cell><cell>89.1</cell><cell>78.6</cell><cell>77.4</cell><cell>81.8</cell><cell>84.1</cell><cell>85.5</cell></row><row><cell cols="8">with external data (not directly comparable to our approach)</cell><cell></cell><cell></cell></row><row><cell>+RS-DA</cell><cell></cell><cell>93.5</cell><cell>87.8</cell><cell>88.8</cell><cell>88.8</cell><cell>79.3</cell><cell>78.3</cell><cell>81.5</cell><cell>84.1</cell><cell>85.4</cell></row><row><cell cols="2">+CoSDA-ML</cell><cell>-</cell><cell>87.3</cell><cell>90.0</cell><cell>89.6</cell><cell>79.4</cell><cell>79.5</cell><cell>83.0</cell><cell>84.8</cell><cell>-</cell></row><row><cell cols="2">+SCOPA</cell><cell>-</cell><cell>88.6</cell><cell>90.3</cell><cell>89.7</cell><cell>81.5</cell><cell>80.1</cell><cell>84.1</cell><cell>85.7</cell><cell>-</cell></row><row><cell>Model</cell><cell>avg.</cell><cell cols="2">avg. (incl. en)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SALT</cell><cell>84.1</cell><cell cols="2">85.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-en-only</cell><cell>83.4</cell><cell cols="2">84.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-w/o mixup</cell><cell>83.3</cell><cell cols="2">84.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mBERT</cell><cell>81.6</cell><cell cols="2">83.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note><p>Ablation study on PAWS-X test set. en-only indicates only substitute original tokens to other English tokens. w/o mixup means embedding mixup is not used.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>. Results for SALT on XNLI. .7 56.1 57.3 54.4 57.0 53.2 43.7 40.8 54.1 56.3 54.6 56.9 57.9 49.9 54.0 tr 62.4 59.2 57.0 58.6 56.7 57.9 54.2 43.7 40.9 54.8 55.1 54.9 52.2 48.8 59.7 54.4 avg. 67.3 63.7 60.3 62.9 59.1 61.2 56.4 44.6 41.4 59.0 57.5 58.8 52.4 49.4 50.9 56.3</figDesc><table><row><cell>en</cell><cell>es</cell><cell>de</cell><cell>fr</cell><cell>bg</cell><cell>ru</cell><cell>el</cell><cell>th</cell><cell>sw</cell><cell>vi</cell><cell>ar</cell><cell>zh</cell><cell>hi</cell><cell>ur</cell><cell>tr avg.</cell></row><row><cell cols="15">en 82.9 73.2 67.5 72.3 66.5 67.4 60.7 46.6 40.4 67.4 62.1 68.7 56.9 53.6 55.2 62.8</cell></row><row><cell cols="15">es 74.9 75.4 64.6 70.7 64.4 66.1 61.1 44.9 39.7 64.6 61.0 63.8 55.0 50.2 54.3 60.7</cell></row><row><cell cols="15">de 72.9 68.9 70.0 68.0 63.9 66.3 59.4 45.1 40.3 63.5 60.5 62.7 56.8 54.2 55.1 60.5</cell></row><row><cell cols="15">fr 75.9 72.2 66.0 74.3 64.2 65.3 60.6 45.5 39.7 65.2 62.0 64.5 55.4 52.0 54.7 61.2</cell></row><row><cell cols="15">bg 69.8 66.0 61.6 64.2 69.6 66.5 59.3 45.5 38.9 60.2 60.2 60.4 54.8 50.6 53.0 58.7</cell></row><row><cell cols="15">ru 70.6 67.0 63.3 65.4 65.0 69.8 58.2 44.2 38.7 61.6 59.9 61.1 54.3 50.1 52.9 58.8</cell></row><row><cell cols="15">el 64.9 63.7 59.1 62.2 60.3 60.5 67.9 45.6 40.2 59.4 59.0 56.3 53.0 49.8 52.1 56.9</cell></row><row><cell cols="15">th 55.0 52.9 49.8 52.0 51.5 51.7 50.8 53.5 37.9 53.2 52.5 51.1 48.4 47.1 46.0 50.2</cell></row><row><cell cols="15">sw 54.2 52.6 48.9 50.5 49.9 49.5 49.2 42.6 50.3 49.9 50.8 49.7 47.5 47.1 47.2 49.3</cell></row><row><cell cols="15">vi 69.8 63.3 59.2 63.1 58.2 61.1 57.0 46.3 38.5 71.0 57.3 64.8 52.9 49.1 49.2 57.4</cell></row><row><cell cols="15">ar 65.7 62.6 57.6 61.4 59.2 59.9 57.1 45.5 39.2 59.3 66.3 58.0 54.1 51.5 51.9 56.6</cell></row><row><cell cols="15">zh 69.8 62.5 58.4 61.7 57.7 59.5 53.1 43.9 38.3 63.0 57.0 70.3 50.8 48.0 49.4 56.2</cell></row><row><cell cols="15">hi 62.2 57.8 56.4 57.4 56.5 57.5 54.9 45.4 38.7 56.5 56.3 56.4 61.9 55.6 52.0 55.0</cell></row><row><cell cols="15">ur 61.0 54.6 54.3 56.6 52.9 55.1 51.2 43.6 38.9 54.0 55.3 54.3 57.2 58.9 50.1 53.2</cell></row><row><cell cols="15">tr 62.2 57.8 56.0 56.8 56.3 55.5 54.2 43.7 39.9 55.4 55.4 55.7 54.0 51.4 62.2 54.4</cell></row><row><cell cols="15">avg. 67.5 63.4 59.5 62.4 59.7 60.8 57.0 45.5 40.0 60.3 58.4 59.8 54.2 51.3 52.3 56.8</cell></row><row><cell>en</cell><cell>es</cell><cell>de</cell><cell>fr</cell><cell>bg</cell><cell>ru</cell><cell>el</cell><cell>th</cell><cell>sw</cell><cell>vi</cell><cell>ar</cell><cell>zh</cell><cell>hi</cell><cell>ur</cell><cell>tr avg.</cell></row><row><cell cols="15">en 82.6 71.2 65.9 70.3 62.0 65.7 57.0 44.1 40.9 64.1 58.9 65.7 52.8 49.2 51.2 60.1</cell></row><row><cell cols="15">es 74.9 75.0 65.4 71.2 63.0 65.6 59.5 44.5 40.8 62.9 60.1 62.6 52.5 48.7 51.7 59.9</cell></row><row><cell cols="15">de 72.6 68.0 70.5 67.4 61.7 64.9 58.0 44.4 41.4 61.0 58.8 61.4 53.6 50.4 52.2 59.1</cell></row><row><cell cols="15">fr 74.7 71.6 65.2 74.1 62.1 64.8 58.4 44.4 40.8 62.7 59.5 62.4 52.7 48.9 51.7 59.6</cell></row><row><cell cols="15">bg 68.5 66.0 62.9 65.1 68.7 66.8 59.4 45.1 41.1 59.7 59.4 59.7 53.5 49.6 51.8 58.5</cell></row><row><cell cols="15">ru 69.9 67.1 63.5 65.9 65.0 69.5 58.2 44.7 40.9 60.8 59.2 60.6 53.1 49.5 51.5 58.6</cell></row><row><cell cols="15">el 63.9 63.3 59.3 62.0 59.8 61.0 67.2 44.7 41.3 57.3 57.7 55.7 51.4 48.2 50.7 56.2</cell></row><row><cell cols="15">th 56.4 54.1 51.7 53.3 51.9 52.9 51.0 50.5 40.1 52.5 51.8 51.3 48.2 46.3 46.8 50.6</cell></row><row><cell cols="15">sw 54.1 52.3 49.6 50.9 49.1 49.7 48.6 41.8 48.4 48.7 49.8 48.1 45.4 44.5 47.1 48.6</cell></row><row><cell cols="15">vi 69.9 65.0 60.5 64.1 58.6 61.8 56.0 45.1 40.4 70.5 57.4 63.4 51.5 48.0 49.1 57.4</cell></row><row><cell cols="15">ar 64.9 62.8 58.7 61.7 58.7 60.7 56.3 44.7 41.1 58.1 65.4 57.1 52.2 49.6 50.3 56.1</cell></row><row><cell cols="15">zh 71.1 64.8 60.8 64.1 59.0 61.6 53.9 43.5 40.8 63.0 56.8 69.7 50.9 47.8 49.7 57.2</cell></row><row><cell cols="15">hi 62.2 58.9 56.7 57.9 56.5 58.3 54.3 44.5 40.8 55.3 55.8 55.3 59.8 54.2 50.4 54.7</cell></row><row><cell cols="2">ur 61.2 56</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results for RS-RP onXNLI.  82.3 70.3 65.8 69.7 60.5 63.1 55.3 44.6 41.1 63.9 57.7 64.6 52.0 49.5 52.3 59.5  es 73.5 74.3 62.9 69.0 60.5 63.7 57.3 44.6 40.6 61.4 57.9 60.8 50.4 47.1 51.6 58.4  de 71.8 65.5 70.8 65.6 59.5 63.3 55.8 44.3  41.0 60.2 56.5 60.1 52.5 49.4 52.0 57.9 fr 73.6 69.0 64.0 73.8 59.5 63.1 55.7 44.1 40.5 62.2 57.3 61.6 51.1 48.5 51.8 58.4 bg 67.8 63.7 60.8 62.5 68.2 64.2 56.0 44.2 39.9 57.4 56.3 57.8 51.2 47.2 50.3 56.5 ru 69.1 65.2 62.6 64.4 62.7 68.7 55.0 44.2 39.9 59.0 56.7 58.6 50.6 46.8 50.0 56.9 el 62.7 61.4 58.0 60.2 57.1 57.7 66.4 44.4 40.5 56.4 55.6 54.0 49.6 46.8 50.7 54.8 th 54.8 52.0 49.9 51.3 49.1 50.4 49.0 53.0 39.4 51.1 49.9 49.3 45.9 44.8 45.4 49.0 sw 54.2 51.2 48.7 50.5 47.2 47.9 47.9 41.8 50.0 48.5 49.1 48.5 45.4 44.4 45.8 48.1 vi 67.4 60.3 57.4 61.2 52.9 57.1 52.9 44.2 39.8 70.3 53.3 62.0 49.2 45.9 47.5 54.8 ar 63.9 60.4 57.0 59.5 54.5 57.1 53.3 43.9 40.4 55.4 64.8 55.2 50.3 48.4 49.9 54.3 zh 67.9 59.9 57.2 59.9 53.4 56.5 50.4 42.7 39.6 60.8 53.5 69.2 48.0 45.7 48.0 54.2 hi 61.4 55.5 55.0 55.3 52.6 54.4 51.9 43.8 40.3 53.8 53.1 53.7 59.7 52.7 49.9 52.9 ur 60.1 54.0 53.9 55.1 48.8 51.5 49.6 41.9 39.7 50.0 52.1 52.3 54.4 57.7 48.2 51.3 tr 61.0 55.1 53.6 55.1 52.0 52.6 50.9 42.4 40.7 52.3 52.0 53.2 49.7 47.3 60.9 51.9 avg. 66.1 61.2 58.5 60.9 55.9 58.1 53.8 44.3 40.9 57.5 55.1 57.4 50.7 48.1 50.3 54.6</figDesc><table><row><cell>en</cell><cell>es</cell><cell>de</cell><cell>fr</cell><cell>bg</cell><cell>ru</cell><cell>el</cell><cell>th</cell><cell>sw</cell><cell>vi</cell><cell>ar</cell><cell>zh</cell><cell>hi</cell><cell>ur</cell><cell>tr avg.</cell></row></table><note><p>en</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Results for mBERT on XNLI.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We adopt the top 10,000 words by frequency in target languages at https://en.wiktionary.org/wiki/ Wiktionary:Frequency_lists.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>We compute the overlap between the model's vocabulary and the word list of each evaluation language, and select the top three languages with the highest overlap ratio. Note that Chinese, Japanese, and Korean are not selected because their words will be tokenized to characters by the model tokenizer used in this study.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We appreciate the reviewers for their insightful comments and suggestions. <rs type="person">Fei Wang</rs> was supported by the <rs type="funder">Annenberg Fellowship at USC</rs> and the <rs type="grantName">Amazon ML Fellowship</rs>. <rs type="person">Muhao Chen</rs> was supported by the <rs type="funder">NSF</rs> Grants <rs type="grantNumber">IIS 2105329</rs> and <rs type="grantNumber">ITE 2333736</rs>, by the <rs type="funder">Air Force Research Laboratory</rs> under agreement number <rs type="grantNumber">FA8750-20-2-10002</rs>, by two <rs type="funder">Amazon Research Awards</rs> and a <rs type="funder">Cisco Research Award</rs>. Computing of this work was partly supported by a subaward of NSF Cloudbank 1925001 through UCSD.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_cdWSSKB">
					<orgName type="grant-name">Amazon ML Fellowship</orgName>
				</org>
				<org type="funding" xml:id="_fw6RTbz">
					<idno type="grant-number">IIS 2105329</idno>
				</org>
				<org type="funding" xml:id="_WYfA8zA">
					<idno type="grant-number">ITE 2333736</idno>
				</org>
				<org type="funding" xml:id="_E4KchGX">
					<idno type="grant-number">FA8750-20-2-10002</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>In this study, we adopt a fixed threshold to select tokens for code-switching. However, the optimal thresholds for different languages and instances can vary. Future research can develop efficient search algorithm to optimize the thresholds. While we have limited the proposed technique to descriminative natural language understanding tasks, future research can extend the proposed technique to generative multilingual PLMs, such as mT5 <ref type="bibr">(Xue et al., 2021)</ref> and mBART <ref type="bibr">(Liu et al., 2020a)</ref>, on text generation tasks <ref type="bibr" target="#b15">(Duan et al., 2019;</ref><ref type="bibr" target="#b9">Chen et al., 2021)</ref>. Furthermore, we have opted for English as the source language. However, extending the application of SALT to other source languages could enhance the comprehensiveness of this study.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On difficulties of cross-lingual transfer with order differences: A case study on dependency parsing</title>
		<author>
			<persName><forename type="first">Wasi</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhisong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2440" to="2452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2021a. Syntax-augmented multilingual BERT for cross-lingual transfer</title>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Wasi Uddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">GATE: graph attention transformer encoder for cross-lingual relation and event extraction</title>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Wasi Uddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating natural language adversarial examples</title>
		<author>
			<persName><forename type="first">Moustafa</forename><surname>Alzantot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-Jhang</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mani</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1316</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2890" to="2896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the cross-lingual transferability of monolingual representations</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="597" to="610" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multilingual alignment of contextual word representations</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Crosslingual ability of multilingual masked language models: A study of language structure</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaobo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multieurlex-a multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer</title>
		<author>
			<persName><forename type="first">Ilias</forename><surname>Chalkidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6974" to="6996" />
		</imprint>
	</monogr>
	<note>Manos Fergadiotis, and Ion Androutsopoulos</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Zero-shot cross-lingual transfer of neural machine translation with multilingual pretrained encoders</title>
		<author>
			<persName><forename type="first">Guanhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Infoxlm: An information-theoretic framework for cross-lingual language model pre-training</title>
		<author>
			<persName><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">2020a. Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Édouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Crosslingual language model pretraining</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems. NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Emerging cross-lingual structure in pretrained language models</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Human Language Technologies (NAACL-HLT</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Zero-shot crosslingual abstractive sentence summarization through teaching generation and attention</title>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Xiangyu Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3162" to="3172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Identifying necessary elements for bert&apos;s multilinguality</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Dufter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Language-agnostic BERT sentence embedding</title>
		<author>
			<persName><forename type="first">Fangxiaoyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A survey of data augmentation approaches for NLP</title>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Gangal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soroush</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics (ACL-Findings)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Augmenting data with mixup for sentence classification: An empirical study</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08941</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation</title>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning (ICML)</title>
		<meeting>the 37th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving zero-shot cross-lingual transfer learning via robust training</title>
		<author>
			<persName><forename type="first">Kuan-Hao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wasi</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1684" to="1697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multilingual generative language models for zero-shot crosslingual event argument extraction</title>
		<author>
			<persName><forename type="first">Kuan-Hao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I-Hung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multilingual codeswitching for zero-shot cross-lingual intent prediction and slot filling</title>
		<author>
			<persName><forename type="first">Jitin</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Multilingual Representation Learning</title>
		<meeting>the 1st Workshop on Multilingual Representation Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="211" to="223" />
		</imprint>
	</monogr>
	<note>Hemant Purohit, and Huzefa Rangwala</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Byung-gon Chun, and Seung-won Hwang. 2021. Scopa: Soft code-switching and pairwise alignment for zero-shot cross-lingual transfer</title>
		<author>
			<persName><forename type="first">Dohyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaeseong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gyewon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<biblScope unit="page" from="3176" to="3180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">MLQA: evaluating cross-lingual extractive question answering</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruty</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="726" to="742" />
		</imprint>
	</monogr>
	<note>Transactions of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention-informed mixed-language training for zero-shot cross-lingual task-oriented dialogue systems</title>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Genta</forename><surname>Indra Winata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaojiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Preserving cross-linguality of pre-trained models via continual learning</title>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Genta</forename><surname>Indra Winata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Representation Learning for NLP</title>
		<meeting>the 6th Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="64" to="71" />
		</imprint>
	</monogr>
	<note>RepL4NLP-2021</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised dependency parsing with transferring distribution via parallel guidance and entropy regularization</title>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1337" to="1348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Target language-aware constrained inference for cross-lingual dependency parsing</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Zero-shot cross-lingual transfer with meta learning</title>
		<author>
			<persName><forename type="first">Farhad</forename><surname>Nooralahzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Bjerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">How multilingual is multilingual bert?</title>
		<author>
			<persName><forename type="first">Telmo</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4996" to="5001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cosda-ml: multi-lingual code-switching data augmentation for zero-shot cross-lingual nlp</title>
		<author>
			<persName><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minheng</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3853" to="3860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">XTREME-R: towards more challenging and nuanced multilingual evaluation</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cross-lingual structure transfer for relation and event extraction</title>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Subburathinam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">On learning universal representations across languages</title>
		<author>
			<persName><forename type="first">Xiangpeng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongxiang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luxi</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In 9th International Conference on Learning Representations (ICLR</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-ofthe-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingqiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ru</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangzhu</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07176</idno>
		<title level="m">Sas: Self-augmented strategy for language model pretraining</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Aditya Barua, and Colin Raffel. 2021. mt5: A massively multilingual pre-trained text-to-text transformer</title>
		<author>
			<persName><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<biblScope unit="page" from="483" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Enhancing cross-lingual transfer by manifold mixup</title>
		<author>
			<persName><forename type="first">Huiyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huadong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
