<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Linguistic Productivity: the Case of Determiners in English</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Raquel</forename><forename type="middle">G</forename><surname>Alhama</surname></persName>
							<email>rgalhama@uvt.nl</email>
						</author>
						<author>
							<persName><forename type="first">Ruthe</forename><surname>Foushee</surname></persName>
							<email>foushee@uchicago.edu</email>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><surname>Byrne</surname></persName>
							<email>djbyrne@uchicago.edu</email>
						</author>
						<author>
							<persName><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
							<email>aettinger@uchicago.edu</email>
						</author>
						<author>
							<persName><forename type="first">Susan</forename><surname>Goldin-Meadow</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tilburg University</orgName>
								<address>
									<country>† The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Chicago</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Chicago</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Departments of Linguistics and Computer Science</orgName>
								<orgName type="institution">University of Chicago</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Departments of Psychology and Comparative Human Development</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Committee on Education</orgName>
								<orgName type="institution">University of Chicago</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Afra Alishahi Tilburg University</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Linguistic Productivity: the Case of Determiners in English</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1DA4A9C0EC8B88E8CD47606F4C39B7C4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Having heard "a pimwit", English-speakers assume that "the pimwit" is also possible. This type of productivity is attributed to syntactic categories such as NOUN and DETERMINER, but the key question is how do humans become endowed with these categories in the first place. We propose a novel approach that combines corpus analysis with computational modeling to analyze the productivity of DETER-MINER+NOUN constructions in child-produced utterances. Our experiments on two corpora of child-adult interactions using two different methods of quantifying linguistic productivity show that children do not display productivity at early stages. Using a model trained on child-directed utterances, we simulate children's developmental trajectory with great precision, suggesting that the emergence of productivity in human language can be explained without the need to postulate a priori access to syntactic categories.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Having heard "a pimwit," English-speakers know immediately that "the pimwit" is possible, even if they have not heard the phrase before. Researchers from diverse theoretical perspectives agree that this type of productivity can be explained with syntactic categories (in this case, DETERMINER and NOUN ), but the key question is how do humans become endowed with these categories in the first place. The acquisition of the English definite and indefinite determiners (the, a) has been frequently used *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Equal contribution. †</head><p>The author is currently affiliated with the Institute for Logic, Language and Computation, University of Amsterdam.</p><p>as a case study to tackle the question of whether syntactic categories are present at birth, or rather are learnt from exposure to linguistic data.</p><p>Linguistic productivity provides evidence for syntactic categories. If English-learning children have a DETERMINER category, they should be able to produce the same noun with different determiners (i.e., a child would produce both a dog and the dog) <ref type="bibr" target="#b24">(Pine et al., 2013;</ref><ref type="bibr" target="#b32">Yang, 2011)</ref>. If they do not have a DETERMINER category, a child who hears a dog may not immediately understand that the dog is also possible. On this reasoning, when a child begins to use the same noun with both a and the, it can be seen as evidence that the child possesses a productive determiner category.</p><p>Much effort is put into formalizing a quantitative measure of productivity that can be applied to spontaneous child productions and give a reliable estimate of when young language learners start to use syntactic categories productively in their speech. <ref type="bibr" target="#b25">Pine and Lieven (1997)</ref> propose such a measure, overlap score, which estimates determiner productivity based on the proportion of nouns in childproduced language that are used with both determiners. They estimate determiner productivity for children in the Manchester corpus (?) and argue that children are not fully productive at early stages of learning. <ref type="bibr" target="#b32">Yang (2011)</ref> questions the validity of the original overlap score and proposes a revised version which takes the Zipfian distribution of nouns and determiners into account. Their analysis of the new score on Manchester corpus suggests that determiner productivity in children is not quantitatively different from that of adults. However, <ref type="bibr" target="#b24">Pine et al. (2013)</ref> argue that Zipfian distributions are not a good approximation of noun frequency in children's productions. <ref type="bibr">Additionally, Meylan et al. (2017)</ref> simulate determiner productivity in a Bayesian model and show that the previous analyses use input data that is too small to yield reliable conclusions. In addition to the size of the input corpus, <ref type="bibr" target="#b19">Meylan et al. (2017)</ref> highlight another important limitation of using the overlap score. This measure is quite sensitive to the amount of child-produced data, and therefore cannot be reliably used to estimate productivity at the very early stages of learning (when children do not produce many DETERMINER+NOUN combinations). Their simulation results suggest that the youngest age groups in previous behavioural studies might not be young enough for their data to show productivity. This paper makes the following contributions. We propose a novel approach that combines corpus analysis with computational modeling. We use a data-driven computational model with no built-in categories to simulate the process of language learning from child-directed utterances. To address the limitations related to the input corpus, in addition to reproducing behavioral patterns of previous studies on the Manchester corpus, we also use the Language Development Project (LDP) corpus <ref type="bibr" target="#b10">(Goldin-Meadow et al., 2014)</ref>, which contains data from many more children and records interactions from a much younger age. To address the concerns regarding the productivity measure, we estimate DETERMINER+NOUN productivity in the utterances produced by humans and by the model not only using the overlap score but also the onset measure, a simple and data-efficient estimate of productivity <ref type="bibr" target="#b4">(Cartmill et al., 2014)</ref>. Altogether, this approach allows us to examine the developmental trajectory of the DETERMINER+NOUN construction in children, and the extent to which it can be explained by a model that has no prior access to syntactic categories. We show that the behaviour of our computational model closely mimics the patterns observed in children over time, revealing the onset of determiner productivity in both model and child. These results strongly suggest that children's linguistic productivity can be achieved based on learning from statistics of the child-directed data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Many computational models have framed the general problem of inducing abstract categories from unannotated text as clustering words into lexical categories based on the distributional properties of their context (e.g., <ref type="bibr" target="#b26">Redington et al., 1998;</ref><ref type="bibr" target="#b7">Clark, 2000;</ref><ref type="bibr" target="#b20">Mintz, 2003;</ref><ref type="bibr" target="#b22">Parisien et al., 2008;</ref><ref type="bibr" target="#b5">Chrupała, 2011)</ref>, showing the possibility of learning categories that resemble parts of speech from raw text. <ref type="bibr" target="#b2">Alishahi and Chrupała (2012)</ref> and <ref type="bibr" target="#b0">Abend et al. (2017)</ref> model concurrent acquisition of word meanings and syntactic categories and focus on the impact of integrating knowledge of syntax (and particularly the syntactic category of a word) into the word learning process. However, none of these models focus on the nature and developmental trajectory of the induced categories, nor do they compare their linguistic productivity to humans. An exception is <ref type="bibr" target="#b22">Parisien et al. (2008)</ref> who present an incremental Bayesian model for learning syntactic categories from linguistic context, and test it on child-directed data from the Manchester corpus. Their analysis of the emerging categories shows that the categories follow the same trend as children's categories in that nouns are learned before verbs, followed by adjectives <ref type="bibr" target="#b15">(Kemp et al., 2005)</ref>. However they do not analyze the emergence of each category and its use in child-produced speech.</p><p>With the increased dominance of deep neural models of language, much effort is put into analyzing the learned representations in inner layers of these models and to search for encoding syntactic information (see <ref type="bibr" target="#b3">Belinkov and Glass, 2019</ref>, for an overview). Various analyses have shown that deep language models encode information about syntactic categories and syntactic dependencies in their learned representations without explicit training (e.g., <ref type="bibr" target="#b1">Adi et al., 2017;</ref><ref type="bibr" target="#b12">Hewitt and Manning, 2019;</ref><ref type="bibr" target="#b6">Chrupała and Alishahi, 2019;</ref><ref type="bibr" target="#b27">Tayyar Madabushi et al., 2022)</ref>. However, the focus of this body of work is mainly on large language models that are trained on massive datasets (with some exceptions; see <ref type="bibr" target="#b11">Grimm et al., 2015)</ref>, and comparison with human language learning is not common. <ref type="bibr" target="#b21">Pannitto and Herbelot (2020)</ref> and <ref type="bibr" target="#b14">Huebner et al. (2021)</ref> are two exceptions, where they each train a neural network from scratch on child-directed utterances and compare the output of the model to human productions. Although these studies are not specifically focused on the emergence of syntactic categories, we use the latter as an inspiration for our own modeling experiments.</p><p>Computational studies of the acquisition of the determiner category are scarce. One such study is the above-mentioned work by <ref type="bibr" target="#b19">Meylan et al. (2017)</ref>, which used a hierarchical Bayesian model for simulating syntactic productivity in the case of DETER-MINER+NOUN constructions, with parameters to represent the role of experience and an a priori tendency to generalize. Their experiments suggest that previous corpus studies use input corpora that are too small to yield statistically reliable conclusions. They used a large dataset of child-adult interactions for a single child and found determiner-noun productivity, but only if the parameters of the model are set in a specific way to encourage generalization (as opposed to memorization). However, the Meylan et al.'s model works best when applied to a large sample of data from each child, and is not applicable in early stages of learning when childproduced language is scarce.</p><p>One other study that uses the overlap score to measure determiner productivity in a deep neural network model is <ref type="bibr" target="#b23">Phillips and Hodas (2017)</ref>. This model uses an autoencoder architecture whose objective is to reconstruct (or repeat) an input utterance, and train it on child-directed utterances from a collection of corpora from CHILDES <ref type="bibr" target="#b17">(MacWhinney, 1995)</ref>. The model learns a compact, latent representation for every incoming utterance, which is then used to regenerate the same utterance. They measure the estimated and empirical overlap scores in adult utterances from the training corpora and in the utterances generated by the autoencoder model, and show that if the model's parameters are set to allow for more generalizability, its estimated overlap scores are closer to those of humans. This study does not compare the behaviour of the trained model to the behavior of language learning children, and therefore says little about the learning trajectory of the determiner class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In the following sections, we present a series of interleaved computational experiments and behavioral analyses to examine linguistic productivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>The Manchester corpus This corpus records a study of 12 monolingual English-speaking children from middle-class households in Manchester, UK, from ages 20 to 36 months. Mothers and children were audio-recorded playing freely in their homes two times every three weeks for a year, for a maximum of 34 sessions per child. At the beginning of the study, the children ranged in age from 1;8.22 to 2;0.25 with mean length of utterances (MLUs) ranging between 1.06 to 2.27 in morphemes <ref type="bibr" target="#b28">(Theakston et al., 2001)</ref>.</p><p>The LDP corpus The Language Development Project corpus (LDP) followed 64 typically developing, monolingual, English-speaking children from the Greater Chicagoland Area. Children and their primary caregivers were video-recorded engaging in spontaneous interactions in their homes for twelve 90-minute visits (M=11.3, SD = 1.8, sessions, range 4-12 sessions), beginning from when the children were 14 months to 58 months. The resulting corpus of caregiver-child interactions contains over 1 million transcribed utterances (n = 646, 685 for primary caregivers and n = 368, 884 for children), and approximately 1,000 hours of videos <ref type="bibr" target="#b10">(Goldin-Meadow et al., 2014)</ref>.</p><p>Preprocessing Both the primary caregivers' and children's utterances were lemmatized, stripped of extraneous punctuation, and all instances of capitalization were removed. All utterances tagged as reading aloud were excluded. We identified syntactic categories using the part-of-speech taggers provided in the spaCy library <ref type="bibr" target="#b13">(Honnibal and Montani, 2017)</ref>. Our preprocessing pipeline is shared online<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Productivity Measures</head><p>We consider two methods of assessing grammatical productivity in our behavioral as well as modelgenerated data.</p><p>Overlap score Following <ref type="bibr" target="#b24">Pine et al. (2013)</ref>, we compute the overlap score as the number of noun types that the child pairs with both a and the, divided by the total number of noun types used with determiners. We refer to this metric as 'observed overlap', to distinguish it from the 'expected' overlap variant proposed in Yang (2011).<ref type="foot" target="#foot_1">2</ref> Since this metric is very sensitive to sample size <ref type="bibr" target="#b19">(Meylan et al., 2017)</ref>, we only report the score for sample sizes that consist of at least 50 productions. <ref type="foot" target="#foot_2">3</ref>Onset measure We assume that a child has productive use of the category under study when the child uses all of its forms (in our case, a and the) with the same noun, and does so with at least two different nouns within the same session (i.e., a car, the car, a bottle, the bottle). This criterion is inspired by <ref type="bibr" target="#b4">Cartmill et al. (2014)</ref>; the only difference is that we do not require that the child continue using the category in the upcoming sessions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Computational Model</head><p>Our goal is to examine the trajectory of determiner productivity in a data-driven computational model trained on child-directed data, and see to what extent it resembles the same trajectory in children. Although we do not seek to simulate the exact mechanism that children use for learning and processing language, it is important to choose a modeling framework and architecture that satisfies a number of criteria. First, the model must not rely on any data or supervision signal that is not available in children. Second, the ultimate task on which the model is trained must be similar to the daily experience of children using language. Third, the model must not rely on any explicit, latent representation of abstract syntactic categories in advance. A model that meets these requirements would allow us to study this phenomenon at Marr's computational level <ref type="bibr" target="#b18">(Marr, 2010)</ref>.</p><p>Many computational models that simulate syntactic category acquisition from large corpora falsify the first criterion, for example by relying on corrective feedback on part of speech labels assigned to words, or on which words must be clustered together (see Section 2 for an overview). Similarly, a number of existing models that are used for investigating the current debate on determiner productivity do not adhere to the second criterion; for example <ref type="bibr" target="#b23">Phillips and Hodas (2017)</ref> use an autoencoder architecture trained on a repetition task, where a model learns to regenerate input utterances, which is an unrealistic task from a language learner's point of view. Criterion three is the core of our study: we want to see how far we can go in reproducing productivity patterns in children without assuming pre-existing abstract categories.</p><p>A natural choice of model for our enterprise is Transformers <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref>. First of all, training Transformers with the parental input and the Masked Language Modelling (MLM) objective is adequate in terms of the information available in the supervision signal, which consists uniquely of information available to children (words in caregiver's utterances). Second, we can use the MLM objective to test the model by masking the determiners in children's productions. In this way, we can simulate children's productivity, thereby reproducing a more naturalistic scenario than previous approaches. Finally, this model does not rely on any pre-existing syntactic knowledge, which meets our third objective.</p><p>The existing Transformer-based models such as BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b16">(Liu et al., 2019)</ref> have become a focus for computational psycholinguistic research and used to simulate many aspects of human language processing, from reading times to brain activities (see <ref type="bibr" target="#b9">Frank et al., 2019</ref>, for an overview), but they often need much more training data than is available to human language learners. However, a much smaller variation of BERT called BabyBERTa <ref type="bibr" target="#b14">(Huebner et al., 2021)</ref> was recently proposed and trained on 5 million tokens of data directed at children between ages of one to six. They performed post-analysis on the hidden representations of this model and showed that it acquires grammatical knowledge comparable to RoBERTa pre-trained on 160GB of text. We follow this approach and use a much smaller variation of BERT in our study which we train from scratch on child-directed data.</p><p>We instantiate a small Transformer-based model using the HuggingFace library <ref type="bibr" target="#b30">(Wolf et al., 2020)</ref> and reduce the number of attention heads to 2 and the number of hidden layers to 1. We tokenize the input with WordPieceTokenizer <ref type="bibr" target="#b31">(Wu et al., 2016)</ref>, with a maximum vocabulary size of 30000 and a minimum frequency of 2. We use the default training configuration provided in HuggingFace's Trainer (Adam), with the exception of the batch size, which we increase from 8 to 64. Remaining hyperparameters are set to their default values in HuggingFace (see exact version in our shared code<ref type="foot" target="#foot_3">4</ref> ). Crucially, we do not apply any pre-training on this model; therefore, our initial model does not have any built-in linguistic knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We train our model on child-directed utterances from the corpus under study, using the MLM objective. Since the child-directed data for each individual child is not enough to train the model from scratch, we accumulate utterances of all the parents. In experiments showing a developmental trajectory over time, we train the model with the input avail-able up to each depicted age.</p><p>To test the determiner productivity of the model, we first extract all the determiner usages in the utterances produced by each individual child. Following prior studies <ref type="bibr" target="#b24">(Pine et al., 2013;</ref><ref type="bibr" target="#b32">Yang, 2011)</ref>, we mask the definite and indefinite determiners in DETERMINER+NOUN constructions; in particular "DETERMINER NOUN-SINGULAR &lt;X&gt;" and "DETERMINER &lt;X&gt; NOUN-SINGULAR", where &lt;X&gt; is any category except NOUN-SINGULAR.</p><p>As an example, for the child's utterance Here's the pink ear, we would present the model with Here's [MASK] pink ear. We then feed the utterances to the model so that it predicts the most likely filler for the masked slot. For each masked slot, we record the prediction to which the model assigns the highest probability. To make sure that the model is properly trained, we checked the accuracy of the model in predicting a determiner in the masked slot.We compute the accuracy of the predictions for the sentences produced by children in a certain session (age) with the model trained with input data up to that session. Figure <ref type="figure" target="#fig_0">1</ref> reports the accuracy of predicting the masked determiner in children's productions by the models trained on data from all sessions. In the case of the Manchester corpus, the model is 67.94% accurate in predicting the exact determiner, while it predicts any of the two determiners with a probability of 91%. For the LDP corpus, the model can predict the exact determiner with similar performance (65.48%) . The accuracy for predicting either of these two determiners is 83%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manchester Corpus LDP Corpus</head><p>Both models are more likely to predict the definite determiner (the) over the indefinite (a). This is more noticeable in the case of the Manchester corpus, in which 55.45% of the predictions are for the definite determiner (47.13% for the LDP data). This likely reflects the fact that the is more frequent in parental speech. For instance, in the case of parental data in LPD, 61% of the constructions that we study (which combine definite and indefinite determinants with singular nouns, as explained in section 4.1) use the, versus 39% using a (but note that the can also appear in the input combined with plural nouns). Despite this bias in the data, the accuracy of the model in predicting the exact determiner is above chance for both models.  Note that the overall performance of the model is underestimated as some of the model's predicted determiners that mismatch the ones used by the child (and therefore considered as errors) might be plausible choices in the given context. Table <ref type="table" target="#tab_0">1</ref> shows a few examples of such errors, some of which sound more plausible than the child's choice (e.g. You're a monster).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Our Study of Linguistic Productivity</head><p>To investigate the development of linguistic productivity in children and to simulate the process using our computational model, we conduct the following three experiments in this study.</p><p>Experiment 1. We establish the reliability of our model by reproducing the behavioral patterns observed in <ref type="bibr" target="#b24">Pine et al. (2013)</ref> and <ref type="bibr" target="#b32">Yang (2011)</ref>: we train our model on child-directed utterances from the Manchester corpus, and use the trained model to predict the masked determiners in childproduced sentences in the same corpus. Following those studies, we estimate and compare overlap scores in the utterances produced by adults, children and our model.</p><p>Experiment 2. We train the model on childdirected utterances from the LDP corpus, which contains data from more children and records interactions from a younger age. We then compare the estimated overlap scores in the utterances produced by adults, children and our model. Experiment 3. We use the same model trained on child-directed utterances from the LDP corpus, but this time we compare the patterns of production of DETERMINER+NOUN combinations by adults, children and by our model using the onset measure.</p><p>Through these experiments, we show that our model closely mimics the behaviour of children when using determiners, but the use of LDP corpus and the choice of the onset measure presents a clearer picture of emergence of determiners as a syntactic class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment 1: Overlap Score on Manchester Corpus</head><p>To reproduce previous studies, we first train our model on child-directed utterances from all parents in the Manchester corpus. After the model is fully trained, for each of the 12 children in this corpus we take all the sentences that contain a DE-TERMINER+NOUN usage, mask the determiner and feed the masked input to the model, then replace the masked word with the top determiner predicted by the model (see Section 4 for details).</p><p>Figure <ref type="figure" target="#fig_2">2</ref> shows the observed overlap scores as measured for the sentences produced by the Manchester parents, children, and our model. We see that there is no noticeable difference between overlap scores of children and adults, which is in line with what <ref type="bibr" target="#b32">Yang (2011</ref><ref type="bibr" target="#b33">Yang ( , 2013) )</ref> reports. However, the overlap scores estimated for utterances produced by our model are closer to those of children than to adults.</p><p>Due to the small size of the Manchester corpus, we could not train the model on this data incrementally and trace the trajectory of overlap scores as a function of the age of children in this corpus. We address this issue in the next experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiment 2: Overlap Score on LDP Corpus</head><p>Although <ref type="bibr" target="#b32">Yang (2011)</ref> interprets similarity between overlap scores for children and adults as evidence for the availability of a priori syntactic categories, children participating in the earliest sessions of the Manchester corpus might already be old enough to have acquired the abstract determiner class. In addition, the sample of children (n=12) is quite small <ref type="bibr" target="#b19">(Meylan et al., 2017)</ref>. Therefore we perform the same experiment on the LDP corpus.</p><p>We train the model on child-directed utterances from the LDP corpus and use it to predict masked determiners in child-produced utterances. Figure <ref type="figure" target="#fig_3">3</ref> shows overlap scores for utterances produced by LDP parents, children and our model, where children (on the x axis) are ranked by their estimated overlap scores. As before, the overlap scores for all three groups are close to each other, with those for the model closer to children than adults. There is a stronger fit to the predictions of our model and the LDP children compared to those of the model and adults. To see whether the distance between the overlap score for children and adults changes as children grow older, we also estimated overlap scores averaged over all children within the same session; that is, we accumulated all the utterances that parents and children were producing at a given point in time. This time the pattern is different: as can be seen in Figure <ref type="figure" target="#fig_4">4</ref>, there is a gap between overlap scores for parents and children in the earlier sessions, which disappears in the later sessions. However, due to the small number of utterances produced by children at earlier sessions, the estimation of overlap score is not reliable (in fact we could not apply the measurement to earlier ages because the sample size was smaller than 50; see Section 3.2). As before, overlap scores estimated for the model are closer to children than adults.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiment 3: Onset on LDP Corpus</head><p>To address the limitations of the overlap score and get a better picture of determiner productivity in   early stages of development, we repeat the previous experiment on LDP corpus but this time we use the onset measure to estimate linguistic productivity.</p><p>Figure <ref type="figure" target="#fig_5">5</ref> shows the median number of noun types that appear with both determiners in each session; the dashed black line corresponds to the onset criterion (i.e., when a minimum of two noun types has been used with both determiners in the same session). The points above this line correspond to learners who have achieved determiner productiv-ity. We see that while productivity is relatively constant in parental speech, children use determiners for several months without displaying productivity. This onset pattern is replicated in the model. As in the previous experiments, we can see that the model shows an excellent fit to the trajectory of children. The small decrease in productivity at the older ages is due to the fact that both children and parents produced fewer utterances overall at these points. In Appendix D we have included a  A random jitter of 0.5 has been added to overlapping points in the graph. graphical demonstration of this pattern for each individual child in LDP to show that this fit is not a result of averaging over all children.</p><p>In addition to producing more noun types with both determiners over time, the moment of onset of determiner productivity in the model also shows a strong correlation with the moment of onset in children. Figure <ref type="figure" target="#fig_6">6</ref> shows this correlation for all the children in LDP and their corresponding model simulation.</p><p>It is worth mentioning that the onset measure also depends on the amount of input produced by children, as there is a correlation between the median number of words produced by children and the number of productive noun types in each session (Kendall's rank correlation τ = -0.25, p=.007); however, we found that 84% of the children had already produced two exemplars of two different nouns in at least one session prior to the hypothesized onset, but without passing the onset criterion. In other words, the children could have displayed productivity, but did not.</p><p>Impact of linguistic context. A possible concern could be that the similarity between the predictions of the model and children might be due to properties of child-produced utterances, rather than the internal representations of syntactic categories. To investigate this potential confound, we ran a control experiment in which we tested the model also on utterances that the parents produced (see Appendix E for details). We found no significant difference between model predictions on parent-produced versus child-produced utterances, suggesting that our findings are not driven by the context used for the test.</p><p>Productivity vs. memorizing. Given the goodness of fit of the model, there is the possibility that the model simply memorizes DETERMINER+NOUN usages it has seen in the training data instead of productively using abstract categories. Since (unlike children) we have full access to the input of the model, we can search for the novel combinations in its output. Table <ref type="table" target="#tab_1">2</ref> shows some examples where the model produces a determiner it has never seen before in combination with the target noun, but is more appropriate for the context. This is an important observation which hints at the abstract nature of the emerging syntactic categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we investigate how and when English-speaking children productively use We furthermore train a Transformer-based model from scratch on child-directed utterances, and simulate determiner production in children. Our model mimics children's developmental trajectory with great precision, suggesting that the emergence of productivity in human language can be explained without the need to postulate access to pre-existing syntactic categories.</p><p>Our hybrid approach is independent of the target syntactic construction. In the future, we plan to apply this methodology to a range of syntactic phenomena, and investigate the trajectory as well as the order of development of different syntactic categories in children exposed to different languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>In this study we combined child-directed data from all caregivers in our corpus to train the computational model, and only used the individual childproduced utterances as test cases for each version of the model. This was due to lack of enough training data, but in the future we must think about creative ways of simulating individual differences (for example by pre-training the model on a generic child-directed corpus, and fine-tune each instance of the model on child-directed data from individual children in our corpus).</p><p>In addition, we focused on the development of only one syntactic category, and only in English. This is due to historical and practical reasons: English determiners have been used as a case study in this domain, likely due to the simplicity of the DE-TERMINER+NOUN construction (which involves closed vocabulary items rather than an open lexical class) and the fact that its acquisition seems to start relatively early. However, there is nothing in the empirical and computational framework we used here that is specific to this particular case, and the same approach can be applied to any other languages and prominent categories such as nouns, verbs, adjectives and adverbs. Comparing their developmental pattern in the computational model can allow us to investigate the order of development often hypothesized in children (e.g. nouns are learned before verbs, adjectives become productive much later, etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>We used the LDP dataset Goldin-Meadow et al., 2014, a corpus that was collected from spontaneous parent-child interactions recorded in a homesetting. This rather personal, intimate setting lends itself to the frequent use of Personally Identifiable Information (PII), either directly in utterances or indirectly through personal artifacts.</p><p>The data collection protocol (approved by University of Chicago, IRB protocols 02-942 and H12078) determined the following privacy considerations:</p><p>1. Subjects were issued subject numbers.</p><p>2. Paper files were identified by number only and kept in locked file drawers.</p><p>3. Electronic files were stored on passwordprotected computers that were accessible only research team members. Other researchers only had access to coded data.</p><p>4. Videos were stored on a securely managed, password-protected server and each researcher was given his/her own secure login and password. Any local copies of videos were deleted.</p><p>5. Videos were not linked to any other identifying information except what is contained in the video.</p><p>6. During transcription, identifiers were removed.</p><p>7. The master list was stored in a passwordprotected computer, was kept separate from data and was only kept during data collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A The LDP Corpus: Recruitment and Demographics</head><p>The data collection of the LDP corpus is fully described in <ref type="bibr" target="#b10">Goldin-Meadow et al. (2014)</ref>. To recruit participants, advertisements were posted in the Chicago Parent magazine, a recruitment letter was written and sent out, flyers were posted, and daycare centers were contacted. 50$ were given per visit to parents for an approximately 2-hourlong visit. Parents who responded participated in a screening questionnaire over the phone during which information was gathered on ethnicity, income, education, language(s) spoken in the home, and child gender. Sixty-four English-speaking families were selected to match as closely as possible the 2000 census data on family income and ethnicity in the greater Chicago area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Computational Simulations</head><p>In this section we report additional details of the computational simulations. The number of parameters of our model is much smaller than that of the original BERT model (16960546 for the Manchester data, and 17373499 for LDP 5 ). We set the hyperparameters of the models (specified in section 4.1) manually, after a few trials. Given the smaller size of models and data, they can even be trained on a laptop with a modest CUDA-compatible GPU (training time is shorter than 8 hours in an MSI Prestige A10SC-006NL with an NVIDIA GeForce GTX 1650).</p><p>The training data for the Manchester corpus consists of 351223 child-directed sentences and 1767057 word tokens. In the case of the LDP corpus, the full size of the training data (also consisting of child-directed sentences) is 646040 sentences and 2544468 word tokens.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Average Maximum Probability</head><p>Our model provides a probability distribution over the vocabulary items, which is then used to predict the word that should fill in the masked slot. As a decoding strategy, we choose the word with maximum probability (the mode). To make sure that this is not an ill-founded choice (as the probability mass attributed to the mode may still be a small percentage of the overall mass), we report the average probability mass that the model attributed to the predicted words.</p><p>For the model trained on the Manchester corpus, the average probability mass for the preferred determiner (for predictions of the exact same determiner) was 0.41±0.24. This is quite substantial, considering that the model needs to distribute this probability between all the units in its vocabulary. If we consider all the sentences for which the model predicted a determiner, then the average probability mass of the top prediction is 0.37±0.23. The average maximum probability across all the tested sentences is 0.24 ±0.19.</p><p>A similar pattern is observed for LDP, although in this case the model has higher certainty, likely because of the greater amount of training data. When trained on this dataset, the average probability mass for the preferred determiner (for predictions of the exact same determiner) was 0.52±0.21. When considering all the sentences for which the model predicted a determiner, then the average probability mass of the top prediction is 0.49±0.21. The average maximum probability across all the tested sentences is 0.46 ±0.22.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Onset measure applied to individual children in LDP</head><p>Figure <ref type="figure" target="#fig_8">8</ref> shows the Onset metric applied to LDP data, for children and model predictions. utterances produced by the individual child).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Control Experiment</head><p>To control for the possibility that the good fit of our model to the children data was due to testing the model using child-produced speech, we ran the following experiment: for each session, we sampled utterances with determiner constructions (n=48) separately for child utterances and for parent utterances, and used them as test frames for the model. We computed the productivity of our model for each set of sentences and performed a paired statistical test. We repeated this experiment 10 times, and none of these tests yielded a significant difference between predictions for determiner-noun test frames taken from parent speech and predictions for determiner-noun test frames taken from child speech. This result rules out the hypothesis that our findings are due to the linguistic context of childproduced speech. The results can be found in Table <ref type="table" target="#tab_6">6</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Confidence Matrix of masked determiner predictions for the model trained on all sessions in Manchester corpus (top) and in LDP corpus (bottom).</figDesc><graphic coords="5,70.87,554.95,226.99,170.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(The average probability mass of the decoded determiners can be found in Appendix C.) Child Model You're the monster You're a monster I see a moon I see the moon Up a stair Up the stair Is this a bug? Is this the bug? Draw a train track Draw the train track Tickle the back Tickle a back No, I want the penguin story No, I want a penguin story He take a bathroom He take the bathroom</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overlap over Manchester corpus. The x-axis is sorted by observed overlap in child-produced language.</figDesc><graphic coords="7,73.13,70.86,449.02,149.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Observed overlap over LDP corpus.</figDesc><graphic coords="7,73.13,254.41,449.02,149.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Observed overlap over LDP corpus, averaged by age group.</figDesc><graphic coords="7,73.13,437.95,449.02,149.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Median number of noun types that are produced with both determiners. The dashed horizontal line represents the lower boundary for our onset criterion (i.e., when two different nouns are each produced with both determiners). Error bars correspond to standard deviation.</figDesc><graphic coords="8,70.87,70.87,453.55,151.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Age of productivity according to the onset measure for LDP children and the model (r = 0.71).A random jitter of 0.5 has been added to overlapping points in the graph.</figDesc><graphic coords="8,70.87,287.80,218.27,163.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Histogram of the size of the test data by individual child in the LDP corpus, for n=63 children (the data of 64 th child was deemed unusable due to not having enough observations).</figDesc><graphic coords="13,71.96,70.87,216.09,162.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Number of noun types that appear with both determiners, for individual children (orange dots) and corresponding model (blue line). Dashed line corresponds to lower boundary to achieve our criterion for productivity (2 nouns with 2 determiners). Horizontal axis represents age, in months old.</figDesc><graphic coords="14,70.87,138.87,453.54,518.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Some examples where the model produces a different determiner from the child.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>DETER-Examples of model-produced DETER-MINER+NOUN combinations that were not in the training data. Age is in months old.MINER+NOUN constructions. Our experiments on two corpora of child-adult interactions suggest that children do not use determiners productively when they first produce them. It isn't until 30 months that children display productivity in their use of determiners. Thanks to the use of the data-efficient onset measure, this outcome is less dependent on sampling effects and therefore more reliable.</figDesc><table><row><cell>Age</cell><cell>Model</cell><cell>Noun</cell></row><row><cell cols="2">(m.o.) Prediction</cell><cell>Frequency</cell></row><row><cell>22</cell><cell>Take a milkie</cell><cell>9</cell></row><row><cell>26</cell><cell>Sylva has a flu</cell><cell>5</cell></row><row><cell>30</cell><cell>That's a daffodil</cell><cell>3</cell></row><row><cell>42</cell><cell>You just call him an idiot</cell><cell>0</cell></row><row><cell>46</cell><cell>Do you want to have a boyfriend?</cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table 3 reports the training data available to the model at each age, for the version of the model trained incrementally. To test the model, we use child-produced (rather than child directed) sentences, hence the training and test data do not have any overlap. In the case of the Manchester corpus, the number of sentences used to test the model are enumerated in Table 4, separated by individual child. The equivalent information for the LDP corpus is summarized in Figure 7 (given the greater number of children, this more compact visualization is more convenient 5 Note that parameters vary with vocabulary size. than a table). The size of the test data used to analyze the model by age is presented in Table 5.</figDesc><table><row><cell cols="3">Age Sentences Words (tokens)</cell></row><row><cell>14</cell><cell>63649</cell><cell>214612</cell></row><row><cell>18</cell><cell>64660</cell><cell>219182</cell></row><row><cell>22</cell><cell>60574</cell><cell>215085</cell></row><row><cell>26</cell><cell>59538</cell><cell>225807</cell></row><row><cell>30</cell><cell>57358</cell><cell>225877</cell></row><row><cell>34</cell><cell>53787</cell><cell>217869</cell></row><row><cell>38</cell><cell>59619</cell><cell>239189</cell></row><row><cell>42</cell><cell>53237</cell><cell>215223</cell></row><row><cell>46</cell><cell>47364</cell><cell>196980</cell></row><row><cell>50</cell><cell>46950</cell><cell>208677</cell></row><row><cell>54</cell><cell>39555</cell><cell>175709</cell></row><row><cell>58</cell><cell>41128</cell><cell>190258</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Training data size for LDP, by children's age.</figDesc><table><row><cell></cell><cell>DETERMINER+NOUN</cell></row><row><cell>Child</cell><cell>Sentences</cell></row><row><cell>Anne</cell><cell>1133</cell></row><row><cell>Aran</cell><cell>1731</cell></row><row><cell>Becky</cell><cell>1340</cell></row><row><cell>Carl</cell><cell>3627</cell></row><row><cell>Dominic</cell><cell>389</cell></row><row><cell>Gail</cell><cell>848</cell></row><row><cell>Joel</cell><cell>1080</cell></row><row><cell>John</cell><cell>1755</cell></row><row><cell>Liz</cell><cell>1392</cell></row><row><cell>Nicole</cell><cell>690</cell></row><row><cell>Ruth</cell><cell>792</cell></row><row><cell>Warren</cell><cell>2256</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Test data size of Manchester corpus, by child.</figDesc><table><row><cell></cell><cell>DETERMINER+NOUN</cell></row><row><cell>Age</cell><cell>Sentences</cell></row><row><cell>14</cell><cell>51</cell></row><row><cell>18</cell><cell>414</cell></row><row><cell>22</cell><cell>850</cell></row><row><cell>26</cell><cell>2062</cell></row><row><cell>30</cell><cell>3073</cell></row><row><cell>34</cell><cell>3546</cell></row><row><cell>38</cell><cell>4388</cell></row><row><cell>42</cell><cell>4010</cell></row><row><cell>46</cell><cell>4576</cell></row><row><cell>50</cell><cell>4468</cell></row><row><cell>54</cell><cell>3781</cell></row><row><cell>58</cell><cell>3973</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Test data size of LDP corpus, by children's age.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Each subgraph corresponds to an individual child and its corresponding model (i.e. a model tested with the Results of significance test, applied over model predictions on child-produced and child-directed speech.</figDesc><table><row><cell></cell><cell cols="2">t-statistic p-value</cell></row><row><cell>1</cell><cell>-1.633</cell><cell>0.178</cell></row><row><cell>2</cell><cell>0.552</cell><cell>0.601</cell></row><row><cell>3</cell><cell>1.000</cell><cell>0.374</cell></row><row><cell>4</cell><cell>0.111</cell><cell>0.919</cell></row><row><cell>5</cell><cell>-1.400</cell><cell>0.220</cell></row><row><cell>6</cell><cell>-1.581</cell><cell>0.175</cell></row><row><cell>7</cell><cell>-1.718</cell><cell>0.161</cell></row><row><cell>8</cell><cell>0.542</cell><cell>0.611</cell></row><row><cell>9</cell><cell>0.535</cell><cell>0.621</cell></row><row><cell cols="2">10 -0.159</cell><cell>0.880</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://osf.io/s2jnm/?view_only= ca2d57aee759426ba1c531a64bc982f0</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We ran all of our analyses also with the 'expected' overlap metric and found a high correlation between both metrics.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>This number was chosen as a threshold after personal communication with Charles Yang.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/rgalhama/defdets_aacl</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bootstrapping language acquisition</title>
		<author>
			<persName><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="page" from="116" to="143" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fine-grained analysis of sentence embeddings using auxiliary prediction tasks</title>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Einat</forename><surname>Kermany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofer</forename><surname>Lavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Concurrent acquisition of word meaning and lexical categories</title>
		<author>
			<persName><forename type="first">Afra</forename><surname>Alishahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grzegorz</forename><surname>Chrupała</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</title>
		<meeting>the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="643" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analysis methods in neural language processing: A survey</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="49" to="72" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pointing and naming are not redundant: children use gesture to modify nouns before they modify nouns in speech</title>
		<author>
			<persName><forename type="first">Erica</forename><forename type="middle">A</forename><surname>Cartmill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dea</forename><surname>Hunsicker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Goldin-Meadow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developmental Psychology</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1660</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient induction of probabilistic word classes with LDA</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chrupała</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Correlating neural and symbolic representations of language</title>
		<author>
			<persName><forename type="first">Grzegorz</forename><surname>Chrupała</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afra</forename><surname>Alishahi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1283</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2952" to="2962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inducing syntactic categories by context distribution clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd workshop on Learning Language in Logic and the 4th conference on Computational Natural Language Learning</title>
		<meeting>the 2nd workshop on Learning Language in Logic and the 4th conference on Computational Natural Language Learning<address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="91" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural network models of language acquisition and processing</title>
		<author>
			<persName><forename type="first">Stefan L</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Padraic</forename><surname>Monaghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chara</forename><surname>Tsoukala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human language: From genes and brain to behavior</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="277" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">New evidence about language and cognitive development based on a longitudinal study: hypotheses for intervention</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Goldin-Meadow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">C</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">V</forename><surname>Hedges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janellen</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Raudenbush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Small</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">588</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards a model of prediction-based syntactic category acquisition: First steps with word embeddings</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Cassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Gillis</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W15-2405</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Cognitive Aspects of Computational Language Learning</title>
		<meeting>the Sixth Workshop on Cognitive Aspects of Computational Language Learning<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="28" to="32" />
		</imprint>
	</monogr>
	<note>Portugal</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A structural probe for finding syntax in word representations</title>
		<author>
			<persName><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1419</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4129" to="4138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ines</forename><surname>Montani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BabyBERTa: Learning more grammar with small-scale child-directed language</title>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">A</forename><surname>Huebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elior</forename><surname>Sulem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.conll-1.49</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Conference on Computational Natural Language Learning</title>
		<meeting>the 25th Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="624" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Young children&apos;s knowledge of the&quot; determiner&quot; and&quot; adjective&quot; categories</title>
		<author>
			<persName><forename type="first">Nenagh</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Lieven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tomasello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Speech, Language &amp; Hearing Research</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The CHILDES Project: Tools for Analyzing Talk</title>
		<author>
			<persName><forename type="first">B</forename><surname>Macwhinney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Lawrence Erlbaum Associates</publisher>
			<pubPlace>Hillsdale, NJ</pubPlace>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Vision: A computational investigation into the human representation and processing of visual information</title>
		<author>
			<persName><forename type="first">David</forename><surname>Marr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The emergence of an abstract grammatical category in children&apos;s early speech</title>
		<author>
			<persName><surname>Stephan C Meylan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><forename type="middle">C</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological science</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="192" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Frequent frames as a cue for grammatical categories in child directed speech</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Mintz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="117" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recurrent babbling: evaluating the acquisition of grammar from limited input data</title>
		<author>
			<persName><forename type="first">Ludovica</forename><surname>Pannitto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélie</forename><surname>Herbelot</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.conll-1.13</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Conference on Computational Natural Language Learning</title>
		<meeting>the 24th Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="165" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An incremental bayesian model for learning syntactic categories</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Parisien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afsaneh</forename><surname>Fazly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzanne</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Assessing the linguistic productivity of unsupervised deep neural networks</title>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Hodas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual Meeting of the Cognitive Sciencey Society</title>
		<meeting>the 39th Annual Meeting of the Cognitive Sciencey Society<address><addrLine>London, United Kingdom. Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Cognitive Science Society</publisher>
			<date type="published" when="2017">2017. CogSci 2017</date>
			<biblScope unit="page" from="937" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Do young children have adult-like syntactic categories? zipf&apos;s law and the case of the determiner</title>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">M</forename><surname>Pine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Freudenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grzegorz</forename><surname>Krajewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernand</forename><surname>Gobet</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2013.02.006</idno>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="345" to="360" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Slot and frame patterns and the development of the determiner category</title>
		<author>
			<persName><forename type="first">M</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Pine</surname></persName>
		</author>
		<author>
			<persName><surname>Vm Lieven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Psycholinguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="138" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributional information: A powerful cue for acquiring syntactic categories</title>
		<author>
			<persName><forename type="first">M</forename><surname>Redington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Crater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Finch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science: A Multidisciplinary Journal</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="425" to="469" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Abstraction not memory: BERT and the English article system</title>
		<author>
			<persName><forename type="first">Dagmar</forename><surname>Harish Tayyar Madabushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Divjak</surname></persName>
		</author>
		<author>
			<persName><surname>Milin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.67</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="924" to="931" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The role of performance limitations in the acquisition of verb-argument structure: an alternative account</title>
		<author>
			<persName><forename type="first">Anna</forename><forename type="middle">L</forename><surname>Theakston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Lieven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">M</forename><surname>Pine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><forename type="middle">F</forename><surname>Rowland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Child Language</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="127" to="152" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A statistical test for grammar</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics</title>
		<meeting>the 2nd Workshop on Cognitive Modeling and Computational Linguistics<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="30" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ontogeny and phylogeny of language</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="6324" to="6327" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
