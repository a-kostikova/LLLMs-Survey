<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Do the Benefits of Joint Models for Relation Extraction Extend to Document-level Tasks?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pratik</forename><surname>Saini</surname></persName>
							<email>pratik.saini@tcs.com</email>
							<affiliation key="aff0">
								<orgName type="institution">TCS Research</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tapas</forename><surname>Nayak</surname></persName>
							<email>nayak.tapas@tcs.com</email>
							<affiliation key="aff0">
								<orgName type="institution">TCS Research</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Indrajit</forename><surname>Bhattacharya</surname></persName>
							<email>b.indrajit@tcs.com</email>
							<affiliation key="aff0">
								<orgName type="institution">TCS Research</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Do the Benefits of Joint Models for Relation Extraction Extend to Document-level Tasks?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2FA890FF288E1B6B9D304E1E97206E2C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two distinct approaches have been proposed</head><p>for relational triple extraction -pipeline and joint. Joint models, which capture interactions across triples, are the more recent development, and have been shown to outperform pipeline models for sentence-level extraction tasks. Document-level extraction is a more challenging setting where interactions across triples can be long-range, and individual triples can also span across sentences. Joint models have not been applied for document-level tasks so far. In this paper, we benchmark state-ofthe-art pipeline and joint extraction models on sentence-level as well as document-level datasets. Our experiments show that while joint models outperform pipeline models significantly for sentence-level extraction, their performance drops sharply below that of pipeline models for the document-level dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction is a crucial NLP task for constructing and enriching knowledge bases. Traditional pipeline approaches <ref type="bibr" target="#b18">(Riedel et al., 2010;</ref><ref type="bibr" target="#b3">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b39">Zeng et al., 2014</ref><ref type="bibr" target="#b38">Zeng et al., , 2015;;</ref><ref type="bibr" target="#b12">Nayak and Ng, 2019;</ref><ref type="bibr" target="#b6">Jat et al., 2017)</ref> first identify entities followed by relation identification one entity pair at a time. In contrast, more recent joint approaches <ref type="bibr" target="#b42">(Zeng et al., 2018;</ref><ref type="bibr" target="#b22">Takanobu et al., 2019;</ref><ref type="bibr" target="#b13">Nayak and Ng, 2020;</ref><ref type="bibr" target="#b29">Wei et al., 2020;</ref><ref type="bibr">Wang et al., 2020b;</ref><ref type="bibr" target="#b45">Zhong and Chen, 2021;</ref><ref type="bibr" target="#b44">Zheng et al., 2021;</ref><ref type="bibr" target="#b9">Li et al., 2021;</ref><ref type="bibr" target="#b29">Wei et al., 2020;</ref><ref type="bibr" target="#b35">Yan et al., 2021;</ref><ref type="bibr" target="#b19">Shang et al., 2022)</ref> not only identify entities and relations for the same triple together but also extract all relational triples together. Thus, these recent approaches are better suited for capturing complex interactions.</p><p>Joint models for relation extraction outperform traditional pipeline models for sentence-level datasets such as NYT <ref type="bibr" target="#b18">(Riedel et al., 2010)</ref>. A more natural and complex setting for relation extraction is at the document-level. In the document-level task, relational triples may also span across sentences. Further, there may be long range interactions between different triples across sentences. As a result, the search space for joint models blows up with document size. So far, research for documentlevel datasets such as DocRED <ref type="bibr" target="#b36">(Yao et al., 2019)</ref> has used pipeline approaches and avoided the joint approach.</p><p>In this paper, we investigate if the benefits of the joint approach extrapolate from sentencelevel to document-level tasks. We benchmark 5 SOTA joint models and 3 SOTA pipeline models on sentence-level (NYT) and document-level (Do-cRED) datasets. We observe that the benefits of the SOTA joint models do not extend to documentlevel tasks. While performance of both classes of models drop sharply, joint models fall significantly below that of pipeline models. We perform extensive analysis to identify the short-comings of the two classes of models highlighting areas of improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Relation Extraction Approaches</head><p>Pipeline RE approaches solve the RE task in two sequential steps. In Step 1, they use an NER model to identify the entities and entity mentions in the input text. In Step 2, they take the predicted entities and entity mentions as input, and predict all possible relations from a pre-defined relation set between pairs of entities. We use PL-Marker <ref type="bibr" target="#b37">(Ye et al., 2022)</ref> as the NER module and KD-DocRE <ref type="bibr" target="#b23">(Tan et al., 2022)</ref>, SSAN <ref type="bibr">(Xu et al., 2021a)</ref>, and experiment with SAIS <ref type="bibr" target="#b30">(Xiao et al., 2022)</ref> as relation classification models for our experiments, and train these for specific datasets. Following standard practice, we use gold standard entity mentions for training and validation of the relation classification models, while for inference of the test instances, we naturally use the predicted entity mentions as input. Note that in both steps, these models perform independent classification for each entity mention   and relation.</p><p>Joint RE approaches identify the entities and relations in a relational triple in an end-to-end fashion. Further, they consider the entire input text (sentence or document) and output a set of relational triples together, thus, capturing complex interactions across triples in theory. The flip side, naturally, is that they need to explore a significantly larger space of candidates, which grows combinatorially with the length of the input text. We use 5 SOTA models for experiments. PtrNet <ref type="bibr" target="#b13">(Nayak and Ng, 2020)</ref> and REBEL <ref type="bibr">(Huguet Cabot and Navigli, 2021)</ref> use the Seq2Seq approach. Ptr-Net generates the index position of entities in text whereas REBEL generates the tokens for the triples. OneRel <ref type="bibr" target="#b19">(Shang et al., 2022)</ref> uses a table-based tagging approach. The tagging approaches of BiRTE <ref type="bibr" target="#b17">(Ren et al., 2022)</ref> and GRTE <ref type="bibr" target="#b16">(Ren et al., 2021)</ref> have a separate entity extraction process in their end-to-end modeling. We train these models in an end-to-end fashion as described in the respective papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Extraction Settings and Datasets</head><p>The original and simpler setting for relation extraction is sentence-level. This setting consists of individual sentences containing one or more relations as context. NYT <ref type="bibr" target="#b18">(Riedel et al., 2010</ref>) is a large-scale and popular benchmark for sentence-level RE, and we use this dataset as it is for our sentence-level experiments.</p><p>This setting, however, is restrictive since a large fraction of relations in natural text spans across multiple sentences. This is captured in the document-level relation extraction setting. Here, relational triples may be intra-sentence or intersentence, meaning that the head and tail entities of the relational triple can span across multiple sentences and require reasoning across multiple sentences to identify them. The task is to predict all these relations given an entire document as context. DocRED <ref type="bibr" target="#b36">(Yao et al., 2019</ref>) is a benchmark document-level dataset that we use for our experiments.</p><p>Contexts in DocRED (avg. number of tokens around 197) are much longer than in NYT (avg. number of tokens around 37). However, training data size is much larger for NYT. Since the relation labels of the DocRED test set are not released, we use the original validation set as test set and split the training data for training and validation. DocRED has mostly been used for pipeline models. We needed additional processing to make it tractable for joint models. We remove the documents with overlapping entity mentions in the training and validation set. We get 2,856 documents from the training set and 924 documents from the validation set. Then, we replace all entity mentions with the first occurring entity mention for each entity so that co-reference resolution is not required. We include an example of document processing for DocRED in Table <ref type="table" target="#tab_1">1</ref>. The details of the dataset splits of the NYT and DocRED for our experiments are included in Table <ref type="table" target="#tab_2">2 1 .</ref> Evaluation Metric: We use 'strict' criteria for evaluation. We consider an extracted relational triple as correct only if two entities and relation exactly match with a ground truth triple. We report triple level precision, recall and F1 scores for the models.</p><p>Parameter Settings: We use BERT BASE (cased) <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref> for document encoding for all the models except REBEL for which we have used BART BASE <ref type="bibr" target="#b8">(Lewis et al., 2019)</ref>. We used NVIDIA Tesla V100 32GB GPU to train the models. We used other hyper-parameters as per provided in their respective papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>Through our experiment, we try to find out the answers to following research questions (RQ). RQ1: How do the two classes of models perform at sentence and documents scales?</p><p>End-to-end performance of the joint models and the pipeline models on the NYT and DocRED datasets is shown in Table <ref type="table" target="#tab_3">3</ref>. On the sentencelevel NYT dataset, both joint models and pipeline models achieve close to 0.90 F1 score. But, on Do-cRED, we see a huge drop in the F1 score for both categories. The pipeline models score below 0.60 whereas among the joint models GRTE and BiRTE perform around 0.45, the others drop to 0.20 or below. One reason for the drop for DocRED is the smaller training data size. But, it does not explain the gap of 10% F1 score between the pipeline and joint models when they performed almost at par for NYT. This suggests that joint models struggle with longer context and cross-sentence relations of documents. We investigate this in more detail next. RQ2: Are some joint models better than others at document scale?</p><p>Out of the 5 joint models, we see significantly higher drop in F1 score for OneRel and PtrNet than REBEL, GRTE, and BiRTE models. Given a document with L tokens and K predefined relations, OneRel maintains a three-dimensional matrix M L×K×L and assigns tags for all possible triples. When context length grows as in documents, M has many more negative tags and very few positive tags, which seems to affect OneRel performance. BiRTE and GRTE, on the other hand, extract the entities first separately and then classify the relations. While this is done in an end-to-end fashion, it is still similar to pipeline approach. This may be the reason for the smaller performance drop compared to pipelines models on DocRED.</p><p>PtrNet and REBEL are Seq2Seq models which use a decoder to extract the triples, so they possibly need more training data to learn from longer document contexts. Additionally, PtrNet extracts index positions for the entities. Since an entity may appear more than once in a document, we mark the first occurring index of the entity-mention to train this model. This very likely contributes to its poorer performance. On the other hand, REBEL outputs entities as text, and does not have this training issue. RQ3: How different are performances for intra vs inter-sentence extraction?</p><p>The fundamental difference between NYT and DocRED is that NYT contains only intra-sentence triples, whereas DocRED contains both intra and inter-sentence (cross-sentence) triples. In Table <ref type="table" target="#tab_5">5</ref>, we first show intra vs inter sentence relational triple distribution for the gold and model predictions on the DocRED dataset. Pipeline models have nearly the same distribution for gold and prediction. But, joint models are skewed towards intra-sentence relations. This suggests that joint models are very good at extracting intra-sentence triples but they struggle with inter-sentence triples. This is why joint models perform very well on the NYT dataset and fail to do so on DocRED.</p><p>In Table <ref type="table" target="#tab_4">4</ref>, we have reported the intra-sentence vs inter-sentence relations performance of the topperforming models on the DocRED test dataset. We see that all the models perform way better at intra-sentence extraction as compared to intersentence extraction, it demonstrates that intersentence extraction is significantly harder. We also see that pipeline models achieve around 10% higher F1 scores than the joint models for both intra and inter categories on DocRED. This shows that even for the familiar intra-sentence setting joint models face more difficulties compared to pipeline models when encountered with longer context and smaller training volume.</p><p>Lastly, we investigate the impact of the distance  between subject and object mentions in the context on the performance of inter-sentence relations. In Table <ref type="table" target="#tab_6">6</ref>, we record recall of SOTA models on intersentence relations for different subject-object hop distances. Hop distance k refers to the minimum sentence-level distance between the subject and object entity of a triple within the document being k. Again, we see that pipeline models outperform joint models by ∼ 12% for all hop distances, and not just for longer ones. RQ4: How is performance affected by training data size? Next, we analyze how training volume affects performance for the two model classes for the simpler intra-sentence extraction task. Note that NYT contains such relations exclusively. Since DocRED has both categories, we prepare DocRED-Intra including only intra-sentence triples and the corresponding sentences. The size of these datasets are significantly different. DocRED-Intra has only ∼ 6.5K training instances compared to 94K for NYT. We train all the models with these intrasentence triples and record their performance for DocRED-Intra in Table <ref type="table">7</ref>. Corresponding NYT performance is in Table <ref type="table" target="#tab_3">3</ref>. We observe a big gap of ∼ 42 -50% for joint models and ∼ 33% for pipeline models in the performance between NYT and DocRED-Intra. This is due to the smaller training volume associated with a larger number of relations in DocRED. The notable disparity between pipeline and joint models in the case of DocRED-Intra demonstrates that joint models are not as effective at generalization compared to pipeline models, particularly when working with limited training data volumes and longer contexts. RQ5: How different are entity extraction performances at sentence and document scales?</p><p>Finally, we aim to analyze if the huge gap in the performance of pipeline and joint models on </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>While joint models for relational triple extraction have been shown to outperform pipeline models for sentence-level extraction settings, in this paper we have demonstrated, with extensive experimentation, that these benefits do not extend to the more realistic and natural document-level extraction setting, which entails longer contexts and cross-sentence relations. Experimenting with 5 SOTA joint models and 3 SOTA pipeline models, we have shown that while performance of both classes of models drops significantly in the more complex document setting, joint models suffer significantly more with longer context, inter-sentence relations and limited train-ing data for the overall task as well as for subtasks such as NER. This aids in establishing a research agenda for joint models to extend the promised benefits of joint entity identification, relation classification, and joint extraction of all triples from the context. This pertains to the more challenging yet natural and crucial setting for relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>The major limitation of this work is that we could only analyze 3 pipeline models and 5 joint models.</p><p>Recently, many models have been proposed for this task both in pipeline and joint class. Out of these, we chose different kinds of SOTA models to cover the different design choices made by these models. We chose PtrNet <ref type="bibr" target="#b13">(Nayak and Ng, 2020)</ref> and REBEL <ref type="bibr">(Huguet Cabot and Navigli, 2021)</ref> as they used Seq2Seq model for this task. OneRel <ref type="bibr" target="#b19">(Shang et al., 2022)</ref> used table-filling method whereas BiRTE <ref type="bibr" target="#b17">(Ren et al., 2022)</ref> and GRTE <ref type="bibr" target="#b16">(Ren et al., 2021)</ref> used sequentially extracting entities and relations in their end-to-end model.</p><p>with adaptive thresholding and localized context pooling. Proceedings of the AAAI Conference on Artificial Intelligence, 35(16):14612-14620.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Details of Joint Models A.1.1 PtrNet <ref type="bibr" target="#b13">(Nayak and Ng, 2020)</ref> PtrNet utilizes a seq2seq approach along with the pointer network-based decoding for jointly extracting entities and relations. Each triple contains the start and end index of the subject and object entities, along with the relation class label. Their decoder has two-pointer networks to identify the start and end index of the two entities and a classifier to identify the relation between the two entities. Decoder extracts a relational triple at each time steps and continue the process till there is no triple to extract.</p><p>To ensure parity with other SOTA models, their BiLSTM encoder is replaced with BERT encoder.</p><p>A.1.2 REBEL <ref type="bibr">(Huguet Cabot and Navigli, 2021)</ref> REBEL utilizes an auto-regressive seq2seq model that streamlines the process of relation extraction by presenting triples as a sequence of text and uses special separator tokens, as markers, to achieve the linearization.. WordDecoder model of (Nayak and Ng, 2020) uses a similar approach using LSTMs whereas REBEL is a BART-based Seq2Seq model that utilizes the advantages of transformer model and pre-training. REBEL uses a more compact representation for the relational triples over Word-Decoder model.</p><p>A.1.3 GRTE <ref type="bibr" target="#b16">(Ren et al., 2021)</ref> GRTE utilizes individual tables for each relation.</p><p>The cell entries of a table denote the presence or absence of relation between the associated token pairs. It uses enhanced table-filling methods by introducing two kinds of global features. The first global feature is for the association of entity pairs and the second is for relations. Firstly, a table feature is generated for each relation, which is then consolidated with the features of all relations. This integration produces two global features related to the subject and object, respectively. These two global features are refined multiple times. Finally, the filled tables are utilized to extract all relevant triples.</p><p>A.1.4 OneRel <ref type="bibr" target="#b19">(Shang et al., 2022)</ref> OneRel frames the joint entity and relation extraction task as a fine-grained triple classification problem. It uses a three-dimensional matrix with relation-specific horns tagging strategy. The rows in this matrix refer to the head entity tokens and the columns in this matrix refer to the tail entity tokens from the original text. The scoring-based classifier checks the accuracy of the decoded relational triples. It discards the triples with low confidence.</p><p>A.1.5 BiRTE <ref type="bibr" target="#b17">(Ren et al., 2022)</ref> In this paper, a bidirectional tagging approach with multiple stages is utilized. BiRTE first discovers the subject entities and then identifies the object entities based on the subject entities. It then does this in the reverse direction, first discovering the object entities and identifying subject entities for the object entities. The final stage involves the relation classification of subject-object pairs. They perform these tasks jointly in a single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Details of Pipeline Models</head><p>A.2.1 SSAN <ref type="bibr">(Xu et al., 2021a)</ref> This paper frames the structure of entities as defined by the specific dependencies between the entity mention pairs in a document. The proposed approach, SSAN, integrates these structural dependencies with the self-attention mechanism at the encoding stage. To achieve this, two transformation modules are included in each self-attention building block, which generates attentive biases to regulate the attention flow adaptively. This approach achieved SOTA performance on the Do-cRED dataset.</p><p>A.2.2 KD-DocRE <ref type="bibr" target="#b23">(Tan et al., 2022)</ref> This paper suggests a semi-supervised framework for extracting document-level relations. To achieve this, they exploit the inter-dependency among the relational triples through the implementation of an axial attention module. This approach leads to enhanced performance when dealing with two-hop relations. In addition to this, an adaptive focal loss is proposed as a means of resolving the issue of imbalanced label distribution for long-tail classes. Finally, to account for the difference between human-annotated data and distantly supervised data, knowledge distillation is utilized. Their experiments on the DocRED show the effectiveness of the approach.</p><p>A.2.3 SAIS <ref type="bibr" target="#b30">(Xiao et al., 2022)</ref> The objective of this paper is to train the model to identify relevant contexts and entity types by using the Supervising and Augmenting Intermediate Steps (SAIS) approach for relation extraction.</p><p>The SAIS framework proposed in this paper results in the extraction of relations that are of superior quality, owing to its more efficient supervision. By utilizing evidence-based data augmentation and ensemble inference, SAIS also improves the accuracy of the supporting evidence retrieval process while minimizing the computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Details of NER Model</head><p>A.3.1 PL-Marker <ref type="bibr" target="#b37">(Ye et al., 2022)</ref> In their approach for span representation, PL-Marker strategically uses levitated markers to consider the interrelation between pairs of spans. They propose a packing strategy that factors in neighbouring spans to improve the modeling of entity boundary information. Additionally, they utilize a subject-oriented packing approach, which groups each subject with its objects to effectively model the interrelations between the same subject span pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Related Work</head><p>Sentence-level Relation Extraction: Early approaches for relation extraction use two steps pipeline approach. The first step, Named Entity Recognition (NER), extracts entities from the text. The second step, Relation Classification (RC), identifies pairwise relations between the extracted entities <ref type="bibr" target="#b39">(Zeng et al., 2014</ref><ref type="bibr" target="#b38">(Zeng et al., , 2015;;</ref><ref type="bibr" target="#b6">Jat et al., 2017;</ref><ref type="bibr" target="#b12">Nayak and Ng, 2019)</ref>. Pipeline methods fail to capture the implicit correlation between the two sub-tasks. They suffer from error propagation between the two stages. They cannot model the interaction among the relational triples.</p><p>To mitigate the drawbacks of pipeline approaches, recent works have focused on Joint entities and relation extraction. Joint approaches referred to as End-to-End Relation Extraction (RE) accomplish both tasks jointly. Training simultaneously on both NER and RC tasks allows for capturing more complex interactions among the multiple relational triples present in the context. <ref type="bibr" target="#b10">Miwa and Bansal (2016)</ref> proposed a model that trained the NER and RC module in a single model. <ref type="bibr" target="#b13">Nayak and Ng (2020);</ref><ref type="bibr">Cabot and Navigli (2021)</ref> propose seq2seq models for extracting the triples in a sequence. <ref type="bibr" target="#b21">Sui et al. (2020)</ref> casts the joint extraction task as a set prediction problem rather than a sequence extraction problem. <ref type="bibr" target="#b43">Zhang et al. (2017)</ref>; <ref type="bibr">Wang et al. (2020b</ref><ref type="bibr" target="#b27">Wang et al. ( , 2021))</ref>; <ref type="bibr" target="#b19">Shang et al. (2022)</ref> formulate the NER and RC tasks as table filling problem where each cell of the table represents the interaction between two tokens. Ren et al. Document-level Relation Extraction: Recently, there has been a shift of interest towards document-level RE <ref type="bibr" target="#b36">(Yao et al., 2019)</ref>. Documentlevel relation extraction (DocRE) is known to be a more complex and realistic task compared to the sentence-level counterpart. DocRE typically involves large volumes of data, which can be computationally expensive to handle using joint models. Recent work in DocRE has avoided using joint models for this task as joint models are not scalable for long documents. In DocRE, there can be multiple mentions of an entity with different surface forms across the document and the evidence of the relations can spread across multiple sentences. In document-level RE, mostly pipeline approaches are proposed, joint extraction approaches are not explored for this task. Earlier works <ref type="bibr" target="#b14">(Peng et al., 2021;</ref><ref type="bibr" target="#b15">Quirk and Poon, 2017;</ref><ref type="bibr" target="#b20">Song et al., 2018;</ref><ref type="bibr" target="#b7">Jia et al., 2019</ref>) used dependency graph between the two entities to find the relations. Recent works <ref type="bibr" target="#b2">(Guo et al., 2019;</ref><ref type="bibr" target="#b11">Nan et al., 2020;</ref><ref type="bibr">Wang et al., 2020a;</ref><ref type="bibr" target="#b41">Zeng et al., 2020</ref><ref type="bibr" target="#b40">Zeng et al., , 2021;;</ref><ref type="bibr">Xu et al., 2021c,b)</ref> proposed graph-based approaches that use advanced neural techniques to do multi-hop reasoning. More recent Transformer-based approaches <ref type="bibr" target="#b26">(Wang et al., 2019;</ref><ref type="bibr" target="#b24">Tang et al., 2020;</ref><ref type="bibr" target="#b4">Huang et al., 2021;</ref><ref type="bibr">Xu et al., 2021a;</ref><ref type="bibr" target="#b46">Zhou et al., 2021;</ref><ref type="bibr" target="#b31">Xie et al., 2022)</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(2022); Zheng et al. (2021); Li et al. (2021); Yan et al. (2021); Wei et al. (2020) have separate NER and RC modules in the same model trained in an end-to-end fashion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>use pre-trained language models to encode long-range contextual dependencies in the documents. Huang et al. (2021); Xie et al. (2022); Xiao et al. (2022); Tan et al. (2022) use neural classifier to identify the evidences for relations along with relation classification for performance improvement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Original Kiato ( , Sidirodromikos Stathmos Kiatou ) is a railway station in Kiato in the northern Peloponnese , Greece . The station is located a kilometre west of the town , near the Greek National Road 8A ( Patras -Corinth highway ) . It opened on 9 July 2007 as the western terminus of the line from Athens Airport . Initially the station served as an exchange point for passengers to Patras on the old metre gauge SPAP line to Patras , but all traffic was suspended indefinitely in December 2010 for cost reasons . The nearby old Kiato station was also closed . Passengers for Patras must now change to bus services at Kiato . The station is served by one train per hour to Piraeus . Processed Kiato ( , Kiato ) is a railway station in Kiato in the northern Peloponnese , Greece . The station is located a kilometre west of the town , near the Greek National Road 8A ( Peloponnese -Corinth highway ) . It opened on 9 July 2007 as the western terminus of the line from Athens Airport . Initially the station served as an exchange point for passengers to Peloponnese on the old metre gauge SPAP line to Peloponnese , but all traffic was suspended indefinitely in December 2010 for cost reasons . The nearby old Kiato station was also closed . Passengers for Peloponnese must now change to bus services at Kiato . The station is served by one train per hour to Peloponnese .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>A sample document from DocRED and its processed version. The different mentions of the entity 'Kiato' and 'Peloponnese' are marked with blue and red respectively. In the processed text, different mentions are normalized with the first mention of these two entities which are 'Kiato' and 'Peloponnese'.</figDesc><table><row><cell cols="2">Dataset # Relations</cell><cell cols="2">Train</cell><cell cols="2">Validation</cell><cell></cell><cell>Test</cell></row><row><cell></cell><cell></cell><cell cols="6"># Context # Triples # Context # Triples # Context # Triples</cell></row><row><cell>NYT</cell><cell>24</cell><cell>56,196</cell><cell>94,222</cell><cell>5,000</cell><cell>8,489</cell><cell>5,000</cell><cell>8,616</cell></row><row><cell>DocRED</cell><cell>96</cell><cell>2,572</cell><cell>20,233</cell><cell>284</cell><cell>2,187</cell><cell>924</cell><cell>7,337</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the NYT24 and DocRED datasets. Context refers to a sentence or document.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance of the SOTA models on end-to-end relation extraction on NYT24 and DocRED datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>NYT24</cell><cell></cell><cell></cell><cell>DocRED</cell></row><row><cell></cell><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell></cell><cell>OneRel</cell><cell cols="6">0.926 0.918 0.922 0.513 0.130 0.208</cell></row><row><cell></cell><cell>BiRTE</cell><cell cols="6">0.914 0.920 0.917 0.522 0.402 0.454</cell></row><row><cell>Joint Models</cell><cell>GRTE</cell><cell cols="6">0.929 0.924 0.926 0.586 0.373 0.456</cell></row><row><cell></cell><cell>PtrNet</cell><cell cols="6">0.898 0.894 0.896 0.222 0.145 0.175</cell></row><row><cell></cell><cell>Rebel</cell><cell cols="6">0.881 0.885 0.883 0.466 0.356 0.404</cell></row><row><cell></cell><cell cols="7">KD-DocRE 0.895 0.910 0.902 0.620 0.556 0.586</cell></row><row><cell>Pipeline Models</cell><cell>SSAN</cell><cell cols="6">0.781 0.798 0.789 0.576 0.529 0.552</cell></row><row><cell></cell><cell>SAIS</cell><cell cols="6">0.864 0.879 0.872 0.640 0.545 0.589</cell></row><row><cell></cell><cell>Model</cell><cell>P</cell><cell>Intra R</cell><cell>F1</cell><cell>P</cell><cell>Inter R</cell><cell>F1</cell></row><row><cell>Joint Models</cell><cell>BiRTE GRTE</cell><cell cols="6">0.600 0.425 0.497 0.420 0.366 0.391 0.677 0.407 0.508 0.460 0.320 0.378</cell></row><row><cell>Pipeline Models</cell><cell cols="7">KD-DocRE 0.666 0.601 0.631 0.545 0.485 0.513 SAIS 0.697 0.594 0.641 0.548 0.467 0.504</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance of top performing SOTA models on Intra vs Inter relational triples on DocRED dataset.</figDesc><table><row><cell></cell><cell>Model</cell><cell cols="2">Predict Intra % Inter %</cell></row><row><cell>-</cell><cell>Gold</cell><cell>61</cell><cell>39</cell></row><row><cell>Joint Models</cell><cell>BiRTE GRTE</cell><cell>56 58</cell><cell>44 42</cell></row><row><cell>Pipeline Models</cell><cell>KD-DocRE SAIS</cell><cell>62 62</cell><cell>38 38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Intra vs Inter relational triple distribution of SOTA models predictions. We include top two models from the joint and pipeline class for this analysis.</figDesc><table><row><cell># Hops</cell><cell cols="4">Pipeline Models KD-DocRE SAIS BiRTE GRTE Joint Models</cell></row><row><cell>1</cell><cell>0.50</cell><cell>0.45</cell><cell>0.38</cell><cell>0.33</cell></row><row><cell>2</cell><cell>0.44</cell><cell>0.44</cell><cell>0.34</cell><cell>0.31</cell></row><row><cell>3</cell><cell>0.48</cell><cell>0.49</cell><cell>0.36</cell><cell>0.31</cell></row><row><cell>4</cell><cell>0.48</cell><cell>0.49</cell><cell>0.37</cell><cell>0.31</cell></row><row><cell>5</cell><cell>0.51</cell><cell>0.54</cell><cell>0.39</cell><cell>0.32</cell></row><row><cell>6</cell><cell>0.42</cell><cell>0.43</cell><cell>0.29</cell><cell>0.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Recall score of Pipeline and Joint SOTA models on inter-sentence relations with respect to distance between subject and object entities.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Performance of PL-Marker, BiRTE and GRTE on the NER task for NYT24 and DocRED dataset.</figDesc><table><row><cell></cell><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>Joint Models</cell><cell>BiRTE GRTE</cell><cell cols="3">0.527 0.462 0.492 0.544 0.346 0.423</cell></row><row><cell>Pipeline Models</cell><cell cols="4">KD-DocRE 0.524 0.619 0.567 SAIS 0.485 0.610 0.540</cell></row><row><cell cols="5">Table 7: Performance of top performing SOTA models</cell></row><row><cell cols="2">on DocRED-Intra dataset.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">DocRED is affected by their performance on NER</cell></row><row><cell cols="5">subtask of relation extraction. In Table 8, we in-</cell></row><row><cell cols="5">clude the performance of these models on the en-</cell></row><row><cell cols="5">tity extraction task. Pipeline models have a sepa-</cell></row><row><cell cols="5">rate NER model. The performance of this model -</cell></row><row><cell cols="5">PL-Marker -on NER task is similar to that of the</cell></row><row><cell cols="5">BiRTE and GRTE models for NYT dataset. But,</cell></row><row><cell cols="5">for DocRED, BiRTE and GRTE perform much</cell></row><row><cell cols="5">worse than PL-Marker, the drop in F1 score being</cell></row><row><cell cols="5">around 25%. This, in turn, hurts their performance</cell></row><row><cell cols="5">on the relational triple extraction. Though train-</cell></row><row><cell cols="5">ing data volume for DocRED is smaller, note that</cell></row><row><cell cols="5">the PL-Marker model is trained on the DocRED</cell></row><row><cell cols="5">dataset itself as are the BiRTE/GRTE models. This</cell></row><row><cell cols="5">shows that, aside from overall extraction perfor-</cell></row><row><cell cols="5">mance, joint models struggle with the NER subtask</cell></row><row><cell cols="5">as well when training data is limited. This suggests</cell></row><row><cell cols="5">that a separate NER model may be more useful in</cell></row><row><cell>such settings.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell cols="5">PL-Marker 0.948 0.955 0.952</cell></row><row><cell>NYT24</cell><cell>BiRTE</cell><cell cols="3">0.955 0.954 0.954</cell></row><row><cell></cell><cell>GRTE</cell><cell cols="3">0.958 0.956 0.957</cell></row><row><cell cols="5">PL-Marker 0.942 0.934 0.938</cell></row><row><cell>DocRED</cell><cell>BiRTE</cell><cell>0.73</cell><cell cols="2">0.647 0.686</cell></row><row><cell></cell><cell>GRTE</cell><cell cols="3">0.757 0.576 0.654</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Our processed DocRED dataset is available at https://github.com/pratiksaini4/nyt-docred-joint-pipelinecomparison</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rebel: Relation extraction by end-to-end language generation</title>
		<author>
			<persName><forename type="first">Pere-Lluís</forename><surname>Huguet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cabot</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention guided graph convolutional networks for relation extraction</title>
		<author>
			<persName><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1024</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="241" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Entity and evidence guided document-level relation extraction</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.repl4nlp-1.30</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)</title>
		<meeting>the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="307" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">REBEL: Relation extraction by end-to-end language generation</title>
		<author>
			<persName><forename type="first">Pere-Lluís</forename><surname>Huguet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cabot</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.204</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2370" to="2381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improving distantly supervised relation extraction using word and entity based attention</title>
		<author>
			<persName><forename type="first">Sharmistha</forename><surname>Jat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhesh</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In Proceedings of the 6th Workshop on Automated Knowledge Base Construction</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Document-level n-ary relation extraction with multiscale representation learning</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1370</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3693" to="3704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Abdel Rahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Bart</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TDEER: An efficient translating decoding schema for joint extraction of entities and relations</title>
		<author>
			<persName><forename type="first">Xianming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daichuan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using LSTMs on sequences and tree structures</title>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the</title>
		<meeting>the 54th Annual Meeting of the</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reasoning with latent structure refinement for document-level relation extraction</title>
		<author>
			<persName><forename type="first">Guoshun</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Sekulic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.141</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1546" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Effective attention modeling for neural relation extraction</title>
		<author>
			<persName><forename type="first">Tapas</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computational Natural Language Learning</title>
		<meeting>the Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effective modeling of encoder-decoder architecture for joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Tapas</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<meeting>The Thirty-Fourth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cross-lingual word embedding refinement by ℓ 1 norm optimisation</title>
		<author>
			<persName><forename type="first">Xutan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Stevenson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.214</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2690" to="2701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction beyond the sentence boundary</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1171" to="1182" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A novel global feature-oriented relational triple extraction model based on table filling</title>
		<author>
			<persName><forename type="first">Feiliang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujuan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bochao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaduo</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A simple but effective bidirectional framework for relational triple extraction</title>
		<author>
			<persName><forename type="first">Feiliang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujuan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bochao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Fifteenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<meeting>the European Conference on Machine Learning and Knowledge Discovery in Databases</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">OneRel: Joint entity and relation extraction with one module in one step</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">N-ary relation extraction using graphstate LSTM</title>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1246</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2226" to="2235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Joint entity and relation extraction with set prediction networks</title>
		<author>
			<persName><forename type="first">Dianbo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengping</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/2011.01675</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A hierarchical framework for relation extraction with reinforcement learning</title>
		<author>
			<persName><forename type="first">Ryuichi</forename><surname>Takanobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiexi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Thirty-Third AAAI Conference on Artificial Intelligence</title>
		<meeting>The Thirty-Third AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with adaptive focal loss and knowledge distillation</title>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-acl.132</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<imprint>
			<publisher>Dublin, Ireland. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1672" to="1681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">HIN: hierarchical inference network for documentlevel relation extraction</title>
		<author>
			<persName><forename type="first">Hengzhu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangxia</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Yin</surname></persName>
		</author>
		<idno>CoRR, abs/2003.12754</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Global-to-local neural networks for document-level relation extraction</title>
		<author>
			<persName><forename type="first">Difeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ermei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.303</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3711" to="3721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Finetune bert for docred with two-step process</title>
		<author>
			<persName><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christfried</forename><surname>Focke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nilesh</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno>CoRR, abs/1909.11898</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">UniRE: A unified label space for entity relation extraction</title>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changzhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.19</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="220" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">TPLinker: Single-stage joint extraction of entities and relations through token pair linking</title>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COL-ING</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A novel cascade binary tagging framework for relational triple extraction</title>
		<author>
			<persName><forename type="first">Zhepei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SAIS: Supervising and augmenting intermediate steps for document-level relation extraction</title>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zecheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.171</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2395" to="2409" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Eider: Empowering documentlevel relation extraction with efficient evidence extraction and inference-stage fusion</title>
		<author>
			<persName><forename type="first">Yiqing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-acl.23</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="257" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">2021a. Entity structure within and throughout: Modeling mention dependencies for document-level relation extraction</title>
		<author>
			<persName><forename type="first">Benfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">2021b. Discriminative reasoning for document-level relation extraction</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.144</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="1653" to="1663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">2021c. Document-level relation extraction with reconstruction</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v35i16.17667</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="14167" to="14175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A partition filter network for joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">DocRED: A large-scale document-level relation extraction dataset</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Packed levitated marker for entity and relation extraction</title>
		<author>
			<persName><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.337</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4904" to="4917" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics</title>
		<meeting>the 25th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">SIRE: Separate intra-and inter-sentential reasoning for document-level relation extraction</title>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.47</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="524" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Double graph based reasoning for documentlevel relation extraction</title>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.127</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1630" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Extracting relational facts by an end-to-end neural model with copy mechanism</title>
		<author>
			<persName><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">End-to-end neural relation extraction with global optimization</title>
		<author>
			<persName><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1182</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1730" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">PRGC: Potential relation and global correspondence based joint relational triple extraction</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Heng Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yefeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A frustratingly easy approach for entity and relation extraction</title>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v35i16.17717</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Document-level relation extraction</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
