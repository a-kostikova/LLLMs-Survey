<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sentiment Aided Graph Attentive Contextualization for Task Oriented Negotiation Dialogue Generation</title>
				<funder ref="#_gS6N9KD">
					<orgName type="full">Accenture Pvt Ltd</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aritra</forename><surname>Raut</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ramakrishna Mission Vivekananda Educational and Research Institute</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sriparna</forename><surname>Saha</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Patna</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anutosh</forename><surname>Maitra</surname></persName>
							<affiliation key="aff2">
								<address>
									<settlement>Accenture</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roshni</forename><surname>Ramnani</surname></persName>
							<affiliation key="aff2">
								<address>
									<settlement>Accenture</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sentiment Aided Graph Attentive Contextualization for Task Oriented Negotiation Dialogue Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6A786F3EC1D17A467C0C6C2B451D983C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Over the past several years, the demand and popularity of using virtual assistants to finish jobs like service scheduling and online shopping have increased. While keeping the user's request in mind, an effective task-oriented virtual agent must strive to improve the seller's profit. Therefore, in order to achieve the best possible trade-off between the parties, this form of virtual agent has to have strong negotiating abilities. Although current conversational agents are quite good at making fluent sentences, they are still unable to use strategic thinking. In order to more effectively contextualize the choice of the next set of negotiation methods while producing answers, we develop Nego-GAT, an end-to-end negotiation system that includes sentiment information and graph attention embedding into GPT-2. Our selfsupervised model outperforms earlier cuttingedge negotiation models in terms of both the precision of strategy/dialogue act prediction and the caliber of the generated dialogue responses 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent natural language processing research has centered on the creation of models for conversational agents, which have a variety of applications in the business, sales, and healthcare sectors. Depending on their intended purpose, conversational agents can be divided into two categories: chitchat agents and task-or goal-oriented virtual agents. Chit-chat agents interact with users as a friend to meet their need for companionship and for conversation, while the former aims to help users complete tasks.</p><p>Conversational bots for tasks have recently become more common in the field of natural language generation (NLG). Customers may get assistance from these agents with a variety of tasks, 1 all codes are available at https://github.com/ aritraraut/Nego-GAT/tree/master including booking hotel rooms, buying tickets, and more. With the use of implicit encoder-decoder designs <ref type="bibr" target="#b23">(Sordoni et al., 2015;</ref><ref type="bibr" target="#b10">Li et al., 2016)</ref> or explicit semantic information, such as slot-value pairs <ref type="bibr" target="#b34">(Young, 2006;</ref><ref type="bibr" target="#b8">Larionov et al., 2018)</ref>, modern conversation systems have made significant progress in characterizing the history and structure of discussion. The user's need may not be met, or there may be other barriers, such as financial constraints, that prevent the user from completing a purchase, which may prevent these agents from being successful. In this case, one of the best strategies for assisting both parties in reaching an understanding is bargaining (An example has been shown in Fig <ref type="figure" target="#fig_0">1)</ref>. Users openly state their intentions in these tasks, which enables computers to map the utterances to specific intent slots <ref type="bibr" target="#b13">(Li et al., 2020)</ref>. However, such mapping is less obvious in difficult non-collaborative tasks like negotiation <ref type="bibr" target="#b5">(He et al., 2018)</ref> and persuasion <ref type="bibr" target="#b31">(Wang et al., 2019)</ref> since the user's intent and the most effective strategies are concealed. Selecting the appropriate dialogue act and negotiating approach is essential for generating appropriate responses. The majority of earlier research on negotiating dialogues really concentrated on improving conversation techniques, ranging from high-level task-specific strategies <ref type="bibr" target="#b9">(Lewis et al., 2017)</ref> to more precise task execution planning <ref type="bibr" target="#b5">(He et al., 2018)</ref>, to fine-grained planning of language outputs given strategic options <ref type="bibr">(Zhou et al., 2019a)</ref>. <ref type="bibr" target="#b7">Joshi et al. (2021)</ref> proposed a framework for modeling intricate negotiation strategies that makes use of intermediate structures and Graph Attention Networks (GAT) to make the model understandable. Their model makes use of the recently proposed hierarchical graph pooling-based methodology to comprehend the relationships between negotiating techniques, such as conceptual and verbal strategies and conversation activities, and their relative value in selecting the best sequence.</p><p>All of these methods frequently employ various modules to precisely simulate different components of the conversation history. The pipeline strategy's apparent downside is that subsequent subtasks could be damaged by error propagation from cascaded components <ref type="bibr" target="#b14">(Liu and Lane, 2018)</ref>. Thus, developing an end-to-end system is essential in order to not only give concise responses but also to decide on the best approach to negotiations. <ref type="bibr" target="#b33">Yang et al. (2021)</ref> suggested UBAR, a fully end-to-end chat system. In a straightforward task-oriented conversation generation setting, this model draws belief states from the context and also independently decides actions and generates responses.</p><p>In keeping with the concept put forward by <ref type="bibr" target="#b7">Joshi et al. (2021)</ref> and <ref type="bibr" target="#b33">Yang et al. (2021)</ref>, we present our model Nego-GAT, which integrates GAT (Graph attention) embeddings into the large pre-trained model GPT-2 <ref type="bibr" target="#b19">(Radford et al., 2019)</ref> to enhance contextualization while producing negotiation answers. On the other hand, in the case of negotiation dialogues, sentiment plays an essential role while generating responses. For instance, <ref type="bibr" target="#b31">Wang et al. (2019)</ref> included user sentiment to make an effective user-adaptive system. With this in mind, we also retrieved the sentiment information from the user text and used it as an additional piece of context-specific data to further enhance our model.</p><p>Using the CraigslistBargain <ref type="bibr" target="#b5">(He et al., 2018)</ref> dataset, we have refined our model at the session level, taking into account the user utterance, sentiment, belief state<ref type="foot" target="#foot_0">2</ref> , bargaining strategy, conversation act, and agent reaction for each dialogue turn. On the same dataset, we ran a number of experiments, and we found that our model outperformed all previous models virtually universally. The following list outlines the study's contributions:</p><p>1. This is the first approach towards building an end-to-end task-oriented model that incorporates GAT embeddings into the large pre-trained model like GPT-2 in an effort to enhance negotiation dialogue generation by choosing the appropriate sequence of strategies. We also incorporate sentiment tokens as an extra bit of information into the context of the same goal as mentioned.</p><p>2. Using a standard dataset on negotiation exchanges (CraigListBargain He et al. ( <ref type="formula">2018</ref>)), we assess the proposed model. The outcomes demonstrate that our model routinely performs better than the most recent models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Our main goal is to create an end-to-end system that can extract belief states, select the appropriate approach, and produce a strong negotiating response. So, we've highlighted a few related and relevant works in this section.</p><p>Task oriented virtual agents: A number of sequence-to-sequence based dialogue <ref type="bibr" target="#b11">(Li et al., 2017)</ref> generation techniques have been proposed, which use RNN (Recurrent Neural Network) units (LSTM/GRU) to encode dialogue context and construct replies using the encoded data. After that, the use of pre-trained models, like GPT-2 <ref type="bibr" target="#b19">(Radford et al., 2019)</ref>, has become more widespread in recent years. <ref type="bibr" target="#b1">Budzianowski and Vulić (2019)</ref> first emphasized the ability to fine-tune all relevant information in plain text on GPT-2 in the context of task-oriented interaction. Later, <ref type="bibr" target="#b33">(Yang et al., 2021)</ref> were able to develop an end-to-end task-oriented agent that outperformed all earlier models with just a little tinkering. Yet, in a few recent methods, researchers <ref type="bibr" target="#b2">(Chiu et al., 2022;</ref><ref type="bibr" target="#b24">Sun et al., 2020)</ref> have tried to bridge the gap between chit-chat and taskoriented dialogue agents in an effort to make the task-oriented conversation more fascinating and appealing.</p><p>Persuasive dialogue generation : On the other hand, attempts were made to add persuasion (which is basically the superset of negotiation) into the NLG(Natural Language generation) module. According to Petty and Cacioppo's Elaboration Likelihood Model (ELM) <ref type="bibr" target="#b17">(Petty and Cacioppo, 1986)</ref>, a person's persuasion is dependent on varying degrees of processing information and a persuasive environment. The Persuasion Knowledge Model (PKM) proposed by <ref type="bibr" target="#b3">Friestad and Wright (1994)</ref> proposes that scientific and common persuasion knowledge are interconnected. The paper <ref type="bibr" target="#b18">(Qiu and Zhang, 2021)</ref> presents a personalized end-toend task-oriented conversation system that uses a memory network to provide appealing and personaconsistent responses. In recent articles, the researchers <ref type="bibr">(Tiwari et al., 2021</ref><ref type="bibr" target="#b26">(Tiwari et al., , 2022) )</ref> highlighted the DST(Dialogue State Tracker) module's application in task-oriented conversation agents to carry out persuasion in order to effectively identify and respond to changing user demands. Other than this, a meta-learned end-to-end model using GPT-2 has been proposed <ref type="bibr" target="#b20">(Raut et al., 2022</ref><ref type="bibr" target="#b21">(Raut et al., , 2023) )</ref> to produce persuasive dialogues in task oriented dialogue generation setting.</p><p>Negotiation dialogue generation: There hasn't been much literature developed in this area. <ref type="bibr" target="#b5">He et al. (2018)</ref> suggested a scenario for negotiation that can take advantage of semantic and tactical events. To learn conversation structure, <ref type="bibr">Zhou et al. (2019b)</ref> employed unsupervised learned FSTs (Finite State Transducer). Although this method clearly incorporates pragmatic tactics, it does not take advantage of the expressive power of neural networks. In contrast, <ref type="bibr" target="#b7">Joshi et al. (2021)</ref> tried to capture the interplay between the negotiation strategies in successive turns using Graph Attention Networks(GAT), and finally used a different decoder module to generate responses. In addition to the possibility of error propagation, they haven't taken any step to make sure the agent would provide a perfect counteroffer during negotiation. Here, we offered a method to deal with each of these issues.</p><p>Graph Neural Networks (GNN) : Graphical structure encoding has been successfully accomplished using hierarchical graph pooling-based encoders <ref type="bibr" target="#b35">(Zhang et al., 2019)</ref>. GNN-based encoders offer better expressive capabilities than HMM (Hid-den Markov Model) and FST-based encoders and can be trained by optimizing the downstream loss.</p><p>As they can be interpreted based on observed explicit sequences, they can also improve the model's interpretability <ref type="bibr" target="#b27">(Tu et al., 2020;</ref><ref type="bibr" target="#b15">Norcliffe-Brown et al., 2018)</ref>. Graphs have been employed in dialogue systems to direct response selections and dialogue policy. However, rather than composing dialogue strategies on-the-fly, they have been used to encode external knowledge <ref type="bibr" target="#b28">(Tuan et al., 2019;</ref><ref type="bibr" target="#b36">Zhou et al., 2018)</ref> or speaker information <ref type="bibr" target="#b4">(Ghosal et al., 2019)</ref>. The first person to combine GATs with hierarchical pooling was <ref type="bibr" target="#b7">Joshi et al. (2021)</ref>, who also used a conversation system to acquire practical dialogue techniques. Their model can be plugged into other models as an explicit sequence encoder, unlike earlier works. In order to provide more effective and appropriate negotiation responses, we have drawn inspiration from their concept and employed GAT embeddings to learn the perfect sequencing of negotiation strategies.  <ref type="bibr">et al. (2017)</ref>. While these games are useful for grounding and evaluation, they limit the dialogue domain and the linguistic richness.</p><p>In CraigListBargain, two agents are given the roles of a buyer and a seller and instructed to nego-   ) and a greater range of utterances. Workers were also urged to adorn the goods and bargain for extras like free delivery or pick-up. This incredibly realistic situation encourages deeper conversations.</p><p>Sentiment annotation : For this task, we used the rule-based sentiment analysis tool VADER (Valence Aware Dictionary and sEntiment Reasoner) <ref type="bibr" target="#b6">(Hutto and Gilbert, 2014)</ref>, which is created to examine the emotional tone of text documents. This tool accepts a statement as input and outputs the probability distribution across a sentiment space with three possible sentiments (positive, negative, and neutral). We determine the user's emotion at that turn by taking the sentiment with the greatest probability value from this output. The sentiment distribution of this dataset has been shown in Fig. <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Methodology</head><p>This section gives a detailed description of how we created our graph and integrated it into GPT-2 to enhance contextualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Graph Neural Network</head><p>This section reviews Graph Attention Networks (GATs) <ref type="bibr" target="#b30">(Velickovic et al., 2017;</ref><ref type="bibr" target="#b12">Li et al., 2021)</ref>, which has been used in this article. With a node set V made up of N nodes v i and an edge set E made up of all connections between nodes, a weighted undirected graph is defined as G = (V, E). A binary symmetric adjacency matrix N × N is defined as S, where [S] ij = 1 if (v i , v j ) ∈ E and 0 otherwise. Each node v i has feature vectors x i ∈ R F attached to it. These are compiled into N × F dimensional matrices called X, where F stands for the input feature size.</p><p>Keep in mind that SX (dot product of matrix S and X) is similar mathematically to transmitting each graph node's attributes to its neighbours. In this manner, S k X = S(S k-1 X) is similar to k rounds of feature swaps with neighbours. As seen in Fig. <ref type="figure">5</ref>, k = 0 represents self-connection, whereas k &gt; 0 combines characteristics from k nodes away.</p><p>An input X ∈ R N ×F is transformed by a GAT layer into an output G(X) ∈ R N ×G . As illustrated in Fig. <ref type="figure">5</ref>, each K-hop GAT layer is made up of P attention heads A (p) that combine k = 0, ..., K -1 cycles of feature aggregation over the graph as </p><formula xml:id="formula_0">A (p) (X; S) = K-1 k=0 (E ⊙ S) k XA (p) k G(X) = 1 P P p=1 σ[A (p) (X; S)],<label>(1)</label></formula><formula xml:id="formula_1">[E] ij = exp(LeakyReLU (e ij )</formula><p>)</p><formula xml:id="formula_2">k∈N i exp(LeakyReLU (e ik )) e ij = (x i ) T Q (p) x j ,<label>(2)</label></formula><p>where N i are nodes that are adjacent to node v i , and Q (p) are trainable F × F matrices that are used to calculate the attention. This is how, assigning dynamic weights to graph edges based on the input node attributes allows a GAT layer to selectively aggregate features.</p><p>GATs are built up from a series of L GAT layers G l , each with its own multi-headed graph attention mechanisms A p l . A set of input features X (0) t are transformed by the GAT into a set of output features</p><formula xml:id="formula_3">X (L) t at time t as X (l) t = G l (X (l-1) t ) for l = 1, ..., L (3)</formula><p>Note that, in this study, we conducted experiments using a range of output dimensions, and the findings for these experiments can be found in Table 6 in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph Formation</head><p>We have developed two separate graphs for negotiation strategies and dialogue acts. For negotiation strategies, we have defined the graph as G ns = (V ns , E ns ). V ns is the set of vertices or nodes where each node represents different negotiation strategies present in the dataset. Finally, we draw a directed edge (e i,j</p><p>ns ∈ E ns ) between node v i ns and node v j ns if these nodes are appearing in a consecutive manner in any turn of the dialogue. Following the same manner we define the graph for dialogue acts (G da ). For both instances, the overall Graph neural network is composed of a number of GAT layers followed by a number of linear layers. A miniature representation of our GNN for Negotiation strategies has been shown in Fig. <ref type="figure">4</ref>. On the left side of this image, we can see a clearer view of one of the graphs where different negotiation strategies ('propose', 'hedge_count', 'pos_sentiment', 'liwc_informal' and 'personal_concern') represents separate nodes and they are connected according to their occurrence on that turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Nego-GAT Architecture</head><p>We mostly adhered to the workflow proposed by <ref type="bibr" target="#b33">Yang et al. (2021)</ref>, incorporated the previously described GNN, and added the sentiment token as Figure <ref type="figure">6</ref>: The overall flow of our architecture. We begin our flow from the very left, with the very first utterance given by the user. Following that, our model continues to extract/generate numerous components (sentiment, belief state, negotiation strategies, dialogue action, and responses) and concatenates them into the context for the upcoming turns Figure <ref type="figure">7</ref>: An example of our overall context. Each component has been encased in different sets of tokens additional information into the context. GPT-2 is intended to simply obtain the whole context (C t ) and predict the next word by computing the conditional probabilities for every word over the entire lexicon (V ), as illustrated below :</p><formula xml:id="formula_4">W t = arg max w∈V P (w|C t )<label>(4)</label></formula><p>Utilizing this structure to our advantage, we trained our model using a self-supervised learning method to be able to generate each and every context component on its own. Our overall work-flow looks like this:</p><p>1. At turn t = 0, the user makes their initial utterance, U 0 .</p><p>2. Based on this {U 0 } the model predicts the user sentiment S 0 and concatenates it into the context.</p><p>3. The model will now derive the belief state, B 0 , based on this {U 0 , S 0 }. The "price" is the sole feasible belief state in our dataset. 5. This N 0 is also concatenated into the context. The model now chooses the collection of dialogue acts A 0 , depending on {U 0 , S 0 , B 0 , N 0 }. We anticipate A 0 using the same method as for negotiation strategies, using the relevant graph (G da ) made specifically for dialogue actions. The model guesses the first action da (0) 0 , gets the graph embedding vector, concatenates it to the context, and then moves on to the next dialogue act prediction. This way, finally it predicts the whole set of actions (A 0 = {da</p><formula xml:id="formula_5">(0) 0 , da (0) 1 , ..., da<label>(0)</label></formula><p>n }) for this turn.</p><p>6. We concatenate all these components in order to form the final context for response generation. So our final context is a stack of the overall history consisting of user utterance (U 0 ), sentiment (S 0 ), belief state (B 0 ), negotiation strategy (N 0 ), and dialogue act (A 0 ). Based on this, our model provides response R 0 and this completes the very first turn.</p><p>Up until the conversation's conclusion, this flow has been maintained. Each turn, the elements from every prior turn are concatenated to construct the final context. One example of our context and our overall architecture are shown in Fig. <ref type="figure">7</ref> and Fig. <ref type="figure">6</ref> respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In this section, we present all the findings from our research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baselines and Implementation Details</head><p>Nego-GAT is our proposed work. We compare Nego-GAT's performance to that of Dialo-Graph <ref type="bibr" target="#b7">(Joshi et al., 2021)</ref>, the most comprehensive model for the negotiating task now available, which also employs GAT to store dialogue actions and strategy sequences. The FST-enhanced hierarchical encoder-decoder model (FeHED) <ref type="bibr">(Zhou et al., 2019b)</ref>, which was the previous state-ofthe-art model, was also used to compare our agent's performance. Furthermore, we conduct a comparative study utilizing three different encoding techniques for negotiation strategies: HED, HED+RNN, and HED+Transformer. HED completely disregards both, in contrast to <ref type="bibr">HED+RNN and HED+Transformer, which employ RNN and Transformers (Vaswani et al., 2017)</ref> to encode the strategy and dialogue act information, respectively.</p><p>Our model has been implemented at the session level using DistilGPT2 <ref type="bibr" target="#b22">(Sanh et al., 2019)</ref>, a distilled version of GPT-2, and HuggingFace's Transformers <ref type="bibr" target="#b32">(Wolf et al., 2019)</ref>. This shows that the model has learned to create or predict the next word depending on the current word and has taken feedback from the entire dialogue into account. In CraigListBargain dataset we have multiple negotiation strategies and dialogue acts on each single turn and they are well mixed up. Keeping this in mind, dividing the dataset randomly into 3:1 ratio (train:test) we trained the model across 15 epochs on the training data using a temperature of 0.7, cross-entropy function as our loss function, and AdamW as our optimizer. Additionally, we defined the size of our embedding space, which is 5 (for both negotiation strategy and dialogue act), and chose the ideal GAT and linear layer configuration for our GNN, which is 3 GAT and 2 linear layers in this scenario, using an ablation study (See Appendix A for detailed results).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Discussion of Results</head><p>We examine our model's propensity for predicting both dialogue actions and negotiating strategies (internal evaluation) as well as its ability to elicit replies (external assessment). We examine dialogue generation in three different contexts: first, without negotiation strategy embeddings; second, without dialogue act embeddings; and third, with both embeddings present. Each outcome is discussed in extensive detail further down.</p><p>Negotiation strategy &amp; dialogue act prediction : Apart from generating dialogues our model is capable of choosing the necessary negotiation strategies and dialogue actions at each turn, which are nothing but a classification tasks. There aren't many full models for generating negotiation discourse that employ distinct modules to carry out these responsibilities. To assess our model's suitability for the task, we compare its performance with that of similar cutting-edge models. Due to the unbalanced class distributions of these two components (negotiation strategy and dialogue act), we only take the F1 score into account when assessing the models. Table <ref type="table" target="#tab_1">2</ref> reports the results of the respective models. The table demonstrates that GPT-2 performs nearly on par with the earlier models even without employing graph embeddings (see 4th row of the table), but the addition of graph embeddings (last row of the table) increases their performance significantly.</p><p>Dialogue generation : Our goal was to produce effective negotiation talks based on a context made up of the sentiment, belief state, negotiation strategy, and dialogue act. To determine how closely the produced replies match the gold human responses, we have utilized a number of automated assessment metrics, including the BLEU (BiLingual Evaluation Understudy) <ref type="bibr" target="#b16">(Papineni et al., 2002)</ref>, precision, recall, and F1 score. The results are provided in Table <ref type="table">4</ref>. As we have already shown, the usage of graph embeddings greatly aided in the selection of the dialogue act and negotiating approach. Now, we can see how it influences dialogue production as well. From Table <ref type="table">4</ref> it is very clear that choosing the right set of negotiation strategies eventually helps our model to generate dialogues closer to our  Human evaluation: To gauge how well these models perform qualitatively, we have also used human evaluations (See Table <ref type="table" target="#tab_2">3</ref>). As part of the human evaluation, we calculated the average number of words per turn, the average sales price ratio on which the deal was finalized, and four user ratings to assess four qualitative parameters (persuasive, coherent, natural, and understandable) in the generated responses on a scale of 1 to 5. In essence, these evaluations assess the following characteristics of the produced responses:</p><p>1. Persuasive : how well it persuades the user to accept a greater offer.</p><p>2. Coherent : How logically related or coherent the answers are.</p><p>3. Natural : To ascertain if the generated responses are more human-like or not.</p><p>4. Understandable : It is to evaluate the grammar and clarity of the responses.</p><p>The outcomes of both tables (Table <ref type="table" target="#tab_2">3</ref> and<ref type="table">Table 4</ref>) show that the replies we developed are more precise, fluid, and calming. Even the increase in the average sales price ratio and average words per turn demonstrates that in addition to generating rational and accurate replies, it performs extremely well in negotiations. An example, of comparing the outcomes of different modules is shown in Appendix B. Additionally, a full generation pipeline has been shown in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Error Analysis</head><p>Although our model is capable of carrying out multiple tasks by itself, it is unable to manage the counter price it is proposing. In Fig. <ref type="figure" target="#fig_5">9</ref> (in Appendix) one example of such scenario is shown.</p><p>Here, we can see that in the very first turn, our model makes an offer of price 6, but at the very next turn, instead of lowering its offer, it raises it to price 10. As a result, the deal did not take place. Our model has only been trained to predict the next token based on the present token. As a result, it is unable to compare the digits to their values. We will seek for a solution to this issue during our future study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Works</head><p>This study demonstrates how combining GNN with the comprehensive pre-trained language model GPT-2 aids in selecting the appropriate set of strategies and dialogue actions, thereby improving the efficacy of negotiating discussions. Because of graph embeddings, which provide information on the sequence in which negotiation strategies were used at the ground truth, our model can successfully imitate these strategies in a real-world context, keeping the generated dialogues fluid and appealing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations</head><p>Despite all the above-mentioned benefits, our approach has certain drawbacks as well. Only the English language can be used to communicate with this model, and it is very reliant on the type of data provided to it during training. For instance, it cannot effectively bargain with a consumer who wants to purchase some flowers, since conversations related to selling flowers were not included in the dataset. In addition, even if this model is producing counter prices that are higher than those of the older models, it may not always be effective at closing deals. To do that, we must create counterproposals based on the person's personality feature or some other calculated tactic. In the future, we'll look for answers to these issues and create a model for this task that is more potent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Ethics Statement</head><p>We used widely used, publicly accessible datasets to model negotiations. Without violating any copyright concerns, we complied with the dataset's regulations. Additionally, we employed another openly accessible and publicly available tool to obtain the sentiment annotations on this dataset. It is necessary to consider an ethical aim while creating bargaining conversational AI. Only the product price is subject to negotiation in our utilized dataset.</p><p>We made an effort to create a flawless model that could negotiate deals with user parties with ease and composure and, in our opinion, can make a significant influence on the field of online exchange and marketing. Last but not least, because generative models lack context-sensitive information, they may produce uninformative statements. As a result, it is necessary to model knowledge grounding or fact-verification. We'll use the findings of this investigation in our future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Ablation Study</head><p>In order to find the best GAT and linear layer combination for our GNN, we currently fixed the embedding size at 3 and experimented with several combinations. After getting the embeddings (for both dialogue acts and negotiation strategies) from each of these GNNs we have trained our model for 5 epochs each and noted their performances in Table <ref type="table">5</ref>. Following these experiments we chose 3 GAT -2 Linear as our final combination of our GNN. After that, we conducted another investigation to determine the ideal embedding size for our task. Using the previous determined GNN structure we trained the model for 20 epochs and noted the result in Table <ref type="table">6</ref>. From this table it is quite clear that it does not provide a lot of information concerning upcoming strategies for size 3. On the other hand, the context typically becomes too vast when the embedding size is increased to 7. Because of this, the majority of contexts are larger than the embedding size of GPT-2, losing other significant context elements in the process. Using an embedding size of 5, it appears we can achieve a flawless trade-off between these two scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Conversations Comparison</head><p>Outcomes of DialoGraph and our model (with and without graph embeddings) is shown in Fig. <ref type="figure">8</ref>. The results clearly demonstrate that DialoGraph <ref type="bibr" target="#b7">(Joshi et al., 2021)</ref> is effective but not very attractive during negotiations. While our models are not just producing greater counter offers, the average number of words at each turn is also significantly higher.</p><p>Another thing to note is that although our model first offered a lesser counter offer, it later chose the appropriate strategy and concluded the sale at a higher price which can actually be effective in real life scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Example of a Complete Conversation</head><p>Nego-GAT is capable of producing sentiment, belief states, negotiation strategies, dialogue actions and finally the responses. An example of this whole generation flow has been shown in Fig. <ref type="figure" target="#fig_0">10</ref>. The graph embedding of each of the negotiation strategies (green in the picture) and dialogue acts (purple in the image) is also displayed for clarification at each step. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of negotiation conversation</figDesc><graphic coords="1,316.06,428.86,198.42,240.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Turn length distribution of CraigListBargain dataset after the removal of faulty utterances</figDesc><graphic coords="3,306.14,366.66,226.77,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Sentiment distribution of CraigListBargain fetched from VADER<ref type="bibr" target="#b6">(Hutto and Gilbert, 2014)</ref> </figDesc><graphic coords="4,70.87,70.87,198.43,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: A dummy representation of our Graph Neural Network for negotiation strategies (a similar kind of architecture has been followed for dialogue acts)</figDesc><graphic coords="5,70.87,71.86,453.55,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>4. The model must select the appropriate collection of negotiating strategies (N 0 ) based on {U 0 , S 0 , B 0 } while maintaining their order. At this point, the previously defined GNN architecture is put into action. At every turn, our model chooses the set of negotiation strategies one by one. After choosing the first negotiation strategy (ns (0) 0 ) of this very first turn we get the respective graph embedding vector from our GNN (G ns ) and concatenate it into the context. Based on this, the model predicts the next negotiation strategy (ns (0) 1 ). Following this, the model chooses the whole set of negotiation strategies (N 0 = {ns for this turn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: An example of an unsuccessful conversation generated by our model. The left side dialogues (in rectangles) are created by the user, while the right side dialogues (in ellipsoidal curves) are generated by the agent</figDesc><graphic coords="13,212.60,290.62,170.08,226.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="12,70.87,496.35,453.55,198.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison of different baselines and our model for the task of negotiation strategy and dialogue act prediction</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Negotiation strategy</cell><cell></cell><cell cols="2">Dialogue act</cell></row><row><cell></cell><cell></cell><cell></cell><cell>F1 score</cell><cell></cell><cell></cell><cell>F1 score</cell></row><row><cell>Model</cell><cell></cell><cell cols="6">Macro Micro Weighted Macro Micro Weighted</cell></row><row><cell cols="2">FeHED [(Zhou et al., 2019b)]</cell><cell>12.3</cell><cell>15.7</cell><cell>17.8</cell><cell>14.2</cell><cell>18.2</cell><cell>25.4</cell></row><row><cell>HED+RNN</cell><cell></cell><cell>15.1</cell><cell>20.5</cell><cell>22.7</cell><cell>17.7</cell><cell>21.3</cell><cell>29.4</cell></row><row><cell>HED+Transformer</cell><cell></cell><cell>17.8</cell><cell>22.1</cell><cell>24.1</cell><cell>18.8</cell><cell>22.6</cell><cell>31.3</cell></row><row><cell cols="2">DialoGraph [(Joshi et al., 2021)]</cell><cell>18.1</cell><cell>23.7</cell><cell>26.9</cell><cell>20.1</cell><cell>24.6</cell><cell>32.7</cell></row><row><cell cols="3">GPT-2 without Graph embedding 18.3</cell><cell>23.5</cell><cell>27.4</cell><cell>21.7</cell><cell>25.5</cell><cell>33.3</cell></row><row><cell>Nego-GAT(our model)</cell><cell></cell><cell>20.3</cell><cell>26.5</cell><cell>33.2</cell><cell>24.3</cell><cell>29.1</cell><cell>37.5</cell></row><row><cell>Model</cell><cell cols="7">Persuasive Coherent Natural Understandable Sale Price Ratio Avg words/turn</cell></row><row><cell>HED</cell><cell>2.50</cell><cell>2.50</cell><cell>3.50</cell><cell>2.50</cell><cell></cell><cell>-2.13</cell><cell>4.25</cell></row><row><cell>FeHED (Zhou et al., 2019b)</cell><cell>3.30</cell><cell>3.75</cell><cell>3.70</cell><cell>3.69</cell><cell></cell><cell>0.25</cell><cell>5.76</cell></row><row><cell>HED+RNN</cell><cell>2.81</cell><cell>3.27</cell><cell>3.36</cell><cell>3.27</cell><cell></cell><cell>-3.68</cell><cell>3.61</cell></row><row><cell>HED+Transformer</cell><cell>3.50</cell><cell>3.50</cell><cell>3.70</cell><cell>3.40</cell><cell></cell><cell>-0.07</cell><cell>4.36</cell></row><row><cell>DIALOGRAPH (Joshi et al., 2021)</cell><cell>3.58</cell><cell>3.94</cell><cell>3.75</cell><cell>3.70</cell><cell></cell><cell>0.49</cell><cell>5.84</cell></row><row><cell cols="2">Nego-GAT (without any embedding) 3.86</cell><cell>3.95</cell><cell>4.12</cell><cell>4.50</cell><cell></cell><cell>0.52</cell><cell>6.22</cell></row><row><cell>Nego-GAT (Our final model)</cell><cell>3.98</cell><cell>4.00</cell><cell>4.12</cell><cell>4.50</cell><cell></cell><cell>0.56</cell><cell>8.54</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Human evaluation results to assess the qualitative aspects of the generated responses</figDesc><table><row><cell>Model</cell><cell cols="4">BLEU Precision Recall F1 score</cell></row><row><cell>HED</cell><cell>10.9</cell><cell>10.0</cell><cell>9.3</cell><cell>7.8</cell></row><row><cell>FeHED (Zhou et al., 2019b)</cell><cell>12.1</cell><cell>12.6</cell><cell>12.0</cell><cell>11.0</cell></row><row><cell>HED+RNN</cell><cell>13.1</cell><cell>12.5</cell><cell>12.9</cell><cell>11.5</cell></row><row><cell>HED+Transformer</cell><cell>15.2</cell><cell>14.7</cell><cell>14.5</cell><cell>13.7</cell></row><row><cell>DialoGraph (Joshi et al., 2021)</cell><cell>15.2</cell><cell>14.9</cell><cell>14.8</cell><cell>14.1</cell></row><row><cell>Nego-GAT without any embedding</cell><cell>16.9</cell><cell>16.5</cell><cell>16.1</cell><cell>15.8</cell></row><row><cell cols="2">Nego-GAT with strategy embedding 17.7</cell><cell>17.5</cell><cell>16.9</cell><cell>15.2</cell></row><row><cell>Nego-GAT (Our final model)</cell><cell>17.9</cell><cell>17.5</cell><cell>17.1</cell><cell>15.6</cell></row><row><cell cols="5">Table 4: The results of automatic evaluation to measure</cell></row><row><cell cols="5">how closely the generated responses correspond to the</cell></row><row><cell>gold standard</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>standard gold responses.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We have performed belief state annotation on the CraigListBargain dataset<ref type="bibr" target="#b5">(He et al., 2018)</ref> </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This paper presents research findings from the project "<rs type="projectName">Conversational Agents with Negotiation and Influencing Ability</rs>" (Project No. <rs type="grantNumber">IITP/2023/814</rs>), sponsored (faculty research grant) by <rs type="funder">Accenture Pvt Ltd</rs>. We are grateful to <rs type="funder">Accenture Pvt Ltd</rs> and the research team for collaborating on the research problem with huge real-world implications.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_gS6N9KD">
					<idno type="grant-number">IITP/2023/814</idno>
					<orgName type="project" subtype="full">Conversational Agents with Negotiation and Influencing Ability</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Discourse structure and dialogue acts in multiparty dialogue: the stac corpus</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Asher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Morey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farah</forename><surname>Benamara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stergos</forename><surname>Afantenos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Conference on Language Resources and Evaluation (LREC 2016)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2721" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Hello, it&apos;s gpt-2-how can i help you? towards the use of pretrained language models for task-oriented dialogue systems</title>
		<author>
			<persName><forename type="first">Paweł</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05774</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Ssu</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maolin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Ting</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.10591</idno>
		<title level="m">Salesbot: Transitioning from chitchat to task-oriented dialogues</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The persuasion knowledge model: How people cope with persuasion attempts</title>
		<author>
			<persName><forename type="first">Marian</forename><surname>Friestad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of consumer research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Deepanway</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niyati</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11540</idno>
		<title level="m">Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Decoupling strategy and generation in negotiation dialogues</title>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anusha</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09637</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vader: A parsimonious rule-based model for sentiment analysis of social media text</title>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Hutto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international AAAI conference on web and social media</title>
		<meeting>the international AAAI conference on web and social media</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="216" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vidhisha</forename><surname>Balachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00920</idno>
		<title level="m">Dialograph: Incorporating interpretable strategy-graph networks into negotiation dialogues</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Tartan: A retrievalbased socialbot powered by a dynamic finite-state machine architecture</title>
		<author>
			<persName><forename type="first">George</forename><surname>Larionov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Kaden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hima</forename><surname>Varsha Dureddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Bayomi T Kalejaiye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranavi</forename><surname>Srividya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit Parag</forename><surname>Potharaju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">I</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><surname>Rudnicky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01260</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deal or no deal? end-to-end learning for negotiation dialogues</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05125</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Georgios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Dolan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06155</idno>
		<title level="m">A persona-based neural conversation model</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06547</idno>
		<title level="m">Adversarial learning for neural dialogue generation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Message-aware graph attention networks for large-scale multi-robot path planning</title>
		<author>
			<persName><forename type="first">Qingbiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Prorok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="5533" to="5540" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Endto-end trainable non-collaborative dialog system</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8293" to="8302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end learning of task-oriented dialogs</title>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="67" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning conditioned graph structures for interpretable visual question answering</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Norcliffe-Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stathis</forename><surname>Vafeias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Parisot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The elaboration likelihood model of persuasion</title>
		<author>
			<persName><forename type="first">E</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">T</forename><surname>Petty</surname></persName>
		</author>
		<author>
			<persName><surname>Cacioppo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning personalized end-to-end task-oriented dialogue for fast and reliable adaptation</title>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on Digital Society and Intelligent Systems (DSInS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="62" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Introducing multi-modality in persuasive task oriented virtual sales agent</title>
		<author>
			<persName><forename type="first">Aritra</forename><surname>Raut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subrata</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhisek</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriparna</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anutosh</forename><surname>Maitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roshni</forename><surname>Ramnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubhashis</forename><surname>Sengupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="543" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reinforcing personalized persuasion in task-oriented virtual sales assistant</title>
		<author>
			<persName><forename type="first">Aritra</forename><surname>Raut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhisek</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subrata</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriparna</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anutosh</forename><surname>Maitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roshni</forename><surname>Ramnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubhashis</forename><surname>Sengupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plos one</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">275750</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<title level="m">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A neural network approach to context-sensitive generation of conversational responses</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06714</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungwhan</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Crook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Becka</forename><surname>Silvert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunjoon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12757</idno>
		<title level="m">Adding chit-chat to enhance task-oriented dialogues</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Shubhashis Sengupta, Anutosh Maitra, Roshni Ramnani, and Pushpak Bhattacharyya. 2021. A dynamic goal adapted task oriented dialogue agent</title>
		<author>
			<persName><forename type="first">Abhisek</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tulika</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriparna</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plos one</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">249030</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A persona aware persuasive dialogue policy for dynamic and co-operative goal setting</title>
		<author>
			<persName><forename type="first">Abhisek</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tulika</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriparna</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubhashis</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anutosh</forename><surname>Maitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roshni</forename><surname>Ramnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">195</biblScope>
			<biblScope unit="page">116303</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9073" to="9080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Dykgchat: Benchmarking dialogue generation grounding on dynamic knowledge graphs</title>
		<author>
			<persName><forename type="first">Yi-Lin</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00610</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="10" to="48550" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Xuewei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoojung</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06725</idno>
		<title level="m">Persuasion for good: Towards a personalized persuasive dialogue system for social good</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-ofthe-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ubar: Towards fully end-to-end task-oriented dialog system with gpt-2</title>
		<author>
			<persName><forename type="first">Yunyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="14230" to="14238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Using pomdps for dialog management</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spoken Language Technology Workshop</title>
		<imprint>
			<biblScope unit="page" from="8" to="13" />
			<date type="published" when="2006">2006. 2006</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengwei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05954</idno>
		<title level="m">Hierarchical graph pooling with structure learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Commonsense knowledge aware conversation generation with graph attention</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haizhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4623" to="4629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Yiheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13426</idno>
		<title level="m">A dynamic strategy coach for effective negotiation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Yiheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13425</idno>
		<title level="m">Augmenting non-collaborative dialog systems with explicit semantic and strategic history</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
