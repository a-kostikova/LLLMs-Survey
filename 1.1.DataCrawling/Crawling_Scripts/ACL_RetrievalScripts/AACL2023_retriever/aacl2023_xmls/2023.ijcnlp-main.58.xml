<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GrailQA++: A Challenging Zero-Shot Benchmark for Knowledge Base Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ritam</forename><surname>Dutt</surname></persName>
							<email>rdutt@andrew.cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Sopan</forename><surname>Khosla</surname></persName>
							<email>sopankh@amazon.com</email>
						</author>
						<author>
							<persName><forename type="first">Vinayshekhar</forename><forename type="middle">Bannihatti</forename><surname>Kumar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rashmi</forename><surname>Gangadharaiah</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">AWS AI Labs</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">AWS AI Labs</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">AWS AI Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GrailQA++: A Challenging Zero-Shot Benchmark for Knowledge Base Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">922430AB1F85E96435CDA5C259BCACE1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most benchmarks designed for question answering over knowledge bases (KBQA) operate with the i.i.d. assumption where one encounters the same schema items during inference as those observed during training. Recently, the GrailQA dataset was established to evaluate zero-shot generalization capabilities of KBQA models as a departure from the i.i.d. assumption. Reasonable performance of current KBQA systems on the zero-shot GrailQA split hints that the field might be moving towards more generalizable systems. In this work, we observe a bias in the GrailQA dataset towards simpler one or two-hop questions, which results in an inaccurate assessment of the aforementioned prowess. We propose GrailQA++, a challenging zero-shot KBQA test set that contains more questions relying on complex reasoning. We leverage the concept of graph isomorphisms to control the complexity of the questions and to ensure that our proposed test set has a fair distribution of simple and complex questions. Existing KBQA models suffer a substantial drop in performance on our constructed new test set as compared to the GrailQA zero-shot split. Our analysis reveals how isomorphisms can be used to understand the complementary strengths of different KBQA models and provide a deeper insight into model mispredictions. Overall, our paper highlights the non-generalizability of existing models and the necessity for designing more challenging benchmarks. Our dataset is available at https://github.com/sopankhosla/ GrailQA-PlusPlus * Work conducted during an internship at Amazon.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of KBQA involves querying a knowledge base (KB) for a set of entities that satisfies a natural language question. Most prior work in KBQA has been restricted to an i.i.d. setting <ref type="bibr" target="#b23">(Yih et al., 2016;</ref><ref type="bibr" target="#b19">Talmor and Berant, 2018)</ref>, where the classes and relations constituting the KB remains unchanged during inference and training. However, the ubiquitous applications of KBQA in different domains such as tax, insurance, and healthcare <ref type="bibr" target="#b14">(Lüdemann et al., 2020;</ref><ref type="bibr" target="#b6">Huang et al., 2021;</ref><ref type="bibr" target="#b16">Park et al., 2020)</ref> has prompted research on KBQA generalizabilition to facilitate transfer to these domains <ref type="bibr" target="#b2">(Dutt et al., 2022;</ref><ref type="bibr">Das et al., 2021;</ref><ref type="bibr">Neelam et al., 2022;</ref><ref type="bibr" target="#b7">Jiang and Usbeck, 2022)</ref>.</p><p>The most salient work is that of <ref type="bibr" target="#b3">Gu et al. (2021)</ref> where they propose the task of KBQA generalizability beyond the i.i.d setting, which they term "zero-shot" generalizability. In a zero-shot setting, KBQA models operate upon classes and relations which were unobserved during training. They also create a dataset called GrailQA to benchmark the generalizability of KBQA models. This dataset has garnered significant research interest with state-ofthe-art KBQA models <ref type="bibr" target="#b22">(Ye et al., 2021;</ref><ref type="bibr" target="#b24">Yu et al., 2022;</ref><ref type="bibr">Gu and Su, 2022;</ref><ref type="bibr" target="#b17">Shu et al., 2022;</ref><ref type="bibr" target="#b13">Liu et al., 2022)</ref> achieving remarkable performance on the leaderboard, specifically on the zero-shot setting. <ref type="foot" target="#foot_0">1</ref>However, a closer inspection of the GrailQA dataset reveals that it is biased towards simpler questions and that existing KBQA systems cannot deal with complex cases in a non-i.i.d. setting. We put forward the notion of graph isomorphisms to characterize the complexity of the questions, which is similar in spirit to the idea of reasoning paths or semantic structures of <ref type="bibr" target="#b11">(Li and Ji, 2022;</ref><ref type="bibr" target="#b0">Das et al., 2022)</ref>. We observe a pronounced skewness in the distribution of isomorphisms in the GrailQA dataset. The simplest isomorphism, where the answer is located one hop away from starting entity, comprises 78.5% of the GrailQA zero-shot samples, while a more complex isomorphism with answers three hops away accounts for only 0.53%. In this work, we leverage the concept of isomorphisms to explore the generalization abilities of KBQA models on questions of varying complexity.</p><p>We propose a new zero-shot benchmark called GrailQA++ that has a balanced distribution of simple and complex isomorphisms. The dataset comprises of questions annotated by domain experts as well as questions from well-known pre-existing KBQA datasets that are built over the same Freebase database as GrailQA. We evaluate two stateof-the-art (SOTA) KBQA models on this benchmark and observe that the performance falls significantly (28.5 on GrailQA++ as opposed to 83.5 on GrailQA). Our analysis shows that this drop can be attributed partly to the skewed distribution in GrailQA and that different models fare better on different isomorphism categories. Our contributions are the following: • We leverage the concept of graph isomorphisms to analyze the complexity of KBQA questions. • We create a new benchmark (GrailQA++) with complex questions to evaluate zero-shot generalizability of KBQA models. • Our experiments show that SOTA models perform poorly on the new dataset, emphasizing that KBQA generalizability is still a challenge.<ref type="foot" target="#foot_1">2</ref> • We also carry out extensive error analysis to inspect model mispredictions and nongeneralizability that would serve subsequent research in creating better benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In this section, we describe the task setting and the different levels of generalization in the context of KBQA. A more detailed description can be found in <ref type="bibr">(Gu et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Formulation</head><p>Knowledge Base: We denote a Knowledge Base or a KB as K = (O, M), where O defines the ontology of the KB and M specifies the set of relational facts present in K on the basis of O. The ontology is a subset of all possible relations R that can exists between two classes, which are denoted by</p><formula xml:id="formula_0">C i.e., O ⊆ C × R × C. Likewise, the set of facts is represented as M ⊆ E × R × (L ∪ E ∪ C)</formula><p>, where E and L denote the set of possible entities and literals respectively. Semantic-parsing based KBQA: Given the KB, K, and a natural language question q, the objective of KBQA is to find a set of entities (answers A) that satisfies the question q. In a semantic-parsing or translation based setting, the task of KBQA involves converting q into its corresponding logical form L q . This L q is executed over the K to obtain the answers. Examples of logical forms include S-expressions, SPARQL queries, and λ-calculus.</p><p>Each logical form L q has a particular schema S q that includes elements from the set of relations, classes, and other constructs specific to the logicalform. The specific composition of items in S q forms a logical template or T q . E.g., the questions "Who wrote Pride and Prejudice?" and "Who was the author of Oliver Twist?" have the same template but different logical forms since they refer to different novels. However the questions "Who wrote Pride and Prejudice?" and "Which author wrote both the Talisman and It?" have the same schema but different logical templates since the former involves only one constraint or entity ("Pride and Prejudice") while the latter specifies two ("Talisman" and "It"),</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">KBQA Generalization</head><p>Gu et al. ( <ref type="formula">2021</ref>) puts forward the three levels of generalization based on how the schema S q and logical template T q for a question q differs from the set of all possible schema items and templates seen during training, i.e. S train and T train respectively.</p><p>(i) I.I.D. generalization occurs when S q ⊂ S train and T q ∈ T train .</p><p>(ii) Compositional generalization occurs when S q ⊂ S train but T q / ∈ T train . Thus the questions operate upon a subset of schema items seen during training but they have new templates.</p><p>(iii) Zero Shot generalization occurs when ∃s ∈ S q such that s / ∈ S train . Thus the questions operate upon novel schemas, mostly new classes and relations that were not encountered during training.</p><p>Conceptually, these three levels of generalization could be stacked in an hierarchical fashion in increasing order of difficulty; with I.I.D. being the least challenging since it operates over templates seen during training, followed by Compositional, which occurs over unseen templates, and then Zero Shot which have unseen schema items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Isomorphisms in GrailQA</head><p>In a semantic-parsing based KBQA setting, a natural language question is first converted to a logical form and then executed over the KB to yield an answer. To ensure generalization, such KBQA models need to handle different kinds of logical forms.  <ref type="table">1</ref>: Distribution of isomorphisms in the GrailQA (Dev) set and our curated GrailQA++ dataset (Tot). We show the total count of isomorphisms for each of the datasets (Freq) and their corresponding proportion in % (Perc). Note that complex isomorphisms belonging to Iso-6, Iso-8, and Iso-11 do not occur in the original GrailQA dataset. The red and green nodes in each isomorphism correspond to the constraints and the final answer respectively.</p><p>In this section we propose a way to categorize these logical forms using the notion of isomorphisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Isomorphisms</head><p>Each logical form L q has an equivalent graphical notation G q , where the set of vertices V q correspond to the different constraints (E, L) and classes C in the L q while edges E q represents the relations R present in L q . This notation is similar to the design of query-graphs <ref type="bibr" target="#b10">(Lan and Jiang, 2020)</ref> but where the operations (aggregation or comparative) do not have any specialized vertices. We however denote one of the vertices in V q that correspond to the answers as A q and call it the root. The nodes corresponding to the root and the constraints are denoted in green and red respectively in Figure <ref type="figure" target="#fig_0">1</ref>. We say two logical forms for questions q i and q j belong to the same isomorphism category, iff their equivalent graphs G q i and G q j are isomorphic. Subsequently, two graphs G q i and G q j are isomorphic iff there exists a mapping function ψ from V q i to V q j such that ∀m, n nodes in V q i , that correspond to an edge in G q i i.e (m, n) ∈ E q i , the mapping of the nodes should also correspond to an edge in G q j or, (ψ(m), ψ(n)) ∈ E q j . This mapping is bijective. Furthermore the roots in the two graphs also share the same mapping, i.e. A q j = ψ(A q i ).</p><p>Isomorphisms describe how the constraints in the query graph are connected to the root (or the answer). It obfuscates any specific information such as the name of the entities or classes in the graph. They provide a unified way to characterize a query graph (and subsequently a logical form) based on the number of constraints, and the number of hops required to reach the answer from said constraints. For example, in Figure <ref type="figure" target="#fig_0">1</ref>, the green Tea node corresponds to Ans while the red constraint nodes, Fujian and White tea, corresponds to E1 and E2 respectively. Thus the given logical form is an instance of Iso-2. The distribution of isomorphisms spanning all datasets appears in Table <ref type="table" target="#tab_8">7</ref>.</p><p>While the notion of isomorphisms is similar in concept to the idea of reasoning paths <ref type="bibr" target="#b0">(Das et al., 2022)</ref> or semantic structures <ref type="bibr" target="#b11">(Li and Ji, 2022)</ref>, we use the generic definition of "isomorphisms" to account for the fact that these graph isomorphisms can also have cycles in them. For example, in Table <ref type="table" target="#tab_8">7</ref>, we note instances of isomorphisms (CIso-0 to CIso-4) where at least one cycle is present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Statistics for GrailQA</head><p>We categorize the questions in GrailQA according to the isomorphism type of the corresponding logical form. We refer to isomorphisms with fewer than 3 relations as simple and the rest as complex isomorphisms. The simple isomorphisms for the remainder of the paper are Iso-0,1,2. We show the distribution of the isomorphisms in the zero-shot development data of GrailQA in Table <ref type="table">1</ref>.</p><p>We observe that the simple isomorphisms (Iso-0, 1, 2) comprise more than 97% of all zero-shot examples in the development set. A similar story holds true for the train set where 95% of all isomorphisms belong to these three classes (See Table <ref type="table" target="#tab_8">7</ref> in the Appendix). We hypothesize that this skewness could exaggerate the perceived generalization capabilities of KBQA models, such that the staggering numbers on the leaderboard reflect the performance on these simpler isomorphisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GrailQA++</head><p>To gauge whether KBQA models exhibit zero-shot generalization capabilities across different isomorphisms, we propose GrailQA++, a challenging dataset with an equal distribution of simple and complex isomorphisms. To create GrailQA++, we not only employ annotators with prior expertise in KBQA, but also leverage pre-existing KBQA datasets. We outline the creation process below and illustrate the same in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Expert Annotated Instances</head><p>We describe our controlled approach to sample and annotate instances of different isomorphism classes. Our process is similar to that of GrailQA albeit with a few differences, namely in terms of query sampling and natural language query generation.</p><p>Query Graph Sampling: GrailQA was created using the OVERNIGHT process <ref type="bibr" target="#b18">(Su et al., 2016)</ref> which extracts templates by traversing Freebase and obtains a query graph. Since traversal is easier for simpler hops and subsequently simpler isomorphisms, they appear higher in GrailQA. We, however, follow a more controlled algorithm to sample the query graph.</p><p>We first choose a particular isomorphism, which determines the number of constraints. If there is exactly one constraint (Iso-0, 1, and 5), we first choose a class at random and then sample an entity randomly from that class. We then follow the relations that originate from the instantiated entity and continue our traversal of the KB till we reach the answer node. In case of multiple constraints (Iso-2, 3, 4, and 11), we first randomly sample the answer class and then traverse the KB by adding relations in a manner that conforms with the isomorphism structure. At each expansion step, we ensure that there exists an entity which can be instantiated using the new relation. This ensures executability of the current sub-query and thus of the main query.</p><p>The authors chose to sample instances corresponding to Iso-0,1,2,3,4,5 since these were already present in the zero-shot split of GrailQA. Additionally, we also sampled and annotated instances of Iso-11, since it was the simplest isomorphism that could be formed with three constraints.</p><p>Filtering: We filter query graphs that do not conform with the zero-shot generalizability criteria. Specifically, the query graph should have at least one class or relation absent from the GrailQA training split. Later, we employ the filtering techniques proposed in <ref type="bibr" target="#b3">Gu et al. (2021)</ref> to discard illegal relations, and ignore instances with entities or relations written in a language other than English.</p><p>Logical Form: Once we obtain the filtered query graph, we convert it to its canonical logical form using the deterministic algorithm of <ref type="bibr" target="#b3">Gu et al. (2021)</ref>. We then execute this logical form over Freebase to obtain the answers, and discard instances where the logical form was inexecutable or unanswerable.</p><p>Natural Language Query Annotation: To create the corresponding natural language question we choose annotators who are fluent in English, are current working professionals with a graduate degree to their name, and have prior domain expertise in KBQA. The annotators are first provided with a design document with examples of query graphs and their corresponding logical form. We also provide the annotators with aliases of the constraints and relations to better interpret the query graph as they compose the corresponding question. <ref type="foot" target="#foot_2">3</ref> We randomly select 35 instances (5 from each isomorphism) to include in the pilot study after which the annotators meet to discuss their interpretations and resolve any differences. We find that all three annotators agree on 75% of the examples, while at least two agree on 97%. The main causes of disagreement was determining how explicitly the entities should be referred in the NL query. The annotators decided to be explicit in specifying the hidden nodes to facilitate evaluation. Finally, we sample a large set with 1000 unique query-graphs equally distributed among the three annotators. We ensure a balanced distribution between the different kinds of isomorphisms (see Table <ref type="table">1</ref>). All annotations were carried out by domain experts and we did not employ any crowd-workers unlike in <ref type="bibr" target="#b3">Gu et al. (2021)</ref> for paraphrasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pre-existing Datasets</head><p>We also leveraged pre-existing public datasets that were built over the same Freebase KB as GrailQA. These datasets were chosen since they were designed to evaluate progress on KBQA.</p><p>WebQSP <ref type="bibr" target="#b23">(Yih et al., 2016)</ref> uses Amazon Mechanical Turk to answer questions from non-experts collected using the Google Suggest API. Since the dataset is restricted to to "wh" questions from nonexperts the questions tend to more colloquial.</p><p>GraphQ <ref type="bibr" target="#b18">(Su et al., 2016)</ref> was created in a fashion similar to GrailQA with questions exhibiting variation in terms of complexity, topic space, and number of answers.</p><p>ComplexWebQuestions (CWQ) (Talmor and Berant, 2018) was created on top of WebQSP with the intention of generating complex questions by incorporating compositions (more hops), conjuctions (more constraints), and superlatives and comparatives (more function types).</p><p>Zero-shot splits: We consider only the questions in the test splits of the pre-existing datasets which satisfy the zero-shot crieteria of <ref type="bibr" target="#b3">Gu et al. (2021)</ref>. Specifically, zero-shot instances have at least one schema item (class or relation) that were not seen during training in the training data of GrailQA. Following <ref type="bibr" target="#b9">Khosla et al. (2023)</ref>, we also exclude questions if a relation's corresponding inverse relation was observed during training to make the task more challenging. We follow the same criteria for the expert annotated dataset as well.</p><p>Isomorphism crietrion: We sample instances corresponding to the following isomorphisms, Iso-0,1,2,3,4,5,6,8. The selection of these isomorphisms were driven by two criteria, namely (i) the isomorphisms should be present in the training split of the GrailQA dataset and (ii) there should be sufficient representation of these isomorphisms in the combined test-split of GrailQA++(&gt;50).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Statistics of GrailQA++</head><p>We present the distribution of isomorphisms corresponding to our curated GrailQA++ in Table <ref type="table">1</ref>. We see that simple and complex isomorphisms are equally represented in the dataset, where the simple isomorphsims that correspond to Iso-0,1,2 comprise 50.1% of the dataset. We also include isomorphisms corresponding to Iso-6, Iso-8, and Iso-11 which are absent in the original dev split of GrailQA. This enables us to evaluate the zero-shot generalization performance of KBQA models on these unseen isomorphism categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>Baselines: We experiment with two semanticparsing baselines for KBQA namely RNG-KBQA <ref type="bibr" target="#b22">(Ye et al., 2021)</ref> and ArcaneQA <ref type="bibr">(Gu and Su, 2022)</ref>.</p><p>We chose these models because they encapsulate two different strategies of carrying out semantic parsing in the context of KBQA <ref type="bibr">(Gu et al., 2022)</ref>. Furthermore, they achieve impressive performance on the GrailQA leaderboard and also have publicly available checkpoints which can be used for evaluation. We follow the inference setting mentioned in their Github repositories, with the single exception that for RNG-KBQA we do not restrict ourselves to the subset of Freebase domains for GrailQA. RNG-KBQA <ref type="bibr" target="#b22">(Ye et al., 2021</ref>) follow a rankingbased approach wherein they first enumerate all possible candidates and then perform semantic matching to rank the enumerated candidates in decreasing order of relevance. They then use a pretrained LM (T5-large) to generate an executable query from the top-ranked candidates.</p><p>ArcaneQA <ref type="bibr">(Gu and Su, 2022)</ref> employ a seq2seq generative LM to obtain the final logical form from the natural language query. They leverage a constrained decoding paradigm that leverages the information in the KB during query generation to ensure executability.</p><p>Evaluation Criteria: We evaluate the performance of the two baselines in terms of EM (exact match) and F1 scores (between the predicted and gold answers). We decouple the impact of entity recognition and entity linking from the main task of KBQA by providing gold entities during inference. All experiments are carried out on a RTX-1080Ti GPU with 12GB RAM, using the author-provided model-checkpoints on the public GrailQA dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>In this section we put forward the following research questions and attempt to answer the same. RQ1. How well do the baselines generalize to our proposed GrailQA++ dataset?</p><p>We present the zero-shot performance of RNG-KBQA and ArcaneQA on GrailQA and GrailQA++ in Table <ref type="table" target="#tab_1">2</ref>. We observe that models show impressive performance on GrailQA with RNG-KBQA achieving a very high F1 score of 86.0 overall. We also note that these models suffer a drop of at least 10 points in <ref type="bibr">Gu and Su (2022)</ref> in absence of gold entities, emphasizing the importance of NER and entity-linking (EL) for KBQA.</p><p>Nevertheless, even while controlling for perfect EL, the performance drops sharply on GrailQA++, resulting in an F1 score of 38.6 and 32.5 for RNG-KBQA and ArcaneQA respectively. We attribute this to the skewed distribution of ismorphisms in the original GrailQA dev split, where the simpler isomorphisms (Iso-0,1,2) accounts for 97% of the dataset. RNG-KBQA achieves an F1 score of 86.5 and 30.1 on the simple and complex isomorphisms in GrailQA respectively (see Table <ref type="table" target="#tab_3">3</ref>).</p><p>We also investigate the models' performance on questions with additional functions. These functions are (i) comparatives (ex. greater than, less than), (ii) superlatives (argmax, argmin), (iii) counting or aggregation, and (iv) none (absence of any specific operation). The results in Table <ref type="table" target="#tab_4">4</ref> highlights that ArcaneQA scores higher on superlatives and comparative functions (in terms of F1 score) as opposed to RNG-KBQA for GrailQA++.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ2. Do models exhibit similar performance on different isomorphism types?</head><p>We present a breakdown of the model performance according to the isomorphism type for GrailQA and GrailQA++ in Table <ref type="table" target="#tab_3">3</ref>.</p><p>The enumeration strategy of RNG-KBQA generates candidates corresponding to the first 5 isomorphisms (Iso-0,1,2,3 and 4). Consquently, we   Iso-0 Iso-1 Iso-2 Iso-3 Iso-4 Iso-5 Iso-6 Iso-8 Iso-11 Other 76 14 0.16 0 0 8.5 0 0 0 1.9 19 69 0.120.12 0 10 0 0 0 1.4 50 10 21 2.4 0.79 11 0 0 0.79 4 51 18 4.7 5.6 0.94 11 0.8 0.13 2.1 5.2 14 37 17 4.5 6.7 10 1.8 0 0.89 8.9 6 46 0 1.9 0 42 0.27 0 0 3.3 17 23 9.5 9.5 1.6 19 6.2 0 5. Iso-0 Iso-1 Iso-2 Iso-3 Iso-4 Iso-5 Iso-6 Iso-8 Iso-11 Other 96 3.5 0 0 0 0.31 0 0 0 0 36 64 0 0 0 0 0 0 0 0 42 2.2 50 2.2 3.4 0 0 0 0 0 51 9.8 15 20 0.76 2.9 0 0 0 0.13 31 7.1 26 3 33 0 0 0 0 0 55 44 0 0 0 0.78 0 0 0 0 64 15 17 1.3 0.65 2.3 0 0 0 0 34 15 45 4.6 0 1.5 0 0 0 0 75 7 17 0 0 1 0 0 0 0 79 14 0 7.1 0 0 0 0 0 0  obtain high scores for those specific isomorphisms and low (or zero) EM for the others. This suggests that a ranking-based approach, such as RNG-KBQA, requires prior knowledge of all possible isomorphisms to facilitate meaningful generalization. Nevertheless, RNG-KBQA achieves a comparatively higher F1 score for most isomorphisms in GrailQA++. ArcaneQA, on the other hand, has a higher score on Iso-5, and Iso-6 for GrailQA++.</p><p>We hypothesize that different KBQA models are biased towards generating/retrieving logical forms that conform to specific isomorphisms. To delve deeper, we categorize the models mispredictions into different isomorphism types. We obtain confusion matrices for correct isomorphisms against the predicted isomorphism type for ArcaneQA and RNG-KBQA in Figure <ref type="figure" target="#fig_4">2</ref>.</p><p>We observe that ArcaneQA is biased towards generating logical forms with longer hops (See the column corresponding to Iso-5 and Iso-1 in Figure <ref type="figure" target="#fig_4">2a</ref>) which explains the higher EM of ArcaneQA on GrailQA++ for Iso-5. Furthermore, since RNG-KBQA outputs logical forms corresponding to the first 5 isomorphisms (Iso-0,1,2,3,4), the mispredictions are mostly confined to those specific forms.</p><p>Our experiments demonstrates the complementary strengths of these models such that RNG-KBQA fares better in presence of multiple con-  straints (Iso-3,4) whereas ArcaneQA is better for multiple hops (Iso-5).</p><p>RQ3. What linguistic characteristics of a dataset enable zero-shot generalization?</p><p>We observe from Table <ref type="table" target="#tab_1">2</ref> that the constituent datasets of GrailQA++ exhibit wide variation in performance for both models. While complex isomorphisms usually have lower scores than the simpler ones, there are a few exceptions . For example, on the GraphQ split in Table3, RNG-KBQA has a very high F1 score of 98.9 on Iso-3 as opposed to 69.9 for Iso-0. This motivates us to delve deeper and investigate whether certain dataset characteristics can explain this variation.</p><p>We inspect the following dataset characteristics namely the sentence length (#W), number of common nouns (#N), number of zero-shot items (#Z), readability, grammaticality, complexity, and coherence. The number of common nouns (#N) serves as a proxy for explicitness, i.e how thorough were the annotators in framing the question. The metrics corresponding to readability, complexity, and grammaticality helps to gauge the naturalness of a question, whereas coherence is used to quantify fluency. We describe the use of the metrics in the Appendix. We also outline the mean and standard deviation for each metric in Table <ref type="table" target="#tab_6">6</ref>.</p><p>We perform a multivariate regression analysis over the combined dataset or "All" with F1 score as the dependent variable and the aforementioned linguistic factors and number of zero-shot items as the independent variables to identify which dimensions are statistically significant. We carry out the same analysis for each individual dataset. We present the results in Table <ref type="table" target="#tab_5">5</ref>.</p><p>For the combined dataset, All, we observe that all factors except grammaticality and sentence length, are significant . We also note that complexity, readability, coherence, and the number of zero-shot items are negatively correlated with F1, while the number of common nouns (#N) is positively correlated.</p><p>While, there are fluctuations in trends, we note that for all the datasets, "coherence" is significantly and negatively correlated with performance. This observation aligns with prior findings of <ref type="bibr" target="#b12">Linjordet and Balog (2022)</ref> where the fluency and naturalness of questions degrades KBQA performance. Moreover, the negative correlation with #Z implies that questions with a greater proportion of unseen classes and relations are harder for models to answer. Furthermore, a positive correlation with #N signifies that being more explicit in framing questions is beneficial for model performance. We see similar trends in #N and #Z across most datasets.</p><p>One interesting observation is that our constructed dataset, EAD, is most similar to GraphQuestions both in terms of the EM/F1 scores (Table <ref type="table" target="#tab_1">2</ref>) as well as coefficients of different linguistic dimensions (Table <ref type="table" target="#tab_5">5</ref>). One possible hypothesis is that both these datasets were created in a similar fashion.</p><p>We observe that GrailQA mostly follows a similar trend to All since it accounts for 50% of the entire dataset. However, the readability metric for All is influenced by the pre-existing datasets (CWQ, GraphQ, and WebQSP) which have comparatively higher scores in Table <ref type="table" target="#tab_6">6</ref>. All in all, we note that KBQA systems struggle with fluent and natural questions (high coherence and readability scores).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We propose a new dataset, GrailQA++, to benchmark the zero-shot generalization capabilities of KBQA models on complex questions. We characterize the question complexity by introducing the concept of graph isomorphisms that characterizes both the dimensions of hops and constraints. Our experiments reveal poor generalization performance of SOTA KBQA models on our proposed dataset even when gold entities are provided during inference. Our analysis also reveals complementary strengths of different KBQA models on different types of isomorphisms and how isomorphisms can be used to categorize and group model mispredictions. We also carry out extensive analysis on our proposed dataset to identify linguistic factors, that are correlated with non-generalizability such as high coherence and readability. Our research sheds light on how to design harder benchmarks to evaluate zero-shot generalization of KBQA models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Limitations</head><p>Since the major contribution of our work is the creation of a new dataset to test the zero-shot generalization capabilities of KBQA, the limitations of the work could be stated with regards to the dataset creation. An important thing to note is that unlike other benchmarks, our proposed GrailQA++ is designed solely for the purpose of evaluation. The dataset was created keeping in mind that the only training data one has access to is the train split of GrailQA. This is because the test splits in preexisting datasets share a huge overlap with their corresponding train splits. Consequently KBQA models trained on any additional dataset might violate the zero-shot criteria.</p><p>Additionally, we decouple the act of entity linking from question answering and thus the performance of models stated in this work are expected to be higher than using models that do not have access to gold entities. Furthermore, to aid machine understanding we avoid paraphrasing and try to construct natural language queries with explicit mention of classes and relations of interest. We intend to address these limitations in the future, but for the time being we wanted to control for complexity using isomorphisms which motivated the following design choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Ethics Statement</head><p>The task of KBQA involves querying a knowledge graph to return an answer. Like most NLP research, the work leverages the prowess of large pre-trained language models like BERT, and T-5 and thus the harms associated with these models are to be noted during deployment. Furthemore, since KBQA involves interaction with an user to answer queries, these models should undergo rigorous model-testing before deployment.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Schematic diagram that outlines the GrailQA++ dataset creation. The dataset comprises of question and corresponding logical forms, from two different sources. The former are instances which are hand-annotated by domain experts, and the latter are instances obtained from pre-existing datasets (WebQSP, CWQ, and GraphQ) which also operate over the same Freebase KB. (more details in Section 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Is o -0 Is o -1 Is o -2 Is o -3 Is o -4 Is o -5 Is o -6 Is o -8 Is o -1 1 O t h e r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>RNG-KBQA on GrailQA++</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Confusion matrices for gold Isomorphisms vs predicted Isomorphisms on the GrailQA++ dataset for ArcaneQA (top) and RNG-KBQA (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>EM and F1 scores for RNG-KBQA and the Ar-caneQA model on the GrailQA and GrailQA++ datasets (with gold entities). EAD stands for the Expert Annotated Dataset that we had created.</figDesc><table><row><cell></cell><cell cols="4">RNG-KBQA ArcaneQA</cell></row><row><cell>Dataset</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell></row><row><cell cols="5">GrailQA (dev) 83.5 86.0 77.9 81.7</cell></row><row><cell>GrailQA++</cell><cell cols="4">28.5 38.6 18.6 32.5</cell></row><row><cell>-EAD</cell><cell cols="4">56.1 70.2 31.5 49.9</cell></row><row><cell>-GraphQ</cell><cell cols="4">53.2 61.7 30.2 44.8</cell></row><row><cell>-WebQSP</cell><cell cols="4">19.9 25.9 17.6 28.7</cell></row><row><cell>-CWQ</cell><cell cols="4">12.6 23.0 10.2 23.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>EM / F1 scores for RNG-KBQA (RNG) and ArcaneQA (Arc), across the different Isomorphisms (Iso) in GrailQA (zero-shot subset) and GrailQA++. EAD stands for the expert annotated dataset that was created.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">RNG-KBQA</cell><cell></cell><cell></cell><cell cols="2">ArcaneQA</cell></row><row><cell>Dataset</cell><cell>None</cell><cell>Count</cell><cell cols="2">Comparative Superlative</cell><cell>None</cell><cell>Count</cell><cell>Comparative Superlative</cell></row><row><cell cols="3">GrailQA (Dev) 90.1/ 90.9 91.1/ 95.3</cell><cell>38.6/ 73.8</cell><cell>0.0/ 7.8</cell><cell cols="2">80.2/ 83.2 68.1/ 71.7</cell><cell>41.2/ 65.5</cell><cell>72.1/ 76.5</cell></row><row><cell>GrailQA++</cell><cell cols="2">29.9/ 40.9 84.3/ 84.3</cell><cell>0.0/ 1.9</cell><cell>0.0/ 6.6</cell><cell cols="2">20.0/ 34.7 20.6/ 21.6</cell><cell>0.0/ 15.5</cell><cell>8.7/ 15.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note><p>EM/ F1 scores for RNG-KBQA and the ArcaneQA model on the GrailQA and GrailQA++ datasets with different functional forms. None means no special function was present.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Coefficients of the different dimensions on the F1 score obtained through linear regression and their corresponding p-values. A positive coefficient indicates a positive correlation and vice versa. *, **, *** indicate that the coefficient is statistically significant with a p-value ≤ 0.05, 0.01, and 0.001 respectively.</figDesc><table><row><cell>Dimension</cell><cell>GrailQA</cell><cell>EAD</cell><cell>GraphQ</cell><cell></cell><cell cols="2">WebQSP</cell><cell>CWQ</cell><cell>All</cell></row><row><cell>Complexity Score</cell><cell cols="2">-0.282  *  *  *  +0.001</cell><cell>+0.00</cell><cell></cell><cell>+0.00</cell><cell></cell><cell>-0.124</cell><cell>-0.093  *</cell></row><row><cell>Grammaticality</cell><cell>+0.013</cell><cell>+0.011</cell><cell>-0.063</cell><cell></cell><cell cols="2">+0.037</cell><cell>+0.027</cell><cell>-0.023</cell></row><row><cell>Readability</cell><cell>+0.000</cell><cell>+0.001</cell><cell>-0.001</cell><cell></cell><cell cols="2">-0.001</cell><cell>-0.001</cell><cell>-0.002  *  *  *</cell></row><row><cell>Coherence</cell><cell cols="6">-0.069  *  *  *  -0.075  *  *  *  -0.085  *  *  *  -0.031  *  *</cell><cell>-0.024  *  *  *  -0.068  *  *  *</cell></row><row><cell cols="3">Sentence Length (#W) +0.010  *  *  *  -0.006</cell><cell cols="2">-0.015  *</cell><cell cols="2">+0.028  *</cell><cell>+0.006</cell><cell>+0.0021</cell></row><row><cell cols="3">Common Nouns (#N) +0.037  *  *  *  +0.000</cell><cell>+0.031</cell><cell></cell><cell cols="2">-0.022</cell><cell>+0.027  *  *  *  +0.026  *  *  *</cell></row><row><cell>Zero-shot Items (#Z)</cell><cell cols="2">-0.065  *  *  *  +0.011</cell><cell>-0.005</cell><cell></cell><cell cols="3">-0.114  *  *  *  -0.100  *  *  *  -0.035  *  *  *</cell></row><row><cell>Dimension</cell><cell cols="2">GrailQA(Dev)</cell><cell>EAD</cell><cell></cell><cell>GraphQ</cell><cell cols="2">WebQSP</cell><cell>CWQ</cell></row><row><cell cols="2">Complexity Score</cell><cell>0.0 (0.1)</cell><cell>0.2 (0.4)</cell><cell cols="2">0.0 (0.0)</cell><cell cols="2">0.0 (0.0)</cell><cell>0.0 (0.1)</cell></row><row><cell>Grammaticality</cell><cell></cell><cell>0.7 (0.5)</cell><cell>0.6 (0.5)</cell><cell cols="2">0.8 (0.4)</cell><cell cols="2">0.7 (0.4)</cell><cell>0.8 (0.4)</cell></row><row><cell>Readability</cell><cell></cell><cell>60.5 (26.9)</cell><cell cols="5">58.4 (24.5) 71.8 (25.7) 77.0 (25.3) 69.9 (22.1)</cell></row><row><cell>Coherence</cell><cell></cell><cell>-9.8 (1.2)</cell><cell>-9.7 (1.2)</cell><cell cols="2">-9.4 (1.2)</cell><cell cols="2">-9.9 (1.3)</cell><cell>-9.3 (1.2)</cell></row><row><cell cols="2">Sentence Length (#W)</cell><cell>12.6 (3.7)</cell><cell>17.3 (5.2)</cell><cell cols="2">11.1 (3.0)</cell><cell cols="2">6.7 (1.6)</cell><cell>14.4 (3.3)</cell></row><row><cell cols="2">Common Nouns (#N)</cell><cell>4.7 (1.8)</cell><cell>6.6 (2.4)</cell><cell cols="2">3.4 (1.3)</cell><cell cols="2">2.2 (1.0)</cell><cell>5.3 (1.6)</cell></row><row><cell cols="2">Zero-shot items (#Z)</cell><cell>2.1 (0.9)</cell><cell>2.6 (1.3)</cell><cell cols="2">2.4 (1.2)</cell><cell cols="2">1.6 (0.7)</cell><cell>2.1 (0.9)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>We present the mean (std) on different linguistic dimensions on the zero-shot split of GrailQA development set (Dev), and GrailQA++.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Distribution of different isomorphisms across the training and test splits for KBQA datasets. We include only instances in the test split that conform with the zero-shot criteria of GrailQA.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://dki-lab.github.io/GrailQA/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Our dataset is available here at https://github.com/ sopankhosla/GrailQA-PlusPlus.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Example screenshots provided in Appendix C.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://pypi.org/project/ py-readability-metrics/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Isomorphism Distribution</head><p>We show the distribution of different isomorphisms in the training and test data of the different datasets in Table <ref type="table">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Analyzing Linguistic Variation</head><p>We adopt the following dimensions of <ref type="bibr" target="#b9">Khosla et al. (2023)</ref> on our proposed dataset.</p><p>Sentence Length (#W): We simply count the number of words for each natural language questions across all datasets.</p><p>Common Nouns (#N): We use NLTK's POStagger to identify common nouns that corresponding to "NN" and "NNS" tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grammaticality &amp; Complexity:</head><p>We use the BLIMP <ref type="bibr" target="#b20">(Warstadt et al., 2020)</ref> and COLA corpora <ref type="bibr" target="#b21">(Warstadt et al., 2019)</ref> to fine-tune BERT-based text classification model to detect whether a given question is grammatical or not. We follow the same to determine whether a given question is complex or not, i.e. has several clauses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Readability:</head><p>We use the Flesch-reading score to characterize the readability of each question in the dataset, using the readability in python. 4   Coherency: We quantify fluency or naturalness of a question using coherency. We measure coherency using a reference free metric called CTRL-Eval <ref type="bibr" target="#b8">(Ke et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Annotation Screenshots</head><p>We present examples of annotation screenshots for different Isomorphisms that we considered for curating the expert-annotations from the GrailQA++ dataset. Each screenshot includes a pictoral representation of a query graph along with additional information, namely the S-expression or logical form corresponding to the query graph, the constraints in the form of entities and literals, and the answers. Each annotator was presented with each of these information before constructing the natural language query for it. For each instance, we provide the S-expression, the friendly name for each entity and relation, as well as the answers.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Knowledge base question answering by case-based reasoning over subgraphs</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameya</forename><surname>Godbole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankita</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliot</forename><surname>Tower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="4777" to="4793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Lazaros Polymenakos, and Andrew McCallum. 2021. Case-based reasoning for natural language queries over knowledge bases</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dung</forename><surname>Thai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameya</forename><surname>Godbole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay-Yoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Tan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Perkgqa: Question answering over personalized knowledge graphs</title>
		<author>
			<persName><forename type="first">Ritam</forename><surname>Dutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kasturi</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashmi</forename><surname>Gangadharaiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolyn</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: NAACL 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="253" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beyond iid: three levels of generalization for question answering on knowledge bases</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sue</forename><surname>Kase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Vanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3477" to="3488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Knowledge base question answering: A semantic parsing perspective</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vardaan</forename><surname>Pahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.04994</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Arcaneqa: Dynamic program induction and contextualized encoding for knowledge base question answering</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.08109</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A knowledge graph based question answering method for medical domain</title>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zisang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ Computer Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">667</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Knowledge graph question answering datasets and their generalizability: Are they enough for future research</title>
		<author>
			<persName><forename type="first">Longquan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Usbeck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.06573</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ctrleval: An unsupervised reference-free metric for evaluating controlled text generation</title>
		<author>
			<persName><forename type="first">Pei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2306" to="2319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploring the reasons for non-generalizability of kbqa systems</title>
		<author>
			<persName><forename type="first">Sopan</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ritam</forename><surname>Dutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinayshekhar</forename><surname>Bannihatti Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashmi</forename><surname>Gangadharaiah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Fourth Workshop on Insights from Negative Results in NLP</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="88" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Query graph generation for answering multi-hop complex questions from knowledge bases</title>
		<author>
			<persName><forename type="first">Yunshi</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.91</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="969" to="974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semantic structure based query graph prediction for question answering over knowledge graph</title>
		<author>
			<persName><forename type="first">Mingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Shihao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2204.10194</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Would you ask it that way? measuring and improving question naturalness for knowledge graph question answering</title>
		<author>
			<persName><forename type="first">Trond</forename><surname>Linjordet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.12768</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.05165</idno>
		<title level="m">Uniparser: Unified semantic parser for question answering on knowledge base and database</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A knowledge graph for assessing agressive tax planning strategies</title>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Lüdemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ageda</forename><surname>Shiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Thymianis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Paulheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="395" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Neelam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hima</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shajith</forename><surname>Ikbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavan</forename><surname>Kapanipathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandana</forename><surname>Mihindukulasooriya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young-Suk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Srivastava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.05793</idno>
		<title level="m">Cezar Pendus, et al. 2022. A benchmark for generalizable and interpretable temporal question answering over knowledge bases</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Knowledge graph-based question answering with electronic health records</title>
		<author>
			<persName><forename type="first">Junwoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngwoo</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haneol</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09394</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Yiheng</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingting</forename><surname>Börje F Karlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.12925</idno>
		<title level="m">Tiara: Multi-grained retrieval for robust question answering over large knowledge bases</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On generating characteristic-rich question sets for qa evaluation</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mudhakar</forename><surname>Srivatsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izzeddin</forename><surname>Gür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zenghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="562" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The web as a knowledge-base for answering complex questions</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1059</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>New Orleans</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="641" to="651" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Blimp: The benchmark of linguistic minimal pairs for english</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Parrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anhad</forename><surname>Mohananey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-Fu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="377" to="392" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural network acceptability judgments</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00290</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="625" to="641" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Xi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.08678</idno>
		<title level="m">Rng-kbqa: Generation augmented iterative ranking for knowledge base question answering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The value of semantic parse labeling for knowledge base question answering</title>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jina</forename><surname>Suh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-2033</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="201" to="206" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Decaf: Joint decoding of answers and logical forms for question answering over knowledge bases</title>
		<author>
			<persName><forename type="first">Donghan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henghui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">Hanbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.00063</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
