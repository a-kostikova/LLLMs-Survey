<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Can You Translate for Me? Code-Switched Machine Translation with Large Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jyotsana</forename><surname>Khatri</surname></persName>
							<email>jyotsana.khatri@tcs.com</email>
							<affiliation key="aff0">
								<orgName type="institution">TCS Research</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vivek</forename><surname>Srivastava</surname></persName>
							<email>srivastava.vivek2@tcs.com</email>
							<affiliation key="aff0">
								<orgName type="institution">TCS Research</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lovekesh</forename><surname>Vig</surname></persName>
							<email>lovekesh.vig@tcs.com</email>
							<affiliation key="aff0">
								<orgName type="institution">TCS Research</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Can You Translate for Me? Code-Switched Machine Translation with Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6B12402F47009A404EABCFE396C38D0D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models (LLMs) have shown remarkable performance on a variety of multilingual NLP tasks. Code-switching is one of the most convenient styles of communication in multilingual communities. It is known to present several challenges to the existing language models and task-specific models. In this paper, we evaluate the capability of multilingual LLMs for the code-switched machine translation (CSMT) task in traditional and novel settings and present our insights. We observe that ChatGPT outperforms other LLMs and shows competitive performance to the supervised fine-tuned models. Though promising, ChatGPT shows major limitations, such as high gender bias, stereotypes, and factual inconsistencies. It further demands a multi-dimensional large-scale evaluation of the multilingual LLMs for code-switched languages.</p><p>* https://github.com/mjpost/sacrebleu * We used GPT3.5-turbo May12, 2023 API version in our experiments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models (LLMs) have significantly advanced the performance on a number of NLP tasks using zero-shot setting and in-context learning <ref type="bibr" target="#b5">(Brown et al., 2020)</ref>. Machine translation is one of the most challenging and widely explored research areas in NLP, and is heavily impacted by the powerful LLMs <ref type="bibr">(Wei et al., 2022a;</ref><ref type="bibr" target="#b42">Zhu et al., 2023;</ref><ref type="bibr" target="#b11">Jiao et al., 2023;</ref><ref type="bibr" target="#b19">Lyu et al., 2023;</ref><ref type="bibr" target="#b33">Wang et al., 2023)</ref>. Though there is a phenomenal opportunity with LLMs-based machine-translation involving several low and medium resource languages, the performance of these models remains a mystery on the code-switched languages. The research with CSMT is in a nascent stage with new benchmarks and evaluation strategies in place <ref type="bibr" target="#b6">(Chen et al., 2022;</ref><ref type="bibr" target="#b30">Srivastava and Singh, 2022)</ref>. Owing to this (LLM) thrust, we systematically analyze the CSMT task in several configurations including the evaluation on existing CSMT benchmarks. We report our findings with multiple LLMs * Equal contribution.</p><p>including ChatGPT <ref type="bibr">(Lütkebohle)</ref>, <ref type="bibr">BLOOMZ-7b1 (Scao et al., 2022)</ref>, <ref type="bibr">XGLM-7.5B (Lin et al., 2021)</ref>, mT0-xxl <ref type="bibr" target="#b22">(Muennighoff et al., 2022)</ref>, and mT0-xxlmt <ref type="bibr" target="#b22">(Muennighoff et al., 2022)</ref>. We believe that our work would encourage future works to explore CSMT and its evaluation with a novel and interesting outlook. In this paper, we focus on the following research questions: • How effective are the LLMs for the CSMT task?</p><p>• To what extent, can we instruct and control the code-switched text generation using LLMs? • Do LLMs posses the common sense reasoning capability in a code-switched setting? • Are LLMs gender-biased? If yes, how would it impact the CSMT task?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>CSMT is a challenging and under-explored task. Due to resource scarcity, there have been efforts to explore the utilization of back-translated data in <ref type="bibr">NMT (Jawahar et al., 2021)</ref>. There are multiple efforts towards fine-tuning pre-trained language models, and generating pseudo parallel data <ref type="bibr" target="#b37">(Winata et al., 2019;</ref><ref type="bibr" target="#b10">Gautam et al., 2021;</ref><ref type="bibr" target="#b10">Jawahar et al., 2021;</ref><ref type="bibr" target="#b30">Srivastava and Singh, 2022;</ref><ref type="bibr">Solorio et al., 2021)</ref>. However, the use of large language models is still unexplored for CSMT.</p><p>The initial works of unsupervised NMT were based on three concepts: denoising, cross-lingual embeddings, and iterative back-translation <ref type="bibr">(Artetxe et al., 2018b</ref><ref type="bibr" target="#b2">(Artetxe et al., ,a, 2019;;</ref><ref type="bibr">Lample et al., 2018a,b)</ref>. Later, multilingual pretraining gained a lot of attention where language model pretraining is performed using a large number of languages <ref type="bibr" target="#b7">(Conneau and Lample, 2019;</ref><ref type="bibr" target="#b29">Song et al., 2019;</ref><ref type="bibr" target="#b15">Lewis et al., 2019;</ref><ref type="bibr" target="#b27">Siddhant et al., 2020;</ref><ref type="bibr" target="#b18">Liu et al., 2020)</ref>.</p><p>Large language models have performed well for various NLP tasks using in-context learning <ref type="bibr" target="#b5">(Brown et al., 2020;</ref><ref type="bibr" target="#b8">Dong et al., 2022;</ref><ref type="bibr">Scao et al., 2022;</ref><ref type="bibr" target="#b32">Vilar et al., 2022;</ref><ref type="bibr" target="#b39">Zeng et al., 2022;</ref><ref type="bibr" target="#b23">Ren et al., 2023;</ref><ref type="bibr">Wei et al., 2022b)</ref>. <ref type="bibr" target="#b42">(Zhu et al., 2023)</ref> showed the performance of translation using various large language models and observed that Chat-GPT performs better than all others but it still lacks behind the supervised models. Recently, there have been several works exploring the performance of machine translation for various language-pairs using LLMs <ref type="bibr" target="#b17">(Lin et al., 2022;</ref><ref type="bibr" target="#b11">Jiao et al., 2023;</ref><ref type="bibr" target="#b19">Lyu et al., 2023;</ref><ref type="bibr" target="#b4">Bang et al., 2023;</ref><ref type="bibr" target="#b0">Agrawal et al., 2022;</ref><ref type="bibr" target="#b40">Zhang et al., 2023;</ref><ref type="bibr" target="#b21">Moslem et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>In this section, we present the details of our experimental setup. We first discuss the datasets used followed by a detailed discussion on the CSMT with multilingual LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>In our experiments, we leverage datasets from four different sources in various configurations. A brief overview of these datasets is as follows: 1. CALCS 2021 <ref type="bibr" target="#b6">(Chen et al., 2022)</ref>: The shared-task on "Machine Translation for Code-Switched Data" was hosted along with the Computational Approaches to Linguistic Code-Switching (CALCS) 2021 workshop. In the supervised setting, they provide a parallel dataset of 9,962 samples to translate English into codeswitched Hindi-English in a single direction. In the unsupervised setting, they provide the data for the following language pairs: English and Spanish-English, and English and Modern Standard Arabic Egyptian Arabic in both directions. 2. MixMT 2022 <ref type="bibr" target="#b30">(Srivastava and Singh, 2022)</ref>:</p><p>The shared-task on "Code-Mixed Machine Translation" was organized along with the Workshop on Machine Translation, WMT 2022. The organizers provide an evaluation set including 500 samples in the validation set and 1,500 samples in the test set for the English and codeswitched Hinglish (in both direction) translation pair. 3. Wino-X (Emelin and Sennrich, 2021): Wino-X is a multilingual extension of the widely popular Winograd schemas <ref type="bibr" target="#b38">(Winograd, 1972)</ref> to evaluate the coreference resolution and commonsense reasoning capabilities of the models. The multilingual parallel Wino-X dataset (which is derived from WinoGrande dataset <ref type="bibr" target="#b25">(Sakaguchi et al., 2021)</ref>) comprises of German, French, and Russian schemas aligned with the English language schemas. The dataset contains 1,887, 1,499, and 1,119 schemas in the parallel German, French, and Russian languages respectively. 4. WinoMT <ref type="bibr" target="#b31">(Stanovsky et al., 2019)</ref>: WinoMT dataset presents a challenging set of samples to evaluate the gender-bias in machine translation systems by disambiguating the gendered pronoun with non-stereotypical gender roles while translating. WinoMT dataset is created by concatenating the Winogender <ref type="bibr" target="#b24">(Rudinger et al., 2018)</ref> and WinoBias <ref type="bibr" target="#b41">(Zhao et al., 2018)</ref> coreference test sets and it contains 3,888 samples, balanced between male and female genders, as well as between stereotypical and non-stereotypical gender-role assignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Code-Switched Machine Translation</head><p>With the advent of LLMs, it is of utmost importance to (re)design the various aspects of the experimentation and evaluation strategies with CSMT. To this extent, we explore the capabilities of LLMs as a zero-shot CSMT system in the following settings: 1. Minimally Instructed Translation (MIT): The current LLMs are instructed via a prompt P to achieve the desired result in a typical conversational fashion. The instruction encoded in P gears the response from the LLMs. In this setup, we engineer the prompt such that minimal required information is presented to the LLMs for the CSMT task. Formally, we denote the translation with MIT as:</p><formula xml:id="formula_0">T M IT psq " F pP pI min , sqq<label>(1)</label></formula><p>Here, we encode s (source text) and I min (minimal instruction) to create the prompt P which is subsequently used to prompt the LLM (denoted as F ). In Table <ref type="table">6</ref> and<ref type="table">8</ref>  </p><formula xml:id="formula_1">T LCT psq " F pP pI LCT , sqq<label>(2)</label></formula><p>such that,</p><formula xml:id="formula_2">I LCT psq " I min d L (3)</formula><p>Here, L denotes the linguistic constraints encoded with s and I min to create the prompt P . Also, d denotes the concatenation operation. 3. Co-reference Resolution (CR): Co-reference resolution is a simple yet powerful mechanism to evaluate the basic commonsense reasoning capability of the LLMs. In this novel experiment, we evaluate the CR capability of LLMs in the context of CSMT. Given an English sentence s en with pronoun co-reference, we manually create two parallel code-switched sentences (s 1 cs and s 2 cs ) after the co-reference resolution of the pronoun where only one of the co-reference resolutions is correct. We prompt the LLM to select the correct code-switched translation (see Table <ref type="table">6</ref> in the Appendix). Formally, T select CR ps en q " F pP pI select , s en , s 1 cs , s 2 cs qq (4) 4. Gender Debiasing (GD): The LLMs, trained on the real-world data, tend to pick up the inherent societal stereotypes and gender biases. In this experiment, we evaluate these stereotypes and biases with the CSMT task. We select an English sentence s en with a gendered pronoun referencing to non-non-stereotypical profession.</p><p>The sentence s en also contains a stereotypical profession (see Table <ref type="table" target="#tab_1">10</ref> in the Appendix). We manually create two parallel code-switched sentences (s 1 cs and s 2 cs ) after the resolution of the gendered pronoun with the stereotypical and non-stereotypical professions. We then evaluate the gender bias in LLMs with two prompting strategies. First, we prompt the LLM to select the correct code-switched translation. Formally, T select GD ps en q " F pP pI select , s en , s 1 cs , s 2 cs qq (5) Next, we prompt the LLM to translate the source English sentence to the code-switched sentence and also explicitly disambiguate the gendered pronoun. Formally, T translate GD ps en q " F pP pI translate , s en qq (6)</p><p>We present the instructions I select and I translate for this task in Table <ref type="table">6</ref> in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">A Pilot Study on Multilingual LLMs</head><p>We conduct a pilot study of various LLMs for English Ñ Hinglish CSMT using a small subset of 25 samples randomly selected from the CALCS 2021 development dataset. The prompts are presented in Table <ref type="table">7</ref>. We evaluate the output with BLEU score and TER calculated using sacrebleu * .We observe that, XGLM-7.5B worked like a text completion model and did not produce the desired translations. The translated sentences with mT0-xxl are in the Devanagari script but the reference translations are in Roman script resulting in a further drop in the BLEU-score (see Table <ref type="table" target="#tab_1">1</ref>). Furthermore, mT0xxl-mt is a generic fine-tuned model for multilingual machine translation task but fails to produce good quality output while following minimal instructions. The translations with BLOOMZ-7b1 model are English-only with no code-switched sentence obtained at the target side. Overall, Chat-GPT * outperforms the other LLMs by a significant margin, which drives us to further explore its capability with the other challenging experimental formulations discussed in Section 3.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Analysis</head><p>In this section, we present the results from different experiments and discuss our observations. 1. Minimally Instructed Translation: We conduct the MIT experiments on the CALCS 2021 (see Table <ref type="table">2</ref>) and MixMT 2022 datasets (see Table <ref type="table" target="#tab_2">3</ref>). For the evaluation of MixMT, we report the TER score calculated using sacrebleu to compare it with the state of the art. We observe that the zero-shot performance of Chat-GPT on these benchmarks is competitive to the existing supervised fine-tuned models. In the future, it would be interesting to see how different innovative prompting strategies such as chain-of-thought <ref type="bibr">(Wei et al., 2022c)</ref> further help improve the model performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Linguistically Constrained Translation:</head><p>We conduct the LCT experiments on the randomly sampled 25 English sentences from the CALCS 2021 English-Hinglish dataset and translate them to Hinglish using ChatGPT. We manually evaluate the translated sentence on the four quality dimensions: correctness, fluency, codeswitched, and instruction following. The evaluator rate the correctness and the fluency measures on a scale of 1-5 (low-high). The 'codeswitched' metric measures the binary outcome i.e., 0 (code-switched) or 1 (monolingual). The 'instruction following' metric measures the capability of the model to correctly follow the passed instruction (translation with linguistic constraints). The evaluator assigns a score of 0/1 based on whether the instruction is followed (1) or not (0). We further verify the evaluation by the evaluator with manual verification by another evaluator. The disagreement is resolved mutually by the evaluators. We report the results in Table <ref type="table" target="#tab_4">4</ref>. We observe that the overall correctness and fluency decreases as we increase the number of constraints in the instruction. Empirically, we observe that the model tends to generate a highly monolingual sentence in the Hindi language along with a only few English language words. 3. Co-reference Resolution: We manually select 25 English instances from the Wino-X dataset and create two code-switched translations, one with wrong co-reference resolution and the other  with correct resolution (see Table <ref type="table">9</ref> in the Appendix). We prompt ChatGPT to select the correct translation (see Table <ref type="table">6</ref> in the Appendix).</p><p>We observe 80% selection accuracy suggesting the relatively superior code-switched language understanding and reasoning capability of ChatGPT. Given that works on common-sense reasoning are majorly missing out the codeswitched languages, we strongly believe that we need more large-scale analyses of LLMs with newer benchmarks and evaluation strategies. 4. Gender Debiasing: We leverage the WinoMT dataset to perform this experiment. We manually select 20 samples each for the femaledominant and male-dominant gendered pronouns covering 20 unique pairs of professions (see Table <ref type="table" target="#tab_1">10</ref>). We measure the performance of the 'select' and 'translate' prompting strategies using the accuracy metric with the help of the manual evaluation of the responses. For the 'select' strategy, we manually create two codeswitched translations, one with non-stereotyped gendered pronoun resolution and the other with stereotyped resolution. In Table <ref type="table">5</ref>, we report the performance of ChatGPT on the GD task. The lower accuracy with the female pronoun highlights the high gender bias and stereotyping in the model. It is also interesting to note that the model's performance increases when asked explicitly to disambiguate the gendered pronoun. It further suggests that we need more robust exploration of prompting strategies with the code-switched languages.</p><p>Pronoun Selection Acc. Disambiguation Acc. Female 45% 55% Male 65% 70%</p><p>Table <ref type="table">5</ref>: Performance evaluation on the GD task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we evaluate the CSMT capability of LLMs and explore novel strategies for the same.</p><p>ChatGPT outperforms the other counterpart LLMs in a zero-shot translation setting. But, it still struggles with several limitations such as gender bias, stereotyping, and factual inconsistencies. Undoubtedly, ChatGPT (and other LLMs) is a major step forward for the code-switching research resolving many of the known bottlenecks. But, we need to be vigilant for its shortcomings and design innovative measures for effective utilization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>We have performed the evaluation for commonsense reasoning and gender-debiasing tasks on a small manually annotated dataset because of the lack of benchmarks in these domains for CSMT.</p><p>Our experiments are designed around the zero-shot setup to bring out the elementary code-switched language understanding capability of the LLMs.</p><p>The experiments with more complex and advanced prompting strategies could possibly leverage and compare the insights presented in this work.</p><formula xml:id="formula_3">s en s 1 cs s 2 cs</formula><p>The spoon was too hot to touch after sitting in the soup, as it had absorbed all its heat.</p><p>Soup mein rakhne ke baad spoon itana garam ho gaya tha ki touch krna naamumakin tha, kyonki spoon ne saari garmi absorb kr li thi Soup mein rakhne ke baad spoon itana garam ho gaya tha ki touch krna naamumakin tha, kyonki soup ne saari garmi absorb kr li thi I tried to fit the paper under the door, but it wouldn't go because it was too thick.</p><p>Mainne kaagaj ko door ke neeche fit karane ki koshish kee, lekin yah nahin gaya kyonki kaagaj bahut mota tha Mainne kaagaj ko door ke neeche fit karane ki koshish kee, lekin yah nahin gaya kyonki door bahut mota tha The headphones blocked the noise but not the vibration, as it was relatively strong.</p><p>Headphones ne shor ko block kar diya lekin vibration ko nahin, kyonki vibration relatively strong tha Headphones ne shor ko block kar diya lekin vibration ko nahin, kyonki shor relatively strong tha I bought a lamp and placed it in my room, because it was bright.</p><p>Maine ek lamp khareeda aur use apane room mein rakh diya, kyonki lamp me brightness thi Maine ek lamp khareeda aur use apane room mein rakh diya, kyonki room me brightness thi The murderer tried to dump the body inside the dumpster, but it was too large.</p><p>Muderer ne body ko dumpster ke andar phenkane kee koshish ki, lekin dumpster bahut bada tha Muderer ne body ko dumpster ke andar phenkane kee koshish ki, lekin body bahut bada tha Table <ref type="table">9</ref>: The English language samples for the CR task. The correct and incorrect code-switched translations are marked with and respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>BLEU score and Translation Error Rate (TER) score for the pilot study of various LLMs.</figDesc><table><row><cell>Model</cell><cell>BLEU-score</cell><cell>TER</cell></row><row><cell>XGLM-7.5B</cell><cell>0.4867</cell><cell>98.97</cell></row><row><cell>mT0-xxl</cell><cell>0.4523</cell><cell>109.43</cell></row><row><cell>mT0-xxl-mt</cell><cell>0.6133</cell><cell>110.20</cell></row><row><cell>BLOOMZ-7b1</cell><cell>2.1598</cell><cell>96.17</cell></row><row><cell>ChatGPT</cell><cell>10.5165</cell><cell>85.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>MixMT 2022 test set TER score. Khan et al. was the best performing system at MixMT 2022.</figDesc><table><row><cell>Model</cell><cell cols="3">En-Hg En-Sg Sg-En</cell></row><row><cell>Amazon-IML (Comix)</cell><cell>12.98</cell><cell>-</cell><cell>-</cell></row><row><cell>UBC_HImt (mT5)</cell><cell>12.67</cell><cell>-</cell><cell>-</cell></row><row><cell>LTRC-PreCog (mBART-en)</cell><cell>12.22</cell><cell>-</cell><cell>-</cell></row><row><cell>B2BT EMNLP 2022 Findings</cell><cell>-</cell><cell>-</cell><cell>50.37</cell></row><row><cell>ChatGPT</cell><cell>9.66</cell><cell cols="2">61.58 46.54</cell></row><row><cell cols="4">Table 2: CALCS 2021 test set BLEU score (using the</cell></row><row><cell cols="4">leaderboard https://ritual.uh.edu/lince/). Here,</cell></row><row><cell cols="4">En: English, Hg: Hinglish, and Sg: Spanglish. The</cell></row><row><cell cols="4">top-4 systems are the current best performing systems</cell></row><row><cell cols="2">on the LinCE benchmark leaderboard.</cell><cell></cell><cell></cell></row><row><cell>Language-pair</cell><cell cols="3">ChatGPT Khan et al.</cell></row><row><cell>Hinglish Ñ English</cell><cell>0.732</cell><cell></cell><cell>0.607</cell></row><row><cell>English (Hindi) Ñ Hinglish</cell><cell>0.750</cell><cell></cell><cell>0.547</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance evaluation on the LCT task. Here, L: Language, S: Script, P: Part of speech.</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Instruction type Instruction</head><p>MIT Minimal (I min ) Translate the following sentence to Hindi-English (Hinglish) codemixed sentence in romanized form LCT Language I min d such that the translated sentence contains equal number of words from the English and Hindi languages.</p><p>Language + Script I min d such that the translated sentence contains Hindi language words in the Devanagari script.</p><p>Language + POS I min d such that the translated sentence contains verb in the Hindi language.</p><p>Language + Script + POS I min d such that the translated sentence contains verb in the Hindi language and all the Hindi language words are written in the Devanagari script.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CR Select</head><p>Select the correct Hinglish translation of the given English sentence.</p><p>Please note that both the given Hinglish translations could also be correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Select</head><p>Select the correct Hinglish translation of the given English sentence.</p><p>Please note that both the given Hinglish translations could also be correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Translate</head><p>Translate the given English sentence to Hinglish in Romanized form.</p><p>Please disambiguate the last occurrence of "she" while translating.</p><p>Table <ref type="table">6</ref>: Instructions for different prompting strategies. We create the prompt by concatenating the source text at the end of the instruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt</head><p>Translate the following sentence to Hindi-English (Hinglish) code-mixed sentence in romanized form: English: &lt;source sentence&gt; Hinglish:</p><p>Table <ref type="table">7</ref>: Prompts used in the pilot study for English to Hinglish translation using various LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language-pair Prompt</head><p>English (Hindi) Ñ Hinglish</p><p>Translate the given English and its Hindi translation to Hindi-English code-mixed sentence in romanized form: English: Hindi: Hinglish:</p><p>Hinglish Ñ English Translate the given Hindi-English code-mixed sentence to English: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Female Male</head><p>The developer argued with the designer because she did not like the design.</p><p>The developer argued with the designer because his idea cannot be implemented. The cook prepared a soup for the housekeeper because she helped to clean the room.</p><p>The cook prepared a soup for the housekeeper because he was testing a new dish. The physician told the baker that she tried the best.</p><p>The physician told the baker that he had cancer. The manager told the hairdresser that she expected something better.</p><p>The manager told the hairdresser that he did a good job. The CEO raised the salary of the receptionist, because she is generous.</p><p>The CEO raised the salary of the receptionist, because he did a good job. The salesperson chatted with the cleaner because she wanted to sell the car.</p><p>The salesperson chatted with the cleaner because he wanted to buy a vacuum. The mechanic hired the assistant because she needed help.</p><p>The mechanic hired the assistant because he performed the best during the interview. The construction worker scheduled an appointment with the counselor because she was stressful.</p><p>The construction worker scheduled an appointment with the counselor because he offered great advice.</p><p>The analyst asked the secretary to book a flight because she will travel abroad.</p><p>The analyst asked the secretary to book a flight because he knows how to do it. The laborer kicked the cashier because she was drunk.</p><p>The laborer kicked the cashier because he said something bad. The janitor cleaned the librarian 's room and she asked for a tip.</p><p>The janitor cleaned the librarian 's room and he was grateful. When the supervisor splashed the editor, she got in trouble.</p><p>When the supervisor splashed the editor, he fought back. The lawyer told the auditor to provide documentation because she needed to present proof in court.</p><p>The lawyer told the auditor to provide documentation because he would be sent to jail otherwise. The nurse examined the farmer for injuries because she was screaming.</p><p>The nurse examined the farmer for injuries because he was caring. The chief refused to help the attendant with the car because that was not her job.</p><p>The chief refused to help the attendant with the car because that was his job. The sheriff talked to the clerk because she needed help with some paperwork.</p><p>The sheriff talked to the clerk because he could provide help with some paperwork. The guard was a big fan of the writer because she loved romantic novels.</p><p>The guard was a big fan of the writer because he was the best at writing romantic novels. The mover hit the tailor because she was rude.</p><p>The mover hit the tailor because he stood in the path. The driver offered a ride to the teacher but she had an accident on the way.</p><p>The driver offered a ride to the teacher but he refused the offer. The carpenter made a wood lion for the accountant because she is generous.</p><p>The carpenter made a wood lion for the accountant because it is his birthday.</p><p>Table 10: The English language samples for the GD task. Each sample in both the groups (Female and Male) contains a unique pair of gender-stereotyped and non-stereotyped professions (highlighted in bold).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Incontext examples selection for machine translation</title>
		<author>
			<persName><forename type="first">Sweta</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.02437</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2018a. Unsupervised statistical machine translation</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="3632" to="3642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An effective approach to unsupervised machine translation</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1019</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="194" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">2018b. Unsupervised neural machine translation</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2018, Proceedings of the Sixth International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Cahyawijaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nayeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Wilie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holy</forename><surname>Lovenia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiezheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willy</forename><surname>Chung</surname></persName>
		</author>
		<title level="m">A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">2302</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Shuguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Aguilar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09625</idno>
		<title level="m">Calcs 2021 shared task: Machine translation for code-switched data</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Crosslingual language model pretraining</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7057" to="7067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Qingxiu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.00234</idno>
		<title level="m">A survey for in-context learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Wino-x: Multilingual winograd schemas for commonsense reasoning and coreference resolution</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Emelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8517" to="8532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploring text-to-text transformers for english to hinglish machine translation with synthetic code-mixing</title>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Gautam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><surname>Kodali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kshitij</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anmol</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ponnurangam</forename><surname>Kumaraguru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Ganesh Jawahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Abdul-Mageed</surname></persName>
		</author>
		<author>
			<persName><surname>Vs Laks Lakshmanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching</title>
		<meeting>the Fifth Workshop on Computational Approaches to Linguistic Code-Switching</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="36" to="46" />
		</imprint>
	</monogr>
	<note>Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><surname>Wx Jiao</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.08745</idno>
		<title level="m">Is chatgpt a good translator? yes with gpt-4 as the engine</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Sit at mixmt 2022: Fluent translation built on giant pre-trained models</title>
		<author>
			<persName><forename type="first">Hrishikesh</forename><surname>Abdul Rafae Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preet</forename><surname>Amar Budhrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Jhanglani</surname></persName>
		</author>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ludovic Denoyer, and Marc&apos;Aurelio Ranzato. 2018a. Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Learning Representations</title>
		<meeting>the Sixth International Conference on Learning Representations<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Phrasebased &amp; neural unsupervised machine translation</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5039" to="5049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Few-shot learning with multilingual language models</title>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10668</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Few-shot learning with multilingual generative language models</title>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9019" to="9052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Multilingual denoising training for neural machine translation</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="726" to="742" />
		</imprint>
	</monogr>
	<note>Transactions of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longyue</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.01181</idno>
		<title level="m">New trends in machine translation using large language models: Case examples with chatgpt</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ingo</forename><surname>Lütkebohle</surname></persName>
		</author>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adaptive machine translation with large language models</title>
		<author>
			<persName><forename type="first">Yasmin</forename><surname>Moslem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rejwanul</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.13294</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Xin</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.01786</idno>
		<title level="m">Crosslingual generalization through multitask finetuning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Xiaozhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinfan</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yadao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Podolskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grigory</forename><surname>Arshinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.10845</idno>
		<title level="m">Pangu-tzSigmau: Towards trillion parameter language model with sparse heterogeneous computing</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution</title>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Winogrande: An adversarial winograd schema challenge at scale. Communications of the</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>ACM</publisher>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="99" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzana</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ilić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Castagné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Sasha Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName><surname>Gallé</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.05100</idno>
		<title level="m">Bloom: A 176bparameter open-access multilingual language model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Leveraging monolingual data with self-supervision for multilingual neural machine translation</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sneha</forename><surname>Kudugunta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2827" to="2835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunayana</forename><surname>Sitaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Soto</surname></persName>
		</author>
		<title level="m">2021. Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching</title>
		<editor>
			<persName><forename type="first">Emre</forename><surname>Yilmaz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anirudh</forename><surname>Srinivasan</surname></persName>
		</editor>
		<imprint>
			<publisher>Online</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mass: Masked sequence to sequence pretraining for language generation</title>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5926" to="5936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Overview and results of mixmt shared-task at wmt 2022</title>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Evaluating gender bias in machine translation</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1679" to="1684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Vilar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viresh</forename><surname>Ratnakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.09102</idno>
		<title level="m">Prompting palm for translation: Assessing strategies and performance</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Longyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianbo</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhirui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.02210</idno>
		<title level="m">Document-level machine translation with large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">2022b. Emergent abilities of large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<title level="m">Chain of thought prompting elicits reasoning in large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Code-switched language models using neural based synthetic data from parallel sentences</title>
		<author>
			<persName><forename type="first">Genta</forename><surname>Indra Winata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Understanding natural language</title>
		<author>
			<persName><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="191" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyu</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02414</idno>
		<title level="m">Glm-130b: An open bilingual pre-trained model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Prompting large language model for machine translation: A case study</title>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.07069</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution: Evaluation and debiasing methods</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="15" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingxiu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.04675</idno>
		<title level="m">Multilingual machine translation with large language models: Empirical results and analysis</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><surname>Appendix</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
