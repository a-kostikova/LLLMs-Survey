<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MasakhaNEWS: News Topic Classification for African languages</title>
				<funder ref="#_6tQ6zzf">
					<orgName type="full">Oracle Cloud</orgName>
				</funder>
				<funder>
					<orgName type="full">Oracle</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">David</forename><forename type="middle">Ifeoluwa</forename><surname>Adelani</surname></persName>
							<email>d.adelani@ucl.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marek</forename><surname>Masiak</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Israel</forename><forename type="middle">Abebe</forename><surname>Azime</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Saarland University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jesujoba</forename><forename type="middle">Oluwadara</forename><surname>Alabi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Saarland University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Atnafu</forename><forename type="middle">Lambebo</forename><surname>Tonja</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Instituto Politécnico Nacional</orgName>
								<address>
									<settlement>Mexico</settlement>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">Lelapa AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christine</forename><surname>Mwase</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Odunayo</forename><surname>Ogundepo</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bonaventure</forename><forename type="middle">F P</forename><surname>Dossou</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">Lelapa AI</orgName>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="institution">McGill University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff8">
								<orgName type="institution">Mila Quebec AI Institute</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff9">
								<address>
									<settlement>Lanfrica</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Akintunde</forename><surname>Oladipo</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Doreen</forename><surname>Nixdorf</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chris</forename><forename type="middle">Chinenye</forename><surname>Emezue</surname></persName>
							<affiliation key="aff9">
								<address>
									<settlement>Lanfrica</settlement>
								</address>
							</affiliation>
							<affiliation key="aff10">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sana</forename><surname>Sabah Al-Azzawi</surname></persName>
							<affiliation key="aff11">
								<orgName type="institution">Luleå University of Technology</orgName>
								<address>
									<settlement>Sweden</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Blessing</forename><forename type="middle">K</forename><surname>Sibanda</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Davis</forename><surname>David</surname></persName>
							<affiliation key="aff12">
								<orgName type="laboratory">Tanzania Data Lab</orgName>
								<address>
									<country key="TZ">Tanzania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lolwethu</forename><surname>Ndolela</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Mukiibi</surname></persName>
							<affiliation key="aff13">
								<orgName type="institution">Makerere University</orgName>
								<address>
									<country key="UG">Uganda</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tunde</forename><forename type="middle">O</forename><surname>Ajayi</surname></persName>
							<affiliation key="aff14">
								<orgName type="department">Insight Centre for Data Analytics</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tatiana</forename><forename type="middle">Moteu</forename><surname>Ngoli</surname></persName>
							<affiliation key="aff15">
								<orgName type="institution">Paderborn University</orgName>
								<address>
									<settlement>Germany</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brian</forename><surname>Odhiambo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Abraham</forename><forename type="middle">Toluwase</forename><surname>Owodunni</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nnaemeka</forename><forename type="middle">C</forename><surname>Obiefuna</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Muhidin</forename><surname>Mohamed</surname></persName>
							<affiliation key="aff16">
								<orgName type="institution">Aston University</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shamsuddeen</forename><forename type="middle">Hassan</forename><surname>Muhammad</surname></persName>
							<affiliation key="aff17">
								<orgName type="institution">University of Porto</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Teshome</forename><forename type="middle">Mulugeta</forename><surname>Ababu</surname></persName>
							<affiliation key="aff18">
								<orgName type="institution">Dire Dawa University</orgName>
								<address>
									<country key="ET">Ethiopia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Saheed</forename><forename type="middle">S</forename><surname>Abdullahi</surname></persName>
							<affiliation key="aff19">
								<orgName type="institution">Kaduna State University</orgName>
								<address>
									<country key="NG">Nigeria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mesay</forename><forename type="middle">Gemeda</forename><surname>Yigezu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Instituto Politécnico Nacional</orgName>
								<address>
									<settlement>Mexico</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tajuddeen</forename><surname>Gwadabe</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Idris</forename><surname>Abdulmumin</surname></persName>
							<affiliation key="aff20">
								<orgName type="institution">Ahmadu Bello University</orgName>
								<address>
									<country key="NG">Nigeria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Mahlet</roleName><forename type="first">Taye</forename><surname>Bame</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Oluwabusayo</forename><forename type="middle">O</forename><surname>Awoyomi</surname></persName>
							<affiliation key="aff21">
								<orgName type="institution">The College of Saint Rose</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Iyanuoluwa</forename><surname>Shode</surname></persName>
							<affiliation key="aff22">
								<orgName type="institution">Montclair State University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anu</forename><surname>Tolulope</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Habiba</forename><forename type="middle">Abdulganiy</forename><surname>Adelani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Abdul-Hakeem</forename><surname>Kailani</surname></persName>
						</author>
						<author>
							<persName><surname>Omotayo</surname></persName>
							<affiliation key="aff23">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adetola</forename><surname>Adeeko</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Afolabi</forename><surname>Abeeb</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anuoluwapo</forename><surname>Aremu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Olanrewaju</forename><surname>Samuel</surname></persName>
							<affiliation key="aff24">
								<orgName type="institution">University of Rwanda</orgName>
								<address>
									<country key="RW">Rwanda</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Clemencia</forename><surname>Siro</surname></persName>
							<affiliation key="aff25">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>The Netherlands, 26 AIMS</addrLine>
									<country key="CM">Cameroon</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wangari</forename><surname>Kimotho</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Raphael</forename><surname>Onyekachi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chinedu</forename><forename type="middle">E</forename><surname>Ogbu</surname></persName>
						</author>
						<author>
							<persName><surname>Mbonu</surname></persName>
							<affiliation key="aff26">
								<orgName type="institution">Nnamdi Azikiwe University</orgName>
								<address>
									<country key="NG">Nigeria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chiamaka</forename><forename type="middle">I</forename><surname>Chukwuneke</surname></persName>
							<affiliation key="aff26">
								<orgName type="institution">Nnamdi Azikiwe University</orgName>
								<address>
									<country key="NG">Nigeria</country>
								</address>
							</affiliation>
							<affiliation key="aff27">
								<orgName type="institution">Lancaster University</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Samuel</forename><surname>Fanijo</surname></persName>
							<affiliation key="aff28">
								<orgName type="institution">Iowa State University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jessica</forename><surname>Ojo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Oyinkansola</forename><forename type="middle">F</forename><surname>Awosan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tadesse</forename><surname>Kebede Guge</surname></persName>
							<affiliation key="aff29">
								<orgName type="institution">Haramaya University</orgName>
								<address>
									<addrLine>31 AIMS</addrLine>
									<settlement>Ethiopia</settlement>
									<country key="SN">Senegal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sakayo</forename><forename type="middle">Toadoum</forename><surname>Sari</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pamela</forename><surname>Nyatsine</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Freedmore</forename><surname>Sidume</surname></persName>
							<affiliation key="aff30">
								<orgName type="institution">BIUST</orgName>
								<address>
									<country key="BW">Botswana</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Oreen</forename><surname>Yousuf</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mardiyyah</forename><surname>Oduwole</surname></persName>
							<affiliation key="aff31">
								<orgName type="institution">NOUN</orgName>
								<address>
									<addrLine>34 PAUSTI</addrLine>
									<country>Nigeria, Kenya</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kanda</forename><forename type="middle">P</forename><surname>Tshinu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ussen</forename><surname>Kimanuka</surname></persName>
							<affiliation key="aff32">
								<orgName type="institution">Jamhuriya University</orgName>
								<address>
									<country key="SO">Somalia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thina</forename><surname>Diko</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Siyanda</forename><surname>Nxakama</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sinodos</forename><forename type="middle">G</forename><surname>Nugussie</surname></persName>
							<affiliation key="aff18">
								<orgName type="institution">Dire Dawa University</orgName>
								<address>
									<country key="ET">Ethiopia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Abdulmejid</forename><surname>Tuni Johar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shafie</forename><surname>Abdi Mohamed</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mire</forename><surname>Fuad</surname></persName>
						</author>
						<author>
							<persName><surname>Hassan</surname></persName>
							<affiliation key="aff33">
								<orgName type="institution">Somali National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Moges</roleName><forename type="first">Ahmed</forename><surname>Mehamed</surname></persName>
							<affiliation key="aff34">
								<orgName type="institution">Wuhan University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Evrard</forename><surname>Ngabire</surname></persName>
							<affiliation key="aff35">
								<orgName type="laboratory">Deutschzentrum</orgName>
								<orgName type="institution">Universität Burundi</orgName>
								<address>
									<country key="BI">Burundi</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jules</forename><surname>Twagirayezu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ivan</forename><surname>Ssenkungu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Masakhane NLP</orgName>
								<address>
									<settlement>Africa</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MasakhaNEWS: News Topic Classification for African languages</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E14D65FBB29FC63EFE9B7C567B8C56E1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T14:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite representing roughly a fifth of the world population, African languages are underrepresented in NLP research, in part due to a lack of datasets. While there are individual language-specific datasets for several tasks, only a handful of tasks (e.g. named entity recognition and machine translation) have datasets covering geographical and typologically-diverse African languages. In this paper, we develop MasakhaNEWS-the largest dataset for news topic classification covering 16 languages widely spoken in Africa. We provide and evaluate a set of baseline models by training classical machine learning models and fine-tuning several language models. Furthermore, we explore several alternatives * Equal contribution</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>to full fine-tuning of language models that are better suited for zero-shot and few-shot learning, such as: cross-lingual parameter-efficient fine-tuning (MAD-X), pattern exploiting training (PET), prompting language models (Chat-GPT), and prompt-free sentence transformer fine-tuning (SetFit and the co:here embedding API). Our evaluation in a few-shot setting, shows that with as little as 10 examples per label, we achieve more than 90% (i.e. 86.0 F1 points) of the performance of fully supervised training (92.6 F1 points) leveraging the PET approach. Our work shows that existing supervised approaches work well for all African languages and that language models with only a few supervised samples can reach competitive performance, both findings which demonstrate the applicability of existing NLP techniques for African languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>News topic classification is a text classification task in NLP that involves categorizing news articles into different categories like sports, business, entertainment, and politics. It has shaped the development of several machine learning algorithms over the years, such as topic modeling <ref type="bibr" target="#b6">(Blei et al., 2001;</ref><ref type="bibr" target="#b11">Dieng et al., 2020)</ref> and deep learning models <ref type="bibr" target="#b55">(Zhang et al., 2015;</ref><ref type="bibr" target="#b23">Joulin et al., 2017)</ref>. Similarly, news topic classification is a popular downstream task for evaluating the performance of large language models (LLMs) for both fine-tuning and prompttuning setups <ref type="bibr" target="#b54">(Yang et al., 2019;</ref><ref type="bibr" target="#b48">Sun et al., 2019;</ref><ref type="bibr" target="#b7">Brown et al., 2020;</ref><ref type="bibr" target="#b28">Liu et al., 2023)</ref>.</p><p>Despite the popularity of the task in benchmarking LMs, most of the evaluation have only been performed on English and a few other highresource languages. It is unclear how this approach extends to pre-trained multilingual language models for low-resource languages. For instance, BLOOM <ref type="bibr" target="#b30">(Scao et al., 2022)</ref> was pre-trained on 46 languages, including 22 African languages (mostly from the Niger-Congo family). However, extensive evaluation on these set of African languages was not performed due to lack of evaluation datasets. In general, only a handful of NLP tasks such as machine translation <ref type="bibr">(Adelani et al., 2022a;</ref><ref type="bibr" target="#b33">NLLB-Team et al., 2022)</ref>, named entity recognition <ref type="bibr">(Adelani et al., 2021</ref><ref type="bibr">(Adelani et al., , 2022b))</ref>, and sentiment classification <ref type="bibr" target="#b31">(Muhammad et al., 2023)</ref> have standardized benchmark datasets covering several geographical and typologically-diverse African languages. Another popular task that can be used for evaluating the downstream performance of language models is news topic classification, but human-annotated datasets for benchmarking topic classification using language models for African languages are scarce.</p><p>In this paper, we address two problems: the lack of evaluation datasets and lack of extensive evaluation of LMs for African languages. We create a large-scale news topic classification dataset covering 16 typologically-diverse languages widely spoken in Africa, including English and French, with the same label categories across all languages. Our dataset is also suitable for news headline generation task <ref type="bibr" target="#b4">(Aralikatte et al., 2023)</ref>: a special type of text summarization. We provide several baseline models using both classical machine learning approaches and fine-tuning LMs. Furthermore, we explore several alternatives to full finetuning of language models that are better suited for zero-shot and few-shot learning (e.g. 5-examples per label) such as cross-lingual parameter-efficient fine-tuning (MAD-X <ref type="bibr" target="#b37">(Pfeiffer et al., 2020)</ref>), pattern exploiting training (PET) <ref type="bibr">(Schick and Schütze, 2021a)</ref>, prompting ChatGPT LLM, and promptfree, sentence transformer fine-tuning (SetFit) <ref type="bibr">(Tunstall et al., 2022a)</ref>, and the co:here embedding API.</p><p>Our evaluation in a zero-shot setting shows the potential of prompting ChatGPT for news topic classification for low-resource African languages. We found that GPT-3.5-Turbo has impressive result for languages that make use of Latin script, but perform poorly for non-Latin based scripts like Amharic and Tigrinya. However, GPT-4 was able to overcome this challenge for non-Latin script with impressive performance matching the result of cross-lingual transfer experiments from a related African language.</p><p>In a few-shot setting, we show that with as little as 10 examples per label, we achieved more than 90% (i.e. 86.0 F1 points) of the performance of full supervised training (92.6 F1 points) leveraging the PET approach. We hope this encourages the NLP community to benchmark and evaluate LLMs on more low-resource languages. For reproducibility, we release our data and code under academic license or CC BY-NC 4.0 on Github.<ref type="foot" target="#foot_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>News topic classification , an application of text classification, is a popular task in natural language processing. There are various news topic classification datasets, including BBC News <ref type="bibr" target="#b18">(Greene and Cunningham, 2006)</ref>, AG News <ref type="bibr" target="#b55">(Zhang et al., 2015)</ref>, and the multimodal N24News <ref type="bibr" target="#b51">(Wang et al., 2022)</ref>, all of which are English datasets. In addition, there is the IndicNLP News <ref type="bibr" target="#b25">(Kunchukuttan et al., 2020)</ref> which is a multilingual dataset for Indian langauges. For African languages, only a handful of human annotated datasets exists, such as the Hausa &amp; Yorùbá dataset <ref type="bibr" target="#b21">(Hedderich et al., 2020)</ref> (only covering news headline), KINNEWS &amp; KIRNEWS datasets for Kinyarwanda and Kirundi <ref type="bibr" target="#b32">(Niyongabo et al., 2020)</ref>, and Tigrinya News <ref type="bibr" target="#b15">(Fesseha et al., 2021)</ref>. Others are semi-automatically created using predefined topics from news websites like Amharic news (Azime and Mohammed, 2021) and ANTC dataset <ref type="bibr">(Alabi et al., 2022)</ref>-that covered five African languages (Lingala, Somali, Naija, Malagasy, and isiZulu). These datasets, however, have limitations due to the fact that they were created with little or no human supervision and using different labeling schemes. In contrast, in this work we present news topic classification data for 16 typologically diverse African languages with a consistent labeling scheme across all languages.</p><p>Prompting Language Models using manually designed prompts to guide text generation has recently been applied to a myriad of NLP tasks, including topic classification. Models such as GPT-3 <ref type="bibr" target="#b7">(Brown et al., 2020)</ref> and T5 <ref type="bibr" target="#b39">(Raffel et al., 2020;</ref><ref type="bibr" target="#b41">Sanh et al., 2022)</ref> are able to learn more structural and semantic relationships between words and have shown impressive results even in multilingual scenarios when tuned for different tasks <ref type="bibr" target="#b8">(Chung et al., 2022;</ref><ref type="bibr" target="#b29">Muennighoff et al., 2023)</ref>. One approach to prompt-tuning a language model for topic classification is to design a "template" for classification and insert a sequence of text into template <ref type="bibr" target="#b17">(Gao et al., 2021;</ref><ref type="bibr" target="#b45">Shin et al., 2020)</ref>.</p><p>There are some other approaches to few-shot learning without prompting. One of them is Set-Fit <ref type="bibr">(Tunstall et al., 2022a)</ref>, which takes advantage of sentence transformers to generate dense representations for input sequences. These representations are then passed through a classifier to predict class labels. The sentence transformers are trained on a few examples using contrastive learning where positive and negative training pairs are sampled by in-class and out-class sampling. Another common approach is Pattern-Exploiting Training also known as PET <ref type="bibr">(Schick and Schütze, 2021a)</ref>. PET is a semisupervised training approach that used restructured input sequences to condition language models to better understand a given task, while iPET <ref type="bibr">(Schick and Schütze, 2021b</ref>) is an iterative variant of PET that is also shown to perform better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Languages</head><p>Table <ref type="table" target="#tab_0">1</ref> presents the languages covered in along with information on their language families, their primary geographic regions in Africa, and the number of speakers. Our dataset consists of a total of 16 typologically-diverse languages, and they were selected based on the availability of publicly available news corpora in each language, the availability of native-speaking annotators, geographical diversity and most importantly, because they are widely spoken in Africa. English and French are official languages in 42 African countries, Swahili is native to 12 countries, and Hausa is native to 6 countries. In terms of geographical diversity, we have four languages spoken in West Africa, seven languages spoken in East Africa, two languages spoken in Central Africa (i.e. Lingala and Kiswahili), and two spoken in Southern Africa (i.e chiShona and isiXhosa). Also, we cover four language families, Niger-Congo (8) Afro-Asiatic (5), Indo-European (2), and English Creole (1). The only English creole language is Nigerian-Pidgin, also known as Naija. Each language is spoken by at least 10 million people, according to Ehnologue <ref type="bibr" target="#b13">(Eberhard et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MasakhaNEWS dataset 4.1 Data Source</head><p>The data used in this research study were sourced from multiple reputable news outlets. The collection process involved crawling the British Broadcasting Corporation (BBC) and Voice of America (VOA) websites. We crawled between 2k-12k articles depending on the number of articles available on the websites. Some of the websites already have some pre-defined categories, we make use of this to additionally filter articles that do not belong to categories we plan to annotate. We took inspiration of news categorization from BBC English with six (6) pre-defined and well-defined categories ("business", "entertainment", "health", "politics", "sports", and "technology") with over 500 articles in each category. For English, we only crawled articles belonging to these categories while for the other languages, we crawled all articles. Our target is to have around 3,000 articles for annotation but three languages (Lingala, Rundi, and Somali) have less than that. Table <ref type="table" target="#tab_1">2</ref> shows the news source per language and the number of articles crawled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data Annotation</head><p>We recruited volunteers from the Masakhane community-an African grassroots community focused on advancing NLP for African languages. <ref type="foot" target="#foot_1">2</ref>The annotators were asked to label 3k articles into eight categories: "business", "entertainment", "health", "politics", "religion", "sports", "technology", and "uncategorized". Six of the categories are based on BBC English major news categories, the "religion" label was added since many African news websites frequently cover this topic. Other articles that do not belong to the first seven categories, are assigned to the "uncategorized" label.</p><p>For each language, the annotation followed two stages. In the first stage, we randomly shuffled the entire dataset and asked annotators to label the first 200 articles manually. In the second stage, we made use of active learning by combining the first 200 annotated articles with articles with predefined labels where available, and trained a classifier (i.e. by fine-tuning AfroXLMR-base <ref type="bibr">(Alabi et al., 2022)</ref>). We ran predictions on the rest of the articles, and asked annotators to correct the mistakes of the classifier. This approach helped to speed up the annotation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotation tool</head><p>We make use of an in-house annotation tool to label the articles. Appendix A shows an example of the interface of the tool. To further simplify the annotator effort, we ask annotators to label articles based on the headlines instead of the entire article. However, since some headlines are not very descriptive, we decided to concatenate the headline and the first two sentences of the news text to provide additional context to annotators.</p><p>Inter-agreement score We report Fleiss Kappa score <ref type="bibr" target="#b16">(Fleiss et al., 1971)</ref> to measure the agreement of annotation. Table <ref type="table" target="#tab_1">2</ref> shows that all languages have a moderate to perfect Fleiss Kappa score (i.e. 0.55 -0.85), which shows a high agreement among the annotators recruited for each language. Languages with only one annotator (i.e. Luganda and Rundi) were excluded in the evaluation.</p><p>Deciding a single label per article After annotation, we assigned the final label to each article by majority voting. Each label of an article needs to be agreed by a minimum of two annotators to be assigned the label. We only had exceptions for Luganda and Rundi, since they had one annotator. Our final dataset for each language consist of a minimum of 72 articles per topic, and a maximum of 500, except for English language where the classes are roughly balanced. We excluded the infrequent labels so we do not have a highly unbalanced dataset. The choice of a minimum of 72 articles ensures a minimum of 50 articles in the training set. <ref type="foot" target="#foot_2">3</ref> Our target is to have at least four topics per language with a minimum of 72 articles. This approach worked smoothly except for two languages: Lingala ("politics", "health" and "sports") and chiShona ("business", "health" and "politics"), where we had only three topics with more than 72 articles. To ensure we have more articles per class, we had to resolve the conflict in annotation between Lingala annotators to ensure we have more labels for the "business" category. This approach still results in infrequent classes for chiShona. We had to crawl additional "sports" articles from a local chiShona website (Kwayedza), followed by manual filtering of unrelated sports news.</p><p>Data Split Table <ref type="table" target="#tab_1">2</ref> provides the data split for languages. We also provide the distribution of articles by topics. We divided the annotated data into TRAIN, DEV and TEST split following 70% / 10% / 20% split ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Baseline Experiments</head><p>We trained baseline text classification models by concatenating the news headline and news text using different approaches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baseline Models</head><p>We trained three classical ML models: Naive Bayes, multi-layer perceptron, and XGBoost using the popular sklearn tool <ref type="bibr" target="#b36">(Pedregosa et al., 2011)</ref>. We employed the "CountVectorizer" method to represent the text data, which converts a collection of text documents to a matrix of token counts. This method allows us to convert text data into numerical feature vectors. Furthermore, we fine-tune nine kinds of multilingual text encoders, seven of them are BERT/RoBERTa-based i.e. XLM-R (base &amp; large) <ref type="bibr" target="#b10">(Conneau et al., 2020)</ref>, AfriBERTalarge <ref type="bibr" target="#b34">(Ogueji et al., 2021)</ref>, <ref type="bibr">RemBERT (Chung et al., 2021)</ref>, AfroXLMR (base &amp; large) <ref type="bibr">(Alabi et al., 2022), and</ref><ref type="bibr">AfroLM (Dossou et al., 2022)</ref>, the other two are mDeBERTaV3 <ref type="bibr">(He et al., 2021a)</ref>, and LaBSE <ref type="bibr" target="#b14">(Feng et al., 2022)</ref>. mDeBERTaV3 pretrained a DeBERTa-style model <ref type="bibr">(He et al., 2021b)</ref> with replaced token detection objective proposed in ELECTRA <ref type="bibr" target="#b9">(Clark et al., 2020)</ref>. On the other hand, LaBSE is a multilingual sentence transformer model that is popular for mining parallel corpus for machine translation.</p><p>Finally, we fine-tuned four multilingual Textto-Text (T2T) models, mT5-base <ref type="bibr">(Xue et al., 2021)</ref>, Flan-T5-base (Chung et al., 2022), AfriMT5-base <ref type="bibr">(Adelani et al., 2022a)</ref>, AfriTeVAbase <ref type="bibr" target="#b24">(Jude Ogundepo et al., 2022)</ref>. The fine-tuning and evaluation of the multilingual text-encoders and T2T models were performed using Hugging-Face Transformers <ref type="bibr" target="#b52">(Wolf et al., 2020)</ref> and Py-Torch Lightning 4 . The models were fine-tuned on 4 https://pypi.org/project/pytorch-lightning/ Nvidia V100 GPU for 20 epochs, batch size of 32, 1e -5/5e -5 lr, and max. sequence length of 256.</p><p>The LMs evaluated were both massively multilingual (i.e. typically trained on over 100 languages around the world) and African-centric (i.e. trained mostly on languages spoken in Africa). The African-centric multilinual text encoders are all modeled after XLM-R. AfriBERTa was pretrained from scratch on 11 African languages, AfroXLMR was adapted to African languages through finetuning the original XLM-R model on 17 African languages and 3 languages commonly spoken in Africa, while AfroLM was pretrained on 23 African languages utilizing active learning. Similar to the multilingual text encoders, the T2T models used in this study were pretrained on hundreds of languages, and they are all based on the T5 model <ref type="bibr" target="#b39">(Raffel et al., 2020)</ref>, which is an encoder-decoder model trained with the span-mask denoising objective. mT5 is a multilingual version of T5, and Flan-T5 was fine-tuned on multiple tasks using T5 as a base. The study also included adaptations of the original models, such as AfriMT5-base, as well as AfriTeVA-base, a T5 model pre-trained on 10 African languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baseline Results</head><p>Table <ref type="table" target="#tab_2">3</ref> shows the result of training several models on TRAIN split and evaluation on the TEST split for each language. Our evaluation shows that classical ML models are worse in general than finetuning multilingual LMs on average, however, the drop in performance is sometimes comparable to LMs if the language was not covered during the pre-training. For example, MLP, NaiveBayes and The best result achieved is by AfroXLMRbase/large with over 4.0 F1 improvement over AfriBERTa. The larger variant gave the overall best result due to the size. AfroXLMR models benefited from being pre-trained on most of the languages we evaluated on. We also tried multilingual T2T models, but none of the models reach the performance of AfroXLMR-large despite their larger sizes. We observe the same trend that the adapted mT5 model (i.e. AfriMT5) gave better result compared to mT5 similar to how AfroXLMR gave better result than XLM-R. We found FlanT5-base to be competitive to AfriMT5 despite seeing few African languages, however, the performance was very low for languages that uses the Ge'ez script like amh and tir since the model do not support Ge'ez.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Headline-only training</head><p>We compare our results using headline+text (as shown in Table <ref type="table" target="#tab_2">3</ref>) with training on the article headline-with shorter content, we find out that fine-tuned LMs gave impressive performance with only headlines while classical ML methods struggle due to shorter content. Figure <ref type="figure">1</ref> shows the result of our comparison. AfroXLMR-base and AfroXLMR-large both improve by (2.3) and (1.5) F1 points respectively if we use headline+text instead of headline. Classical ML models improve the most when we make use of headline+text instead of headline; MLP, NaiveBayes and XGBoost improve by large F1 points (i.e. 7.4 -9.7). Thus, for the remainder of this paper, we make use of headline+text. Appendix B provides the breakdown of the result by languages for the comparison of headline and headline+text. Table <ref type="table">4</ref>: Zero-shot learning on . We compare several approaches such as using MAD-X, PET and SetFit. We excluded the source languages hau and swa from the average (AVG src ).</p><p>6 Zero-shot and Few-shot transfer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Methods</head><p>Here, we compare different zero-shot and few-shot methods:</p><p>Fine-tune (Fine-tune on a source language, and evaluate on a target language) using AfroXLMRbase. This is only used in the zero-shot setting.</p><p>MAD-X <ref type="bibr" target="#b37">(Pfeiffer et al., 2020</ref><ref type="bibr" target="#b38">(Pfeiffer et al., , 2021</ref>) -a parameter efficient approach for cross-lingual transfer leveraging the modularity, and portability of adapters <ref type="bibr" target="#b22">(Houlsby et al., 2019)</ref>. We followed the same zero-shot setup as Alabi et al. ( <ref type="formula">2022</ref>), however, we make use of hau and swa as source languages since they cover all the news topics used by all languages. The setup is as follows: (1) We train language adapters using monolingual news corpora of our focus languages. We perform language adaptation on the news corpus to match the domain of our dataset, similar to <ref type="bibr">(Alabi et al., 2022)</ref>. ( <ref type="formula">2</ref>) We train a task adapter on the source language labelled data using source language adapter. (3) We substitute the source language adapter with the target language to run prediction on the target language test set, while retaining the task adapter.</p><p>PET/iPET <ref type="bibr">(Schick and Schütze, 2021a,b)</ref>, also known as (Iterative) Pattern Exploiting Training is a semi-supervised approach that makes use of few labelled examples and a prompt/pattern to a LM for few-shot learning. It involves three steps.</p><p>(1) designing of a prompt/pattern and a verbalizer (that maps each label to a word from LM vocabulary). ( <ref type="formula">2</ref>) train an LM on each pattern based on few labelled examples (3) distill the knowledge of the LM on unlabelled data. Therefore, PET leverages unlabelled examples to improve few-shot learning. iPET on the other hand, repeats step 2 and 3 iteratively. We make use of the same set of patterns used for AGNEWS English dataset <ref type="bibr" target="#b55">(Zhang et al., 2015)</ref> provided by the PET/iPET authors. The patterns are (1) Step 2, the fine-tuned sentence transformer model is used to extract rich sentence representation for each labelled example, followed by logistic regression for classification. The advantage of this approach is that it is faster and requires no prompt unlike PET. We use this in both zero-and few-shot setting. For the zero-shot setting, SetFit creates dummy example N -times (we set N = 8, similar to the SetFit paper) like "this sentence is {}" where {} can be any news topic like "sports".</p><formula xml:id="formula_0">P 1 (x) = ____ : a, b (2) P 2 (x) = a(____)b (3) P 3 (x) = ____ -ab (4) P 4 (x) = ab(____)<label>(</label></formula><p>Co:here multilingual sentence transformer co:here introduced a multilingual embedding model multilingual-22-12<ref type="foot" target="#foot_3">5</ref> , which supports over a hundred languages, including most of the languages included in . This is only for the few-shot setting.</p><p>OpenAI ChatGPT API<ref type="foot" target="#foot_4">6</ref> is an LLM trained on a large chunk of texts to predict the next word like GPT-3 <ref type="bibr" target="#b7">(Brown et al., 2020)</ref>, followed by a set of instructions in a prompt based on human feedback. It leverages Reinforcement Learning from Human Feedback (RLHF), similar to InstructGPT <ref type="bibr" target="#b35">(Ouyang et al., 2022)</ref> to make the LLM to interact in a conversational way. We prompt the OpenAI API based on GPT-3.5 Turbo and GPT-4 to categorize articles into news topics. For the prompting, we make use of a simple template from <ref type="bibr" target="#b41">Sanh et al. (2022)</ref>:</p><p>'Is this a piece of news regarding {{"business, entertainment, health, politics, religion, sports or technology"}}? {{INPUT}}'. We make use of the first 100 tokens of headline+text as {{INPUT}}.</p><p>The completion of the LLM can be a single word, a sentence, or multiple sentences. We check if a descriptive word relating to any of the news topics has been predicted. For example, "economy", "economic", "finance" is mapped to "business" news. We provide more details on the ChatGPT evaluation in Appendix C. For all few-shot settings, we tried K samples/shots per class where K = 5, 10, 20, 50. We make use of LaBSE as the sentence transformer for SetFit, and AfroXLMR-large as the LM for PET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Zero-shot evaluation</head><p>GPT-3.5-Turbo performs poorly on non-Latin scripts Table <ref type="table">4</ref> shows the result of zero-shot evaluation using FINE-TUNE, MAD-X, PET, SETFIT and GPT-3.5-TURBO (March 2023 version). Our result shows that cross-lingual zero-shot transfer from a source language with same domain and task (i.e FINE-TUNE &amp; MAD-X), gives superior result (+11 F1) than PET, SetFit, and GPT-3.5-TURBO. GPT-3.5-TURBO gave better results with over +9.0 F1 point better than SETFIT and PET showing that capabilities of instruction-tuned LLMs over smaller LMs. However, the results of CHATGPT were poor (&lt; 42.0) for non-Latin based languages like Amharic and Tigrinya which makes use of the Ge'ez script. The languages that make use of Latin script have over 59.0%. Surprisingly, some results of GPT-3.5-TURBO are comparable to the FINE-TUNE approach for some languages (English, Luganda, Oromo, Naija, Somali, isiXhosa, and Yorùbá), without leveraging any additional technique apart from prompting the LLM.</p><p>GPT-3.5-Turbo evaluation improves with newer versions We repeated GPT-3.5-TURBO evaluation using a newer version (May 23, 2023 version), our results suggest a significant improvement of the result for 14 (out of 16) languages in our evaluation. This implies that the newer version of the model seems to be better than older versions for the news topic classification task.</p><p>GPT-4 overcomes the limited non-Latin capabilities of GPT-3.5-Turbo We also evaluated on GPT-4 on the 16 languages in zero-shot setting.</p><p>Our results shows a significant improvement in performance over GPT-3.5-TURBO by over +9 points. Surprinsingly, GPT-4 was able to overcome the limitation of GPT-3.5-TURBO for languages with non-Latin script (i.e Amharic and Tigrinya) with impressive performance, matching the performance of cross-lingual transfer experiment from a related African language (i.e. FINE-TUNE hau/swa→ xx and MAX-X hau→ xx).</p><p>The large performance gap between GPT-3.5-Turbo and GPT-4 may be due to either the former being a distilled version of a more powerful model created to reduce inference cost, which also significantly affected its performance on non-Latin scripts.<ref type="foot" target="#foot_5">7</ref> <ref type="foot" target="#foot_6">8</ref> Alternatively, GPT-4 may just be a bigger and better model with more multilingual and non-Latin capabilities.</p><p>Leveraging labelled data from other languages is more effective In general, it may be advantageous to consider leveraging knowledge from other languages with available training data when no labelled data is available for the target language. Also, we observe that Swahili (swa) achieves better result as a source language than Hausa (hau) especially when transferring to fra (+13.8), lug (+9.0), and eng (+3.6). The reason for the impressive performance from Swahili to Luganda might be due to both languages belonging to the same Greater Lake Bantu language sub-group, but it is  <ref type="bibr">-shots 75.5 75.2 65.9 64.6 86.1 72.6 31.3 56.8 95.8 87.3 80.8 38.9 73.8 36.3 61.7 69.4 67.0 20-shots 88.5 85.6 78.3 85.2 90.4 80.8 48.4 41.1 97.4 90.0 92.3 63.6 82.9 67.3 83.1 84.3 78.7 50-shots 91.4 87.5 86.9 88.8 87.3 91.0 75.2 71.3 96.4 89.8 95.5 85.3 86.6</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Few-shot evaluation</head><p>Table <ref type="table" target="#tab_5">5</ref> shows the result of the few-shot learning approaches. With only 5-shots, we find all the fewshot approaches to be better than the usual FINE-TUNE baselines for most languages. However, as the number of shots increases, they have comparable results with SETFIT and CO:HERE API especially for K = 20, 50 shots. However, we found that PET achieved very impressive results with 5shots (81.9 on average), matching the performance of SETFIT/CO:HERE API with 50-shots. The results are even better with more shots i.e (k = 10, 86.0 F1), (k = 20, 87.9 F1), and (k = 50, 89.9 F1). Surprisingly, with 50-shots, PET gave competitive result to the full-supervised setting (i.e. fine-tuning all TRAIN data) that achieved (92.6 F1) (see Table <ref type="table" target="#tab_2">3</ref>). It's important to note that PET make use of additional unlabelled data while SetFit and Cohere API do not. In general, our result highlight the importance of getting few labelled examples for a new language we are adapting to, even if it is as little as 10 examples per class-which is typically not time-consuming to annotate <ref type="bibr" target="#b26">(Lauscher et al., 2020;</ref><ref type="bibr" target="#b21">Hedderich et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we created the largest news topic classification dataset for 16 typologically diverse languages spoken in Africa. We provide an extensive evaluation using both full-supervised and few-shot learning settings. Furthermore, we study different techniques of adapting prompt-based tuning and non-prompt methods of LMs to African languages. Our experimental results shows that prompting LLMs like ChatGPT perform poorly on the simple task of text classification for several under-resourced African languages especially for non-Latin based scripts. Furthermore, we showed the potential of prompt-based few-shot learning approaches like PET (based on smaller LMs) for African languages. Our work shows that existing supervised approaches work well for all African languages and that language models with only a few supervised samples can reach competitive performance, both findings which demonstrate the applicability of existing NLP techniques for African languages.</p><p>In the future, we plan to extend this dataset to more African languages, include the evaluation of other multilingual LLMs like BLOOM, mT0 <ref type="bibr" target="#b30">(Muennighoff et al., 2022)</ref> and XGLM <ref type="bibr">(Lin et al., 2022)</ref>, and extend analysis to other text classification tasks like sentiment classification <ref type="bibr" target="#b46">(Shode et al., 2022</ref><ref type="bibr" target="#b47">(Shode et al., , 2023;;</ref><ref type="bibr" target="#b31">Muhammad et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Limitations</head><p>One major limitation of our work is that we did not evaluate extensively the performance of ChatGPT LLM on several African languages and tasks such as question answering, and text generation tasks. Our evaluation is only limited to text classification and may not generalize to many tasks. However, we feel that if it perform poorly on text classification, the result may even be worse on more difficult NLP tasks. Also, there is a challenge that our result may not be fully reproducible since we use the ChatGPT API where the underlining LLM are often updated or improved with time. It might be that the support for non-Latin based script may improve significantly in few months. This limitation also applied to the co:here embedding API.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Ethics Statement</head><p>Our work aims to provide benchmark dataset for African languages, we do not see any potential harms when using our news topic classification datasets and models to train ML models, the annotated dataset is based on the news domain, and the articles are publicly available, and we believe the dataset and news topic annotation is unlikely to cause unintended harm. Also, we do not see any privacy risks in using our dataset and models because it is based on news domain. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Annotation Tool</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Comparing different article content types</head><p>Table <ref type="table" target="#tab_7">7</ref> provides the comparison between using only news headline and headline+text for training. We find significantly improvement on average when we make use of headline+text for training across all models and languages especially for classical ML methods (MLP, NaiveBayes, and XG-Boost).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ChatGPT Evaluation</head><p>We prompted ChatGPT for news topic classification using the following template: 'Is this a piece of news regarding {{"business, entertainment, health, politics, religion, sports or technology"}}? {{IN-PUT}}'. The completion may take different forms e.g. a single word, sentence or multiple sentences.</p><p>Examples of such predictions are:</p><p>1. sports 2. This is a piece of news regarding sports.</p><p>3. This is a piece of sports news regarding the CHAN 2021 football tournament in Cameroon. It reports that the Mali national football team has advanced to the semi-finals after defeating the Congo national team in a match that ended in a penalty shootout.</p><p>4. This is a piece of news regarding sports. It talks about the recent match between Tunisia and Angola in the African Cup of Nations.</p><p>Both teams scored a goal, and the article mentions some of the details of the game, such as the penalty and missed chances.</p><p>5. I'm sorry, but I'm having trouble understanding this piece of news as it appears to be in a language I don't recognize. Can you please provide me with news in English so I can assist you better?</p><p>To extract the right category, we make use of a simple verbalizer that maps the news topic to several indicative words (capitalization ignored) for the category like: (a) 'business': {'business', 'finance', 'economy'.</p><p>'economics' } When the right category is not obvious, like (5 : "I'm sorry, but I'm having trouble understanding this piece of news as it appears to be in a language I don't recognize. "), we choose a random category before computing F1-score. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>5) P 5 (x) = ____N ews : ab (6) P 6 x) = [Category : ____]ab, where a is the news headline and b is the news text. In evaluation, we take average over all patterns.SetFit(Tunstall et al., 2022b) is a few-shot learning framework based on sentence transformer models<ref type="bibr" target="#b40">(Reimers and Gurevych, 2019)</ref> like LaBSE following two steps. Step 1 fine-tunes the sentence transformer model using a few labelled examples with contrastive learning-where positive examples, are K-examples from a class c, and negative examples pairs are labelled examples with random labels from other classes. Contrastive learning approach enlarges the size of training data in few-shot scenarios. In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2 provides an example of the interface of our in-house annotation tool.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Interface of our in-house Annotation tool. Annotators can correct the pre-defined category assigned and also edit their annotation</figDesc><graphic coords="15,83.11,70.86,426.34,218.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Languages covered in and Data Source: including language family, region, number of L1 &amp; L2 speakers, and number of articles from each news source.</figDesc><table><row><cell>Language</cell><cell>Family/branch</cell><cell>Region</cell><cell># speakers News Source</cell><cell># articles</cell></row><row><cell cols="3">Amharic (amh) Afro-Asiatic / Ethio-Semitic East Africa</cell><cell>57M BBC</cell><cell>8,204</cell></row><row><cell>English (eng)</cell><cell>Indo-European / Germanic</cell><cell>Across Africa</cell><cell>1268M BBC</cell><cell>5,073</cell></row><row><cell>French (fra)</cell><cell>Indo-European /Romance</cell><cell>Across Africa</cell><cell>277M BBC</cell><cell>5,683</cell></row><row><cell>Hausa (hau)</cell><cell>Afro-Asiatic / Chadic</cell><cell>West Africa</cell><cell>77M BBC</cell><cell>6,965</cell></row><row><cell>Igbo (ibo)</cell><cell>Niger-Congo / Volta-Niger</cell><cell>West Africa</cell><cell>31M BBC</cell><cell>4,628</cell></row><row><cell>Lingala (lin)</cell><cell>Niger-Congo / Bantu</cell><cell>Central Africa</cell><cell>40M VOA</cell><cell>2,022</cell></row><row><cell>Luganda (lug)</cell><cell>Niger-Congo / Bantu</cell><cell>Central Africa</cell><cell>11M Gambuuze</cell><cell>2,621</cell></row><row><cell>Naija (pcm)</cell><cell>English Creole</cell><cell>West Africa</cell><cell>121M BBC</cell><cell>7,783</cell></row><row><cell>Oromo (orm)</cell><cell>Afro-Asiatic / Cushitic</cell><cell>East Africa</cell><cell>37M BBC</cell><cell>7,782</cell></row><row><cell>Rundi (run)</cell><cell>Niger-Congo / Bantu</cell><cell>East Africa</cell><cell>11M BBC</cell><cell>2,995</cell></row><row><cell cols="2">chiShona (sna) Niger-Congo / Bantu</cell><cell>Southern Africa</cell><cell>11M VOA &amp; Kwayedza</cell><cell>11,146</cell></row><row><cell>Somali (som)</cell><cell>Afro-Asiatic / Cushitic</cell><cell>East Africa</cell><cell>22M BBC</cell><cell>2,915</cell></row><row><cell cols="2">Kiswahili (swa) Niger-Congo / Bantu</cell><cell cols="2">East &amp; Central Africa 71M-106M BBC</cell><cell>6,431</cell></row><row><cell>Tigrinya (tig)</cell><cell cols="2">Afro-Asiatic / Ethio-Semitic East Africa</cell><cell>9M BBC</cell><cell>4,372</cell></row><row><cell>isiXhosa (xho)</cell><cell>Niger-Congo / Bantu</cell><cell>Southern Africa</cell><cell>19M Isolezwe</cell><cell>24,658</cell></row><row><cell>Yorùbá (yor)</cell><cell>Niger-Congo / Volta-Niger</cell><cell>West Africa</cell><cell>46M BBC</cell><cell>6,974</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>MasakhaNEWS dataset. The size of the annotated data, news topics, and number of annotators. Topics are labelled by their prefixes in the table (topics): business, entertainment, health, politics, religion, sport, technology.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="5">Topics (number of articles per topic)</cell><cell></cell><cell></cell><cell></cell><cell>Fleiss</cell></row><row><cell>Language</cell><cell cols="11">Train/Dev/Test # topics # bus # ent # health # pol # rel # sport # tech # Annotator Kappa</cell></row><row><cell>Amharic (amh)</cell><cell>1311/ 188/ 376</cell><cell>4</cell><cell>404</cell><cell>-</cell><cell>500</cell><cell>500</cell><cell>-</cell><cell>471</cell><cell>-</cell><cell>5</cell><cell>0.81</cell></row><row><cell>English (eng)</cell><cell>3309/ 472/ 948</cell><cell>6</cell><cell>799</cell><cell>750</cell><cell>746</cell><cell>821</cell><cell>-</cell><cell>1000</cell><cell>613</cell><cell>7</cell><cell>0.81</cell></row><row><cell>French (fra)</cell><cell>1476/ 211/ 422</cell><cell>5</cell><cell>500</cell><cell>-</cell><cell>500</cell><cell>500</cell><cell>-</cell><cell>500</cell><cell>109</cell><cell>3</cell><cell>0.83</cell></row><row><cell>Hausa (hau)</cell><cell>2219/ 317/ 637</cell><cell>7</cell><cell>399</cell><cell>500</cell><cell>493</cell><cell cols="2">500 493</cell><cell>497</cell><cell>291</cell><cell>5</cell><cell>0.85</cell></row><row><cell>Igbo (ibo)</cell><cell>1356/ 194/ 390</cell><cell>6</cell><cell>292</cell><cell>366</cell><cell>424</cell><cell>500</cell><cell>73</cell><cell>285</cell><cell>-</cell><cell>4</cell><cell>0.65</cell></row><row><cell>Lingala (lin)</cell><cell>608/ 87/ 175</cell><cell>4</cell><cell>82</cell><cell>-</cell><cell>193</cell><cell>500</cell><cell>-</cell><cell>95</cell><cell>-</cell><cell>2</cell><cell>0.56</cell></row><row><cell>Luganda (lug)</cell><cell>771/ 110/ 223</cell><cell>5</cell><cell>169</cell><cell>-</cell><cell>228</cell><cell>500</cell><cell>91</cell><cell>116</cell><cell>-</cell><cell>1</cell><cell>-</cell></row><row><cell>Oromo (orm)</cell><cell>1015/ 145/ 292</cell><cell>4</cell><cell>-</cell><cell>119</cell><cell>447</cell><cell>500</cell><cell>-</cell><cell>386</cell><cell>-</cell><cell>3</cell><cell>0.63</cell></row><row><cell>Naija (pcm)</cell><cell>1060/ 152/ 305</cell><cell>5</cell><cell>97</cell><cell>460</cell><cell>159</cell><cell>309</cell><cell>-</cell><cell>492</cell><cell>-</cell><cell>4</cell><cell>0.66</cell></row><row><cell>Rundi (run)</cell><cell>1117/ 159/ 322</cell><cell>6</cell><cell>76</cell><cell>158</cell><cell>372</cell><cell>500</cell><cell>73</cell><cell>419</cell><cell>-</cell><cell>1</cell><cell>-</cell></row><row><cell>chiShona (sna)</cell><cell>1288/ 185/ 369</cell><cell>4</cell><cell>500</cell><cell>-</cell><cell>425</cell><cell>500</cell><cell>-</cell><cell>417</cell><cell>-</cell><cell>3</cell><cell>0.63</cell></row><row><cell>Somali (som)</cell><cell>1021/ 148/ 294</cell><cell>7</cell><cell>114</cell><cell>139</cell><cell>354</cell><cell>500</cell><cell>73</cell><cell>148</cell><cell>135</cell><cell>3</cell><cell>0.55</cell></row><row><cell cols="2">Kiswahili (swa) 1658/ 237/ 476</cell><cell>7</cell><cell>316</cell><cell>98</cell><cell>500</cell><cell cols="2">500 292</cell><cell>500</cell><cell>165</cell><cell>4</cell><cell>0.72</cell></row><row><cell>Tigrinya (tir)</cell><cell>947/ 137/ 272</cell><cell>6</cell><cell>80</cell><cell>167</cell><cell>395</cell><cell>500</cell><cell>-</cell><cell>125</cell><cell>89</cell><cell>2</cell><cell>0.63</cell></row><row><cell>isiXhosa (xho)</cell><cell>1032/ 147/ 297</cell><cell>5</cell><cell>72</cell><cell>500</cell><cell>100</cell><cell>308</cell><cell>-</cell><cell>496</cell><cell>-</cell><cell>3</cell><cell>0.89</cell></row><row><cell>Yorùbá (yor)</cell><cell>1433/ 206/ 411</cell><cell>5</cell><cell>-</cell><cell>500</cell><cell>398</cell><cell cols="2">500 317</cell><cell>335</cell><cell>-</cell><cell>5</cell><cell>0.80</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Baseline results on . We compare several ML approaches using both classical ML and LMs. Average is over 5 runs. Evaluation is based on weighted F1-score. Africa-centric models are in gray color</figDesc><table><row><cell cols="2">Model</cell><cell></cell><cell>size</cell><cell>amh</cell><cell>eng</cell><cell cols="2">fra hau</cell><cell>ibo</cell><cell>lin</cell><cell>lug orm pcm</cell><cell>run</cell><cell>sna som swa</cell><cell>tir</cell><cell>xho</cell><cell>yor AVG</cell></row><row><cell cols="3">classical ML</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MLP</cell><cell></cell><cell cols="11">&lt;20K 92.0 88.2 84.6 86.7 80.1 84.3 82.2 86.7 93.5 85.9 92.6 71.1 77.9 81.9 94.5 89.3</cell><cell>85.7</cell></row><row><cell cols="3">NaiveBayes</cell><cell cols="11">&lt;20K 91.8 83.7 84.3 85.3 79.8 82.8 84.0 85.6 92.8 79.9 91.5 74.8 76.6 71.4 91.0 84.0</cell><cell>83.7</cell></row><row><cell cols="2">XGBoost</cell><cell></cell><cell cols="11">&lt;20K 90.1 86.0 81.2 84.7 78.6 74.8 83.8 83.2 93.3 79.2 94.3 68.5 74.9 75.2 91.1 85.2</cell><cell>82.8</cell></row><row><cell cols="4">multilingual text encoders</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">AfriBERTa</cell><cell>126M</cell><cell cols="10">90.6 88.9 76.4 89.2 87.3 87.0 85.1 89.4 98.1 91.3 89.3 83.9 83.3 87.0 86.9 90.3</cell><cell>87.8</cell></row><row><cell cols="3">XLM-R-base</cell><cell>270M</cell><cell cols="10">90.9 90.6 90.4 88.4 82.5 87.9 65.3 82.2 97.8 85.9 88.9 73.8 85.6 54.6 78.6 84.5</cell><cell>83.0</cell></row><row><cell cols="3">AfroXLMR-base</cell><cell>270M</cell><cell cols="10">94.2 92.2 92.5 91.0 90.7 93.0 89.4 92.1 98.2 91.4 95.4 85.2 88.2 86.5 94.7 93.0</cell><cell>91.7</cell></row><row><cell cols="2">AfroLM</cell><cell></cell><cell>270M</cell><cell cols="10">90.3 87.7 77.5 88.3 85.4 85.7 88.0 83.5 95.9 86.8 92.5 72.0 83.2 83.5 91.4 86.5</cell><cell>86.1</cell></row><row><cell cols="3">mDeBERTa</cell><cell>276M</cell><cell cols="10">91.7 90.8 89.2 88.6 88.3 81.6 65.7 84.7 96.8 89.4 93.9 72.0 84.6 78.7 90.5 89.3</cell><cell>86.0</cell></row><row><cell cols="2">LABSE</cell><cell></cell><cell>471M</cell><cell cols="10">92.5 91.6 90.9 90.0 91.6 89.6 86.8 86.7 98.4 91.1 94.6 82.1 87.6 83.8 94.7 92.1</cell><cell>90.3</cell></row><row><cell cols="3">XLM-R-large</cell><cell>550M</cell><cell cols="10">93.1 92.2 91.4 90.6 84.2 91.8 73.9 88.4 98.4 87.0 88.9 76.1 85.6 62.7 89.2 84.5</cell><cell>86.1</cell></row><row><cell cols="4">AfroXLMR-large 550M</cell><cell cols="10">94.4 93.1 91.1 92.2 93.4 93.7 89.9 92.1 98.8 92.7 95.4 86.9 87.7 89.5 97.3 94.0</cell><cell>92.6</cell></row><row><cell cols="3">RemBERT</cell><cell>559M</cell><cell cols="10">92.4 92.4 90.8 90.5 91.1 91.5 86.7 88.7 98.2 90.6 93.9 75.9 86.7 69.9 92.5 93.0</cell><cell>89.1</cell></row><row><cell cols="4">multilingual text-to-text LMs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">AfriTeVa-base</cell><cell>229M</cell><cell cols="10">87.0 80.3 71.9 85.8 79.9 82.8 60.2 82.9 95.2 80.0 84.4 58.0 80.7 55.2 69.4 86.4</cell><cell>77.5</cell></row><row><cell cols="3">mT5-base</cell><cell>580M</cell><cell cols="10">78.2 89.8 59.0 82.7 76.8 80.8 75.0 79.2 96.1 85.7 90.4 75.0 76.1 65.1 71.8 86.2</cell><cell>80.0</cell></row><row><cell cols="3">Flan-T5-base</cell><cell>580M</cell><cell cols="10">54.5 92.4 88.9 84.5 86.6 90.6 84.1 85.8 97.8 87.3 90.6 76.0 79.0 41.5 90.8 88.0</cell><cell>82.4</cell></row><row><cell cols="3">AfriMT5-base</cell><cell>580M</cell><cell cols="10">90.2 90.3 87.4 87.9 88.0 88.6 84.8 83.9 96.6 91.0 91.5 77.8 84.4 80.8 91.6 88.8</cell><cell>87.7</cell></row><row><cell></cell><cell>90 95</cell><cell cols="3">Headline Headline+Text</cell><cell></cell><cell>89.4</cell><cell>91.7</cell><cell>91.1</cell><cell>92.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>85</cell><cell>85.7</cell><cell>83.7</cell><cell></cell><cell>82.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>F1-score</cell><cell>75 80</cell><cell>78.3</cell><cell>73.4</cell><cell></cell><cell>73.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>60</cell><cell>MLP</cell><cell cols="7">NaiveBayes XGBoost AfroXLMR-B AfroXLMR-L</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">News topic classification models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">Figure 1: Comparison of article content type used for</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">training news topic classification models. We report</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">the average across all languages when either headline</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">or headline+text is used</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">XGBoost have better performance than AfriBERTa</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">on fra and sna since they were not seen during pre-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">training of the LM. Similarly, AfroLM had worse</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">result for fra for the same reason. On average,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">XLM-R-base, AfroLM, mDeBERTaV3, XLM-R-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">large gave 83.0 F1, 86.1 F1, 86.0 F1, and 86.1 F1</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">respectively, with worse performance compared to</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">the other LMs (87.8 -92.6 F1) because they do</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">not cover some of the African languages during</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">pre-training (see Table 6) or they have been pre-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">trained on a small data (e.g. AfroLM pretrained</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">on less than 0.8GB despite seeing 23 African lan-</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>guages during pre-training). Larger models such as LABSE and RemBERT that cover more languages performed better than the smaller models, for example, LABSE achieved over of 2.5 F1 points over AfriBERTa.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Few-shot learning on . We compare several few-shot learning approaches: PET, SetFit and Cohere Embedding API.</figDesc><table><row><cell>86.2 94.1 90.2</cell><cell>87.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Languages covered by different multilingual Models and their sizes</figDesc><table><row><cell>Model</cell><cell>size</cell><cell>amh</cell><cell>eng</cell><cell>fra hau</cell><cell>ibo</cell><cell>lin</cell><cell>lug orm pcm</cell><cell>run</cell><cell>sna som swa</cell><cell>tir</cell><cell>xho</cell><cell cols="2">yor AVG</cell></row><row><cell>Headline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MLP</cell><cell cols="12">&lt;20K 86.7 72.6 69.8 80.4 77.8 79.4 74.6 81.9 87.5 73.8 84.9 71.4 69.3 80.7 79.1 83.0</cell><cell>78.3</cell></row><row><cell>NaiveBayes</cell><cell cols="12">&lt;20K 88.8 71.6 70.0 76.6 75.8 74.0 74.6 74.2 82.6 64.3 79.5 61.7 60.6 66.0 72.5 81.4</cell><cell>73.4</cell></row><row><cell>XGBoost</cell><cell cols="12">&lt;20K 83.6 71.3 67.8 77.4 71.3 76.7 68.7 77.7 80.8 71.3 84.6 63.4 66.4 62.1 69.4 77.5</cell><cell>73.1</cell></row><row><cell>AfroXLMR-base</cell><cell>270M</cell><cell cols="11">91.8 87.0 92.0 89.2 87.8 89.0 87.4 87.4 97.4 87.8 94.5 85.9 85.0 85.7 93.5 88.6</cell><cell>89.4</cell></row><row><cell cols="2">AfroXLMR-large 550M</cell><cell cols="11">93.0 89.3 91.8 91.0 90.7 91.4 87.7 90.9 98.2 89.3 95.9 87.1 86.6 88.5 96.2 90.3</cell><cell>91.1</cell></row><row><cell>Headline+Text</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MLP</cell><cell cols="12">&lt;20K 92.0 88.2 84.6 86.7 80.1 84.3 82.2 86.7 93.5 85.9 92.6 71.1 77.9 81.9 94.5 89.3</cell><cell>85.7</cell></row><row><cell>NaiveBayes</cell><cell cols="12">&lt;20K 91.8 83.7 84.3 85.3 79.8 82.8 84.0 85.6 92.8 79.9 91.5 74.8 76.6 71.4 91.0 84.0</cell><cell>83.7</cell></row><row><cell>XGBoost</cell><cell cols="12">&lt;20K 90.1 86.0 81.2 84.7 78.6 74.8 83.8 83.2 93.3 79.2 94.3 68.5 74.9 75.2 91.1 85.2</cell><cell>82.8</cell></row><row><cell>AfroXLMR-base</cell><cell>270M</cell><cell cols="11">94.2 92.2 92.5 91.0 90.7 93.0 89.4 92.1 98.2 91.4 95.4 85.2 88.2 86.5 94.7 93.0</cell><cell>91.7</cell></row><row><cell cols="2">AfroXLMR-large 550M</cell><cell cols="11">94.4 93.1 91.1 92.2 93.4 93.7 89.9 92.1 98.8 92.7 95.4 86.9 87.7 89.5 97.3 94.0</cell><cell>92.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Baseline results on . We compare different article content types (i.e headline and headline+text) used to train news topic classification models. Average is over 5 runs. Evaluation is based on weighted F1-score.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/masakhane-io/ masakhane-news</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>all annotators are were included as authors of the paper.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>since we require 50 instances per class or 50-shots for the few-shot experiments in ( §6.2.2)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://docs.cohere.ai/docs/ text-classification-with-classify</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://openai.com/blog/chatgpt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>https://arstechnica.com/informationtechnology/2023/07/is-chatgpt-getting-worse-over-timestudy-claims-yes-but-others-arent-</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>sure/ 8 https://platform.openai.com/docs/models/gpt-3-5</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank <rs type="person">Yuxiang Wu</rs> for the suggestions on the few-shot experiments. We are grateful for the feedback from the anonymous reviewers of AfricaNLP and IJCNLP-AACL that helped improved this draft. <rs type="person">David Adelani</rs> acknowledges the support of <rs type="grantName">DeepMind Academic Fellowship programme</rs>. This work was supported in part by <rs type="funder">Oracle Cloud</rs> credits and related resources provided by <rs type="funder">Oracle</rs>. Finally, we are grateful to OpenAI for providing API credits through their Researcher Access API programme to Masakhane for the evaluation of GPT-3.5 and GPT-4 large language models.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6tQ6zzf">
					<orgName type="grant-name">DeepMind Academic Fellowship programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2022a. A few thousand translations go a long way! leveraging pre-trained models for African news translation</title>
		<author>
			<persName><forename type="first">David</forename><surname>Adelani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesujoba</forename><surname>Alabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Kreutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Machel</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Ruiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Nabende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernie</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tajuddeen</forename><surname>Gwadabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Freshia</forename><surname>Sackey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Bonaventure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dossou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Emezue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shamsuddeen</forename><surname>Beukman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guyo</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oreen</forename><surname>Jarso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Yousuf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Niyongabo Rubungo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hacheme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><forename type="middle">Umair</forename><surname>Peter Wairagala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Nasir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tunde</forename><surname>Ajibade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvonne</forename><surname>Ajayi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Gitau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Millicent</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuoluwapo</forename><surname>Ochieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perez</forename><surname>Aremu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ogayo</surname></persName>
		</author>
		<author>
			<persName><surname>Mukiibi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.223</idno>
	</analytic>
	<monogr>
		<title level="m">Fatoumata Ouoba Kabore, Godson Kalipe, Derguene Mbaye, Allahsera Auguste Tapo, Victoire Memdjokam Koagne, Edwin Munkoh-Buabeng, Valencia Wagner, Idris Abdulmumin, Ayodele Awokoya, Happy Buzaaba, Blessing Sibanda, Andiswa Bukula, and Sam Manthalu</title>
		<meeting><address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="3053" to="3070" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Ifeoluwa Adelani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D'</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantine</forename><surname>Kreutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chester</forename><surname>Lignos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Happy</forename><surname>Palen-Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Buzaaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Rijhwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mayhew ; Shamsuddeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">Chinenye</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joyce</forename><surname>Emezue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perez</forename><surname>Nakatumba-Nabende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aremu</forename><surname>Ogayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Anuoluwapo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derguene</forename><surname>Gitau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesujoba</forename><surname>Mbaye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seid</forename><surname>Alabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tajuddeen</forename><surname>Muhie Yimam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignatius</forename><surname>Rabiu Gwadabe</surname></persName>
		</author>
		<author>
			<persName><surname>Ezeani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Rubungo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Niyongabo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verrah</forename><surname>Mukiibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iroro</forename><surname>Otiende</surname></persName>
		</author>
		<author>
			<persName><surname>Orife</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00416</idno>
		<title level="m">Israel Abebe Azime</title>
		<editor>
			<persName><forename type="first">Davis</forename><surname>David</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Samba</forename><surname>Ngom</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tosin</forename><surname>Adewumi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Rayson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mofetoluwa</forename><surname>Adeyemi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gerald</forename><surname>Muriuki</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emmanuel</forename><surname>Anebi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chiamaka</forename><surname>Chukwuneke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nkiruka</forename><surname>Odu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Eric</forename><surname>Peter Wairagala</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Samuel</forename><surname>Oyerinde</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Clemencia</forename><surname>Siro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tobius</forename><surname>Saul Bateesa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Temilola</forename><surname>Oloyede</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yvonne</forename><surname>Wambui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Victor</forename><surname>Akinode</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Deborah</forename><surname>Nabagereka</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maurice</forename><surname>Katusiime</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ayodele</forename><surname>Awokoya</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mboup</forename><surname>Mouhamadane</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dibora</forename><surname>Gebreyohannes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henok</forename><surname>Tilaye</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kelechi</forename><surname>Nwaike</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Degaga</forename><surname>Wolde</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Abdoulaye</forename><surname>Faye</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Blessing</forename><surname>Sibanda</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Orevaoghene</forename><surname>Ahia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Bonaventure</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kelechi</forename><surname>Dossou</surname></persName>
		</editor>
		<editor>
			<persName><surname>Ogueji</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1116" to="1131" />
		</imprint>
		<respStmt>
			<orgName>Thierno Ibrahima DIOP, Abdoulaye Diallo, Adewale Akinfaderin, Tendai Marengereke</orgName>
		</respStmt>
	</monogr>
	<note>and Salomey Osei. 2021. MasakhaNER: Named entity recognition for African languages. Transactions of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Africa-centric Transfer Learning for Named Entity Recognition</title>
		<author>
			<persName><forename type="first">David</forename><surname>Ifeoluwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adelani</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Rijhwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Beukman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chester</forename><surname>Palen-Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantine</forename><surname>Lignos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesujoba</forename><forename type="middle">O</forename><surname>Alabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shamsuddeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><surname>Nabende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andiswa</forename><surname>Bamba Dione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rooweither</forename><surname>Bukula</surname></persName>
		</author>
		<author>
			<persName><surname>Mabuya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Bonaventure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blessing</forename><surname>Dossou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Happy</forename><surname>Sibanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Buzaaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Godson</forename><surname>Mukiibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derguene</forename><surname>Kalipe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amelia</forename><surname>Mbaye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatoumata</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Kabore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuoluwapo</forename><surname>Chinenye Emezue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perez</forename><surname>Aremu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Ogayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edwin</forename><surname>Gitau</surname></persName>
		</author>
		<author>
			<persName><surname>Munkoh-Buabeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Victoire</surname></persName>
		</author>
		<author>
			<persName><surname>Koagne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Auguste</forename><surname>Allahsera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tebogo</forename><surname>Tapo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vukosi</forename><surname>Macucwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elvis</forename><surname>Marivate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tajuddeen</forename><surname>Mboning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tosin</forename><surname>Gwadabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orevaoghene</forename><surname>Adewumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joyce</forename><surname>Ahia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neo</forename><forename type="middle">L</forename><surname>Nakatumba-Nabende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignatius</forename><surname>Mokono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiamaka</forename><surname>Ezeani</surname></persName>
		</author>
		<author>
			<persName><surname>Chukwuneke</surname></persName>
		</author>
		<idno>MasakhaNER 2.0</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Mofetoluwa</forename><surname>Adeyemi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gilles</forename><forename type="middle">Q</forename><surname>Hacheme</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Idris</forename><surname>Abdulmumin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Odunayo</forename><surname>Ogundepo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Oreen</forename><surname>Yousuf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tatiana</forename><forename type="middle">Moteu</forename><surname>Ngoli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</editor>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4488" to="4508" />
		</imprint>
	</monogr>
	<note>United Arab Emirates. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adapting pretrained language models to African languages via multilingual adaptive fine-tuning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Jesujoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Alabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Ifeoluwa Adelani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietrich</forename><surname>Mosbach</surname></persName>
		</author>
		<author>
			<persName><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics</title>
		<meeting>the 29th International Conference on Computational Linguistics<address><addrLine>Gyeongju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4336" to="4349" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Vārta: A largescale headline-generation dataset for indic languages</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Aralikatte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziling</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Doddapaneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackie</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kit</forename><surname>Cheung</surname></persName>
		</author>
		<idno>ArXiv, abs/2305.05858</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An amharic news text classification dataset</title>
		<author>
			<persName><forename type="first">Israel</forename><surname>Abebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azime</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Nebil</forename><surname>Mohammed</surname></persName>
		</author>
		<idno>CoRR, abs/2103.05639</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<editor>
			<persName><forename type="first">Thibault</forename><surname>Fevry</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Henry</forename><surname>Tsai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc. Hyung Won Chung</publisher>
			<date type="published" when="2020">2020. 2021</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2210.11416</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Scaling instruction-finetuned language models</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ELECTRA: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Topic modeling in embedding spaces</title>
		<author>
			<persName><forename type="first">B</forename><surname>Adji</surname></persName>
		</author>
		<author>
			<persName><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><surname>Blei</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00325</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="439" to="453" />
		</imprint>
	</monogr>
	<note>Transactions of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Afrolm: A self-active learning-based multilingual pretrained language model for 23 african languages</title>
		<author>
			<persName><forename type="first">Atnafu</forename><surname>Bonaventure Fp Dossou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oreen</forename><surname>Lambebo Tonja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salomey</forename><surname>Yousuf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abigail</forename><surname>Osei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iyanuoluwa</forename><surname>Oppong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oluwabusayo</forename><surname>Shode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">Chinenye</forename><surname>Olufunke Awoyomi</surname></persName>
		</author>
		<author>
			<persName><surname>Emezue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.03263</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Eberhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">F</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">D</forename><surname>Fennig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Ethnologue: Languages of the world. twenty-third edition</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Language-agnostic BERT sentence embedding</title>
		<author>
			<persName><forename type="first">Fangxiaoyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.62</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="878" to="891" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Text classification based on convolutional neural networks and word embedding for low-resource languages: Tigrinya</title>
		<author>
			<persName><forename type="first">Awet</forename><surname>Fesseha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengwu</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.3390/info12020052</idno>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Eshete Derb Emiru, Moussa Diallo, and Abdelghani Dahou</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Measuring nominal scale agreement among many raters</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Fleiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="378" to="382" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.295</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3816" to="3830" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Practical solutions to the problem of diagonal dominance in kernel document clustering</title>
		<author>
			<persName><forename type="first">Derek</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pádraig</forename><surname>Cunningham</surname></persName>
		</author>
		<idno type="DOI">10.1145/1143844.1143892</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning, ICML &apos;06</title>
		<meeting>the 23rd International Conference on Machine Learning, ICML &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">2021a. Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno>ArXiv, abs/2111.09543</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">2021b. Deberta: Decoding-enhanced bert with disentangled attention</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transfer learning and distant supervision for multilingual transformer models: A study on African languages</title>
		<author>
			<persName><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hedderich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Adelani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesujoba</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udia</forename><surname>Alabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietrich</forename><surname>Markus</surname></persName>
		</author>
		<author>
			<persName><surname>Klakow</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.204</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2580" to="2591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for nlp</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Short Papers; Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">AfriTeVA: Extending ?small data? pretraining approaches to sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">Jude</forename><surname>Odunayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akintunde</forename><surname>Ogundepo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mofetoluwa</forename><surname>Oladipo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelechi</forename><surname>Adeyemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ogueji</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.deeplo-1.14</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Deep Learning for Low-Resource Natural Language Processing</title>
		<meeting>the Third Workshop on Deep Learning for Low-Resource Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="126" to="135" />
		</imprint>
	</monogr>
	<note>Hybrid. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Kunchukuttan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Divyanshu</forename><surname>Kakwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satish</forename><surname>Golla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Gokul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avik</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyush</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00085</idno>
		<title level="m">Ai4bharatindicnlp corpus: Monolingual corpora and word embeddings for indic languages</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers</title>
		<author>
			<persName><forename type="first">Anne</forename><surname>Lauscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinit</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Glavaš</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.363</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4483" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Few-shot learning with multilingual generative language models</title>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakanth</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian O'</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Horo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9019" to="9052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.1145/3560815</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Crosslingual generalization through multitask finetuning</title>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng Xin</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.acl-long.891</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alham</forename><surname>Fikri Aji</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Khalid</forename><surname>Almubarak</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Edward</forename><surname>Raff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</editor>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="15991" to="16111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Crosslingual generalization through multitask finetuning</title>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<editor>Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Shamsuddeen</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Idris</forename><surname>Abdulmumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abinew</forename><surname>Ali Ayele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nedjma</forename><surname>Ousidhoum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ifeoluwa Adelani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seid</forename><surname>Muhie Yimam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Sa'id Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meriem</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oumaima</forename><surname>Hourrane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Brazdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dário</forename><surname>Felermino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">António</forename><surname>Mário</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davis</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salomey</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><surname>Osei</surname></persName>
		</author>
		<editor>Bello Shehu Bello, Falalu Ibrahim, Tajuddeen Gwadabe, Samuel Rutunda, Tadesse Belay, Wendimu Baye Messelle, Hailu Beshada Balcha, Sisay Adugna Chala, Hagos Tesfahun Gebremichael, Bernard Opoku, and Steven Arthur</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Afrisenti: A twitter sentiment analysis benchmark for african languages</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">KINNEWS and KIRNEWS: Benchmarking cross-lingual text classification for Kinyarwanda and Kirundi</title>
		<author>
			<persName><forename type="first">Andre</forename><surname>Rubungo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qu</forename><surname>Niyongabo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Kreutzer</surname></persName>
		</author>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.480</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5507" to="5521" />
		</imprint>
	</monogr>
	<note>Online. International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">No language left behind: Scaling human-centered machine translation</title>
		<author>
			<persName><forename type="first">Marta</forename><surname>Nllb-Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Ruiz Costa-Jussà</surname></persName>
		</author>
		<author>
			<persName><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maha</forename><surname>Onur Ccelebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Elbayad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elahe</forename><surname>Heffernan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janice</forename><surname>Kalbassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Licht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skyler</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alison</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bapi</forename><surname>Youngblood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Akula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><forename type="middle">Mejia</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prangthip</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hansanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semarley</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ram</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Sadagopan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shannon</forename><forename type="middle">L</forename><surname>Rowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spruit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Necip</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Fazil Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedanuj</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Guzm'an</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Mourachko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Safiyyah</forename><surname>Ropers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Saleem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno>ArXiv, abs/2207.04672</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Small data? no problem! exploring the viability of pretrained multilingual language models for lowresourced languages</title>
		<author>
			<persName><forename type="first">Kelechi</forename><surname>Ogueji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.mrl-1.11</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Multilingual Representation Learning</title>
		<meeting>the 1st Workshop on Multilingual Representation Learning<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="116" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fraser</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><forename type="middle">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maddie</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Francis Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">J</forename><surname>Lowe</surname></persName>
		</author>
		<idno>ArXiv, abs/2203.02155</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.617</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7654" to="7673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">UNKs everywhere: Adapting multilingual language models to new scripts</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.800</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10186" to="10203" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urmish</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanya</forename><surname>Sharma Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Szczechla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taewoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nihal</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debajyoti</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Tian-Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng Xin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<editor>
			<persName><forename type="first">Jos</forename><surname>Neeraj</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Abheesht</forename><surname>Rozen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrea</forename><surname>Sharma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thibault</forename><surname>Santilli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jason</forename><surname>Fevry</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alan</forename><surname>Fries</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ryan</forename><surname>Teehan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</editor>
		<editor>
			<persName><surname>Scao</surname></persName>
		</editor>
		<meeting><address><addrLine>Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth-Jane</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzana</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ili'c</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Castagn'e</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franccois</forename><surname>Sasha Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Gallé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">Rose</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawan</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Sasanka Ammanamanchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Villanova Del Moral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stas</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Bekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huu</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samson</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Ortiz</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Laurenccon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Launay</surname></persName>
		</author>
		<author>
			<persName><surname>Mitchell</surname></persName>
		</author>
		<idno>ArXiv, abs/2211.05100</idno>
	</analytic>
	<monogr>
		<title level="j">Dragomir R. Radev, Eduardo G. Ponferrada</title>
		<editor>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aaron</forename><surname>Gokaslan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Adi</forename><surname>Simhi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aitor</forename><surname>Soroa Etxabe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alham</forename><surname>Fikri Aji</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Amit</forename><surname>Alfassy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ariel</forename><forename type="middle">Kreisberg</forename><surname>Nitzav</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chenghao</forename><surname>Mou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chris</forename><forename type="middle">C</forename><surname>Emezue</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christopher</forename><surname>Klamm</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Colin</forename><surname>Leong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniel</forename><surname>Alexander Van Strien</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Ifeoluwa Adelani</surname></persName>
		</editor>
		<imprint>
			<publisher>Efrat Levkovizh</publisher>
		</imprint>
	</monogr>
	<note>and Thomas Wolf. 2022. Bloom: A 176b-parameter open-access multilingual language model</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Exploiting cloze-questions for few-shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.20</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="255" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">2021b. It&apos;s not just size that matters: Small language models are also fewshot learners</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.185</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="2339" to="2352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.346</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4222" to="4235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Iyanuoluwa</forename><surname>Shode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ifeoluwa Adelani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Feldman</surname></persName>
		</author>
		<title level="m">yosm: A new yoruba sentiment corpus for movie reviews</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">NollySenti: Leveraging transfer learning and machine translation for Nigerian movie sentiment classification</title>
		<author>
			<persName><forename type="first">Iyanuoluwa</forename><surname>Shode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ifeoluwa Adelani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Feldman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.acl-short.85</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="986" to="998" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">How to fine-tune bert for text classification</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yige</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">China National Conference on Chinese Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Moshe Wasserblat, and Oren Pereg. 2022a. Efficient few-shot learning without prompts</title>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Unso</forename><surname>Eun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seo</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Korat</surname></persName>
		</author>
		<idno>ArXiv, abs/2209.11055</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Moshe Wasserblat, and Oren Pereg. 2022b. Efficient few-shot learning without prompts</title>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Unso</forename><surname>Eun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seo</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Korat</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2209.11055</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">N24News: A new dataset for multimodal news classification</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangxie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Language Resources and Evaluation Conference</title>
		<meeting>the Thirteenth Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="6768" to="6775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer</title>
		<author>
			<persName><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.41</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="483" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
	<note>LLM LLM size # Lang. # African Lang. Focus languages covered XLM-R-base/large 270M/550M</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">AfriBERTa-large 126M 11 11 amh, hau, ibo, orm, pcm, run, swa, tir, yor mDeBERTa 276M 110 8 amh, eng, fra, hau, orm, swa, xho RemBERT 575M 110 12 amh, eng, fra, hau, ibo, sna, swa, xho, yor AfriTeVa-base 229M 11 11 amh</title>
		<imprint/>
	</monogr>
	<note>run, hau, ibo, orm, pcm, swa, tir, yor AfroXLMR-base/large 270M/550M</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">17 amh, eng, fra, hau, ibo, orm, pcm, run, sna, swa, xho, yor AfriMT5-base 580M 20 17 amh, eng, fra, hau, ibo, orm, pcm, run, sna, swa, xho</title>
		<imprint/>
	</monogr>
	<note>yor FlanT5-base 580M 60 5 eng, fra, ibo, swa, yor</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
