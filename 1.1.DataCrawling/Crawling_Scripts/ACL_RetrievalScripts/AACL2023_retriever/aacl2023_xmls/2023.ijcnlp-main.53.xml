<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LexicoMatic: Automatic Creation of Multilingual Lexical-Semantic Dictionaries</title>
				<funder ref="#_pqJa4dM #_7ashmhm #_wx3UV94">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Federico</forename><surname>Martelli</surname></persName>
							<email>martelli@diag.uniroma1.it</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Sapienza NLP Group</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luigi</forename><surname>Procopio</surname></persName>
							<email>luigi.procopio@sunglasses.ai</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Sapienza NLP Group</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Edoardo</forename><surname>Barba</surname></persName>
							<email>barba@diag.uniroma1.it</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Sapienza NLP Group</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
							<email>navigli@diag.uniroma1.it</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Sapienza NLP Group</orgName>
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LexicoMatic: Automatic Creation of Multilingual Lexical-Semantic Dictionaries</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A0DB7BF2CCD6D940A513ABAC945E1195</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lexical-semantic resources such as wordnets and multilingual dictionaries often suffer from significant coverage issues, especially in languages other than English. While improving their coverage manually is a prohibitively expensive undertaking, current approaches to the automatic creation of such resources fail to investigate the latest advances achieved in relevant fields, such as cross-lingual annotation projection. In this work, we address these shortcomings and propose LEXICOMATIC, a novel resource-independent approach to the automatic construction and expansion of multilingual semantic dictionaries, in which we formulate the task as an annotation projection problem. In addition, we tackle the lack of a comprehensive multilingual evaluation framework and put forward a new entirely manually-curated benchmark featuring 9 languages. We evaluate LEXICOMATIC with an extensive array of experiments and demonstrate the effectiveness of our approach, achieving a new state of the art across all languages under consideration. We release our novel evaluation benchmark at: https://github.com/SapienzaNLP/lexicomatic.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Lexical-semantic resources, like wordnets and computational lexicons, play a key role in a wide range of Natural Language Understanding (NLU) tasks <ref type="bibr" target="#b29">(Navigli, 2018)</ref> such as Word Sense Disambiguation <ref type="bibr">(Bevilacqua et al., 2021, WSD)</ref>, Semantic Role Labeling <ref type="bibr">(Gildea and Jurafsky, 2000, SRL)</ref>, <ref type="bibr">Semantic Parsing (Martinez Lorenzo et al., 2022)</ref>, when investigating semantic biases in Machine Translation <ref type="bibr">(Campolungo et al., 2022, MT)</ref>, and in a broad spectrum of NLU approaches. For instance, in WSD not only do these dictionary-like resources enable an explicit representation of words and their meanings, leveraged in many knowledge-based approaches <ref type="bibr" target="#b0">(Agirre et al., 2014;</ref><ref type="bibr" target="#b28">Moro et al., 2014;</ref><ref type="bibr" target="#b39">Scozzafava et al., 2020)</ref>, but they have also proven to be highly beneficial when integrated into neural systems <ref type="bibr" target="#b4">(Bevilacqua and Navigli, 2020;</ref><ref type="bibr" target="#b15">El Sheikh et al., 2021;</ref><ref type="bibr" target="#b11">Conia and Navigli, 2021)</ref>. For the purposes of this work, we refer to such resources as multilingual semantic dictionaries, and differentiate between these and the bilingual dictionaries typically used in MT <ref type="bibr" target="#b20">(Klementiev et al., 2012;</ref><ref type="bibr" target="#b18">Irvine and Callison-Burch, 2014)</ref>. In fact, bilingual dictionaries contain a list of possible translations in a target language for each word in a source language, with no distinction between the different meanings conveyed by such words.</p><p>Notably, a crucial limitation affecting multilingual semantic dictionaries is their insufficient coverage, especially when scaling to multiple languages or when considering mid-and low-resource ones. While, on the one hand, creating such resources manually is a very expensive endeavour, on the other hand, current automatic approaches have failed to investigate the benefits derived from recent breakthroughs achieved in relevant fields, such as cross-lingual label propagation <ref type="bibr" target="#b34">(Procopio et al., 2021)</ref>, WSD <ref type="bibr">(Barba et al., 2021b)</ref> and word alignment.</p><p>In this paper we address these shortcomings and propose LEXICOMATIC, a novel approach to the automatic construction of multilingual semantic dictionaries. Starting from a monolingual semantic dictionary D in language L 0 and a set of target languages {L 1 , . . . , L n }, we first build a synthetic L 0centric parallel corpus and, then, leveraging WSD and word alignment, generate, for every sense in D, its corresponding lexicalizations in each target language. We use the wordnet creation task <ref type="bibr" target="#b31">(Neale, 2018)</ref>, i.e. the computational task of automatically constructing a wordnet, either from scratch, or by leveraging an already existing one, as our main test case. Since, to the best of our knowledge, no comprehensive multilingual evaluation suite is currently available, we propose a novel manually-curated framework comprising 9 languages, including mid-and low-resource ones. For each of these, we find that our approach achieves significantly better performances than its state-of-the-art alternatives in terms of both F 1 score, F 0.5 score and WordNet core coverage.<ref type="foot" target="#foot_0">1</ref> Furthermore, since LEXICOMATIC also generates silver WSD datasets for each language considered, we investigate their quality in our experiments.</p><p>Our contributions are therefore as follows:</p><p>1. We propose a novel approach to the automatic construction of multilingual semantic dictionaries.</p><p>2. We put forward a new manually-curated multilingual evaluation suite for the wordnet creation task, covering 9 languages, ranging from mid-to low-resources ones.</p><p>3. We evaluate our approach extensively and carry out a performance analysis. Furthermore, we demonstrate the scalability of LEXICOMATIC when adopting a sense inventory different from Princeton WordNet<ref type="foot" target="#foot_1">2</ref>  <ref type="bibr">(Miller, 1995, PWN)</ref> (see Appendix A).</p><p>We release our novel evaluation benchmark at: https://github.com/SapienzaNLP/lexicomatic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Among lexical-semantic resources, wordnets are arguably the most popular and widely used. Their automatic creation has been addressed by several approaches put forward during the course of recent decades <ref type="bibr" target="#b31">(Neale, 2018)</ref>. Depending on the strategy adopted, such approaches have been divided into two main paradigms <ref type="bibr" target="#b42">(Vossen, 1998)</ref> While the merge approaches are able to overcome several linguistic issues, the expand or extend approaches have become the de facto standard in literature, thanks to their speed gain and ease of connection with PWN.</p><p>Broadly speaking, expand approaches present a common structure consisting of two steps: i) candidate retrieval, where a list of candidate words in the target language to be assigned to a given PWN synset is produced; ii) candidate selection, where a scoring function is used to discard or assign candidate words to a given PWN synset. For example, <ref type="bibr" target="#b22">Lee et al. (2000)</ref> propose the construction of a Korean wordnet by linking words in Korean derived from a bilingual machine-readable dictionary to PWN. Semantic ambiguities are then removed by combining 6 different heuristics with decisiontree learning. Instead, <ref type="bibr" target="#b27">Montazery and Faili (2010)</ref> rely on different word similarity scores, such as the mutual information and other measures based on WordNet, to link words in Persian to PWN synsets. Along these lines, <ref type="bibr" target="#b21">Lam et al. (2014)</ref> leverage publicly available wordnets, machine translation and bilingual dictionaries to translate synsets derived from existing wordnets into different languages, including endangered languages, such as Dimasa and Karbi. Finally, a scoring method is used to identify the best translations. Instead, <ref type="bibr" target="#b40">Taghizadeh and Faili (2016)</ref> propose the automatic construction of a Persian WordNet, by leveraging a bilingual dictionary and a monolingual corpus as resources and unsupervised WSD to disambiguate candidate words.</p><p>With the advent of word embeddings, several approaches started using these new representations. For instance, Al Tarouti and Kalita (2016) use word2vec <ref type="bibr" target="#b24">(Mikolov et al., 2013)</ref> to improve the candidate selection step. Instead, <ref type="bibr" target="#b19">Khodak et al. (2017)</ref> compute word and synset representations to score the association between a candidate word and a given synset, discarding (word, synset) pairs below a certain threshold.</p><p>More closely related to our work, some approaches rely on WSD, parallel corpora and word alignment systems to retrieve, for each synset, a list of words in the target language. For instance, <ref type="bibr" target="#b37">Sagot and Fišer (2008)</ref> use five different parallel corpora and disambiguate each aligned word by intersection of its possible senses in its corresponding inventories. Finally, closest to our work, Oliver (2014) leverages a WSD system on the English side of a parallel corpus and retrieves all the possible translations in the target language by using a word alignment system. More recent research works, however, did not investigate this promising direction further. To fill this gap, in this work we aim to demonstrate that recent neural breakthroughs in WSD <ref type="bibr">(Barba et al., 2021a)</ref> and cross-lingual label projection via word alignment <ref type="bibr" target="#b34">(Procopio et al., 2021)</ref> allow us to achieve a new state of the art in the wordnet creation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LexicoMatic</head><p>In this section, we introduce LEXICOMATIC, our approach to the automatic construction and expansion of multilingual semantic dictionaries. First, we outline our overall process (Section 3.1). Then, we focus on the core modules of our approach, describing the systems we adopt for WSD (Section 3.2) and word alignment (Section 3.3). Finally, we detail our data aggregation strategy (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formulation</head><p>Starting from a monolingual dictionary in language L, we frame the automatic expansion of this dictionary towards a set of target languages as a 3-stage process over L-centric parallel corpora. <ref type="foot" target="#foot_2">3</ref> Formally, let D be a dictionary in language L, comprising a list of lexemes l 1 , . . . , l m , with each</p><formula xml:id="formula_0">l i ∀i ∈ [1, m] associated with a collection of c(l i ) textual defini- tions δ(l i ) = {d l i 1 , . . . , d l i c(l i )</formula><p>} expressing its possible meanings; both l 1 , . . . , l m and their definitions are in language L. Then, given a list of target languages L1 , . . . , Lk , we formulate our objective as follows: for each target language L, we wish to yield the lexemes in L corresponding to every pair</p><formula xml:id="formula_1">(l i , d l i j ) ∀i ∈ [1, m] ∀j ∈ [1, c(l i )].</formula><p>To achieve this objective, we put forward the following approach: denote by s 1 , . . . , s n a list of sentences in language L, and by t 1 , . . . , t n its parallel counterparts in a target language L. Then, for each (l i , d l i j ), we generate the corresponding lexemes in L as follows: we first disambiguate the words in s 1 , . . . , s n against D, that is, we pair each word with its most suitable meaning in D. Subsequently, we perform word alignment on each pair of parallel sentences (s j , t j ) ∀j ∈ [1, n]. These two steps essentially yield a corpus where words in language L are paired with their meanings in D and linked to their counterparts in language L. With this data at our disposal, we conclude our process by employing an aggregation strategy over the processed parallel sentences, producing, for each (l i , d l i j ), a list of corresponding lexicalizations in language L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Word Sense Disambiguation</head><p>The first step in our approach aims at disambiguating the words in the sentences s 1 , . . . , s n against D. That is, ∀j ∈ [1, n], we need to link each word form in s j , whose lexeme we denote by l, to the definition in δ( l) that best expresses its meaning. Note that these word forms might be single tokens (e.g., nouns) or multi-words (e.g., phrasal verbs); henceforth, we will use σ j,1 , . . . , σ j,|σ j | to denote the |σ j | word forms occurring in sentence s j .</p><p>To perform this task, previous WSD approaches <ref type="bibr" target="#b4">(Bevilacqua and Navigli, 2020;</ref><ref type="bibr" target="#b11">Conia and Navigli, 2021)</ref> adopt supervised classifiers that require training corpora sense-tagged against D. However, often such corpora labeled with D are not available across languages and, since manually producing them is prohibitively expensive, using these approaches would restrict the applicability of our process to a limited number of dictionaries. Thus, in order to drop this requirement, we focus here on the recent trend in literature of models tackling WSD via definition-selection formulations <ref type="bibr" target="#b17">(Huang et al., 2019;</ref><ref type="bibr" target="#b6">Blevins and Zettlemoyer, 2020;</ref><ref type="bibr">Barba et al., 2021a)</ref>: given a word in context, models dynamically receive a pool of textual definitions and are trained to select the most suitable one from among these. This framing allows us to perform disambiguation against D even in cases where D has either scarce or no sense-tagged data. Indeed, in these scenarios, we can train on already existing WSD corpora, annotated with some D ′ ̸ = D, and, then, zero-shot over D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Word Alignment</head><p>As our second step, we perform word alignment between each pair of parallel sentences (s j , t j ) ∀j ∈ [1, n]. To this end, we build on top of the Transformer-based discriminative model for word alignment introduced by <ref type="bibr" target="#b34">Procopio et al. (2021)</ref> and employ a framework consisting of 2 steps inspired by the Expectation-Maximization algorithm <ref type="bibr" target="#b14">(Dempster et al., 1977)</ref>.</p><p>Given a list of parallel sentences (s j , t j ) ∀j ∈ [1, n], first we use the discriminative model to perform word alignment, pairing the tokens s j,1 , . . . , s j,|s j | in s j to their counterparts t j,1 , . . . , t j,|t j | in t j . Subsequently, since the disambiguation is performed over word forms that might span over multiple tokens, we bring together the alignments produced at span level. Note that, however, since the discriminative model may emit non-contiguous alignments (e.g., s j,1 aligned to t j,1 and t j,4 ), ∀k ∈ [1, |σ j |], this operation involves choosing the most suitable option for σ j,k from among the multiple aligned spans τ j,1 , . . . , τ j,|τ j | identified over t j .</p><p>To address this issue, we adopt a strategy consisting of 2 passes. We first employ a simple heuristic, processing σ j,1 , . . . , σ j,|σ j | by increasing number of spans aligned over t j and selecting each time the first target span that does not overlap with any of those previously chosen. Once this pass is completed, we leverage the aligned parallel sentences to compute a score for each proposed possible alignment:</p><formula xml:id="formula_2">f (σ, τ ) = # sentences with (σ, τ ) aligned # sentences with (σ, τ )</formula><p>where σ and τ are two possible word forms in languages L and L, respectively. We now perform a second pass over the parallel sentences, using the computed score in the selection process: ∀k ∈ [1, |σ j |], we select the target span aligned to σ j,k with the highest score, enforcing a minimum of α to reduce potential noise. We use α = 0.5 in all our experiments since it expresses the condition where both the prior probability and the conditional probability given by the classifier<ref type="foot" target="#foot_3">4</ref> line up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Aggregation Strategy</head><p>Once completed, the disambiguation and word alignment steps of our process produce a corpus where each source span</p><formula xml:id="formula_3">σ j,k ∀j ∈ [1, n] ∀k ∈ [1, |σ j |]</formula><p>is paired with its most suitable definition, and either associated with its counterpart span</p><formula xml:id="formula_4">τ j,k ′ , with k ′ ∈ [1, |τ j |],</formula><p>occurring in sentence t j or marked as unaligned. <ref type="foot" target="#foot_4">5</ref>Therefore, to conclude our process, we first group the collection of successfully aligned target spans by the lexeme-definition pair assigned to their source counterparts; this process results in a set of translation candidates T(l i , d l i j ) = {τ 1 , . . . , τ p } for each lexemedefinition pair (l i , d l i j ). Then, we sort these candidates and produce T(l i , d</p><formula xml:id="formula_5">l i j ) = [τ 1 , . . . , τp ] s.t. g(l i , d l i j , τz ) &gt; g(l i , d l i j , τz+1 ) ∀z ∈ [1, p],</formula><p>where g(l i , d l i j , τz ) represents the number of times τz was aligned in the bitext to l i with the meaning expressed by d l i j . Finally, to reduce the amount of noise introduced by spurious alignments and wrong disambiguation, we apply a simple method to remove the translation candidates that are most likely to be wrong: for each lexeme-definition pair (l i , d l i j ), we apply an L1-normalization on the vector v = g(l i , d l i j , τ1 ), . . . , g(l i , d l i j , τp ) and select the first h candidates such that their normalized scores sum up to a hyperparameter β. Besides filtering out potential noise, this hyperparameter also allows us to bias LEXICOMATIC towards a more precision-or recall-oriented behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Wordnet Construction</head><p>We now assess the effectiveness of LEXICOMATIC, using the wordnet creation task as our test case. We first present a novel evaluation suite comprising 9 languages that we propose for this task. Subsequently, we describe the experimental setup which we use in this setting and, finally, evaluate LEXICOMATIC with current state-of-the-art alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Suite</head><p>To the best of our knowledge, no comprehensive multilingual evaluation suite is currently available for the wordnet creation task. Therefore, we address this limitation and propose a novel framework partially inspired by ML50 <ref type="bibr" target="#b41">(Tang et al., 2020)</ref> and spanning over 9 languages, namely, Arabic, Chinese, French, German, Italian, Korean, Russian, Spanish and Swedish. Since parallel corpora are the main requirement on target languages for LEXICOMATIC, <ref type="foot" target="#foot_5">6</ref> we follow the same classification adopted by <ref type="bibr" target="#b41">Tang et al. (2020)</ref> and divide these languages into three groups depending on the availability of such resources: low-resource (Swedish), mid-resource (Arabic, Italian and Korean) and highresource (Chinese, French, German, Russian and Spanish). <ref type="foot" target="#foot_6">7</ref>For each of these languages, we manually create a test set as follows. First, we lemmatize and label with the part of speech (POS) the corresponding Wikipedia corpus<ref type="foot" target="#foot_7">8</ref> using Stanza<ref type="foot" target="#foot_8">9</ref>  <ref type="bibr" target="#b35">(Qi et al., 2020)</ref> and compute the absolute frequency ϕ of each lexeme, that is, each (lemma, POS) pair. <ref type="foot" target="#foot_9">10</ref> Then, we discard lexemes with ϕ ≤ 1000, to reduce potential noise, and divide those remaining into three frequency classes, depending on their ϕ value: specifically, denoting by ϕ 25th and ϕ 50th the 25-th and 50-th percentiles, the three classes are comprised of samples such that ϕ ≥ ϕ 50th , ϕ 25th ≤ ϕ &lt; ϕ 50th and ϕ &lt; ϕ 25th , respectively. For each class, we manually validate all extracted lexemes, discarding spurious ones,<ref type="foot" target="#foot_10">11</ref> and, then, randomly sample 200 elements. Finally, for each of these, we retrieve the corresponding synsets from BabelNet <ref type="bibr" target="#b30">(Navigli and Ponzetto, 2012)</ref>, a large multilingual semantic network built by combining a number of heterogeneous resources including PWN, and ask professional linguists to manually validate each (lexeme, synset) pair. These final pairs over the three classes constitute the test set for the language under consideration. Table <ref type="table" target="#tab_1">1</ref> reports coverage statistics of our test sets on each language, both per POS class and aggregated. Further information regarding the annotation process and guidelines can be found in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Annotation Process and Guidelines</head><p>The manual creation of our comprehensive multilingual evaluation suite is carried out by six professional linguists or translators. We require each professional annotator to work in a language in which they have a C2 level of proficiency according to the Common European Framework of Reference for Languages, as well as proven experience in the creation and expansion of lexical-semantic resources. All annotators are paid at an agreed hourly rate which is higher than the legal minimum pay per hour in their country of residence, if available.</p><p>In order to ensure data consistency across languages, we devise and adopt specific annotation guidelines. In this way, shared linguistic criteria are adopted to perform the manual annotation and validation. For instance, we use hypernyms to de- termine whether a (lemma, POS) pair should be associated with a given synset, i.e. lemmas pertaining to a given synset should share the same hypernym according to reputable lexicographic resources such as WordNet for the English language. Importantly, during the annotation process, we encounter some language-specific peculiarities and exceptions, e.g. verb aspects in Russian or compounds (Komposita) in German. Such cases are discussed and subsequently addressed in joint annotation sessions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">LexicoMatic Setup</head><p>Word Sense Disambiguation As our disambiguation system, we use ESCHER <ref type="bibr">(Barba et al., 2021a)</ref>, a Transformer-based architecture that frames WSD as a text extraction problem. Specifically, since the dictionary under consideration is PWN, we employ the model released by the authors<ref type="foot" target="#foot_11">12</ref> that is trained on SemCor <ref type="bibr" target="#b26">(Miller et al., 1993)</ref>, a large manually-annotated English dataset featuring 33 362 sentences and 226 036 tagged instances. Our choice of this system is motivated by the strong performance which ESCHER attains both when evaluated on the same sense inventory used at training time and in zero-shot scenarios.</p><p>Word Alignment To train the word alignment model, we use the manually-annotated datasets made available by <ref type="bibr" target="#b34">Procopio et al. (2021)</ref> covering English and one or other of the following languages: French, German, Spanish and Italian. Instead, as far as the remaining languages are concerned, we leverage proprietary in-house datasets, which are created with the same method as that adopted for the aforementioned datasets: approximately 300 sentences are collected from WikiMatrix <ref type="bibr" target="#b38">(Schwenk et al., 2019)</ref> and professional linguists are asked to manually annotate them.</p><p>Parallel Corpora LEXICOMATIC relies on parallel corpora to transfer sense annotations and gather different lexicalizations for each synset. However, the quality and the coverage of the resources created is directly proportional to the heterogeneity and number of source sentences considered. This is especially troublesome for lowand mid-resource languages, where the amount of gold parallel data is relatively low (&lt;1M sentences). Therefore, to cope with this issue, we here adopt synthetic parallel sentences, that is, sentences translated via MT systems. Specifically, we use the neural machine translation system presented by <ref type="bibr">Tang et al. (2020, mBART50)</ref>. This strategy allows us to generate an arbitrarily large amount of parallel sentences for each language in our evaluation suite. To ensure a good coverage, we use a sample of 1M randomly selected sentences from English Wikipedia as the source corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison Systems</head><p>As our baseline, we consider a simple approach (PWN + MT) where we use mBART50 to translate the lexicalizations of each synset in PWN. For each language, in order to give more context to the MT systems, we postpend the synset definition to the comma-separated list of its lexicalizations. For example, to get the lexicalizations for the synset {fire, flame, flaming} with definition the process of combustion of inflammable materials producing heat and light, we input to the model the sequence fire, flame, flaming: the process of combustion of inflammable materials producing heat and light, and extract, from the translated sentence, the lexicalizations in the target language. As comparison systems, we consider Universal Wordnet <ref type="bibr">(De Melo and Weikum, 2009, UWN)</ref>, an expand approach built upon bilingual dictionaries and an ensemble of statistical heuristics, and Extended Open Multilingual Wordnet <ref type="bibr">(Bond and Foster, 2013, EOWN)</ref> which merged Open Multilingual Wordnet <ref type="bibr" target="#b9">(Bond and Paik, 2012)</ref> with data collected automatically from Wiktionary; 13 both these works cover all languages in our evaluation suite. Furthermore, we consider <ref type="bibr">Sagot and Fišer (2008, WOLF)</ref> for French and the system recently proposed by <ref type="bibr">Khodak et al. (2017, AWCWE)</ref> for French and Russian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results</head><p>Table <ref type="table" target="#tab_3">2</ref> shows the performances achieved by the systems under consideration over our evaluation suite. In particular, besides precision, recall and F 1 13 https://www.wiktionary.org/ score, we further report as in <ref type="bibr" target="#b19">Khodak et al. (2017)</ref> the F .5 score, a variant of F 1 score that is more biased towards precision, and the coverage statistic, that is, the percentage of synsets in core WordNet<ref type="foot" target="#foot_12">14</ref> that are present in the resource under evaluation. As a first result, we note the significant performances that the MT baseline attains, particularly in terms of recall and coverage. This is especially interesting since the only discerning signals the models receive as regards the desired meaning of a given term are the definition and the other English lexicalizations of the corresponding synset.</p><p>Moving to our actual system, we consider here how LEXICOMATIC fares for different β values, namely [0.7, 0.9, 1.0]. Indeed, differently from the other systems that are skewed towards precision by design since it is more useful in practical scenarios <ref type="bibr" target="#b19">(Khodak et al., 2017)</ref>, our approach enables us to select the desired trade-off, which might depend upon the use case under consideration, by adjusting β: lower values result in more conservative selection strategies that favour precision over recall, whereas higher ones produce the opposite behavior. We can see this trend in Table <ref type="table" target="#tab_3">2</ref>, where moving from β = 0.7 to β = 1.0 causes precision to decrease and recall to increase.</p><p>Finally, compared to its competitors, LEXICO-MATIC surpasses all its alternatives considered here across the board in terms of F 1 score and F .5 score. Interestingly, even with β = 0.7, LEXICOMATIC is still more oriented towards recall than the majority of its alternatives. These findings suggest that LEXICOMATIC is indeed an effective option for the automatic creation of wordnets. As a matter of interest, we investigate the scalability of LEXICOMATIC when adopting a sense inventory different from PWN and report the results in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Multilingual WSD</head><p>As a by-product of our first two steps, our process results in the automatic creation of sense-tagged corpora in languages L1 , . . . , Lk that can be used to train WSD systems. This training operation, besides naturally yielding models for each language we cover, also acts as an interesting evaluation proxy for the created wordnets. Indeed, the results attained on multilingual WSD benchmarks provide hints as to the quality of the automatically-  disambiguated sentences t 1 , . . . , t n produced for each language. Therefore, in this section, we examine the performance on multilingual WSD that a reference architecture achieves once trained on the silver corpora that our process generated for the experiments detailed in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>As the evaluation framework, we consider XL-WSD, a benchmark recently proposed by <ref type="bibr" target="#b33">Pasini et al. (2021)</ref> that includes a set of languagespecific development and test sets in different languages. In particular, we now focus on German (DE), Spanish (ES), French (FR), Italian (IT), Korean (KO) and Chinese (ZH) <ref type="foot" target="#foot_13">15</ref> and use the corresponding resources to assess the performances of LEXICOMATIC on multilingual WSD. In what follows, we illustrate the architecture of our reference model and provide an analysis of the silver training corpora under consideration.</p><p>Model Architecture To be comparable with the resources evaluated in <ref type="bibr" target="#b33">Pasini et al. (2021)</ref>, we use the same architecture as the WSD classifier, that is, a Transformer-based encoder, namely XLMR-Large <ref type="bibr" target="#b12">(Conneau et al., 2020)</ref>, followed by a 2-layer feedforward network with swish activation function, batch-normalization and a softmax layer on top. We represent each subword in the input sentence as the sum of the last 4 layers of the Transformer encoder and each word as the average of the vectors corresponding to the subwords it was split into. Finally, the model is trained to assign each instance to its corresponding synset in PWN.</p><p>Training Data For each language in our evaluation suite, we use as the training corpus its automatically-disambiguated sentences t 1 , .  <ref type="bibr" target="#b33">Pasini et al. (2021)</ref>. We highlight the best system in bold.</p><p>We show in Table <ref type="table" target="#tab_4">3</ref> the number of lexemes, instances and distinct synsets for both our datasets and the silver training resources included in <ref type="bibr" target="#b33">Pasini et al. (2021)</ref> for the four European languages. 16  To counter possible excessive skewness towards the most common sense that might occur for some word sense distributions, we limit the number of occurrences for each sense and randomly select up to 10 000 instances when preprocessing this data. Interestingly, besides the difference on the number of instances caused by the bigger collection of sentences we consider, our corpora cover an amount of synsets that is either on par with their reference counterparts (Spanish and Italian) or significantly higher (German and French). Finally, note that, as our purpose here is to further assess the quality of our process, we do not perform any kind of inventory filtering, that is, we do not employ the mapping from word to its possible synsets included in XL-WSD in order to avoid the inclusion of incorrect instances that our process might have generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>We report in Table <ref type="table" target="#tab_5">4</ref> the results attained in terms of F 1 score by LEXICOMATIC over the languages considered. For comparison, we consider three systems reported in <ref type="bibr" target="#b33">Pasini et al. (2021)</ref>, namely i) MCS, where words are always disambiguated to their most common sense, ii) ∅-shot, where the reference architecture is trained on English sensetagged resources and tasked to zero-shot over the test languages, and iii) T-SC+WNG, where the training is performed, instead, over the silver resources released in the reference paper, i.e., an automatically-translated version of SemCor and the Princeton WordNet Gloss Corpus. Furthermore, we also include ConSeC <ref type="bibr">(Barba et al., 2021b)</ref>, a recent extractive approach to WSD that is trained 16 No silver resource was released for Korean and Chinese.</p><p>on the silver resources released in XL-WSD and that represents the current state of the art in this benchmark.</p><p>As Table <ref type="table" target="#tab_5">4</ref> highlights, training upon the corpora which LEXICOMATIC generates results in performances that are at least on par with the alternatives reported. Specifically, we achieve a new state of the art on 3 languages, namely Spanish, French and Italian, and significantly close the gap between ∅-shot and silver resources on German. On Korean and Chinese, we attain performances inferior to plain zero-shot, but still competitive and significantly higher than the MCS baseline. Therefore, the first two steps of our process do generate highquality corpora and this finding has interesting ramifications. Indeed, on the one hand, it suggests that the aggregation strategy is similarly expected to generate high-quality lexicalizations and, on the other hand, the fact that our process produces a competitive sense-tagged corpus on each language it is applied upon, is a significant result in its own right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work, we introduce LEXICOMATIC, a novel resource-independent approach to the automatic construction and expansion of multilingual semantic dictionaries. By leveraging recent advances in WSD and word alignment, we frame this task as an annotation projection problem over parallel corpora and find this strategy to be particularly effective. Using the wordnet creation task as our main test case, we find that LEXICOMATIC surpasses its alternatives by a large margin, in terms of both F 1 score and F 0.5 score, when testing against a new evaluation suite covering 9 languages which we put forward. Crucially, our new benchmark is intended to address the current lack of a comprehensive multilingual alternative for this task.</p><p>As future work, we plan to further develop our evaluation benchmark, especially so as to expand the number of low-resource languages covered, and investigate the applicability of LEXICOMATIC to other resources beyond PWN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>In this section we discuss some limitations that we believe our work currently presents.</p><p>First, our method requires a Machine Translation system to generate a translated silver corpus and human annotators to create the training data for word alignment. This might constrain its applicability for some mid-to-low resource languages.</p><p>Second, our evaluation requires the availability of human annotators and a coverage with good recall from lemmas to BabelNet synsets in order to scale over a new language. Depending on the language under consideration, the availability of these resources might be limited and, paired with the overall complexity of the task, this implies that expanding our evaluation to span over more languages will be a costly process that requires time.</p><p>Finally, our framing as annotation projection might not be applicable to languages that present a high number of translation divergences (e.g., the English adverb usually in John usually goes home corresponds to the Spanish verb suele in Juan suele ir a casa <ref type="bibr" target="#b7">(Blloshmi et al., 2020)</ref>) as senses would be paired with lemmas that have different syntactic properties from their English counterparts. at their disposal. In these settings, LEXICOMATIC has two possible strategies to work around this obstacle. On the one hand, it can rely upon the zero-shot capabilities of the WSD model under consideration. On the other hand, as most dictionaries contain examples for each meaning enumerated, it may use these to generate silver training data. In this section, we compare these two approaches and highlight the key differences between them, conducting our studies on Wiktionary. Wiktionary presents a few major differences compared to PWN, besides the lack of sense-tagged corpora. Most importantly, it is less coarse-grained, with an average of 2.70 senses per lemma in contrast to 1.67 in Princeton WordNet 17 , and it has significantly more senses (752 473 compared to the 206 941 in PWN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 LEXICOMATIC Setup</head><p>Word Sense Disambiguation Depending on the strategy chosen, the disambiguation model, which is the only dictionary-dependent component in LEXICOMATIC, needs to be adapted as follows. When resorting to sense inventory zero-shot, the underlying model remains identical and the changes only pertain to the definitions provided at inference time, which originate from Wiktionary rather than PWN. Conversely, when using its meaning examples, we first need to convert these into WSD silver data. To this end, we retrieve the examples provided for each sense s and process them so as to tag the words corresponding to s. This operation results in 66 570 annotated instances, for as many sentences, and we partition them into 3 datasets, namely, train, validation and test, amounting to 60 570, 3000 and 3000 instances, respectively. Then, we replace the underlying disambiguation model with ESCHER trained on these data. We report in Table <ref type="table" target="#tab_6">5</ref> the F 1 score ESCHER achieves when trained with this configuration (Wiktionary), along with the score achieved, instead, when trained on PWN and tasked to zero-shot on Wiktionary (Wiktionary zs ). To better contextualize these results, we further show the performances ESCHER attains when trained and tested on PWN, reporting the F 1 score on the framework proposed by <ref type="bibr" target="#b36">Raganato et al. (2017)</ref>. 18 We note that, despite the training data being silver and available in a smaller quantity, ESCHER reaches a significant 84.6 F 1 score. As for Wiktionary zs , although it ex-17 Average senses per lemma computed on the intersection of the lemmas in the two inventories.</p><p>18 Results taken from <ref type="bibr">Barba et al. (2021a)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Results</head><p>Table <ref type="table">6</ref> shows the overall scores on the test sets. As a first result, we note that, even in this setting, the baseline has competitive performances and, interestingly, reaches F 1 and F .5 scores even higher than when translating PWN. This is likely due to the longer definitions<ref type="foot" target="#foot_15">20</ref> Wiktionary provides for each sense, which help the translation system better contextualize the lexicalizations. This trend is reflected on LEXICOMATIC, with Arabic, Chinese and Korean being slight exceptions, where, compared to the rest of the board, LEXICOMATIC achieves a significantly lower recall. <ref type="foot" target="#foot_16">21</ref> Furthermore, as in PWN, decreasing β increases the precision and lowers the recall, even if, in this case, the precision gain is more substantial than the recall loss on average.</p><p>Nevertheless, arguably the most interesting finding is the behavior of LEXICOMATIC when used in zero-shot (LEXICOMATIC zs β=0.7 ).<ref type="foot" target="#foot_17">22</ref> Indeed, while</p><p>Table <ref type="table" target="#tab_6">5</ref> showed a significant gap between Wiktionary and Wiktionary zs , the F 1 and F 0.5 scores of LEXICOMATIC β=0.7 and LEXICOMATIC zs β=0.7</p><p>are almost identical for each language. We believe this is a consequence of the large amount of text disambiguated which, combined with the filtering heuristics adopted, helps the model fill the gap between the two systems. The only significant difference between these lies in the number of senses produced, with LEXICOMATIC zs β=0.7</p><p>emitting consistently less senses than LEXICO-MATIC β=0.7 . Nonetheless, this result further backs our claim that leveraging the zero-shot capabilities of the disambiguation model considered is a viable option when translating resources with neither sense-tagged data nor examples.</p><p>Finally, the number of total senses covered by our approach compared to the baseline is as few as one-ninth when β = 0.7 and German is considered (and even lower if we take into account the zeroshot setting). The low percentage of senses covered is due to the large number of Named Entities that are present in Wiktionary but which are, instead, under-represented in parallel corpora.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Number of distinct synsets, divided by POS, in LEXICOMATIC test sets.</figDesc><table><row><cell></cell><cell>Low</cell><cell></cell><cell>Mid</cell><cell></cell><cell></cell><cell></cell><cell>High</cell><cell></cell><cell></cell></row><row><cell>POS</cell><cell>SV</cell><cell>AR</cell><cell>IT</cell><cell cols="2">KO DE</cell><cell>ES</cell><cell>FR</cell><cell>RU</cell><cell>ZH</cell></row><row><cell cols="10">NOUN 1543 1768 1672 801 1352 2021 1703 1178 1076</cell></row><row><cell>ADJ</cell><cell>94</cell><cell>213</cell><cell>646</cell><cell>16</cell><cell>324</cell><cell>524</cell><cell>506</cell><cell>-</cell><cell>-</cell></row><row><cell>VERB</cell><cell>151</cell><cell>581</cell><cell>503</cell><cell>40</cell><cell>641</cell><cell>910</cell><cell>596</cell><cell>516</cell><cell>445</cell></row><row><cell>ADV</cell><cell>21</cell><cell>5</cell><cell>112</cell><cell>53</cell><cell>21</cell><cell>94</cell><cell>84</cell><cell>-</cell><cell>30</cell></row><row><cell cols="10">TOTAL 1809 2567 2933 910 2338 3549 2889 1694 1551</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on our evaluation framework. In bold the best scores per language for precision, recall, F 1 and F .5 .</figDesc><table><row><cell></cell><cell>DE</cell><cell>ES</cell><cell>FR</cell><cell>IT</cell><cell>KO</cell><cell>ZH</cell></row><row><cell>LEXICOMATIC</cell><cell cols="6"># lexemes # instances 5.2M 6.0M 6.0M 6.3M 2.0M 3.2M 21k 21k 23k 20k 21k 16k # synsets 30k 31k 31k 31k 15k 20k</cell></row><row><cell>XL-WSD</cell><cell cols="4"># lexemes # instances 185k 393k 253k 385k 16k 22k 18k 24k # synsets 16k 32k 22k 30k</cell><cell>---</cell><cell>---</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Statistics of the training sets from LEXICOMATIC (top) and XL-WSD (bottom) in terms of number of lexemes, instances and distinct synsets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>. . , t n . F1 comparison of LEXICOMATIC against the MCS, ∅-shot and T-SC+WNG systems reported in</figDesc><table><row><cell>Model</cell><cell>DE</cell><cell>ES</cell><cell>FR</cell><cell>IT</cell><cell cols="2">KO ZH</cell></row><row><cell>MCS</cell><cell cols="6">76.0 55.6 59.3 52.8 52.5 29.6</cell></row><row><cell>∅-shot</cell><cell cols="6">83.2 75.8 83.9 77.7 64.2 51.6</cell></row><row><cell>T-SC+WNG</cell><cell cols="4">73.8 77.3 71.4 77.7</cell><cell>-</cell><cell>-</cell></row><row><cell>ConSeC</cell><cell cols="4">84.2 77.4 84.4 79.3</cell><cell>-</cell><cell>-</cell></row><row><cell cols="7">LEXICOMATIC 80.0 78.5 85.3 79.4 62.2 49.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>F 1 scores of ESCHER when trained on different sense inventories, i.e., PWN and Wiktionary. hibits a significant drop, the overall F 1 score is still remarkable, especially taking into consideration the differences between the two inventories. This finding is particularly promising for our setting, as Wiktionary zs effectively provides an effective estimate of how well LEXICOMATIC can be applied to dictionaries where neither sense-tagged data nor examples are available.Test Set &amp; Comparison SystemTo evaluate the resources LEXICOMATIC creates, we leverage additional in-house datasets for each language in our evaluation suite. As comparison systems, since no other work attempts to translate Wiktionary to the best of our knowledge, here we report only the performance of our MT baseline (Wiktionary + MT).19   </figDesc><table><row><cell cols="3">Sense Inventory Training Instances Dev Test</cell></row><row><cell>PWN</cell><cell>226036</cell><cell>76.3 80.7</cell></row><row><cell>Wiktionary</cell><cell>60570</cell><cell>84.5 84.6</cell></row><row><cell>Wiktionary zs</cell><cell>226036</cell><cell>71.5 70.9</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://wordnetcode.princeton.edu/standoff-files/ core-wordnet.txt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://wordnet.princeton.edu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>As will be seen, note that L-centric parallel corpora are not a strict requirement for LEXICOMATIC, which can also simply rely on (non L-centric) standard parallel corpora. We highlight this requirement at this point only for presentation simplicity and computational efficiency.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>For a given (σ, τ ) pair to be considered, the classifier must have yielded a probability for their alignment that is &gt; 0.5.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>This might occur if the selection process filters out all possible alignments, or no alignments have been provided.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>While the alignment model needs manually-aligned data, a few hundred sentences suffice<ref type="bibr" target="#b34">(Procopio et al., 2021)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>Corresponding to 10k-100K, 100K-1M and 10M+ groups in<ref type="bibr" target="#b41">Tang et al. (2020)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>https://en.wikipedia.org/. We use the dump of December 2021.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>https://stanfordnlp.github.io/stanza/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>In order to include both single tokens and multiword expressions, we create a vocabulary of multiword expressions from titles of Wikipedia pages and a manually-selected set of common multiword expressions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10"><p>Lexemes are considered to be incorrect and thus discarded if one or more of the following issues can be identified: i) lexicalization issues; ii) wrong language, i.e., a lemma is in a language other than the one under consideration.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11"><p>https://github.com/SapienzaNLP/esc</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_12"><p>In order to enable multilinguality, we convert senses to synsets.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_13"><p>The choice of this language set is the result of the intersection between the languages we considered in Section 4 and those included in XL-WSD.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_14"><p>See Section 4.4.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_15"><p>Wiktionary has 12.19 tokens on average, whereas PWN has 10.02.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_16"><p>We believe this phenomenon is the result of the different behavior the annotators had: significantly less candidates were produced for each element compared to the other languages.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_17"><p>Due to space constraints, we only report LEXICO-</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors gratefully acknowledge the support of the <rs type="grantName">ERC Consolidator Grant</rs> MOUSSE No. <rs type="grantNumber">726487</rs>, of the <rs type="projectName">ELEXIS</rs> project No. <rs type="grantNumber">731015</rs> under the European Union's <rs type="programName">Horizon 2020 research and innovation programme</rs>, and of the <rs type="projectName">PNRR MUR</rs> project <rs type="grantNumber">PE0000013-FAIR</rs>. The authors thank Babelscape for supporting and performing the annotation and evaluation work in many languages.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_pqJa4dM">
					<idno type="grant-number">726487</idno>
					<orgName type="grant-name">ERC Consolidator Grant</orgName>
					<orgName type="project" subtype="full">ELEXIS</orgName>
				</org>
				<org type="funded-project" xml:id="_7ashmhm">
					<idno type="grant-number">731015</idno>
					<orgName type="project" subtype="full">PNRR MUR</orgName>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
				<org type="funding" xml:id="_wx3UV94">
					<idno type="grant-number">PE0000013-FAIR</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Scaling to Other Dictionaries</head><p>In this Appendix, we show how LEXICOMATIC can be used effectively to construct and expand multilingual semantic dictionaries when adopting a sense inventory other than PWN.</p><p>Differently from PWN for which a considerable amount of manually-annotated data is available, most dictionaries do not have this kind of resource : Results on Wiktionary for the 9 languages under consideration. We mark in bold the best scores per language for precision, recall, F 1 and F .5 .</p><p>MATIC zs for β = 0.7.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Random walks for knowledge-based word sense disambiguation</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Oier López De Lacalle</surname></persName>
		</author>
		<author>
			<persName><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="84" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enhancing automatic wordnet construction using word embeddings</title>
		<author>
			<persName><forename type="first">Feras</forename><surname>Al</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tarouti</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jugal</forename><surname>Kalita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Multilingual and Cross-lingual Methods in NLP</title>
		<meeting>the Workshop on Multilingual and Cross-lingual Methods in NLP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="30" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ESC: Redesigning WSD with extractive sense comprehension</title>
		<author>
			<persName><forename type="first">Edoardo</forename><surname>Barba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Pasini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.371</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4661" to="4672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">2021b. ConSeC: Word sense disambiguation as continuous sense comprehension</title>
		<author>
			<persName><forename type="first">Edoardo</forename><surname>Barba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Procopio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.112</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="1492" to="1503" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Breaking through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2854" to="2864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recent trends in word sense disambiguation: A survey</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Pasini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2021/593</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4330" to="4338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Moving down the long tail of word sense disambiguation with gloss informed bi-encoders</title>
		<author>
			<persName><forename type="first">Terra</forename><surname>Blevins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.95</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1006" to="1017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">XL-AMR: Enabling cross-lingual AMR parsing with transfer learning techniques</title>
		<author>
			<persName><forename type="first">Rexhina</forename><surname>Blloshmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rocco</forename><surname>Tripodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.195</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2487" to="2500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Linking and extending an open multilingual Wordnet</title>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1352" to="1362" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Survey of Wordnets and their Licenses</title>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyonghee</forename><surname>Paik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Global WordNet Conference</title>
		<meeting>the 6th Global WordNet Conference</meeting>
		<imprint>
			<publisher>GWC</publisher>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DiBiMT: A Novel Benchmark for Measuring Word Sense Disambiguation Biases in Machine Translation</title>
		<author>
			<persName><forename type="first">Niccolò</forename><surname>Campolungo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Martelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Saina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4331" to="4352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Framing Word Sense Disambiguation as a Multi-Label Problem for Model-Agnostic Knowledge Integration</title>
		<author>
			<persName><forename type="first">Simone</forename><surname>Conia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL</title>
		<meeting>the EACL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised Cross-lingual Representation Learning at Scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Édouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards a Universal Wordnet by Learning from Combined Evidence</title>
		<author>
			<persName><forename type="first">Gerard</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melo</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<idno type="DOI">10.1145/1645953.1646020</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM conference on Information and knowledge management</title>
		<meeting>the 18th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="513" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">M</forename><surname>Arthur P Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the royal statistical society: series B (methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Integrating Personalized Pagerank into Neural Word Sense Disambiguation</title>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9092" to="9098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic labeling of semantic roles</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="DOI">10.3115/1075218.1075283</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 38th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="512" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">GlossBERT: BERT for word sense disambiguation with gloss knowledge</title>
		<author>
			<persName><forename type="first">Luyao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1355</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3509" to="3514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hallucinating phrase translations for low resource MT</title>
		<author>
			<persName><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-1617</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="160" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automated Wordnet Construction Using Word Embeddings</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</title>
		<meeting>the 1st Workshop on Sense, Concept and Entity Representations and their Applications</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="12" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Toward statistical machine translation without parallel corpora</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter</title>
		<meeting>the 13th Conference of the European Chapter<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="130" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatically Constructing Wordnet Synsets</title>
		<author>
			<persName><forename type="first">Khang Nhut</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feras</forename><surname>Al Tarouti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jugal</forename><surname>Kalita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="106" to="111" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic Wordnet Mapping Using Word Sense Disambiguation</title>
		<author>
			<persName><forename type="first">Changki</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geunbae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seo</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 Joint SIG-DAT conference on Empirical methods in natural language processing and very large corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 2000 Joint SIG-DAT conference on Empirical methods in natural language processing and very large corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully-Semantic Parsing and Generation: the BabelNet Meaning Representation</title>
		<author>
			<persName><forename type="first">Abelardo</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martinez</forename><surname>Lorenzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Maru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1727" to="1741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<title level="m">Distributed representations of words and phrases and their compositionality. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">WordNet: a lexical database for English</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1145/219717.219748</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A semantic concordance</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randee</forename><surname>Tengi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">T</forename><surname>Bunker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT</title>
		<meeting>of HLT</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="303" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic persian wordnet construction</title>
		<author>
			<persName><forename type="first">Mortaza</forename><surname>Montazery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heshaam</forename><surname>Faili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling 2010: Posters</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="846" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Entity linking meets word sense disambiguation: a unified approach</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="231" to="244" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Natural Language Understanding: Instructions for (present and future) use</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5697" to="5702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ba-belNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Paolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ponzetto</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="217" to="250" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A survey on automaticallyconstructed wordnets and their evaluation: Lexical and word embedding-based approaches</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Neale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA)</publisher>
			<date type="published" when="2018-05-07">2018. May 7-12, 2018</date>
		</imprint>
	</monogr>
	<note>LREC 2018</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Wn-toolkit: Automatic generation of WordNets following the expand model</title>
		<author>
			<persName><forename type="first">Antoni</forename><surname>Oliver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Global Wordnet Conference</title>
		<meeting>the Seventh Global Wordnet Conference</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="7" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">XL-WSD: An extra-large and crosslingual evaluation framework for word sense disambiguation</title>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Pasini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="13648" to="13656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multimirror: Neural crosslingual word alignment for multilingual word sense disambiguation</title>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Procopio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edoardo</forename><surname>Barba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Martelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2021/539</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3915" to="3921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stanza: A Python natural language processing toolkit for many human languages</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Word sense disambiguation: A unified evaluation framework and empirical comparison</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<title level="s">the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="99" to="110" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Building a free French wordnet from multilingual resources</title>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darja</forename><surname>Fišer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05791</idno>
		<title level="m">Wiki-Matrix: Mining 135m Parallel Sentences in 1620 Language Pairs from Wikipedia</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Personalized PageRank with syntagmatic information for multilingual word sense disambiguation</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Scozzafava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Maru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Brignone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Torrisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL 2020</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="37" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Automatic Wordnet Development for Low-Resource Languages Using Cross-Lingual WSD</title>
		<author>
			<persName><forename type="first">Nasrin</forename><surname>Taghizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hesham</forename><surname>Faili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="61" to="87" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chau</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng-Jen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00401</idno>
		<title level="m">Multilingual translation with extensible multilingual pretraining and finetuning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">EuroWordNet: A multilingual database with lexical semantic networks</title>
		<author>
			<persName><forename type="first">Piek</forename><surname>Vossen</surname></persName>
		</author>
		<idno type="DOI">10.5555/314515</idno>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="978" to="994" />
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
