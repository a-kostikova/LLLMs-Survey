<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RECESS: Resource for Extracting Cause, Effect, and Signal Spans</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Fiona</forename><forename type="middle">Anting</forename><surname>Tan</surname></persName>
							<email>tan.f@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Data Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computing and Digital Technology</orgName>
								<orgName type="institution">BCU</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hansi</forename><surname>Hettiarachchi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">KNAW Humanities</orgName>
								<address>
									<addrLine>Cluster DHLab</addrLine>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ali</forename><surname>Hürriyetoglu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Centre for Language Studies</orgName>
								<orgName type="institution">Radboud University</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nelleke</forename><surname>Oostdjik</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">CLCG</orgName>
								<orgName type="institution" key="instit2">University of Groningen</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tommaso</forename><surname>Caselli</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">National Institute of Japanese Literature</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tadashi</forename><surname>Nomoto</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Department of Sociology</orgName>
								<orgName type="institution">Mersin University</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Onur</forename><surname>Uca</surname></persName>
							<affiliation key="aff7">
								<orgName type="department">School of Computing Sciences</orgName>
								<orgName type="institution">University of East Anglia</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Farhana</forename><forename type="middle">Ferdousi</forename><surname>Liza</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Data Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">See-Kiong</forename><surname>Ng</surname></persName>
						</author>
						<title level="a" type="main">RECESS: Resource for Extracting Cause, Effect, and Signal Spans</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">26222826428BBEDB993CB5181AD3CCB6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T14:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Causality expresses the relation between two arguments, one of which represents the cause and the other the effect (or consequence). Causal relations are fundamental to human decision making and reasoning, and extracting them from natural language texts is crucial for building effective natural language understanding models. However, the scarcity of annotated corpora for causal relations poses a challenge in the development of such tools. Thus, we created Resource for Extracting Cause, Effect, and Signal Spans (RECESS), a comprehensive corpus annotated for causality at different levels, including Cause, Effect, and Signal spans. The corpus contains 3,767 sentences, of which, 1,982 are causal sentences that contain a total of 2,754 causal relations. We report baseline experiments on two natural language tasks (Causal Sentence Classification, and Cause-Effect-Signal Span Detection), and establish initial benchmarks for future work. We conduct an in-depth analysis of the corpus and the properties of causal relations in text. RECESS is a valuable resource for developing and evaluating causal relation extraction models, benefiting researchers working on topics from information retrieval to natural language understanding and inference.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A causal relation encodes a semantic relationship between two arguments, where one is the Cause argument, and the other is the Effect argument, in which the occurrence of the Cause leads to the occurrence of the Effect <ref type="bibr" target="#b0">(Barik et al., 2016)</ref>. A Cause can be a reason, explanation or justification that leads to an Effect <ref type="bibr" target="#b45">(Webber et al., 2019)</ref>. Causal relation extraction is an important information retrieval (IR) and natural language processing (NLP) task. Research has shown the usefulness of extracting causal relations in text for applications like summarization and next event prediction <ref type="bibr" target="#b34">(Radinsky et al., 2012;</ref><ref type="bibr" target="#b35">Radinsky and Horvitz, 2013;</ref><ref type="bibr" target="#b18">Izumi et al., 2021;</ref><ref type="bibr" target="#b12">Hashimoto et al., 2014)</ref>, question answering <ref type="bibr" target="#b5">(Dalal et al., 2021;</ref><ref type="bibr" target="#b13">Hassanzadeh et al., 2019;</ref><ref type="bibr" target="#b41">Stasaski et al., 2021)</ref>, inference and understanding <ref type="bibr" target="#b19">(Jo et al., 2021;</ref><ref type="bibr" target="#b8">Dunietz et al., 2020)</ref>. For example, <ref type="bibr" target="#b18">Izumi et al. (2021)</ref> built a prototype using a database of extracted causal relations from financial documents such that users can search for historical causal chains to anticipate upcoming events. Despite the benefits of causal relation extraction, answering why something has happened is not a trivial task for multiple reasons, ranging from the fact that a clear definition is needed of what the optimal answer should contain <ref type="bibr" target="#b8">(Dunietz et al., 2020)</ref>, to the lack of explicit evidence in a text due to a reference to commonsense knowledge. Another difficulty arises because causal relationships may be general or specific <ref type="bibr" target="#b25">(Mackie, 1980;</ref><ref type="bibr" target="#b15">Hitchcock, 1995)</ref>, and so it is crucial to identify in what context the causal relation occurs. The importance of causal relations to humans, and the difficulty in understanding them, is the main drive of our work to annotate a dedicated dataset for causal text mining. By providing a comprehensive annotated corpus of causal relations, we aim to facilitate the development of more effective causal relation extraction models.</p><p>In this paper, we present a Resource for Extract-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence</head><p>Label Span Annotations The bombing created panic among villagers .</p><p>Causal &lt;cause&gt;The bombing&lt;/cause&gt; &lt;effect&gt;&lt;signal&gt;created&lt;/signal&gt; panic among villagers&lt;/effect&gt; . Lack of medical services because of the strike left several patients in agony .</p><p>Causal &lt;effect&gt;Lack of medical services&lt;/effect&gt; &lt;signal&gt;because of&lt;/signal&gt; &lt;cause&gt;the strike&lt;/cause&gt; left several patients in agony . &lt;cause&gt;Lack of medical services&lt;/cause&gt; because of the strike &lt;ef fect&gt;&lt;signal&gt;left&lt;/signal&gt; several patients in agony&lt;/effect&gt; . KSRTC buses were attacked at ten places .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noncausal</head><p>-Table <ref type="table">1</ref>: Annotating sentences with binary labels, Causal or Non-causal, and annotating Causal sentences with Cause, Effect and Signal spans.</p><p>ing Cause, Effect, and Signal Spans (RECESS). 1  The most relevant contribution is the fine-grained annotation of Cause, Effect and Signal spans in causal sentences. Some examples are shown in Table 1. These richer annotations allow us to perform investigations into properties of causal relations in text (See Section 6 for detais). Additionally, we create annotation guidelines and evaluation rules that can accommodate multiple causal relations in one sequence, so as to allow the study of causal chains.</p><p>We provide competitive baseline scores based on state-of-the-art models for two NLP tasks:</p><p>(1) Causal Sentence Classification and (2) Cause-Effect-Signal Span Detection. With a total of 2,574 causal relations, RECESS is larger than other causal text mining benchmarks: CausalTimeBank (CTB) <ref type="bibr" target="#b26">(Mirza et al., 2014)</ref> contains 318 causal pairs and EventStoryLine (ESL) <ref type="bibr" target="#b3">(Caselli and Vossen, 2017)</ref> contains 1,770 causal pairs. In our experiments, we demonstrate that RECESS' data size is sufficient to train models that return reasonable F1 scores for both tasks. We also propose a rule-based scheme to convert RECESS into SQuAD format, and demonstrate its relevance to Why-Questions. To promote research on causal text mining, we hosted a shared task using RECESS. 2  The rest of the paper is organized as follows: Section 2 discusses related work. Section 3 outlines the creation process of RECESS while Section 4 analyzes the properties of the final dataset. Section 5 presents our experiments, models, and their evaluation against RECESS. Section 6 investigates some research questions about causal relations in text. Finally, Section 7 concludes this paper.</p><p>1 Our repository is available at: https://github.com/ tanfiona/CausalNewsCorpus. RECESS is equivalent to CNC-V2.</p><p>2 At the point of this paper's submission, the Event Causality Identification Shared Task <ref type="bibr" target="#b43">(Tan et al., 2023)</ref> was in progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Extraction of causality from text is a non-trivial task since the semantic understanding of the context and world knowledge is often necessary. Automatic extraction of causal knowledge from text has been of interest to many computational linguistic researchers <ref type="bibr" target="#b1">(Blanco et al., 2008;</ref><ref type="bibr" target="#b7">Do et al., 2011;</ref><ref type="bibr" target="#b21">Kontos and Sidiropoulou, 1991;</ref><ref type="bibr" target="#b40">Riaz and Girju, 2013)</ref>. There have been corpora that have been annotated for causal events, like the CTB <ref type="bibr" target="#b26">(Mirza et al., 2014)</ref>, CaTeRS <ref type="bibr" target="#b27">(Mostafazadeh et al., 2016)</ref> and ESL <ref type="bibr" target="#b3">(Caselli and Vossen, 2017)</ref>. However, they only include annotations of event root words and, thus, do not take into account contexts relevant to the Cause and Effect events. Furthermore, CTB only annotates explicit causal relations. On the other hand, the Penn Discourse Treebank (PDTB) <ref type="bibr" target="#b31">(Prasad et al., 2008;</ref><ref type="bibr" target="#b45">Webber et al., 2019;</ref><ref type="bibr" target="#b32">Prasad et al., 2006)</ref> includes many causal relations but only between clauses. Hence, PDTB does not capture relationships of more fine-grained arguments within clauses. Our approach is mostly inspired by the BE-CauSE 2.0 corpus <ref type="bibr" target="#b9">(Dunietz et al., 2017)</ref> to include more varied constructions of causality. However, BECauSE 2.0 only contains 1,803 causal relations, of which, more than half rely on resources that require paid access.</p><p>To address the lack of publicly available corpora suitable for causal text mining, the Causal News Corpus (CNC) <ref type="bibr">(Tan et al., 2022b)</ref> was created. Subsequently, the authors also annotated a small subset of their data with Cause-Effect-Signal spans for the Event Causality Identification shared task <ref type="bibr">(Tan et al., 2022a)</ref>. However, their work was incomplete: only 264 sentences were annotated from CNC, and the guidelines, descriptions and analyses were brief. In RECESS, we annotated all of CNC's Causal sentences. Having a completed dataset allows us to provide deeper analyses on the CNC data which previously was not possible, especially about the properties of causal relationships in text, as discussed in Section 6. Furthermore, we also revised the binary labels of sentences for which we did not find causal relations during the span annotation phase, and performed sentence splitting for examples found to contain multiple sentences. Therefore, on top of having more fine-grained span annotations, RECESS is also a larger and cleaner corpus than CNC for binary causal relation classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The RECESS Corpus</head><p>In RECESS, we annotated Causal sentences with Cause, Effect and Signal spans, where available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Defining Causal Relations</head><p>A sentence that contains at least one Cause and Effect pair is said to be Causal. The causal relation must be expressed in the target sentence, regardless of its truthfulness in the world.</p><p>To define causality more concretely, we used the five tests for causality from <ref type="bibr" target="#b11">Grivaz (2010)</ref> to logically verify the Cause and Effect spans, in a similar way to previous works <ref type="bibr" target="#b9">(Dunietz et al., 2017;</ref><ref type="bibr">Tan et al., 2022b,a)</ref>. The tests are described as follows:</p><p>1. Why: The event pair is not causal if the reader cannot construct a "Why" question based on the Effect.</p><p>2. Temporal order: The event pair is not causal if the Cause does not precede the Effect in terms of time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Counterfactual:</head><p>The event pair is not causal if the Effect is equally likely to occur or not occur without the Cause.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Ontological asymmetry:</head><p>The event pair is not causal if the reader can swap the Cause and Effect spans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Linguistic:</head><p>The event pair is likely to be causal if it can be rephrased into "X causes Y" or "Because of X, Y."</p><p>All five checks must be met in order for a pair of events to be considered Causal. Annotators used this framework to propose annotations, and Table <ref type="table" target="#tab_0">2</ref> demonstrates this in application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Source</head><p>RECESS builds on the CNC <ref type="bibr">(Tan et al., 2022b)</ref>, which is based on the GLOCON dataset of an Event Extraction Shared Task at CASE2021 <ref type="bibr">(Hürriyetoglu et al., 2021)</ref>. CNC contains 869 news documents and 3,559 English sentences. The news were reported from the year 2000 to the beginning of 2018 <ref type="bibr">(Hürriyetoglu et al., 2021)</ref>. After postcuration and cleaning (discussed later), the final dataset comprises of 3,767 sentences. We used the same train-validation-test splits as CNC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Annotation Guidelines</head><p>We define a Cause or Effect span as a continuous set of words sufficient for interpreting the causal relation. This means that any context modifying or describing the argument relevant to the causal relation is included. Each Cause or Effect span must contain an event, where an event is defined by a situation that happens or occurs, or a predicate that describes a state or circumstance in which something obtains or holds true <ref type="bibr" target="#b33">(Pustejovsky et al., 2003)</ref>.</p><p>Signal spans refer to words that explicitly relate the Cause and the Effect argument. Signals can occur in any position in a sentence relative to the Cause and Effect arguments. Signals can be comprised of multiple words (E.g. "so intense that"), and can also be discontinuous (E.g. "if...then"), again, consistent with the treatment of connectives by BECAUSE <ref type="bibr" target="#b9">(Dunietz et al., 2017)</ref> and PDTB <ref type="bibr" target="#b31">(Prasad et al., 2008;</ref><ref type="bibr" target="#b45">Webber et al., 2019;</ref><ref type="bibr" target="#b32">Prasad et al., 2006)</ref>. Signals are usually classified into three types: explicit (relations are signaled by discourse connectives), alternative lexicalizations (AltLex) (relations are signaled by a different lexical form), and implicit (no connectives) <ref type="bibr" target="#b45">(Webber et al., 2019;</ref><ref type="bibr" target="#b20">Knaebel and Stede, 2022;</ref><ref type="bibr" target="#b14">Hidey and McKeown, 2016)</ref>.</p><p>In the Appendix A.1, we highlight the exclusion rules, and on how we annotated multiple causal relations. For detailed explanation and examples of the annotation guidelines, please refer to the Annotation Manual available in our repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Annotation and Curation Process</head><p>Annotation and curation were conducted using We-bAnno <ref type="bibr" target="#b10">(Eckart de Castilho et al., 2016)</ref>, and postprocessed with Python. Appendix A.2 provides a description and screenshots of this tool.</p><p>Five annotators, from different academic back- grounds, were involved in the span annotations. Four curators, comprising of a linguistics undergraduate, linguistics graduate, NLP PhD student, and linguistics professor, were also involved. After each annotation round, a curator considered the spans proposed by all annotators and decided on the final annotations. For some subsets, a second curator looked through these final annotations and discussed disagreements. At the end of every annotation round, the final span annotations were made available to all annotators and curators to review and dispute.</p><p>Curators also helped to revise annotations from earlier rounds to adhere to the latest guidelines. Some examples wrongly contained multiple sentences due to the reliance on automatic splitting of sentences from the GLOCON dataset. Thus, curators also helped to mark locations to split up such instances into single sentences. For sentences where no causal relations were found, we revised the sequence classification label to Non-causal instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Inter-annotator Agreement</head><p>Given two proposed annotations for a causal relation, we first computed the Krippendorf's Alpha (K-Alpha) <ref type="bibr" target="#b22">(Krippendorff, 2011)</ref> <ref type="foot" target="#foot_0">3</ref> scores for the Cause, Effect and Signal spans independently. Given a sentence of n/2 words, we revised the span annotations into a list of 0s and 1s, where 1 represents that the word is part of the identified span, 0 otherwise. n 0 is the number of words that are not tagged as part of the span in the both annotations, while n 1 is the opposite. This means that n = n 0 + n 1 is the total number of words from both annotations. o 01 is the number of words where the two annotators disagree. K-Alpha is then represented by: Since annotators can propose multiple causal relations per example, we considered all possible combination to match the proposed relations, and kept the match that returned the highest possible sum of Exact Match (EM), One-Side Bound (OSB) and Token Overlap (TO) scores. The calculations for EM, OSB and TO are detailed in Appendix Section A.3.</p><formula xml:id="formula_0">α binary = 1 -(n -1) • o 01 n 0 • n 1 (1)</formula><p>To obtain the K-Alpha score per example, we weighted the score of each span by its true length (i.e. number of words). Essentially, the K-Alpha score weights each true word equally.</p><p>Finally, we aggregated the scores across examples to obtain the dataset level scores. Again, each example's score was weighted by its true span lengths. Overall, the inter-annotator agreement score (based on K-Alpha <ref type="bibr" target="#b22">(Krippendorff, 2011)</ref>) is 42.66%, 44.36% and 28.45% for Cause, Effect and Signal spans respectively, and 42.96% overall.   In RECESS, Causal sentences are longer than Non-causal sentences in terms of word counts. More specifically, Causal sentences have an average length of 33.75 words while Non-causal sen-tences have an average length of 26.90 words. Finally, since Signals can be implicit, the average number of relations that have Signals marked is less than one, at 0.69.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset Analysis</head><p>Table <ref type="table" target="#tab_4">5</ref> presents some annotated sentences while Table <ref type="table" target="#tab_5">6</ref> highlights the 20 most common Signals from RECESS. The annotation guidelines support a wide array of linguistic, syntactic and semantic structures in terms of (1) argument order in text, (2) number and flow of events, (3) signal type, and (4) sense type. Therefore, RECESS is a useful, diverse corpus for various types of linguistic and NLP research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this Section, we demonstrate use-cases of RE-CESS for NLP research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Tasks</head><p>RECESS is suitable for Event Causality Identification tasks, which aim to design models that tackle problems such as:</p><p>1. Causal Sentence Classification (CSC): Does an event sentence contain any cause-effect meaning?</p><p>2. Cause-Effect-Signal Span Detection (CES-SD): Which spans correspond to cause, effect or signal per causal sentence?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Models</head><p>For the CSC task, we replicate CNC's BERT benchmark <ref type="bibr">(Tan et al., 2022b)</ref>. The model fine-tunes the pre-trained (PTM) BERT model <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> for sequence classification. After BERT encodes sentences into word embeddings, the hidden state corresponding to the [CLS] token is fed through a binary classification head to obtain the predicted logits. We experimented with both bert-base-cased and bert-large-cased.</p><p>For the CES-SD task, we replicate the winning submission <ref type="bibr">(Chen et al., 2022)</ref>  4 for the CES-SD Event Causality Identification Shared Task <ref type="bibr">(Tan et al., 2022a)</ref>. Chen et al. framed the challenge as a reading comprehension task that aims to predict the start and end token positions of each Cause, Effect, and Signal span. On top of this baseline, they developed three components to further improve model performance: Beam-search span selector (BSS), signal classifier (SC), and data augmentation (DA). We incrementally incorporated these components on top of Baseline, resulting in the three additional models investigated: Baseline+BSS, Base-line+BSS+SC and Baseline+BSS+SC+DA. All models fine-tune albert-xxlarge-v2 <ref type="bibr" target="#b23">(Lan et al., 2019)</ref>. We describe the model and components in detail in the Appendix B. Model hyperparameters are available in the Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Setup</head><p>We trained each model on the training set and used the development set to select the best model with the highest F1 score at the end of each epoch. Subsequently, this best model was used to predict the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation Metrics</head><p>For CSC, we evaluate using Accuracy (Acc), standard Precision (P), Recall (R) and F1 per class, and Matthews Correlation Coefficient (MCC).</p><p>For CES-SD, we evaluate using token-wise Macro Precision (P), Recall (R) and F1 metrics. We used the FairEval implementation 5 of seqeval <ref type="bibr" target="#b28">(Nakayama, 2018;</ref><ref type="bibr" target="#b39">Ramshaw and Marcus, 1995)</ref>  that prevents double penalties of close-to-correct predictions <ref type="bibr" target="#b30">(Ortmann, 2022)</ref>. We evaluate at the relation level: each relation contributed equally to the final score. For sentences with multiple causal relations, as in <ref type="bibr">Tan et al. (2022a)</ref>, we returned the highest F1 score out of every possible way to match the predicted and true causal relations to each other. We only compare predictions with the number of true causal relations available. Conversely, any missing predictions were interpreted to predict the Other (O) label for all tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Baseline Scores</head><p>From Table <ref type="table" target="#tab_7">7</ref>, for the CSC task, the base BERT variant achieved an F1 score of 86.32% for the development set. Although the large BERT variant performed worse than base for the development set, it performed much better than base for the test set, achieving an F1 score of 83.15%.</p><p>From Table <ref type="table" target="#tab_8">8</ref>, for the CES-SD task, the best model is Baseline+BSS+SC, with a score of 70.51% for the development set and 67.69% for the test set. For <ref type="bibr">Chen et al. (2022)</ref>, Base-line+BSS+SC+DA was the best performing model. DA might have been useful for them because their training data size was small, with only 264 sentences. For us, incorporating artificially augmented data did not improve performance. This might be because there is limited linguistic diversity in the augmented data, since the DA approach fixed the signals and only paraphrased the Cause and Effect arguments. Furthermore, upon investigation, many of the sentences are grammatically unsound. For more analyses about the Subtask 2 model variants, please refer to Appendix B.</p><p>All in all, we report scores that will serve as initial baselines for future researchers to beat. Coupled with strong CSC and CES-SD models, researchers can perform end-to-end extraction of causal relations from text. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Investigations</head><p>In this section, we explore research questions about causal relations in text using RECESS' training and development set. We intend to keep the test set as an unseen, hold-out set to be used for future shared tasks, and thus, do not perform analyses on it.</p><p>6.1 When is causality easy/hard to detect?</p><p>First, we investigate the identification of causality in text by humans. We categorize sentence classification instances into two types: (1) Annotators all agree with one another, and (2) At least one annotator disagrees with another. From Figure <ref type="figure" target="#fig_1">1a</ref>, it is more likely for annotators to all agree that a sentence is Causal compared to when it is Non-causal. We analyse CES-SD using the best model, Base-line+BSS+SC. While it managed to score 70.51% for Overall F1, when we focus only on exam-ples with multiple causal relations, the F1 falls to 53.97%. This is because the current model is not properly designed to detect multiple causal relations in a sentence. The models by <ref type="bibr">(Chen et al., 2022)</ref> can only predict one Signal span per causal relation, and the multiple Cause and Effect spans obtained from BSS are still with reference to the same Signal. Therefore, the variation in Cause and Effect arises from having only slightly different word boundaries, instead of having different semantic arguments. We hope future researchers will improve on this aspect.</p><p>Out of the 249 causal relations in the development set, the model managed to predict 98 sequences exactly as the gold label. Of these, 62 contain explicit/AltLex markers, while the remaining 36 were implicitly expressed. Consistent with earlier findings, it is easier for models to detect causality if explicit/AltLex markers are present.</p><p>Models perform well for cases where the Cause and Effect spans are located near to each other. Out of the 62 explicit/AltLex marked examples, 35 had their causal signals located between the Cause and Effect arguments. The remaining examples either had the arguments following one another, if not, separated only by a comma. Likewise, for the 36 implicitly expressed examples, 26 had one-worded discourse markers lying between the Cause and Effect spans. The markers are non-causal ones, like "after", "when" and "in". Again, the remaining 10 had sequential arguments, or at most, had arguments separated by a comma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Do signals matter?</head><p>In the investigations above, the presence of causal signals helps indicate the presence of causality. In this subsection, we perform a qualitative study by using the CSC and CES-SD models to predict on six examples in Table <ref type="table">9</ref>. We notice that the models are very sensitive to explicit causal markers (E.g. "because") and non-causal markers (E.g. "but") to the extent where the content of the sentence no longer matters. For the examples without linguistic markers (E.g. S/N 2 and 3), the content of the sentence matters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">How does the presence of causality</head><p>correlate with the number of events?</p><p>The Event Extraction Shared Task <ref type="bibr">(Hürriyetoglu et al., 2021)</ref>  The protest was becoming overheated, the police said they were aware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noncausal</head><p>-Non-causal 4</p><p>The protest was becoming overheated, but the police rushed down onsite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noncausal</head><p>-Illogical -With explicit non-causal marker "but" 5</p><p>The protest was becoming overheated, thus, the protestors were calm.</p><p>Causal &lt;cause&gt;The protest was becoming overheated,&lt;/cause&gt;&lt;signal&gt;thus,&lt;/signal&gt;&lt;ef fect&gt;the protestors were calm.&lt;/effect&gt; Illogical -With explicit causal marker "thus"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6</head><p>Because fire extinguishes water, pigs can fly.</p><p>Causal &lt;signal&gt;Because&lt;/signal&gt; &lt;cause&gt;fire extinguishes water,&lt;/cause&gt;&lt;effect&gt;pigs can fly.&lt;/effect&gt; Illogical -With explicit causal marker "because"</p><p>Table <ref type="table">9</ref>: Ablation study: End-to-end predictions on example sentences. "Illogical" reflects Cause and Effect pairs that are not realistic according to world knowledge and commonsense.  Table <ref type="table">10</ref>: QA Performance.</p><p>&lt;fname&gt;. An example event sentence annotation is: "&lt;target&gt;KSRTC&lt;/target&gt; buses were &lt;trigger&gt;attacked&lt;/trigger&gt; at ten places ."</p><p>Only events under the scope of contentious politics were annotated. In other words, they do not follow the definition of events that we use in Section 3.3 based on Pustejovsky et al.. The scripts to add the event annotations to RECESS' examples are provided in our repository.</p><p>Based on Figure <ref type="figure" target="#fig_1">1c</ref>, sentences are more likely to be Causal if they contain more contentious events. On average, Causal sentences contain 4.58 contentious events while Non-causal sentences contain 3.96 contentious events. Since Causal sentences have to contain at least a pair of Cause and Effect event, while Non-causal sentences have no such restriction, it makes sense that Causal sentences, on average, contain more events. This could also explain why Causal sentences tend to be longer in Table <ref type="table" target="#tab_1">3</ref>, since Causal sentences need to describe at least two events. However, better event annotations are needed for better analyses of causality correlating with events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">How are causal relations related to causal question answering (QA)?</head><p>In extractive QA, given a question and the context document, the aim is to find the words that form the answer. SQuAD <ref type="bibr" target="#b38">(Rajpurkar et al., 2016</ref><ref type="bibr" target="#b37">(Rajpurkar et al., , 2018) )</ref> is a popular benchmark for QA. A natural way to convert causal relations to suit QA is to form a question using the Cause or Effect, then create the answer using the corresponding argument. This can be done by using templates, such as asking "Why did &lt;effect&gt;?" and using the Cause span as the answer. Each causal relation in the train set returned two QA examples, created by randomly selecting two out of six templates. Therefore, we obtained 4,514 Why-Questions. The templates and conversion scripts are available in our repository. Our baseline model is the t5-small <ref type="bibr" target="#b36">(Raffel et al., 2020)</ref> model trained on the train set and evaluated on the development set of SQuAD. The improved version first pre-trains on our Whyquestions before following the same training sched-ule. In evaluation, we identify 335 questions that contain the words 'reason', 'why', 'cause', or 'resulted' and refer to them as Why-Questions. The results are available in Table <ref type="table">10</ref>. While the original model achieved 53.42 EM and 63.21 F1, the model with additional pre-training on RECESS achieved a superior score of 55.52 EM and 65.61 F1 for Why-Questions. This exercise shows that RECESS has potential applications for QA too, especially surrounding Why-Questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">How are causal relations related to</head><p>natural language inference (NLI)?</p><p>Benchmarks like SNLI <ref type="bibr" target="#b2">(Bowman et al., 2015)</ref>, MNLI <ref type="bibr" target="#b46">(Williams et al., 2018)</ref> and ANLI <ref type="bibr" target="#b29">(Nie et al., 2020)</ref> classify a premise and hypothesis to one of the three class labels: neutral, entailment or contradict. One way of using RECESS for an NLI task is to rephrase either the Cause or Effect span as the premise, while the corresponding span will be the hypothesis. The label will certainly not be contradict. Typically, NLI treats causal relations as neutral events. For example, in "The farmworkers' demands were not met. The farmworkers' strike resumed on Tuesday.", a competitive model by Nie et al. 6 classifies this as neutral with 99.6% probability. Many possible outcomes can arise when farmworkers' demands were not met. For example, they could have initiated an online campaign, or stormed the capital. Since entailment requires that the hypothesis is " definitely correct about the situation or event" in the premise <ref type="bibr" target="#b46">(Williams et al., 2018)</ref>, most of the time, causal relations are considered neutral in NLI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Our paper introduces RECESS, a corpus consisting of 3,767 sentences, among which 1,982 are causal sentences containing a total of 2,754 causal relations. We detail the guidelines and process we used to annotate the Cause, Effect, and Signal spans, covering a broad range of linguistic, semantic, and syntactic structures. Additionally, we benchmarked our baseline models, which achieved competitive scores, with F1 scores of 83.15% and 67.69% on test sets for the CSC and CES-SD tasks, respectively. Our work also investigated causal relations in text and explored the relevance of causal relations to QA and NLI applications.</p><p>RECESS is an large data that offers opportunities to design specialized models that can extract multiple causal relations. To encourage research in this direction, we are organizing a shared task using RECESS.</p><p>Finally, RECESS is a valuable resource for studying NLP topics related to storyline and causal event chains. Researchers can use the contentious political event span annotations marked by earlier works <ref type="bibr">(Hürriyetoglu et al., 2021)</ref> to investigate events in the text. With its extensive coverage of causal relations, we also demonstrated that RE-CESS can be adapted into a causal QA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgments</head><p>This project is supported by the National Research Foundation, Singapore under its Industry Alignment Fund -Pre-positioning (IAF-PP) Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Guidelines</head><p>The Annotation Manual provides definitions, examples, and exclusions in better detail. This Section merely highlights important elements of the Manual and does not serve to be an exhaustive outline of the Manual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Exclusions</head><p>Sentences described by any of the following conditions were excluded from RECESS: 4. The Cause or Effect span requires discontinuous spans.</p><p>5. The Cause and Effect spans must overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 Multi-relation Annotation</head><p>In RECESS, each sentence may have multiple causal relations, which will all be annotated. The annotation tool and evaluation scheme were designed to support such scenarios. A consequence of annotating multiple causal relations per sentence is that we are able to detect consecutive events, if any. In cases where a span could be split into subparts that describe a series of Cause and Effect events, annotators were instructed to annotate them as separate spans. This is because we are interested to recreate a storyline, and such examples are important to test if models can truly understand narratives, and identify a series of events where one event leads to another.</p><p>In Example 1., there is a series of causal events, where → represents causation: "an altercation" &lt;EventA&gt; → "a youth . . . was allegedly severely injured in a thrashing ..." &lt;EventB&gt; → "the clashes erupted " &lt;EventC&gt;. Instead of combining &lt;EventA&gt; and &lt;EventB&gt; into a single Cause span for 1.(a), we asked annotators to mark only the most recent event as the Cause span.</p><p>1. (a) &lt;effect&gt;The clashes erupted&lt;/effect&gt; after &lt;cause&gt;a youth belonging to a minority Muslim sect was allegedly severely injured in a thrashing by a youth from the majority sect&lt;/cause&gt; following an altercation. (b) The clashes erupted after &lt;effect&gt;a youth belonging to a minority Muslim sect was allegedly severely injured in a thrashing by a youth from the majority sect&lt;/effect&gt; &lt;signal&gt;following&lt;/sig nal&gt; &lt;cause&gt;an altercation&lt;/cause&gt;.</p><p>One interesting observation is that for consecutive events where one causes the other (E.g.</p><p>&lt;EventA&gt; → &lt;EventB&gt; and &lt;EventB&gt; → &lt;EventC&gt;), it is also possible to claim that the union of two previous events (&lt;EventA&gt;+&lt;EventB&gt;) causes the third event: &lt;EventA&gt;+&lt;EventB&gt; → &lt;EventC&gt;. This is consistent with <ref type="bibr" target="#b24">Lewis (Lewis, 1974)</ref>'s claims that causation is a transitive relation. Take for example the second sentence shown in Table <ref type="table">1</ref>: If asked "Why were several patients left in agony EventA?", notice that we can answer this question with either "Because of the lack of medical services &lt;EventB&gt;." or "Because of the lack of medical services &lt;EventB&gt; because of the strike &lt;EventA&gt;.". A similar exercise can be done for Example 1. by asking "Why did the clashes erupt?". Annotators used this transitive property to check if spans contained sub-events that can be broken down to form consecutive causal events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Annotation Tool</head><p>Annotation and curation were conducted on We-bAnno <ref type="bibr" target="#b10">(Eckart de Castilho et al., 2016)</ref>.</p><p>Figure <ref type="figure" target="#fig_2">2</ref> shows the annotation interface used by annotators. On WebAnno, annotators first highlighted the words corresponding to a span, then indicated if the span is a Cause, Effect or Signal. Subsequently, to reflect the causal relations, annotators linked the spans together by directing the Cause to its Effect span, and if present, the Signal to the same Effect span. Annotations were then downloaded as JSON files and automatically validated for avoidable human errors, such as invalid links (E.g. An Effect points to another Effect) or missing links (E.g. An Effect has no Cause). If such errors were present, an error report was produced and sent to annotators to consider correcting their annotations. Finally, the curator assesses all  the annotations proposed and makes the final selection.</p><p>Figure <ref type="figure" target="#fig_3">3</ref> shows the curation interface used by curators. Curators have access to all annotators' annotations, and decide the final annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Inter-annotator Agreement</head><p>Calculating Agreement Scores Given two proposed annotations for a causal relation, we first computed agreement scores for the Cause, Effect and Signal spans individually. On top of the Krippendorf's Alpha (K-Alpha) <ref type="bibr" target="#b22">(Krippendorff, 2011)</ref> score described in Section 3.5, we also computed three other agreement metrics as follows:</p><p>• Exact Match (EM): 1 if two annotations are exactly the same, 0 otherwise</p><p>• One-Side Bound (OSB): 1 if either boundary of the span is the same between two annotations, 0 otherwise</p><p>• Token Overlap (TO): 1 if two annotations have at least one overlapped token, 0 otherwise Since annotators can propose multiple causal relations per example, we considered all possible combination of ways to match the proposed relations. After which, we retained the match that returned the highest possible sum of EM, OSB and TO scores. For example, if one annotator proposed two relations (A,B) while the next annotator proposed three relations (C), then we assessed the scores for (A-C, B-None) and (A-None, B-C) matches, and finally only kept the match that gives the highest sum of scores. If one annotator identified more causal relations than the other, then EM, OSB and TO scores for that relation is automatically zero.</p><p>To compute the Total score for each relation, EM, OSB, and TO was marked with 1 if and only if the Cause, Effect and Signal all had 1s, otherwise, the example would have an Total score of 0.</p><p>Finally, we aggregated the scores across examples to obtain the dataset level scores presented in Table <ref type="table" target="#tab_13">11</ref>. For the overall EM, OSB and TO scores, we take the average score across the dataset by weighting each example equally.</p><p>In Figure <ref type="figure" target="#fig_4">4</ref>, we illustrate one sentence with proposed annotations by three annotators. Since every annotator identified the same Effect span, all four metrics have the score of 1, suggesting full agreement. Meanwhile, since every annotator had a different Signal span, the score for EM, OSB and TO is 0. K-Alpha is negative, suggesting there is less agreement than one would expect by chance. Two annotators had the same proposed Cause span, therefore, EM is equivalent to 1/3, representing 1 out of the 3 possible annotator pairs have exactly matched spans. The Cause span has a score of 1 for OSB and TO, because all three annotators are aligned on the "banners" being the last word and part of Cause.</p><p>Evaluating Agreement Annotating causal arguments is very challenging. When comparing annotations across two annotators, the statistic is that they will only agree exactly with one another 6.03% of the time. Nevertheless, they will agree with each other 32.51% of the time based on having at least one same token in all three Cause, Effect and Signal arguments. In fact, they do agree with each other's Cause and Effect spans around 57-58% of the time based on having at least one same token. To summarize our annotation experience, it is hard for annotators to agree on both boundaries of a span, perhaps due to the subjectivity of what information is relevant to the event. However, they typically can identify the main event and do include them in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B CES-SD Model</head><p>We benchmark four different models based on the winning solution <ref type="bibr">(Chen et al., 2022)</ref> 7 of the CASE 2022 CES-SD Shared Task <ref type="bibr">(Tan et al., 2022a)</ref> by training and testing them on RECESS. The models frame the task as a reading comprehension task that aims to predict the start and end token positions of each Cause, Effect, and Signal span. All models fine-tune the albert-xxlarge-v2 <ref type="bibr" target="#b23">(Lan et al., 2019)</ref> PLM. More details about the model architectures are described below.</p><p>Consider a sentence T with n tokens, such that T = [t 1 , t 2 , ..., t n ], where t i represents a token.</p><p>Baseline utilizes a BERT-based encoder to convert T into a contextualized representation R = [r 1 , r 2 , ..., r n ]. Each r i has a depth d representing the hidden size of the PLM, so R has a dimension of n × d. The sequence output R is then fed through a dropout layer followed by a linear layer plus a softmax layer that serves as a classifier. The classifier returns a logit representation,   </p><p>We incrementally incorporated three components on top of Baseline as proposed in <ref type="bibr">(Chen et al., 2022)</ref>, resulting in the three additional models investigated: Baseline+BSS, Baseline+BSS+SC and Baseline+BSS+SC+DA. The components are described below in the next three paragraphs: BSS Beam-search span selector (BSS) is a postprocessing technique used to introduce constraints suited to the task such that (1) the start position is always before an end position, (2) the predicted Cause and Effect spans do not overlap each other, and (3) it is able to return multiple Cause and Effect spans per input sentence. The detailed pseudocode is presented in the original paper <ref type="bibr">(Chen et al., 2022)</ref>.</p><p>SC Signal classifier (SC) is a separate model that fine-tunes the bert-base-uncased <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> PLM on the binary classification task that detects if a Signal exists in a sequence. If the signal classifier model predicts that a sentence contains a Signal, then the Signal predictions from the span detection model will be retained.</p><p>DA Data augmentation (DA) was used to generate additional training examples. Cause and Effect spans within each sentence were paraphrased using a PEGASUS model <ref type="bibr" target="#b47">(Zhang et al., 2020)</ref>. 2257 additional training examples were generated based on the train set. This augmented data is available in our repository. 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Results &amp; Discussion</head><p>Due to space limitations in the main paper, we deep dive into the performance for the different CES-SD model variants in this section.</p><p>The findings suggest that BSS is a beneficial component to have for CES-SD. Intuitively, BSS constraints the predicted Cause and Effect spans to the task. Therefore, it helps to return a higher score relative to Cause and Effect span predictions. Notice that Signal span predictions, and hence the corresponding performance metrics, are the same compared to the Baseline.</p><p>For both datasets, performance metrics of Base-line+BSS and Baseline+BSS+SC have the same scores for Cause and Effect spans. This observation is expected since the SC only affects Signal span predictions by removing them when the classifier identifies the signal to be missing.</p><p>Baseline+BSS+SC+DA was the best performing model for <ref type="bibr">(Chen et al., 2022)</ref>. In their work, the model performance improves when the model is trained on a large set of training data. However, we could not replicate this finding. When we trained the model with augmented data, F1 score fell slightly (from 70.51% to 70.06%) on development set. The test set also observed a drop in performance. When we trained the model on even more augmented data (9 augments per original example), the development set F1 fell even further to 69.28%. Our hypothesis is that the artificially augmented data does not add much linguistic diversity to the training examples. This is because the DA approach fixes the signals and only paraphrases the 8 We also explored augmenting up to nine additional examples per original example. By obtaining three new phrases per span, we could combine spans such that each causal relation could return nine new examples for training. However, we found poorer results with larger augments in our experiments. The augmented datasets are available in our repository.</p><p>Cause and Effect arguments. Furthermore, many of the generated sentences are grammatically unsound, for example: "&lt;cause&gt;There was a lot of violence there&lt;/cause&gt; has &lt;effect&gt;fears of more attacks and heightened tensions&lt;/effect&gt;." In essence, DA corrupts the model by exposing the model with examples that are repetitive and can contain errors. DA might have been useful for <ref type="bibr">(Chen et al., 2022)</ref> because the training set back then was extremely small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hyper-parameters</head><p>In this Section, we provide the hyper-parameters for each training set up.</p><p>For the CSC model, both bert-base-cased and bert-large-cased variants had the following parameters:</p><formula xml:id="formula_2">per_device_train_batch_size=32 num_train_epochs=10 load_best_model_at_end=True metric_for_best_model=eval_f1 learning_rate=5e-05</formula><p>The base model took approximately 5 minutes, while the large model took approximately 12 minutes to train.</p><p>For the CES-SD models, the parameters were:</p><p>dropout=0.3 learning_rate=2e-05 model_name_or_path=albert-xxlarge-v2 num_train_epochs=10 num_warmup_steps=200 per_device_train_batch_size=8 weight_decay=0.005</p><p>The models took an average of 2 hours to train. The model trained on additional augmented data took slightly longer, around 2.68 hours to train.</p><p>For the QA model, to pre-train the model on RECESS, we used the following parameters: model_name_or_path=t5-small per_device_train_batch_size=12 learning_rate=3e-5 num_train_epochs=5 max_seq_length=512 doc_stride=128</p><p>For the actual training on SQuAD, we used the following parameters: model_name_or_path=t5-small per_device_train_batch_size=128 learning_rate=0.001</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) No. of causal relations. (b) Signal type. (c) No. of events.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Count plots of train + development set.</figDesc><graphic coords="8,229.61,271.07,136.06,91.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Screenshot of the annotation page used to mark Cause-Effect-Signal spans.</figDesc><graphic coords="14,78.66,154.79,202.68,128.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Screenshot of the curation page used to mark Cause-Effect-Signal spans.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example with three annotators, and its corresponding agreement scores reported in percentages (%).</figDesc><graphic coords="15,70.87,70.87,224.32,198.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>P</head><label></label><figDesc>= [p cs , p ce , p es , p ee , p ss , p se ] with a dimension of n × 6. Each p b vector reflects the probability of each token being the span boundary position. Subscripts cs, ce, es, ee, ss and se refer to Cause-Start, Cause-End, Effect-Start, Effect-End, Signal-Start and Signal-End, respectively. The position with maximum probability was selected as the final prediction using the following formula, where B represents the final position boundary predicted given the logits in p b :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Examples illustrating how to use the Five Tests for Causality to check span annotations.</figDesc><table><row><cell>Sentence</cell><cell></cell><cell></cell><cell cols="4">Causality Tests Why? Temporal Counter-Onto.</cell><cell>Linguis-</cell><cell>Label</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Order</cell><cell>fact.</cell><cell>Asym.</cell><cell>tic</cell><cell></cell></row><row><cell cols="3">&lt;cause&gt;This strike&lt;/cause&gt; &lt;signal&gt;is causing&lt;/signal&gt; &lt;ef</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>Causal</cell></row><row><cell>fect&gt;huge disruptions&lt;/effect&gt;. . .</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>&lt;potential-effect&gt;Some</cell><cell>protesters</cell><cell>attacked</cell><cell>✗</cell><cell>✓</cell><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>Non-</cell></row><row><cell cols="3">me&lt;/potential-effect&gt; when &lt;potential-cause&gt;I was</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>causal</cell></row><row><cell cols="2">clicking pictures&lt;potential-cause&gt;. . .</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Sequence Labels for Event Sentences Summary Statistics.</figDesc><table><row><cell cols="2">Stat. Label</cell><cell cols="2">Train Dev</cell><cell>Test</cell><cell>Total</cell></row><row><cell>#</cell><cell>Causal</cell><cell>1624</cell><cell>185</cell><cell>173</cell><cell>1982</cell></row><row><cell>Sent-</cell><cell cols="2">Non-causal 1451</cell><cell>155</cell><cell>179</cell><cell>1785</cell></row><row><cell>ences</cell><cell>Total</cell><cell>3075</cell><cell>340</cell><cell>352</cell><cell>3767</cell></row><row><cell>Avg.</cell><cell>Causal</cell><cell cols="4">33.44 34.41 35.93 33.75</cell></row><row><cell>#</cell><cell cols="5">Non-causal 26.69 26.85 28.67 26.90</cell></row><row><cell>words</cell><cell>Total</cell><cell cols="4">30.25 30.96 32.24 30.50</cell></row><row><cell cols="2">Statistic</cell><cell cols="2">Train Dev</cell><cell>Test</cell><cell>Total</cell></row><row><cell cols="2"># Sentences</cell><cell>1624</cell><cell>185</cell><cell>173</cell><cell>1982</cell></row><row><cell cols="2"># Relations</cell><cell>2257</cell><cell>249</cell><cell>248</cell><cell>2754</cell></row><row><cell cols="2">Avg. rels/sent</cell><cell>1.39</cell><cell>1.35</cell><cell>1.43</cell><cell>1.39</cell></row><row><cell cols="2">Avg. # words</cell><cell cols="4">33.44 34.41 35.93 33.75</cell></row><row><cell cols="2">Cause</cell><cell cols="4">11.56 12.20 12.96 11.74</cell></row><row><cell cols="2">Effect</cell><cell cols="4">10.71 10.18 11.54 10.74</cell></row><row><cell cols="2">Signal</cell><cell>1.45</cell><cell>1.53</cell><cell>1.46</cell><cell>1.46</cell></row><row><cell cols="2">Avg # Sig./rel</cell><cell>0.70</cell><cell>0.64</cell><cell>0.79</cell><cell>0.70</cell></row><row><cell cols="3">Prop. of rels w/ Sig. 0.68</cell><cell>0.63</cell><cell>0.76</cell><cell>0.69</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Span Annotations for Causal Sentences Summary Statistics.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Cause before Effect &lt;signal&gt;Since&lt;/signal&gt; &lt;cause&gt;the work on the bridge was progressing at a snail ' s pace&lt;/cause&gt; , &lt;effect&gt;the locals had begun an agitation since June 16&lt;/effect&gt; . Effect before Cause The R82 , &lt;effect&gt;an arterial road linking Vereeniging to Johannesburg remains closed&lt;/ef fect&gt; &lt;signal&gt;due to&lt;/signal&gt; &lt;cause&gt;the public unrest&lt;/cause&gt; . &lt;cause&gt;the work on the bridge was progressing at a snail ' s pace&lt;/cause&gt; , &lt;effect&gt;the locals had begun an agitation since June 16&lt;/effect&gt; . Concurrent Events &lt;cause&gt;Police opened fire&lt;/cause&gt; , &lt;effect&gt;&lt;signal&gt;killing&lt;/signal&gt; 34 striking workers and &lt;signal&gt;wounding&lt;/signal&gt; 78&lt;/effect&gt; ... Consecutive Events &lt;effect&gt;Lack of medical services&lt;/effect&gt; &lt;signal&gt;because of&lt;/signal&gt; &lt;cause&gt;the strike&lt;/cause&gt; left several patients in agony . &lt;cause&gt;Lack of medical services&lt;/cause&gt; because of the strike &lt;effect&gt;&lt;sig nal&gt;left&lt;/signal&gt; several patients in agony&lt;/effect&gt; . &lt;effect&gt;an arterial road linking Vereeniging to Johannesburg remains closed&lt;/ef fect&gt; &lt;signal&gt;due to&lt;/signal&gt; &lt;cause&gt;the public unrest&lt;/cause&gt; . AltLex &lt;cause&gt;&lt;signal&gt;Irked over&lt;/signal&gt; the arrests&lt;/cause&gt; , &lt;effect&gt;the protestors staged dharna&lt;/effect&gt; . Implicit However, &lt;cause&gt;trade unions refused to accept it&lt;/cause&gt; and &lt;signal&gt;(Im-plicit=thus)&lt;/signal&gt; &lt;effect&gt;continued their strike&lt;/effect&gt;.</figDesc><table><row><cell>Topic Type</cell><cell>Example Annotated Sentence</cell></row><row><cell>Arg.</cell><cell></cell></row><row><cell>Order</cell><cell></cell></row><row><cell>in Text</cell><cell></cell></row><row><cell cols="2">No. and Flow of Events &lt;signal&gt;Since&lt;/signal&gt; Signal Single Event Explicit The R82 ,</cell></row></table><note><p><p><p>Sense</p>Cause</p>&lt;effect&gt;Spokesman Keith Khoza said they had decided to March to Prime Media&lt;/effect&gt; &lt;signal&gt;because&lt;/signal&gt; &lt;cause&gt;the cartoon had raised various concerns&lt;/cause&gt;. Purpose &lt;effect&gt;The protesters planted the saplings in potholes&lt;/effect&gt; &lt;cause&gt;&lt;sig nal&gt;to&lt;/signal&gt; draw the attention of the officials to the poor condition of the road&lt;/cause&gt; . Condition &lt;signal&gt;If&lt;/signal&gt; &lt;cause&gt;it does not act&lt;/cause&gt;, &lt;effect&gt;the protests will con-tinue&lt;/effect&gt; . NegCondition or NegResult &lt;effect&gt;We will continue our strike&lt;/effect&gt; &lt;signal&gt;till&lt;/signal&gt; &lt;cause&gt;we get an assurance from the Government&lt;/cause&gt; .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Examples from RECESS.</figDesc><table><row><cell cols="2">Rank Signal</cell><cell>Count</cell><cell cols="2">Rank Signal</cell><cell>Count</cell></row><row><cell>1</cell><cell>to</cell><cell>364</cell><cell>11</cell><cell>because</cell><cell>20</cell></row><row><cell>2</cell><cell>for</cell><cell>170</cell><cell>12</cell><cell>in protest</cell><cell>19</cell></row><row><cell>3</cell><cell>as</cell><cell>141</cell><cell>13</cell><cell>in connection with</cell><cell>17</cell></row><row><cell>4</cell><cell>demanding</cell><cell>111</cell><cell>14</cell><cell>in support of</cell><cell>14</cell></row><row><cell>5</cell><cell>following</cell><cell>78</cell><cell>15</cell><cell>seeking</cell><cell>12</cell></row><row><cell>6</cell><cell>by</cell><cell>64</cell><cell>16</cell><cell>even as</cell><cell>12</cell></row><row><cell>7</cell><cell>over</cell><cell>64</cell><cell>17</cell><cell>protesting</cell><cell>10</cell></row><row><cell>8</cell><cell>if</cell><cell>46</cell><cell>18</cell><cell>despite</cell><cell>10</cell></row><row><cell>9</cell><cell>with</cell><cell>44</cell><cell>19</cell><cell>in the wake of</cell><cell>10</cell></row><row><cell>10</cell><cell>due to</cell><cell>25</cell><cell>20</cell><cell>led to</cell><cell>10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Top 20 Signals in RECESS.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Performance Metrics for Causal Sentence Classification. All scores are reported in percentages (%). Highest score per dataset and metric is in bold.</figDesc><table><row><cell cols="2">Eval PTM</cell><cell>R</cell><cell>P</cell><cell>F1</cell><cell>Acc MCC</cell></row><row><cell>Dev</cell><cell cols="5">base 88.65 84.10 86.32 84.71 69.13 large 84.86 85.79 85.33 84.12 68.02</cell></row><row><cell>Test</cell><cell cols="5">base 89.02 75.86 81.91 80.68 62.37 large 88.44 78.46 83.15 82.39 65.35</cell></row></table><note><p><p>4 https://github.com/Gzhang-umich/ 1CademyTeamOfCASE</p>5 https://huggingface.co/spaces/hpi-dhc/ FairEval</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Performance Metrics for Cause-Effect-Signal Span Detection. All scores are reported in percentages (%). Highest score per dataset and metric is in bold. Detailed results available at Appendix Table 12.</figDesc><table><row><cell>Eval</cell><cell>Model</cell><cell>R</cell><cell>Overall P</cell><cell>F1</cell></row><row><cell></cell><cell>Baseline</cell><cell cols="3">66.32 59.48 62.71</cell></row><row><cell>Dev</cell><cell>+BSS +BSS+SC</cell><cell cols="3">71.39 64.43 67.73 71.22 69.81 70.51</cell></row><row><cell></cell><cell>+BSS+SC+DA</cell><cell cols="3">70.89 69.25 70.06</cell></row><row><cell></cell><cell>Baseline</cell><cell cols="3">61.49 61.89 61.69</cell></row><row><cell>Test</cell><cell>+BSS +BSS+SC</cell><cell cols="3">67.30 66.98 67.14 66.56 68.86 67.69</cell></row><row><cell></cell><cell>+BSS+SC+DA</cell><cell cols="3">64.43 67.56 65.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>The average observed agreement score of Causal sentences is 87.10%, but only 78.23% for Noncausal sentences. From Figure 1b, causal relations are easier for humans to detect if there are causal markers present in the text. For Causal examples with explicit or AltLex signals, annotators fully agree with each other around 69% of the time. For implicit Causal examples, annotators fully agree with each other less frequently -around 55% of the time. Next, we perform error analysis on the base CSC model. For the Causal examples of the development set, 122 had explicit/AltLex markers while 63 were implicitly expressed. The model failed to identify 10/122 ≈ 8% explicit/AltLex and 11/63 ≈ 17% implicit examples. Similar to humans, the model finds it harder to identify causality if relations are expressed implicitly.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Inter-annotator Agreement Scores. Reported in percentages (%).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>Code to calculate K-Alpha across multiple annotators: https://github.com/emerging-welfare/kAlpha.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_1"><p>https://huggingface.co/ynie/ roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Limitations &amp; Ethics Statement</head><p>The main limitation of RECESS and our models is that we only focus on extracting causal relations from text, without checking if the relations hold in the real-world. For example, in Table <ref type="table">9</ref>, the CES-SD model will identify causal relations for illogical statements with explicit causal markers. In application, users must perform fact-checking of the extracted causal relations, and/or only extract causal relations from reliable sources, to avoid drawing the wrong conclusions.</p><p>All annotators and curators voluntarily participated in the creation of the corpus and are the authors of this paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>num_train_epochs=20 max_seq_length=512 doc_stride=128</head><p>We change the model_name_or_path to the RE-CESS' pre-trained model's directory for the updated model. For each run to train and predict on SQuAD, it takes around 1.10h. All experiments were ran on NVIDIA Tesla V100 SXM2 32 GB GPU, CUDA Version: 11.3.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Event Causality Extraction from</title>
		<author>
			<persName><forename type="first">Biswanath</forename><surname>Barik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erwin</forename><surname>Marsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Öztürk</surname></persName>
		</author>
		<ptr target="http://rcs.cic.ipn.mx/2016_117/" />
	</analytic>
	<monogr>
		<title level="j">Natural Science Literature. Res. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="97" to="107" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note>Event%20Causality%20Extraction%20from% 20Natural%20Science%20Literature.pdf</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Causal relation extraction</title>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuria</forename><surname>Castell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)</title>
		<meeting>the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large anno-tated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1075</idno>
		<ptr target="https://doi.org/10.18653/v1/D15-1075" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Lisbon, Portugal</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Lisbon, Portugal</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Event StoryLine Corpus: A New Benchmark for Causal and Temporal Relation Extraction</title>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piek</forename><surname>Vossen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-2711</idno>
		<ptr target="https://doi.org/10.18653/v1/W17-2711" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Events and Stories in the News Workshop. Association for Computational Linguistics</title>
		<meeting>the Events and Stories in the News Workshop. Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">2022. 1Cademy @ Causal News Corpus 2022: Enhance Causal Span Detection via Beam-Searchbased Position Selector</title>
		<author>
			<persName><forename type="first">Xingran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Nik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.case-1.14" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE)</title>
		<meeting>the 5th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE)<address><addrLine>Abu Dhabi, United Arab Emirates (Hybrid</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="100" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</title>
		<author>
			<persName><forename type="first">Dhairya</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Mihael Arcan</surname></persName>
		</author>
		<author>
			<persName><surname>Buitelaar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.deelio-1.8</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.deelio-1.8" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Deep Learning Inside Out</title>
		<meeting>Deep Learning Inside Out<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="70" to="80" />
		</imprint>
	</monogr>
	<note>Enhancing Multiple-Choice Question Answering with Causal Knowledge</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics, Minneapolis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Minimally supervised event causality identification</title>
		<author>
			<persName><forename type="first">Quang</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><surname>Seng Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="294" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">To Test Machine Comprehension, Start by Defining Comprehension</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dunietz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Burnham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akash</forename><surname>Bharadwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Ferrucci</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.701</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.acl-main.701" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7839" to="7859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The BECauSE Corpus 2.0: Annotating Causality and Overlapping Relations</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dunietz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lori</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-0812</idno>
		<ptr target="https://doi.org/10.18653/v1/W17-0812" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Linguistic Annotation Workshop</title>
		<meeting>the 11th Linguistic Annotation Workshop<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Web-based Tool for the Integrated Annotation of Semantic and Syntactic Structures</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Eckart De Castilho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Éva</forename><surname>Mújdricza-Maydt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seid</forename><surname>Muhie Yimam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvana</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anette</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W16-4011" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Language Technology Resources and Tools for Digital Humanities</title>
		<meeting>the Workshop on Language Technology Resources and Tools for Digital Humanities<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="76" to="84" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human Judgements on Causation in French Texts</title>
		<author>
			<persName><forename type="first">Cécile</forename><surname>Grivaz</surname></persName>
		</author>
		<ptr target="http://www.lrec-conf.org/proceedings/lrec2010/pdf/145_Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10)</title>
		<meeting>the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA), Valletta, Malta</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Toward Future Scenario Generation: Extracting Event Causality Exploiting Semantic Relation, Context, and Association Features</title>
		<author>
			<persName><forename type="first">Chikara</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Kloetzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Motoki</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">István</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong-Hoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Kidawara</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1093</idno>
		<ptr target="https://doi.org/10.3115/v1/P14-1093" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers). Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="987" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Answering Binary Causal Questions Through Large-Scale Text Mining: An Evaluation Using Cause-Effect Pairs from Human Experts</title>
		<author>
			<persName><forename type="first">Oktie</forename><surname>Hassanzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debarun</forename><surname>Bhattacharjya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Feblowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kavitha</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Perrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirin</forename><surname>Sohrabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Katz</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/695</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2019/695" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19. International Joint Conferences on Artificial Intelligence Organization</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19. International Joint Conferences on Artificial Intelligence Organization</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5003" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identifying Causal Relations Using Parallel Wikipedia Articles</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hidey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Mckeown</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1135</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-1135" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1424" to="1433" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The mishap at Reichenbach fall: Singular vs. general causation. Philosophical Studies</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">Read</forename><surname>Hitchcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">An International Journal for Philosophy in the Analytic Tradition</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="257" to="291" />
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multilingual Protest News Detection -Shared Task 1, CASE 2021</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Hürriyetoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osman</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erdem</forename><surname>Yörük</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farhana</forename><surname>Ferdousi Liza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ritesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Ratan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.case-1.11</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.case-1.11" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021)</title>
		<meeting>the 4th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021)</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="79" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cross-Context News Corpus for Protest Event-Related Knowledge Base Construction</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Hürriyetoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erdem</forename><surname>Yörük</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osman</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Çagri</forename><surname>Firat Durusan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deniz</forename><surname>Yoltar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burak</forename><surname>Yüret</surname></persName>
		</author>
		<author>
			<persName><surname>Gürel</surname></persName>
		</author>
		<idno type="DOI">10.1162/dint_a_00092</idno>
		<ptr target="https://doi.org/10.1162/dint_a_00092" />
	</analytic>
	<monogr>
		<title level="j">Data Intell</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="308" to="335" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Economic Causal-Chain Search and Economic Indicator Prediction using Textual Data</title>
		<author>
			<persName><forename type="first">Kiyoshi</forename><surname>Izumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hitomi</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Sakaji</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.fnp-1.3" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Financial Narrative Processing Workshop</title>
		<meeting>the 3rd Financial Narrative Processing Workshop<address><addrLine>Lancaster, United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="19" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Classifying Argumentative Relations Using Logical Mechanisms and Argumentation Schemes</title>
		<author>
			<persName><forename type="first">Yohan</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seojin</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<ptr target="https://transacl.org/ojs/index.php/tacl/article/view/2717" />
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="721" to="739" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards Identifying Alternative-Lexicalization Signals of Discourse Relations</title>
		<author>
			<persName><forename type="first">René</forename><surname>Knaebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Stede</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.coling-1.70" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics. International Committee on Computational Linguistics</title>
		<meeting>the 29th International Conference on Computational Linguistics. International Committee on Computational Linguistics<address><addrLine>Gyeongju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="837" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the acquisition of causal knowledge from scientific texts with attribute grammars</title>
		<author>
			<persName><forename type="first">John</forename><surname>Kontos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Sidiropoulou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Applied Expert Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="31" to="48" />
			<date type="published" when="1991">1991. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Computing Krippendorff&apos;s alpha-reliability</title>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Krippendorff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<ptr target="http://arxiv.org/abs/1909.11942" />
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">David</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Causation. The journal of philosophy</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="556" to="567" />
			<date type="published" when="1974">1974. 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The cement of the universe: A study of causation</title>
		<author>
			<persName><forename type="first">John</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mackie</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>Clarendon Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Annotating Causality in the TempEval-3 Corpus</title>
		<author>
			<persName><forename type="first">Paramita</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachele</forename><surname>Sprugnoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Tonelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuela</forename><surname>Speranza</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-0702</idno>
		<ptr target="https://doi.org/10.3115/v1/W14-0702" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language (CAtoCL)</title>
		<meeting>the EACL 2014 Workshop on Computational Approaches to Causality in Language (CAtoCL)<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="10" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CaTeRS: Causal and Temporal Relation Scheme for Semantic Annotation of Event Structures</title>
		<author>
			<persName><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alyson</forename><surname>Grealish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-1007</idno>
		<ptr target="https://doi.org/10.18653/v1/W16-1007" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Events. Association for Computational Linguistics</title>
		<meeting>the Fourth Workshop on Events. Association for Computational Linguistics<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="51" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">seqeval: A Python framework for sequence labeling evaluation</title>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Nakayama</surname></persName>
		</author>
		<ptr target="https://github.com/chakki-works/seqeval" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adversarial NLI: A New Benchmark for Natural Language Understanding</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.441</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.acl-main.441" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4885" to="4901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fine-Grained Error Analysis and Fair Evaluation of Labeled Spans</title>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Ortmann</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.lrec-1.150" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Language Resources and Evaluation Conference</title>
		<meeting>the Thirteenth Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1400" to="1407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The Penn Discourse Tree-Bank 2.0</title>
		<author>
			<persName><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><forename type="middle">L</forename><surname>Webber</surname></persName>
		</author>
		<ptr target="http://www.lrec-conf.org/proceedings/lrec2008/summaries/754.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation</title>
		<meeting>the International Conference on Language Resources and Evaluation<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-05-26">2008. 26 May -1 June 2008</date>
		</imprint>
	</monogr>
	<note>European Language Resources Association</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The penn discourse treebank 1.0 annotation manual</title>
		<author>
			<persName><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><forename type="middle">L</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">IRCS Technical Reports Series</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">TimeML: Robust Specification of Event and Temporal Expressions in Text</title>
		<author>
			<persName><forename type="first">James</forename><surname>Pustejovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Castaño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roser</forename><surname>Ingria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Saurí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Setzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New Directions in Question Answering, Papers from 2003 AAAI Spring Symposium</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Maybury</surname></persName>
		</editor>
		<meeting><address><addrLine>Stanford, CA, USA, Mark</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="28" to="34" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning causality for news events prediction</title>
		<author>
			<persName><forename type="first">Kira</forename><surname>Radinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sagie</forename><surname>Davidovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
		<idno type="DOI">10.1145/2187836.2187958</idno>
		<ptr target="https://doi.org/10.1145/2187836.2187958" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st World Wide Web Conference</title>
		<editor>
			<persName><forename type="first">Alain</forename><surname>Mille</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fabien</forename><surname>Gandon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jacques</forename><surname>Misselis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Rabinovich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Steffen</forename><surname>Staab</surname></persName>
		</editor>
		<meeting>the 21st World Wide Web Conference<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012-04-16">2012. 2012. 2012. April 16-20, 2012</date>
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mining the web to predict future events</title>
		<author>
			<persName><forename type="first">Kira</forename><surname>Radinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<idno type="DOI">10.1145/2433396.2433431</idno>
		<ptr target="https://doi.org/10.1145/2433396.2433431" />
	</analytic>
	<monogr>
		<title level="m">Sixth ACM International Conference on Web Search and Data Mining, WSDM 2013</title>
		<editor>
			<persName><forename type="first">Stefano</forename><surname>Leonardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alessandro</forename><surname>Panconesi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paolo</forename><surname>Ferragina</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aristides</forename><surname>Gionis</surname></persName>
		</editor>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013-02-04">2013. February 4-8, 2013</date>
			<biblScope unit="page" from="255" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/20-074.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Know What You Don&apos;t Know: Unanswerable Questions for SQuAD</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2124</idno>
		<ptr target="https://doi.org/10.18653/v1/P18-2124" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
	<note>Melbourne, Australia</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
		<ptr target="https://doi.org/10.18653/v1/D16-1264" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Text Chunking using Transformation-Based Learning</title>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitch</forename><surname>Marcus</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/W95-0107" />
	</analytic>
	<monogr>
		<title level="m">Third Workshop on Very Large Corpora</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Toward a better understanding of causality between verbal events: Extraction and analysis of the causal power of verbverb associations</title>
		<author>
			<persName><forename type="first">Mehwish</forename><surname>Riaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGDIAL 2013 Conference</title>
		<meeting>the SIGDIAL 2013 Conference</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Automatically Generating Cause-and-Effect Questions from Passages</title>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Stasaski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manav</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marti</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.bea-1.17" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the 16th Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="158" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Onur Uca, Farhana Ferdousi Liza, and Nelleke Oostdijk. 2022a. Event Causality Identification with Causal News Corpus -Shared Task 3, CASE 2022</title>
		<author>
			<persName><forename type="first">Fiona</forename><surname>Anting Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hansi</forename><surname>Hettiarachchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Hürriyetoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Caselli</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.case-1.28" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE)</title>
		<meeting>the 5th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE)<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates (Hybrid</publisher>
			<biblScope unit="page" from="195" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Onur Uca, Surendrabikram Thapa, and Farhana Ferdousi Liza</title>
		<author>
			<persName><forename type="first">Fiona</forename><surname>Anting Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hansi</forename><surname>Hettiarachchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Hürriyetoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelleke</forename><surname>Oostdijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE). Association for Computational Linguistics</title>
		<meeting>the 6th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE). Association for Computational Linguistics<address><addrLine>Varna; Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Event Causality Identification -Shared Task 3, CASE 2023. Hybrid</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Onur Uca, Farhana Ferdousi Liza, and Tiancheng Hu. 2022b. The Causal News Corpus: Annotating Causal Relations in Event Sentences from News</title>
		<author>
			<persName><forename type="first">Fiona Anting</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Hürriyetoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelleke</forename><surname>Oostdijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tadashi</forename><surname>Nomoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hansi</forename><surname>Hettiarachchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iqra</forename><surname>Ameer</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.lrec-1.246" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Language Resources and Evaluation Conference. European Language Resources Association</title>
		<meeting>the Thirteenth Language Resources and Evaluation Conference. European Language Resources Association<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="2298" to="2310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">The penn discourse treebank 3.0 annotation manual</title>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
		<ptr target="https://doi.org/10.18653/v1/N18-1101" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization</title>
		<author>
			<persName><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v119/zhang20ae.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">2020. 13-18 July 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="11328" to="11339" />
		</imprint>
	</monogr>
	<note>Virtual Event (Proceedings of Machine Learning Research</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
