<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Borderless Azerbaijani Processing: Linguistic Resources and a Transformer-based Approach for Azerbaijani Transliteration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Reihaneh</forename><surname>Zohrabi</surname></persName>
							<email>zohrabi@sharif.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">IR AI Innovation Center</orgName>
								<orgName type="department" key="dep2">Data:Lab</orgName>
								<orgName type="laboratory">Language Processing and Digital Humanities Laboratory Computer Engineering Department</orgName>
								<orgName type="institution" key="instit1">Sharif University of Technology</orgName>
								<orgName type="institution" key="instit2">Volkswagen AG</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mostafa</forename><surname>Masumi</surname></persName>
							<email>m.masumi@sharif.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">IR AI Innovation Center</orgName>
								<orgName type="department" key="dep2">Data:Lab</orgName>
								<orgName type="laboratory">Language Processing and Digital Humanities Laboratory Computer Engineering Department</orgName>
								<orgName type="institution" key="instit1">Sharif University of Technology</orgName>
								<orgName type="institution" key="instit2">Volkswagen AG</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Omid</forename><surname>Ghahroodi</surname></persName>
							<email>omid.ghahroodi98@sharif.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">IR AI Innovation Center</orgName>
								<orgName type="department" key="dep2">Data:Lab</orgName>
								<orgName type="laboratory">Language Processing and Digital Humanities Laboratory Computer Engineering Department</orgName>
								<orgName type="institution" key="instit1">Sharif University of Technology</orgName>
								<orgName type="institution" key="instit2">Volkswagen AG</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Parham</forename><surname>Abedazad</surname></persName>
							<email>parhamabedazad@sharif.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">IR AI Innovation Center</orgName>
								<orgName type="department" key="dep2">Data:Lab</orgName>
								<orgName type="laboratory">Language Processing and Digital Humanities Laboratory Computer Engineering Department</orgName>
								<orgName type="institution" key="instit1">Sharif University of Technology</orgName>
								<orgName type="institution" key="instit2">Volkswagen AG</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hamid</forename><surname>Beigy</surname></persName>
							<email>beigy@sharif.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">IR AI Innovation Center</orgName>
								<orgName type="department" key="dep2">Data:Lab</orgName>
								<orgName type="laboratory">Language Processing and Digital Humanities Laboratory Computer Engineering Department</orgName>
								<orgName type="institution" key="instit1">Sharif University of Technology</orgName>
								<orgName type="institution" key="instit2">Volkswagen AG</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Rohban</surname></persName>
							<email>rohban@sharif.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">IR AI Innovation Center</orgName>
								<orgName type="department" key="dep2">Data:Lab</orgName>
								<orgName type="laboratory">Language Processing and Digital Humanities Laboratory Computer Engineering Department</orgName>
								<orgName type="institution" key="instit1">Sharif University of Technology</orgName>
								<orgName type="institution" key="instit2">Volkswagen AG</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ehsaneddin</forename><surname>Asgari</surname></persName>
							<email>asgari@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">IR AI Innovation Center</orgName>
								<orgName type="department" key="dep2">Data:Lab</orgName>
								<orgName type="laboratory">Language Processing and Digital Humanities Laboratory Computer Engineering Department</orgName>
								<orgName type="institution" key="instit1">Sharif University of Technology</orgName>
								<orgName type="institution" key="instit2">Volkswagen AG</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Borderless Azerbaijani Processing: Linguistic Resources and a Transformer-based Approach for Azerbaijani Transliteration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F1B3FF7FCBC83EA9030D5F4406C3A609</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T14:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advancements in neural language models have revolutionized natural language understanding. However, many languages still face the risk of being left behind without the benefits of such advancements, potentially leading to their extinction. One such language is Azerbaijani in Iran, which suffers from limited digital resources and a lack of alignment between spoken and written forms. In contrast, Azerbaijani in the Republic of Azerbaijan has seen more resources and is not considered as low-resource as its Iranian counterpart. In this context, our research focuses on the computational progress made in Iranian Azerbaijani language. We propose a transliteration model that leverages an Azerbaijani parallel dataset, effectively bridging the gap between the Latin and Persian scripts. By enabling seamless communication between these two scripts, our model facilitates cultural exchange and serves as a valuable tool for transfer learning. The effectiveness of our approach surpasses traditional rule-based methods, as evidenced by the significant improvements in performance metrics. We observe a minimum 15% increase in BLEU scores and a reduction of at least 1/3 in edit distance. Furthermore, our model's online demo is accessible at https://azeri.parsi.ai/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Azerbaijani language belongs to the Turkish language family and is spoken in two distinct dialects, primarily in the Republic of Azerbaijan and the Azerbaijani regions of Iran. While these dialects exhibit minor variations, they share a considerable linguistic commonality, making it feasible to transition from one dialect to the other by adapting existing letters and phonetic elements. This linguistic compatibility enables seamless communication between the two dialects, facilitating the utilization of their shared linguistic resources.</p><p>? , † Equal contribution Azerbaijani is the official language of the Republic of Azerbaijan. However, according to statistics from the Population and Housing Census conducted by the Statistical Center of Iran<ref type="foot" target="#foot_0">1</ref> in 2015, the population of East and West Azerbaijan, Ardabil, and Zanjan provinces in Iran was approximately 9.5 million. Iranian Azerbaijani, spoken in these regions, is recognized as an ethnic spoken language without an established official writing system, rendering it a low-resource language. Unlike Iranian Azerbaijani, the Azerbaijani language has recieved substantial attention within the field of natural language processing. For instance, <ref type="bibr" target="#b17">Suleymanov et al. (2019)</ref> employs machine learning techniques, including decision trees, support vector machines, and Naive Bayes, to categorize Azerbaijani texts for various applications, such as news classification, sentiment analysis, and recommender systems. Moreover, <ref type="bibr" target="#b1">Akhundova (2021)</ref> introduces models that incorporate both rule-based and machine learning approaches for Azerbaijani named entity recognition. Additionally, while Azerbaijani benefits from a repository<ref type="foot" target="#foot_1">2</ref> dedicated to collecting data and computing resources, which greatly simplifies computational tasks related to the language, no similar resources or initiatives had existed for Iranian Azerbaijani until the recent pioneering work <ref type="bibr" target="#b9">(Marzia et al., 2023)</ref>. This innovative research effort represents the initial and significant step toward establishing essential NLP resources for Iranian Azerbaijani, encompassing the development of standard datasets and starter models for various NLP tasks. It plays a pivotal role in preserving the language and culture of Iranian Azerbaijani. In this research, we have developed a transliteration model bridging the Iranian and Azerbaijani variants, facilitating the processing of Iranian multilingual texts. We refer to the Iranian variant of Azerbaijani spoken in Iran and the Azerbaijani variant spoken in Azerbaijan. Due to the scarcity of linguistic resources and prior computational work for the Iranian variant, challenges arise when processing this language. There is a lack of pre-processing tools and pre-trained language models, even within extensive multilingual resources like Facebook's FastText <ref type="bibr" target="#b4">(Bojanowski et al., 2017)</ref>. For the Azerbaijani variant, there are existing works <ref type="bibr">(Huseynov et al., 2021)</ref> that have utilized word2vec <ref type="bibr" target="#b10">(Mikolov et al., 2013)</ref> and Glove <ref type="bibr" target="#b13">(Pennington et al., 2014)</ref> for vocabulary representation. Furthermore, the existence of different scripts for the Iranian variant has led to data source multiplicity, necessitating script unification through pre-processing. "Suzelrin Vahid Yazilishi: Calligraphy Style and Orthographic Culture of Azerbaijani Turkish (Consistent Spelling of Words)" describes the currently approved Iranian variant of the Azerbaijani script (Perso-Arabic script). Our primary contributions include: I. The creation of a parallel dataset encompassing both Iranian and Azerbaijani variants of Azerbaijani. II. A thorough analysis of existing transliteration tools for Iranian Azerbaijani. III. The development of a transliteration model, applied in <ref type="bibr" target="#b9">(Marzia et al., 2023)</ref>, to enhance resources for Iranian Azerbaijani. We facilitate access to our dataset and code on GitHub and Hugging Face via the provided links<ref type="foot" target="#foot_2">3</ref> ,<ref type="foot" target="#foot_3">4</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Considering the significance of transliteration between the two variants of the Azerbaijani language, various efforts have been made to address this challenge, exemplified by tools like AzConvert<ref type="foot" target="#foot_4">5</ref> and the Azalpha plugin. However, these prior approaches rely on predefined rules and exhibit limitations in terms of accuracy, adherence to standard scripts, and contextual awareness. The rule-based methodologies employed can become overly intricate due to the inherent ambiguities across languages, with one such challenge being homographs. In languages like Azerbaijani, homographs can introduce errors in rule-based transliteration. Fur-thermore, the lack of adherence to standard scripts in numerous existing Iranian Azerbaijani sources renders rule-based methods unsuitable for handling such content. There are other works for transliteration between different languages, such as polyglot<ref type="foot" target="#foot_5">6</ref>  <ref type="bibr" target="#b5">(Chen and Skiena, 2016)</ref>, a natural language processing tool with various applications, including transliteration between different languages. This tool can support transliteration between 69 different languages, including Azerbaijani and Persian, but there is no such tool for Iranian Azerbaijani. Other studies focus on transliteration between closely related languages, which we categorize into three groups:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Rule-based methods:</head><p>In these methods, transliteration is accomplished through the application of pre-defined rules. <ref type="bibr" target="#b3">Bhalla et al. (2013)</ref> employs a rule-based model for syllabification and statistical techniques, specifically for translating English into Punjabi. Similarly, Ahmadi (2019) adopts a rule-based approach for converting the two primary written systems of the Surani Kurdish language (Middle Kurdish) into each other. This is achieved by identifying characters in words, resolving potential ambiguities, and mapping them to the target text. Moreover, <ref type="bibr" target="#b11">Oh and Choi (2002)</ref> utilizes pronunciation and content rules for the transliteration process, specifically from English to Korean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">PGM methods:</head><p>These methods typically employ probabilistic modeling techniques, aiming to maximize the probability of parallel word pairs between source and target languages. For instance, in the work of <ref type="bibr" target="#b14">Pingali et al. (2008)</ref>, a sophisticated statistical transliteration approach is introduced. Remarkably, this technique stands out for its language-independence, making it applicable to a wide range of language pairs. In the initial phase, it leverages hidden Markov model alignment, a powerful tool for capturing linguistic patterns and associations. Subsequently, in the next phase, the approach employs conditional random fields, further enhancing its transliteration capabilities. This multi-phase approach not only contributes to the robustness of the transliteration model but also allows for adaptability across various language </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Neural network methods:</head><p>These methods include models based on LSTM and transformers. <ref type="bibr" target="#b16">(Shao and Nivre, 2016)</ref> is an example of using neural networks for transliteration between English and Chinese. In this paper, the network architecture includes a convolutional layer to extract character-level information and a recurrent layer to process the text. Mahdi Mahsuli and Safabakhsh (2017) uses the encoder-decoder approach with the attention mechanism for transliteration from English to Persian, where instead of randomly initializing the weights of the encoder network, they use the vector representation of the words of the source language as the initial value for the weights. In general, sequence-to-sequence models with an attention mechanism use a recurrent neural network encoder to learn input text representations and a recurrent neural network decoder to generate output sequences from hidden representations created by the encoder. The attention mechanism <ref type="bibr" target="#b2">(Bahdanau et al., 2014)</ref> allows the decoder to focus on different input parts for each time step in the output sequence. It can be seen as similar to the alignment mechanism used in traditional statistical translation models. <ref type="bibr" target="#b15">Rosca and Breuel (2016)</ref> also uses this type of model with LSTM and GRU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data collection and preprocessing</head><p>Training of the transliteration model necessitates parallel datasets in both the source and target languages. However, due to the unavailability of such a dataset for processing purposes, we collaborated with the Azerbaijani Turkish Language Department of the University of Tabriz (Faculty of Persian and Foreign Languages). Through their cooperation, we were able to gather sources containing parallel texts in both Latin and Persian</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latin Iranian</head><p>Example scripts. Our primary contribution lies in generating resources for this low-resource language. These sources encompass a portion of the Azerbaijani epic 'Koroglu' written in both languages and utilizing the standard Azerbaijani script. Additionally, for out-of-domain evaluation, we obtained the Azerbaijani folklore tale 'Qurqud D@d@' written in both scripts, with the assistance of the aforementioned group.Additionally, we obtained the 'Azerbaycan sözlügü' (Esmayil Jafarli, 2013) dictionary, which comprises 120,000 original words, Azerbaijani translations, place names, and the names of renowned individuals and poets, compiled in three PDF volumes.</p><formula xml:id="formula_0">ü ⇠  Gün- ‡ ⇠ Ò√ u  vurdu-XP o  yol-» ÒK ⌦ ö  Göz-P Ò√ I ⇠ ¯baxdı-⇠ ¯Y gAK . I ¯bir-Q ⌦ K . e ¯dedi-¯Y KX</formula><p>After collecting the data above, we performed a series of word extraction and pre-processing operations to ensure that the words closely align with the standard script. The pre-processing procedures applied to the data typically adhered to the subsequent steps illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>In the data normalization stage, we carefully assessed specific conditions to ensure the accurate substitution of Iranian Azerbaijani vowel characters with their corresponding Latin counterparts. It is important to note that the Iranian Azerbaijani language has multiple vowel characters that resemble the character "" (such as " ", " ⇠ " and " "). However, in numerous existing sources, these vowels are often represented by the initial form, leading to inaccuracies in the written Iranian Azerbaijani words. Therefore, we implemented pre-processing techniques to precisely replace the vowels based on their Latin forms, thereby enhancing the accuracy of the word representations. The table 1 shows vowel characters of the Iranian Azerbaijani language that must be observed in the standard script and may be used incorrectly. We have summarized the information from the final dataset in To train the transliteration model, the data was divided into 5 clusters based on character patterns (using n-grams with lengths of 2 to 6). Clustering helped create distinct training and validation sets by reducing similarity between sub-words. Mean and standard deviation analysis assessed model overfitting and stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model</head><p>The Transformer model, known for its powerful architecture in natural language processing tasks, including transliteration, has shown exceptional performance in capturing long-range dependencies. This makes it particularly suitable for transliteration tasks involving languages like Iranian and Azerbaijan variants of the Azerbaijani language. It employs a self-attention mechanism that allows the model to focus on different parts of the input sequence, capturing contextual relationships crucial for accurate transliteration. The Transformer model consists of an encoderdecoder architecture. The encoder generates a representation capturing relevant information from the source language sequence, while the decoder uses this representation to generate the transliterated output sequence. By utilizing positional encodings, multiple stacked layers, and large-scale parallel data during training, the Transformer model effectively learns the mappings between input and output sequences.</p><p>Given its ability to handle sequence-to-sequence tasks and capture long-range dependencies, the Transformer model has become a prominent choice for transliteration tasks across various languages.</p><p>In our research on two variants of Azerbaijani transliteration, we employ the Transformer model as the primary framework. Our model operates at the character level, where each character serves as an individual token. Token embeddings are initialized with random values to capture a wide range of linguistic nuances.</p><p>To assess the performance of the trained model, we measure the BLEU score <ref type="bibr" target="#b12">(Papineni et al., 2002)</ref> and Levenshtein distance. During training, crossvalidation is employed, with one cluster serving as the test set and the others as training and validation data. A ten percent subset is allocated for validation. The optimal hyperparameters for training are determined based on this setup, with further details provided in the appendix 1.2. We calculated the mean length in a collection of prefixes, suffixes, and morphemes in Iranian Azerbaijani to be near 3. We thus opted to calculate the BLEU score using an n-gram level of 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The comparison results presented in ric. The model's character-level output exhibits a high level of accuracy, requiring minimal edits compared to the existing models. Additionally, the low standard deviation of the BLEU score indicates the model's stability during training, even when faced with variations in subword patterns across different cross-validation folds. The consistently high BLEU scores across all categories, coupled with excellent subword discrimination, indicate that the model effectively avoids overfitting on the training data. A detailed analysis of the trained models and their corresponding outputs is presented in the appendix 1.3 for further examination and understanding.</p><p>For out-of-domain evaluation, we employed the 'Qurqud D@d@' dataset and also utilized ChatGPT's predictions to gain additional perspectives. The evaluation results revealed that there was no significant drop in the BLEU score. In fact, for Persianto-Latin conversion, the BLEU score increased by 2 points, while for Latin-to-Persian conversion, it decreased by just 3 points. These findings indicate that the model's performance remains robust. Detailed results can be found in Table <ref type="table" target="#tab_5">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The rapid growth of language technologies emphasizes the importance of linguistic resources and computational tasks for endangered languages like  performance was unsatisfactory. Furthermore, this model operates at a slower speed compared to other models.</p><formula xml:id="formula_1">⇣ áJ ⌦ À'¯QK ⌦ @ ¡J ⌦ KQK ⌦ @ ⇣ á ⇠ J ⇠ KQK ⌦ @</formula><p>The Azconvert model: This model does not have some Iranian Azerbaijani vowels such as " ⇠ " and " ⇠ ¯" which can be seen in table 8 for the words "-P ⇠ Ò K ⇠ Ò ⌘ É ⇠ X", " ⇠ ¯Y gAK . " and " ⇠ XP Ò√". One of the advantages of this rule-based model is that it uses more appropriate writing for better readability; for example, for the word " ⇣ á ⇠ J ⇠ KQK ⌦ @", separating the sub-words "¯QK ⌦ @" (separate) and " ⇣ áJ ⌦ À" (infinitive noun suffix) helps for its readability. This method does the same for this word.</p><p>The Transformer model: The transformer model can produce a variety of consonant and vowel patterns due to various learning data from the dictionary. Therefore, the trained model is very consistent with the standard script and can recognize words that are similar in appearance. However, sometimes it does not correctly recognize the spelling of ambiguous words that it did not see in the training, such as "…J ⌦ É@" in the second line of table 9, which should be "…J ⌦ ì@".</p><p>It is important to highlight that the trained model can also be utilized for the transliteration of entire sentences since there is a one-to-one correspondence between words in both languages. Examples of sentence transliterations are illustrated in tables 9 and 10.</p><p>Latin Persian türkün dili t@k sevgili ist@kli dil olmaz PA÷oe @ …K ⌦ X ˙OEae ⇣ JÇ ⌦ @ ˙OEJ ⌦ √Ò JÉ π ⇣ K ˙OEK ⌦ X ‡ ⇠ ÒªP ⇠ Ò ⇣ K özg@ dili qatsan bu @sil dil @sil olmaz PA÷oe @ …J ⌦ É@ …K ⌦ X …J ⌦ É@ ÒK . ‡AÇ ⇣ A ⇣ Ø ˙OEK ⌦ X È√ P @ h@r k@sin fikir v@ söz azadligi vardir QK ⌦ XP@  ˘ ™J ⌦ ÀX@ P @ P ÒÉ Ë Q∫ Ø • ⌦ Çª QÎ nec@ görs@nir Q ⌦ JÓDÖP Ò√ Èj . J K m@n onu dün@n gönd@rdim 'Á' ⌦ XPY K Ò√ • K ⇠ X Ò K @ •" </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Persian Latin</head><p>QK ⌦ X È KAÇ Ø@ ‡ ⇠ Ò ⇣ K ⇠ ÒK . AJ ⌦ K ⇠ X AÉA÷oe @ ⇣ á ⌘ Ç´'Ê ⌦ J ⌦ √Ò JÉ sevgilim @ş@q olmasa dünya bütün @fsan@ dir p ÒK ⌦ AK ⌦ P@  QÀ Q . g P Ò√ gör x@b@rl@r var ya yox •É Q ⌦ J ⌦ KX P ⇠ X duz deyirs@n QK ⌦ YÀ P Ò√ p Òk ✓ çox göz@ldir </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of Preprocessing Steps Applied to the Dataset</figDesc><graphic coords="3,70.87,70.87,453.54,56.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Potentially misused vowel characters of Iranian Azerbaijani.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>, showcasing the count of parallel word pairs available in both languages. Additionally, you can find sample examples from this dataset in Appendix 1.1. In Iranian variant, the average token length is 7.34 characters, while in Azerbaijan variant, it is 7.73 characters. These word pairs serve as valuable resources for training our transliteration model. It's important to emphasize that the limited availability of linguistic resources for this language significantly hinders the applicability of data augmentation techniques.</figDesc><table><row><cell></cell><cell cols="2">all tokens after pre-processing</cell></row><row><cell>Dictionary</cell><cell>120000</cell><cell>72000</cell></row><row><cell>Koroglu</cell><cell>700</cell><cell>500</cell></row><row><cell>Total</cell><cell>120700</cell><cell>72500</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Summary of the number of Parallel Word Pairs in the Final Azerbaijani Dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>high-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of Transliteration Methods: Persian to Latin and Latin to Persian Scripts. The table presents the minimum edit distance, average word length, and BLEU score for each method. Performance of the Transformer method was evaluated using five folds in a cross-validation setup, and the mean and standard deviation are reported.</figDesc><table><row><cell></cell><cell></cell><cell>Persian to Latin</cell><cell></cell><cell></cell><cell>Latin to Persian</cell><cell></cell></row><row><cell>Method</cell><cell cols="6">Min. Edit Dist. d Avg. Len. BLEU Min. Edit Dist Avg. Len. BLEU</cell></row><row><cell>Transformer</cell><cell>0.17</cell><cell>5.2</cell><cell>0.96</cell><cell>0.32</cell><cell>5.3</cell><cell>0.91</cell></row><row><cell>polyglot</cell><cell>2.34</cell><cell>5.2</cell><cell>0.21</cell><cell>2.09</cell><cell>5.3</cell><cell>0.51</cell></row><row><cell>Azconvert</cell><cell>0.64</cell><cell>5.2</cell><cell>0.83</cell><cell>0.96</cell><cell>5.3</cell><cell>0.71</cell></row><row><cell>ChatGPT</cell><cell>1.49</cell><cell>5.2</cell><cell>0.66</cell><cell>1.28</cell><cell>5.3</cell><cell>0.65</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Out-of-Domain Comparison of Transliteration Methods: Persian to Latin and Latin to Persian Scripts. The table presents the minimum edit distance, average word length, and BLEU score for each method on the out-domain data.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Sample Outputs of Transliteration Models for transliterating Azerbaijani Words between two variants.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Sample Outputs of Azerbaijani Sentences transliterated from Latin to Persian Script.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Sample Outputs of Azerbaijani Sentences transliterated from Persian to Latin Script.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.amar.org.ir/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/alexeyev/ awesome-Azerbaijani-nlp</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/language-ml/ Borderless-Azerbaijani-Processing</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://huggingface.co/datasets/ language-ml-lab/parallel_azeri_dataset</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://github.com/mousamk/azconvert</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://github.com/aboSamoor/polyglot</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix 1.1 Examples of Parallel Azerbaijani Dataset</head><p>Table <ref type="table">5</ref> presents several examples from the prepared dataset, illustrating parallel word pairs in both variants of the Azerbaijani language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latin</head><p>Iranian </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Hyperparameters</head><p>The model training process is optimized with the following set of hyperparameters provided in table 6. Also, table 7 provides the test parameters used to calculate the minimum edit distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Model Output Analysis and Review</head><p>The table <ref type="table">8</ref> showcases examples of transliteration model outputs for words between the Latin and Iranian scripts. The correct spellings of the words are presented in the first two columns, while the corresponding model outputs are displayed in the subsequent columns. Analyzing these outputs allows us to identify the strengths and weaknesses of each Azerbaijani transliterator.</p><p>The Polyglot model: Since there is no direct mapping between the Latin and the Iranian scripts, we have employed a linguistic bridge in this model. Consequently, the model does not generate special Azerbaijani vowels. Substituting unfamiliar vowels with Persian counterparts often results in unfamiliar words. Even for common words shared between Azerbaijani and Persian, like the word "©J ⌦ ØQ ⇣ K" in table 8, the model's</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A rule-based kurdish text transliteration system</title>
		<author>
			<persName><forename type="first">Sina</forename><surname>Ahmadi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3278623</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Asian Low-Resour. Lang. Inf. Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Named entity recognition for the azerbaijani language</title>
		<author>
			<persName><forename type="first">Natavan</forename><surname>Akhundova</surname></persName>
		</author>
		<idno type="DOI">10.1109/AICT52784.2021.9620336</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 15th International Conference on Application of Information and Communication Technologies (AICT)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rule based transliteration scheme for english to punjabi</title>
		<author>
			<persName><forename type="first">Deepti</forename><surname>Bhalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nisheeth</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iti</forename><surname>Mathur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1307.4300</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">False-friend detection and entity matching via unsupervised transliteration</title>
		<author>
			<persName><forename type="first">Yanqing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06722</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Azerbaijani Türkçe Sözlügü (Azerbaijani Sözlügü) -Turkish Dictionary</title>
		<author>
			<persName><forename type="first">Esmayil</forename><surname>Jafarli</surname></persName>
		</author>
		<ptr target="https://literature.tabrizu.ac.ir/en" />
	</analytic>
	<monogr>
		<title level="m">Ehrar Publications in cooperation with Sumer Publishing, Tabriz. Faculty of Persian and Foreign Languages. Faculty of Persian and Foreign Languages</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Samir Rustamov, and Javid Huseynov. 2021. Training and evaluation of word embedding models for azerbaijani language</title>
		<author>
			<persName><forename type="first">Kamran</forename><surname>Huseynov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umid</forename><surname>Suleymanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Interaction and Machine Intelligence</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">English to persian transliteration using attentionbased approach in deep learning</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Mahdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahsuli</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Safabakhsh</surname></persName>
		</author>
		<idno type="DOI">10.1109/IranianCEE.2017.7985375</idno>
	</analytic>
	<monogr>
		<title level="m">2017 Iranian Conference on Electrical Engineering (ICEE)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="174" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The language model, resources, and computational pipelines for the underresourced iranian azerbaijani</title>
		<author>
			<persName><forename type="first">Nouri</forename><surname>Marzia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amani</forename><surname>Mahsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zohrabi</forename><surname>Reihaneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsaneddin</forename><surname>Asgari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: AACL-IJCNLP 2023</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An English-Korean transliteration model using pronunciation and contextual rules</title>
		<author>
			<persName><forename type="first">Jong-Hoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Key-Sun</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2002: The 19th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statistical transliteration for cross language information retrieval using HMM alignment model and CRF</title>
		<author>
			<persName><forename type="first">Prasad</forename><surname>Pingali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreeharsha</forename><surname>Yella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasudeva</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd workshop on Cross Lingual Information Access (CLIA) Addressing the Information Need of Multilingual Societies</title>
		<meeting>the 2nd workshop on Cross Lingual Information Access (CLIA) Addressing the Information Need of Multilingual Societies</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Rosca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09565</idno>
		<title level="m">Sequence-tosequence neural network models for transliteration</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Applying neural networks to English-Chinese named entity transliteration</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-2710</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Named Entity Workshop</title>
		<meeting>the Sixth Named Entity Workshop<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="73" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Text classification for azerbaijani language using machine learning and embedding</title>
		<author>
			<persName><forename type="first">Umid</forename><surname>Suleymanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiani</forename><surname>Behnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elkhan</forename><surname>Kalejahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashid</forename><surname>Amrahov</surname></persName>
		</author>
		<author>
			<persName><surname>Badirkhanli</surname></persName>
		</author>
		<idno>CoRR, abs/1912.13362</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
