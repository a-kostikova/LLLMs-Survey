<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">date selection method would benefit from being 576</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mrinal</forename><surname>Rawat</surname></persName>
							<email>mrinal.rawat@uniphore.com</email>
						</author>
						<author>
							<persName><forename type="first">Hithesh</forename><surname>Sankararaman</surname></persName>
							<email>hithesh.sankararaman@uniphore.com</email>
						</author>
						<author>
							<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><surname>Aaron</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yonglong</forename><surname>Sarna</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Phillip</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aaron</forename><surname>Isola</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ce</forename><surname>Maschinot</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dilip</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><surname>Krishnan</surname></persName>
						</author>
						<author>
							<persName><surname>Su</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stefan</forename><surname>Larson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anish</forename><surname>Mahendran</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><forename type="middle">J</forename><surname>Peper</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Leach</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ting-En</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hua</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hanlei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><surname>Dis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Neal</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
						</author>
						<author>
							<persName><surname>Catas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Polanyi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Amartya</forename><forename type="middle">2009</forename><surname>Sen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">793 Controllable Discovery of Intents: Incremental Deep Clustering Using Semi-Supervised Contrastive Learning</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">650 Computational Linguistics</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>655 Christopher Clarke, Parker Hill</addrLine>
									<settlement>Andrew Lee</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">671 AAAI Conference on Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Ikujirō Nonaka and Hirotaka Takeuchi</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">683 dimension</orgName>
								<orgName type="institution">University of Chicago press</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">date selection method would benefit from being 576</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CB4051F81754392EF568391AB70210C4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deriving value from a conversational AI system depends on the capacity of a user to translate the prior knowledge into a configuration. In most cases, discovering the set of relevant turn-level speaker intents is often one of the key steps. Purely unsupervised algorithms provide a natural way to tackle discovery problems but make it difficult to incorporate constraints and only offer very limited control over the outcomes. Previous work has shown that semisupervised (deep) clustering techniques can allow the system to incorporate prior knowledge and constraints in the intent discovery process. However they did not address how to allow for control through human feedback. In our Controllable Discovery of Intents (CDI) framework domain and prior knowledge are incorporated using a sequence of unsupervised contrastive learning on unlabeled data followed by finetuning on partially labeled data, and finally iterative refinement of clustering and representations through repeated clustering and pseudolabel fine-tuning. In addition, we draw from continual learning literature and use learningwithout-forgetting to prevent catastrophic forgetting across those training stages. Finally, we show how this deep-clustering process can become part of an incremental discovery strategy with human-in-the-loop. We report results on both CLINC and BANKING datasets. CDI outperforms previous works by a significant margin: 10.26% and 11.72% respectively.</p><p>Customer: Hello, I recently received my new credit card, and I'd like to activate it. Agent: Hi there! Could you please provide me with your card number for verification? Customer: Here's my card number : XXXX-XXXX-XXXX-XXXX Agent: Great, thank you for providing you details. Your card is now successfully activated. Is there anything else I can assist you with today? Customer: Yes, actually. I was trying to transfer money to a beneficiary, but I received a message that the beneficiary is not allowed. Can you help me understand why? Agent : Of course, I'd be happy to help. To better assist you, could you please provide me with the beneficiary's account number and the error message you received? Customer: …..</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Conversational AI encompasses human-machine interactions (e.g. voice assistants, self-service bots, ...), speaker assistance during human-human conversations (e.g. customer support agent guidance, speaker coaching, ...), batch analysis of conversations, and many more use cases. Most of those make use of the concept of 'intent' to conceptualize the relevant dimensions at the level of a conversational turn. Getting value out of those systems rests therefore in finding the intent set, i.e. helping them evaluate and reshape their understanding of the intent landscape (that can be sub-optimal in its current form).</p><p>Unsupervised algorithms provide a natural way to tackle such problems (Chatterjee and Sengupta, 2020; <ref type="bibr" target="#b0">Benayas et al., 2023)</ref>. Purely unsupervised algorithms however suffer from the fact that they lack the capacity to incorporate prior knowledge and do not offer any control over the outcome (beyond the setting of certain hyper-parameters). The objective therefore is to provide a tool that helps align an intent set with business needs. This tool should facilitate at a minimum: (1) the incorporation of domain knowledge, including the specification of required intents, and (2) the efficient intervention of an expert to guide the system toward relevant solutions.</p><p>Previous work has shown how using a combination of contrastive learning, fine-tuning, and semisupervised learning in addition to (deep) clustering allows the system to learn to incorporate prior knowledge and constraints <ref type="bibr" target="#b17">(Zhang et al., 2021;</ref><ref type="bibr" target="#b8">Shen et al., 2021)</ref>. A parallel line of research has focused on using human-in-the-loop approaches to iteratively incorporate human feedback <ref type="bibr" target="#b11">(Williams et al., 2015)</ref>. To our knowledge, however, no work so far has looked into combining all those elements into a single architecture.</p><p>We present a novel approach to intent discovery that satisfies the 3 requirements mentioned above.</p><p>Our contributions can be summarized as follows:</p><p>• We show how domain and prior knowledge can be incorporated using a sequence of unsupervised contrastive learning on unlabeled data followed by fine-tuning on par-  or back-translation <ref type="bibr" target="#b5">(Fang et al., 2020)</ref> can also be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>As shown in Figure <ref type="figure" target="#fig_1">2</ref> we begin with a domain adaptation step, using unsupervised contrastive learning (UCL) to adapt a sentence transformer on the unlabeled dataset. This is followed by a two-stage supervised training approach using the labeled dataset to cluster the unlabelled data and identify new intents. To ensure that the model can continuously learn and adapt to new data, we im-plement the learning without forgetting technique 197 <ref type="bibr">(Li and Hoiem, 2018)</ref>. This allows the model to  272 </p><formula xml:id="formula_0">L ucl = - i∈N log e sim(h z i i ,h z ′ i i )/τ N j̸ =i e sim(h z i i ,h z ′ j j )/τ<label>(</label></formula><formula xml:id="formula_1">L LwF = - 1 N N i=1 f (h ′ i ).logf (h i ) (3) 273 f (h ′ i ) = e wy i h ′ i K k=1 e wy k h ′ i , f (h i ) = e wy i h i K k=1 e wy k h i (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Controllable Intent Discovery (CDI)</head><p>In this section, we introduce a novel approach for allowing the user to control the intent discovery process. Discovery is done in an incremental manner </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>333</head><p>We hypothesize that real clusters tend to be dense 334 even with a large K ′ , and that the size of more 335 confident clusters is larger than some threshold 336 t <ref type="bibr" target="#b17">(Zhang et al., 2021)</ref>. Hence, we drop the low-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>337</head><p>confidence cluster which has a size smaller than t,</p><formula xml:id="formula_2">338</formula><p>where t is calculated as the expected cluster mean We then present the user with the resulting clusters along with the high-confidence samples sorted by confidence score s i per cluster. Our tool allows them to interactively select or deselect samples within each cluster. The user can also merge similar clusters. At the end of each iteration, we have a labeled dataset D t , and a set of newly identified intents denoted as I t = (i 1 , i 2 , ...i Kt ). We expand the labeled dataset as D L = D L ∪ D t and intent set as I = I ∪ I t respectively and use them to perform the stage 1 and stage 2 training along with the LwF loss to avoid catastrophic forgetting as described in above sections. In the next iteration, if the number of identified intents |I|, exceeds the value of K t , we expand and update K t+1 to be equal to |I|. Otherwise, we keep K t+1 the same as K t and continue this iterative process to discover new intents and label the data. We terminate this process once the value of K t stops increasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We conduct our experiments on two public benchmark intent datasets and one private dataset. Table <ref type="table" target="#tab_1">1</ref> shows the dataset statistics.</p><p>CLINC is a dataset for intent classification <ref type="bibr">(Larson et al., 2019)</ref> that includes 22,500 queries span-798 ning 150 intents in 10 different domains.</p><p>BANKING is a detailed dataset in the banking domain <ref type="bibr" target="#b2">(Casanueva et al., 2020</ref>) that consists of 13,083 queries related to customer service and covers 77 distinct intents.</p><p>TELECOM Dataset is our private dataset which comprises of manually annotated transcripts of human-human spoken telephone conversations from the telecom customer support domain. Transcripts were generated by our in-house Kaldi-based ASR system consisting of several turns between agent and customer. In total, 1513 transcripts were collected, and for each one, our annotators identified the turn in which the caller's intent was expressed and assigned it to one of 16 pre-defined classes. However, this work only considers the intent turns as the input. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>In our work, we conducted a direct comparison between our proposed approach and two other existing methods, namely Deep Aligned Cluster (DAC) <ref type="bibr" target="#b17">(Zhang et al., 2021)</ref> and Supervised Contrastive Learning (SCL) <ref type="bibr" target="#b8">(Shen et al., 2021)</ref>. DAC utilizes a pre-training strategy on a BERT-based backbone with limited known intent data, followed by training on pseudo-labeled data generated through a clustering algorithm. In contrast, SCL uses MPNet as the backbone and trains it on limited known intent data using a Supervised Contrastive loss <ref type="bibr">(Khosla et al., 2020)</ref>. To evaluate the performance of these methods, we ran experiments and reported the results by running their code if it was available, and if not, we implemented their methods based on the description provided in their papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics</head><p>Following established practices in the field, for each experiment, we report the normalized mutual information (NMI), adjusted rand index (ARI), and accuracy (ACC).</p><p>Algorithm 1: Pseudo-code for automatic incremental discovery evaluation     Table <ref type="table">3</ref>: Iteration-wise clustering results for the human-in-the-loop approach on three datasets. K indicates the number of intents discovered, while %_Labeled indicates the percentage of labeled samples until that iteration. We stop reporting the results when K reaches the ground truth intent value. Future work will however need to focus on running real user experiments both to validate our current approach as well as to improve the automatic testing procedure.</p><formula xml:id="formula_3">Input: Unlabeled Dataset D U ,Model trained using UCL M , True_Intents Y 1 I ← ∅; -&gt; Intents 2 t ← 1; 3 D L ← ∅; -&gt; Labeled Dataset 4 K t ← P REDICT _K(M, D U ); 5 while |I| ̸ = |Y | do 6 C t ← GET _CLU ST ERS(K t , M, D U );</formula><formula xml:id="formula_4">I ← I ∪ unique(L); 12 D L ← D L ∪ D(S, L); 13 D U ← D U -D L Remove</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>tially labeled data, and finally iterative refinement of clustering and representations through repeated clustering and pseudo-label fine-tuning. • We show how using the learning-withoutforgetting method from continual learning prevents catastrophic forgetting across those training stages, leading to improved clustering results compared to previous work. • Finally we show how this deep-clustering process can become part of an incremental discovery strategy with human-in-the-loop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our proposed architecture. We begin by training the MPNet model with unsupervised contrastive loss (UCL) on the unlabeled dataset, followed by a two-stage training process along with LwF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>1) 219 for N sentences mini-batch where τ is the tem-220 perature hyper parameter, and sim(h 1 , h 2 ) the number of known intents, w 247 is the classifier weights, h i is the final encoded 248 representation and Y (y 1 , y 2 , ..., y N ) are the labels. 249 Once the training is complete, we remove the 250 classifier layer and utilize the rest of the network 251 as a feature extractor to generate sentence embedwithout Forgetting (LwF) 254 As the model learns from the labeled data, we want 255 to ensure that it does not forget what has been 256 learned during the domain adaptation phase: dis-257 covery of relevant new intent requires the informa-258 tion carried by both the domain and the labeled 259 data to be integrated prior to clustering. The threat 260 of catastrophic forgetting is a well-known threat 261 for transfer learning approaches (McCloskey and 262 Cohen, 1989). To address this problem, Learning 263 without Forgetting (LwF) (Li and Hoiem, 2018) 264 was proposed which aims to preserve the previ-265 ously learned knowledge while learning new tasks. 266 It is inspired by KL-divergence which imposes an 267 additional constraint that the parameters of the net-268 work while learning a new task and the parameters 269 of the old network do not shift significantly. For 270 our work, we adopt the LwF technique and use the 271 following objective:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>274where K is the number of known intents i.e 275 classes, w is the classifier weights, h i is the model 276 output after learning and h ′ i is the old model's out-277 put. 278 Finally, we combine this objective with the clas-279 sification objective: 280 L sup = L ce + λL LwF (5) 281 where λ is the hyper-parameter 3.3 Stage2: Deep-Clustering using Pseudo Labels Training We use the fine-tuned model to generate embeddings for all the turns in the dataset and perform K-means clustering. We assign pseudo-labels to each data point based on the K-means output and use these pseudo-labels for the supervised training of the model. Here also, to prevent catastrophic forgetting, we add an LwF objective to the crossentropy loss. Furthermore, this stage differs significantly from the first stage, where the number of labels or classes also changes which may lead to catastrophic forgetting. Hence, we incorporate the LwF objective alongside cross-entropy to mitigate this issue (see 5). We repeat this clustering + pseudo-labeling training step multiple times. To handle the assignment inconsistency problem -the K-means cluster indices are randomly assigned at each iteration resulting in different labels -we follow the method proposed in DAC (Li and Hoiem, 2018) and employ the Hungarian algorithm (Kuhn, 1955) to align the centroids and obtain the consistent labeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Our proposed architecture for incremental intent discovery via human-in-the-loop involves utilizing a pre-trained model on a labeled dataset for generating representations using unsupervised contrastive learning (UCL). Then we perform K-means, and the user is presented with the clusters for input. The user can provide labeled samples and newly discovered intents by selecting or deselecting samples. Stage-1 and stage-2 training are performed using the labeled and unlabeled datasets along with LwF, and the process is iterated until no new intents are discovered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>an incremental method to dis-342 cover new intents and label the data simultaneously. 343 At each iteration t, we use the trained model M t-1 344 to extract representations of the unlabeled dataset 345 D U and perform K-means clustering on these rep-346 resentations based on a chosen value of K t . We 347 want to highlight that at t 1 iteration, we use the 348 model pre-trained with UCL loss. In the early itera-349 tions, the model may not have been fine-tuned suf-350 ficiently, leading to wrong cluster assignments for 351 some samples. To mitigate this problem, we select 352 only those samples which are closer to their cluster 353 centroids, based on some threshold γ. Specifically, 354 we compute the cosine similarity s i between each 355 sample and its corresponding cluster centroid C i 356 and select the sample if s i &gt; γ. We find that a 357 threshold value between 0.7 and 0.99 works well 358 in practice.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Selects the top 75 % samples if all have the same label else selects the ones having the same label in the top 20 sentences. 10 L ← Provide Labels to the sentences from Y 11</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Effectiveness of Known Ratio on three datasets.</figDesc><graphic coords="7,359.30,339.63,161.48,91.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: TSNE plots at different iterations for TELECOM dataset.</figDesc><graphic coords="8,75.83,312.59,104.77,78.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of CLINC, BANKING, and TELE-COM Dataset describing the number of instances used in train, validation, and test set respectively along with the number of classes.</figDesc><table><row><cell></cell><cell cols="4"># Classes # Train # Val # Test</cell></row><row><cell>CLINC</cell><cell>150</cell><cell cols="3">18000 2250 2250</cell></row><row><cell cols="2">BANKING 77</cell><cell>9003</cell><cell cols="2">1000 3080</cell></row><row><cell cols="2">TELECOM 16</cell><cell>1013</cell><cell>250</cell><cell>250</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Clustering results on CLINC, BANKING and TELECOM test dataset at known ratio of 25%, 50% and 75%.</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated creation of an intent model for conversational agents</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Benayas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><forename type="middle">Angel</forename><surname>Sicilia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marçal</forename><surname>Mora-Cantallops</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2164401</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="139" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ajay Chatterjee and Shubhashis Sengupta. 2020. Intent mining from past conversations for conversational agent</title>
		<author>
			<persName><forename type="first">Iñigo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tadas</forename><surname>Temčinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Gerz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.366</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI</title>
		<meeting>the 2nd Workshop on Natural Language Processing for Conversational AI<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4140" to="4152" />
		</imprint>
	</monogr>
	<note>Proceedings of the 28th International Conference on Computational Linguistics. International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, KDD&apos;96</title>
		<meeting>the Second International Conference on Knowledge Discovery and Data Mining, KDD&apos;96</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Cert: Contrastive self-supervised learning for language understanding</title>
		<author>
			<persName><forename type="first">Hongchao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<idno>ArXiv, abs/2005.12766</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using Siamese BERTnetworks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Nagib Hakim, and Lama Nachman. 2021. Semi-supervised interactive intent labeling</title>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Sahay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eda</forename><surname>Okur</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised intent discovery with contrastive learning</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinge</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mani</forename><surname>Najmabadi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.nlp4convai-1.12</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI</title>
		<meeting>the 3rd Workshop on Natural Language Processing for Conversational AI</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="120" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mpnet: Masked and permuted pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS&apos;20</title>
		<meeting>the 34th International Conference on Neural Information Processing Systems, NIPS&apos;20<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised clustering with contrastive learning for discovering new intents</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghong</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Guo</surname></persName>
		</author>
		<idno>CoRR, abs/2201.07604</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rapidly scaling dialog systems with interactive learning. Natural language dialog systems and intelligent assistants</title>
		<author>
			<persName><forename type="first">Jason D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nobal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Niraula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aparna</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Lakshmiratan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jurado</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mouni</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><surname>Zweig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<idno>CoRR, abs/1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
	<note>ICML&apos;16</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ConSERT: A contrastive framework for self-supervised sentence representation transfer</title>
		<author>
			<persName><forename type="first">Yuanmeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rumei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.393</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5065" to="5075" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards k-means-friendly spaces: Simultaneous deep learning and clustering</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyi</forename><surname>Hong</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="3861" to="3870" />
		</imprint>
	</monogr>
	<note>ICML&apos;17</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Curran Associates Inc</publisher>
			<pubPlace>Red Hook, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discovering new intents with deep aligned clustering</title>
		<author>
			<persName><forename type="first">Hanlei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Ting-En Lin</surname></persName>
		</author>
		<author>
			<persName><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="14365" to="14373" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
