<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Are Machine Reading Comprehension Systems Robust to Context Paraphrasing?</title>
				<funder>
					<orgName type="full">University of Manchester Department of Computer Science Kilburn Scholarship</orgName>
				</funder>
				<funder>
					<orgName type="full">ACL Rolling Review April</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yulong</forename><surname>Wu</surname></persName>
							<email>yulong.wu@manchester.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Manchester</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Viktor</forename><surname>Schlegel</surname></persName>
							<email>viktor_schlegel@asus.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Manchester</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">ASUS Intelligent Cloud Services (AICS)</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Riza</forename><surname>Batista-Navarro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Manchester</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Are Machine Reading Comprehension Systems Robust to Context Paraphrasing?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1DD451FA7592FED1C5E9095ED7D30F31</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T14:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Investigating the behaviour of Machine Reading Comprehension (MRC) models under various types of test-time perturbations can shed light on the enhancement of their robustness and generalisation capability, despite the superhuman performance they have achieved on existing benchmark datasets. In this paper, we study the robustness of contemporary MRC systems to context paraphrasing, i.e., whether these models are still able to correctly answer the questions once the reading passages have been paraphrased. To this end, we systematically design a pipeline to semi-automatically generate perturbed MRC instances which ultimately lead to the creation of a paraphrased test set. We conduct experiments on this dataset with six state-of-the-art neural MRC models and we find that even the minimum performance drop of all these models exceeds 41%, whereas human performance remains high. Retraining models with augmented perturbed examples results in improved robustness, though the performance remains lower than on the original dataset. These results demonstrate that the existing high-performing MRC systems are still far away from real language understanding 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine reading comprehension (MRC), the task of automatically reading a passage of text and answering related questions, serves as an important testbed for evaluating various Natural Language Understanding (NLU) capabilities of computer systems <ref type="bibr" target="#b1">(Chen, 2018)</ref>. While neural MRC systems approach or even surpass human performance on benchmark datasets <ref type="bibr" target="#b4">(Devlin et al., 2019;</ref><ref type="bibr" target="#b11">Lan et al., 2020;</ref><ref type="bibr" target="#b7">He et al., 2021)</ref>, it remains uncertain whether they can indeed solve the MRC task <ref type="bibr" target="#b18">(Schlegel et al., 2020;</ref><ref type="bibr">Wu et al., 2021b;</ref><ref type="bibr" target="#b22">Sugawara et al., 2022;</ref><ref type="bibr" target="#b20">Shinoda et al., 2023;</ref><ref type="bibr" target="#b17">Schlegel et al., 2023)</ref>. In particular, recent studies have shown that instead of  <ref type="bibr" target="#b12">(Liu et al., 2019)</ref> can get the answer correct over the original reading passage, but is misled when presented with a whole context paraphrased version.</p><p>performing consistently well, contemporary models are brittle under various test-time perturbations <ref type="bibr" target="#b14">(Ribeiro et al., 2020;</ref><ref type="bibr" target="#b21">Si et al., 2021;</ref><ref type="bibr">Wu et al., 2021a;</ref><ref type="bibr" target="#b16">Schlegel et al., 2021;</ref><ref type="bibr">Yan et al., 2022)</ref>. This raises the question of the suitability of existing gold standard datasets to establish a model's robustness and the need to improve the reliability of these MRC systems <ref type="bibr" target="#b25">(Wang et al., 2022)</ref>.</p><p>Paraphrase understanding plays a role in measuring the robustness and generalisation ability of MRC models. Intuitively, a trustworthy MRC system should demonstrate robust generalisation on paraphrased contexts and/or questions, i.e., those that convey the same semantic meaning using different surface forms. Previous studies have attempted to paraphrase the questions (Gan and Ng, 2019) and strategically modify portions of the reading passage, e.g., paraphrase only the answer sentence using the back-translation <ref type="bibr" target="#b10">(Lai et al., 2021)</ref> or generate paraphrases that exclude the top five im-portant words in the context <ref type="bibr">(Wu et al., 2021a)</ref>. By assessing model performance on the paraphrased test sets, they concluded that MRC models might be vulnerable to paraphrasing-oriented attacks.</p><p>The reading comprehension task assesses a model's real understanding of a given context, i.e., a passage. Though the findings in the work of <ref type="bibr" target="#b10">Lai et al. (2021)</ref> and <ref type="bibr">Wu et al. (2021a)</ref> provide insights into the weaknesses of MRC datasets to benchmark partial-context paraphrasing understanding, their designed strategic paraphrasing approach may hinder the generated perturbed examples from accurately simulating real-world text disruptions, which can pervade any part of a passage, not just specific words or answer sentences. Furthermore, it is not clear whether the modifications introduced as part of the perturbations changed the meaning of the original context. Therefore, to precisely reveal the capability of existing gold standard datasets to benchmark paraphrase understanding, we argue that it is crucial to examine the robustness of MRC systems to paraphrasing the whole context as well.</p><p>In this paper, our aim is to evaluate how well current reading comprehension systems generalise to a modified benchmark in which all contexts were paraphrased while preserving the same meaning and thus keeping the same gold standard answer. Different from prior robustness assessment research <ref type="bibr" target="#b6">(Gan and Ng, 2019;</ref><ref type="bibr">Wu et al., 2021a;</ref><ref type="bibr">Yan et al., 2022)</ref>, we design a pipeline to generate and identify perturbations of MRC examples that demonstrate the lack of robustness of a strong MRC system RoBERTa-large <ref type="bibr" target="#b12">(Liu et al., 2019)</ref> to context paraphrasing (see Figure <ref type="figure" target="#fig_0">1</ref> for an example). In doing so, we also underscore the limitation of a large language model to handle the paraphrased contexts. This proposed evaluation framework leads to the construction of a paraphrased test set drawn from the original Stanford Question Answering Dataset v1.1 (SQuAD 1.1) benchmark <ref type="bibr" target="#b13">(Rajpurkar et al., 2016)</ref>. Results of our experiments show that the performance of five other MRC models except the RoBERTa-large on our created dataset is substantially lower, indicating the transferability of such adversarial attack against MRC models and the insufficiency of the SQuAD 1.1 to benchmark context paraphrasing understanding. Utilising a straightforward training data augmentation approach, we also show the possibility to enhance the robustness of these models in dealing with context paraphrasing. These suggest that there is a need to create gold standard datasets in which context paraphrasing challenges are sufficiently represented.</p><p>2 Experiment Setup MRC Dataset. In this paper, we investigated an extractive English MRC dataset SQuAD 1.1 <ref type="bibr" target="#b13">(Rajpurkar et al., 2016)</ref> (License: CC-BY 4.0) due to its simplicity and the fact that it is the dataset on which current MRC models have already achieved superhuman performance, hence allowing us to focus on analysing the robustness of models to context paraphrasing. The statistics for the dataset are reported in Appendix A.</p><p>Models. We chose the following models for the task of machine translation and reading comprehension, respectively.</p><p>Machine translation: We used the neural translation models provided by OPUS-MT <ref type="bibr" target="#b24">(Tiedemann and Thottingal, 2020)</ref> which are based on the popular Marian-Neural Machine Translation framework <ref type="bibr" target="#b9">(Junczys-Dowmunt et al., 2018)</ref> pre-trained on the OPUS <ref type="bibr" target="#b23">(Tiedemann, 2012)</ref> multilingual corpus.</p><p>Reading comprehension: We selected the RoBERTa-large model <ref type="bibr" target="#b12">(Liu et al., 2019)</ref> to generate the paraphrased test set mainly due to its impressive performance (93.1% F1) on the original development set of SQuAD 1.1 <ref type="bibr" target="#b13">(Rajpurkar et al., 2016)</ref>. The process of generating the challenge set also entails the utilisation of a Generative Pretrained Transformer (GPT) <ref type="bibr" target="#b0">(Brown et al., 2020)</ref> series model, specifically GPT-3.5-turbo, through the OpenAI ChatGPT API. In the final evaluation stage, we used multiple strong MRC models including BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, DistilBERT <ref type="bibr" target="#b15">(Sanh et al., 2019)</ref>, ALBERT <ref type="bibr" target="#b11">(Lan et al., 2020)</ref>, Span-BERT <ref type="bibr" target="#b8">(Joshi et al., 2020)</ref> and DeBERTa <ref type="bibr" target="#b7">(He et al., 2021)</ref>, to comprehensively demonstrate the challenge posed by our created dataset. We fine-tuned these pre-trained language models on the training set of SQuAD 1.1 and evaluated them on each of the original and perturbed test sets by making use of HuggingFace's Transformers library <ref type="bibr" target="#b27">(Wolf et al., 2020)</ref>. Model details and the hyperparameters used in model fine-tuning are shown in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Context Paraphrasing-Oriented Challenge Set Generation</head><p>In this section, we describe our methodology for generating a semantics-preserving contextparaphrased dataset. Four steps are involved in the perturbation pipeline, which are detailed below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Automatic Context Paraphrasing</head><p>We explored paraphrasing the reading passages in the development set of an MRC dataset using a back-translation approach, by which each sentence in the context is translated from a source language (English) to a pivot language and then back to the source language. We identified twelve languages across five language families as the pivot language, informed by their number of speakers <ref type="bibr">(Eberhard et al., 2022)</ref> and the performance of their associated pre-trained neural translation models (Tiedemann and Thottingal, 2020). After obtaining the paraphrases, we kept only those with at least one question where all annotated answers can still be found in the paraphrased context. The original contexts of those paraphrases were then extracted from the development set, to keep it aligned with the modified test set and the performance comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Preliminary Evaluation</head><p>As presented in Section 3.1, we generated perturbed test subsets (one for each of the 12 pivot languages) in which contexts were paraphrased using back-translation, and their corresponding original versions. Then, we examined the performance of a strong MRC model, RoBERTa-large <ref type="bibr" target="#b12">(Liu et al., 2019)</ref>, on these datasets, as demonstrated in Table <ref type="table">1</ref>. It can be seen from Table <ref type="table">1</ref> that paraphrasing the contexts using different pivot languages caused various degrees of degradation in terms of the performance of the RoBERTa-large model. Nonetheless, we cannot simply conclude that this indicates the vulnerability of MRC models to the context paraphrasing attack as it is unclear whether these context paragraphs were indeed paraphrased, i.e., remain semantically equivalent while lexical/syntactic features were changed. Therefore, we manually verified the validity of the perturbed MRC instances in the next step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Human Evaluation</head><p>With the aim of studying the lack of robustness of MRC models to context paraphrasing, from each generated perturbed test set, we identified MRC examples on which the RoBERTa-large model <ref type="bibr" target="#b12">(Liu et al., 2019)</ref> predicts a wrong answer span whereas it provides the correct answer given the original passage. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Paraphrased Test Set Generation</head><p>While human evaluation enables us to identify suitable MRC instances precisely, it requires significant human annotation effort. Hence, we explored the viability of two different approaches to auto- The best-performing model, GPT-3.5-turbo under zero-shot scenario (0.69 precision in predicting suitable example), was then applied on the filtered perturbed instances generated using Finnish, Spanish, Vietnamese, Italian and Swedish, 182 of which were classified as suitable (from 150 original contexts). For multiple paraphrased contexts that correspond to the same original passage, we only kept the perturbed one with the most questions preserved, or in case of a tie, the one with the lowest average question-context lexical overlap <ref type="bibr" target="#b19">(Shinoda et al., 2021)</ref>. Our final paraphrased test set contains 150 contexts and 158 questions in total. For the purposes of comparison, we also created an Original version of the test set keeping only the original passages and questions corresponding to those that were included in the Paraphrased version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation</head><p>We assessed the performance of six state-of-theart MRC models on the newly created challenge set, as shown in Table <ref type="table" target="#tab_3">2</ref>. The table shows that all the evaluated neural language models demonstrated poor generalisation to our generated test set. RoBERTa-large suffered the largest performance drop of 85.07%-this is within our expectation since its errors were used to identify suit-  able examples. For the other five model architectures, the relative changes were smaller than that of RoBERTa-large, but still very noticeable with over 41% performance decrease. This demonstrates the poor capability of these reading comprehension systems to properly deal with the paraphrased contexts. Apart from RoBERTa and ALBERT, the performance of other four MRC models remained consistent across both original and paraphrased test set, with DeBERTa achieving the highest EM and F1 score, followed by SpanBERT, BERT and Dis-tilBERT. While the performance of ALBERT on the original dataset was slightly lower than that of DeBERTa, it notably outperformed the latter on the paraphrased test set, attaining the highest performance score (54.16 F1). We also found that the consistency in original model performance rankings might not apply to their robustness to context paraphrasing, with the BERT-large model demonstrating the greatest F1 decrease (50.2%) and the ALBERT-xxlarge-v1 exhibiting the smallest performance decline (41.54%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Error Analysis</head><p>To explore the source of model inaccuracies in paraphrased contexts, we manually checked 50 perturbed examples on which the examined MRC models failed and identified three potential sources of model errors. We observed that the paraphrasing of keywords in the sentence that is required to answer the question, along with some other lexical changes, might lead models to provide an incorrect answer (see Figure <ref type="figure">9</ref> in Appendix F). Moreover, another source of errors might be the change in the answer sentence structure (see Figure <ref type="figure" target="#fig_0">10</ref> in Appendix F as an example). Paraphrasing other contextual sentences may also inadvertently lead to the generation of incorrect responses by MRC models, particularly when such paraphrases result in keyword overlap with the question. However, unraveling the sources of these errors in the midst of full-context paraphrasing perturbation remains a complex problem that requires further investigation. Overall, our findings suggest that these high-performing systems might mostly rely on certain words matching between the question and the context to generate the answer, rather than truly understanding the passage. However, we also observed in a small proportion of examples that a mismatch between the answer provided by a model and the gold standard answer, does not necessarily mean that the model's answer is erroneous: in some cases, the semantic meaning of the paraphrased context has changed or the model's answer is arguably correct. This indicates that this work might be underestimating the robustness of the investigated models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Robustness Improvement</head><p>An intuitive strategy to enhance the models' robustness to context paraphrasing involves exposing them to suitable examples. To this end, we selected 2694 MRC contexts (comprising 12723 questions) from the original SQuAD 1.1 training set <ref type="bibr" target="#b13">(Rajpurkar et al., 2016)</ref> and paraphrased them using Finnish, a language that has been demonstrated to be the most effective for generating suitable MRC examples. We then curated the perturbed examples where the answer span still contained within the corresponding paraphrased context, yielding 2459 paraphrased contexts across a total of 8075 questions. The investigated models were then re-trained on the SQuAD 1.1 training set, augmented with these perturbed instances. Table <ref type="table" target="#tab_4">3</ref> shows their performance on both the original and the paraphrased test sets, before and after re-training.</p><p>From Table <ref type="table" target="#tab_4">3</ref>, we can see that on the original test set, apart from the DistilBERT-base, which experienced a slight performance decline in terms of the EM metric, all retrained models demonstrated higher performance than the one trained on the original training set of SQuAD 1.1, though the augmented contexts-paraphrased set contains noises, i.e., those are not suitable examples. These findings showcase the potential for enhancing performance on the original dataset by training models with context-paraphrased MRC examples. On the paraphrased challenge set, for all models expect the DistilBERT-base, re-training with the additional perturbed examples improved the performance and thus their robustness to context paraphrasing. How- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we reveal the weaknesses of contemporary reading comprehension systems to context paraphrasing. With the proposed perturbation framework, we generated a paraphrased challenge set, to which six high-performing MRC models generalise poorly. We also demonstrate that a training data augmentation approach can enhance the robustness of the majority of models when exposed to the paraphrased contexts. This informs us that to equip models with context paraphrasing understanding ability, there is a need to create benchmarks in which this reasoning challenge is precisely represented. Future work will include the design of better techniques to remove the noise existing in the challenge set and the optimise of the perturbation pipeline so that it can be generalisable to more challenging datasets.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyperparameters of the Neural Reading Comprehension Models</head><p>Table <ref type="table">5</ref> shows the hyperparameters used to finetune the pre-trained MRC models in this work. We utilised 2 16GB Nvidia v100 GPUs to fine-tune and evaluate each model. 512 4 2e-5 4.0 ALBERT-xxlarge-v1 (223)  <ref type="table" target="#tab_7">384 4 3e-5 2.0   DeBERTa-large (350)   384 4 3e-6 3.0   Table 5:</ref> The hyperparameters used to fine-tune each pre-trained MRC model (with its number of parameters). d is the size of the token sequence fed into the model, b is the training batch size, lr is the learning rate, and ep is the number of training epochs. We used stride = 128 for documents longer than d tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Human Annotation</head><p>each example. Based on this, we paid the annotators for their work by offering them coupons with a value of 20 pence for each example they annotate.</p><p>For the randomly sampled 247 candidate instances, we first asked two annotators to answer each question based on the corresponding paraphrased context, respectively. The annotators were required to select the shortest continuous span in the paraphrased context that answered the question only if they are confident that the paraphrased context still makes it possible to answer the associated question and were allowed to leave the answer as blank if the question is not answerable anymore. Full text of instruction given to the annotators can be seen in Figure <ref type="figure" target="#fig_2">3</ref>. Afterwards, for each example, we measured the correctness of the answer span provided by each annotator through comparing it with the ground truth answers, respectively , and labelled the example as suitable or unsuitable based on the criteria described in Section 3.3. To conduct a precise analysis, we manually checked all examples with the answer span given by the annotator(s) does not exact match any of the ground truth answers and decided the correctness of the answer by taking into account the corresponding context and the question as well. Figure <ref type="figure">4</ref> demonstrates one such example. We then measured the inter-annotator agreement by computing the Cohen's kappa coefficient <ref type="bibr" target="#b2">(Cohen, 1960)</ref>, which is around 0.48. This might indicate that there exists moderate discrepancies between the two annotators concerning the answerability of the questions predicated on the contexts that have been paraphrased. Finally, we presented the examples on which the two annotators share a disagreement to the third annotator and provided them the label that agreed by the majority of annotators. This yielded a total of 66 suitable examples. From the identified 66 examples, we further manually eliminated 13 wherein the prediction of the RoBERTa-large <ref type="bibr" target="#b12">(Liu et al., 2019)</ref> could be reasonably deemed accurate, thus rendering them unsuitable for the robustness assessment (see Figure <ref type="figure">5</ref> as an example). Our final annotated dataset contains 53 (out of 247) suitable examples. In an effort to curtail potential bias, all annotators were solely provided with the paraphrased context and the corresponding question for their examination.</p><p>Thanks for contributing to this project! Your task is to read each given context and answer a question about it. We will compare the answer you provide with the ground truth answers to determine the human answerability of the question, and then screen out the examples that are suitable for the robustness assessment of reading comprehension systems. When you are answering the questions:</p><p>(1) If you meet a question that you truly think you can answer it based on the given context, then select the shortest continuous span in the context as the answer.</p><p>(2) If you meet a question that is completely unanswerable, leave the answer as blank. Context: Kenya [. . . ], officially the Republic of Kenya, is a country in Africa and a founding member of the East African Community (EAC). Its capital and largest city is Nairobi. Kenya's territory lies on the equator and overlies the East African Rift covering a diverse and expansive terrain that extends roughly from Lake Victoria to Lake Turkana (formerly called Lake Rudolf) and further south-east to the Indian Ocean. [. . . ] Paraphrased Context: Kenya (Kenya: "Kenya") is a country in Africa and one of the founding members of the East African Community (EAC). The capital and largest city is Nairobi. The area of Kenya lies on the equator and survives the East African Rift which covers a diverse and vast area that stretches roughly from Lake Victoria to Lake Turkana (formerly Lake Rudolf) and further south-eastern to the Indian Ocean. [. . . ] Question: Where is Kenya located? Ground Truth Answers: Africa, in Africa RoBERTa-large's Prediction Under Context Paraphrasing: on the equator Figure <ref type="figure">5</ref>: A perturbed example that is not suitable for the robustness assessment since the answer span offered by the RoBERTa-large model <ref type="bibr" target="#b12">(Liu et al., 2019)</ref> is reasonably accurate, albeit not an exact match for any of the ground truth answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Automated Identification of Suitable MRC Instances</head><p>To circumvent the substantial effort required for manual annotation, we attempted to automatically classify whether a perturbed reading comprehension example is qualified to demonstrate the lack of robustness of MRC models to context paraphrasing. In the following, we elaborate on the two approaches undertaken and present the empirical results derived from these experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ML-based Approach:</head><p>We trained and evaluated multiple classifiers on our 247 annotated examples with 129 input features that were calculated by TAACO <ref type="bibr" target="#b3">(Crossley et al., 2019)</ref>, a tool that measures various linguistic features of the passage such as lexical density and adjacent sentence overlap. The designed classification pipeline involves data standardisation, features selection and random oversampling. Hyperparameter tuning was carried out to determine the optimal configuration. The obtained best-performing model, Random Forest (with 40 selected features), only achieved 0.39 precision in predicting suitable example, which implies that those extracted features might not sufficient to represent this challenging task. Therefore, we shifted our attention to the GPT series models, given their exceptional efficacy in transforming many tasks into generative tasks.</p><p>GPT Series Models: Compared to traditional ML methods, GPT series models offer the advantage of not requiring the construction of linguistic features, thereby simplifying the approach to automatically classify suitable MRC examples. Drawing upon the human annotation process described in Appendix C, we first manually constructed the zero-shot prompt encompassing the paraphrased context, question, ground truth answers, the answer span given by the RoBERTa-large <ref type="bibr" target="#b12">(Liu et al., 2019)</ref>, and tasked the model to generate binary output (0 or 1) to indicate whether an example is suitable for robustness assessment, adhering to a predefined set of decision rules. We also experimented with the few-shot prompt by adding three randomly selected in-context exemplars of inputlabel pairs (demonstrations) <ref type="bibr" target="#b0">(Brown et al., 2020)</ref> in the zero-shot prompt. Under both zero-shot and few-shot scenarios, we further investigated the use of the Chain-of-Thought (CoT) <ref type="bibr" target="#b26">(Wei et al., 2022)</ref> by adding "let's think step by step" and CoT demonstrations in the corresponding prompt, respectively. The templates for the four prompting strategies are shown in Appendix G. In order to mitigate the influence of prior dialogues, each request was sent individually to produce the corresponding response. When processing the responses, especially under the zero-shot CoT and few-shot CoT scenarios, we only consider an example as suitable if its response includes a solid explanation and judgement. Labels generated by the model under the four distinct test configurations were subsequently compared with the gold labels annotated by human evaluators, respectively. The results are shown in Table <ref type="table">6</ref>.</p><p>It can be seen from Table <ref type="table">6</ref> that on the precision of predicting suitable MRC example, prompting under the zero-shot scenario provides the best result, which is 0.41. Surprisingly, the incorporation of demonstrations and the adoption of the CoT prompting considerably attenuate model performance, a finding that deviates from existing literature asserting enhancements in performance across many NLU tasks with the inclusion of incontext demonstrations <ref type="bibr" target="#b0">(Brown et al., 2020)</ref> and the CoT method <ref type="bibr" target="#b26">(Wei et al., 2022)</ref>. The observed unsatisfactory performance could potentially be attributed to two factors: (1) The ambiguity inherent to the task of automated identification of suitable MRC example, as viewed from the dataset annotation perspective. As indicated in Appendix C, a moderate level of disagreement was even observed between two human annotators in determining whether a question is indeed answerable based on the paraphrased context, with an inter-annotator agreement score of 0.48. This suggests that our annotated set of 247 examples might contain contentious cases, thereby rendering the task notably challenging for the model. (2) From the model's perspective, we investigated potential reasons for performance degradation following the adoption of in-context examples and CoT by manually scrutinizing some responses under these testing conditions. Our findings reveal that despite guidance from demonstrations and CoT, the model frequently produces reasoning that contradicts the predicted label or even generates hallucinations. For instance, under the zero-shot CoT scenario, model produces response like "This example is suitable for robustness assessment. The ground truth answers (GTAs) and RoBERTa's answer A are different, indicating that there is potential for the model to make mistakes. Therefore, it is important to test the model's robustness by presenting it with similar but slightly different contexts and questions to ensure that it can generalize well and provide accurate answers.", which even not relevant to the task. While we acknowledge that there exists scope to improve the prompts used in this work, it remains evident that the GPT-3.5-turbo model, despite its significant accomplishments in some NLU tasks, still falls short of attaining human-level language comprehension. Though the performance of the obtained best model, i.e., GPT-3.5-turbo under the zero-shot sce-nario, is not satisfactory, we attempted to measure its precision in predicting suitable example from the candidate perturbed instances generated by using each pivot language, respectively, as shown in Table <ref type="table">7</ref>. From Table <ref type="table">7</ref>, we can see that in classifying perturbed examples paraphrased using Finnish, the model exhibits flawless performance, achieving a precision score of 1.0. In contrast, for other languages, such as Russian, Chinese and Indonesian, the precision score is notably low or even zero. Therefore, to generate our paraphrased challenge set, we restricted model's application to filtered perturbed examples produced using Finnish, Spanish, Vietnamese, Italian and Swedish (with precision greater than 0.5), which yielded a final precision score of 0.69, while excluding instances generated using other languages. Table <ref type="table">7</ref>: Precision of the GPT-3.5-turbo in predicting suitable example from the candidate perturbed instances generated using each of the 12 pivot languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pivot Language(s) Precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Analysis of Disparities in Adversarial</head><p>Examples Perception: GPT-3.5-turbo vs. Human</p><p>The zero-shot prompt, as designed in Appendix D, directly solicits a binary response from the GPT-3.5-turbo model concerning the suitability of a perturbed MRC example for robustness assessment. To validate the stability of the obtained performance (0.41 precision) and also examine the divergence in the perception of whole context-paraphrased adversarial examples between the GPT-3.5-turbo model and human observers, we conducted further experiments by directly asking the model to extract the shortest continuous span as the answer given the paraphrased context and the question. We utilised the prompting method based on both instruction and opinion <ref type="bibr">(Zhou et al., 2023)</ref> to improve the faithfulness of the model to paraphrased context when formulating responses, thereby precluding the use of its parametric knowledge to a great extent. Additionally, an "I do not know" option was allowed to encourage the model to abstain from providing the answer if the paraphrased context does not make it possible to answer the question anymore. Figure <ref type="figure">6</ref> demonstrates the used prompt template.</p><p>Instruction: read the given information and answer the corresponding question. The output should only be the shortest continuous span from the context and should not include any explanation. Output "I do not know" if the context makes it impossible to answer the corresponding question. Bob said, "context" Q: question in Bob's opinion based on the given text?</p><p>Figure <ref type="figure">6</ref>: An instruction-opinion based prompt template <ref type="bibr">(Zhou et al., 2023)</ref>.</p><p>Afterwards, we determined the accuracy of each response generated by the GPT-3.5-turbo model and assigned a binary label (1 or 0) to signify its appropriateness for robustness assessment. We then compared the obtained results with the gold standard labels of the annotated dataset version containing 66 suitable examples (see Appendix C), as our provided prompt does not require the model to consider the correctness of the prediction made by the RoBERTa-large <ref type="bibr" target="#b12">(Liu et al., 2019)</ref>. Experimental results revealed that the GPT-3.5-turbo model maintained a consistent 0.41 precision score in predicting suitable MRC example, thereby suggesting its stability on this task to some extent. Figure <ref type="figure">7</ref> and Figure <ref type="figure">8</ref> demonstrate a failure case of the GPT-3.5-turbo model on the suitable example classification, respectively. In Figure <ref type="figure">7</ref>, the model is still able to extract the correct answer span "John Sutcliffe" from the paraphrased context, though both human annotators deem that the question is not answerable. On the contrary, as can be seen from Figure <ref type="figure">8</ref>, while human annotators can get the answer correct, the model abstains from answering the question and thus generates "I do not know" as the answer. These findings suggest the existence of a significant gap between the GPT-3.5turbo model and human performance in discerning whole context paraphrasing oriented textual attacks and the GPT-3.5-turbo is still substantially distant</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Question:Figure 1 :</head><label>1</label><figDesc>Figure1: An instance where the RoBERTa-large model<ref type="bibr" target="#b12">(Liu et al., 2019)</ref> can get the answer correct over the original reading passage, but is misled when presented with a whole context paraphrased version.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Percentage of suitable MRC examples within the candidate instances generated by using each pivot language, respectively.</figDesc><graphic coords="4,70.87,70.87,218.27,167.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Instructions for the annotation task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Finnish</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The performance (%) of the fine-tuned MRC models on the original and the paraphrased test set.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The performance (%) of MRC systems on the original and the paraphrased test set, before and after re-training. Performance figures displayed in white cells correspond to results obtained on the original test set, whereas the results shown in the shaded areas represent the performance on the generated challenge set.</figDesc><table><row><cell>Model</cell><cell cols="2">Performance</cell></row><row><cell></cell><cell>(EM/F1)</cell></row><row><cell></cell><cell>Before</cell><cell>After</cell></row><row><cell>DistilBERT</cell><cell cols="2">66.46/73.5 65.19/73.58</cell></row><row><cell>(base)</cell><cell cols="2">27.22/39.05 24.68/36.18</cell></row><row><cell>BERT</cell><cell cols="2">75.32/81.7 81.65/85.77</cell></row><row><cell>(large)</cell><cell cols="2">32.91/40.69 34.18/43.58</cell></row><row><cell>SpanBERT</cell><cell cols="2">77.85/84.72 83.54/88.96</cell></row><row><cell>(large)</cell><cell cols="2">32.91/42.62 38.61/48.29</cell></row><row><cell>ALBERT</cell><cell cols="2">88.61/92.64 88.61/93.52</cell></row><row><cell cols="3">(xxlarge-v1) 44.3/54.16 46.84/55.78</cell></row><row><cell>DeBERTa</cell><cell cols="2">89.24/93.58 90.51/94.81</cell></row><row><cell>(large)</cell><cell cols="2">41.14/49.74 43.04/51.77</cell></row><row><cell cols="3">ever, for the DistilBERT-base model, exposing it</cell></row><row><cell cols="3">to the paraphrased examples even resulted in a</cell></row><row><cell cols="3">moderate performance drop (7.35% F1), further</cell></row><row><cell cols="3">compromising its robustness in handling context</cell></row><row><cell cols="3">paraphrasing. This might due to the unsuitable</cell></row><row><cell cols="3">examples included in the augmented training set,</cell></row><row><cell cols="3">but also demonstrate the challenging nature of the</cell></row><row><cell cols="3">whole context paraphrasing perturbation on the</cell></row><row><cell cols="2">DistilBERT-base (Sanh et al., 2019).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note><p><p><p>Number of contexts and questions in the SQuAD 1.1 training and development sets</p><ref type="bibr" target="#b13">(Rajpurkar et al., 2016)</ref></p>.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Our code and data are available at https://github. com/Yulong-W/context-paraphrasing.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors express the gratitude to the anonymous reviewers from the <rs type="funder">ACL Rolling Review April</rs> and <rs type="person">June 2023</rs> for their invaluable suggestions. We would also like to thank <rs type="person">Michael White</rs> and <rs type="person">Jing Wang</rs> in annotating the perturbed MRC examples. Part of the experiments were conducted with the support of the <rs type="affiliation">Computational Shared Facility at The University of Manchester</rs>, for which the authors are grateful. This work was supported by the <rs type="funder">University of Manchester Department of Computer Science Kilburn Scholarship</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>In this work, our annotated gold dataset might contain potentially debatable instances of suitable MRC examples. To address this concern, there is a pressing need for the establishment of theoretical foundations which clearly define human answerable under the context-paraphrasing oriented perturbations and other types of perturbations. Building upon this, research efforts are needed to evaluate and enhance the precision of automatic approaches for identifying suitable examples, enabling precise assessment of models robustness against test-time perturbations. Further, there is potential to design better document-level paraphrasing methods and expand this study to include other sophisticated MRC datasets and diverse NLU tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>The SQuAD 1.1 benchmark and models utilized in this paper are all publicly available. During the phase of human annotation, explicit instructions regarding the annotation task were provided to all annotators, and they were informed about the intended use of their annotations. Only their responses to the questions were collected, and no sensitive information was gathered.  Context: Sudbury model democratic schools claim that popularly based authority can maintain order more effectively than dictatorial authority for governments and schools alike. They also claim that in these schools the preservation of public order is easier and more efficient than anywhere else. [. . . ] Paraphrased Context: Model schools in Sudbury argue that popular authority can maintain order more effectively than dictatorial authority for governments and schools. They also claim that, in these schools, the preservation of public order is easier and more effective than anywhere else. [. . . ] Question: In addition to schools, where else is popularly based authority effective? Prediction Under Context Paraphrasing: Human Annotators: governments GPT-3.5-turbo: I do not know </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Suitable Examples Demonstration</head><p>We present two perturbed examples from the constructed challenge set on which the MRC mod-els demonstrated unsatisfactory generalisation, as shown in Figure <ref type="figure">9</ref> and Figure <ref type="figure">10</ref>, respectively.</p><p>Paragraph: The game's media day, which was typically held on the Tuesday afternoon prior to the game, was moved to the Monday evening and re-branded as Super Bowl Opening Night.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Templates for Various Prompting Strategies</head><p>Figure <ref type="figure">11</ref> illustrates the diverse templates employed to prompt the GPT-3.5-turbo model to classify the suitable perturbed MRC example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instructions:</head><p>Given an example which contains a context, question, ground truth answers (GTAs) and RoBERTa's answer A, decide whether it is suitable for robustness assessment by choosing one of the following options: '0': A is reasonably correct or A is wrong and you cannot correctly answer the question purely relying on the context as well. '1': A is wrong but you can correctly answer the question purely relying on the context. zero-shot:</p><p>[Instructions] Generate either '0' or '1', do not include the explanation. A: governments and schools. They also claim that, in these schools, the preservation of public order is easier and more effective than anywhere else. Response: Firstly, compare RoBERTa's answer A with GTAs. Since governments and schools. They also claim that, in these schools, the preservation of public order is easier and more effective than anywhere else. is wrong, then there is a need to thoroughly check the context and question. Since the context provides sufficient information to enable us to get the answer correct, the response is 1. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Alec Radford, Ilya Sutskever, and Dario Amodei</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Neural Reading Comprehension and Beyond</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A coefficient of agreement for nominal scales. Educational and Psychological Measurement</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.1177/001316446002000104</idno>
		<imprint>
			<date type="published" when="1960">1960</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="37" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Tool for the Automatic Analysis of Cohesion 2.0: Integrating semantic similarity and text overlap</title>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">A</forename><surname>Crossley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristopher</forename><surname>Kyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Dascalu</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-018-1142-4</idno>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="27" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Eberhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName><surname>Simons</surname></persName>
		</author>
		<ptr target="https://www.ethnologue.com" />
		<title level="m">2022. Ethnologue: Languages of the World</title>
		<editor>
			<persName><forename type="first">Charles</forename><forename type="middle">D</forename><surname>Fennig</surname></persName>
		</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving the robustness of question answering systems to question paraphrasing</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Wee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1610</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6065" to="6075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">{DEBERTA}: {DECODING}-{enhanced} {bert} {with} {disentangled} {attention}</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Span-BERT: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00300</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Marian: Fast neural machine translation in C++</title>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Neckermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alham</forename><surname>Fikri Aji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bogoychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-4020</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2018, System Demonstrations</title>
		<meeting>ACL 2018, System Demonstrations<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="116" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Why machine reading comprehension models learn shortcuts?</title>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quzhe</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.85</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="989" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Beyond accuracy: Behavioral testing of NLP models with CheckList</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongshuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.442</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4902" to="4912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th Workshop on Energy Efficient Machine Learning and Cognitive Computing @ NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantics altering modifications for evaluating comprehension in machine reading</title>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Schlegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Nenadic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riza</forename><surname>Batista-Navarro</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v35i15.17622</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="13762" to="13770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey of methods for revealing and overcoming weaknesses of data-driven natural language understanding</title>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Schlegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Nenadic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riza</forename><surname>Batista-Navarro</surname></persName>
		</author>
		<idno type="DOI">10.1017/S1351324922000171</idno>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A framework for evaluation of machine reading comprehension gold standards</title>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Schlegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Valentino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Nenadic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riza</forename><surname>Batista-Navarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Language Resources and Evaluation Conference</title>
		<meeting>the Twelfth Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5359" to="5369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Can question generation debias question answering models? a case study on question-context lexical overlap</title>
		<author>
			<persName><forename type="first">Kazutoshi</forename><surname>Shinoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saku</forename><surname>Sugawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.mrqa-1.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Machine Reading for Question Answering</title>
		<meeting>the 3rd Workshop on Machine Reading for Question Answering<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="63" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Which shortcut solution do question answering models prefer to learn?</title>
		<author>
			<persName><forename type="first">Kazutoshi</forename><surname>Shinoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saku</forename><surname>Sugawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Benchmarking robustness of machine reading comprehension models</title>
		<author>
			<persName><forename type="first">Chenglei</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.56</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="634" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What makes reading comprehension questions difficult?</title>
		<author>
			<persName><forename type="first">Saku</forename><surname>Sugawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.479</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6951" to="6971" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Parallel data, tools and interfaces in OPUS</title>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA)</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2214" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">OPUS-MT -building open translation services for the world</title>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santhosh</forename><surname>Thottingal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</title>
		<meeting>the 22nd Annual Conference of the European Association for Machine Translation<address><addrLine>Lisboa, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>European Association for Machine Translation</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="479" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Measure and improve robustness in NLP models: A survey</title>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.339</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Seattle, United States. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4569" to="4586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dustin Arendt, and Svitlana Volkova. 2021a. Evaluating neural model robustness for machine comprehension</title>
		<author>
			<persName><forename type="first">Winston</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.210</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th</title>
		<meeting>the 16th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
