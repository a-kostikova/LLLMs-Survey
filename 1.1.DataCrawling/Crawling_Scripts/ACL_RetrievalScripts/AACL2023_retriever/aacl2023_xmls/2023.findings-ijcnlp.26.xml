<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GECTurk: Grammatical Error Correction and Detection Dataset for Turkish</title>
				<funder>
					<orgName type="full">Scientific and Technological Research Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Farrin</roleName><forename type="first">Atakan</forename><surname>Kara</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Engineering Department</orgName>
								<orgName type="institution">Koç University</orgName>
								<address>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marouf</forename><surname>Safian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Engineering Department</orgName>
								<orgName type="institution">Koç University</orgName>
								<address>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Bond</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Engineering Department</orgName>
								<orgName type="institution">Koç University</orgName>
								<address>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gözde</forename><forename type="middle">Gül</forename><surname>Şahin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Engineering Department</orgName>
								<orgName type="institution">Koç University</orgName>
								<address>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GECTurk: Grammatical Error Correction and Detection Dataset for Turkish</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7D3FDECEDB840320E2F201FFE03B946A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Grammatical Error Detection and Correction (GEC) tools have proven useful for native speakers and second language learners. Developing such tools requires a large amount of parallel, annotated data, which is unavailable for most languages. Synthetic data generation is a common practice to overcome the scarcity of such data. However, it is not straightforward for morphologically rich languages like Turkish due to complex writing rules that require phonological, morphological, and syntactic information. In this work, we present a flexible and extensible synthetic data generation pipeline for Turkish covering more than 20 expert-curated grammar and spelling rules (a.k.a., writing rules) implemented through complex transformation functions. Using this pipeline, we derive 130,000 high-quality parallel sentences from professionally edited articles. Additionally, we create a more realistic test set by manually annotating a set of movie reviews. We implement three baselines formulating the task as i) neural machine translation, ii) sequence tagging, and iii) prefix tuning with a pretrained decoder-only model, achieving strong results. Furthermore, we perform exhaustive experiments on out-of-domain datasets to gain insights on the transferability and robustness of the proposed approaches. Our results suggest that our corpus, GECTurk, is high-quality and allows knowledge transfer for the out-of-domain setting. To encourage further research on Turkish GEC, we release our datasets, baseline models, and the synthetic data generation pipeline at https: //github.com/GGLAB-KU/gecturk.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Grammatical Error Correction (GEC) is among the well-established NLP tasks with dedicated shared tasks (e.g., BEA <ref type="bibr" target="#b4">(Bryant et al., 2019)</ref>), benchmarks, and even specific evaluation measures. With increasing interest from the community, the field is in constant need of novel tools, methods, and more importantly, extensions to other languages.</p><p>Recently, there has been an explosion of research about GEC for high-resource languages, especially for English <ref type="bibr" target="#b26">(Rothe et al., 2021;</ref><ref type="bibr" target="#b25">Omelianchuk et al., 2020;</ref><ref type="bibr" target="#b4">Bryant et al., 2019)</ref>. These recent techniques either formulate the task as neural machine translation, i.e., generation <ref type="bibr" target="#b26">(Rothe et al., 2021)</ref>, or token classification to detect erroneous tokens <ref type="bibr" target="#b25">(Omelianchuk et al., 2020)</ref>. The first approach mainly utilizes and engineers vanilla Transformers to generate the corrected text, while the second focuses on engineering a set of errors and transformation rules. Both formulations require a large set of parallel corpora containing grammatically correct and incorrect sentence pairs. Also, the latter approach additionally requires a highly curated dataset with annotations for correcting errors (i.e., location and type of the error). Constructing such a corpus with error annotations is nontrivial-especially for low-resource languages with rich morphology like Turkish. The challenge is due to grammar rules, a.k.a., writing errors being entangled in several layers, such as phonology, morphology, syntax, and semantics. As of today, there are no spelling or grammatical error datasets, as mentioned by <ref type="bibr">Çöltekin et al. (2023)</ref>, with the exception of the dataset introduced by <ref type="bibr" target="#b3">Arikan et al. (2019)</ref>.</p><p>To address this, we focus on the Turkish Language and utilize the official writing rules established by the Turkish Language Association 1 . We implement corruption, a.k.a. transformation, functions to generate instances that violate a specific rule, which requires challenging analysis of sentences on several linguistic levels, as well as curation of specialized lexicons. Then, we generate a large, synthetic, high-quality annotated corpus by applying transformation functions to professionally edited, modern Turkish articles. In addition to the transformation functions, we implement and share the reverse-transformation functions for validating the generated datasets and developing sequence tagger models, which achieve state-of-the-art in English.</p><p>In addition, we compile a corpus of movie reviews and manually annotate 300 sentences with the proposed error types to evaluate the models in a real-life setting. Furthermore, we design and implement several baselines using standard neural machine translation (NMT), sequence tagging and prefix-tuning. The NMT model only generates the correct sentece, but the sequence tagging model is trained to tag the tokens with the error type (if any) and then use our reverse transformation function on this error to generate the correct text. Finally, we perform prefix-tuning (Li and Liang, 2021) on mGPT <ref type="bibr" target="#b28">Shliazhko et al. (2022)</ref> for both detection and correction to test more recent techniques.</p><p>Our findings indicate that our pipeline approach using smaller models perform better than employing larger pretrained models in an end-to-end fashion when it comes to both synthetic and real-world datasets-particularly for the grammatical error detection task. Conversely, we observe that pretraining benefits the models in more realistic cases, despite the larger models still falling behind their simpler counterparts. Our results from out-of-domain tests imply that training on the synthetic dataset gives a strong prior to both smaller and larger models.</p><p>Our contributions can be summarized as follows:</p><p>• We propose the first comprehensive, expertcurated grammatical error schema for Turkish that covers 25 error types.</p><p>• We present a synthetic data generation pipeline that can be used to create arbitrary sized datasets, and can be easily extended to include new grammatical error types or lexicons, and can easily be modified to include custom tools (e.g., morphological analyzer and disambiguator).</p><p>• We present the first large-scale, fine-grained public dataset for Turkish grammatical correction and detection, along with a manually annotated realistic test set and strong baseline models.</p><p>We make our datasets, baseline models, and synthetic data generation pipeline publicly avail-able at https://github.com/GGLAB-KU/ gecturk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>English GEC Despite having a long history, with the BEA-2019 Shared Task on Grammatical Error Correction <ref type="bibr" target="#b4">(Bryant et al., 2019)</ref>, the GEC community started using neural models and formulating GEC as a neural machine translation task (i.e., translate from grammatically incorrect to correct sentences), which has become the dominant approach. Another recent approach, GEC-ToR <ref type="bibr" target="#b25">(Omelianchuk et al., 2020)</ref>, uses the idea of reverse transformations, which can be applied to a list of source tokens [x 1 , . . . , x n ], in order to produce the desired correct grammar. They use a sequence tagger with a BERT encoder. Each tag corresponds to a transformation where transformations are applied after the sequence tagging finishes. In contrast, the gT5 model released by <ref type="bibr">Xue et al. (2021)</ref> is a multilingual mT5 model fine-tuned on artificially corrupted sentences from the mC4 corpus and uses a span prediction and classification task to fix grammatical errors <ref type="bibr" target="#b26">(Rothe et al., 2021)</ref>. This does require a lot of additional training time, since the original mT5 model is not initially prepared for a similar task. Their model achieves SOTA results in 4 languages while only training once.</p><p>Turkish GEC Previously, <ref type="bibr" target="#b3">Arikan et al. (2019)</ref> proposed a neural sequence tagger model and a synthetically generated dataset to correct "de/da" clitic errors. In Turkish grammar, "de/da" is used both as a locative suffix and a conjuction meaning also, too that is written separately. For instance, "-de" is a locative suffix in the sentence "Evde (At home)"; while used as a conjuction here: "Ben de geliyorum (I'm coming too)". Mistakes in using these clitics are common among native speakers, often due to some contextual subtleties and oral dialect influencing the written language. Öztürk et al.</p><p>(2020) combined a contextual word embedding model, namely BERT, with a sequence tagger to correct "de/da" clitic errors. Although these errors are common, they constitute only a small portion of grammatical errors made by native speakers. In addition, while there are various forms of this error, the previous work only considers a few. Our data generation strategy considers multiple versions of the "de/da" clitic errors and many more common grammatical errors.  2) Then, morphological analysis is performed. 3) The validity of the sentence for the transformation function is checked. If the sentence is eligible, the transformation is applied with some probability p. 4)First, selecting tokens to modify, 5) then, checking if the reverse transformations can recover the original tokens. 6) If original cannot be recovered, the sentence is removed. If can be recovered 7) the transformed sentence is added to the corpus.</p><p>Parsing-based Approaches While most of the current approaches focus on reverse transformations and sequence tagging, there are several studies that involve the use of parsing techniques. <ref type="bibr" target="#b14">Flickinger and Yu (2013)</ref> create a parsing tree, and identify malformed parts of the tree to detect grammatical errors. da Costa (2021) use symbolic parsers and computational grammars for GEC and GED. On the same research line, Flickinger and Packard use bridged analyses combined with parsing to better allow for connecting two phrases in Head-driven Phrase Structure Grammar (HPSG).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Synthetic Data Generation</head><p>The overall generation process is given in Fig. <ref type="figure" target="#fig_0">1</ref>. First, we randomly sample from professionally edited Turkish corpora ( §3.1). Then, sentences are corrupted-if possible-following the expertcurated transformation rules explained in §3.2, as well as the use of a morphological analyzer. Finally, pairs of grammatically correct and corrupted sentences are added to the final Turkish GEC corpus following the M<ref type="foot" target="#foot_1">2</ref> scorer (Dahlmeier and Ng, 2012) (MaxMatch) data format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Corpus</head><p>Our proposed data generation pipeline is built upon the assumption that all input sentences are grammatically correct. Hence, we base our study on previously compiled newspaper corpora <ref type="bibr" target="#b12">(Diri and Amasyali, 2003;</ref><ref type="bibr" target="#b2">Amasyalı and Diri, 2006;</ref><ref type="bibr" target="#b5">Can and Amasyalı, 2016;</ref><ref type="bibr" target="#b16">Kemik NLP Group, 2022)</ref> that are proofread and went through a professional editing process. The articles are on various topics, including politics, sports, and medicine, and have been written by more than 95 authors for three different newspapers; in total, more than 7000 singly authored documents were collected between 2004-2012. Once we obtained grammatically correct source sentences, we performed several preprocessing steps, such as removing duplicates (2.9% of the combined dataset), ending with 138K unique sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transformation</head><p>The Turkish Language Association (TDK) 2 , a government agency founded in 1932, is responsible for providing resources to conduct scientific research on written and oral sources of Turkish. Within this scope, they specify and maintain a comprehensive list of publicly available writing rules 3 . We rely on this expert-curated list to generate forward and backward transformation rules, which we refer to as f and f -1 respectively. However, the list is long, and some writing rules are intuitive to native speakers, so that any errors made on these rules sound abnormal to them. We select the grammar rules that are most commonly used incorrectly by native speakers, determined by consulation with Turkish language experts and filtering the list from TDK using their feedback. We do not include any rules that are common for Turkish language learners but rarely made by native speakers. Table <ref type="table">1</ref> provides the full list of the transformation rules produced by this work. The transformations rely on a morphological analyzer, which was essential to get the transformations right for a morphologically rich language like Turkish. For more information on Turkish Morphology, we refer to <ref type="bibr" target="#b24">Oflazer (2014)</ref> and <ref type="bibr" target="#b18">Lewis (1985)</ref>.</p><p>Applying f For each sampled sentence, first, we shuffle the list of f s. This ensures that mutually exclusive transformation functions are applied with desired frequencies. Then, we iteratively apply each f on the sentence given with the pseudo-code in Algorithm 1. Here, f gets an input sentence s, morphological analysis of the sentence M s , an array of indicators for whether any transformation has been applied to the word-f lags, and parameter p ∈ (0,1). The algorithm, then, iterates over tokens (or pairs) and checks whether the token has been transformed. If not, it checks whether the token(s) are eligible for f . If eligible, we apply f with the probability p, since not all errors are made with the same frequency by native speakers. <ref type="foot" target="#foot_3">4</ref>Eligibility Check Some official writing rules require syntactic analysis at the token and sentence levels. For instance, to apply the transformation function CONJ_DE_SEP, one must perform morphological analysis and disambiguation to analyze the part-of-speech tags at the morpheme level. That is, CONJ_DE_SEP transformation can be applied only if a "-de/da" morpheme with a CONJUC-TION part-of-speech tag is found. Additionally, a small set of rules requires specialized lexicons, e.g., a list of exceptional foreign words for FOR-EIGN_R2_EXC. To address the former, we use a state-of-the-art morphological analyzer <ref type="bibr" target="#b9">Dayanik et al. (2018)</ref>, and the lexicons are taken from the official lists provided by TDK 2 .</p><p>Annotation Format We use the standard GEC annotation format following <ref type="bibr" target="#b23">Ng et al. (2013)</ref> and <ref type="bibr" target="#b4">Bryant et al. (2019)</ref>. An example annotation is given in Fig. <ref type="figure">2</ref>. Here, S and A refer to the ungrammatical sentence and edit annotations respectively. Each A contains starting and ending indices, the error type, the corrected phrase, and the id of the annotator.</p><p>Uyuyakaldıgı için hem işe gitmedi hem de akşamki yemege gelemeyecek .</p><p>(Because they overslept, they didn't go to work and won't be able to come to dinner tonight.) Postprocessing Despite the use of professionally edited source sentences, there are still some grammatically incorrect sentences that slip through. We detect these cases by taking advantage of a key property of our reverse transformations: since each f is reversible, we should see S = f -1 (f (S)) for each sentence S. Therefore, at the end of the transformation process, we perform this check on every generated, grammatically incorrect sentence. If a sentence fails this check, then we know it is problematic, and we remove it from the corpus. The final sentences are thus properly modified in the desired way, with no unintentional side effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Annotated Corpus</head><p>The annotated corpus includes more than 138K sentences, with 104K error annotations belonging to 25 error types given in Table <ref type="table">1</ref>. In this corpus, 50% of sentences are error free, in order for models to learn how to detect/correct sentences that are already grammatically correct. The generative pipeline controls the frequency of those error types, aiming to mimic the human error frequencies (see App. B). As in the dataset of CoNLL-2014 shared task <ref type="bibr" target="#b22">(Ng et al., 2014)</ref>, some error types appear more frequently than others. These frequencies are by the probability parameters p; therefore, the difference between frequencies of error types is an intended result. Our dataset is finally split into a train/val/test set of 70%/15%/15%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Curated Test Corpus</head><p>For a more realistic test setting, we used movie reviews from a popular website 5 shared by <ref type="bibr" target="#b1">Altinok (2023)</ref>. We asked a domain expert to annotate the sentences for the proposed error types, following the standard GEC annotation format. As a results, we curated a test dataset of 300 sentences, wherein half of the sentences were grammatically correct and the other half contained errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Tasks and Models</head><p>In this paper, we consider two tasks: Grammatical Error Correction (GEC) and Grammatical Error Detection (GED).</p><p>Grammatical Error Correction (GEC) takes as input a grammatically incorrect sentence and outputs the corrected version of the sentence. Formally, given an input sentence x = (x 1 , • • • , x T ) which may contain some grammar mistakes, the aim is to produce an output sentence y = (y 1 , • • • , y T ′ ) which contains no grammatical errors. Conditions are not imposed on how the model produces grammatically correct sentences.</p><p>Grammatical Error Detection (GED) takes a slightly different approach to this problem, with the goal of producing detailed information about the errors in the source sentence. This includes details about the type of error and the location of the error in the sentence. Formally, given an input sentence x = (x 1 , • • • , x T ), we can represent the problem as a token-level classification task, where the output is c = (c 1 , • • • , c T ), and c i represents the error type of token i. Given the knowledge that an error of type c k occurred at the location from m to n, it is then possible to apply the corresponding reverse transformation f -1 , and fix the error.</p><p>5 www.beyazperde.com</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Models</head><p>We introduce three models to evaluate the performance of GECTurk: An NMT baseline, a sequence tagger using BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> pretrained on Turkish, and mGPT using prefix-tuning. All models are trained using 1 Nvidia V100 GPU. We only provide the essential information about the models here. More details are available in Appendix A.</p><p>NMT Baseline: We train a vanilla transformer model <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref> for GEC. This choice is inspired by the most recent shared task on grammatical correction <ref type="bibr" target="#b4">(Bryant et al., 2019)</ref>, where many of the winning teams used transformer-based models and modeled the problem as a Neural Machine Translation (NMT). The training dataset consists of triples {(x i , y i , a i )} N i=1 , where x i is the i-th input sentence, y i is the corresponding ground truth corrected sentence, and a i are the annotations. During training, the model receives x i as input and tried to predict y i as output. Due to the nature of the formulation, NMT is only used for correction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence</head><p>Tagger: Similar to recent work <ref type="bibr" target="#b25">(Omelianchuk et al., 2020)</ref>, we train a sequence tagging model using a cased BERT encoder, pretrained on Turkish text <ref type="bibr" target="#b27">(Schweter, 2020)</ref> with default configurations and additional linear and softmax layers on the top. The BERT model uses the WordPiece tokenizer <ref type="bibr" target="#b30">(Wu et al., 2016)</ref> that segments tokens into subwords. Therefore, each sentence in the dataset is first tokenized into subwords and passed into the BERT encoder. We only hold the first subword's representation for words with multiple subword tokens. Then, the encoder's representations are linearly transformed and passed to the softmax layer to classify into possible error types described in Table <ref type="table">1</ref> or no error. The model is finetuned for token classification objective using cross-entropy loss.</p><p>The advantage of this model is the ability to per- Sınıf [da -&gt; ta] temizlendi. 99 4. YADA "-de/-da" written together with the word "ya" is always written separately.</p><p>Sen [ya da -&gt; yada] o buradan gidecek. 472 5. CONJ_DE_APOS Conjunction "-de/-da" cannot be used with an apostrophe.</p><p>[Ayşe de -&gt; Ayşe'de] geldi. 10859</p><p>6. CASE_DE Suffix "-de/-da" is written adjacent.</p><p>[Evde -&gt; Ev de] hiç süt kalmamıştı. Prefix Tuning: Inspired by the recent successes of prefix tuning (Li and Liang, 2021) as an alternative to model fine-tuning, we use Open-Prompt <ref type="bibr" target="#b11">(Ding et al., 2022)</ref> to perform prefix tuning on mGPT <ref type="bibr" target="#b28">(Shliazhko et al., 2022)</ref>. Despite being multilingual and primarily focused on other languages, mGPT achieves encouraging results on morphologically rich languages <ref type="bibr" target="#b0">(Acikgoz et al., 2022)</ref>. In prefix tuning, we append N trainable (soft) tokens to the front of each input. Therefore, given input</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>37462</head><formula xml:id="formula_0">x = (x 1 , • • • , x T ), the new input be- comes x = (s 1 , • • • , s N , x 1 , • • • , x T ),</formula><p>where the s i 's are the added artificial tokens. We then optimize only these tokens during training, while leaving the original model frozen.</p><p>Here, we model both correction and detection tasks in the same sequence generation approach, where the corrected sentence is first generated, and then information about the violated rule, and the location of this error is generated at the end of the sentence. This allows for one trained model to output both results. In order to train this correctly, the target sentence was appended with the details of the error type and location, and used for loss calculations. An example is provided in Fig 6 . 

5 Experimental Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>The list of datasets and their statistics are given in Table <ref type="table">2</ref>. GECTurk and MovieReview datasets are already described in §3.3 and §3.4 accordingly. The BOUN dataset <ref type="bibr" target="#b3">(Arikan et al., 2019)</ref> is a relatively smaller dataset of 15K training and 2K test sentences, containing only 2 error types. It also includes a complex split, a list of 100 sentences that are mentioned to be extra challenging by the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation</head><p>Grammatical Error Correction Following the <ref type="bibr" target="#b25">Omelianchuk et al. (2020)</ref> and <ref type="bibr" target="#b4">Bryant et al. (2019)</ref>, we report Precision (P ), Recall (R), and F 0.5 scores using the M 2 (MaxMatch) scorer <ref type="bibr" target="#b8">(Dahlmeier and Ng, 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grammatical Error Detection</head><p>To allow for a fair comparison with the BOUN dataset from <ref type="bibr" target="#b3">Arikan et al. (2019)</ref>, we use the same metrics, namely Precision (P ), Recall (R), and F 1 . Since the task is modeled as a sequence tagging problem, this aligns with the standard evaluation for sequence tagging, such as in <ref type="bibr" target="#b15">Huang et al. (2015)</ref>. For all GED results, we report macro metrics, which are computed by taking unweighted average of each classes' result. We use macro metrics over micro ones since distribution of grammatical errors types made by humans are imbalanced. To calculate these scores, we use SeqEval <ref type="bibr" target="#b21">(Nakayama, 2018)</ref>, a common library for evaluating sequence tagging tasks and use one tag for each error type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments and Results</head><p>We train the baseline models using the setup explained in Section §5 with three different fixed seeds. The mean and standard deviation of their performances on GEC and GED are given in Table 3. As can be seen, both SeqTag and mGPT provide strong results over 0.94 F 0.5 score for the GEC task, compared to the NMT baseline model. On the other hand, the detection task is performed more competently by SeqTag-as expected-than mGPT, which again achieves around 0.90 F 1 score. Moreover, the experiment on the effect of dataset sizes shows that the proposed dataset is more challenging than existing ones, with a steeper learning curve due to the larger number of error types. More information on the dataset size experiments can be found in Appendix C.</p><p>Successful detection does not always translate into successful correction because erroneous edits undermine the grammatical accuracy of the sentence, even when the grammatical error is successfully detected. Here, the M 2 scorer does identify this anomaly and explains the decrease in the correction scores. It is also worth noting that GED and GEC are two separate tasks, and models handle them differently. For example, generative models such as mGPT generate both the corrections and detections by predicting the next tokens, so there isn't necessarily a strong correlation between what is generated for each of them. On the other hand, SeqTag uses a pipeline approach, where it detects the errors first, then applies reverse transformations to fix them. Hence there is a stronger correlation between the detection and correction performances for SeqTag, as expected.</p><p>In order to test whether the performance of our models would transfer to different domains, we perform zero-shot experiments on the curated test set from the movie domain. We use our highest performing checkpoints on our synthetic test set, and evaluate without any additional training on the hand-annotated corpus as given in Table <ref type="table" target="#tab_4">3</ref>, second row. For detection, SeqTag performs similarly to synthetic setting, while mGPT's performance increases dramatically, proving the importance of being exposed to real-life data during pretraining. However, SeqTag still outperforms mGPT by a large margin due to its classification objective. On the other hand, both models perform significantly worse on the correction task for the currated test data compared to the synthetic setting, suggesting larger room for improvement on this more challenging test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Knowledge Transfer</head><p>Next, we investigate the transfer capacity of our models on unseen datasets using a different set of errors (i.e., mostly a subset) originally introduced to our models. We first evaluate our pretrained models on the BOUN <ref type="bibr" target="#b3">(Arikan et al., 2019)</ref> standard and complex test splits to see their zero-shot ability, seen in Table <ref type="table" target="#tab_6">4</ref>, first row. Surprisingly, our best model, SeqTag, achieves 0,80 F 1 , on-par with state-of-the-art for the standard split. It also surpasses state-of-the-art accuracy on the complex split by a large margin together with the mGPT model. This suggests that the error type knowledge  is mostly transferrable to other domains. Similar to our results on GECTurk, mGPT scores much lower on detection compared to SeqTag. However, the performance of mGPT is higher on BOUN dataset due to the small number of error types that are relatively more balanced. We note that despite the claims made by the authors of the BOUN dataset, our results suggest no additional complexity in the "complex" split as shown in Table <ref type="table" target="#tab_6">4</ref>, second row. Finally, we investigate the effectiveness of our general approach by training our proposed models from scratch on the BOUN <ref type="bibr" target="#b3">(Arikan et al., 2019)</ref> training split, given in Table <ref type="table" target="#tab_6">4</ref>, BOUN Full Training. For this setup, the NMT model was not able to converge, and just produced noise, hence, shown as 0.</p><p>Following our previous results, SeqTag achieves F 1 score of 0.91, surpassing the state-of-the-art by 0.04 pp, and its zero-shot performance by a large margin (0.11 pp). This suggests that pipeline approach is able to transfer a lot of knowledge. However, there is still a large gap that can be compensated by directly training on the actual domain and error types. On the other hand, the high scores provide cues for the strength of the proposed model. Surprisingly, prefix tuning of mGPT directly on the BOUN training dataset does not increase the performance compared to the zero-shot setting. This suggests two things: i) synthetic dataset such as ours, GECTurk, provides quality prior knowledge on the Turkish grammatical error types and ii) pretrained models have a considerably larger transfer capacity compared to training from scratch, as expected. Furthermore, for languages where the error types are mostly identified and can be fixed by a set of rules, a pipeline approach such as SeqTag proves more effective, efficient and robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this work, we have presented an annotated dataset for Grammatical Error Correction (GEC) and Detection (GED), GECTurk, containing more than 20 Turkish writing error types proposed by Turkish language experts. We have also introduced a flexible and extensible data generation pipeline that can be used to create a synthetic dataset from grammatically correct sentences. We used this pipeline to create a large-scale dataset using multiple opinion columns from Turkish newspapers. In addition, we have manually constructed a more challenging test set by annotating the moviereviews with the proposed error types.</p><p>Finally, we implemented a diverse set of strong baseline models, by training from scratch, finetuning, or using prefix tuning. Our results show that smaller models focusing on the simpler problem of detecting the error types outperform large pretrained models on both the synthetic and real-life datasets, especially for the detection task. On the other hand, we observe that pretraining helps the models to handle more realistic cases, even though they still lag behind the simpler models. Our outof-domain results suggest that training on the synthetic data gives a strong prior to both smaller and larger models.</p><p>Türkiye (TÜB İTAK) as part of the project "Automatic Learning of Procedural Language from Natural Language Instructions for Intelligent Assistance" with the number 121C132. We also gratefully acknowledge KUIS AI Lab for providing computational support. We thank our anonymous reviewers and the members of GGLab who helped us improve this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>One key issue is that mGPT is very computationally intensive to work with, even when only doing prefix tuning. This meant we could only train for 1 epoch. Another limitation is that the data generation pipeline is very time-consuming due to the use of a morphological analyzer. This means many resources are needed for very large-scale datasets. Additionally, needing hand-crafted rules and reverse transformations makes adding new rules slow.</p><p>Another key limitation is the need of dictionaries for exceptions to grammatical rules. Words in Turkish originating from other languages (notably Persian, French, and Arabic) don't always follow normal grammar rules, and thus require special lists of exceptions. While we included as many as possible, our list is not exhaustive, which can lead to rare edge cases where our pipeline fails. While dictionaries allow for adding learned knowledge directly, and is an invaluable part of our pipeline, these edge cases can cause problems during dataset generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations</head><p>One ethical issue is the misuse of grammatical correction models for cheating. Having such models and datasets mean that students can more easily use these to score better than normal on assignments and exams. This is bad for the student's learning, and also affects others who can be negatively impacted by this artificial success.</p><p>Despite this, we believe that grammatical error correction models are more beneficial than harmful. Many people, from authors to language learners, can benefit from having grammar corrections. By introducing a dataset and demonstrating models on Turkish, an under-served language in the NLP community, more people will be able to take advantage of this, similar to the many existing tools for English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Model Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 NMT Baseline</head><p>For tokenization, we used BerTurk-cased <ref type="bibr" target="#b27">(Schweter, 2020)</ref> tokenizer, passed to the NMT model. The transformer model has 6 encoders with embedding size 512, 6 decoder layers, and 8 heads. A dropout of 0.1 is used directly after the positional embeddings. For training, an Adam (Kingma and Ba, 2014) optimizer with β 1 = 0.9, β 2 = 0.98, and ε = 1e -9, and a learning rate of 1e -4 is used. We used batch size of 32, and trained the model for 100 epochs on a single V100. We use a standard cross-entropy loss during training, as follows:</p><formula xml:id="formula_1">L GEC (ŷ, y) = - N n=1 V c=1 log exp(ŷ n,c )y n,c V i=1 exp(ŷ n,i )<label>(1</label></formula><p>) Here, N is the batch size, V is the number of error classes, x is the model output, and y is the target. For the data size experiments, we used the same architecture but with slightly different hyperparameters. For both the 75% and 100% experiments, the model was trained for 100 epochs. For the 50% experiment, we only trained for 50 epochs. When training 10% and 25%, the Adam optimizer is used with the same β values, a learning rate of 5e -4, and a weight decay of 1e -4, for 100 epochs. The zero-shot testing on the BOUN <ref type="bibr" target="#b3">(Arikan et al., 2019)</ref> dataset is tokenized with the same tokenizer, and the best pre-trained model from GECTurk is used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Sequence Tagger</head><p>For training, we used the AdamW <ref type="bibr" target="#b20">(Loshchilov and Hutter, 2019)</ref> optimizer for 3 epochs, using batch size 16, learning rate 2e -5, weight decay 0.01, β 1 = 0.9, and β 2 = 0.999.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Prefix Tuning</head><p>We used the standard mGPT tokenizer and the OpenPrompt prefix tuning template. All experiments use 5 soft tokens at the beginning. Teacher forcing is used during training, and both the correction and detection tasks are formulated as a sequence generation problem. Following the settings from <ref type="bibr" target="#b0">Acikgoz et al. (2022)</ref>, we don't use weight decay for the bias and LayerNorm weights. The AdamW optimizer is used, with an initial learning rate of 5e -5, linearly decaying to 0 over the entire training. We clip the norm of the gradient at 1.0. Due to the computational requirements of mGPT, we only train on GECTurk for a single epoch on all experiments. However, on the smaller BOUN dataset, we train for 5 epochs. For inference, we also follow the hyperparameters from Acikgoz et al. ( <ref type="formula">2022</ref>), using a temperature of 1.0, top p of 0.9, no repetition penalty, and a beam search of 5 beams. For all experiments, a batch size of 3 was used. The max sequence length, including soft tokens, is set to 512. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B GECTurk Error Frequencies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Effect of Dataset Size</head><p>In order to obtain a better understanding of how important the dataset size is for this task, we conducted training on 1, 10, 25, 50, 75, and 100 percent of GECTurk and evaluated each model using the same evaluation measures. Fig. <ref type="figure">4</ref> shows how the performance of the models vary with more training data. NMT reaches its top point with around 75% of the training data, while SeqTag and mGPT achieve high F 0.5 scores with 25% of the training split. However, as discussed before, correction scores can be misleadingly high, since high frequency and easy to correct errors will push the results much higher. Hence we also plot the F1 scores for the detection task both on TurkishGEC and BOUN datasets in Fig. <ref type="figure">5</ref>. The plot shows that the GECTurk dataset is richer than the BOUN, since SeqTag and mGPT F1 performances are much steeper on the former. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Data generation pipeline. 1) First, a correct sentence is obtained from the grammatically correct corpus. 2) Then, morphological analysis is performed. 3) The validity of the sentence for the transformation function is checked. If the sentence is eligible, the transformation is applied with some probability p. 4)First, selecting tokens to modify, 5) then, checking if the reverse transformations can recover the original tokens. 6) If original cannot be recovered, the sentence is removed. If can be recovered 7) the transformed sentence is added to the corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>S</head><label></label><figDesc>Figure 2: The grammatically correct sentence is given in (a), the transformed version is given in (b), and the annotation format is given in (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Number of sentences with each writing rule type.</figDesc><graphic coords="12,306.14,70.86,218.25,174.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3</head><label>3</label><figDesc>Fig.3shows the frequencies of each error type in GECTurk dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 6 :</head><label>46</label><figDesc>Figure 4: Correction performance of each model trained on varying sizes of GECTurk</figDesc><graphic coords="13,70.87,99.99,218.24,218.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 Apply f Require: s := sentence, Ms := morphological analysis, flags, p Ensure: tags tags ← [ ]</figDesc><table /><note><p>n ← Number of tokens in s for i = 1 → n do if flags[i] and is_eligible(s, Ms) and f lipCoin(p) then tags.insert("A i {i + 1} ||| ruleID ||| sentence[i] ||| REQUIRED ||| -NONE-||| 0") sentence[i] ← transformed token at index i flags[i] ← False</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Detection and Correction results of the baselines on GECTurk (in-domain) and curated test dataset (out-ofdomain).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>GECTurk</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Detection</cell><cell></cell><cell></cell><cell>Correction</cell><cell></cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F 1</cell><cell>P</cell><cell>R</cell><cell>F 0.5</cell></row><row><cell>NMT</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.50 ± 0.01</cell><cell>0.84 ± 0.01</cell><cell>0.55 ± 0.01</cell></row><row><cell cols="7">SeqTag 0.90 ± 0.003 0.90 ± 0.013 0.90 ± 0.006 0.98 ± 0.001 0.98 ± 0.001 0.98 ± 0.001</cell></row><row><cell cols="4">mGPT 0.52 ± 0.02 0.38 ± 0.004 0.41 ± 0.01</cell><cell>0.95 ± 0.01</cell><cell>0.92 ± 0.03</cell><cell>0.94 ± 0.01</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Curated Test Data</cell><cell></cell><cell></cell></row><row><cell>NMT</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.31</cell><cell>0.62</cell><cell>0.35</cell></row><row><cell>SeqTag</cell><cell>0.94</cell><cell>0.87</cell><cell>0.89</cell><cell>0.85</cell><cell>0.80</cell><cell>0.84</cell></row><row><cell>mGPT</cell><cell>0.73</cell><cell>0.52</cell><cell>0.59</cell><cell>0.75</cell><cell>0.61</cell><cell>0.72</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Performance metrics of various models on the BOUN dataset. The table is divided into three sections: models trained on the GECTurk dataset and evaluated zero-shot on two different BOUN splits, and models exclusively trained and evaluated on BOUN.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://tdk.gov.tr/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.tdk.gov.tr/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://www.tdk.gov.tr/kategori/ icerik/yazim-kurallari/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>We choose the probabilities intuitively after an initial analysis on web corpus and student essays.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work has been supported by the <rs type="funder">Scientific and Technological Research Council</rs> of</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Transformers on multilingual clause-level morphology</title>
		<author>
			<persName><forename type="first">Tilek</forename><surname>Emre Can Acikgoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muge</forename><surname>Chubakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gözde</forename><surname>Kural</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deniz</forename><surname>Şahin</surname></persName>
		</author>
		<author>
			<persName><surname>Yuret</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.mrl-1.10</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the The 2nd Workshop on Multi-lingual Representation Learning (MRL)</title>
		<meeting>the The 2nd Workshop on Multi-lingual Representation Learning (MRL)<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates (Hybrid). Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="100" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A diverse set of freely available linguistic resources for Turkish</title>
		<author>
			<persName><forename type="first">Duygu</forename><surname>Altinok</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.acl-long.768</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="13739" to="13750" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic Turkish text categorization in terms of author, genre and gender</title>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Fatih</forename><surname>Amasyalı</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Banu</forename><surname>Diri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Information Systems</title>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="221" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting clitics related orthographic errors in Turkish</title>
		<author>
			<persName><forename type="first">Ugurcan</forename><surname>Arikan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Güngör</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzan</forename><surname>Uskudarli</surname></persName>
		</author>
		<idno type="DOI">10.26615/978-954-452-056-4_009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Recent Advances in Natural Language Processing</title>
		<meeting>the International Conference on Recent Advances in Natural Language Processing<address><addrLine>Varna, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>INCOMA Ltd</publisher>
			<date type="published" when="2019-09-02">2019. 2019. September 2-4, 2019</date>
			<biblScope unit="page" from="71" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The BEA-2019 shared task on grammatical error correction</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Øistein</forename><forename type="middle">E</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-4406</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="52" to="75" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Text2arff: A text representation library</title>
		<author>
			<persName><forename type="first">Ender</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehmet</forename><surname>Fatih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amasyalı</forename></persName>
		</author>
		<idno type="DOI">10.1109/SIU.2016.7495711</idno>
	</analytic>
	<monogr>
		<title level="m">2016 24th Signal Processing and Communication Application Conference (SIU)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="197" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Correction to: Resources for Turkish natural language processing: A critical survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Çagri Çöltekin</surname></persName>
		</author>
		<author>
			<persName><surname>Seza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Özlem</forename><surname>Dogruöz</surname></persName>
		</author>
		<author>
			<persName><surname>Çetinoglu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10579-022-09625-0</idno>
	</analytic>
	<monogr>
		<title level="j">Lang. Resour. Evaluation</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">489</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Using rich models of language in grammatical error detection</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Luís Morgado da Costa</publisher>
		</imprint>
		<respStmt>
			<orgName>Nanyang Technological University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Better evaluation for grammatical error correction</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="568" to="572" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Morphnet: A sequence-to-sequence model that combines morphological analysis and disambiguation</title>
		<author>
			<persName><forename type="first">Erenay</forename><surname>Dayanik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><surname>Akyürek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
		<idno>CoRR, abs/1805.07946</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Openprompt: An open-source framework for promptlearning</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-demo.10</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, ACL 2022 -System Demonstrations</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics, ACL 2022 -System Demonstrations<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05-22">2022. May 22-27, 2022</date>
			<biblScope unit="page" from="105" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Automatic author detection for Turkish texts</title>
		<author>
			<persName><forename type="first">Banu</forename><surname>Diri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehmet</forename><surname>Fatih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amasyali</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust parsing in hpsg: Bridging the coverage chasm</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Woodley</forename><surname>Packard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd International Conference on HPSG</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Toward more precision in correction of grammatical errors</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiye</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL Shared Task</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bidirectional LSTM-CRF models for sequence tagging</title>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno>CoRR, abs/1508.01991</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Our datasets</title>
		<author>
			<persName><forename type="first">Nlp</forename><surname>Kemik</surname></persName>
		</author>
		<author>
			<persName><surname>Group</surname></persName>
		</author>
		<ptr target="http://www.kemik.yildiz.edu.tr/data/File/2500koseyazisi.rar.Online" />
		<imprint>
			<date type="published" when="2022-11">2022. November-2022</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Turkish Grammar</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lewis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno>abs/2101.00190</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">seqeval: A python framework for sequence labeling evaluation</title>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Nakayama</surname></persName>
		</author>
		<ptr target="https://github.com/chakki-works/seqeval" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Software available from</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The CoNLL-2014 shared task on grammatical error correction</title>
		<author>
			<persName><forename type="first">Tou</forename><surname>Hwee</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">Hendy</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName><surname>Bryant</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-1701</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The CoNLL-2013 shared task on grammatical error correction</title>
		<author>
			<persName><forename type="first">Tou</forename><surname>Hwee</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning: Shared Task<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Turkish and its challenges for language processing</title>
		<author>
			<persName><forename type="first">Kemal</forename><surname>Oflazer</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10579-014-9267-2</idno>
	</analytic>
	<monogr>
		<title level="j">Lang. Resour. Evaluation</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="639" to="653" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GECToR -grammatical error correction: Tag, not rewrite</title>
		<author>
			<persName><forename type="first">Kostiantyn</forename><surname>Omelianchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaliy</forename><surname>Atrasevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Chernodub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Skurzhanskyi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.bea-1.16</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>Seattle, WA, USA → Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="163" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A simple recipe for multilingual grammatical error correction</title>
		<author>
			<persName><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Malmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-short.89</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="702" to="707" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Berturk -bert models for Turkish</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Schweter</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3770924</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">mGPT: Few-shot learners go multilingual</title>
		<author>
			<persName><forename type="first">Oleh</forename><surname>Shliazhko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alena</forename><surname>Fenogenova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Tikhonova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladislav</forename><surname>Mikhailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Kozlova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Shavrina</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2204.07580</idno>
		<idno>CoRR, abs/2204.07580</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Rudnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Macduff</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR, abs/1609.08144</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer</title>
		<author>
			<persName><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.41</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="483" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The role of contextual word embeddings in correcting the &apos;de/da&apos; clitic errors in Turkish</title>
		<author>
			<persName><forename type="first">Alperen</forename><surname>Hasan Öztürk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Degirmenci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzan</forename><surname>Güngör</surname></persName>
		</author>
		<author>
			<persName><surname>Uskudarli</surname></persName>
		</author>
		<idno type="DOI">10.1109/SIU49456.2020.9302477</idno>
	</analytic>
	<monogr>
		<title level="m">2020 28th Signal Processing and Communications Applications Conference (SIU)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
