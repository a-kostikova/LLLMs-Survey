<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">All Labels Together: Low-shot Intent Detection with an Efficient Label Semantic Encoding Paradigm</title>
				<funder ref="#_x7MdNAU">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiangshu</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Congying</forename><surname>Xia</surname></persName>
							<email>c.xia@salesforce.com</email>
						</author>
						<author>
							<persName><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
							<email>wenpeng@psu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Tingting</forename><surname>Liang</surname></persName>
							<email>liangtt@hdu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
							<email>psyu@uic.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Penn State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Hangzhou Dianzi University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">All Labels Together: Low-shot Intent Detection with an Efficient Label Semantic Encoding Paradigm</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CD594F6C4543E480D44122DCBDF717F3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In intent detection tasks, leveraging meaningful semantic information from intent labels can be particularly beneficial for few-shot scenarios. However, existing few-shot intent detection methods either ignore the intent labels, (e.g. treating intents as indices) or do not fully utilize this information (e.g. only using part of the intent labels). In this work, we present an endto-end One-to-All system that enables the comparison of an input utterance with all label candidates. The system can then fully utilize label semantics in this way. Experiments on three few-shot intent detection tasks demonstrate that One-to-All is especially effective when the training resource is extremely scarce, achieving state-of-the-art performance in 1-, 3-and 5-shot settings. Moreover, we present a novel pretraining strategy for our model that utilizes indirect supervision from paraphrasing, enabling zeroshot cross-domain generalization on intent detection tasks. Our code is at https://github. com/jiangshdd/AllLablesTogether.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Few-shot intent detection aims to identify the intents of user utterances with only a few labeled examples. Recent works can be mainly summarized into three categories: 1) Standard classifier-based approaches <ref type="bibr">(He et al., 2022a,b;</ref><ref type="bibr" target="#b27">Zhang et al., 2022</ref><ref type="bibr" target="#b28">Zhang et al., , 2021))</ref>, which leverage pretrained language models (PLMs) equipped with a standard classifier layer (e.g., MLP), treating intent labels as indices; 2) Example-based approaches <ref type="bibr" target="#b29">(Zhang et al., 2020;</ref><ref type="bibr" target="#b15">Mehri and Eric, 2021;</ref><ref type="bibr" target="#b22">Vulić et al., 2021)</ref>, which learn to compare the similarities between different examples and classify an input utterance based on the closest neighbor in the training data; 3) Intent semantic aware approaches <ref type="bibr" target="#b19">(Qu et al., 2021;</ref><ref type="bibr" target="#b25">Xia et al., 2021;</ref><ref type="bibr" target="#b4">Du et al., 2022)</ref>, which explicitly incorporate intent label words during training. However, both classifier-based and example-based methods disregard label semantic information, which is an important source of supervision in few-shot scenarios. Exiting intent semantic aware approaches also suffer from different drawbacks, such as relying on large-scale pretraining datasets and only partially using intent labels. More details on related work are discussed in Appendix A.</p><p>To solve these issues, we propose an end-to-end intent semantic aware model, One-to-All. It concatenates each utterance with the entire intent label set as the input and then encodes them simultaneously. In this way, the semantic information of all intents is fully utilized and integrated with utterances. The encoded embeddings of labels and utterances are subsequently used for contrastive learning. We define a new contrastive learning paradigm by comparing the representations of utterances and intents directly. This approach ensures utterances are moved closer to their gold intents while distancing them from any incorrect ones. Furthermore, we introduce a novel pretraining strategy for One-to-All that leverages indirect supervision from paraphrase identification datasets. Through this strategy, the model develops the ability to understand semantic similarities and distinctions among sentences, generalizing its comprehension to unseen intents in zero-shot intent detection tasks.</p><p>To demonstrate the effectiveness of our proposed model, we conduct experiments on three fine-grained intent detection tasks: BANKING77 <ref type="bibr" target="#b1">(Casanueva et al., 2020)</ref>, HWU64 <ref type="bibr">(Liu et al., 2019a)</ref> and CLINC150 <ref type="bibr" target="#b11">(Larson et al., 2019)</ref>, under low-shot settings (0-, 1-, 3-and 5-shot). The results show that One-to-All is especially effective in extreme few-shot scenarios, with an average improvement of 4.62% in 1-shot and 2.60% in 3-shot settings over the state-of-the-art (SOTA) without any pretraining. Our model also achieves SOTA in 5-shot scenarios with pretraining on out-of-domain (OOD) data. Furthermore, One-to-All shows great cross-domain generalization capabilities in the zero-shot setting when further pretrained on paraphrasing identification datasets.</p><p>Our contributions can be summarized as follows. First, we proposed an end-to-end One-to-All system that enables the comparison of an input utterance with all label candidates via a newly defined contrastive learning paradigm. To our knowledge, this is the first work that can encode the entire label space while modeling the intent identification problem. Second, we go beyond few-shot intent detection and further achieve zero-shot cross-domain generalization with a novel pretraining stage of our model: pretraining on paraphrase identification datasets. This is the first work that effectively uses indirect supervision from paraphrasing to handle zero-shot intent identification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">One-to-All Input Sequence Construction</head><p>Incorporating additional labels as contexts alongside utterances within an input sequence can help the model make a better decision <ref type="bibr" target="#b4">(Du et al., 2022)</ref>. One-to-All concatenates each utterance with the complete intent label set as the input and encodes them together. However, the limitation of the maximum input sequence length makes it impractical to include all labels in a single sequence, especially when the label space is large. Therefore, given an intent detection task with n intents, we take every k intents as a group and then all the intents will be divided into m = ⌈n/k⌉ groups. To keep each group with a consistent number of elements, we introduce a special placeholder token &lt;plh&gt;. For the group whose number of intents (s) is less than k, we fill it with (k -s) &lt;plh&gt;s to maintain consistency. Each utterance U is then duplicated m times and appended with the m groups of intents, respectively, yielding m input sequences, as shown in Figure <ref type="figure" target="#fig_0">1(A)</ref>. In this way, the model works on the entire intent space and incorporates multiple intents into a single sequence. In practice, we choose k by minimizing n mod k to avoid introducing too many &lt;plh&gt;s. Each input sequence is then shuffled k times during training to perform data augmentation, which is beneficial for few-shot tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Intent-Aware Contrastive Learning</head><p>To better exploit intent semantic information, we apply contrastive learning (CL) between utterances and intents. Previous works perform CL between input texts and their augmented views <ref type="bibr" target="#b5">(Gao et al., 2021;</ref><ref type="bibr" target="#b26">Yan et al., 2021;</ref><ref type="bibr" target="#b2">Chen et al., 2020;</ref><ref type="bibr" target="#b17">Mou et al., 2022)</ref>  ( <ref type="bibr" target="#b28">Zhang et al., 2021;</ref><ref type="bibr">Gunel et al., 2021;</ref><ref type="bibr" target="#b12">Lin et al., 2022)</ref>, while we directly perform CL between input texts (utterances) and classes (intents) since intents usually contain useful semantic meanings. Therefore, the representations of intents can be regarded as different views of utterances or explicit cluster centroids. As shown in Figure <ref type="figure" target="#fig_0">1</ref> (B), we feed the input sequence constructed from Section 2.1 into RoBERTa and obtain a list of token-level representations. Then for the utterance U and all the intents I j , j ∈ {1, ..., k} in the input sequence, we average the representations of their corresponding tokens as their representations z U and z j , j ∈ {1, ..., k}.</p><p>Finally, we use a shared MLP projector to project z U and z j into the same semantic space, yielding the final utterance representation h U and intent representation h j . We use h + U to denote the representation of the gold intent I U for U . Let sim(u, v) denote the cosine similarity between u and v. The contrastive learning loss is then defined as follows:</p><formula xml:id="formula_0">l = - 1 N b N b i=1 log e sim(h i U ,h i+ U )/τ k j=1 e sim(h i U ,h i j )/τ<label>(1)</label></formula><p>, where N b is the batch size. h i * means the representation in the i-th input in a batch. τ is the temperature parameter. Through the contrastive loss, One-to-All pushes utterances closer to their gold intents and meanwhile drives them away from all the incorrect intents. For the input sequences that do not include gold intents, we simply set the numerator of Equation 1 to 1 so the model only pushes the utterance away from other intents.</p><p>By shuffling the order of concatenated intents within the sequence, a large number of training instances and distinct contrastive pairs can be generated. This is particularly beneficial for both few-shot and contrastive learning. Furthermore, a more robust model is yielded since we enforce the model to recognize the correct intent regardless of its location in the concatenated input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Zero-shot Intent Detection via Paraphrase Identification Pretraining</head><p>Paraphrase identification <ref type="bibr" target="#b21">(Socher et al., 2011</ref>) is a task that aims at identifying if two sentences have the same meaning. Pretraining on the paraphrase detection identification dataset encourages models to capture the essence of utterances rather than relying solely on surface-level patterns. This can improve the ability of One-to-All to distinguish between similar intents that might have subtle differences in wording or phrasing. Therefore, we propose a novel pretraining stage to effectively use indirect supervision from paraphrasing for zeroshot intent detection task. Specifically, for each sentence, we regard its paraphrase as its gold label and select other t most similar sentences with the help of Sentence-BERT <ref type="bibr" target="#b20">(Reimers and Gurevych, 2019)</ref> as the negative labels. For the target task with n intents, we set t = n -1 so each sentence has totally n labels to compare as well. We set k, which is the number of intents in each group, the same as the target task to keep the pretraining and downstream tasks consistent.</p><p>Pretraining on out-of-domain (OOD) data can further improve the performance of One-to-All. Given an intent detection task, we treat other data which do not have domain overlapping with this task as the OOD data. During OOD pretraining, the intent space is a union of intents from all the OOD data. We follow the same sequence construction strategy and training process stated in Section 2.1 and Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We conduct experiments on three intent detection tasks under both few-shot and zero-shot settings.</p><p>Datasets. Our model is evaluated on three finegrained intent detection datasets: BANKING77 <ref type="bibr" target="#b1">(Casanueva et al., 2020)</ref>, HWU64 <ref type="bibr">(Liu et al., 2019a)</ref>, and CLINC150 <ref type="bibr" target="#b11">(Larson et al., 2019)</ref>. Each dataset contains one or multiple domains and a finegrained intent space. Dataset statistics are shown in Appendix Table <ref type="table" target="#tab_3">2</ref>. We randomly sample 10% training data as the dev set, following <ref type="bibr" target="#b28">Zhang et al. (2021)</ref> and <ref type="bibr" target="#b16">Mehri et al. (2020)</ref>. For each target task, we use the other two datasets excluding the similar domains and intents as its OOD pretraining data. For example, we take BANKING77 as the target task, which contains banking-related intents.</p><p>To obtain its OOD pretraining data, we combine the data from HWU64 and CLINC150 but remove the "Banking" and "Credit Cards" domains. The statistics of the OOD data for each target task are shown in Appendix Table <ref type="table" target="#tab_4">3</ref>. We also sample 10% OOD data as the dev set for the OOD pretraining.</p><p>For paraphrase detection pretraining, we leverage the public Quora Question Pairs (QQP) dataset<ref type="foot" target="#foot_0">1</ref> . To incorporate more sentences in a single pair, we filter short sentence pairs from QQP by setting the max number of words and characters in a sentence as 10 and 40, respectively. The filtered QQP dataset contains 31,412 paraphrase pairs.</p><p>Baselines. We compare our method with six baselines in the three categories as we described in Section 1. Standard classifier-based baselines: 1) RoBERTa with a standard classifier. 2) CPFT <ref type="bibr" target="#b28">(Zhang et al., 2021)</ref> performs self-supervised CL on multiple intent detection datasets and applies supervised CL on the target task. 3) RoBERTa-SPI <ref type="bibr" target="#b27">(Zhang et al., 2022)</ref> introduces two regularizers to improve supervised pretraining via isotropization. For example-based approaches, we consider its prior SOTA model: 4) DNNC <ref type="bibr" target="#b29">(Zhang et al., 2020)</ref>, which identifies intents by finding the nearest neighbors of utterances in the training set and is pretrained on three natural language inference datasets. For intent semantic aware methods, we compare: 5) Context-TE and 6) Parallel-TE <ref type="bibr" target="#b4">(Du et al., 2022)</ref>. These approaches incorporate multiple intents into a single textual entailment sequence. Context-TE relies on indirect supervision from MNLI <ref type="bibr" target="#b23">(Williams et al., 2018)</ref>. Parallel-TE is more comparable to One-to-All, it also encodes utterances and intents simultaneously, but it is a pipeline model that only selects top-k intents for each utterance. Among all the baselines, CPFT and DNNC are the prior SOTA models for 5-and 10-shot settings. Our paper considers more challenging scenarios, specifically the 1-and 3-shot settings. The implementation of the baselines and our model are detailed in Appendix C.</p><p>Results. For few-shot experiments, we conduct three runs with distinct training data samples, following <ref type="bibr" target="#b4">Du et al. (2022)</ref> accuracy and standard deviation on three datasets under 1-, 3-, and 5-shot settings. One-to-All outperforms all the baselines remarkably in 1-and 3-shot scenarios across three datasets without any pretraining. For example, One-to-All improves the state-of-the-art result for 1-shot on HWU64 by 6.81%. After pretraining on OOD data, the improvement percentage increases to 13.64%. For the 5-shot setting, One-to-All achieves comparable results without pretraining and outperforms all the baselines with pretraining on OOD. The examplebased model DNNC performs extremely poorly on 1-shot tasks even though it achieves good performances in 5-shot, showing its limitation when training resources are extremely scarce.</p><p>For the zero-shot setting, we first did preliminary experiments evaluating all baselines and our model on the target tasks without extra pretraining. The results are reported in Appendix Table 4. Despite poor performance across all models, One-to-All remains the top-performing approach. Then we pretrain the model on OOD/QQP data without accessing any in-domain training data. The results are shown in Figure <ref type="figure" target="#fig_1">2</ref>. The zero-shot performance of One-to-All exhibits significant improvement following pretraining on QQP data, indicating the effectiveness of our novel pretraining strategy. Comparing the zero-shot results with the few-shot results in Table <ref type="table" target="#tab_1">1</ref>, we can observe that the performance of One-to-All (OOD) under the zero-shot setting even outperforms some baselines (RoBERTa, CPFT, and DNNC) under the 1-shot setting. Thus, we compare our model, One-to-All, with the two strongest baselines, Context-TE and Parallel-TE, in the zero-shot setting. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, One-to-All (OOD) outperforms both Context-TE (OOD) and Parallel-TE (OOD) in most settings. The zero-shot performance of One-to-All is further boosted by pre-  Analysis We investigate how our end-to-end design impacts our model performance by comparing it with Parallel-TE through a case study on BANKING77. We run One-to-All and Parallel-TE under the 3-shot setting three times. The top-k filtering step in Parallel-TE misses gold intents for 89 utterances in the test set, while One-to-All can correctly predict 34.7 of them on average, which brings a 1.1% improvement given the size of test set is 3080. This suggests that our end-to-end design effectively identifies partial intents that may have been overlooked by the pipeline system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose an end-to-end intent semantic aware intent detection model One-to-All to fully leverage intent semantics via contrastive learning. Experiments show that it is especially effective when training resource is scarce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Although we perform OOD pretraining on our model and gain performance improvement, the OOD data we use in our experiment is only from at most two datasets. There are many publicly available intent detection datasets from different domains, such as ATIS <ref type="bibr" target="#b9">(Hemphill et al., 1990)</ref> and SNIPS <ref type="bibr" target="#b3">(Coucke et al., 2018)</ref>, can be used for the OOD pretraining. We believe pretraining on largescale OOD datasets can further boost the performance of our model, and we leave it for our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Related Work</head><p>Recent research on low-shot intent detection can be broadly classified into three main categories: standard classifier-based, example-based, and intent semantic aware approaches.</p><p>Standard-classifier methods discard intent semantics and usually require an extra pretraining process on the extra corpus. For example, <ref type="bibr">He et al. (2022a,b)</ref>; <ref type="bibr" target="#b24">Wu et al. (2020)</ref> pretrain models on large-scale dialog datasets. <ref type="bibr" target="#b28">Zhang et al. (2021)</ref> pretrain a RoBERTa model on various intent datasets via self-supervised contrastive learning. <ref type="bibr" target="#b27">Zhang et al. (2022)</ref> propose two regularizers to improve its supervised pretraining step via isotropization.</p><p>Example-based approaches aim to learn the similarities between various examples and classify an input utterance by identifying its closest neighbor in the training data. For instance, <ref type="bibr" target="#b29">Zhang et al. (2020)</ref>; <ref type="bibr" target="#b15">Mehri and Eric (2021)</ref> determines the intent of an utterance by searching for its nearest neighbors among all training utterances. These approaches also ignore the semantic information of intents.</p><p>Existing intent semantic aware approaches also exhibit various drawbacks and limitations. For example, LSAP <ref type="bibr" target="#b18">(Mueller et al., 2022)</ref> incorporates intent semantics into generative models via pretraining. More specifically, during the pretraining stage, LSAP takes partially masked utterance-intent pairs as the input and predicts the masked contents. However, this approach relies on large-scale pretraining data to obtain decent performance. <ref type="bibr" target="#b19">Qu et al. (2021)</ref> and <ref type="bibr" target="#b25">Xia et al. (2021)</ref> cast ID as textual entailment (TE), treating utterances and intents as premises and hypotheses, respectively. But these two models are only able to compare one single utterance with one single intent, which makes them unaware of other intent options. <ref type="bibr" target="#b4">Du et al. (2022)</ref> learn to select the best intent for an utterance by providing the top-k intents for that utterance in one training example. Despite providing a one-to-many comparison, <ref type="bibr" target="#b4">Du et al. (2022)</ref> are only able to view the top-k intents rather than the entire intent label set during training. Additionally, <ref type="bibr" target="#b4">Du et al. (2022)</ref> propose a pipeline model. Their performance is constrained by the accuracy of the first stage of the pipeline, which is identifying the top-k intents. Moreover, <ref type="bibr">Lamanov et al. (2022)</ref> propose a template-based approach for modeling intents and utterances as sentence pairs. <ref type="bibr" target="#b0">Burnyshev et al. (2021)</ref> use a deep contextualized model to embed utterances and the natural language descriptions of user intents in zero-shot scenarios.</p><p>Different from all the literature we discussed above, we propose an end-to-end intent semantic aware system. By fully utilizing label semantics via contrastive learning, our model achieves SOTA performance even without pretraining on additional datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Dataset Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation Details.</head><p>All the baselines and One-to-All adopt RoBERTa-base <ref type="bibr">(Liu et al., 2019b)</ref> as the backbones for a fair comparison.</p><p>Baseline implementation. For the RoBERTa model, we implement it with the Hugging Face 2 library. For RoBERTa-SPI, DNNC, Context-TE, and Parallel-TE, we directly run their open-source code under our experiment settings. It is important to note that the original Context-TE and Parallel-TE models utilize RoBERTa-large as their base models. However, in our implementation, we replace it with RoBERTa-base to ensure a fair comparison. Regarding CPFT, we strive to replicate its methodology to the best of our ability, given the unavailability of its code and pretrained checkpoints to the public. The original paper trained CPFT on a fixed set of 5-shot data, while in our experiments, 2 https://huggingface.co/  we conduct three runs with uniquely sampled fewshot training data, which mitigates the potential influence of data sampling bias.</p><p>One-to-All implementation. For few-shot tasks and OOD pretraining, we train the model for 10 and 3 epochs, respectively, and keep the best ones on the dev set. For paraphrase detection pretraining, we train the model for 3 epochs. All the training batch size is set to 8 and the learning rate is 2e-5. The output dimension of the MLP projector is set to 768 and the temperature parameter τ = 0.1. We set k to 26, 32, 30 for BANKING77, HWU64, and CLINC150, respectively, according to the observations on their dev sets. Details are discussed in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Zero-shot Preliminary Experiments E Additional Analysis</head><p>We try to explore how the number of intents in each input sequence, k, influences the performance. We explore the influence of k by conducting experiments on the dev set of HWU64 and BANKING77 with different k values. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, the model performance stays similar when k &gt; 20 but drops sharply when k is below 10. This is probably due to the reduction of contrastive instances in a</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of One-to-All. (A) shows the input sequence construction strategy. I U indicates the gold intent of U . (B) shows positive and negative instances in contrastive learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Zero-shot performance. OOD or QQP indicates that the model is pretrained on OOD or QQP data. training on both OOD and QQP data, outperforming both Context-TE and Parallel-TE by a large margin. Once again, this finding highlights the effectiveness of utilizing indirect supervision from paraphrase identification datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Model performance with different k values on the dev set of HWU64 and BANKING77.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Positive Negative Negative (A) Pair Construction (B) Contrastive Learning</head><label></label><figDesc>, or instances under different classes</figDesc><table><row><cell cols="3">I would like to get some extra cards?</cell><cell></cell><cell cols="3">getting spare card</cell></row><row><cell>&lt;s&gt; &lt;s&gt; &lt;s&gt;</cell><cell>&lt;/s&gt; &lt;/s&gt; &lt;/s&gt;</cell><cell>&lt;/s&gt; &lt;/s&gt; &lt;/s&gt;</cell><cell>... ... ...</cell><cell>&lt;/s&gt; &lt;/s&gt; &lt;/s&gt;</cell><cell>&lt;plh&gt;</cell><cell>&lt;/s&gt; &lt;/s&gt; &lt;/s&gt;</cell></row><row><cell></cell><cell cols="4">RoBERTa + Projector</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>. Table1shows the average Test accuracy (%) and standard deviation on three dataset under three few-shot scenarios. The first and second highest results are formatted in bold and underline, respectively.</figDesc><table><row><cell>Model</cell><cell></cell><cell>BANKING77</cell><cell></cell><cell></cell><cell>HWU64</cell><cell></cell><cell></cell><cell>CLINC150</cell></row><row><cell></cell><cell>1-shot</cell><cell>3-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>3-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>3-shot</cell><cell>5-shot</cell></row><row><cell>RoBERTa</cell><cell cols="9">35.31(2.22) 64.78(0.76) 75.47(1.69) 40.34(2.66) 67.44(1.37) 75.71(1.26) 48.48(1.03) 80.91(2.10) 86.82(1.40)</cell></row><row><cell>CPFT</cell><cell cols="9">40.71(2.25) 71.57(0.34) 79.73(0.52) 50.61(2.18) 72.91(2.32) 79.82(1.64) 58.58(0.57) 83.12(0.43) 90.60(0.70)</cell></row><row><cell cols="10">RoBERTa-SPI 43.81(2.01) 65.63(0.66) 72.20(1.45) 53.75(2.13) 70.95(1.14) 75.58(1.24) 67.57(0.99) 83.31(1.78) 87.76(1.08)</cell></row><row><cell>DNNC</cell><cell cols="9">30.23(2.51) 72.21(1.02) 79.94(1.77) 29.77(1.45) 75.25(2.69) 79.31(0.19) 30.17(2.33) 87.07(0.44) 90.44(1.03)</cell></row><row><cell>Context-TE</cell><cell cols="9">64.22(0.50) 73.27(0.43) 77.07(0.57) 64.39(1.64) 73.45(1.72) 78.16(0.94) 74.71(0.91) 84.56(1.54) 87.61(0.79)</cell></row><row><cell>Parallel-TE</cell><cell cols="9">64.34(1.23) 72.20(1.00) 76.23(0.37) 61.96(0.46) 74.10(0.51) 78.10(1.40) 74.54(0.81) 83.66(0.84) 86.61(0.58)</cell></row><row><cell>One-to-All</cell><cell cols="9">66.36(0.46) 76.13(0.45) 79.75(0.78) 68.77(1.94) 78.16(2.04) 79.89(0.30) 77.63(0.63) 87.09(1.44) 89.88(0.81)</cell></row><row><cell>w/ OOD</cell><cell cols="9">67.93(0.28) 76.92(0.18) 80.51(0.88) 73.17(0.37) 79.95(0.68) 82.50(1.05) 79.21(0.43) 88.01(1.46) 90.76(0.63)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Dataset statistics.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Domain Utterance Intent</cell></row><row><cell>BANKING77</cell><cell>1</cell><cell>13,083</cell><cell>77</cell></row><row><cell>HWU64</cell><cell>21</cell><cell>10,030</cell><cell>64</cell></row><row><cell>CLINC150</cell><cell>10</cell><cell>22,500</cell><cell>150</cell></row><row><cell>Target</cell><cell cols="3">Domain Utterance Intent</cell></row><row><cell>BANKING77</cell><cell>29</cell><cell>28,030</cell><cell>183</cell></row><row><cell>HWU64</cell><cell>11</cell><cell>35,453</cell><cell>225</cell></row><row><cell>CLINC150</cell><cell>21</cell><cell>9,854</cell><cell>63</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Statistics of the OOD pretraining data used for each target task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Zero-shot performance (%) without pretraining on additional datasets. DNNC is not included since it requires at least one training example.</figDesc><table><row><cell>Model</cell><cell cols="3">BANKING HWU CLINC</cell></row><row><cell>RoBERTa</cell><cell>1.65</cell><cell>1.77</cell><cell>2.01</cell></row><row><cell>CPFT</cell><cell>1.91</cell><cell>2.03</cell><cell>2.78</cell></row><row><cell>RoBERTa-SPI</cell><cell>1.90</cell><cell>1.87</cell><cell>2.52</cell></row><row><cell>Context-TE</cell><cell>1.04</cell><cell>1.20</cell><cell>1.43</cell></row><row><cell>Parallel-TE</cell><cell>1.66</cell><cell>2.14</cell><cell>2.24</cell></row><row><cell>One-to-All</cell><cell>6.04</cell><cell>13.05</cell><cell>5.84</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors appreciate the reviewers for their insightful comments and suggestions. This work is supported in part by <rs type="funder">NSF</rs> under grant <rs type="grantNumber">III-2106758</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_x7MdNAU">
					<idno type="grant-number">III-2106758</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>single pair. Therefore, we set k to 26, 32, 30 for BANKING77, HWU64, and CLINC150, respectively, and it can bring two benefits: 1) it is more friendly for the paraphrase detection pretraining as it is the max number of the sentences that can be incorporated into a single pair; 2) it minimizes n mod k as we discussed in Section 2.1.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">InFoBERT: Zero-shot approach to natural language understanding using contextualized word embedding</title>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Burnyshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Bout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Malykh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Piontkovskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RANLP</title>
		<meeting>RANLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="208" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient intent detection with dual sentence encoders</title>
		<author>
			<persName><forename type="first">Iñigo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tadas</forename><surname>Temčinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Gerz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI</title>
		<meeting>the 2nd Workshop on Natural Language Processing for Conversational AI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Snips voice platform: an embedded spoken language understanding system for privateby-design voice interfaces</title>
		<author>
			<persName><forename type="first">Alice</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaa</forename><surname>Saade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Théodore</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Caulier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clément</forename><surname>Doumouro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Caltagirone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maël</forename><surname>Primet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Dureau</surname></persName>
		</author>
		<idno>ArXiv, abs/1805.10190</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning to select from multiple options</title>
		<author>
			<persName><forename type="first">Jiangshu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno>ArXiv, abs/2212.00301</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SimCSE: Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6894" to="6910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Alexis Conneau, and Veselin Stoyanov. 2021. Supervised contrastive learning for pre-trained language model fine-tuning</title>
		<author>
			<persName><forename type="first">Beliz</forename><surname>Gunel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tree-structured semisupervised contrastive pre-training for task-oriented dialog understanding</title>
		<author>
			<persName><forename type="first">Wanwei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinpei</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="553" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unified dialog model pre-training for task-oriented dialog understanding and generation</title>
		<author>
			<persName><forename type="first">Wanwei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinpei</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="187" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The ATIS spoken language systems pilot corpus</title>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">T</forename><surname>Hemphill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">J</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Natural Language: Proceedings of a Workshop</title>
		<meeting><address><addrLine>Hidden Valley, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-06-24">1990. June 24-27,1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Andrey Bout, and Irina Piontkovskaya. 2022. Template-based approach to zeroshot intent recognition</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lamanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Burnyshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katya</forename><surname>Artemova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Malykh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INLG</title>
		<meeting>INLG</meeting>
		<imprint>
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An evaluation dataset for intent classification and out-of-scope prediction</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">J</forename><surname>Peper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Laurenzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjia</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Mars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
		<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1311" to="1316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Detect rumors in microblog posts for low-resource domains via adversarial contrastive learning</title>
		<author>
			<persName><forename type="first">Hongzhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingfei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Guang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of NAACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2543" to="2556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Benchmarking natural language understanding services for building conversational agents</title>
		<author>
			<persName><forename type="first">Xingkun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Eshghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawel</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<idno>ArXiv, abs/1903.05566</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>ArXiv, abs/1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Example-driven intent prediction with observers</title>
		<author>
			<persName><forename type="first">Shikib</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2979" to="2992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dialoglue: A natural language understanding benchmark for task-oriented dialogue</title>
		<author>
			<persName><forename type="first">Shikib</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2020. 2009.13570</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Disentangled knowledge transfer for OOD intent discovery with unified contrastive learning</title>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huixing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL (Short Papers)</title>
		<meeting>ACL (Short Papers)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="46" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Label semantic aware pre-training for few-shot text classification</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvatore</forename><surname>Romeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saab</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="8318" to="8334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Few-shot intent classification by gauging entailment relationship between utterance and semantic label</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI</title>
		<meeting>the 3rd Workshop on Natural Language Processing for Conversational AI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using Siamese BERTnetworks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
		<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPs</title>
		<meeting>NeurIPs</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ConvFiT: Conversational fine-tuning of pretrained language models</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Coope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Gerz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paweł</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iñigo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Mrkšić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1151" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">TOD-BERT: Pre-trained natural language understanding for task-oriented dialogue</title>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="917" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Incremental few-shot text classification with multi-round new classes: Formulation, dataset and system</title>
		<author>
			<persName><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1351" to="1360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ConSERT: A contrastive framework for self-supervised sentence representation transfer</title>
		<author>
			<persName><forename type="first">Yuanmeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rumei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5065" to="5075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fine-tuning pre-trained language models for few-shot intent detection: Supervised pre-training and isotropization</title>
		<author>
			<persName><forename type="first">Haode</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haowen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Ming</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="532" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Few-shot intent detection via contrastive pre-training and fine-tuning</title>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trung</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghyun</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1906" to="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discriminative nearest neighbor few-shot intent detection by transferring natural language inference</title>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5064" to="5082" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
