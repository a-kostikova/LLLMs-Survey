<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Theia: Weakly Supervised Multimodal Event Extraction from Incomplete Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Farhad</forename><surname>Moghimifar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fatemeh</forename><surname>Shiri</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Van</forename><surname>Nguyen</surname></persName>
							<email>van.nguyen5@defence.gov.au</email>
							<affiliation key="aff1">
								<orgName type="department">Information Sciences Division, Defence Science and Technology Group</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Theia: Weakly Supervised Multimodal Event Extraction from Incomplete Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F07097A0F817326011487E8254512BB7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Event extraction from multimodal documents is an important yet under-explored problem. One challenge faced by this task is the scarcity of paired image-text datasets, making it difficult to fully exploit the strong representation power of multimodal language models. In this paper, we present Theia, an end-to-end multimodal event extraction framework that can be trained on incomplete data. Specifically, we couple a generation-based event extraction model with a customised image synthesizer that can generate images from text. Our model leverages capabilities of pre-trained visionlanguage models and can be trained on incomplete (i.e. text-only) data. Experimental results on existing multimodal datasets demonstrate the effectiveness of our approach for both synthesising missing data and extracting events over state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Event extraction is an important task in natural language processing that aims to identify and extract structured information about events and their arguments from text. Despite the challenging nature of this task, being rooted in the ambiguity, complexity and diversity of natural language, recent years have seen significant improvements from end-to-end deep learning models <ref type="bibr" target="#b24">(Wadden et al., 2019;</ref><ref type="bibr" target="#b6">Du and Cardie, 2020;</ref><ref type="bibr" target="#b9">Hsu et al., 2022)</ref> over the traditional rule-based approaches <ref type="bibr" target="#b23">(Valenzuela-Escárcega et al., 2015;</ref><ref type="bibr" target="#b4">Bui et al., 2013)</ref>.</p><p>There is a rapid increase of multimodal documents online, propelled by the high prevalence of camera-enabled devices. It has been shown that other modalities supplement the information that is available in text <ref type="bibr" target="#b13">(Li et al., 2020)</ref>. However, event extraction from multimodal information is an under-explored area, as existing methods were primarily developed for textual information.</p><p>Some recent works studied this question through the use of the visual modality <ref type="bibr">(Li et al., 2022a</ref><ref type="bibr" target="#b13">(Li et al., , 2020))</ref>. These approaches, however, suffer from two shortcomings. Firstly, existing models formulate event extraction as a classification problem, in which trigger words and entity recognition modules are used as features. This limits their performance in capturing high-level complex event structures. Secondly, these methods require a complete set of text-(corresponding)image pairs at the time of training, which limits their performance when models face missing data, where images are unavailable for all or a portion of the training data.</p><p>To overcome these shortcomings, we propose an end-to-end sequence-generation-based multimodal event extraction model. Our proposed approach incorporates an image synthesizer model to handle the missing images during training. The image synthesizer, conditioned on the given textual information, generates a visual representation that helps to train the encoder-decoder structure of our event extraction model, and customises the generated images by reducing the domain shift between the original domain of the pre-trained models and the target domain (Figure <ref type="figure" target="#fig_0">1</ref>). Experiments on the task of multimodal event extraction confirm the strong superiority of our model, by a margin of 7%, over state-of-the-art models. Furthermore, empirical results using the images synthesise show the capability of our model to portray domain-dependant visual context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The task of event extraction has been widely studied <ref type="bibr" target="#b0">(Ahn, 2006;</ref><ref type="bibr" target="#b8">Hogenboom et al., 2011)</ref>, where initially rule-based classification approaches were developed to address this task <ref type="bibr" target="#b4">(Bui et al., 2013;</ref><ref type="bibr" target="#b20">Ritter et al., 2012)</ref>. With the advances in deep learning, models based on neural networks have also been developed for this task <ref type="bibr" target="#b18">(Nguyen and Nguyen, 2019;</ref><ref type="bibr" target="#b28">Zhang et al., 2019)</ref>. More recently, several studies leverage the strong representation and reasoning capabilities of pre-trained language models <ref type="bibr">(Li et al.</ref>, 2021; <ref type="bibr" target="#b25">Wu et al., 2022;</ref><ref type="bibr" target="#b9">Hsu et al., 2022)</ref>.</p><p>While these studies achieve excellent performance on text-only benchmarks, they fail to account for other modalities. By proposing a new dataset and a multimodal event extraction model, <ref type="bibr" target="#b13">Li et al. (2020)</ref>; <ref type="bibr" target="#b27">Zhang et al. (2017)</ref>; <ref type="bibr" target="#b16">Moghimifar et al. (2023)</ref> showed that including the visual context results in better extraction performance. <ref type="bibr">Li et al. (2022a)</ref> proposed an approach based on pre-trained vision-language models <ref type="bibr" target="#b19">(Radford et al., 2021)</ref> for addressing multimodal event extraction. However, these approaches cast event extraction as a classification problem, whereby their model is limited to a fixed schema. Furthermore, these models require a complete set of text-image pairs for training, which hinders their real-world practicality where it is highly likely that a part of the visual data is missing. Unlike these approaches, we propose a sequence generation model that is capable of synthesising the visual context to alleviate the problem of missing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multimodal Event Extraction</head><p>Problem Formulation. Given a corpus of sequences of tokens X = {x 1 , . . . , x N } of length N , where each x i = (w 1 i , w 2 i , . . . ) represents the i-th sentence of the corpus and w m i is a token from the vocabulary W. In addition, there is the corresponding visual context V = {v 1 , . . . , v M } of length M , where M &lt; N , and for each v ∈ V, it is paired with one sentence x ∈ X . Our goal is to extract event mentions, including the event type e ∈ E, and roles r ∈ R together with their arguments a ∈ W.</p><p>Multimodal Event Generation. We formulate the task of multimodal event extraction as a sequence generation task, where our proposed model outputs a linearised representation of event mentions in a given sentence-image pair (x i , v i ). Each event then is represented in the form of y i = t i , e i , a i , r i 1 , a i , r i 2 , . . . , where t i ∈ W refers to a trigger word, e i ∈ E indicates the event type, and a i ∈ W and r i ∈ R represents an argument token and role, respectively. Thus, given an input pair (x, v), the goal of our proposed model is to generate sequence y of length T as follows:</p><formula xml:id="formula_0">p θ SEQ (y|x, v) = T t=1 p θ SEQ (y t |y &lt;t , x, v) (1) = T t=1 DEC θ DEC (y t |y &lt;t , ENC θ ENC (x, v)),</formula><p>where ENC θ ENC = (ENC L , ENC V ) and DEC θ DEC refers to an encoder and a decoder structure, respectively, and θ SEQ := {θ ENC , θ DEC } denotes the parameters of our sequence generation model.</p><p>Multimodal Architecture. The encoder ENC(•) consists of a language encoder (ENC L ) and a visual encoder (ENC V ) to compute hidden representations of the textual (H L ) and visual (H v ) inputs separately. Hence, the hidden representation (H) of the input (x, v) is formulated as:</p><formula xml:id="formula_1">H = ENC(x, v) = [ENC L (x); ENC V (v)] (2)</formula><p>The DEC(•) is a multi-layer Transformer-based decoder, where each layer is a Transformer block. At step t, the decoder generates the t-th token of sequence y i and the hidden state h t as follows:</p><formula xml:id="formula_2">y t , h t = DEC(y t-1 ; H y&lt;t , H),<label>(3)</label></formula><p>where H y&lt;t ∈ R (t-<ref type="foot" target="#foot_0">1</ref>)×d denotes the past hidden state used for self-attention during decoding. Given the complete data (x, v, y), the ENC-DEC architecture can be trained by minimising the loss:</p><formula xml:id="formula_3">L seq = E x,v,y [-log p(y|x, v; θ seq )]<label>(4)</label></formula><p>4 Training with Incomplete Data</p><p>During training, when the visual context of an input x i is unavailable, we leverage an image synthesizer that produces visual representation z i based on x i .</p><p>We then adapt the image synthesizer based on the complete and incomplete event extraction data.</p><p>Our proposed method is based on Denoising Diffusion Probabilistic Models <ref type="bibr" target="#b7">(Ho et al., 2020)</ref>. This generative model consists of a pre-trained autoencoder that maps images to a spatial latent code, a corresponding decoder that learns to map the latent representation back to the image, and a diffusion model that is conditioned on the textual input (x i ). Inspired by <ref type="bibr" target="#b22">Ruiz et al. (2022)</ref>, during training, we use the textual input of complete pairs of (x com , v com ) to condition the model to regenerate v com . Furthermore, we use the textual input of incomplete data points x inc to synthesise visual context z. We then resort to the following reconstruction-based loss function to train the image synthesiser on the synthesised image z,</p><formula xml:id="formula_4">L syn = E x com ,x inc ,v com ,z, , ,t,t<label>(5)</label></formula><formula xml:id="formula_5">[ω t || Vθsyn (α t v + σ t , x com ) -v com || 2 2 + λ syn ω t || Vθsyn (α t z + σ t , x inc ) -z|| 2 2 ],</formula><p>where Vθsyn is the image synthesizer function, ∼ N (0, I) is a noise term, and ω t , α t , σ t and ω t , α t , σ t are the terms that control the noise schedule and sample quality for complete and incomplete data, respectively, where t ( and t ) ∼ U([0, 1]). λ syn controls the trade-off between the images synthesizer's capability to regenerate the v com conditioned on x com and synthesising images conditioned on x inc . The only trainable parameters of the image synthesier are those of the textual encoder, and we keep the other parameters frozen.</p><p>We train our models in an end-to-end manner, where the overall optimisation objective is defined as a weighted sum of the sequence generation loss and the image synthesizer loss:</p><formula xml:id="formula_6">L = L seq + λL syn ,<label>(6)</label></formula><p>where the hyperparameter λ controls the trade-off between extracting textual and visual semantic information for sequence generation and synthesising the visual features.</p><p>To train our model, we apply online hard EM <ref type="bibr" target="#b17">(Neal and Hinton, 1999</ref>) by interleaving the following steps in an iterative manner:</p><p>• E-step: generate images z for data points x inc using the current parameters of our proposed image synthesizer θ syn . • M-step: the model parameters (θ seq and θ syn ) are the updated by minimising the loss Eq. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we report the performance of our model on the task of multimodal event extraction in comparison to the current state-of-the-art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Evaluation Metrics. Following previous work on multimodal event extraction <ref type="bibr" target="#b13">(Li et al., 2020)</ref>, we report the results of the macro-averaged F1 score (F1), precision (P) and recall (R). Since we have formulated this as a sequence generation task, we also report the BLEU score.</p><p>Baselines. We compare our model, Theia (VL), against Valhalla <ref type="bibr">(Li et al., 2022b)</ref>, RMMT <ref type="bibr" target="#b26">(Wu et al., 2021)</ref>, and Gated Fusion <ref type="bibr" target="#b26">(Wu et al., 2021)</ref> 1 . Furthermore, we report the performance of GPT3.5 <ref type="bibr" target="#b3">(Brown et al., 2020)</ref> and Flamingo <ref type="bibr" target="#b1">(Alayrac et al., 2022)</ref>, as two in-context learning approaches. In order to use visual information, we use BLIP-2 <ref type="bibr" target="#b11">(Li et al., 2023)</ref> to extract scene description from images and then we feed them to the context of the model (GPT3.5/SC). In addition, we report the performance of our model in two ablation settings, firstly, when the visual context is fully disregarded in training (Theia (L)), and secondly, instead of the image synthesizer, we use an image retrieval model to fill in the missing data with the most relevant images from the full set of images in the dataset (Theia (R)).</p><p>Dataset. M2E2 <ref type="bibr" target="#b13">(Li et al., 2020)</ref>  Compared to our full model, the poorer performance when the visual context is discarded (Theia (L)) suggests that using the visual context indeed improves the extraction of events. Moreover, the lower performance of Theia (R) indicates the positive effect of our image synthesizer in providing more context-related features.</p><p>To evaluate the effect of missing visual data, we reconfigure the training settings from 10 complete data points to 5, 50, 100, and the full set of data points, respectively. Figure <ref type="figure" target="#fig_1">2</ref> shows the results of this study. The x-axis shows the number of data points that, during training, include both textual and visual inputs, and the y-axis shows the F1 score. We see that on event trigger detection, our model outperforms all of the baselines in all settings. This observation suggests that the images generated by our image synthesizer model can capture more contextual information related to the textual input, hence resulting in improved performance. For argument extraction, our model performs the best in all settings except full, where Valhalla and Gated Fusion achieve better performance. The drop in performance of our model from 100 to full setting, suggests that the images associated with textual inputs (on full setting) carry less relevant information, and on the 100 setting when missing images are synthesised, the auxiliary images can provide better contextual information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Ethics Statement</head><p>This project aimed at customising existing pretrained image generation models (Stable Diffusion) towards the domain of news. While our image synthesizer might be biased toward specific attributes of the domain, it enables our model to extract better events from the text. Similar to existing generative models, the generated content might be used to mislead or manipulate. Therefore, our model inherits similar potential risks from this family of generative models. Similarly, in the second example, while stable diffusion fails to portray the smoke, our model represents this with fire/smoke, similar to the original image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Data Preparation</head><p>In order to develop our model, and run the experiments for all of the baselines as well, we convert the structured events into a sequence of tokens. To this end, a structured event represented in form of y i = t i , e i , a i , r i 1 , a i , r i 2 , . . . (Section 3), is converted to the following span of tokens: &lt;TRIGGER&gt; t i &lt;/TRIGGER&gt; &lt;EVENT&gt; e i &lt;/EVENT&gt; &lt;ARG&gt; a i &lt;/ARG&gt; &lt;ROLE&gt; r i &lt;/ROLE&gt; where the tokens between &lt;•&gt; are special tokens added to the vocabulary set W, and the embedding space of our encoder is then adjusted to the new length of vocab.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimental Details</head><p>To train our model, we use CLIP ViT-L/14 <ref type="bibr" target="#b19">(Radford et al., 2021)</ref> as a text and vision encoder. We also use the parameters of the pre-trained stable diffusion model <ref type="bibr" target="#b21">(Rombach et al., 2022)</ref> to initialise the image synthesizer. This model is licensed with a CreativeML OpenRAIL++ license and free to use. We use an embedding size of 768, and a dropout of 0.3 over the textual encoder. We use Transformer-Base decoder in the sequence generator. Our model include more than 1.4B parameters, which belongs to both image synthesise and sequence generator. Optimisation is handled by Adam (Kingma and Ba, 2015) with a square root learning rate schedule and warm-up steps. We set the λ to 0.01 and use beam search with a beam size of 5 during inference. We ran our model on a machine with one NVIDIA A100 gpu. For the baselines, we follow the implementation details explained in their papers, and each model is trained with the same data preparation as our model. The best performing checkpoint on the development set then was used for testing. Since we kept the random seed constant throughout the experiments, we report the single-run results of each model. For running the experiments of GPT3.5, we prepare a prompt including task definition, definition of each event type and argument type and one example per event type, in addition to scene description, and ask the model to generate event information according to the instruction. We use model engine gpt-3.5-turbo. BLIP-2<ref type="foot" target="#foot_3">2</ref> is used to extract description of the images, as a part of the prompt. The experiments related to Open-Flamingo<ref type="foot" target="#foot_4">3</ref> are conducted by using the open source implementation of Flamingo <ref type="bibr" target="#b1">(Alayrac et al., 2022)</ref>, where a prompt similar to prompt used in GPT3.5 experiments is used to instruct the model to generate information about events given a text and the corresponding image.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An overview of our proposed model. The textual encoder (ENC L ) takes the text as input and feeds the encoded version to the image synthesizer, as well as the decoder. The visual encoder (ENC V ) takes the synthesised image as input and feeds the encoded image into the multimodal transformer-based decoder, and the structured event information is generated as a sequence.</figDesc><graphic coords="2,70.87,70.87,453.55,141.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance of the models on M2E2, with different numbers of data point with both modalities during the training time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The images generated by our model for given textual inputs, compared to the original images from the news article, and the image generated by Stable Diffusion model (Rombach et al., 2022).</figDesc><graphic coords="7,70.87,70.87,453.53,224.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Experimental results of multimodal event extraction and ablation studies, on M2E2 dataset. The column "Method" refers to the learning method, where ICL and SL indicate in-context and supervised learning. argument roles. We split M2E2 into training, development, and test sets with a ratio of 60:20:20. We consider a missing data setting, where the training session has only 10 complete data points with textimage pairs, and the rest of the data lacks the visual modality. During inference, complete data points of text-image pairs are provided to the model.</figDesc><table><row><cell>Event</cell><cell>Argument</cell></row></table><note><p><p><p>is a multimodal news event extraction dataset that expands upon ACE</p><ref type="bibr" target="#b5">(Doddington et al., 2004)</ref></p>. It consists of 6,167 sentences, with 1,297 event mentions and 1,965</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The results ofLi et al. (2022a,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2020) are not reproducible, due to incomplete source code/data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>ConclusionIn this work, we address the task of multimodal event extraction, when the model faces missing</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p>https://github.com/salesforce/LAVIS/tree/main/projects/blip2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4"><p>https://github.com/mlfoundations/open_flamingo</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Examples of Images by Theia</head><p>In this section, we present two samples of images generated by our model, Theia. We randomly selected two data points from the test set of M2E2.</p><p>We then used the textual information to condition our image synthesizer into generating the visual representation of the text (Figure <ref type="figure">3</ref>). As it can be seen, in the first example, churches is properly captured in the image generated by our model, Theia.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The stages of event extraction</title>
		<author>
			<persName><forename type="first">David</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Annotating and Reasoning about Time and Events</title>
		<meeting>the Workshop on Annotating and Reasoning about Time and Events</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Flamingo: a visual language model for few-shot learning</title>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yana</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serkan</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sina</forename><surname>Samangooei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianne</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahand</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikolaj</forename><surname>Binkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Barreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>ArXiv, abs/2204.14198</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Anas</forename><surname>Awadalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irena</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Hanafy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanrong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Kalyani Marathe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samir</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenia</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Jitsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidt</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.7733589</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Openflamingo</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast rule-based approach for biomedical event extraction</title>
		<author>
			<persName><forename type="first">Quoc-Chinh</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Van Mulligen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the BioNLP shared task 2013 workshop</title>
		<meeting>the BioNLP shared task 2013 workshop</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="104" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The automatic content extraction (ace) program-tasks, data, and evaluation</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>George R Doddington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><forename type="middle">A</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><forename type="middle">M</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><forename type="middle">M</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lrec</title>
		<meeting><address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="837" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Event extraction by answering (almost) natural questions</title>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="671" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An overview of event extraction from text</title>
		<author>
			<persName><forename type="first">Frederik</forename><surname>Hogenboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flavius</forename><surname>Frasincar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uzay</forename><surname>Kaymak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franciska</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DeRiVE@ ISWC</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="48" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Degree: A data-efficient generation-based event extraction model</title>
		<author>
			<persName><forename type="first">I-Hung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan-Hao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Boschee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter</title>
		<meeting>the 2022 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1890" to="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization. san diego</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Annual International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12597</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Clip-event: Connecting text and images with event structures</title>
		<author>
			<persName><forename type="first">Manling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16420" to="16429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cross-media structured common space for multimedia event extraction</title>
		<author>
			<persName><forename type="first">Manling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Whitehead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2557" to="2568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Documentlevel event argument extraction by conditional generation</title>
		<author>
			<persName><forename type="first">Sha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="894" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">and Nuno Vasconcelos. 2022b. Valhalla: hallucination for machine translation</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Fu</forename><surname>Richard Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rogerio</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="page" from="5216" to="5226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fewshot domain-adaptative visually-fused event detection from text</title>
		<author>
			<persName><forename type="first">Farhad</forename><surname>Moghimifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatemeh</forename><surname>Shiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Van</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2023 26th International Conference on Information Fusion (FUSION)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A view of the EM algorithm that justifies incremental, sparse, and other variants</title>
		<author>
			<persName><forename type="first">Radford</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in Graphical Models</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Michael</surname></persName>
		</editor>
		<editor>
			<persName><surname>Jordan</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="355" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">One for all: Neural joint modeling of entities and events</title>
		<author>
			<persName><forename type="first">Minh</forename><surname>Trung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Huu Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6851" to="6858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Open domain event extraction from twitter</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1104" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation</title>
		<author>
			<persName><forename type="first">Nataniel</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yael</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kfir</forename><surname>Aberman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.12242</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A domainindependent rule-based framework for event extraction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gus</forename><surname>Valenzuela-Escárcega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Hahn-Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><surname>Hicks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP 2015 System Demonstrations</title>
		<meeting>ACL-IJCNLP 2015 System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="127" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5784" to="5789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Kcgee: Knowledge-based conditioning for generative event extraction</title>
		<author>
			<persName><forename type="first">Tongtong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatemeh</forename><surname>Shiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingqi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Good for misconceived reasons: An empirical revisiting on the need for visual context in multimodal machine translation</title>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Kao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6153" to="6166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving event extraction via multimodal integration</title>
		<author>
			<persName><forename type="first">Tongtao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Whitehead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="270" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Empower event detection with bi-directional neural language model</title>
		<author>
			<persName><forename type="first">Yunyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangluan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinglei</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="page" from="87" to="97" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
