<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-shot Named Entity Recognition with Supported and Dependent Label Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yasuhide</forename><surname>Miura</surname></persName>
							<email>yasuhide.a.miura@fujifilm.com</email>
						</author>
						<author>
							<persName><forename type="first">Takumi</forename><surname>Takahashi</surname></persName>
							<email>takumi.c.takahashi@fujifilm.com</email>
						</author>
						<title level="a" type="main">Few-shot Named Entity Recognition with Supported and Dependent Label Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B55C9CCA71732B438362FA75C0B4185A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T14:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We explore the problem of few-shot named entity recognition (NER) by introducing two ideas to improve label representations. Recently, the use of token representations with a distance metric has been shown to be effective in few-shot NER, and we take an approach to use label representations along with token representations. Firstly, we add support examples to a label name (e.g., "person; example: Federic Krupp, Gao, Honecker, Bush, Deverow") when obtaining a label representation. Secondly, we estimate a transition score among labels with a bilinear function among label representations. The proposed approach is evaluated on 4 open few-shot NER datasets and we found that the approach can improve the performance of one-stage few-shot NER.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The advance of large language models (e.g., BERT, GPT) has brought some of natural language understanding tasks to be tackled with few training samples. One such task that has especially gathered an attention from researchers is named entity recognition (NER) where a simple nearest neighbor classification using an NER model and a distance metric has shown to achieve a moderate performance in a few-shot setting <ref type="bibr" target="#b21">(Yang and Katiyar, 2020)</ref>. <ref type="bibr">Ma et al. (2022a)</ref> proposed a related but slightly different approach where they prepare an additional BERT to encode labels into representations.</p><p>We extend an idea to use label representations to improve few-shot NER. A simple approach to obtain label representations is to encode just label names <ref type="bibr">(Ma et al., 2022a)</ref>, and we add randomly sampled label examples to label names and encode the combined label names and examples to improve label representations. Figure <ref type="figure">1</ref> illustrates our approach to use label examples as the supports of label names. In this approach, an input text and all labels are encoded with a dual encoder architecture.</p><p>For each token, similarities against all labels are calculated to decide a label. The extension to add label examples may seem like a naive approach to improve label representations but this approach follows previous findings to obtain fine-grained label representations. Firstly, in the context of zero-shot NER, <ref type="bibr" target="#b0">Aly et al. (2021)</ref> explored the effectiveness of using label descriptions to encode labels. They have found that the use of label name is a strong baseline to represent a label and a label description can further improve the performance of zero-shot NER depending on its quality. One downside of using label descriptions is that fine-grained descriptions are not always available in an NER dataset. Secondly, an approach to use multiple examples to estimate a label is a well-known approach of Prototypical Networks <ref type="bibr" target="#b13">(Snell et al., 2017)</ref>. In a typical Prototypical Networks setting, a label prototype can be represented as an average of multiple examples.</p><p>We further extend the approach to use label representations in few-shot NER by estimating a label dependency between two labels. A straightforward approach to model label dependencies in NER is to add a Conditional Random Field (CRF, <ref type="bibr" target="#b8">Lafferty et al., 2001)</ref> layer after a token encoder <ref type="bibr" target="#b9">(Lample et al., 2016)</ref>. However, this CRF layer is known to be difficult to transfer since it directly learns a K × K transition matrix over K labels <ref type="bibr" target="#b21">(Yang and Katiyar, 2020;</ref><ref type="bibr" target="#b5">Hou et al., 2020)</ref>. We estimate a transition score between two labels with a trainable function which maps two label representations into a single scalar score. We show that this estimation works quite effectively in the dual encoder architecture.</p><p>In summary, the contributions of this paper are the followings:</p><p>1. We propose an approach to sample support examples to improve label representations for few-shot NER and confirm its effectiveness on 4 few-shot NER datasets.  <ref type="bibr">et al., 2022)</ref>. Like these approaches, our approach is in the paradigm of one-stage few-shot NER where named entities are recognized simply as the labels of input tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Two-stage Few-shot NER</head><p>Recently, the paradigm of two-stage few-shot NER <ref type="bibr">(Wang et al., 2022a)</ref> where entity spans are extracted in the first stage and their types are recognized in the second stage are investigated to extend the one-stage few-shot NER. In this paradigm, span or entity prototypes are defined <ref type="bibr">(Wang et al., 2022b;</ref><ref type="bibr" target="#b7">Ji et al., 2022;</ref><ref type="bibr">Ma et al., 2022b;</ref><ref type="bibr">Wang et al., 2022a)</ref> to achieve stronger performances with the complexity of an additional stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Label Representation in NER</head><p>The use of label description has been also explored in a low-resource NER. <ref type="bibr" target="#b0">Aly et al. (2021)</ref> explored the effect of label description in a zero-shot NER and <ref type="bibr" target="#b18">Wang et al. (2021)</ref> has utilized label descriptions along with entity representations in a few-shot and a zero-shot NER. <ref type="bibr">Ma et al. (2022a)</ref> has shown that simply using label names is quite effective in few-shot NER. Our approach extends these ideas to use support samples to improve label representation in few-shot NER.</p><p>3 Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dual Encoder Model</head><p>We followed the approach of dual encoders that was taken by <ref type="bibr" target="#b0">Aly et al. (2021)</ref> and <ref type="bibr">Ma et al. (2022a)</ref> as the base architecture of our model. As shown in Figure <ref type="figure">1</ref>, we prepare an encoder for an input tokens and an encoder for labels. Given input tokens u I , the tokens are encoded with a language model v = LM token (u I ). The tokens of given labels u L are similarly encoded with another language model m = LM label (u L ), and the representations of CLS<ref type="foot" target="#foot_0">1</ref> are pooled as label representations l. For each token representation v n , similarities against all label representations are calculated with a similarity function as o n = sim(v n , l). These token-label similarities are used to calculate a loss against true labels. As done in previous studies, we first pre-finetune this model on a large scale NER end if 13: end while 14: return S dataset (e.g., OntoNotes 5.0) and then fine-tune it on a few-shot NER dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Support Example Sampling</head><p>The dual encoder model encodes label tokens to obtain label representations. Our idea improves label representations by extending label tokens with support examples. Algorithm 1 shows processes to sample support examples S for input text x and label y. In the case of PER label in Figure <ref type="figure">1</ref>, n = 5 examples of Federic Krupp, Gao, Honecker, Bush and Deverow are sampled from entire training data X<ref type="foot" target="#foot_1">2</ref> . These examples are then combined with the label name person with the fixed text snippet of "; example:"<ref type="foot" target="#foot_2">3</ref> . One exceptional label that needs to be considered in this sampling is O label. Since O is not a label for named entity and does not have an entity boundary, we decided to samples a nonentity word from text x y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Estimation of Transition Score</head><p>Lample et al. ( <ref type="formula">2016</ref>) have shown that a CRF layer can be added to a token encoder to improve NER. However, the few-shot transfer of this CRF layer is known to be difficult since the prediction score is defined as s(x, y) = i A y i ,y i+1 + i P i,y i where i is a token index, A is a transition matrix and P is an emission matrix. A typical approach to realize A is to prepare a trainable K × K matrix when there are K labels. We estimate a transition score among two labels A y i ,y i+1 simply with a bilinear function as</p><formula xml:id="formula_0">A y i ,y i+1 = l T i W l i+1 + b (1)</formula><p>where W is a trainable weight matrix of size D×D, b is a bias and D is the embedding size of the label encoder. We call a score estimated with this approach Bilinear-transition CRF (BCRF) score.</p><p>The estimation of a transition score has been investigated in a more resource rich setting in <ref type="bibr" target="#b6">Hu et al. (2020)</ref>. Our BCRF score takes a simple estimation approach since we focus on a resource poor few-shot setting.</p><p>4 Experiment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Baselines</head><p>We evaluate the effectiveness of our approach using 4 datasets: Few-NERD <ref type="bibr" target="#b3">(Ding et al., 2021)</ref>, <ref type="bibr">CoNLL-2003 (Tjong Kim Sang and</ref><ref type="bibr" target="#b15">De Meulder, 2003)</ref>, <ref type="bibr">WNUT-2017</ref><ref type="bibr" target="#b2">(Derczynski et al., 2017)</ref> and i2b2-2014 <ref type="bibr" target="#b14">(Stubbs and Özlem Uzuner, 2015)</ref>. Few-NERD is a large-scale dataset specialized for the evaluation of few-shot NER. CoNLL-2003, WNUT-2017 and i2b2-2014 are datasets that were used in a few-shot domain transfer setting in previous studies <ref type="bibr" target="#b21">(Yang and Katiyar, 2020;</ref><ref type="bibr">Ma et al., 2022a;</ref><ref type="bibr" target="#b1">Das et al., 2022;</ref><ref type="bibr" target="#b7">Ji et al., 2022)</ref>. We compare our approach against various stateof-the-art one-stage baselines ( §2.1) and two-stage baselines ( §2.2). For the one-stage baselines, we compare against ProtoBERT <ref type="bibr" target="#b13">(Snell et al., 2017)</ref>, StructShot <ref type="bibr" target="#b21">(Yang and Katiyar, 2020)</ref>, LabelSem <ref type="bibr">(Ma et al., 2022a)</ref> and <ref type="bibr">CONTaiNER (Das et al., 2022)</ref>. For the two-stage baselines, we compare against ESD <ref type="bibr">(Wang et al., 2022b)</ref>, MAML-ProtoNet <ref type="bibr">(Ma et al., 2022b)</ref>, EPNet <ref type="bibr" target="#b7">(Ji et al., 2022)</ref> and SpanProto <ref type="bibr">(Wang et al., 2022a)</ref>. For the scores of ProtoBERT, StructShot and CONTaiNER, we refer to the values reported in <ref type="bibr" target="#b1">Das et al. (2022)</ref>. For the scores of LabelSem, ESD, MAML-ProtoNet, EPNet and SpanProto, we refer to the values reported in the original papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Configuration</head><p>We first pre-finetuned the dual encoder model ( §3.1) on a large-scale NER dataset. The training section is used for Few-NERD and OntoNotes 5.0 <ref type="bibr" target="#b19">(Weischedel et al., 2013)</ref>   <ref type="bibr" target="#b21">(Yang and Katiyar, 2020)</ref>. The values with parenthesis are the scores with the downsampling algorithm <ref type="bibr" target="#b5">(Hou et al., 2020)</ref>. used as the tagging scheme of NER. The Viterbi algorithm is used to decide the best label sequence as in the decoding process of CRF. The further detail of the training configuration, label configuration and dataset statistics are shown in §A.1, §A.2 and §A.3, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The Effects of Support Examples and BCRF</head><p>We examined the effects of two ideas with an ablation study on Few-NERD. The -SupEx and -BCRF scores in Table <ref type="table" target="#tab_2">1</ref> shows the result of ablation study. BCRF and support examples have shown effective on all 8 settings. We further confirmed the performance of BCRF without the label encoder by using randomly initialized label embeddings as in <ref type="bibr" target="#b6">Hu et al. (2020)</ref> (-SupEx, +BCRF[R]). The low performance of this setting indicates the strength of BCRF combined with the label encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The Effects of n Support Examples</head><p>The labels encoder encodes n support examples to obtain label representations. In the experiment ( §4.3), we chose n = 5 so that our approach can consider enough examples in the 5~10 shot settings of Few-NERD. We have additionally tried n = 1, 3 on Few-NERD and found the result to be quite stable regardless of the value of n. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed two ideas to improve label representations that can be effective for few-shot NER. These ideas have shown effectiveness to achieve strong performances compared against previous one-stage approaches and comparable performances to some of two-stage approaches. As future work, we would like to explore whether these ideas can be applied to span representations which have shown superiority compared to simpler token representations that we have explored in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Our approach has shown strong performances on 4 widely used few-shot NER datasets. Additional datasets and transfer settings have been tested in previous studies <ref type="bibr" target="#b4">(Fritzler et al., 2019;</ref><ref type="bibr" target="#b21">Yang and Katiyar, 2020;</ref><ref type="bibr" target="#b18">Wang et al., 2021;</ref><ref type="bibr">Ma et al., 2022a;</ref><ref type="bibr" target="#b1">Das et al., 2022;</ref><ref type="bibr">Ma et al., 2022b;</ref><ref type="bibr" target="#b7">Ji et al., 2022)</ref> and our approach can be suboptimal on them. The result of few-shot domain transfer settings in CoNLL-2003, WNUT-2017 and i2b2-2014 depends on randomly sampled few-shot samples. Since these random samples differ among our approach and previous studies, the comparison is not a fair comparison in an exact manner. This variance in random samples is alleviated in Few-NERD since the episode evaluation use pre-sampled 5000 episodes. The evaluation of our approach requires certain amount of computational resources to run, especially in Few-NERD. Even though a single episode evaluation can be done quite quickly (e.g., 3 minutes), the full evaluation on Few-NERD will take 3 × 5000 × 8 minutes ≈ 2000 hours on single gpu.</p><p>broadcast news (OntoNotes 5.0), broadcast conversation (OntoNotes 5.0), telephone conversation (OntoNotes 5.0), web data (OntoNotes 5.0), social media (WNUT-2017) and clinical narratives (i2b2-2014). Protected health information in the clinical narratives are de-identified and we have made the agreement with the data provider on a research and development use of them.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Label Configuration</head><p>The model uses label names along with support examples to obtain label representations ( §3.2). We used the label names defined in <ref type="bibr">Ma et al. (2022a)</ref> for CoNLL-2003, WNUT-2017 and i2b2-2014. For example, "person" is used as the label name of "PER" in CoNLL-2003. We combined the coarse type and the fine type of a named entity with hyphen in Few-NERD which is available in Table <ref type="table">8</ref> of <ref type="bibr" target="#b3">Ding et al. (2021)</ref>. For example, "Location-GPE" is used for the named entity with the coarse type of "Location" and the fine type of "GPE". We additionally prepared "start of sentence" and "end of sentence" label names for BCRF which are used in the first token of a sentence and the last token of a sentence, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Dataset Statistics</head><p>Table <ref type="table" target="#tab_7">3</ref> shows the number of sentences included in OntoNotes 5.0, Few-NERD, CoNLL-2003, WNUT-2017 and i2b2-3014. For OntoNotes 5.0, we used the splits of CoNLL-2012 9 following the setting of <ref type="bibr" target="#b21">Yang and Katiyar (2020)</ref>. The language of the datasets is English for all datasets. All datasets are designed to evaluate NER, and Few-NERD is specifically designed for few-shot settings. Note that the actual training splits of the experiment ( §4) are samples of the training split in Table <ref type="table" target="#tab_7">3</ref>.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>cased). Transformers library 7 and PyTorch 8 are used to implement the proposed model. The number of support examples is set to n = 5. NVIDIA Tesla V100 with 32GB memory is used to train and evaluate the proposed model. The training time of the proposed model is short: 1-8 minutes for a 100 epochs fine-tuning on a target dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>7</head><label></label><figDesc>https://huggingface.co/transformers 8 https://pytorch.org/ 9 http://conll.cemantix.org/2012/data.html</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The F 1 scores on Few-NERD with varying number of support examples.</figDesc><graphic coords="7,307.93,71.45,214.70,170.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure1: The overview of our approach to encode label representations by adding randomly sampled support examples. Encoder is a large language model such as BERT, sim is a similarity function among representations, v n is the token representation of n-th token, l is label representations and BCRF is Bilinear-trainsition CRF ( §3.3).</figDesc><table><row><cell>Labels</cell><cell cols="2">Label names and support examples</cell><cell></cell></row><row><cell>PER</cell><cell cols="2">person; example: Federic Krupp, Gao, Honecker, Bush, Deverow</cell><cell></cell></row><row><cell>ORG</cell><cell cols="3">organization; example: Comdek, Xinhua News Agency,theWarsaw Sinfonia, …</cell></row><row><cell>…</cell><cell></cell><cell>…</cell><cell></cell></row><row><cell>O</cell><cell cols="2">other; example: that, in, of, previous, had</cell><cell></cell></row><row><cell>Input tokens</cell><cell></cell><cell></cell><cell>Predictions</cell></row><row><cell>[CLS] The</cell><cell cols="2">Encoder (label) + CLS Pooling</cell><cell>O O</cell></row><row><cell>Denver -based concern , which …</cell><cell>Encoder (token)</cell><cell>Token-label similarity BCRF sim 𝒗 1 , 𝒍 sim 𝒗 2 , 𝒍 … sim(𝒗 𝑁 , 𝒍)</cell><cell>I-GPE O O O O O …</cell></row><row><cell>[SEP]</cell><cell></cell><cell></cell><cell>O</cell></row><row><cell cols="2">2. We define a trainable function between two</cell><cell></cell><cell></cell></row><row><cell cols="2">label representation to estimate a transition</cell><cell></cell><cell></cell></row><row><cell cols="2">score of the two labels and show its transfer</cell><cell></cell><cell></cell></row><row><cell cols="2">capability in a few-shot setting.</cell><cell></cell><cell></cell></row><row><cell>2 Related Work</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">2.1 One-stage Few-shot NER</cell><cell></cell><cell></cell></row><row><cell cols="2">The use of distance metric has shown to be ef-</cell><cell></cell><cell></cell></row><row><cell cols="2">fective in a few-shot setting, where Wiseman and</cell><cell></cell><cell></cell></row><row><cell cols="2">Stratos (2019) and Yang and Katiyar (2020) found</cell><cell></cell><cell></cell></row><row><cell cols="2">that a nearest neighbor search among token repre-</cell><cell></cell><cell></cell></row><row><cell cols="2">sentations can be a promising approach for few-</cell><cell></cell><cell></cell></row><row><cell cols="2">shot NER. Fritzler et al. (2019) and Hou et al.</cell><cell></cell><cell></cell></row><row><cell cols="2">(2020) have explored Prototypical Networks to</cell><cell></cell><cell></cell></row><row><cell cols="2">model token-level entity prototypes in few-shot</cell><cell></cell><cell></cell></row><row><cell cols="2">NER. These ideas are further investigated to train</cell><cell></cell><cell></cell></row><row><cell cols="2">a model with a contrastive learning objective (Das</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 Support example sampling</figDesc><table><row><cell cols="2">Require: input_text x, label y, training_texts X, # of exam-</cell></row><row><cell></cell><cell>ple n</cell></row><row><cell cols="2">1: S ← ϕ</cell></row><row><cell cols="2">2: while |S| &lt; n do</cell></row><row><cell>3:</cell><cell>// Sample a text including y from X</cell></row><row><cell>4:</cell><cell>xy ← sample_text(X, y)</cell></row><row><cell>5:</cell><cell>if xy ̸ = x then</cell></row><row><cell>6:</cell><cell>if y ̸ = O then</cell></row><row><cell>7:</cell><cell>sy ← sample_entity(xy)</cell></row><row><cell>8:</cell><cell>else</cell></row><row><cell>9:</cell><cell>sy ← sample_word(xy)</cell></row><row><cell>10:</cell><cell>end if</cell></row><row><cell>11:</cell><cell>S ← S ⊎ {sy}</cell></row><row><cell>12:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>is used forCoNLL-2003,  WNUT-2017 and i2b2-2014. BERT base (cased)  is used as language models and dot product is used as the similarity function of the model. IO scheme is The episode evaluation F 1 scores on Few-NERD over 5000 episodes. The shaded models are the proposed model with alternative configurations ( §5): -SupEx is without support examples, -BCRF is trained on cross-entropy loss, +BCRF[R] is trained on BCRF with randomly initialized label embeddings. The bold values are the best scores and the underlined values are the second-best scores for each N -way K-shot setting.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">INTRA</cell><cell></cell><cell></cell><cell></cell><cell cols="2">INTER</cell></row><row><cell></cell><cell></cell><cell cols="2">5-way</cell><cell cols="2">10-way</cell><cell cols="2">5-way</cell><cell></cell><cell cols="2">10-way</cell></row><row><cell cols="2">Stage Model</cell><cell cols="2">1~2-S 5~10-S</cell><cell cols="2">1~2-S 5~10-S</cell><cell cols="3">1~2-S 5~10-S</cell><cell cols="2">1~2-S 5~10-S</cell></row><row><cell></cell><cell>ProtoBERT</cell><cell>23.45</cell><cell>41.93</cell><cell>19.76</cell><cell>34.61</cell><cell>44.44</cell><cell cols="2">58.80</cell><cell>39.09</cell><cell>53.97</cell></row><row><cell>one</cell><cell>StructShot CONTaiNER</cell><cell>35.92 40.40</cell><cell>38.83 53.71</cell><cell>25.38 33.82</cell><cell>26.39 47.51</cell><cell>57.33 56.10</cell><cell cols="2">57.16 61.90</cell><cell>49.46 48.36</cell><cell>49.39 57.13</cell></row><row><cell></cell><cell>DualEnc++ proposed</cell><cell>50.81</cell><cell cols="2">64.20 46.89</cell><cell>58.64</cell><cell>63.98</cell><cell cols="2">72.04</cell><cell>62.31</cell><cell>69.95</cell></row><row><cell></cell><cell>-BCRF</cell><cell>49.51</cell><cell>61.09</cell><cell>43.90</cell><cell>54.86</cell><cell>61.56</cell><cell cols="2">69.88</cell><cell>58.90</cell><cell>67.10</cell></row><row><cell></cell><cell>-SupEx</cell><cell>49.71</cell><cell>63.95</cell><cell>46.26</cell><cell>58.41</cell><cell>62.76</cell><cell cols="2">72.06</cell><cell>61.62</cell><cell>69.95</cell></row><row><cell></cell><cell>-SupEx, +BCRF[R]</cell><cell>17.14</cell><cell>39.55</cell><cell>12.51</cell><cell>29.39</cell><cell>23.07</cell><cell cols="2">51.83</cell><cell>17.06</cell><cell>42.27</cell></row><row><cell></cell><cell>ESD</cell><cell>36.08</cell><cell>52.14</cell><cell>30.00</cell><cell>42.15</cell><cell>59.29</cell><cell cols="2">69.06</cell><cell>52.16</cell><cell>64.00</cell></row><row><cell>two</cell><cell>MAML-ProtoNet EPNET</cell><cell>52.04 43.36</cell><cell>63.23 58.85</cell><cell>43.50 36.41</cell><cell>56.84 46.40</cell><cell>68.77 62.49</cell><cell cols="2">71.62 65.24</cell><cell>63.26 54.39</cell><cell>68.32 62.37</cell></row><row><cell></cell><cell>SpanProto</cell><cell>54.49</cell><cell>73.10</cell><cell>45.39</cell><cell cols="2">64.63 73.36</cell><cell cols="3">82.68 66.26</cell><cell>78.69</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">1-shot</cell><cell></cell><cell></cell><cell></cell><cell cols="2">5-shot</cell></row><row><cell cols="2">Stage Model</cell><cell>CoNLL</cell><cell cols="2">WNUT</cell><cell>i2b2</cell><cell cols="2">CoNLL</cell><cell></cell><cell>WNUT</cell><cell>i2b2</cell></row><row><cell></cell><cell>ProtoBERT</cell><cell>49.9±8.6</cell><cell cols="2">17.4±4.9</cell><cell>13.4±3.0</cell><cell cols="2">61.3±9.1</cell><cell cols="2">22.8±4.5</cell><cell>17.9±1.8</cell></row><row><cell></cell><cell>StructShot</cell><cell>62.4±10.5</cell><cell cols="2">24.2±8.0</cell><cell>21.4±3.5</cell><cell cols="2">74.8±2.4</cell><cell cols="2">30.4±6.5</cell><cell>30.3±2.1</cell></row><row><cell>one</cell><cell>CONTaiNER</cell><cell>61.2±10.7</cell><cell cols="2">27.5±1.9</cell><cell>21.5±1.7</cell><cell cols="2">75.8±2.7</cell><cell cols="2">32.5±3.8</cell><cell>36.7±2.1</cell></row><row><cell></cell><cell>LabelSem</cell><cell cols="9">(68.4±6.7) (38.3±1.7) (61.9±4.3) (76.6±2.1) (40.8±2.1) (76.8±2.0)</cell></row><row><cell></cell><cell>DualEnc++ proposed</cell><cell>71.0±3.8</cell><cell cols="2">36.1±4.9</cell><cell>44.4±5.1</cell><cell cols="2">74.8±5.1</cell><cell cols="2">40.3±2.3</cell><cell>46.5±4.9</cell></row><row><cell>two</cell><cell>EPNet</cell><cell>64.8±10.4</cell><cell cols="2">32.3±4.8</cell><cell>27.5±4.6</cell><cell cols="2">78.8±2.7</cell><cell cols="2">38.4±5.2</cell><cell>44.9±2.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The F 1 scores onCoNLL-2003, WNUT-2017 and i2b2-2014. The scores of the proposed models are average with standard deviation on 10 different K-shot samples. The bold values are the best scores and the underlined values are the second-best scores for each K-shot setting with the greedy sampling algorithm</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>shows the result on Few-NERD over IN-</cell></row><row><cell>TRA and INTER configurations. In Few-NERD,</cell></row><row><cell>coarse-grained entity types are shared (INTER) or</cell></row><row><cell>not shared (INTRA) among the training data and</cell></row><row><cell>the test data. DualEnc++ has shown best scores</cell></row><row><cell>on all 8 settings against one-stage previous mod-</cell></row><row><cell>els. For the comparison against two-stages models,</cell></row><row><cell>DualEnc++ has shown best or second-best scores</cell></row><row><cell>on 5 settings. Table 2 shows the results on CoNLL-</cell></row><row><cell>2003, WNUT-2017 and i2b2-2014.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>The standard deviation of F 1 score was largest on INTRA 10-way 1~2 shot with the value of 47.27 ± 0.33. The more detailed effect of the number of support examples can be confirmed in §A.4.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>The number of sentences included in the datasets of the experiment ( §4).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>A special token of a language model that is prepended to an input text.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>This sampling is done for every training batch. This random process is important for the dual encoder model to avoid overfitting to certain examples.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>In 1-shot NER, this algorithm can fail to sample examples. In such case, we used the text snippet of "; example: none".</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://catalog.ldc.upenn.edu/LDC2013T19</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://www.clips.uantwerpen.be/conll2003/ ner/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://n2c2.dbmi.hms.harvard.edu/data-sets</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>The language resources used in our paper are all publicly available from the corresponding websites. The licenses of the resources are: CC BY-SA 4.0 for Few-NERD, LDC User Agreement for Non-Members </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Training Configuration</head><p>The model is pre-finetuned for 3 epochs and is further fine-tuned on a few-shot dataset for 100 epochs. We optimized the model with support examples ( §3.2) and BCRF scores ( §3.3) using AdamW <ref type="bibr" target="#b10">(Loshchilov and Hutter, 2019)</ref>. The learning rate of the optimization is set to 1e -5 following <ref type="bibr">Ma et al. (2022a)</ref> with linear warmup throughout pre-finetuning and fine-tuning for BERT-base</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Leveraging type descriptions for zero-shot named entity recognition and classification</title>
		<author>
			<persName><forename type="first">Rami</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.120</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1516" to="1528" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CONTaiNER: Few-shot named entity recognition via contrastive learning</title>
		<author>
			<persName><forename type="first">Sarkar</forename><surname>Snigdha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarathi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Passonneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.439</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6338" to="6353" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Results of the WNUT2017 shared task on novel and emerging entity recognition</title>
		<author>
			<persName><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marieke</forename><surname>Van Erp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nut</forename><surname>Limsopatham</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4418</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Noisy User-generated Text</title>
		<meeting>the 3rd Workshop on Noisy User-generated Text<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="140" to="147" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Few-NERD: A few-shot named entity recognition dataset</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.248</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3198" to="3213" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Few-shot classification in named entity recognition task</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Fritzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksim</forename><surname>Kretov</surname></persName>
		</author>
		<idno type="DOI">10.1145/3297280.3297378</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing, SAC &apos;19</title>
		<meeting>the 34th ACM/SIGAPP Symposium on Applied Computing, SAC &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="993" to="1000" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Few-shot slot tagging with collapsed dependency transfer and label-enhanced task-adaptive projection network</title>
		<author>
			<persName><forename type="first">Yutai</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongkui</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.128</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1381" to="1393" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An investigation of potential function designs for neural CRF</title>
		<author>
			<persName><forename type="first">Zechuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.236</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2600" to="2609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Few-shot named entity recognition with entity-level prototypical network enhanced by dispersedly distributed prototypes</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shasha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoduo</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huijun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics</title>
		<meeting>the 29th International Conference on Computational Linguistics<address><addrLine>Gyeongju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1842" to="1854" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning, ICML &apos;01</title>
		<meeting>the Eighteenth International Conference on Machine Learning, ICML &apos;01<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1030</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">2022a. Label semantics for few shot named entity recognition</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srikanth</forename><surname>Doss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishita</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunil</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-acl.155</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="1956" to="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Decomposed metalearning for few-shot named entity recognition</title>
		<author>
			<persName><forename type="first">Tingting</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianhui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-acl.124</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<imprint>
			<publisher>Dublin, Ireland. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1584" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Annotating longitudinal clinical narratives for de-identification: The 2014 i2b2/UTHealth corpus</title>
		<author>
			<persName><forename type="first">Amber</forename><surname>Stubbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Özlem</forename><surname>Uzuner</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbi.2015.07.020</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="20" to="S29" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meulder</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">2022a. SpanProto: A two-stage span-based prototypical network for few-shot named entity recognition</title>
		<author>
			<persName><forename type="first">Jianing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="3466" to="3476" />
		</imprint>
	</monogr>
	<note>United Arab Emirates. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">2022b. An enhanced span-based decomposition method for few-shot sequence labeling</title>
		<author>
			<persName><forename type="first">Peiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunbo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.369</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>United States. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="5012" to="5024" />
		</imprint>
	</monogr>
	<note>Seattle</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning from language description: Low-shot named entity recognition via decomposed framework</title>
		<author>
			<persName><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoda</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.139</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1618" to="1630" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hovy</forename><surname>Eduard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Franchini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>El-Bachouti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Belvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Houston</surname></persName>
		</author>
		<idno type="DOI">10.35111/xmhb-2b84</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>OntoNotes Release 5.0.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Label-agnostic sequence labeling by copying nearest neighbors</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1533</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5363" to="5369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Simple and effective few-shot named entity recognition with structured nearest neighbor learning</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.516</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6365" to="6375" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
