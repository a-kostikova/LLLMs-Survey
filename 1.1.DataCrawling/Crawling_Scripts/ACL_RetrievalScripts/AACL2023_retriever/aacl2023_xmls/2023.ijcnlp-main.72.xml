<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VACASPATI: A Diverse Corpus of Bangla Literature</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pramit</forename><surname>Bhattacharyya</surname></persName>
							<email>pramitb@cse.iitk.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Kanpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joydeep</forename><surname>Mondal</surname></persName>
							<email>joydeep@cse.iitk.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Kanpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Subhadip</forename><surname>Maji</surname></persName>
							<email>subhadip@cse.iitk.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Kanpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arnab</forename><surname>Bhattacharya</surname></persName>
							<email>arnabb@cse.iitk.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Kanpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VACASPATI: A Diverse Corpus of Bangla Literature</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5C6BF756CF27E0745C4614D452EA9025</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bangla (or Bengali) is the fifth most spoken language globally; yet, the state-of-the-art NLP in Bangla is lagging for even simple tasks such as lemmatization, POS tagging, etc. This is partly due to lack of a varied quality corpus. To alleviate this need, we build V ĀCASPATI, a diverse corpus of Bangla literature. The literary works are collected from various websites; only those works that are publicly available without copyright violations or restrictions are collected. We believe that published literature captures the features of a language much better than newspapers, blogs or social media posts which tend to follow only a certain literary pattern and, therefore, miss out on language variety and vocabulary. Our corpus V ĀCASPATI is varied from multiple aspects, including type of composition, topic, author, time, space, etc. It contains more than 11 million sentences and 115 million words. We have also built a word embedding model, V ĀC-FT, using Fast-Text from V ĀCASPATI as well as trained an Electra model, V ĀC-BERT, using the corpus. V ĀC-BERT has far fewer parameters and requires only a fraction of resources compared to other state-of-the-art transformer models and yet performs either better or similar on various downstream tasks. Similarly, V ĀC-FT outperforms other FastText-based models on multiple downstream tasks. We also demonstrate the efficacy of V ĀCASPATI as a corpus by showing that similar models built from other corpora are not as effective. The models are available at https://bangla.iitk.ac.in/ projects/vacaspati.html.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated computational processing of natural language tasks has witnessed tremendous improvements in recent years, mostly due to availability of large text corpora and novel deep learning models that can process those corpora. In case of English and some western European languages where such corpora are available, e.g., the billion word corpus <ref type="bibr" target="#b44">(Pomikálek et al., 2012)</ref>, state-of-the-art models in standard NLP tasks comprising of deep learning architectures outperform traditional rule-based models. Most other languages, however, do not enjoy such improved performances in NLP tasks due to lack of large quality corpora.</p><p>Hence, in this paper, we build and release a large corpus, V ĀCASPATI, for Bangla (Bengali, বাংলা, bAMlA)<ref type="foot" target="#foot_0">1</ref> , which is the fifth most spoken language globally. It is the state language of several states in India including West Bengal and Tripura, besides being one of the official languages of India. Further, it is the main language for Bangladesh. Recently, there are proposals to adopt Bangla as one of the official languages in the UN (https://en.wikipedia.org/wiki/Official_ languages_of_the_United_Nations).</p><p>There have been several attempts in the past to build a Bangla text corpus. Some of the notable ones are IndicCorp <ref type="bibr" target="#b27">(Kakwani et al., 2020)</ref>, <ref type="bibr" target="#b12">(Doddapaneni et al., 2023)</ref> that compiled a dataset for 11 Indian languages with news articles including Bangla, and BanglaBERT <ref type="bibr" target="#b3">(Bhattacharjee et al., 2022)</ref> that scraped data from social media posts and ebooks from 110 sites. Sec. 2 details more related works.</p><p>One of the major concerns for all these corpora, however, is the quality and variety of the language. The sources mostly comprise of newspapers, blogs and social media posts. Newspapers are known to follow a certain style of language which is typically urban and devoid of common words such as "mahIpati" (king), "hotRRigaNa" (priest), etc. In addition, words such as "sarakAra" (government) and "pulisha" (police) are unnaturally more frequent. Blogs and social media posts, on the other hand, are often full of grammatical mistakes, typos and non-native words and phrases <ref type="bibr" target="#b32">(Kundu et al., 2013)</ref>. Consequently, any language model built from such corpora are bound to suffer from quality problems.</p><p>Thus, in this paper, we focus on only classical Bangla literature as our text source. We do not include any news article, blog or social media post. To the best of our knowledge, this is the first attempt to build a Bangla corpus using only literary works. BookCorpus <ref type="bibr" target="#b54">(Zhu et al., 2015)</ref> with 11,038 books is a similar dataset in English.</p><p>We name our corpus V ĀCASPATI which means "master of speech (or language)". V ĀCASPATI is diverse from multiple aspects, including type of composition, topic, author, time, space, etc. It consists of more than 11 million sentences and 115 million words, and is 2.1 GB in size.</p><p>To examine if NLP tasks are getting facilitated by the corpus, we construct word embeddings using FastText, and build a transformer model. We test the effectiveness of these through several downstream tasks including word similarity task, poem and sentiment classification, and spelling error detection. Notably, our models use only a fraction of the resources (running time, memory, number of parameters) as compared to the competing state-of-the-art models and yet either outperform them or give comparable results on these tasks.</p><p>In sum, our contributions in this paper are: 1. We create a large quality corpus, V ĀCASPATI, using only Bangla literature (Sec. 3). 2. We construct word embeddings and build language models using V ĀCASPATI (Sec. 4). Our models require 3-15 times less running time and 1.5-3 times less space as compared to competing ones. 3. Models built using V ĀCASPATI either outperform or are similar to competing models on several downstream tasks (Sec. 5).</p><p>The models and test datasets are available at https://bangla.iitk.ac.in/projects/vacaspati.html.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Corpora: Most Bangla datasets to date have been generated from newspaper articles. The EMILLE-CIIL <ref type="bibr" target="#b35">(McEnery et al., 2000)</ref> monolingual written corpus consists of 1,980,000 words from newspapers, whereas the spoken corpus consists of only 442,000 words. Wikipedia dump of Bangla is of size 326 MB (as on July 1, 2023). The Leipzig corpus <ref type="bibr" target="#b19">(Goldhahn et al., 2012)</ref> on Bangla, curated by crawling newspapers, consists of 1,200,255 sentences and 16,632,554 tokens.</p><p>The SU-Para corpus <ref type="bibr" target="#b38">(Mumin et al., 2012)</ref> is an English-Bangla sentence-aligned parallel corpus consisting of more than 200,000 words. OPUS <ref type="bibr" target="#b49">(Tiedemann and Nygaard, 2004)</ref>, a multilingual corpus of translated open-source documents, curated a sentence-aligned parallel corpus for Bangla with 4.7 million tokens and 0.51 million sentences. The BNLP <ref type="bibr" target="#b47">(Sarker, 2021)</ref> dataset for embedding was curated by collecting 127,867 news articles and Wikipedia dumps, and was trained on 5.83 million sentences and 93.43 million tokens. <ref type="bibr">Hasan et al. (2020b)</ref> built a sentence-aligned Bangla-English parallel corpus with 2.75M sentences focusing mainly on machine translation work. The Indic-Corp corpus <ref type="bibr" target="#b27">(Kakwani et al., 2020)</ref> for Bangla consists of 39.9 million sentences and 836 million tokens generated from 3.89 million news articles. IndicCorp v2 <ref type="bibr" target="#b12">(Doddapaneni et al., 2023)</ref> generated from newspaper articles, e-books and other webpages contains 936 million tokens in Bangla. The OSCAR project <ref type="bibr" target="#b39">(Ortiz Suárez et al., 2019)</ref> built by crawling websites using CommonCrawl (https://commoncrawl.org) contains 632 million words for Bangla. BanglaBERT <ref type="bibr" target="#b3">(Bhattacharjee et al., 2022</ref>) generated a 27.5 GB Bangla dataset with 2.5 billion tokens by crawling 110 websites. BanglaBERT contains ebooks but also contains newspaper articles, blogs, and social media texts in addition to that. BanglaBERT only provides a pre-trained BERT model but does not provide any word embedding for their corpus. It is challenging to use for word-level tasks such as word similarity and word analogy as the words may not be present in vocabulary in their proper form.</p><p>Word Embeddings: Word embeddings have been trained on Bangla, but either on limited data or only over news articles and social media blogs. The Polyglot <ref type="bibr" target="#b2">(Al-Rfou' et al., 2013)</ref> project contains articles from Wikipedia and has only 55,000 words. <ref type="bibr" target="#b0">Ahmad and Amin (2016)</ref> released a word embedding of 210,000 words created from newspaper articles. FastText <ref type="bibr" target="#b4">(Bojanowski et al., 2017)</ref> provides Bangla word embeddings at https://nlp.johnsnowlabs.com/2021/ 02/10/bengali_cc_300d_bn.html, trained either only on Wikipedia, or Wikipedia+CommonCrawl corpora <ref type="bibr" target="#b20">(Grave et al., 2018)</ref>. IndicFT <ref type="bibr" target="#b27">(Kakwani et al., 2020)</ref> provides embeddings trained on 3.9 million news articles and has 839 million tokens. Hossain and Hoque (2020) curated a dataset from  <ref type="bibr" target="#b51">(Vaswani et al., 2017)</ref> has been shown to be successful in various downstream NLP tasks <ref type="bibr" target="#b45">(Radford et al., 2019)</ref>. For Indian languages, these include multilingual BERT such as XLM-R <ref type="bibr" target="#b9">(Conneau et al., 2020)</ref>, multilingual BERT (mBERT) <ref type="bibr" target="#b43">(Pires et al., 2019)</ref>, IndicBERT <ref type="bibr" target="#b27">(Kakwani et al., 2020)</ref>, MuRIL <ref type="bibr" target="#b28">(Khanuja et al., 2021)</ref>, and models built specifically for Bangla such as BanglaBERT <ref type="bibr" target="#b3">(Bhattacharjee et al., 2022)</ref>, BanglishBERT <ref type="bibr" target="#b3">(Bhattacharjee et al., 2022</ref><ref type="bibr">), sahajBERT (Diskin et al., 2021)</ref>.</p><p>V ĀC-BERT is lighter than all these variants. Downstream Tasks: The detailed related work on downstream tasks performed in this paper is in Appendix A. Some of the notable works include <ref type="bibr" target="#b46">(Rakshit et al., 2015)</ref> for poem classification, <ref type="bibr" target="#b10">(Das and Bandyopadhyay, 2010;</ref><ref type="bibr">Hasan et al., 2020a)</ref> for sentiment classification, <ref type="bibr" target="#b34">(Mandal and Hossain, 2017;</ref><ref type="bibr" target="#b50">UzZaman and Khan, 2005;</ref><ref type="bibr" target="#b25">Islam et al., 2018)</ref> for spelling correction, <ref type="bibr">(Ekbal and</ref><ref type="bibr">Bandyopadhyay, 2008c,b, 2007;</ref><ref type="bibr" target="#b6">Chaudhuri and Bhattacharya, 2008;</ref><ref type="bibr" target="#b7">Chowdhury et al., 2018)</ref> for named entity recognition, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The V ĀCASPATI Corpus</head><p>Our aim is to generate a monolingual corpus for Bangla that can help build better and more accurate models for standard NLP tasks as well as in specialized downstream tasks <ref type="bibr">(Jurafsky and Martin, 2021)</ref>. Hence, we focused primarily on collecting literary data covering various types, including social, political, and religious. We scraped literature articles from various websites with publishers' permissions. While some metadata, such as type and date of publication, are available readily, other information, such as genre and region, was marked manually after going through the preface of each work. Only the works of authors available on these websites are used for curating the dataset. More information regarding the ethical considerations are  Overall, the V ĀCASPATI corpus consists of more than 115.1 million words and over 11.7 million sentences, and its size is 2.1 GB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Variety in V ĀCASPATI</head><p>The works collected in V ĀCASPATI are written between 1310 CE and the present year. The earliest works of Bangla language go back to this date and, thus, our corpus captures the temporal changes in the language features over time. One of the unique temporal features in Bangla is the transformation of sAdhu bhAShA (or refined language) to chalita bhAShA (or colloquial language). While till the 19th century, all written works were exclusively in sAdhu bhAShA, authors started switching to chalita bhAShA in different decades of the 20th century. Currently, almost all the works are in chalita bhAShA. The two differ mostly in verb forms and pronouns, and use exclusive sets of these. Consequently, a modern day native reader will find it hard to understand/read a piece of literature written in sAdhu bhAShA without practice. Newspaper articles, blogs and social media posts fail to capture this extremely unique transition phenomenon of the language. Table <ref type="table">1</ref> shows the break-up of the different time periods.</p><p>In addition to the temporal aspect, Bangla has a lot of variety in language that differs from one region to another. Specific words and phrases are often used by authors from these regions. Hence, we ensured that our corpus contains works from authors of both sides of erstwhile Bengal (currently the West Bengal state in India and the country Bangladesh). This enables us to capture the spatial features of the language across regions that speak quite far apart dialects such as in Bankura in India and Chattogram in Bangladesh. Table <ref type="table" target="#tab_2">2</ref> shows the spatial variations. In addition, it also shows the statistics for translated works.</p><p>V ĀCASPATI consists of works of 6 prominent types. It includes poetry, since poems capture a different flavor of the language including words, such as "mora" (my), "sAthe" (with), etc. that are exclusive to it. Table <ref type="table" target="#tab_4">3</ref> shows the statistics of the  types in our corpus. We collected essays as well.</p><p>The works collected in V ĀCASPATI are also arranged according to topics (Table <ref type="table" target="#tab_6">4</ref>). Notably, we have specialized categories such as children, law and religion, most of which are generally completely absent in newspapers and are rare even in blogs and social media posts. This highlights the variety of our corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Cleaning and Pre-Processing</head><p>Since V ĀCASPATI comprises literary texts from the 14th century, usage of old punctuation marks are prevalent in the corpus. For example, "।…", "।।" were prevalent in the 16th century and earlier texts, but are extinct now. Due to this issue, tokenization modules such as iNLTK (https: //inltk.readthedocs.io/en/latest/api_docs.html) fail to capture these punctuation marks and consider them as part of words. iNLTK fails to capture many such characters and words, and introduces (arbitrary) Unicode characters in their place.</p><p>Appendix B shows a list of such punctuation marks and Unicode characters. After cleaning all such non-Bangla Unicode characters and old punctuation marks, we split the text based on whitespaces to generate words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Embeddings and Models</head><p>We now describe the various word embedding and other models built using V ĀCASPATI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">V ĀC-FT</head><p>Bangla is morphologically rich and is invested with inflections. Hence, we work with FastText that can integrate sub-word information using ngram embeddings during training.</p><p>We train FastText embeddings for Bangla using V ĀCASPATI and evaluate their quality on two tasks, word similarity and spell-checking. We train word embeddings on a 300-dimensional vector space using FastText with Gensim (https://pypi. org/project/gensim/). Our skip-gram models have been trained for 100 iterations with a window  Since <ref type="bibr" target="#b30">(Kumar et al., 2020)</ref> showed that for inflectional languages such as Bangla, FastText performs better than Glove <ref type="bibr" target="#b42">(Pennington et al., 2014)</ref> and Word2Vec <ref type="bibr" target="#b37">(Mikolov et al., 2013)</ref>, hence, we show the results for only our FastText word vectors. We name this V ĀC-FT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">V ĀC-BERT</head><p>We next introduce V ĀC-BERT, which is built from V ĀCASPATI by pre-training using Electra, with the Replaced Token Detection (RTD) objective. A sequence with 15% as masked tokens is fed to the generator, which predicts the rest of the input. After replacing masked tokens with the generator's output distribution, the discriminator must predict whether each token is from the original sequence. The discriminator is used for fine-tuning. Since Electra shows comparable performance <ref type="bibr" target="#b8">(Clark et al., 2020)</ref> in several downstream tasks as compared to larger models such as RoBERTa <ref type="bibr" target="#b33">(Liu et al., 2019)</ref> and XLNet <ref type="bibr" target="#b53">(Yang et al., 2019)</ref> with only a fraction of their training time and memory, we used Electra for our implementation of V ĀC-BERT. We next describe in detail the pre-training and fine-tuning steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Pre-training</head><p>Using V ĀCASPATI, we first train a Word Piece tokenizer <ref type="bibr" target="#b52">(Wu et al., 2016)</ref> using n-gram embeddings to tokenize the sentences. We use this tokenized corpus to train V ĀC-BERT. We pre-trained the small variant of Electra model (a 12-layer transformer encoder having an embedding dimensionality of 128, hidden layer size of 256, 4 attention heads, feed-forward size of 3072, generatorto-discriminator ratio 1:3, and 50,000 vocabulary size, amounting to a total of 16.8 million parameters) with 64 batch sizes for 250K steps on a 40 GB instance of NVidia A100 GPU. We used the Adam optimizer (Kingma and Ba, 2015) with a 2e-4 learning rate. Training time was &lt;12 hours.</p><p>Our model is 7 times lighter than BanglaBERT <ref type="bibr" target="#b3">(Bhattacharjee et al., 2022)</ref>, which has 110M parameters and is trained for 2.5M steps in a TPU v3.0 instance. In-dicBERT <ref type="bibr" target="#b27">(Kakwani et al., 2020)</ref>, on the other hand, has 18M parameters and is trained for 400k steps on a TPU v3.0 instance. Other than these, we have also compared the performance of our V ĀC-BERT with a few other standard BERT models, including XLM-R base (280M parameters) <ref type="bibr" target="#b9">(Conneau et al., 2020)</ref>, XLM-R large (550M parameters) <ref type="bibr" target="#b9">(Conneau et al., 2020)</ref>, sahajBERT (18M parameters) <ref type="bibr" target="#b11">(Diskin et al., 2021)</ref> mBERT (180M parameters) <ref type="bibr" target="#b43">(Pires et al., 2019)</ref>, and MuRIL (236M parameters) <ref type="bibr" target="#b28">(Khanuja et al., 2021)</ref>. Thus, compared to all the state-of-the-art BERT models for Bangla, our V ĀC-BERT model is the lightest and fastest to build.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Fine-tuning</head><p>After pre-training, we fine-tune V ĀC-BERT on each task using the respective training sets. The fine-tuning is done independently for each task (i.e., we have a task-specific model) as described next in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>In this section, we quantitatively evaluate our corpus V ĀCASPATI through various downstream tasks. We test both simple word embeddings as well as transformer-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Word Embedding</head><p>We benchmark V ĀC-FT word embeddings against three FastText embeddings: (1) IndicFT, trained on the Bangla subset of IndicCorp <ref type="bibr" target="#b27">(Kakwani et al., 2020)</ref>, and pre-trained embeddings released by the FastText project trained on (2) Wikipedia, denoted by FT-W <ref type="bibr" target="#b4">(Bojanowski et al., 2017)</ref>, and (3) Wiki+CommonCrawl, denoted by FT-WC <ref type="bibr" target="#b20">(Grave et al., 2018)</ref>. Since the datasets for the other Bangla corpora are not released publicly, we could not test against them.</p><p>We evaluate the FastText word embeddings in two classes of tasks: (1) word similarity, and (2) classification-based. Unless otherwise mentioned, we evaluate the macro-F1 score throughout to compare the methods. We choose macro-F1 since the data classes are unbalanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Word Similarity</head><p>Bangla suffers from a lack of quality test sets for the word similarity task. IIIT-Hyderabad released a dataset <ref type="bibr" target="#b1">(Akhtar et al., 2017)</ref> for similarity measures of 100-200 word pairs for seven languages that do not include Bangla.</p><p>We created a benchmark dataset for Bangla with 600 word pairs to fill the gap. These words are chosen directly from a well-known Bangla grammar book <ref type="bibr" target="#b5">(Chattopadhyay, 1942)</ref> and are classified in 4 categories similar to the one specified in the book: synonyms, antonyms, homonyms, and dissimilar.</p><p>The details of the 4 categories are: • Synonyms: There are 347 synonymous word pairs such as "mAtA" and "jananI" (both mean mother) in the test dataset. • Antonyms: These word pairs (119 in number) are opposite to each other, e.g., "duShTa" (bad) and "shiShTa" (good). • Homonyms: These word pairs (84 in number) sound the same, but their meanings are different, e.g., "kona" (which) and "koNa" (angle). • Dissimilar: These words are completely dissimilar, e.g., "nadI" (river) versus "shIta" (cold).</p><p>There are 50 such word pairs. The word similarity task has been treated as a binary classification problem where a cosine similarity value between the two word embeddings above a threshold is considered as a "similar" class; otherwise, the class is "dissimilar". While synonyms form the "similar" class of word pairs, the other three constitute the "dissimilar" class. We kept the cosine similarity threshold as 0.5 after running experiments at intervals of 0.05 from 0.2 to 0.9.</p><p>Table <ref type="table" target="#tab_8">5</ref> shows that the macro-F1 score of V ĀC-FT is the best (64.5%), and is significantly better than others (by 8-10%). The performance of V ĀC-FT on each class of words is described in Table <ref type="table" target="#tab_10">6</ref>.</p><p>V ĀC-FT correctly classifies synonymous word pairs such as "rAjA" (king) and "mahIpati" (king); other embeddings fail to even capture the word  "mahIpati" due to lack of variety in their corpora.</p><p>For word pairs such as "udvRRitta" (excess) and "ghATati" (dearth) that are antonyms of each other, only V ĀC-FT gives the correct result. This is again due to superior and larger vocabulary of V ĀCASPATI.</p><p>Since antonyms are semantically opposite of each other, they often occur in similar context in texts and, thus, are predicted to have similar Fast-Text embeddings. Also, in Bangla, antonyms often differ in only one character ("sakarmaka" or "transitive verb" versus "akarmaka" or "intransitive verb"), which is hard for models such as Fast-Text to capture.</p><p>We next evaluate the FastText embeddings on different text classification tasks:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Poem Classification</head><p>We evaluate a subject-based classification task on poems written by Rabindranath Tagore, who is one of the greatest poets of Bangla, into 5 categories as indicated by the author himself. The categories are pUjA (devotion), prema (love), prAkR-Riti (nature), svadesha (nationalism), and vichitrA (miscellaneous). We collected the poems from the website of The Complete Works of Tagore available at https://tagoreweb.in/. Five-fold crossvalidation was done on all 1,451 poems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Sentiment Classification</head><p>Sentiment analysis is the task of classifying people's opinions and emotions towards entities such as products, services, organizations, and others. Sentiment classification is a prevalent downstream task in Bangla to indicate the efficacy of the corpus. Some of the earlier works of sentiment classification were done a decade back <ref type="bibr" target="#b10">(Das and Bandyopadhyay, 2010)</ref>.</p><p>For our work, we use two publicly available datasets. The first dataset <ref type="bibr" target="#b48">(Sazzed, 2020)</ref> consists of 3,307 negative and 8,500 positive reviews annotated on YouTube Bangla drama. The second dataset <ref type="bibr">(Islam et al., 2021)</ref> comprises 3 polarity labels, positive, negative, and neutral, and is collected from social media comments on news and videos covering 13 domains, including politics, ed-  The results of all the tasks are shown in Table <ref type="table" target="#tab_8">5</ref> (the best results for all the FastText models were obtained for k=11). V ĀC-FT performs the best for all the classification tasks. This shows that V ĀC-FT captures the semantics of words better, For literary data such as poems, it outperforms others by a significant margin (6-8%). Even for tasks such as sentiment classification that does not use literary data, it is effective (1-4% better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Transformer-based Models</head><p>After establishing the superior performance of our corpus for word embedding based tasks, we switch to state-of-the-art transformer models and test the performance on various downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Tasks</head><p>In addition to poem classification (5-class) and sentiment classification (both 2-class and 3-class) tasks as described previously, we conduct two more tasks that are possible with transformerbased models.</p><p>Of the 5 tasks that we test, poem classification and spelling error detection can be considered more as literature tasks in the sense that a corpus with literature data is likely to do better, while the other three, namely sentiment classification (both tasks) and NER are non-literature tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Spelling error detection</head><p>Spelling error is a customary problem in every language. The prevalent Bangla spell-checkers <ref type="bibr" target="#b34">(Mandal and Hossain, 2017;</ref><ref type="bibr" target="#b50">UzZaman and Khan, 2005)</ref> do not take into account the context and, therefore,</p><formula xml:id="formula_0">Model Correct Sentence Incorrect Sentence P R F1 P R F1</formula><p>BanglaBERT 87% 98% 92% 89% 52% 66% V ĀC-BERT 88% 95% 91% 77% 55% 64% fail to deal with many real-world spelling errors such as "িবষ" (viSha, poison) versus "িবশ" (visha, twenty) and "েকান" (kona, which) versus "েকাণ" (koNa, angle) since both the words are present in Bangla vocabulary. The issues get greatly manifested in texts produced through OCR, such as when "সূ যর্ অস্ত েগল" (sUrya asta gela, "the sun set") gets changed to "সূ যর্ অন্ত েগল" (sUrya anta gela, "the sun ended"). In both these sentences, all the words are correct and are part of Bangla vocabulary. There is no flaw in the grammar either. To correct such errors, the model needs to understand the semantics and context of a sentence.</p><p>To the best of our knowledge, there is no publicly available dataset for context-sensitive spelling error detection in Bangla. Hence, we created our own dataset using OCR.</p><p>We took two books for which both pdf copies as well as the actual text (through scraping) are available freely. The pdf versions were subjected to OCR using Tesseract (https://pypi.org/ project/pytesseract/). The corresponding text data were considered as the ground truth. This spellchecking dataset was not present during the pretraining of V ĀC-BERT.</p><p>Since OCR consists of fragmented and missed sentences, we map 30-length word sequences between OCR data and scraped data, and map the pair of words with highest similarity in the sentence using edit distance. The threshold for similarity is kept at 0.6 which was chosen empirically after considering all the thresholds between 0.1 and 1.0 at intervals of 0.05. We generated a dataset of 110,356 sequence pairs.</p><p>We used V ĀC-BERT to detect errors in the sentence. We treated this problem as a sequence pair classification task where a sentence is passed as one sequence, and every word in it is passed as the second sequence to determine whether it is contextually correct. The dataset is divided into 80%-20% for training and testing. Table <ref type="table" target="#tab_11">7</ref> shows the performance of BanglaBERT and V ĀC-BERT on the spelling error detection task. V ĀC-BERT has a higher recall for the error class, which means it can detect more context-sensitive spelling errors. Detecting more errors gives a better chance of correcting errors in the next phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Named Entity Recognition (NER)</head><p>Named Entity Recognition (NER) is a sequence labeling task that finds spans of text and tags them to a named entity class <ref type="bibr">(Jurafsky and Martin, 2021)</ref>. We feed each sentence to the model as a single sequence. For every token, a softmax layer computes the probability distribution over the classes. Multi-class cross-entropy loss is used for fine-tuning the model.</p><p>We chose the publicly available Wikiann (Bangla subset) <ref type="bibr" target="#b41">(Pan et al., 2017)</ref> NER dataset created from Wikipedia data. The dataset consists of 15,445 sentences (with a test-train split of 80-20%) and more than 135,000 tokens. The tokens are tagged into 4 classes: person (Per), location (Loc), organization (Org), and other (O).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance of V ĀC-BERT</head><p>We evaluated the performance of our transformerbased model, V ĀC-BERT, built on our V ĀCASPATI corpus vis-a-vis other state-ofthe-art transformer-based models available in Bangla on the tasks specified above. The models include monolingual models such as BanglaBERT and sahajBERT as well as multilingual models such as XLM (base and large), mBERT, IndicBERT, MuRIL, and BanglishBERT. The details of these models are discussed in Section 2. Unless otherwise mentioned, the macro-F1 score is used as the evaluation metric for all the tasks.</p><p>Table <ref type="table" target="#tab_12">8</ref> shows the performance of V ĀC-BERT and other transformer-based models on different tasks. We ran each task 5 times and reported the best result among them. The standard deviation for each task was small and varied between 0.003 and 0.005.</p><p>The macro-F1 score of our V ĀC-BERT model is 60.39% for poem classification, which is the best.</p><p>We have also performed the best for sentiment classification (3-class) and is within 0.25% of macro-F1 score of the best competing model (MuRIL and BanglaBERT) in sentiment classification (2-class). The task of sentiment classification is performed on public non-literary datasets.</p><p>In the spelling error detection task, we are within ∼1% of BanglaBERT. An in-depth analysis, however, shows that V ĀC-BERT gives 55% recall on the error class (i.e., wrongly spelled words) as compared to 52% for BanglaBERT (Table <ref type="table" target="#tab_11">7</ref>). This implies that V ĀC-BERT is able to detect more context-sensitive spelling errors. Detecting more errors gives a better chance of correcting errors in the next phase. Thus, the lower precision in the error class can be improved in the error correction stage later.</p><p>Our model performs close to the best model (within ∼2%) on NER. Since the NER task is measured on a public dataset taken from Wikipedia, BanglaBERT and MuRIL (who are better than V ĀC-BERT) may have already seen this data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Corpus Efficacy</head><p>In the previous section, we established that models built on V ĀCASPATI with fewer parameters either perform better or are comparable to state-ofthe-art models. In this section, we will show that V ĀCASPATI, as a corpus, has telling effects on the results obtained for the downstream tasks and, thus, it is not only the models due to which gains are observed.</p><p>To establish that, we pre-trained an Electrasmall transformer, Indic-ELECTRA, with the same hyper-parameters and architecture as V ĀC-BERT, on the Bangla subset of IndicCorp <ref type="bibr" target="#b27">(Kakwani et al., 2020)</ref>. We could not do this for the other models since their corresponding corpora are not available publicly. We also combine (i.e., take union with) the Bangla subset of the IndicCorp corpus with our V ĀCASPATI corpus, and pre-train another Electra-small transformer with again the same hyper-parameters and architecture. We call this model IV-ELECTRA.</p><p>Table <ref type="table" target="#tab_13">9</ref> shows that V ĀC-BERT outperforms IndicElectra on all the tasks. Since the only change between the two models is the corpus (V ĀCASPATI versus IndicCorp), this indicates the possible superiority of V ĀCASPATI as a corpus. V ĀC-BERT also outperforms IV-ELECTRA on average, even on non-literary datasets. It indicates that models built from only V ĀCASPATI (literary corpus) are good enough to perform language tasks. Further, the requirement for a large dataset, which is cumbersome to curate for a low-resource language like Bangla, is also done away with.</p><p>Table <ref type="table" target="#tab_13">9</ref> captures one more interesting effect. The performance of Indic-ELECTRA is enhanced on adding V ĀCASPATI to the IndicCorp dataset much more than is done for V ĀCASPATI by adding IndicCorp to it. Thus, the incremental benefit in performance of models by adding a literature dataset over a non-literature dataset is much more significant than that by adding a non-literature dataset over a literature dataset. In fact, in the literature tasks, adding non-literature data reduces the performance. This is due to poorer quality of language as compared to a literature corpus and presence of various types of noise and errors. This reinforces the importance of quality data in a corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Necessity of Temporal Variation</head><p>To assess the necessity of inclusion of works from different temporal periods, we segregated V ĀCASPATI into two parts, pre-1941 and post-1941. The most influential litterateur of Bangla, Rabindranath Tagore, passed away in 1941, which had a large effect on the Bangla literary style. We pre-train the same Electra model, with the same hyper-parameters and architecture as used in V ĀC-BERT on both divisions of the dataset.</p><p>Table <ref type="table" target="#tab_14">10</ref> shows the performance of the split corpora on 3 downstream classification tasks. Pre- 1941 data significantly outperformed post-1941 data on the poem classification task, whereas for sentiment classification tasks, the post-1941 data performed better. The complete V ĀCASPATI corpus performed equally well on both the tasks. The poem classification data was based on Rabindranath Tagore's poetry, which shows that a corpus with only modern writings may not have the literary variety to capture nuances of earlier works. The difference in performance is more than 7%. This also shows why corpora built from newspapers, blogs and social media posts will perform poorly on older literary data. The test set for sentiment classification task is on modern writing. Although the pre-1941 data performs fairly, having modern examples of the language helps to improve the performance. V ĀCASPATI, which includes both pre-1941 as well as post-1941 data achieves the best balance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Efficiency of Transformer Models</head><p>We monitored the training time and memory usage during the fine-tuning stage for downstream tasks for the transformer-based models for Bangla including V ĀC-BERT. The results are shown in Table <ref type="table" target="#tab_15">11</ref>. All the tasks were run on an A100 GPU machine with 40 GB memory. All the models had the same batch size (16) and sequence length (128) for comparisons. V ĀC-BERT was the fastest and it used the least amount of memory. This makes it easiest to deploy for real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussions and Future Work</head><p>In this paper, we proposed a quality Bangla corpus based only on literature, called V ĀCASPATI, that includes variations across several aspects.</p><p>The experiments showed that V ĀCASPATI, as a corpus, has better quality than other corpora, since it could outperform models built on those corpora using the same hyper-parameters and architecture. Also, the temporal variation in V ĀCASPATI is useful since it provides a nice balance between downstream tasks on modern Bangla and older variants.</p><p>Further, V ĀC-FT model built using V ĀCASPATI has far fewer out-of-vocabulary words as com- pared to other FastText models and, thus, outperforms them on the word similarity task. V ĀC-FT also outperforms other models on all the classification tasks which also establishes the quality of V ĀCASPATI as a corpus.</p><p>V ĀC-BERT, which is an Electra-small variant built from V ĀCASPATI, has performance either better then or similar to other BERT models on various downstream tasks. On average, V ĀC-BERT outperforms all the competing BERT models. Since V ĀCASPATI is a corpus made from only literary data, V ĀC-BERT did not use newspaper or social media articles during pre-training. Most of the other BERT models had most likely used those data during pre-training. Hence, the performance of V ĀC-BERT on non-literary data is significant to establish the quality of V ĀCASPATI as a corpus.</p><p>The experiments with (the Bangla subset of) In-dicCorp using the exact same model architecture and hyper-parameters shows that V ĀCASPATI is a better corpus. Also, addition of non-literary data on top of literary data produces minimal incremental improvement while the reverse can produce significant benefits.</p><p>Finally, V ĀC-BERT needed the least amount of time and memory for fine-tuning. This makes it the easiest model to deploy for real-world applications and in resource-constrained scenarios such as mobile phones, etc.</p><p>Future Work: A quality corpus for any language is useful to get better performing models for various NLP tasks including low-level tasks such as lemmatization, POS tagging, NER, dependency parsing, etc. The corpus can also be used for designing generative AI tasks and models. We hope our V ĀCASPATI corpus will fuel more research in building better models for Bangla and other similar low-resource languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations</head><p>Curating a quality dataset from literary articles for Bangla is challenging as not many books are available as text files without copyright issues. OCR texts are noisy and are, thus, not included in the dataset. FastText embeddings and BERT models may improve with more data; however, training larger models is hindered by resource constraints. More data may not always reflect a gain in performance, since it may be noisy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Ethics Statement</head><p>The V ĀCASPATI corpus dataset has been curated by scraping literary works available in the public domain. They are taken from websites that have the authors' permission to showcase their works. We restricted our dataset to only such works. Hence, no copyright infringement has been done. Since the corpus is very large, it was not feasible to take steps to detect offensive content. However, since we only used published literary works, it is unlikely that our dataset contains highly objectionable content. Other than these, we see no other ethical concerns in the paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Spatial variation of V ĀCASPATI presented in Sec. 8.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Type of works in V ĀCASPATI</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Distribution of topics in V ĀCASPATI</figDesc><table /><note><p><p><p>size of 10 and a minimum word size of 4 characters. For each instance, 10 negative examples were used. These parameters are chosen based on suggestions by</p><ref type="bibr" target="#b20">(Grave et al., 2018)</ref> </p>and based on the average length of words and characters in words of a Bangla sentence (9.84 words per sentence and 5.36 characters per word) present in V ĀCASPATI.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Macro-F1 of different FastText models.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>FastText models for word similarity task</figDesc><table><row><cell>ucation, and agriculture. It consists of 5,709 nega-</cell></row><row><cell>tive, 6,410 positive, and 3,609 neutral sentences.</cell></row><row><cell>5.1.4 Results on Classification Tasks</cell></row><row><cell>Since we wanted to evaluate the effect of FastText</cell></row><row><cell>embeddings built from the corpus on the classifica-</cell></row><row><cell>tion tasks, we chose k-nearest-neighbors (kNN) as</cell></row><row><cell>our classifier following Meng et al. (2019) who ar-</cell></row><row><cell>gued that the classification performance of a non-</cell></row><row><cell>parametric classifier indicates the best how well</cell></row><row><cell>text semantics have been captured by the Fast-</cell></row><row><cell>Text embeddings. The input text embedding is the</cell></row><row><cell>mean of word embeddings in the article.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Performance on spelling error detection.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Parameters and performance of different transformer-based models (performance measure is macro-F1). ∆ shows the difference with V ĀC-BERT.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Literature</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Non-literature</cell><cell></cell><cell cols="2">Overall</cell></row><row><cell>Model</cell><cell>Parameters</cell><cell cols="3">Poem Spell Error Literature</cell><cell>∆</cell><cell cols="4">Sentiment Sentiment NER Non-Literature</cell><cell>∆</cell><cell>Overall</cell><cell>∆</cell></row><row><cell></cell><cell></cell><cell cols="2">(5-class) Detection</cell><cell cols="3">Average Literature (2-class)</cell><cell cols="2">(3-class) (4-class)</cell><cell>Average</cell><cell cols="3">Non-Literature Average Overall</cell></row><row><cell>mBERT</cell><cell cols="2">180M 59.85</cell><cell>41.00</cell><cell>50.43</cell><cell>-18.63</cell><cell>94.80</cell><cell>63.68</cell><cell>86.51</cell><cell>81.66</cell><cell>-2.75</cell><cell>69.17</cell><cell>-9.10</cell></row><row><cell>XLM-R (base)</cell><cell cols="2">270M 35.00</cell><cell>42.50</cell><cell>38.75</cell><cell>-33.12</cell><cell>94.64</cell><cell>67.8</cell><cell>87.50</cell><cell>83.31</cell><cell>-2.20</cell><cell>65.50</cell><cell>-14.47</cell></row><row><cell>XLM-R (large)</cell><cell cols="2">550M 31.86</cell><cell>40.00</cell><cell>35.93</cell><cell>-30.30</cell><cell>93.89</cell><cell>65.75</cell><cell>87.60</cell><cell>82.21</cell><cell>-1.10</cell><cell>63.80</cell><cell>-12.77</cell></row><row><cell>IndicBERT</cell><cell cols="2">18M 43.67</cell><cell>41.50</cell><cell>42.59</cell><cell>-26.46</cell><cell>91.86</cell><cell>59.34</cell><cell>88.31</cell><cell>79.84</cell><cell>-4.57</cell><cell>64.94</cell><cell>-13.33</cell></row><row><cell>MuRIL</cell><cell cols="2">236M 52.36</cell><cell>42.50</cell><cell>47.43</cell><cell>-21.62</cell><cell>95.00</cell><cell>68.73</cell><cell>90.62</cell><cell>84.78</cell><cell>+0.37</cell><cell>69.84</cell><cell>-8.43</cell></row><row><cell>BanglishBERT</cell><cell cols="2">110M 49.86</cell><cell>74.56</cell><cell>62.21</cell><cell>-6.84</cell><cell>93.45</cell><cell>66.58</cell><cell>89.52</cell><cell>83.18</cell><cell>-1.23</cell><cell>74.79</cell><cell>-3.48</cell></row><row><cell>BanglaBERT</cell><cell cols="2">110M 54.40</cell><cell>78.90</cell><cell>66.65</cell><cell>-2.40</cell><cell>95.00</cell><cell>68.68</cell><cell>91.52</cell><cell>85.06</cell><cell>+0.65</cell><cell>77.72</cell><cell>-0.55</cell></row><row><cell>sahajBERT</cell><cell cols="2">18M 46.34</cell><cell>70.89</cell><cell>58.61</cell><cell>-10.44</cell><cell>93.75</cell><cell>67.52</cell><cell>85.50</cell><cell>82.25</cell><cell>-2.16</cell><cell>72.80</cell><cell>-5.47</cell></row><row><cell>V ĀC-BERT</cell><cell cols="2">17M 60.39</cell><cell>77.72</cell><cell>69.05</cell><cell>0.00</cell><cell>94.75</cell><cell>68.79</cell><cell>89.70</cell><cell>84.41</cell><cell>0.00</cell><cell>78.27</cell><cell>0.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Literature</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Non-literature</cell><cell></cell><cell cols="2">Overall</cell></row><row><cell>Model</cell><cell>Parameters</cell><cell cols="3">Poem Spell Error Literature</cell><cell>∆</cell><cell cols="4">Sentiment Sentiment NER Non-Literature</cell><cell>∆</cell><cell>Overall</cell><cell>∆</cell></row><row><cell></cell><cell></cell><cell cols="2">(5-class) Detection</cell><cell cols="3">Average Literature (2-class)</cell><cell cols="2">(3-class) (4-class)</cell><cell>Average</cell><cell cols="3">Non-Literature Average Overall</cell></row><row><cell>Indic-ELECTRA</cell><cell cols="2">17M 48.46</cell><cell>74.58</cell><cell>61.52</cell><cell>-7.53</cell><cell>93.86</cell><cell>67.20</cell><cell>89.08</cell><cell>83.38</cell><cell>-1.03</cell><cell>74.63</cell><cell>-3.64</cell></row><row><cell>IV-ELECTRA</cell><cell cols="2">17M 60.00</cell><cell>76.64</cell><cell>68.32</cell><cell>-0.73</cell><cell>94.25</cell><cell>66.31</cell><cell>90.3</cell><cell>83.62</cell><cell>-0.79</cell><cell>77.50</cell><cell>-0.77</cell></row><row><cell>V ĀC-BERT</cell><cell cols="2">17M 60.39</cell><cell>77.72</cell><cell>69.05</cell><cell>0.00</cell><cell>94.75</cell><cell>68.79</cell><cell>89.70</cell><cell>84.41</cell><cell>0.00</cell><cell>78.27</cell><cell>0.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Comparison with IndicElectra. ∆ shows the difference with V ĀC-BERT.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Performance on temporal variations</figDesc><table><row><cell>Time-Period</cell><cell>Poem (5-class)</cell><cell cols="2">Sentiment Sentiment (2-class) (3-class)</cell></row><row><cell>Pre-1941 V ĀC-BERT</cell><cell>63.89</cell><cell>92.67</cell><cell>60.89</cell></row><row><cell>Post-1941 V ĀC-BERT</cell><cell>56.56</cell><cell>94.75</cell><cell>68.95</cell></row><row><cell>Complete V ĀC-BERT</cell><cell>60.39</cell><cell>94.75</cell><cell>68.79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Time and space usage of transformer-based models for Bangla.</figDesc><table><row><cell>Model</cell><cell cols="2">Max Fine-tuning Max Memory Time (secs/epoch) Usage (GB)</cell></row><row><cell>IndicBERT</cell><cell>9.74</cell><cell>14.30</cell></row><row><cell>MuRIL</cell><cell>38.66</cell><cell>24.45</cell></row><row><cell>BanglaBERT</cell><cell>43.09</cell><cell>25.65</cell></row><row><cell>BanglishBERT</cell><cell>39.35</cell><cell>25.65</cell></row><row><cell>sahajBERT</cell><cell>46.35</cell><cell>27.15</cell></row><row><cell>mBERT</cell><cell>44.45</cell><cell>25.73</cell></row><row><cell>XLM-R (base)</cell><cell>44.95</cell><cell>25.74</cell></row><row><cell>XLM-R (large)</cell><cell>52.35</cell><cell>34.63</cell></row><row><cell>V ĀC-BERT</cell><cell>3.46</cell><cell>10.60</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We have used "ITrans" format for transliteration of Bangla words (https://en.wikipedia.org/wiki/ITRANS).</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Related Work for Downstream Tasks</head><p>In this section, we discuss in detail the related work for the downstream tasks tested in the paper. Poem Classification: Poem classification into subject-based categories or genres is a challenging downstream task as it requires understanding the semantics and nuances of the language. <ref type="bibr" target="#b46">Rakshit et al. (2015)</ref> tried the same on Rabindranath Tagore's works by segregating poems into 4 categories. In this paper, we include the more challenging miscellaneous category to make it a 5-class problem. Sentiment Classification: Das and Bandyopadhyay (2010) has evaluated the polarity of opinionated phrases on news articles using SVM into two classes, positive and negative. <ref type="bibr">Hasan et al. (2020a)</ref> has, however, shown that transformer-based models outperform other models for Bangla. Spelling Correction: The prevalent Bangla spell checkers are mostly context-free. <ref type="bibr" target="#b34">Mandal and Hossain (2017)</ref>  NER: NER (Named Entity Recognition) is a sequence labelling task that finds spans of text constituting proper names and tags them to a named entity <ref type="bibr">(Jurafsky and Martin, 2021)</ref>. Most of the NER works in Bangla are conducted a decade ago. Ekbal and Bandyopadhyay (2008c) focused on corpus development. Different models were tried for the NER task, namely, feature engineering <ref type="bibr">(Ekbal and Bandyopadhyay, 2008b)</ref>   <ref type="bibr" target="#b13">Bandyopadhyay, 2007)</ref>, CRF <ref type="bibr">(Ekbal et al., 2008)</ref>, SVM <ref type="bibr">(Ekbal and Bandyopadhyay, 2008a)</ref>, and ME (Multi-Engine) <ref type="bibr" target="#b17">(Ekbal and Bandyopadhyay, 2009)</ref>. The reported micro-F1 varies between 82% to 91% for various entities. Chaudhuri and Bhattacharya ( <ref type="formula">2008</ref>) uses a hybrid approach using rule and n-gram based statistical modeling. <ref type="bibr" target="#b7">Chowdhury et al. (2018)</ref> developed LSTM with CRF model and achieved a micro-F1 score of 72%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Data Cleaning</head><p>• Cleaning of Unicode characters: Unicode characters "0020" (space), "00a0" (no-break space), "200c" (zero width non-joiner), "1680" (Ogham space mark), "180e" (Mongolian vowel separator), "202f " (narrow no-break space), "205f " (medium mathematical space), "3000" (ideographic space), "2000" (en quad), and "200a" (hair space) were separated from the texts. • Cleaning of different punctuation marks: In Bangla, usage of punctuation marks has also evolved alongside words. In particular, we have treated the following as punctuation marks: "…", "।…", "।।", "!-", and "-".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Poem Classification</head><p>Table <ref type="table">A1</ref> shows the details. The vichitrA (miscellaneous) class is the toughest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bengali word embeddings and it&apos;s application in solving document classification problem</title>
		<author>
			<persName><forename type="first">Adnan</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Ruhul Amin</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCITECHN.2016.7860236</idno>
	</analytic>
	<monogr>
		<title level="m">2016 19th International Conference on Computer and Information Technology (ICCIT)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="425" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Word Similarity Datasets for Indian languages: Annotation and Baseline systems</title>
		<author>
			<persName><forename type="first">Arihant</forename><surname>Syed Sarfaraz Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avijit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjit</forename><surname>Vajpayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><surname>Shrivastava</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-0811</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Linguistic Annotation Workshop</title>
		<meeting>the 11th Linguistic Annotation Workshop<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="91" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Polyglot: Distributed Word Representations for Multilingual NLP</title>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Banglabert: Lagnuage Model Pretraining and Benchmarks for Low-Resource Language Understanding Evaluation in Bangla</title>
		<author>
			<persName><forename type="first">Abhik</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tahmid</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wasi</forename><surname>Ahmad Uddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazi</forename><surname>Mubasshir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><forename type="middle">Saiful</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anindya</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sohel Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rifat</forename><surname>Shahriyar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>In Findings of the North American Chapter of the Association for Computational Linguistics: NAACL 2022</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enriching Word Vectors with Subword Information</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00051</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Sunitikumar</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<title level="m">Bhasha-prakash Bangala Byakaran</title>
		<imprint>
			<date type="published" when="1942">1942</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Calcutta</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An Experiment on Automatic Detection of Named Entities in Bangla</title>
		<author>
			<persName><forename type="first">Bidyut</forename><surname>Baran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaudhuri</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Suvankar</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IJCNLP-08 Workshop on Named Entity Recognition for South and South East Asian Languages</title>
		<meeting>the IJCNLP-08 Workshop on Named Entity Recognition for South and South East Asian Languages</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards Bangla Named Entity Recognition</title>
		<author>
			<persName><forename type="first">Firoj</forename><surname>Shammur Absar Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naira</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><surname>Khan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCITECHN.2018.8631931</idno>
	</analytic>
	<monogr>
		<title level="m">2018 21st International Conference of Computer and Information Technology (ICCIT)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ELECTRA: Pretraining Text Encoders as Discriminators Rather Than Generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised Cross-lingual Representation Learning at Scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Phraselevel Polarity Identification for Bangla</title>
		<author>
			<persName><forename type="first">Amitava</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Linguistics Appl</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="169" to="182" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed Deep Learning In Open Collaborations</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Michael Diskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Bukhtiyarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Ryabinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Sinitsin</surname></persName>
		</author>
		<author>
			<persName><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Pyrkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kashirin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Borzunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Villanova Del Moral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Mazur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Kobelev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gennady</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><surname>Pekhimenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7879" to="7897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards leaving no Indic language behind: Building monolingual corpora, benchmark and models for Indic languages</title>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Doddapaneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Aralikatte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gowtham</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Kunchukuttan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyush</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.acl-long.693</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="12402" to="12426" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Hidden Markov Model Based Named Entity Recognition System: Bengali and Hindi as Case Studies</title>
		<author>
			<persName><forename type="first">Asif</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-77046-6_67</idno>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition and Machine Intelligence, Second International Conference, PReMI 2007, Kolkata, India</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007-12-18">2007. December 18-22, 2007</date>
			<biblScope unit="volume">4815</biblScope>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bengali Named Entity Recognition Using Support Vector Machine</title>
		<author>
			<persName><forename type="first">Asif</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IJCNLP-08 Workshop on Named Entity Recognition for South and South East Asian Languages</title>
		<meeting>the IJCNLP-08 Workshop on Named Entity Recognition for South and South East Asian Languages</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Development of Bengali Named Entity Tagged Corpus and its Use in NER Systems</title>
		<author>
			<persName><forename type="first">Asif</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Asian Language Resources, ALR@IJCNLP 2008</title>
		<meeting>the 6th Workshop on Asian Language Resources, ALR@IJCNLP 2008<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<publisher>Asian Federation of Natural Language Processing</publisher>
			<date type="published" when="2008">2008. Jnuary 11-12, 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A web-based Bengali news corpus for named entity recognition</title>
		<author>
			<persName><forename type="first">Asif</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="182" />
			<date type="published" when="2008">2008. 2008</date>
			<publisher>CO-DEN -COHUAD</publisher>
			<pubPlace>Science+Business Media B.V.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Named Entity Recognition in Bengali: A Multi-Engine Approach</title>
		<author>
			<persName><forename type="first">Asif</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<idno type="DOI">10.3384/nejlt.2000-1533.091226</idno>
	</analytic>
	<monogr>
		<title level="j">Northern European Journal of Language Technology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="26" to="58" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Named Entity Recognition in Bengali: A Conditional Random Field Approach</title>
		<author>
			<persName><forename type="first">Asif</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rejwanul</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Joint Conference on Natural Language Processing</title>
		<meeting>the Third International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>II</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 languages</title>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Goldhahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Eckart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uwe</forename><surname>Quasthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA)</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="759" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning Word Vectors for 157 Languages</title>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA)</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Shammur Absar Chowdhury, and Firoj Alam. 2020a. Sentiment Classification in Bangla Textual Content: A Comparative Study</title>
		<author>
			<persName><forename type="middle">Arid</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jannatul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><surname>Tajrin</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2011.10106</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Not Low-Resource Anymore: Aligner Ensembling, Batch Filtering, and New Datasets for Bengali-English Machine Translation</title>
		<author>
			<persName><forename type="first">Tahmid</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhik</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazi</forename><surname>Samin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masum</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madhusudan</forename><surname>Basak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sohel Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rifat</forename><surname>Shahriyar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.207</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2612" to="2623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards Bengali Word Embedding: Corpus Creation, Intrinsic and Extrinsic Evaluations</title>
		<author>
			<persName><surname>Md</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Natural Language Processing</title>
		<meeting>the 17th International Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Rajib Hossain and Mohammed Moshiul Hoque</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sentnob: A Dataset for Analysing Sentiment on Noisy Bangla Texts</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Ittehadul</forename><surname>Khondoker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Md</forename><surname>Islam</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sudipta</forename><surname>Saiful Islam</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Ruhul</forename><surname>Kar</surname></persName>
		</editor>
		<editor>
			<persName><surname>Amin</surname></persName>
		</editor>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Context-Sensitive Approach to Find Optimum Language Model for Automatic Bangla Spelling Correction</title>
		<author>
			<persName><forename type="first">Muhammad Ifte Khairul</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><forename type="middle">Tarek</forename><surname>Habib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Sadekur Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><forename type="middle">Riazur</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farruk</forename><surname>Ahmed</surname></persName>
		</author>
		<idno type="DOI">10.14569/IJACSA.2018.091126</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Computer Science and Applications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<title level="m">2021. Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">IndicNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages</title>
		<author>
			<persName><forename type="first">Divyanshu</forename><surname>Kakwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Kunchukuttan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satish</forename><surname>Golla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Gokul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avik</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyush</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.445</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In Findings of the Association for Computational Linguistics: EMNLP 2020</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Simran</forename><surname>Khanuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diksha</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarvesh</forename><surname>Mehtani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Savya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atreyee</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Gopalan</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2103.10730</idno>
		<title level="m">Muril: Multilingual Representations for Indian Languages</title>
		<editor>
			<persName><forename type="first">Dilip</forename><surname>Kumar Margam</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pooja</forename><surname>Aggarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rajiv</forename><surname>Teja Nagipogu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shachi</forename><surname>Dave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shruti</forename><surname>Gupta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Subhash</forename><surname>Chandra Bose</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vish</forename><surname>Gali</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Partha</forename><surname>Subramanian</surname></persName>
		</editor>
		<editor>
			<persName><surname>Talukdar</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">a Passage to India&quot;: Pre-trained Word Embeddings for Indian Languages</title>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saunack</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st</title>
		<meeting>the 1st</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Diptesh Kanojia, and Pushpak Bhattacharyya</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint Workshop on Spoken Language Technologies for Under-resourced languages and Collaboration and Computing for Under-Resourced Languages</title>
	</analytic>
	<monogr>
		<title level="m">SLTU/CCURL@LREC 2020</title>
		<meeting><address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources association</publisher>
			<date type="published" when="2020-05">May 2020</date>
			<biblScope unit="page" from="352" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Complexity Guided Active Learning for Bangla Grammar Correction</title>
		<author>
			<persName><forename type="first">Bibekananda</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sutanu</forename><surname>Chakraborti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Choudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Natural Language Processing</title>
		<meeting>the 10th International Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1907.11692</idno>
		<title level="m">Roberta: A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Clustering-based Bangla spell checker</title>
		<author>
			<persName><forename type="first">Prianka</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Mainul Hossain</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIVPR.2017.7890878</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Imaging, Vision Pattern Recognition (icIVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">EMILLE: building a corpus of South Asian languages</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Mcenery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamish</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Translation and Multilingual Applications in the new Millennium: MT 2000</title>
		<meeting>the International Conference on Machine Translation and Multilingual Applications in the new Millennium: MT 2000</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>University of Exeter, UK.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglei</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<title level="m">Spherical Text Embedding</title>
		<meeting><address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mohammad Selim, and Muhammed Iqbal</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Mumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abu</forename><surname>Awal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Shoeb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supara: A Balanced English-Bengali Parallel Corpus</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="46" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Javier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ortiz</forename><surname>Suárez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
		<idno type="DOI">10.14618/IDS-PUB-9021</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">United</forename><surname>Cardiff</surname></persName>
		</author>
		<author>
			<persName><surname>Kingdom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Leibniz-Institut für Deutsche Sprache</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Crosslingual name tagging and linking for 282 languages</title>
		<author>
			<persName><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1178</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1946" to="1958" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">How Multilingual is Multilingual BERT?</title>
		<author>
			<persName><forename type="first">Telmo</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1493</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4996" to="5001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Building a 70 billion word corpus of English from ClueWeb</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Pomikálek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miloš</forename><surname>Jakubíček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Rychlý</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)</meeting>
		<imprint>
			<publisher>Istanbul</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="502" to="506" />
		</imprint>
	</monogr>
	<note>Turkey. European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Automated Analysis of Bangla Poetry for Classification and Poet Identification</title>
		<author>
			<persName><forename type="first">Geetanjali</forename><surname>Rakshit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anupam</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Natural Language Processing</title>
		<meeting>the 12th International Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Pushpak Bhattacharyya, and Gholamreza Haffari</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Sagor</forename><surname>Sarker</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2102.00405</idno>
		<title level="m">Bnlp: Natural language processing toolkit for Bengali language</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cross-lingual sentiment classification in low-resource Bengali language</title>
		<author>
			<persName><forename type="first">Salim</forename><surname>Sazzed</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.wnut-1.8</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)</title>
		<meeting>the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="50" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The OPUS Corpus -Parallel and Free</title>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Nygaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC&apos;04), Lisbon, Portugal</title>
		<meeting>the Fourth International Conference on Language Resources and Evaluation (LREC&apos;04), Lisbon, Portugal</meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA)</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A Double Metaphone encoding for Bangla and its application in spelling checker</title>
		<author>
			<persName><forename type="first">Naushad</forename><surname>Uzzaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Natural Language Processing and Knowledge Engineering</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="705" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshikiyo</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideto</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1609.08144</idno>
		<title level="m">Google&apos;s Neural Machine Translation system: Bridging the Gap between Human and Machine Translation</title>
		<editor>
			<persName><forename type="first">Alex</forename><surname>Riesa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Oriol</forename><surname>Rudnick</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Greg</forename><surname>Vinyals</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Macduff</forename><surname>Corrado</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jeffrey</forename><surname>Hughes</surname></persName>
		</editor>
		<editor>
			<persName><surname>Dean</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">1130</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books</title>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.11</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
	<note>Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
