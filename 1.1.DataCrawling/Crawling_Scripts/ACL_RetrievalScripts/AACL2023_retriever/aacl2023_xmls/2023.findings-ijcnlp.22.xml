<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks</title>
				<funder ref="#_wcpdx3v #_yNUsXJv">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tanmay</forename><surname>Chavan</surname></persName>
							<email>chavantanmay1402@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<country>L3Cube</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Omkar</forename><surname>Gokhale</surname></persName>
							<email>ogokhale3@gatech.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
								<address>
									<postCode>L3Cube 3</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aditya</forename><surname>Kane</surname></persName>
							<email>adityakane1@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
								<address>
									<postCode>L3Cube 3</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shantanu</forename><surname>Patankar</surname></persName>
							<email>spatankar34@gatech.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
								<address>
									<postCode>L3Cube 3</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raviraj</forename><surname>Joshi</surname></persName>
							<email>ravirajoshi@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Pune Institute of Computer Technology</orgName>
								<address>
									<postCode>L3Cube</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2C99CA8CD00F531833353430868572F3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The research on code-mixed data is limited due to the unavailability of dedicated code-mixed datasets and pre-trained language models. In this work, we focus on the low-resource Indian language Marathi which lacks any prior work in code-mixing. We present L3Cube-MeCorpus, a large code-mixed Marathi-English (Mr-En) corpus with 10 million social media sentences for pretraining. We also release L3Cube-MeBERT and MeRoBERTa, code-mixed BERT-based transformer models pre-trained on MeCorpus. Furthermore, for benchmarking, we present three supervised datasets MeHate, MeSent, and MeLID for downstream tasks like codemixed Mr-En hate speech detection, sentiment analysis, and language identification respectively. These evaluation datasets individually consist of manually annotated ~12,000 Marathi-English code-mixed tweets. Ablations show that the models trained on this novel corpus significantly outperform the existing state-of-the-art BERT models. This is the first work that presents artifacts for code-mixed Marathi research. All datasets and models are publicly released at https://github.com/ l3cube-pune/MarathiNLP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The modern world has been engulfed by the presence of social media platforms like Twitter and Facebook <ref type="bibr" target="#b15">(Salem and Mourtada, 2011)</ref>. Moreover, websites like YouTube have witnessed considerable user interaction in the comments section of videos <ref type="bibr" target="#b17">(Siersdorfer et al., 2010)</ref>. These posts and comments closely reflect the thoughts of the general public. It is common for users from different linguistic backgrounds to discuss social, political, and other topics over such social media. This leads to users using a mixed language for communicating over social media platforms.</p><p>Code-mixing is known as the mixing of words from multiple languages while retaining the script * First author, equal contribution of a single language. The Latin script is often used to encapsulate the terms of some languages. For example, a given text can be of the Marathi language written in Latin script, as opposed to the Devanagari script, which is the original script of the Marathi language <ref type="bibr">(Joshi, 2022a)</ref>. Code-mixed data is inherently difficult to process and analyze due to its linguistic complexity, variance in spelling and grammar, and long-tailed distribution of uncommon terms and phrases, which are often specific to the geography and demographics of the source location. It is observed that a large number of tweets, comments, and posts on social media are codemixed in nature. Thus, with the advent of social media analytics, effectively analyzing code-mixed data has gained the utmost importance.</p><p>Marathi is a language which has its origins in Maharashtra, a state in India. Due to the state's geographic and demographic expanse, Marathi has evolved into a language with multiple varieties and dialects. Recently, there has been some focus on Marathi NLP based on the Devanagari script <ref type="bibr">(Joshi, 2022b,a;</ref><ref type="bibr" target="#b9">Kulkarni et al., 2021;</ref><ref type="bibr" target="#b14">Patil et al., 2022;</ref><ref type="bibr" target="#b11">Litake et al., 2022)</ref>. However, a large chunk of tweets, posts, and comments in Marathi are in codemixed form. In spite of this, no efforts have been made to curate models and datasets pertaining to Marathi code-mixed data in the past. This work presents the following.</p><p>1. We release three supervised datasets (L3Cube-MeHate, MeLID, and MeSent) and one unsupervised dataset (L3Cube-MeCorpus)<ref type="foot" target="#foot_0">1</ref> . The unsupervised corpus comprises 5 million (70.9M tokens) Roman script code-mixed Marathi-English samples compiled from various sources. We further include 5M Devanagari sentences based on original text making it a 10 million (139.5M tokens) mixed-script MeCorpus.</p><p>2. The supervised dataset contains labels for code-mixed Marathi-English (Roman script) hate classification, sentiment detection, and language identification. These datasets were manually annotated by native Marathi speakers.</p><p>3. Finally, we release a plethora of codemixed MeBERT-based pre-trained and finetuned models for downstream tasks trained on these novel corpora. These models include MeBERT 2 , MeBERT-Mixed 3 , MeBERT-Mixed-v2 4 , MeRoBERTa-Mixed 5 , and MeRoBERTa 6 . The models suffixed as 'Mixed' were trained on full 10M MeCorpus while others were trained on 5M Roman MeCorpus. The supervised models include MeSent-RoBERTa 7 , MeHate-RoBERTa 8 , and MeLID-RoBERTa 9 . This work is a major milestone towards democratizing NLP for the Marathi language. Additionally, we present several ablations with fine-tuned models. This is the first work to present a large unsupervised corpus, multiple pre-trained models, and high-quality supervised datasets. This work is a strong foundation in the domain of Marathi and code-mixed Marathi NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>The use of regional scripts, such as Devanagari, Gurmukhi, Bengali, etc., presents a significant challenge in India due to keyboards primarily designed for the Roman script and the population's familiarity with it. The demand for code-mix datasets and models tailored to regional languages has increased exponentially. These resources play a crucial role in enabling enhanced analysis and moderation of social media content that is code-mixed.</p><p>In the realm of language models, BERT-based architectures <ref type="bibr" target="#b19">(Vaswani et al., 2017)</ref>, including variations such as RoBERTa <ref type="bibr" target="#b12">(Liu et al., 2019)</ref> and ALBERT <ref type="bibr" target="#b10">(Lan et al., 2019)</ref>, have gained popularity due to their application in pre-training and 2 MeBERT 3 MeBERT-Mixed 4 MeBERT-Mixed-v2 5 MeRoBERT-Mixed 6 MeRoBERTa 7 MeSent-RoBERTa 8 MeHate-RoBERTa 9 MeLID-RoBERTa fine-tuning on various tasks. Multilingual models like multilingual-BERT, XLM-RoBERTa <ref type="bibr" target="#b1">(Conneau et al., 2019)</ref>, and MuRIL <ref type="bibr" target="#b8">(Khanuja et al., 2021)</ref> have specifically focused on data representations that are multilingual and cross-lingual in nature, offering improvements in accuracy and latency. However, these models are pre-trained on less than a hundred thousand real code-mix texts.</p><p>While previous research efforts have addressed code mixing in other Indian languages, the specific domain of code-mix Marathi remains largely unexplored. Notably, there is a scarcity of prior work and an absence of a dedicated code-mix Marathi dataset. However, other Indian languages have seen some notable contributions. For instance, <ref type="bibr" target="#b4">Hande et al. (2020)</ref> presents KanCMD a code-mixed Kannada dataset for sentiment analysis and offensive language detection. Chakravarthi et al. ( <ref type="formula">2021</ref>) and have released datasets encompassing Tamil-English and Malayalam-English code-mixed texts. <ref type="bibr" target="#b13">Nayak and Joshi (2022)</ref> have made available Hing-Corpus, a Hindi-English code-mix dataset, and also open-sourced pre-trained models trained on codemix corpora. <ref type="bibr" target="#b18">Srivastava and Singh (2021)</ref> provide HinGE, a dataset for the generation and evaluation of code-mixed Hinglish text, and demonstrate techniques for algorithmically creating synthetic Hindi code-mixed texts.</p><p>In the realm of transliteration, there have been attempts to pre-train language models using transliterated texts. However, these models often underperform due to the rule-based nature of most transliteration techniques, which struggle to account for the diverse spelling variations present in real-life code-mixed texts <ref type="bibr" target="#b16">(Santy et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MeCorpus -Pretraining Data Creation</head><p>We introduce MeCorpus, a new pre-training corpus of 10 million code-mixed Marathi sentences. This consists of 5M Roman and 5M Devanagari sentences. These sentences are extracted from the social media platforms YouTube and Twitter. We also used synthetic data obtained by transliterating Tweets written in the Devanagari script. The complete data collection process is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Twitter data</head><p>A part of the pretraining corpus is obtained from the social networking site Twitter. We utilize snscrape, a scraper for social networking sites to scrape the data from Twitter. We use a keywordbased approach to curate the data. We use frequently used Marathi words as keywords and fetch all the tweets containing the given word. The list of seed keywords is generated by selecting a few common Marathi words and scraping tweets containing these words. Then we identify the most frequently occurring Marathi words in these tweets and add them to our list. The seed words are properly vetted before being added to the list. Proper care is taken to manually check that the word is predominantly exclusive to the Marathi language and doesn't occur in texts from other languages. We fetch a fixed number of tweets belonging to a certain keyword and discard the keyword if the tweets fail to satisfactorily meet the aforementioned conditions after manual verification. Otherwise, we scrape all the tweets containing the keyword and add them to our corpus. This manual verification process ensures that the curated data largely contains Marathi text. The Twitter data amounts to over a million tweets. All of the tweets at least partly contain code-mixed Marathi. A significant number of the tweets exhibit code-switching between Marathi and English. A small portion of the tweets also contain code-switched Hindi-Marathi text. We anonymize the data before using it for pre-training. The username mentions are replaced with the '@USER' text. We also remove links and hashtags from the tweets. The Twitter corpus contains 50M tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">YouTube data</head><p>YouTube comments are an excellent source of Code-mixed Marathi data. We scrape all the comments from 200 Marathi YouTube channels using the youtube-comments-scraper library. This gave us a mix of English, Devanagari, and codemixed Marathi sentences. We then removed the Devanagari and English comments to obtain the code-mixed Marathi data. This data was then preprocessed and used in our pre-training dataset. Devanagari words were identified and removed by checking their utf-8 encoding. We remove all comments that have more than 80% Devanagari words. This gives us comments that are either English or Marathi-English code-mixed. We used a fast text classifier to identify sentences that are English. We removed these sentences and were left with codemixed Marathi sentences. Thus at the end of both filtering steps, we are left with 2,278,097 of the original 7,599,588 comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Transliteration</head><p>We scraped around 1.7 million Marathi Devanagari tweets from Twitter using the snscrape library. We then used the indic-trans Python library to transliterate 1,685,233 of these Devanagari tweets to Codemixed Marathi and added them to our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Me Corpus -Transliteration</head><p>We created an additional 5 million Devanagari sentences and added them to the corpus. This was done by transliterating the Tweets and YouTube comments mentioned in sections 3.1 and 3.2 respectively to the Devanagari script. We also used the scraped Devanagari tweets mentioned in section 3.3. This gave us a total of 5M Devanagari sentences. These 5M sentences were used to pre-train the multilingual models mentioned in the future sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">MeEval -Downstream dataset creation</head><p>We aim to create a large dataset of code-mix Marathi-English data, annotated with sentiment and hatefulness. In this study, we selected a set of tweets from a larger corpus of 1,037,659 tweets obtained from the social media platform Twitter. Half of the tweets chosen were posted on Twitter before 2013, and the other half were posted after 2013. This helped to provide a more diachronic distribution of tweets, as the number of tweets posted in the past few years far outnumber the old tweets. The tweets were selected randomly apart from this criteria. This ensures a realistic representation of the sentiment, hate, and profanity distributions present in the real-world data across the past several years. For data annotation, we selected four annotators who are proficient in Marathi, Hindi, and English languages. All four annotators are native Marathi speakers and hold undergraduate-level proficiency in English. Any discrepancies discovered within the dataset were systematically resolved through collaborative consensus among the annotators. The collected data was labeled according to three distinct categories: sentiment, hate, and language identification. We followed a set of guidelines while labeling the data to ensure the veracity of the annotation. We annotated the data after anonymizing it. This helped remove any bias or knowledge of the entity posting it. We also disregarded any additional information that could be inferred by us based on external context but is not apparent by reading the text by itself. Here, we outline the dataset statistics and annotation procedure. The dataset statistics are described in Table <ref type="table">3</ref>. A few annotated examples are presented in Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MeSent Dataset</head><p>The code-mixed Marathi-English sentiment data is termed the MeSent Dataset. Tweets expressing good or heartening emotions such as thankfulness, happiness, applause, and appreciation are labeled positive. Tweets expressing negative or disheartening emotions like strong dissent, disappointment, sorrow, derision, and hate are labeled negative. Plain facts, statements, and simple responses are labeled neutral. If a tweet contains conflicting emotions, the stronger emotion is chosen.</p><p>• +1 indicating a positive sentiment,</p><p>• -1 indicating a negative sentiment, and • 0 indicating a neutral sentiment.</p><p>While annotating the data, we removed unsuitable and ambiguous tweets. Finally, we selected 4,000 tweets from each sentiment category, leading to the dataset containing a total of 12,000 tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">MeHate Dataset</head><p>For the hatefulness annotation, we labeled any tweets expressing strongly negative feelings such as insults, mockery, abuse, intimidation, and threats as hateful. Any tweet not containing such hateful content is labeled as non-hateful. We use 1 for hateful content and 0 for non-hateful content. The MeHate dataset contains 1384 hateful and 1384 non-hateful tweets, totaling 2768 tweets. We also release the full 12k labeled tweets with the majority of non-hate labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">MeLID dataset</head><p>Additionally, a Language Identification (LID) dataset is created. Each word within the selected tweets is labeled based on its language as Marathi, English, or Other. The Other category contains invalid words, words from languages other than English or Marathi, and literals such as numbers and proper nouns. The MeLID dataset contains 11,814 tweets. For all three supervised datasets, we provide a pre-defined train, test, and validation split of 80:10:10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Models trained on code-mixed MeCorpus</head><p>We train several well-known models on our novel pretraining corpora. In this section, we outline these models and their training details. We used pre-trained BERT, RoBERTa, mBERT, MuRIL, and XLM Roberta as the base models and trained them on the novel MeCorpus using the Masked Language Modelling (MLM) objective. For MLM training, we train the models for two ["Good" : ENG, "morning" : ENG, "sir" : ENG, "mast" : MAR, "tumhala" : MAR, "bhetayche" : MAR, "ahe" : MAR ] nalayak jalavu nakos amhala 1 (Hateful)</p><p>-1 (Negative)</p><p>["nalayak" : MAR, "jalavu" : MAR, "nakos" : MAR, "amhala" : MAR ] Ssc cha decision lavkar ghya 0 (Non-hateful) 0 (Neutral)</p><p>["Ssc" : OTH, "cha" : MAR, "decision" : ENG, "lavkar" : MAR, "ghya" : MAR ] epochs at a learning rate of 2e -5, with a weight decay of 0.01 and a mask probability of 0.15.</p><p>The monolingual models were pre-trained on the Roman 5M codemixed data mentioned in section 3. While the multilingual models were trained on the full 10M corpus (5M Roman + 5M Devanagari sentences) mentioned in section 4. The models pre-trained on mixed-script corpus are suffixed as 'Mixed'.</p><p>The resulting models were named similarly to the original models, prefixed with "me", which stands for Marathi-English. Therefore, the models MeBERT, MeBERT-Mixed, MeBERT-Mixed-v2, MeRoBERTa-Mixed, and MeRoBERTa are the BERT, mBERT, MuRIL, XLM-RoBERTa, and RoBERTa models trained on the MeCorpus respectively. Note that these models are further "finetuned" on the MeCorpus using the MLM training objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>We fine-tune our MeBERT models on the MeSent, MeHate, and MeLID datasets as mentioned in section 5 and test them on the respective test data. The same process is repeated for their base models and a few state-of-the-art Marathi models like Indic-BERT <ref type="bibr" target="#b7">(Kakwani et al., 2020)</ref>, Marathi-Tweets-BERT <ref type="bibr" target="#b3">(Gokhale et al., 2022)</ref>, and Marathi Codemixed Abusive MuRIL <ref type="bibr" target="#b2">(Das et al., 2022)</ref>. The results obtained from this are showcased in Table <ref type="table" target="#tab_1">2</ref>. It is observed that MeBERT-Mixed-v2 outperforms all other models on the MeHate evaluation set with an F1 score of 78.3%. For the sentiment analysis corpus MeSent, MeRoBERTa outperforms the others by obtaining an F1 score of 67.27%. Testing the models on the MeLID dataset, MeBERT-Mixed-v2 outperforms the other models by obtaining an F1 score of 88.6%. The newly pre-trained code-mixed MeBERT-based models consistently outperform their base models as well as the state-of-the-art Marathi models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>This work lays the necessary groundwork for future work on code-mixed Marathi. We introduce a novel pretraining corpus of 5 million code-mixed text examples. In addition to that, we present five new models trained on this code-mixed corpus. Furthermore, we present three supervised datasets of 12,000 tweets for hate classification, sentiment analysis, and language identification annotated by native Marathi speakers. We also present thorough ablations and show that our code-mixed MeBERT models outperform the previous state-of-the-art models by a considerable margin.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Dataset creation process for our 5 million code-mixed corpora. These 5 million examples are further transliterated to the Devanagari script, which results in a combined corpora of 10 million examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2:Table containing code-mixed examples with their annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Macro F1 scores (in %) of models on the MeHate, MeSent, and MeLID datasets.</figDesc><table><row><cell>Model</cell><cell cols="3">MeHate MeSent MeLID</cell></row><row><cell>Indic-BERT</cell><cell>61.62</cell><cell>55.29</cell><cell>87.37</cell></row><row><cell>MahaTweets-BERT</cell><cell>63.18</cell><cell>57.59</cell><cell>87.38</cell></row><row><cell>Abusive-MuRIL</cell><cell>67.69</cell><cell>-</cell><cell>-</cell></row><row><cell>BERT</cell><cell>61.98</cell><cell>59.06</cell><cell>87.89</cell></row><row><cell>MeBERT</cell><cell>73.78</cell><cell>61.92</cell><cell>88.01</cell></row><row><cell>mBERT</cell><cell>66.80</cell><cell>60.25</cell><cell>87.64</cell></row><row><cell>MeBERT-Mixed</cell><cell>77.39</cell><cell>65.73</cell><cell>88.25</cell></row><row><cell>MuRIL</cell><cell>67.93</cell><cell>63.38</cell><cell>87.55</cell></row><row><cell>MeBERT-Mixed-v2</cell><cell>78.3</cell><cell>64.23</cell><cell>88.6</cell></row><row><cell>XLM-RoBERTa</cell><cell>64.66</cell><cell>61.06</cell><cell>87.54</cell></row><row><cell>MeRoBERTa-Mixed</cell><cell>78.07</cell><cell>67.17</cell><cell>87.42</cell></row><row><cell>RoBERTa</cell><cell>66.10</cell><cell>58.86</cell><cell>86.46</cell></row><row><cell>MeRoBERTa</cell><cell>77.85</cell><cell>67.27</cell><cell>88.41</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>MarathiNLP</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was done under the <rs type="programName">L3Cube Pune mentorship program</rs>. We would like to express our gratitude towards our mentors at L3Cube for their continuous support and encouragement. This work is a part of the <rs type="projectName">L3Cube</rs>-<rs type="projectName">MahaNLP</rs> project (Joshi, 2022b).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_wcpdx3v">
					<orgName type="project" subtype="full">L3Cube</orgName>
					<orgName type="program" subtype="full">L3Cube Pune mentorship program</orgName>
				</org>
				<org type="funded-project" xml:id="_yNUsXJv">
					<orgName type="project" subtype="full">MahaNLP</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>A major problem while dealing with Romanized Marathi is the lack of a singular correct spelling of words. A Marathi word can be written in several ways in Marathi, all of which are equally valid and correctly convey meaning despite having significantly different spellings. Developing efficient approaches to tackle this issue will lead to a significant increase in performance on NLP tasks dealing with code-mixed languages. Our keywordbased scraping method uses words primarily from the western Maharashtra dialect of Marathi, which might not sufficiently represent samples from other Marathi dialects. Efforts to increase the dataset to include examples from other dialects will make the dataset more diverse and robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>All of the data used in our experiments has been scraped by legal and valid means, adhering to the provided guidelines. We anonymized the data before usage to protect the privacy of the original authors of the data. This data might contain biases and thus must be used with care. This data also contains strong language which might be unsuitable for some applications. This data should be used only for research purposes and not for training any model for deployment.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Overview of the HASOC-DravidianCodeMix Shared Task on Offensive Language Detection in Tamil and Malayalam</title>
		<author>
			<persName><forename type="first">Raja</forename><surname>Bharathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasanna</forename><surname>Chakravarthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ratnasingam</forename><surname>Kumar Kumaresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Sakuntharaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sajeetha</forename><surname>Kumar Madasamy</surname></persName>
		</author>
		<author>
			<persName><surname>Thavareesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Premjith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subalalitha</forename><surname>Chinnaudayar Navaneethakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Mccrae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Mandl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of FIRE 2021 -Forum for Information Retrieval Evaluation</title>
		<imprint>
			<publisher>CEUR</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02116</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Data bootstrapping approaches to improve low resource abusive language detection for indic languages</title>
		<author>
			<persName><forename type="first">Mithun</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somnath</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd ACM Conference on Hypertext and Social Media</title>
		<meeting>the 33rd ACM Conference on Hypertext and Social Media</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="32" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Spread love not hate: Undermining the importance of hateful pretraining for hate speech detection</title>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Gokhale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanmay</forename><surname>Chavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Patankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raviraj</forename><surname>Joshi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.04267</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kancmd: Kannada codemixed dataset for sentiment analysis and offensive language detection</title>
		<author>
			<persName><forename type="first">Adeep</forename><surname>Hande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruba</forename><surname>Priyadharshini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharathi</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chakravarthi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Computational Modeling of People&apos;s Opinions, Personality, and Emotion&apos;s in Social Media</title>
		<meeting>the Third Workshop on Computational Modeling of People&apos;s Opinions, Personality, and Emotion&apos;s in Social Media</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="54" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">2022a. L3cube-mahacorpus and mahabert: Marathi monolingual corpus, marathi bert language models, and resources</title>
		<author>
			<persName><forename type="first">Raviraj</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the WILDRE-6 Workshop within the 13th Language Resources and Evaluation Conference</title>
		<meeting>the WILDRE-6 Workshop within the 13th Language Resources and Evaluation Conference</meeting>
		<imprint>
			<biblScope unit="page" from="97" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Raviraj</forename><surname>Joshi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.14728</idno>
		<title level="m">Marathi natural language processing datasets, models, and library</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="cube" to="mahanlp" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Indicnlpsuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for indian languages</title>
		<author>
			<persName><forename type="first">Divyanshu</forename><surname>Kakwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Kunchukuttan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satish</forename><surname>Golla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avik</forename><surname>Gokul</surname></persName>
		</author>
		<author>
			<persName><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyush</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4948" to="4961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Simran</forename><surname>Khanuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diksha</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarvesh</forename><surname>Mehtani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Savya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atreyee</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Gopalan</surname></persName>
		</author>
		<title level="m">Muril: Multilingual representations for indian languages</title>
		<editor>
			<persName><forename type="first">Dilip</forename><surname>Kumar Margam</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pooja</forename><surname>Aggarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rajiv</forename><surname>Teja Nagipogu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shachi</forename><surname>Dave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shruti</forename><surname>Gupta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Subhash</forename><surname>Chandra Bose</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vish</forename><surname>Gali</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Partha</forename><surname>Subramanian</surname></persName>
		</editor>
		<editor>
			<persName><surname>Talukdar</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">L3cubemahasent: A marathi tweet-based sentiment analysis dataset</title>
		<author>
			<persName><forename type="first">Atharva</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meet</forename><surname>Mandhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manali</forename><surname>Likhitkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gayatri</forename><surname>Kshirsagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raviraj</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
		<meeting>the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="213" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">L3cube-mahaner: A marathi named entity recognition dataset and bert models</title>
		<author>
			<persName><forename type="first">Onkar</forename><surname>Litake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravindra</forename><surname>Maithili</surname></persName>
		</author>
		<author>
			<persName><surname>Sabane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the WILDRE-6 Workshop within the 13th Language Resources and Evaluation Conference</title>
		<meeting>the WILDRE-6 Workshop within the 13th Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="29" to="34" />
		</imprint>
	</monogr>
	<note>Parth Sachin Patil, Aparna Abhijeet Ranade, and Raviraj Joshi</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Ravindra</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raviraj</forename><surname>Joshi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.08398</idno>
		<title level="m">L3cubehingcorpus and hingbert: A code mixed hindi-english dataset and bert language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">L3cube-mahahate: A tweet-based marathi hate speech detection dataset and bert models</title>
		<author>
			<persName><forename type="first">Hrushikesh</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Velankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raviraj</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Threat, Aggression and Cyberbullying (TRAC 2022)</title>
		<meeting>the Third Workshop on Threat, Aggression and Cyberbullying (TRAC 2022)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Civil movements: The impact of facebook and twitter</title>
		<author>
			<persName><forename type="first">Fadi</forename><surname>Salem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Racha</forename><surname>Mourtada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arab Social Media Report</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BERTologiCoMix: How does codemixing interact with multilingual BERT?</title>
		<author>
			<persName><forename type="first">Sebastin</forename><surname>Santy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monojit</forename><surname>Choudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Domain Adaptation for NLP</title>
		<meeting>the Second Workshop on Domain Adaptation for NLP<address><addrLine>Kyiv, Ukraine</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="111" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">How useful are your comments? analyzing and predicting youtube comments and comment ratings</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Siersdorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergiu</forename><surname>Chelaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Nejdl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename></persName>
		</author>
		<idno type="DOI">10.1145/1772690.1772781</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hinge: A dataset for generation and evaluation of code-mixed hinglish text</title>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems</title>
		<meeting>the 2nd Workshop on Evaluation and Comparison of NLP Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="200" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><surname>Appendix</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
