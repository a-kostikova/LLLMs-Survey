<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Target-Aware Contextual Political Bias Detection in News</title>
				<funder ref="#_MSwMS3q">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Iffat</forename><surname>Maab</surname></persName>
							<email>iffatmaab@weblab.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Edison</forename><surname>Marrese-Taylor</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Advanced Industrial Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
							<email>matsuo@weblab.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Target-Aware Contextual Political Bias Detection in News</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">536652C3DBA70D403F4399F1EC6DE90E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Media bias detection requires comprehensive integration of information derived from multiple news sources. Sentence-level political bias detection in news is no exception, and has proven to be a challenging task that requires an understanding of bias in consideration of the context. Inspired by the fact that humans exhibit varying degrees of writing styles, resulting in a diverse range of statements with different local and global contexts, previous work in media bias detection has proposed augmentation techniques to exploit this fact. Despite their success, we observe that these techniques introduce noise by over-generalizing bias context boundaries, which hinders performance. To alleviate this issue, we propose techniques to more carefully search for context using a bias-sensitive, target-aware approach for data augmentation. Comprehensive experiments on the well-known BASIL dataset show that when combined with pre-trained models such as BERT, our augmentation techniques lead to state-of-the-art results. Our approach outperforms previous methods significantly, obtaining an F1-score of 58.15 over state-of-the-art bias detection task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>News media companies publish thousands of articles every day. While we generally regard these articles as containing factual, true information, studies have shown that various kinds of bias exist in news <ref type="bibr" target="#b11">(Fan et al., 2019;</ref><ref type="bibr" target="#b19">Lim et al., 2020;</ref><ref type="bibr" target="#b12">Gentzkow et al., 2015;</ref><ref type="bibr">Prat and Str√∂mberg, 2013)</ref>. Further studies have studied the effects that these biases have on readers, particularly in voting. A study by <ref type="bibr" target="#b13">Groseclose and Milyo (2005)</ref> suggests that indeed media has a sizable political impact on voting, where for example <ref type="bibr" target="#b7">DellaVigna and Kaplan (2007)</ref> found significant effect of exposure to Fox News in increased turnout to the polls.</p><p>Clearly, biased media have the potential to sway readers in potentially detrimental paths. Therefore, it is crucial to unveil the true nature of media bias. Furthermore, as all journalism contains narratives (Unesco, 2018), given its role on transforming individual and public opinion, we consider it is worth measuring and understanding the political bias phenomenon. We think bias detection is important as a proxy or mechanism to assess the quality of information in news media. As stated by <ref type="bibr">Unesco (2018)</ref>, there is no problem with the existence of narratives in substandard journalism, rather poor professionalism.</p><p>Bias in news from different aspects has been studied in the past, where for example <ref type="bibr" target="#b4">Chen et al. (2018)</ref> and <ref type="bibr" target="#b1">Arapakis et al. (2016)</ref> created news quality corpus of 561 articles and study how various news constituents characterize the quality of editorial articles. While these works are highly relevant to the bias problem, they did not specifically or directly target at the issue.</p><p>Foundational work in political bias was performed by <ref type="bibr" target="#b11">Fan et al. (2019)</ref>, who released a humanannotated dataset named Bias Annotation Spans on the Informational Level (BASIL), containing 300 fine-grained bias annotations. Concretely, political bias is identified at the sentence-level, where spans are annotated and a target (the main entity) is identified, in addition to a few other labels. Significantly, BASIL stands as the first dataset to be annotated with different types of bias. Informational bias, which depends broadly on the context of the sentence <ref type="bibr">(Guo and Zhu, 2022a)</ref> and arises from manipulation of information or selective presentation of content in a factual way, e.g., use of quotes, to evoke specific reader's emotions towards news entities <ref type="bibr" target="#b11">(Fan et al., 2019;</ref><ref type="bibr">van den Berg and Markert, 2020)</ref>, and lexical bias, which stems from the choice of specific words or linguistic phrases that influence the interpretation of a subject, and perpetuate the understanding of information <ref type="bibr">(Re-casens et al., 2013;</ref><ref type="bibr">Iyyer et al., 2014;</ref><ref type="bibr" target="#b16">Hube and Fetahu, 2019)</ref> are present in BASIL. To the best of our knowledge, BASIL is the first dataset that annotates informational bias together with specific targets.</p><p>With the release of BASIL, work on political bias detection has mostly focused on informational bias, with a strong emphasis on informational context within and across news media articles, as informational bias is highly content-dependent. In the seminal work, van den Berg and Markert (2020) feed the whole document/article as context for sentencelevel bias classification. Though this approach worked relatively well in practice, using long documents in this context brings considerable noise, redundancy and can increase vocabulary size, which can ultimately decrease the performance of the classifier as evidenced by previous work <ref type="bibr" target="#b0">(Akhter et al., 2020;</ref><ref type="bibr">Guo and Zhu, 2022b)</ref>. Moreover, as shown by <ref type="bibr" target="#b3">Chen et al. (2020)</ref>, detecting bias at article level remains even more challenging and difficult task.</p><p>In light of this issue, several works have recently focused on introducing more specific contextual information to perform classification <ref type="bibr">(Cohan et al., 2019b;</ref><ref type="bibr">van den Berg and Markert, 2020;</ref><ref type="bibr">Guo and Zhu, 2022b)</ref>, for example by mixing contexts of informational and lexical bias at both the articlelevel (entire article encompassing target sentence) and event-level (triplet of articles discussing the same event).</p><p>While the aforementioned approaches have resulted in improved performance, we think their applicability is limited. On one hand, articles in BASIL have no overall bias label, instead each sentence is labeled as evidence of a certain kind of bias or as a neutral statement, suggesting that these should be treated separately when detecting different kinds of bias. Previous studies <ref type="bibr" target="#b24">(Rao et al., 2018;</ref><ref type="bibr" target="#b26">Tripathy et al., 2017)</ref> have already shown that on documentlevel classification, paragraphs can belong to multiple categories, which <ref type="bibr" target="#b3">Chen et al. (2020)</ref>, also observed on BASIL, where paragraphs belong to either informational bias, lexical bias or no bias spans. Furthermore, as highlighted by <ref type="bibr" target="#b3">Chen et al. (2020)</ref>, by mixing contexts of informational and lexical bias, it becomes difficult for the model to distinguish and predict different type of bias, which may result in lower model performance.</p><p>In light of this issue, in this work, we provide a framework to generate more consistent and similar bias contexts to improve performance. As shown in Table <ref type="table">1</ref>, each instance of annotated bias span also identifies the "target", i.e., the main entity or topic of the sentence that is also annotated in BASIL. Using this information, our key insight is to create event-level contexts that are target-aware and also sensitive to the bias label.</p><p>For example, for the target "Obama Campaign", sentences from three different news sources are combined to form a single contextual example for informational bias classification, as highlighted in light gray. A similar procedure is applied for "Romney Campaign", where sentences are concatenated to form an example for lexical bias classification, highlighted in dark gray. Inspired by ideas from modeling context in informational bias detection (van den <ref type="bibr">Berg and Markert, 2020;</ref><ref type="bibr" target="#b3">Chen et al., 2020;</ref><ref type="bibr">Guo and Zhu, 2022b)</ref>, our approach is able to augment examples with richer contexts and less noise, and follows previous work in determining that the detection of lexical bias should hold equal importance as informational bias <ref type="bibr">(Zhou and Bansal, 2020;</ref><ref type="bibr" target="#b21">Marinov and Efremov, 2019;</ref><ref type="bibr" target="#b20">Maab et al., 2023)</ref>.</p><p>Following recent work <ref type="bibr" target="#b20">(Maab et al., 2023)</ref>, we tackle a variety of bias detection tasks including INF/OTH and INF/LEX using data from BASIL.</p><p>Through extensive experimentation, we demonstrate the effectiveness of our approach by obtaining state-of-the-art performance on all of our studied tasks. In addition, our holistic view on bias enables us to unveil inconsistent terminologies used for contextual information of BASIL, therefore we gather such contexts to improve clarity and uniformity, and to avoid previous work problems as indicated in our comparison with the state-of-theart.</p><p>2 Related Work  <ref type="formula">2020</ref>) find that finetuned BERT has a strong efficacy and use it to reimplement <ref type="bibr" target="#b11">(Fan et al., 2019)</ref> results. In light of the findings, our proposed approach also utilize BERT <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref>.</p><p>3 Proposed Approach</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bias-Aware Neighborhood Context</head><p>Previous work has shown that phrases surrounding a sentence annotated with bias can be used as local context to perform bias classification, and that this local context can contribute to the ability of models to identify and label types of bias. However, by ignoring the nature of these sentences, existing approaches that utilize neighborhood context (van den Berg and Markert, 2020; Guo and Zhu, 2022b) can run into problems by introducing ambiguous content, for example when adding sentences that are annotated with the opposite bias. As shown by (van den <ref type="bibr">Berg and Markert, 2020)</ref>, this can also lead to massive data leakage problems across train and test sets.</p><p>To account for the disparity in how different bias contexts are overlooked in previous work, in this paper, we propose to care for the bias label of neighboring sentences, advancing to generate Bias-Aware Neighborhood Contexts (BANC), and adding neighboring sentences to the model input as long as they have a related bias label. Table <ref type="table" target="#tab_2">2</ref> shows an example of how this procedure works. Since, our approach is bias-sensitive, sentences with informational and lexical bias are treated separately. Therefore, for a given target sentence with index 1, the former (index 0) and next (index 2) sentences become neighbor sentences of lexical bias as they exhibit no bias span as highlighted in green. Correspondingly, to generate a BANC for informational bias classification, we combine sentences with indices 2, 3 and 4 as highlighted in blue. Teal (green + blue) color is shown by sentence index 2, since it is common between the two BANC text spans. According to the same principle, for cases where the first sentence of an article has bias, next sentence is checked and combined, whereas in the event where it is last sentence, former sentence gets checked and successively combined. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Target-Aware Context</head><p>While our neighboring approach helps identify local context relevant for bias classification, we believe that global context, either at the article or event levels, can also be exploited. To that end, we note that BASIL contains annotations that also identify the "target" of a given sentence where either lexical or informational bias is present. This "target" label refers to the main entity or topic of the sentence that is annotated, with some of the most prominent targets in BASIL being entities or people that lie at the core of news reports, such as Donald Trump, Romney Campaign, Secure America Now, among others.</p><p>We further note that although the frequency of appearance of a given "target" varies substantially, as long as we keep the annotated label constant (e.g., lexical), the context remains the same. This motivated us to gather all surrounding linguistic cues pertaining to a specific target at both the article-level and event-level. Concretely, we create targetaware contextual information by making use of all possible combination spans having the same bias and target, and propose article-based target-aware (ABTA) and event-based target-aware (EBTA) contexts, which we explain below.</p><p>As show in Table <ref type="table">3</ref>, using ABTA context, for instance, the target "Barack Obama" which has 5 sentences annotated with informational bias in the FOX article and 1 in HPO, generates all possible combinations of two sentences within FOX giving us 10 contextualized examples, and 1 same example in HPO because this article has only one sentence, respectively. Note that possible combinations of sentences within articles are combined in groups of two only, which we do to emulate the natural distribution of occurrence of sentences with the same bias and same "target".</p><p>EBTA contexts shown in the "Event-level" column in Table <ref type="table">3</ref>, are computed for common targets across articles, for instance, the same target "Barack Obama" with informational bias appear across FOX and HPO with 5 and 1 sentences, therefore all unique possible combinations in groups of two generates 5 new contextualized examples across the two aforementioned articles. Finally, following the example in the table for "Barack Obama", the combined contexts of ABTA and EBTA give us a total of 10+1+5 = 16 contextualized informational bias examples for a single target.</p><p>Note that we repeat this procedure for generating target-aware lexical bias contexts.</p><p>Because of the way in which we combine sentences, it is evident that our approach is significant in providing contextualized examples for infrequent targets as well, therefore also contributing towards mitigating imbalanced bias distribution and skewed nature of "targets" as observed in BASIL articles <ref type="bibr" target="#b3">(Chen et al., 2020)</ref>.  tees that the model is not relying solely on shallow lexical features of a complete article as in previous work (van den Berg and Markert, 2020; Guo and Zhu, 2022b), and instead looking for cues on the same categories or bias type <ref type="bibr" target="#b24">(Rao et al., 2018;</ref><ref type="bibr" target="#b26">Tripathy et al., 2017)</ref> relevant for the task at hand.</p><p>Since, prior studies focused solely on informational bias and overlooked other bias spans, we surmise that lexical bias detection is also significant as supported by <ref type="bibr" target="#b20">Maab et al. (2023)</ref>, and provide a more concise and sensitive bias narratives with neighborhood contexts together with target-aware contexts.</p><p>Finally, based on successful results reported by previous work <ref type="bibr" target="#b22">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b20">Maab et al., 2023)</ref>, we additionally use a backtranslation approach to generate more data, which we apply to our contextualized samples using Spanish as a pivot language. By incorporating multiple viewpoints in our neighborhood and target-aware contexts, we facilitate our model in providing a broad and inherent semantics of biased targets to manifest variations in bias representations. Our extensive experiments will further demonstrate the impact of proposed context in different training settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>To streamline the comparison with prior work (van den Berg and Markert, 2020; Guo and Zhu, 2022b), we use a 10-fold cross-validation setting where bias-aware neighborhood and event-based target aware contexts never appear at the same time in non-overlapping train-val-test split sets of 80-10-10, respectively. Average performance of our model using three seed runs is reported in all our experiments.</p><p>For the sentence-level bias detection, we perform Baselines Majority of approaches in previous studies concentrate on deep learning methods for identifying media bias. We compare our work with models that use different kinds of BASIL contexts for sentence-level bias detection to ensure consistent and impartial evaluation. We consider multiple contextual models that address the detection of informational bias, for example, SSC (Sequential Sentence Classification) <ref type="bibr">Cohan et al. (2019a)</ref> and its variant WinSCC (windowed Sequential Sentence Classification) (van den Berg and Markert, 2020), RoBERTa, ArtCIM for target sentences within an article, and EvCIM for triplets of articles covering the same event (van den Berg and Markert, 2020; <ref type="bibr">Guo and Zhu, 2022b)</ref>. <ref type="bibr">Guo and Zhu (2022b)</ref> further proposed MultiCTX model and reproduce the results using WinSCC and EvCIM for informational bias detection. We also compare against the fine-tuned RoBERTa model <ref type="bibr" target="#b18">(Lei et al., 2022)</ref>, as well as BERT <ref type="bibr" target="#b20">(Maab et al., 2023;</ref><ref type="bibr" target="#b3">Chen et al., 2020;</ref><ref type="bibr" target="#b8">Devlin et al., 2018;</ref><ref type="bibr" target="#b11">Fan et al., 2019)</ref>.</p><p>Implementation Details We use the PyTorch to implement our models, borrowing from Hug-gingFace (Face, 2021), our classifiers are based on BERT-base <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref>, and all our models are trained with 5 √ó 10 -5 as learning rate, 32 as batch size, and 15 as a maximum epoch count.</p><p>We utilize a server with an NVIDIA V-100 GPU for our experiments. 5 Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Study</head><p>To show the effectiveness of our proposed techniques, we rely on both INF/OTH and INF/LEX tasks. For BANC, (ABTA and EBTA), we vary the percentage of augmented data that is added to the training, and compare against the "regular" setting. Table <ref type="table" target="#tab_7">5</ref> and Figure <ref type="figure" target="#fig_0">1</ref> shows a summary of our obtained results. Overall, we observe that with the increase in size of context-augmented samples for both neighborhood and target-aware context, the model yields improvements in F1-scores and accuracy of both bias tasks. Furthermore, we see that by only using BANC, we can achieve substantial performance improvements, regardless of the fact that this technique neglects event information.</p><p>Owing to the fact that target-  When we combine our neighborhood augmentation technique (BANC) and target-aware article-based and event-based contexts (ABTA and EBTA) as our final model, it is observed that the performance begin excelling against the regular even when 40% of the combined context-augmented examples are fed to the model. Our results further demonstrate the effectiveness of backtranslation-based augmentation technique on BASIL, following the findings of <ref type="bibr" target="#b20">Maab et al. (2023)</ref>, and showing that this technique can be combined with our proposed components to attain further performance improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with Prior Work</head><p>Having established the efficacy of our proposed approach, we now proceed to compare our model with previous studies. Concretely, we can only compare our work against one studied INF/OTH bias task of BASIL using contextual information as indicated by prior work <ref type="bibr" target="#b20">(Maab et al., 2023)</ref>, therefore we solely present our work pertaining to this task with state-of-the-art.</p><p>Based on our comprehensive analysis on how prior studies use different contexts on BASIL, we align similar contexts of our proposed method to allow meaningful comparisons as shown in the Table <ref type="table">6</ref>, using three corresponding sections.</p><p>To compare with previous work where only within article context is used, we concretely utilize our top performing models for comparison, i.e., BERT combined with 100% BANC (BERT + BANC), and with backtranslation (BERT + BANC + BT). Similarly, prior work using event contexts are compared with our BERT model trained on 100% target-aware (BERT + ABTA + EBTA), and with backtranslation (BERT + ABTA + EBTA + BT), respectively. Since MultiCTX by <ref type="bibr">Guo and Zhu (2022b)</ref> uses multi-contrast learning of both article and event contexts, we compare and use our best BERT model with fusion of both proposed context techniques (BERT + BANC + ABTA + EBTA), and with backtranslation (BERT + BANC + ABTA + EBTA + BT), which in essence is our final model. Based on our results, and supporting findings of our ablation study, both BANC and target-aware (ABTA &amp; EBTA) hold significance in our approach, however target-aware contexts contributes more than BANC parallel to previous findings <ref type="bibr">(Guo and Zhu, 2022b)</ref>. Our approach outperforms previous work significantly, obtaining an F1-score of 58.15 in INF label.</p><p>In summary, our results show that our proposed approach leads to state-of-the-art results, offering compelling empirical evidence suggesting that adding multiple contextual information is effective at recognizing sentence-level informational and lexical bias as a type of misinformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Role of "target" frequency</head><p>To confirm the effectiveness of using target-aware (ABTA &amp; EBTA) contexts, we conduct a study on most frequent bias targets of BASIL, and consequently experiment with BERT <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref>, which serves as a baseline model for recent studies (van den Berg and Markert, 2020; <ref type="bibr">Guo and Zhu, 2022b;</ref><ref type="bibr" target="#b3">Chen et al., 2020)</ref>. From Table <ref type="table" target="#tab_4">4</ref>, we see that the "target" "Donald Trump" appears as the most attracted and significant media entity with substantial coverage of informational and lexical bias sentences. Out of 6,538 total target-aware contexts that we create, we found that around 42% (2,767) of them come from this target. In light of this issue, we are interested in studying the effect of target frequency in the creation of richer context, and propose an ablation analysis to gain insight.</p><p>We begin by first introducing target-aware contexts of only "Donald Trump" in various fractions to the regular setting, again for the INF/LEX task. We compare the contribution of the most frequent target towards performance by testing models trained solely on this data, and compare to models trained on the entire target-aware contexts.</p><p>As shown in Figure <ref type="figure" target="#fig_1">2</ref>, our results are consistent with the performance rise of LEX-F1 scores after 50% data using all targets, no significant performance change is observed until 50% of Donald  from 56.7 to 58.02 is more prominent in 100% with BT. In addition to the non-overlapping trainval-test, for this study we carefully choose testing examples so that the majority of targets have an equal %age in the test set to avoid the problems of overfitting the same target.</p><p>In addition to Donald Trump in INF/OTH task, we also introduce the second most frequent target "Barack Obama*", and the fusion of the two as shown in Figure <ref type="figure" target="#fig_2">3</ref>. Consistent with our findings, our approach works well for even a single target like "Donald Trump" having approximately not far from half target-aware contextualized examples towards total. Following prior work <ref type="bibr" target="#b20">(Maab et al., 2023)</ref> Trump". This study confirms the general nature of our approach in detecting different types of bias, since it is not uncommon in real world scenarios to run into similar and parallel target entities as reported by various media outlets <ref type="bibr" target="#b1">(Arapakis et al., 2016;</ref><ref type="bibr" target="#b19">Lim et al., 2020)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We study a challenging and significant task of detecting misinformation and shed light on bias prevalence in news media. Our work focuses on incorporating bias sensitive (BANC) and target-aware contexts (ABTA &amp; EBTA) for sentence-level bias detection tasks. Our proposed approach exploits the distinct influence of informational and lexical bias in news media writing styles, emulating the principle of human learning. Our model encompass the process by which individuals acquire new knowledge in real-world settings, i.e., gathering the associated type of bias from common news media targets covering the same event coupled with past experiences, and subsequently utilizing such contexts to make predictions about unfamiliar aspects.</p><p>Our model concretely outperforms classification performance of strong baselines in all bias tasks and we provide statistical significance of our proposed components through extensive experiments.</p><p>We find that the best performance is achieved when target-aware contexts are combined with BANC, and our methodological stand-point in using smallaugmented data of frequent targets suggests that our model is better at recognising bias in mass media. In addition, we conclude its important to keep different bias separately for accurate prediction of bias and we intend to explore other bias features as part of future work. Consequently, future work could also extend contextual information to other misinformation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Bias can vary based on human perspectives and existing NLP models have limitations to interpret the subjective nature of bias. Due to the lack of bias representations and annotated media coverage in other languages, our work is based only on English news articles. To the best of our knowledge, BASIL is the only dataset annotated with informational bias, and although our approach provides valuable insights and findings on detecting bias, we provide no evidence to suggest the significance of our findings regarding other contexts surrounding bias or misinformation detection tasks. Similarly, due to the disproportionate number of political ideologies in our dataset, we cannot say for sure if our model will perform equally well for other tasks, and we believe this requires further analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations</head><p>In this work, since we highlight some frequent bias targets in political news to propose the significance of our approach, we do not intend to promote media bias entities rather we advocate media literacy and ethical journalism practices. Further, the results we reported in our work highlight deeper understanding of bias contexts, and the need for bias mitigation at various levels of the mass media.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Plot showings various fractions of augmented contexts using BANC (neighbourhood), ABTA &amp; EBTA (target-aware), and integration of multiple contexts (fusion = BERT + BANC + ABTA + EBTA) to examine the effect of INF-F1 score (blue) in INF/OTH task, and INF-F1 (green) &amp; LEX-F1 (red) in INF/LEX task.</figDesc><graphic coords="6,70.87,70.87,218.27,163.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: In INF/LEX task, plot showing comparison of performance on most frequent target "Donald Trump" v/s. All Targets-aware context (BERT + ABTA + EBTA) using INF/LEX bias task.</figDesc><graphic coords="8,70.87,70.87,226.78,169.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: In INF/OTH task, plot showing comparison of F1-scrore and accuracy on target "Donald Trump", "Barack Obama*", and "Donald Trump + Barack Obama*" combined Vs. All Targets-aware context (BERT + ABTA + EBTA)</figDesc><graphic coords="8,306.14,70.87,218.27,163.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>An article of New York Times section extracted from BASIL showing bias-aware neighborhood context of informational bias in green and lexical in blue.</figDesc><table><row><cell cols="4">Index Position Sentence</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Bias</cell></row><row><cell>0</cell><cell cols="8">Neighbor Israel and Middle East policy have a ten-</cell><cell>-</cell></row><row><cell></cell><cell cols="8">dency of surfacing in presidential poli-</cell><cell></cell></row><row><cell></cell><cell cols="7">tics in rather combustible ways.</cell><cell></cell><cell></cell></row><row><cell>1</cell><cell cols="8">Target And a new advertisement that will run in</cell><cell cols="2">LEX</cell></row><row><cell></cell><cell cols="8">areas of Florida with large Jewish pop-</cell><cell></cell></row><row><cell></cell><cell cols="8">ulations attempts to stoke anxiety over</cell><cell></cell></row><row><cell></cell><cell cols="8">American policies in the region, using a</cell><cell></cell></row><row><cell></cell><cell cols="8">news clip of Prime Minister Benjamin</cell><cell></cell></row><row><cell></cell><cell cols="8">Netanyahu of Israel warning of the risks</cell><cell></cell></row><row><cell></cell><cell cols="4">of a nuclear Iran.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell cols="8">Neighbor The fact is that every day that passes,</cell><cell cols="2">-</cell></row><row><cell></cell><cell cols="8">Iran gets closer and closer to nuclear</cell><cell></cell></row><row><cell></cell><cell cols="8">arms, Mr. Netanyahu is shown saying.</cell><cell></cell></row><row><cell>3</cell><cell cols="8">Target For dramatic effect, a soundtrack fit for</cell><cell cols="2">INF</cell></row><row><cell></cell><cell cols="8">an episodic drama like Homeland plays</cell><cell></cell></row><row><cell></cell><cell cols="7">as the prime minister continues.</cell><cell></cell><cell></cell></row><row><cell>4</cell><cell cols="8">Neighbor The world tells Israel, Wait. There's still</cell><cell>-</cell></row><row><cell></cell><cell cols="2">time.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5</cell><cell cols="6">Neighbor And I say wait for what?</cell><cell></cell><cell></cell><cell>-</cell></row><row><cell>6</cell><cell cols="4">Neighbor Wait until when?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">Sentences</cell><cell></cell><cell></cell><cell cols="3">Target-aware examples</cell><cell></cell></row><row><cell></cell><cell>Target</cell><cell cols="6">Article-level FOX HPO NYT FOX HPO NYT</cell><cell cols="3">Event-level Total</cell></row><row><cell></cell><cell>Benjamin Netanyahu</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>1</cell></row><row><cell>18</cell><cell>Barack Obama</cell><cell>5</cell><cell>1</cell><cell>-</cell><cell>10</cell><cell>1</cell><cell>-</cell><cell cols="2">5 (fox √ó hpo)</cell><cell>16</cell></row><row><cell></cell><cell>Secure America Now</cell><cell>-</cell><cell>2</cell><cell>2</cell><cell>-</cell><cell>1</cell><cell>1</cell><cell cols="2">4 (hpo √ó nyt)</cell><cell>6</cell></row><row><cell>Total</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">within Art. = 14</cell><cell>9</cell><cell></cell><cell>23</cell></row><row><cell></cell><cell>Hillary Clinton</cell><cell>5</cell><cell>-</cell><cell>3</cell><cell>10</cell><cell>-</cell><cell>3</cell><cell cols="3">15 (fox √ó nyt) 28</cell></row><row><cell>22</cell><cell>Barack Obama</cell><cell>2</cell><cell>2</cell><cell>-</cell><cell>1</cell><cell>1</cell><cell>-</cell><cell cols="2">4 (fox √ó hpo)</cell><cell>6</cell></row><row><cell></cell><cell>Nancy Pelosi</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>1</cell></row><row><cell>Total</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">within Art. = 16</cell><cell>19</cell><cell></cell><cell>35</cell></row><row><cell cols="11">Table 3: Detail of the number of contextualized in-</cell></row><row><cell cols="11">stances obtained by applying our proposed ABTA and</cell></row><row><cell cols="11">EBTA to a set of the original examples from BASIL, in</cell></row><row><cell cols="11">this case taken from events (E) 18 and 22, for the case</cell></row><row><cell cols="3">of informational bias.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Most frequent bias targets in BASIL across events and their possible combinations using targetaware context. Barack Obama* includes three similar targets: Barack Obama, Obama's administration, Sasha and Malia Obama with 119, 21, and 16 bias sentences.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell>than INF-F1 highlighted in green after 50% con-</cell></row><row><cell>text, because number of lexical bias contexts are</cell></row><row><cell>partially comparable to informational bias contexts,</cell></row><row><cell>whereas in INF/OTH task the informational bias</cell></row><row><cell>contexts are still reasonably lower than OTH (non</cell></row><row><cell>bias + lexical samples). Similarly, since backtrans-</cell></row><row><cell>lation is only performed on lexical bias contexts in</cell></row><row><cell>INF/LEX task, LEX-F1 scores are more amplified</cell></row><row><cell>than INF F1-scores.</cell></row></table><note><p><p><p>Results of our ablation studies, in terms of accuracy and micro-F1 scores, when varying the amount (as percentage) of contextualized examples obtained with ABTA and EBTA that are added to the training data, where BT stands for the backtranslation augmentation approach from</p><ref type="bibr" target="#b20">(Maab et al., 2023)</ref></p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Your actions both past and present are incompatible with your duty as Chairman of this Committee, the letter stated. We have no faith in your ability to discharge your duties in a manner consistent with your Constitutional responsibility and urge your immediate resignation as Chairman of this Committee.The letter follows the conclusion of Special Counsel Robert Mueller's Russia probe, which turned up no evidence of collusion between Trump campaign members and Russia during the 2016 presidential election. It doesn't appear that was any part of [special counsel Robert] Mueller's report. In a letter dated Thursday, the GOP committee members accused Schiff of standing at the center of a well-orchestrated media campaign about a possible Trump-Russia connection. Democrats will stop at nothing to ruin his presidency, and bristle at Democrats accusing them of turning a blind eye to the Russian threat. And at the center of their wrath is Mr. Schiff, whose doughy-faced demeanor hardly evokes an attack dog. The findings of the special counsel conclusively refute your past and present assertions and have exposed you as having abused your position to knowingly promote false information, having damaged the integrity of this committee, and undermined faith in U.S. government institutions, Representative K. Michael Conaway, Republican of Texas, said to Mr. Schiff. had spoken out against the military's former "don't ask, don't tell" policy. According to The Hill, Cheney sought to clarify her position after an alleged poll in Wyoming said she "supports abortion and aggressively promotes gay marriage. Her opposition also puts her at odds with her father, who offered support for gay marriage in 2009. That position deferring to the will of the voters on a state-by-state basis may represent something of a compromise between total support or opposition. But it did little to placate her sister. It's not something to be decided by a show of hands, Mary Cheney wrote. Combined bias sentence examples with three news media sources extracted from event 7 (7fox, 7hpo, 7nyt) for informational bias and event 33 (33fox, 33hpo, 33nyt) for lexical bias showing the influence of local and target-aware global contexts that aids the model in effectively determining bias. In this example we see how sentences in bold, representing bias target sentences with global indexing (event-based), are harmoniously integrated with contextual information from neighboring sentences (local indexing, i.e., preceding and subsequent sentences within the article.)</figDesc><table><row><cell>Source Target Local Index Global Index Sentence</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors wish to express gratitude to the funding organization as this work has been supported by the <rs type="programName">Mohammed bin Salman Center for Future Science and Technology for Saudi-Japan Vision 2030 at The University of Tokyo</rs> (<rs type="grantNumber">MbSC2030</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_MSwMS3q">
					<idno type="grant-number">MbSC2030</idno>
					<orgName type="program" subtype="full">Mohammed bin Salman Center for Future Science and Technology for Saudi-Japan Vision 2030 at The University of Tokyo</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table 6: Comparison of our approach with previous work, separated by usage of context. We report average results of three runs with different random seeds. In the Table, Acc, P, and R stand for Accuracy, Precision and Recall respectively. BT denotes the augmentation approach from <ref type="bibr" target="#b20">(Maab et al., 2023)</ref>, who are also the only authors to report accuracy.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Document-level text classification using single-layer multisize filters convolutional neural network</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Pervez Akhter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Jiangbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irfan</forename><surname>Raza Naqvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Abdelmajeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atif</forename><surname>Mehmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Tariq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadiq</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="42689" to="42707" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Linguistic benchmarks of online news article quality</title>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Arapakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filipa</forename><surname>Peleja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barla</forename><surname>Berkant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Magalhaes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1893" to="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Ramy</forename><surname>Baly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgi</forename><surname>Karadzhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrhman</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00542</idno>
		<title level="m">Multi-task ordinal regression for jointly predicting the trustworthiness and the leading political ideology of news media</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Wei-Fan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Al-Khatib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henning</forename><surname>Wachsmuth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10649</idno>
		<title level="m">Detecting media bias in news articles using gaussian bias distributions</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to flip the bias of news headlines</title>
		<author>
			<persName><forename type="first">Wei-Fan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henning</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Khatib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International conference on natural language generation</title>
		<meeting>the 11th International conference on natural language generation</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pretrained language models for sequential sentence classification</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavana</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Weld</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d19-1383</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP). Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pretrained language models for sequential sentence classification</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavana</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.04054</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Dellavigna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Kaplan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The fox news effect: Media bias and voting</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page" from="1187" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Framing bias: Media in the distribution of power</title>
		<author>
			<persName><surname>Robert M Entman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of communication</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="163" to="173" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<ptr target="https://huggingface.co" />
		<title level="m">The ai community building the future</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">In plain sight: Media bias through the lens of factual reporting</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marshall</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruisi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kumar Choubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1664</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6343" to="6349" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Gentzkow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><forename type="middle">M</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">F</forename><surname>Stone</surname></persName>
		</author>
		<title level="m">Media bias in the marketplace: Theory. In Handbook of media economics</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="623" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A measure of media bias</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Groseclose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Milyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The quarterly journal of economics</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1191" to="1237" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Modeling multilevel context for informational bias detection by contrastive learning and sentential graph network. Shijia Guo and Kenny Q Zhu. 2022b. Modeling multilevel context for informational bias detection by contrastive learning and sentential graph network</title>
		<author>
			<persName><forename type="first">Shijia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenny</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2201.10376</idno>
		<idno type="arXiv">arXiv:2201.10376</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automated identification of media bias in news articles: an interdisciplinary literature review</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hamborg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Donnay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bela</forename><surname>Gipp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Digital Libraries</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="391" to="415" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Political ideology detection using recursive neural networks</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Hube</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Besnik</forename><surname>Fetahu</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1105</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Acm</forename><forename type="middle">Mohit</forename><surname>Iyyer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Peter</forename><surname>Enns</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</editor>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2019. 2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1113" to="1122" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junting</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03485</idno>
		<title level="m">Multi-view models for political ideology detection of news articles</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sentence-level media bias analysis informed by discourse structures</title>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Beauchamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10040" to="10050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Annotating and analyzing biased sentences in news articles using crowdsourcing</title>
		<author>
			<persName><forename type="first">Sora</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Jatowt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>F√§rber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masatoshi</forename><surname>Yoshikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Language Resources and Evaluation Conference</title>
		<meeting>the Twelfth Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1478" to="1484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An effective approach for informational and lexical bias detection</title>
		<author>
			<persName><forename type="first">Iffat</forename><surname>Maab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edison</forename><surname>Marrese-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Fact Extraction and VERification Workshop (FEVER)</title>
		<meeting>the Sixth Fact Extraction and VERification Workshop (FEVER)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="66" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Representing character sequences as sets : A simple and intuitive string encoding algorithm for nlp data cleaning</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Marinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Efremov</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASC48083.2019.8946281</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Advanced Scientific Computing (ICASC)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Content based fake news detection using knowledge graphs</title>
		<author>
			<persName><forename type="first">Siyana</forename><surname>Jeff Z Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Pavlova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangmei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinshuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07104</idno>
	</analytic>
	<monogr>
		<title level="m">The political economy of mass media. Advances in economics and econometrics</title>
		<editor>
			<persName><surname>Springer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bennett</forename><surname>Ver√≥nica P√©rez-Rosas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alexandra</forename><surname>Kleinberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rada</forename><surname>Lefevre</surname></persName>
		</editor>
		<editor>
			<persName><surname>Mihalcea</surname></persName>
		</editor>
		<meeting><address><addrLine>Monterey, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Andrea Prat and David Str√∂mberg</publisher>
			<date type="published" when="2013">2018. October 8-12, 2018. 2017. 2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">135</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>The Semantic Web-ISWC 2018: 17th International Semantic Web Conference</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lstm with sentence representations for document-level sentiment classification</title>
		<author>
			<persName><forename type="first">Guozheng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Zhiyong Feng, and Qiong Cong</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">308</biblScope>
			<biblScope unit="page" from="49" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Linguistic models for analyzing and detecting biased language</title>
		<author>
			<persName><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Danescu-Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1650" to="1659" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Document-level sentiment classification using hybrid machine learning approach</title>
		<author>
			<persName><forename type="first">Abinash</forename><surname>Tripathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santanu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rath</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="805" to="831" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">fake news&apos;&amp; disinformation: handbook for journalism education and training</title>
		<idno type="DOI">10.18653/v1/2020.coling-main.556</idno>
		<idno type="arXiv">arXiv:2005.04732</idno>
	</analytic>
	<monogr>
		<title level="m">International Committee on Computational Linguistics. Xiang Zhou and Mohit Bansal. 2020. Towards robustifying nli models against lexical dataset biases</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>UNESCO</publisher>
			<date type="published" when="2018">2018. 2020</date>
			<biblScope unit="page" from="6315" to="6326" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the 28th International Conference on Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
