<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Consistent Narrative Prompts on Abductive Natural Language Inference</title>
				<funder ref="#_aUREbHp">
					<orgName type="full">NSFC Fund</orgName>
				</funder>
				<funder ref="#_XVD43UU #_wn66ugv">
					<orgName type="full">GRF</orgName>
				</funder>
				<funder ref="#_RsB6sSv #_9pAtqcA #_AQU73Kp #_DYRmqGZ #_pZEMfFf #_E5hECQm">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_4USVFd8 #_qGfBhem">
					<orgName type="full">RIF</orgName>
				</funder>
				<funder>
					<orgName type="full">RGC of Hong Kong</orgName>
				</funder>
				<funder>
					<orgName type="full">NSFC of China</orgName>
				</funder>
				<funder ref="#_fPTHZBz">
					<orgName type="full">UGC Research Matching Grants</orgName>
				</funder>
				<funder>
					<orgName type="full">NVIDIA AI Technology Center (NVAITC)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chunkit</forename><surname>Chan</surname></persName>
							<email>ckchancc@cse.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">HKUST</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">HKUST</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tszho</forename><surname>Chan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiayang</forename><surname>Cheng</surname></persName>
							<email>jchengaj@cse.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">HKUST</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
							<email>yqsong@cse.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">HKUST</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ginny</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
							<email>gwong@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">HKUST</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">NVIDIA AI Technology Center (NVAITC)</orgName>
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<region>Santa Clara</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>See</surname></persName>
							<email>ssee@nvidia.com</email>
							<affiliation key="aff1">
								<orgName type="department">NVIDIA AI Technology Center (NVAITC)</orgName>
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<region>Santa Clara</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Consistent Narrative Prompts on Abductive Natural Language Inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">321CE5DAA2C2DE2AF67BC7B6AF0E59E9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abduction has long been seen as crucial for narrative comprehension and reasoning about everyday situations. The abductive natural language inference (αNLI) task has been proposed, and this narrative text-based task aims to infer the most plausible hypothesis from the candidates given two observations. However, the inter-sentential coherence and the model consistency have not been well exploited in the previous works on this task. In this work, we propose a prompt tuning model α-PACE 1 , which takes self-consistency and intersentential coherence into consideration. Besides, we propose a general self-consistent framework that considers various narrative sequences (e.g., linear narrative and reverse chronology) for guiding the pre-trained language model in understanding the narrative context of input. We conduct extensive experiments and thorough ablation studies to illustrate the necessity and effectiveness of α-PACE. The performance of our method shows significant improvement against extensive competitive baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Abductive reasoning aims to find the most plausible explanation based on incomplete observations <ref type="bibr" target="#b45">(Peirce, 1974)</ref>. Abduction has long been seen to be essential for understanding narratives <ref type="bibr" target="#b21">(Hobbs et al., 1993)</ref> and reasoning about everyday situations <ref type="bibr" target="#b1">(Andersen, 1973)</ref>. <ref type="bibr" target="#b3">Bhagavatula et al. (2020)</ref> investigated the language-based abduction in narrative texts and introduced the abductive natural language inference (αNLI) benchmark, which is a multiple-choice question answering task for identifying the most likely explanation among two hypotheses based on two observations. One example is illustrated in Figure <ref type="figure">1</ref>, where "O 1 " and "O 2 " are Figure <ref type="figure">1</ref>: A data example from αNLI and its corresponding narrative sequences, including linear narrative and reverse chronology. Two sequences explain the same narrative example seamlessly by utilizing the discourse connectives (i.e., " in fact," " then," " as a result,"). two observations. Abductive reasoning is identifying a possible hypothesis (either H 1 or H 2 ) that can best explain the consequences by evaluating and comparing the plausibility of these two hypotheses.</p><p>Traditional works on the αNLI task focus on ranking the hypotheses among "H 1 " and "H 2 " <ref type="bibr" target="#b57">(Zhu et al., 2020;</ref><ref type="bibr">Li et al., 2021a)</ref> or incorporating the knowledge from various sources into pre-trained language models, such as general commonsense knowledge <ref type="bibr" target="#b37">(Mitra et al., 2019;</ref><ref type="bibr" target="#b15">Du et al., 2021)</ref> and social commonsense knowledge <ref type="bibr" target="#b43">(Paul et al., 2020)</ref>. However, one crucial piece of information, i.e., the inter-sentential coherence and the consistency of the model, has yet to be investigated and explored.</p><p>These prior studies often concatenate the observations and hypotheses as the model input, ignoring the coherence between sentences and their intersentential relations in this narrative-based task. Nar-rative, as a semiotic representation of a sequence of events meaningfully connected in a temporal and causal way <ref type="bibr" target="#b50">(Ryan et al., 2007;</ref><ref type="bibr" target="#b40">Onega and Landa, 2014)</ref>, intrinsically encodes the information required for abductive reasoning that makes it logical, sensible, and coherent. For instance, Figure <ref type="figure">1</ref> illustrates that the relation connected from "H 2 " to "O 2 " is a causal relation emphasized by a discourse connective "as a result" and provides the extra causal information needed for pre-trained language models (PLMs) to comprehend these observations and hypotheses in depth. Furthermore, the consistency of a model, a highly desirable characteristic for a model in natural language processing, refers to the invariant in behavior despite meaningpreserving alterations in its input. Prior research has highlighted the significance of mode consistency and revealed that language models could exhibit inconsistencies in various contexts, including conversation, explanation generation, and factual knowledge extraction <ref type="bibr" target="#b0">(Adiwardana et al., 2020;</ref><ref type="bibr" target="#b7">Camburu et al., 2020;</ref><ref type="bibr">Elazar et al., 2021)</ref>. These inconsistencies may result in output variability and local optimality. Since the prompt tuning-based method reduces the model variability by freezing the pre-trained model without altering the representations, we propose a self-consistent prompt tuning model that considers the inter-sentential coherence in the αNLI task.</p><p>We have noticed that <ref type="bibr" target="#b55">Wang et al. (2022)</ref> proposed a self-consistent framework (i.e., sampleand-marginalize method) that focuses on the answer consistency among diverse reasoning paths. It relies on an individual prompt to sample various outputs and perform majority voting to address the inconsistency issue that language models suffer. However, this method may not be an optimal method for the αNLI task as it does not take the narrative sequences into account. A narrative usually describes the sequence of events in various narrative orders, utilizing different inter-sentential relations. In particular, people can understand the same narrative context by utilizing alternative narrative sequences instead of linear narrative, such as nonlinear narrative and reverse chronology. For example, Figure <ref type="figure">1</ref> shows that both linear narrative and reverse chronology can explain the same narrative seamlessly by employing discourse connectives. In this linguistic phenomenon, two narrative sequences with different description orders emphasize different partial information about these events, while expressing the same narrative context. When applying machine learning for abductive reasoning, with context sequences being different, the performance of models can vary as pre-trained language models interpret the context information from diverse perspectives and extents.</p><p>In this paper, we attempt to imitate the cognitive process of narrative understanding, and propose a general self-consistent framework to facilitate a PLM understanding of the narrative context based on the above linguistic phenomenon considering different narrative sequences. For each narrative sequence, we design a specific prompt template to distinguish the difference in narrative order while still incorporating inter-sentential coherence and self-consistency.</p><p>Our contributions are summarized as follows: 1. This work is the first to consider inter-sentential coherence and self-consistency through the prompt tuning method in the task. 2. We propose a general self-consistent framework based on the linguistic phenomenon that allows various narrative sequences for undertaking abductive reasoning. 3. We conduct extensive experiments and thorough ablation studies to illustrate the necessity and effectiveness of the specific prompt template and general self-consistent framework. The results support our claims and the success of our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Abductive Reasoning Abduction has long been thought necessary for comprehending narrative <ref type="bibr" target="#b21">(Hobbs et al., 1993)</ref> and reasoning about everyday events <ref type="bibr" target="#b1">(Andersen, 1973)</ref>. Most earlier research has concentrated on formal logic-based abductive reasoning <ref type="bibr" target="#b26">(Levesque, 1989;</ref><ref type="bibr" target="#b39">Ng and Mooney, 1990;</ref><ref type="bibr" target="#b44">Paul, 1993)</ref>. However, the rigidity of formal logic restricts its application in the field of NLP. Hence, <ref type="bibr" target="#b3">Bhagavatula et al. (2020)</ref> developed a languagebased abductive reasoning task to help with this, and they developed baselines that adopt the pretrained language models (i.e., BERT <ref type="bibr" target="#b14">(Devlin et al., 2019)</ref>) under their probabilistic framework. To solve this task, <ref type="bibr" target="#b43">Paul et al. (2020)</ref> proposed a multihead knowledge attention approach to enhance RoBERTa <ref type="bibr" target="#b34">(Liu et al., 2019)</ref> by incorporating the structured social commonsense knowledge generated from COMET <ref type="bibr" target="#b5">(Bosselut et al., 2019)</ref>. <ref type="bibr" target="#b15">Du et al. (2021)</ref> employed a latent variable to acquire commonsense knowledge from the event graph and</p><formula xml:id="formula_0">Input … O 1 H j O 2 … … … O 1 H j O 2 … … … O 1 H j O 2 … … … O 1 H j O 2 … … … O 1 H j O 2 … … O 1 H j … O 2 … … PLM choice1 choice2</formula><p>Label Tokens enhance the pre-trained language model RoBERTa.</p><formula xml:id="formula_1">H 1 choice1 H 2 choice2 Verbalizer Narrative Sequence Patterns Prompt Pattern O 1 O 2 H 1 H 2 choice1 choice2 choice1 choice2 choice1 choice2 choice1 choice2 choice1 choice2 choice1 choice2 Voting Overall,[MASK] is plausible. P 1 O 2 P 2 H 1 P 3 O 1 O 2 P 6 H 2 P 7 O 1 P 0 choice1: choice2: P 5 P 4 It is [MASK]. It is [MASK].</formula><p>Apart from incorporating commonsense knowledge to tackle this task, <ref type="bibr" target="#b57">Zhu et al. (2020)</ref> reformulated the αNLI task as a ranking task using a learning-toranking framework to rank candidate hypotheses. <ref type="bibr">Li et al. (2021a)</ref> proposed an interactive language model that groups the correct and incorrect hypotheses instead of ranking these hypotheses and adopts joint softmax focal loss for this αNLI task. However, prior works did not exploit model consistency and various narrative sequences in this task.</p><p>Prompt Tuning By relaxing the constraint that prompts token embedding to be the natural language, <ref type="bibr" target="#b30">Li and Liang (2021)</ref> and <ref type="bibr" target="#b18">Hambardzumyan et al. (2021)</ref> proposed combining a PLM's input token embeddings with additional continuous vectors. Some studies <ref type="bibr" target="#b25">(Lester et al., 2021;</ref><ref type="bibr" target="#b47">Qin and Eisner, 2021;</ref><ref type="bibr" target="#b30">Li and Liang, 2021)</ref> proposed only tuning continuous prompts, while some works <ref type="bibr" target="#b19">(Han et al., 2021;</ref><ref type="bibr" target="#b56">Zhong et al., 2021;</ref><ref type="bibr">Liu et al., 2021b;</ref><ref type="bibr">Chan et al., 2023b)</ref> explore combining discrete prompts and continuous prompts. They tune the embedding of these additional continuous vectors, and the parameters of PLMs are frozen in their task. In our work, we also adopt this strategy, but we focus on utilizing this approach to investigate the model consistency and the narrative coherence information underlying various observations and hypotheses in this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">α-PACE</head><p>In order to explore the inter-sentential coherence and model consistency for the abductive natural language inference (αNLI) task, we propose the abductive self-consistent Prompt tuning model on nAtural language inferenCE task (α-PACE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>Abduction is to infer the most reasonable explanation for incomplete observations <ref type="bibr" target="#b45">(Peirce, 1974)</ref>.</p><p>In the αNLI task <ref type="bibr" target="#b3">(Bhagavatula et al., 2020)</ref>, given two observations O 1 i and O 2 i , we choose the most plausible hypothesis among H 1 i and H 2 i :</p><formula xml:id="formula_2">H * i = arg max H j i P H i = H j i | O 1 i , O 2 i ,<label>(1)</label></formula><p>where H * i is the most reasonable hypothesis, and</p><formula xml:id="formula_3">i indicates i-th instance of the dataset D = {(x i , y i )} |D| i=1 where x i = O 1 i , O 2 i , H 1 i , H 2 i .</formula><p>We will omit the index i without causing ambiguity in the following part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">T5 Foundation Model</head><p>T5 <ref type="bibr" target="#b48">(Raffel et al., 2020)</ref>, an encoder-decoder model, has been pre-trained on a multi-task mixture of unsupervised and supervised tasks. The unsupervised denoising training task focused on training this model to predict consecutive masked spans of tokens. For instance, the input "She had the best sleep she had in a long time." was corrupted as "She &lt;X&gt; the best sleep she had in a &lt;Y&gt;." The target output was "&lt;X&gt; had &lt;Y&gt; long time &lt;/s&gt;" &lt;/s&gt; is the eos_token. The supervised pre-trained task required the model to perform a sequence-tosequence input-output mapping with the instruction of a task prefix (e.g., "translate German to English:" or "summarize:"). However, discovering the specific textual prefix token was arduous and required enormous human effort. To overcome this issue, prefix tuning <ref type="bibr" target="#b30">(Li and Liang, 2021)</ref> and prompt tuning <ref type="bibr" target="#b25">(Lester et al., 2021)</ref> methods were proposed, which relaxed the constraint of discrete textual tokens to continuous and tunable ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Self-Consistent Prompt Tuning Model</head><p>To predict the hypothesis H * i for each instance x i , we employed a human-tailored template T (•) transforming the data instances x i to the prompt input xi = T (x i ), and a verbalizer V(•) is utilized to map a set of words to class labels. Figure <ref type="figure">2</ref> illustrates the architecture of α-PACE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Task-Specific Self-Consistent Prompt</head><p>The meticulously devised template, crafted to investigate inter-sentential coherence and selfconsistency, comprises essential discrete tokens, masked tokens, and learnable continuous tokens.</p><p>Inter-Sentential Coherence For the intersentential coherence, we concatenate the <ref type="figure"></ref>and<ref type="figure">H</ref> 2 i together as the model input. These two sequences are to help PLMs easily capture the coherence information inherently in various sequences. <ref type="bibr">Moreover, Chatman (1980)</ref> explains that the story is the context of narrative (the what of the narrative), and the discourse is the form of narrative (the how). Specifically, the discourse is the means by which the narrative content is expressed <ref type="bibr" target="#b11">(Chatman, 1980;</ref><ref type="bibr" target="#b54">Tomaščíková, 2009)</ref>. Therefore, by adding discourse connectives between two events, the pre-trained language model can understand the narrative context more easily and enhance the inter-sentential coherence. Nevertheless, employing diverse discourse connectives for each data instance presents a formidable challenge. Hence, we insert the continuous tunable prompt tokens to represent the discourse connectives between each sentence (i.e.,O O 2 i and O 1 i ) to learn the coherence information between these sentences. Since some discourse connectives naturally start before the first sentence (such as "since" and "although"), we assign the continuous tunable prompt tokens before the first sentence of the sentence sequence. We follow <ref type="bibr">Liu et al. (2021a)</ref> to name the continuous prompts in our method. The continuous prompts are denoted as</p><formula xml:id="formula_4">O 1 i , H 1 i , O 2 i and O 1 i , H 2 i , O 2 i as two sentence sequences S 1 i and S 2 i , instead of directly connect the O 1 i , O 2 i , H 1 i ,</formula><formula xml:id="formula_5">{P k ∈ R p k ×d |k = 0, 1, • • • , 7}</formula><p>, where the P 0 and P 4 serve as the prefix prompt to learn the instruction guiding the model to perform the αNLI task by following <ref type="bibr" target="#b25">Lester et al. (2021)</ref>. Other prompt tokens correspond to the cloze prompt between two different sentences (or before the first sentence) utilized to represent the discourse connectives to learn the coherence information between sentences. p k is the length of the k-th prompt.</p><p>Self-Consistent Prompt For the self-consistency of model output, three <ref type="bibr">[MASK]</ref> tokens are included: a [MASK] combined with the discrete token "is plausible." forming the manual template "Overall, [MASK] is plausible." for facilitating model inference, and each of another two <ref type="bibr">[MASK]</ref> merges with the discrete token ", it is" to constitute ", it is [MASK]" append after each sentence sequence. Furthermore, the discrete tokens "choice1:" and "choice2:" are placed before sequences S 1 i and S 2 i respectively for splitting two sequences. These three <ref type="bibr">[MASK]</ref> tokens are used for achieving the purpose of model self-consistency by ensuring three model outputs consistently. By considering the mentioned sentence sequences and the example in Figure <ref type="figure">1</ref>, humans are able to recognize that H 2 i is more plausible, resulting from the sentence sequence S 1 i is not plausible or less plausible than S 2 i . In this case, the pre-trained language model guided to predict "not plausible" for S 1 i , "plausible" for S 2 i , and "choice2" in the third mask in the learning process. Throughout this learning process, the model will learn the output consistency ability. Therefore, we introduce three masks in our model to predict the plausibility of S 1 i and S 2 i , and the last mask for final determined labels (i.e., "choice1" or "choice2" representing the H 1 i and H 2 i ).</p><p>Self-Consistent Verbalizer A typical verbalizer usually maps a label y to a single answer token z or a series of spans z 1 , z 2 , • • • greedily <ref type="bibr">(Schick and Schütze, 2021;</ref><ref type="bibr">Liu et al., 2021a)</ref>. We extend it by mapping two class labels (i.e., H 1 i and H 2 i ) to three tokens, i.e. {H j } → Z × Z × Z, where Z is the vocabulary and three [MASK] tokens denoted as z 1 , z 2 , and z 3 . Using the tailored prompt template featuring three [MASK]s and the verbalizer, the probability distribution over {H j } can be formalized as the joint probabilities of z 1 , z 2 , and z 3 , i.e. Pr(</p><formula xml:id="formula_6">H j | xi ) = Pr(V(H j ) | xi ) = Pr(z 1 i = h j 3 , z 2 i = h j 1 , z 3 i = h j 2 | xi )</formula><p>, where a hypothesis H j consists of h j 1 (the plausibility of S 1 i ), h j 2 (the plausibility of S 2 i ), and h j 3 (the probability of H 1 i and H 2 i ). Table <ref type="table" target="#tab_1">1</ref> summarizes the label words. Given that T5 is able to predict masked tokens synchronously, the joint probability can be written as</p><formula xml:id="formula_7">Pr(H j | xi ) = 3 k=1 Pr(z k i = v k (H j ) | xi ),<label>(2)</label></formula><p>where</p><formula xml:id="formula_8">v k (•) : {H j } → Z is the submap of V(•) for the k-th [MASK].</formula><p>And then the final learning objective of α-PACE is to maximize</p><formula xml:id="formula_9">J = 1 |D| (x i ,y i )∈D log 3 k=1 Pr(z k i = v k (H j ) | xi ). (3)</formula><p>The final prediction of H * i by choosing the maximum joint probability (i.e., self-consistency score) as Eq. (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">General Self-Consistent Narrative framework</head><p>The self-consistent framework introduced by previous studies may not be the optimal approach for the αNLI task due to its lack of consideration for various narrative sequences <ref type="bibr" target="#b55">(Wang et al., 2022)</ref>. Furthermore, <ref type="bibr" target="#b3">Bhagavatula et al. (2020)</ref> and their follow-up works <ref type="bibr" target="#b15">(Du et al., 2021;</ref><ref type="bibr" target="#b43">Paul et al., 2020)</ref> only form probabilistic models focusing on the Linear Chain Model (P r(O 2 |H j )P (H j |O 1 ), where H j can be H 1 or H 2 ) and Fully Connected Model (P r(O 2 |H j , O 1 )P (H j |O 1 )). This means their framework primarily considers the given fixed time sequence, i.e., O 1 , H j , and O 2 , and may not align with the representation of the pre-trained language model. Therefore, we permute {O 1 , O 2 , (H 1 , H 2 )} and design six narrative sequence patterns for this task according to the or-  ders of observations and the pair of hypotheses.</p><p>The six patterns are illustrated in the overall framework in Figure <ref type="figure">2</ref>. For example, the O2HO1 sequence pattern means that we put H j in the middle of O 2 and O 1 and try to utilize the possible inter-sentential coherence information among them in this order. After receiving the joint generation probabilities from each pattern, we normalize the probability distribution between "H 1 " and "H 2 " to make it more contrastive. Then, we perform the majority voting over six narrative sequence pattern distributions and map token predictions to the label prediction.</p><p>4 Experimental Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">αNLI Dataset</head><p>The experiments are conducted on the ART dataset, aimed at assessing the performance of our model on the αNLI task <ref type="bibr" target="#b3">(Bhagavatula et al., 2020)</ref>. The observations of ART data were collected from a story corpus known as ROCstory <ref type="bibr" target="#b38">(Mostafazadeh et al., 2016)</ref>, while the corresponding hypotheses were generated by crowdsourcing. Moreover, αNLI has a dedicated leaderboard with 3,040 test instances to measure the generalizability of the models. The detailed data statistics can be found in Table <ref type="table" target="#tab_3">2</ref>. By following previous work <ref type="bibr" target="#b3">(Bhagavatula et al., 2020;</ref><ref type="bibr" target="#b15">Du et al., 2021;</ref><ref type="bibr" target="#b57">Zhu et al., 2020)</ref>, we employ accuracy as an evaluation metric to evaluate the empirical performance of our method in experiments and ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>The  <ref type="bibr" target="#b14">(Devlin et al., 2019)</ref> 69.10 68.90 RoBERTa <ref type="bibr" target="#b34">(Liu et al., 2019)</ref> 85.76 84.48 McQueen <ref type="bibr" target="#b37">(Mitra et al., 2019)</ref> 86.68 -MHKA <ref type="bibr" target="#b43">(Paul et al., 2020)</ref> 87.44 87.12 ege-RoBERTa <ref type="bibr" target="#b15">(Du et al., 2021)</ref> -87.50 L2R 2 <ref type="bibr" target="#b57">(Zhu et al., 2020)</ref> 88.44 86.11 RoBERTa-L+IMSL <ref type="bibr">(Li et al., 2021a)</ref> 89.20 -Prefix-Tuning (T5) <ref type="bibr" target="#b25">(Lester et al., 2021)</ref> 84.20 83.88 Prompt-Tuning (T5) <ref type="bibr" target="#b30">(Li and Liang, 2021)</ref> 86.23 85.98 Fine-Tuning (T5) <ref type="bibr" target="#b48">(Raffel et al., 2020)</ref> 87  methods such as Prefix-Tuning (T5) <ref type="bibr" target="#b30">(Li and Liang, 2021)</ref> and Prompt-Tuning (T5) <ref type="bibr" target="#b25">(Lester et al., 2021)</ref> are included as baselines for exhibiting the exact contribution of our proposed method. The implementation details of the T5 fine-tuning model are described in Appendix A.3, while Prefix-Tuning and Prompt-Tuning methods are appended in Appendix A.4. More details of the baselines are in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Results</head><p>Table <ref type="table" target="#tab_6">3</ref> and  <ref type="bibr" target="#b14">(Devlin et al., 2019)</ref> 66.75 RoBERTa <ref type="bibr" target="#b34">(Liu et al., 2019)</ref> 83.91 McQueen <ref type="bibr" target="#b37">(Mitra et al., 2019)</ref> 84.18 ege-RoBERTa <ref type="bibr" target="#b15">(Du et al., 2021)</ref> 85.95 L2R 2 <ref type="bibr" target="#b57">(Zhu et al., 2020)</ref> 86.81 UNICORN (T5) <ref type="bibr" target="#b35">(Lourie et al., 2021)</ref> 87.34 RoBERTa-L+IMSL <ref type="bibr">(Li et al., 2021a)</ref> 87.83 DeBERTa <ref type="bibr" target="#b20">(He et al., 2021)</ref> 89.70 DeBERTa(Ensemble) <ref type="bibr" target="#b20">(He et al., 2021)</ref> 90.00 UNIMO <ref type="bibr">(Li et al., 2021b)</ref> 91 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>To better study the factors of the α-PACE model, we have devised numerous ablations on the joint probability for self-consistency, general narrative self-consistency, continuous prompt length, prompt engineering, and model size.</p><p>Joint Probability for Task-Specific Self-Consistency In our method, by estimating the likelihood of three masks to achieve selfconsistency purposes, the dependencies of these three masks are exploited to enhance the ability of the pre-trained language model on this αNLI task. According to the experimental results in Table <ref type="table" target="#tab_9">5</ref>, we can conclude that 1) The performance of our task-specific self-consistent prompt model incorporating the signals from all three masks surpasses other models (e.g., α-PACE First &amp; Second ), emphasizing the significance of dependencies and effectiveness of self-consistency; 2) The model with a single mask (e.g., α-PACE First ), without integrating information from the other two masks, exhibits the worst performance; 3) The model with the third mask (e.g., α-PACE Second &amp; Third ), which selects the best hypothesis, performs better than other models that lack the third mask. This finding highlights the importance and necessity of the third mask, summarizing the overall plausibility of two narrative sequences. General Narrative Self-Consistency The prior research on the self-consistent prompt-based method (i.e., sample-and-marginalize method) relied on an individual prompt to sample various outputs and perform majority voting to resolve the inconsistency issue that language models suffered <ref type="bibr" target="#b55">(Wang et al., 2022)</ref>. Therefore, we conducted experiments to compare the performance of both this method and our proposed general self-consistent method on our model, and the result is displayed in Table <ref type="table" target="#tab_9">5</ref>. By combining the likelihood of various outputs, the performance of sample-and-marginalize is slightly improving over the original single pattern-based model. Simultaneously, our general self-consistent approach surpasses this sample-and-marginalize method in two settings, with or without considering task consistency. Therefore, the results evidence the significance of our linguistic phenomenon-based selfconsistent prompt, which considers various narrative sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt Length &amp; Prompt Engineering &amp; Model</head><p>Size Furthermore, we conduct the ablation study on the continuous prompt length, prompt engineering, and the model size of our model in both fewshot and full training configurations. The details we described are in Appendix B.1. The vital information worth mentioning is that without inserting prefix prompt and cloze prompt into our prompt template, the performance will significantly drop, and it illustrates the necessity of these two parts of learnable prompt tokens in our model.  stances, we summarize our experimental results for the αNLI task in Table <ref type="table" target="#tab_10">6</ref>. We report the mean accuracy and standard deviation for five random seeds. As shown in Table <ref type="table" target="#tab_10">6</ref>, our proposed model consistently outperforms other prompt-based models and appears more beneficial in this few-shot setting. Both our general consistency model and single narrative pattern model provide a significant gain over all stated baselines. With training on 100 instances, we observe that our proposed model with a single narrative sequence pattern significantly exceeds the Fine-Tuning (T5) in accuracy on the dev and test datasets by 12.68% and 10.72%, respectively. Furthermore, compared with the prompt-based models, our model still surpasses at least 6.52% test accuracy. The large gap between our model and other T5-based models emphasizes the significance of the task-specific self-consistent method by considering the inter-sentential coherence information, proving that our model can effectively elicit and utilize temporal and causal information between observations and hypotheses. Moreover, after considering six narrative patterns, our α-PACE General Consistency outperforms all our single pattern models by at least 1.23% and 2.06% in validation and test accuracy,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Few-Shot</head><formula xml:id="formula_10">2 2 + 2 + 2 + 2 2 + 2 2 2 2 + 2 + 2 6HTXHQFH3DWWHUQ $FFXUDF\ 1RQH '</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pattern Example</head><p>O1O2H Meanwhile, Carl went to the store desperately searching for flour tortillas for a recipe. In fact, Carl left the store very frustrated because the store had corn tortillas, but not flour ones.</p><p>O1HO2 In fact, Carl went to the store desperately searching for flour tortillas for a recipe. As the store had corn tortillas, but not flour ones. As a result, Carl left the store very frustrated.</p><p>HO1O2 Because the store had corn tortillas, but not flour ones. Meantime, Carl went to the store desperately searching for flour tortillas for a recipe. As a result, Carl left the store very frustrated.</p><p>HO2O1 If the store had corn tortillas, but not flour ones. As a result, Carl left the store very frustrated after Carl went to the store desperately searching for flour tortillas for a recipe.</p><p>O2O1H Meanwhile, Carl left the store very frustrated if Carl went to the store desperately searching for flour tortillas for a recipe as the store had corn tortillas, but not flour ones.</p><p>O2HO1 In fact, Carl left the store very frustrated as long as the store had corn tortillas, but not flour ones. Because Carl went to the store desperately searching for flour tortillas for a recipe. respectively. We also study the influence of various training examples. We randomly subsample the entire dataset to obtain smaller datasets of size {1, 5, 10, 20, 50}. More details for the performance are shown in Figure <ref type="figure">9</ref> and Figure <ref type="figure">10</ref> in Appendix B.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Prompt Adaptation For ChatGPT</head><p>With the powerful ability of LLMs exhibited on numerous tasks, we are curious about the capability of ChatGPT on zero-shot abductive commonsense reasoning tasks. Therefore, we test the ability of ChatGPT with four designed templates. The performance is shown in Table 7. All the baselines can outperform random prediction. ChatGPT T askConsistency improves the performance by 1.5% over ChatGPT P rompt by utilizing the prompt template in the taskspecific consistency method. We also find that the general self-consistent prompting (ChatGPT General &amp; Task Consistency ) demonstrates the additional performance boost over other baselines. Compared with ChatGPT Sample-and-Marginal , instead of an individual prompt template sampling six times, our narrative framework, which considers six sequences, performs better on the αNLI task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Interpretability</head><p>An ideal interpretable prompt should be composed of natural language that makes it obvious why this prompt elicited such behavior from the model <ref type="bibr" target="#b25">(Lester et al., 2021)</ref>. Since the prompt tuning process only updates the prompt parameters and freezes the pre-trained language model, the learned prompt is expected to encode the inter-sentential coherence information (e.g., temporal and causal information) in our method. Therefore, the nearest neighbors discourse connectives of our learned cloze prompt (used to represent the discourse connectives in each pattern) should reasonably and appropriately describe the relationship between each sentence in various sentence sequences. The motivation of the interpretability section is to provide a view of the perspective of the significance of discourse connective and explore the possibility of different narrative sequences on the αNLI task.</p><p>To obtain the nearest neighbors discourse connectives of these continuous cloze prompts in our method, we compute the cosine similarity between the averaged representation of learned cloze prompt tokens and the embedding vector of discourse connectives. The top selected connectives for each sequence pattern are shown in Table <ref type="table" target="#tab_13">8</ref>, and more details of discourse connectives can be found in Appendix B.3. We use the data example utilized to illustrate the full-connect model in <ref type="bibr" target="#b3">Bhagavatula et al. (2020)</ref> and insert the top selected connectives in between the sentences to form a narrative text, as shown in Table <ref type="table" target="#tab_14">9</ref>. We observe that the learned discourse connectives can describe the same collection of sentences in various sentence sequences in a rational and acceptable way. More case studies are shown in Table <ref type="table" target="#tab_6">13</ref> in Appendix B.3.</p><p>We further test the performance on three input settings: (1) without continuous prompts inserted, (2) with inserting the top selected connectives as the discrete prompts, and (3) with the cloze continuous prompts. The results are shown in Figure <ref type="figure">3</ref>, and we see that the discrete connective is substantially superior to the without one. This finding underscores the plausibility and effectiveness of adopting the discourse marker to elicit coherent information from PLM. Moreover, the overall performance of our method with the continuous prompts outperforms the other two settings except for the O1HO2 pattern, where the discrete prompts are slightly better than the continuous prompts. It emphasizes the significance of utilizing continuous prompts to represent the connectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We developed a model that considers intersentential coherence and self-consistency through prompt tuning for improving the narrative understanding on the αNLI task. Moreover, we propose a general self-consistent framework based on linguistic phenomena. The extensive experiments evidence the effectiveness of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Since all utilized information is only elicited from pre-trained language models (PLMs), our method relies on information or knowledge implicitly stored in the PLMs and the task dataset. This limitation restricts the capability owing to the reporting bias <ref type="bibr" target="#b17">(Gordon and Durme, 2013)</ref> in the pretrained language models (PLMs). Moreover, our method is limited to the information type that can be elicited from PLMs. The future work for the constraint is to incorporate more abundant and sufficient knowledge to equip the model with more vital abilities. A possible method is adopting the grounding method <ref type="bibr">(Lin et al., 2019)</ref> or retrieving the relevant nodes in the knowledge graph for each data instance, providing more contextual information and enhancing the capability of the model on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>In this work, we conformed to accepted privacy practices and strictly followed the data usage policy. This paper presents a framework for guiding the PLM to understand the narrative context of input from the abductive natural language inference task. The ART dataset from the αNLI task <ref type="bibr" target="#b3">(Bhagavatula et al., 2020)</ref> we used to train and evaluate the abductive inference ability of our model is publicly available, and this work is in the intended use. This dataset is collected from the manually curated story corpus ROCstory and should not contain any information that names or uniquely identifies individual people. Since we do not introduce social and ethical bias into the model or amplify any bias from the data, we can foresee no direct social consequences or ethical issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix for Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Baselines</head><p>We compare α-PACE with the following competitive baselines : (a) BERT <ref type="bibr" target="#b14">(Devlin et al., 2019</ref>) is a language model trained with masked-language modeling and the next sentence prediction objective. (b) RoBERTa <ref type="bibr" target="#b34">(Liu et al., 2019</ref>) is a powerful encoder that has the same architecture as BERT with robust optimization and more pre-training data. (c) McQueen <ref type="bibr" target="#b37">(Mitra et al., 2019)</ref> is a method to integrate external knowledge (e.g., commonsense knowledge) into a pre-trained language model (i.e., RoBERTa) to address the αNLI task. (d) MHKA <ref type="bibr" target="#b43">(Paul et al., 2020)</ref> enhances RoBERTa by incorporating the social commonsense knowledge for the αNLI task. (e) ege-RoBERTa-large <ref type="bibr" target="#b15">(Du et al., 2021</ref>) is a variational autoencoder-based model that learns commonsense knowledge by utilizing a latent variable for guiding the abductive reasoning task. (f) L2R 2 <ref type="bibr" target="#b57">(Zhu et al., 2020)</ref> reformulates the αNLI task as a ranking problem using the learning-toranking framework to rank candidate hypotheses. (g) IMSL <ref type="bibr">(Li et al., 2021a</ref>) is an interactive language model that groups the correct/wrong hypotheses instead of ranking the hypotheses and adopts joint softmax focal loss for this αNLI task. (h) Fine-tuning (T5) <ref type="bibr" target="#b48">(Raffel et al., 2020)</ref> is an encoder-decoder model pre-trained on a multi-task mixture, where each task is converted into a textto-text format. T5 performs well out of the box on many tasks by prepending a different prefix to the inputs. (i) Prefix-Tuning (T5) <ref type="bibr" target="#b30">(Li and Liang, 2021)</ref>: a method concatenates the tunable prefix tokens before the discrete input text, keeps language model parameters frozen, and optimizes these continuous task-specific prefix tokens. The implementation details of the Prefix-Tuning methods are appended in Appendix A.4. (j) Prompt-Tuning (T5) <ref type="bibr" target="#b25">(Lester et al., 2021)</ref>: a vanilla Prompt Tuning-based model conditioning on a frozen model, releasing the constraints of the prompt templates from discrete to learnable prompts. The implementation details of the prompt tuning methods are appended in Appendix A.4. (k) UNICORN (T5) <ref type="bibr" target="#b35">(Lourie et al., 2021</ref>) is a universal commonsense reasoning model with multi-task pre-training based on T5-11b. (l) DeBERTa <ref type="bibr" target="#b20">(He et al., 2021)</ref> improves RoBERTa with disentangled attention and enhanced mask decoder training. It is only trained with half of the data used in RoBERTa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 α-PACE Implementation Details</head><p>Our method is built upon the FLAN-T5 <ref type="bibr" target="#b13">(Chung et al., 2022)</ref> model was an enhanced version of the T5 model <ref type="bibr" target="#b48">(Raffel et al., 2020)</ref> that has been finetuned in a mixture of tasks. We primarily use the 11B version but also experiment with various sizes (Small, Base, Large, and 3B versions) for the ablations. All the T5-based baselines are built upon the same FLAN-T5 model size (e.g., Fine-Tuning, Prompt-Tuning, and Prefix-Tuning). The general configuration follows the setting in <ref type="bibr" target="#b25">Lester et al. (2021)</ref>. For the full-data training setting, the batch size and maximum sequence length are 1 and 350. We set the prefix length p 0 , p 4 as 30, and all remaining cloze prompt lengths as 3. We adopt an Adafactor optimizer by selecting a learning rate in {8e-4, 8e-5, 6e-5, 5e-5, 3e-5}, which yields the best performance on the dev set. The training is performed using cross-entropy loss, and the training steps are 30,000.</p><p>For the few-shot learning, we follow the full dataset setting except for the batch size and training steps being 3 and 5,000. Furthermore, we primarily use training set size K = 100 but explore K = {1, 5, 10, 20, 50} in the ablations. We sample the K examples from the full training data with five fixed seeds {55, 58, 68, 72, 1,000}. In this setting, we report the performance by averaging results along with the variance obtained for five different seeds. Prompt tuning is conducted on two NVIDIA RTX A6000 GPUs, and it takes around 52 hours for fulldata training and 3 hours for few-shot training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Implementation Details of T5 Model</head><p>Fine-Tuning</p><p>All the fine-tuning experiments are run on a server with 4 V100-32GB GPUs. When fine-tuning the 11b version, we use DeepSpeed <ref type="bibr" target="#b49">(Rajbhandari et al., 2020)</ref> with ZeRO stage 3 to offload parameters to memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Input and Output</head><p>The T5-based model serves as a competitive baseline in the main experiment by adopting the same model and model size. We use the template "Observation 1: {}\nHypothesis 1: {}\nHypothesis 2: {}\nObservation 2: {}" to transform a dataset instance into an input string. The model is asked to generate either Hypothesis 1 or Hypothesis 2 as</p><formula xml:id="formula_11">hypothesis1 hypothesis2 Prediction O 2 H 1 H 2 O 1 [MASK] P 0 hypothesis1 hypothesis2 Prediction P 1 O 2 P 2 H 1 P 3 O 1 H 2 [MASK] P 0 P 4 hypothesis1 hypothesis2 Prediction O 2 H 1 H 2 O 1 [MASK]</formula><p>Fine-Tuning Prefix-Tuning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt-Tuning</head><p>Figure <ref type="figure">4</ref>: Fine-Tuning, Prefix-Tuning, and Prompt Tuning Templates. Two prompt tuning-based templates perform best among all designed templates in the template searching process for these baselines. The order of observations and hypotheses following the fully connected model proposed by <ref type="bibr" target="#b3">Bhagavatula et al. (2020)</ref>.</p><p>the predicted label. The order of two observations and two hypotheses following the fully connected model proposed by <ref type="bibr" target="#b3">Bhagavatula et al. (2020)</ref> and shown in Figure <ref type="figure">4</ref> in Appendix.</p><p>Hyperparameter Search We first conduct a preliminary experiment to determine the range of hyper-parameters. For base and large model sizes, we set the per-device train and validation batch size as 16 and 64, respectively. For the 11b version, they are set as 8 and 32. Then, we search for the optimal learning rate within {3e-5, 1e-4, 3e-4}. The test performance of the model with the best validation accuracy is reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Implementation Details of the Prefix-Tuning and Prompt Tuning</head><p>The prefix tuning <ref type="bibr" target="#b30">(Li and Liang, 2021)</ref> and prompt tuning <ref type="bibr" target="#b25">(Lester et al., 2021)</ref> methods have been implemented as the baseline in full data learning and few-shot setting for comparison with our model. For a fair comparison, we count all the discrete textual tokens (non-tunable tokens) and the tunable tokens in our prompt template. There are 55 tokens, including 46 tunable tokens and nine textual tokens. In these two baselines, we will insert 55 tunable tokens into the respective prompt template. Moreover, we also adopt the same scale of T5 for these two baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prefix-Tuning</head><p>The overall configuration of this model follows the settings of prefix tuning <ref type="bibr" target="#b30">(Li and Liang, 2021)</ref>. The batch size and maximum se-quence length of this model are 8 and 350. The training is performed using cross-entropy loss with an Adafactor optimizer <ref type="bibr" target="#b51">(Shazeer and Stern, 2018)</ref>.</p><p>A learning rate selecting in {3e-1, 5e-1, 8e-1} yields the best performance on the validation set, and the training steps are 30,000. We insert 55 prefix tunable tokens into the prefix part of the input template. Since <ref type="bibr" target="#b3">Bhagavatula et al. (2020)</ref> stated that the given fixed time sequence (i.e., O 1 , H i , O 2 ) perform best among all the sequence, the order of two observations and two hypotheses is shown in Figure <ref type="figure">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt-Tuning</head><p>The overall configuration of this model follows the settings of prompt tuning <ref type="bibr" target="#b25">(Lester et al., 2021)</ref>. The batch size and maximum sequence length of this model are 8 and 350. The training is performed using cross-entropy loss, an Adafactor optimizer <ref type="bibr" target="#b51">(Shazeer and Stern, 2018)</ref>, and a learning rate selecting in {3e-1, 5e-1, 8e-1} yields the best performance on the validation set, with 30,000 training steps. We insert 55 tunable tokens evenly into inter-sentences or between sentences and mask tokens in the input template of this model. The order of two observations and two hypotheses is the same as the above method shown in Figure <ref type="figure">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 The Approximation of Learnable Parameters</head><p>To demonstrate the efficiency of our method, we attach the approximation of the learnable parameters for all models, including our model and the Model Parameters BERT-large <ref type="bibr" target="#b14">(Devlin et al., 2019)</ref> 340M RoBERTa-large <ref type="bibr" target="#b34">(Liu et al., 2019)</ref> 355M McQueen <ref type="bibr" target="#b37">(Mitra et al., 2019)</ref> 355M MHKA <ref type="bibr" target="#b43">(Paul et al., 2020)</ref> 355M ege-RoBERTa-large <ref type="bibr" target="#b15">(Du et al., 2021)</ref> 355M L2R 2 <ref type="bibr" target="#b57">(Zhu et al., 2020)</ref> 355M IMSL <ref type="bibr">(Li et al., 2021a)</ref> 355M DeBERTa-large <ref type="bibr" target="#b20">(He et al., 2021)</ref> 304M UNICORN (T5) <ref type="bibr" target="#b35">(Lourie et al., 2021)</ref> 11,000M Prompt Tuning <ref type="bibr" target="#b25">(Lester et al., 2021)</ref> 1M Prefix Tuning <ref type="bibr" target="#b30">(Li and Liang, 2021)</ref> 1M Fine-Tuning <ref type="bibr" target="#b48">(Raffel et al., 2020)</ref> 11,000 M α-PACE HO2O1 34 M  baselines. The approximation of the learnable parameters is displayed in Table <ref type="table" target="#tab_15">10</ref> in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Appendix for Evaluation Result and Analysis</head><p>B.1 Ablation study on the α-PACE Prompt Length Within our designed prompt template, two parts of continuous prompts are concatenated with the input sentences. The first part is two prefix prompts with p 0 and p 4 tokens inserted before template tokens "choice1" and "choice2". The other part is the cloze prompts inserted to two positions: p 1 (or p 5 ) tokens between "choice1" (or "choice2") and the first input sentence, and p 2 , p 3 (or p 6 , p 7 ) tokens between input sentences (observations or hypotheses).</p><p>For the prefix prompt, we train prompts for our model on 100 training instances by varying the  prefix prompt length in {None, 20, 30, 40, 80} while keeping the other setting unchanged. Figure <ref type="figure" target="#fig_0">5</ref> shows that the performance of most models with continuous prefix prompts exceeds the "None" one.</p><p>Inserting the prefix prompt is critical to achieving good performance. After increasing beyond 30 prefix prompt tokens, the performance for different patterns becomes unstable, and some patterns yield low performance, which hurts the performance of the voting method.</p><p>For the cloze prompt, we tune our model on 100 training instances by varying the cloze prompt length in {None, 2, 3, 4} while fixing other settings. The result is given in Figure <ref type="figure">6</ref>, and the overall performance of our model with the cloze prompt is better than the "None" one. Hence, inserting the cloze prompt is another essential factor in obtaining good performance. With the cloze prompt excess of three prompt tokens, the performance of each pattern does not improve significantly, and the performance of the voting method falls.</p><formula xml:id="formula_12">P 1 O 2 P 2 H 1 P 3 O 1 Overall, [MASK] is plausible. P 0 choice1: P 4 It is [MASK]. P 5 O 2 P 6 H 2 P 7 O 1 choice2: It is [MASK]. Optimal P 1 O 2 P 2 H 1 P 3 O 1 Overall, [MASK] is plausible. P 0 choice1: P 4 [MASK]. P 5 O 2 P 6 H 2 P 7 O 1 choice2: [MASK]. Template1 P 1 O 2 P 2 H 1 P 3 O 1 Overall, [MASK] is plausible. P 0 P 4 It is [MASK]. P 5 O 2 P 6 H 2 P 7 O 1 It is [MASK]. Template2 P 1 O 2 P 2 H 1 P 3 O 1 [MASK] P 0 choice1: P 4 It is [MASK]. P 5 O 2 P 6 H 2 P 7 O 1 choice2: It is [MASK]. P 1 O 2 P 2 H 1 P 3 O 1 [MASK] P 0 P 4 [MASK]. P 5 O 2 P 6 H 2 P 7 O 1 [MASK].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Template3</head><p>Template4 Prompt Engineering Apart from the continuous prompt in our designed prompt template, there are some tokens in natural textual form and discrete non-tunable tokens. As shown in Figure <ref type="figure">8</ref>, we gradually remove different portions of these discrete textual tokens and evaluate their importance. The performance shown in Table <ref type="table" target="#tab_17">11</ref> demonstrates that all discrete textual prompts are essential for achieving satisfactory performance compared with those without manual tips (i.e., Template 4). Among all discrete prompt tokens, the portion "It is &lt;mask&gt;" significantly affects the performance of our model as this discrete part facilitates eliciting the evaluation of PLMs on the plausibility of two hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Size</head><p>We compare the performance of various T5 model sizes in both few-shot and full training configurations. As demonstrated in Figure <ref type="figure" target="#fig_1">7</ref>, increasing the model size from T5-Large to T5-11B results in an average accuracy increase of around 10% for the few-shot setting and 6% for the full data learning setting. The results imply that a larger model encodes richer narrative knowledge and is advantageous for effectively eliciting more narrative knowledge, which is also our motivation for experimenting with pre-trained models as large as feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Training Instances</head><p>To further study the influence of various training examples. We randomly subsample the entire dataset to obtain smaller datasets of size {1, 5, 10, 20, 50}.</p><p>More training examples means more narrative context information our model can learn. Figure <ref type="figure">9</ref> shows that the average performance of six patterns of α-PACE increases as the number of training instances increases and consistently keeps a large gap against other baselines (e.g., RoBERTa). Interestingly, with a single training instance, the performance of the HO1O2 pattern (from Figure <ref type="figure">10</ref>) achieves 57.37% test accuracy, much greater than the other five patterns in our model. Therefore, employing an instance-specific narrative sequence pattern may give the pre-trained model a better outcome on limited training instances. This again verifies the motivation for why we involve six narrative sequence patterns in our method. Furthermore, all prompt-based methods received excellent performance in this few-shot setting, consistent with previous works <ref type="bibr" target="#b25">(Lester et al., 2021;</ref><ref type="bibr" target="#b30">Li and Liang, 2021)</ref>. Furthermore, when baselines train with ten times more data than our method, our single model still outperforms most of these baselines. For example, in Figure <ref type="figure">9</ref>, the performance of α-PACE O1HO2 training with five instances significantly exceeds almost all baseline training with 50 instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Discourse Connectives in Interpretability Section</head><p>The Penn Discourse Treebank 2.0 (PDTB 2.0) is a commonly used dataset in discourse parsing tasks and is a large-scale corpus containing 2,312 Wall Street Journal (WSJ) articles annotated by experts <ref type="bibr" target="#b46">(Prasad et al., 2008)</ref>. There are many discourse connectives in PDTB 2.0 that belong to various discourse relations. Discourse relations are of utmost importance for achieving textual coherence and are deemed an essential step for a multitude of downstream tasks that involve more context, including but not limited to question answering <ref type="bibr" target="#b22">(Jansen et al., 2014</ref>), text generation <ref type="bibr" target="#b4">(Bosselut et al., 2018)</ref>  and argument mining <ref type="bibr">(Liu et al., 2021c;</ref><ref type="bibr">Chan and Chan, 2023)</ref>.</p><p>To obtain the nearest neighbors discourse connectives of these continuous cloze prompts in our method, we compute the cosine similarity between the averaged representation of learned cloze prompt tokens and the embedding vector of discourse connectives. We acquired these discourse connectives from the Penn Discourse Treebank 2.0 <ref type="bibr" target="#b46">(Prasad et al., 2008)</ref>, a commonly used dataset in discourse analysis. These connectives are composed of two main categories of discourse relations: Contingency and Temporal. There are 23 connectives in these two categories after removing duplicates. The details of connectives can be found in Table <ref type="table" target="#tab_3">12</ref> and Figure <ref type="figure">11</ref> in Appendix. The top selected connectives for each sequence pattern are shown in Table <ref type="table" target="#tab_13">8</ref> in the few-shot setting. We use the data example utilized to illustrate the full-connect model in <ref type="bibr" target="#b3">Bhagavatula et al. (2020)</ref> and insert the top selected connectives in between the sentences to form a narrative text, as shown in Table <ref type="table" target="#tab_14">9</ref>. We observe that the learned discourse connectives can describe the same collection of sentences in various sentence sequences in a rational and acceptable way. More case studies are shown in Table <ref type="table" target="#tab_6">13</ref> in Appendix B.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 ChatGPT Capability on Abductive Commonsense Reasoning</head><p>The impressive ability of instruction-following large language models (e.g., <ref type="bibr">ChatGPT (OpenAI, 2022)</ref> and <ref type="bibr">GPT-4 (OpenAI, 2023)</ref>) has been exhibited by many studies <ref type="bibr" target="#b6">(Bubeck et al., 2023;</ref><ref type="bibr" target="#b2">Bang et al., 2023;</ref><ref type="bibr" target="#b24">Kocon et al., 2023;</ref><ref type="bibr">Chan et al., 2023a;</ref><ref type="bibr" target="#b53">Taori et al., 2023;</ref><ref type="bibr" target="#b12">Chiang et al., 2023;</ref><ref type="bibr" target="#b23">Jiang et al., 2023)</ref>. There are some challenges remain unresolved such as the associated ethi-cal implications and privacy concerns <ref type="bibr" target="#b27">(Li et al., 2023;</ref><ref type="bibr" target="#b52">Susnjak, 2022;</ref><ref type="bibr" target="#b36">Lukas et al., 2023)</ref>. In this work, we are curious about the capability of Chat-GPT on zero-shot abductive commonsense reasoning tasks. Hence, we test the ability of Chat-GPT<ref type="foot" target="#foot_1">2</ref> with four designed templates on the test set of αNLI task. ChatGPT prompt reformulate the task as the multi-choice questions to predict the class label by following <ref type="bibr" target="#b2">Bang et al. (2023)</ref>.</p><p>ChatGPT T askConsistency concatenates the O 1 ,O 2 , and H j as two narrative sentence sequences, which is the same as the prompt template shown in Figure <ref type="figure">2</ref>. ChatGPT Sample-and-Marginalize is to sample six times with an individual prompt template.</p><p>ChatGPT General &amp; Task Consistency utilizes six narrative patterns as the input template and each time only feeds only one narrative pattern. Furthermore, it is imperative to note that the input template, which incorporates in-context learning, is heavily dependent on the chosen training examples that form the prefix demonstration of the prompt template. The performance of in-context learning is subject to high variance based on the specific examples chosen, the quantity of examples, as well as the order in which they are presented. Consequently, this particular template has been excluded from consideration in this particular section. The performance of the random guess model is derived via the averaging of the results obtained from five distinct iterations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance of our method with different numbers of prefix continuous prompt tokens (p 0 , p 4 ) on the test dataset using 100 training instances. The red line indicates the performance of α-PACE General &amp; Task Consistency .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Performance (test accuracy %) comparison on various T5 model sizes in the few-shot and full training settings on the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Figure 9: Model performance comparison by using various numbers of the training instances on the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The label word set on αNLI task.</figDesc><table><row><cell>1 i , H 1 i ,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Statistics of ART and αNLI leaderboard data.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>The accuracy (%) is evaluated on the αNLI task. The approximation of learnable parameters for all models is displayed in Table10in Appendix A.5.</figDesc><table><row><cell>Fine-Tuning General Consistency (T5) means the fine-tuned</cell></row><row><cell>T5 model with our general narrative self-consistent</cell></row><row><cell>framework, which considers six narrative sequences.</cell></row><row><cell>α-PACE O1HO2 &amp; w/o Consistency means this model with a</cell></row><row><cell>single mask to predict the "choice1" or "choice2" and</cell></row><row><cell>the O1HO2 pattern is the best model among all patterns.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>report the main experimen-</cell></row><row><cell>tal results on the αNLI task, from which we de-</cell></row><row><cell>rive the following conclusions. First, our model</cell></row><row><cell>significantly outperforms all competitive baselines</cell></row><row><cell>on the αNLI task. Specifically, our method (i.e.,</cell></row><row><cell>α-PACE HO1O2 ) achieved a considerable improve-</cell></row><row><cell>ment of 5.36% on the development set, 4.15% on</cell></row><row><cell>the test set over the fine-tuning of the T5 model</cell></row><row><cell>in the αNLI task. It demonstrates that our model</cell></row><row><cell>effectively utilizes a task-specific self-consistent</cell></row><row><cell>method to validate the model's output and final-</cell></row><row><cell>ize a consistent answer. Second, α-PACE HO1O2</cell></row><row><cell>excels the prompt-based baselines (e.g., Prefix-</cell></row><row><cell>Tuning and Prompt-Tuning) with at least 5.85%</cell></row></table><note><p>test accuracy. This result exhibits the exact contribution of the task-specific self-consistent tailored prompt tuning-based model. Third, by adopting the general self-consistent narrative prompts, α-PACE General &amp; Task Consistency obtains 92.54% test</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>The accuracy (%) is evaluated on the test dataset from the αNLI task leaderboard. The approximation of learnable parameters for all models is displayed in Table10in Appendix A.5.</figDesc><table><row><cell>.18</cell></row></table><note><p>accuracy and 92.01% accuracy on the leaderboard test set. This result demonstrates that eliciting the inter-sentential coherence from the pre-trained language model and utilizing a general self-consistent framework considering six narrative sequences can partially solve this abductive reasoning task.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Ablation study in the joint probability for task-specific self-consistency and general narrative self-consistency on our model with HO1O2 patterns. α-PACE SM means adopting the sample-and-marginalize method proposed by<ref type="bibr" target="#b55">Wang et al. (2022)</ref> on our prompt model without the task-specific discrete prompt tokens.</figDesc><table><row><cell>Model</cell><cell>Dev (%)</cell><cell>Test (%)</cell></row><row><cell>α-PACEFirst</cell><cell>87.41</cell><cell>86.94</cell></row><row><cell>α-PACESecond</cell><cell>88.98</cell><cell>88.09</cell></row><row><cell>α-PACEThird</cell><cell>90.60</cell><cell>89.93</cell></row><row><cell>α-PACEFirst &amp; Second</cell><cell>88.78</cell><cell>89.03</cell></row><row><cell>α-PACEFirst &amp; Third</cell><cell>90.24</cell><cell>89.48</cell></row><row><cell>α-PACESecond &amp; Third</cell><cell>91.05</cell><cell>90.38</cell></row><row><cell>α-PACESM</cell><cell>91.20</cell><cell>90.13</cell></row><row><cell>α-PACEGeneral Consistency</cell><cell>91.78</cell><cell>90.64</cell></row><row><cell>α-PACESM &amp; Task Consistency</cell><cell>92.43</cell><cell>92.06</cell></row><row><cell>α-PACEGeneral &amp; Task Consistency</cell><cell>93.15</cell><cell>92.54</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Model accuracy (%) using 100 training instances compared with prompt-based models. We report the mean and standard deviation of five runs with different random seeds.</figDesc><table><row><cell>Model</cell><cell>Dev (%)</cell><cell>Test (%)</cell></row><row><cell>BERT-large</cell><cell cols="2">49.96±0.73 50.79±0.60</cell></row><row><cell>RoBERTa-large</cell><cell cols="2">58.20±2.78 58.68±2.80</cell></row><row><cell>McQueen</cell><cell cols="2">60.38±3.23 58.71±2.25</cell></row><row><cell>ege-RoBERTa-large</cell><cell cols="2">65.80±4.30 65.18±3.27</cell></row><row><cell>L2R 2</cell><cell cols="2">64.81±2.40 65.68±3.33</cell></row><row><cell>Fine-Tuning(T5)</cell><cell cols="2">69.57±2.01 71.18±2.06</cell></row><row><cell>Prefix-Tuning(T5)</cell><cell cols="2">73.96±5.36 72.29±5.10</cell></row><row><cell>Prompt-Tuning(T5)</cell><cell cols="2">76.34±1.70 75.38±1.83</cell></row><row><cell>α-PACEO1HO2(T5)</cell><cell cols="2">82.25±1.09 81.90±1.22</cell></row><row><cell cols="3">α-PACEGeneral Consistency(T5) 83.48±0.93 83.15±0.93</cell></row><row><cell>Model</cell><cell cols="2">Test (%)</cell></row><row><cell>Random</cell><cell></cell><cell>50.40</cell></row><row><cell>ChatGPTPrompt</cell><cell></cell><cell>71.07</cell></row><row><cell>ChatGPTTask Consistency</cell><cell></cell><cell>72.57</cell></row><row><cell cols="2">ChatGPTSample-and-Marginalize</cell><cell>73.17</cell></row><row><cell cols="2">ChatGPTGeneral &amp; Task Consistency</cell><cell>74.42</cell></row></table><note><p><p>Setting</p>Few-Shot Setting Comparing with Promptbased methods With the sampled 100 training in-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>The performance of ChatGPT performs on the test set of αNLI task. ChatGPT T askConsistency means utilizing the concatenate the O 1 ,O 2 , and H j as the HO1O2 narrative patterns, instead of ChatGPT P rompt treat it as multi-choice questions.ChatGPT Sample-and-Marginalize means a prompt template sampling six times while the ChatGPT General &amp; Task Consistency means sampling with six narrative patterns.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Top selected connectives for different patterns based on the model performance in the fewshot learning setting.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Case study for discourse connectives of different model patterns using the same case. The learned connectives are indicated in boldface.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>The approximation of tunable parameters for models. Most baselines use RoBERTa-large as the backbone model, and their tunable parameters are approximated to be similar.</figDesc><table><row><cell>22+</cell><cell>+22</cell><cell>22+</cell></row><row><cell>2+2</cell><cell>+22</cell><cell>2+2</cell></row><row><cell>$FFXUDF\</cell><cell></cell><cell></cell></row><row><cell>1RQH</cell><cell cols="2">3UHIL[&amp;RQWLQXRXV3URPSW/HQJWK</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>, p 2 , p 3 , p 5 , p 6 , p 7 ) on the test dataset using 100 training instances. The red line indicates the performance of α-PACE General &amp; Task Consistency .</figDesc><table><row><cell cols="2">22+</cell><cell></cell><cell>+22</cell><cell>22+</cell><cell></cell></row><row><cell cols="2">2+2</cell><cell></cell><cell>+22</cell><cell>2+2</cell><cell></cell></row><row><cell>$FFXUDF\</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1RQH</cell><cell cols="4">&amp;OR]H&amp;RQWLQXRXV3URPSW/HQJWK</cell><cell></cell></row><row><cell cols="6">Figure 6: Performance of our method with dif-</cell></row><row><cell cols="6">ferent numbers of cloze continuous prompt tokens</cell></row><row><cell>$FFXUDF\ (p 1 7VPDOO</cell><cell>7EDVH</cell><cell>)XOO</cell><cell>7ODUJH 76L]H</cell><cell>7%</cell><cell>7%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 11 :</head><label>11</label><figDesc>Figure8: α-PACE prompt template searching. The "Optimal Templates" is the finalized optimal template for implementing experiments to compare with extensive baselines. The red rectangle highlights the modified parts. Ablation study on the discrete textual prompt tokens of α-PACE on αNLI task in few-shot (100 instances) setting.</figDesc><table><row><cell>Models</cell><cell>Dev</cell><cell>Test</cell></row><row><cell cols="3">Optimal Template 82.25±1.09 81.90±1.22</cell></row><row><cell>Template 1</cell><cell cols="2">81.81±0.59 80.69±1.19</cell></row><row><cell>Template 2</cell><cell cols="2">81.78±0.34 81.59±1.90</cell></row><row><cell>Template 3</cell><cell cols="2">81.98±1.09 81.26±1.22</cell></row><row><cell>Template 4</cell><cell cols="2">80.61±0.91 80.48±0.95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>,</figDesc><table><row><cell>%(57ODUJH</cell><cell>3UHIL[7XQLQJ</cell></row><row><cell>5R%(57DODUJH /5 2</cell><cell>3URPSW7XQLQJ 3$&amp;(</cell></row><row><cell>HJH5REHUWD</cell><cell>3$&amp;(*HQHUDO&amp;RQVLVWHQF\</cell></row><row><cell>)LQH7XQLQJ</cell><cell></cell></row><row><cell>$FFXUDF\</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The source code is available at https://github.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The evaluation is performed in February 2023 by calling ChatGPT API.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors of this paper were supported by the <rs type="funder">NSFC Fund</rs> (<rs type="grantNumber">U20B2053</rs>) from the <rs type="funder">NSFC of China</rs>, the <rs type="funder">RIF</rs> (<rs type="grantNumber">R6020-19</rs> and <rs type="grantNumber">R6021-20</rs>) and the <rs type="funder">GRF</rs> (<rs type="grantNumber">16211520</rs> and <rs type="grantNumber">16205322</rs>) from <rs type="funder">RGC of Hong Kong</rs>. We also thank the support from <rs type="funder">NVIDIA AI Technology Center (NVAITC)</rs> and the <rs type="funder">UGC Research Matching Grants</rs> (<rs type="grantNumber">RMGS20EG01-D</rs>, <rs type="grantNumber">RMGS20CR11</rs>, <rs type="grantNumber">RMGS20CR12</rs>, <rs type="grantNumber">RMGS20EG19</rs>, <rs type="grantNumber">RMGS20EG21</rs>, <rs type="grantNumber">RMGS23CR05</rs>, <rs type="grantNumber">RMGS23EG08</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_aUREbHp">
					<idno type="grant-number">U20B2053</idno>
				</org>
				<org type="funding" xml:id="_4USVFd8">
					<idno type="grant-number">R6020-19</idno>
				</org>
				<org type="funding" xml:id="_qGfBhem">
					<idno type="grant-number">R6021-20</idno>
				</org>
				<org type="funding" xml:id="_XVD43UU">
					<idno type="grant-number">16211520</idno>
				</org>
				<org type="funding" xml:id="_wn66ugv">
					<idno type="grant-number">16205322</idno>
				</org>
				<org type="funding" xml:id="_fPTHZBz">
					<idno type="grant-number">RMGS20EG01-D</idno>
				</org>
				<org type="funding" xml:id="_RsB6sSv">
					<idno type="grant-number">RMGS20CR11</idno>
				</org>
				<org type="funding" xml:id="_9pAtqcA">
					<idno type="grant-number">RMGS20CR12</idno>
				</org>
				<org type="funding" xml:id="_AQU73Kp">
					<idno type="grant-number">RMGS20EG19</idno>
				</org>
				<org type="funding" xml:id="_DYRmqGZ">
					<idno type="grant-number">RMGS20EG21</idno>
				</org>
				<org type="funding" xml:id="_pZEMfFf">
					<idno type="grant-number">RMGS23CR05</idno>
				</org>
				<org type="funding" xml:id="_E5hECQm">
					<idno type="grant-number">RMGS23EG08</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pattern Example</head><p>O1O2H Meanwhile, Jimmy had to get a root canal. In fact, He did not feel a thing and the procedure went smoothly because Jimmy got plenty of novocaine for the procedure. O1HO2 In fact, Jimmy had to get a root canal.As Jimmy got plenty of novocaine for the procedure. As a result, He did not feel a thing and the procedure went smoothly. HO1O2 Because Jimmy got plenty of novocaine for the procedure. Meantime, Jimmy had to get a root canal. As a result, he did not feel a thing and the procedure went smoothly. HO2O1 If Jimmy got plenty of novocaine for the procedure. As a result, he did not feel a thing and the procedure went smoothly after Jimmy had to get a root canal. O2O1H Meanwhile, he did not feel a thing and the procedure went smoothly if Jimmy had to get a root canal as Jimmy got plenty of novocaine for the procedure. O2HO1 In fact, he did not feel a thing and the procedure went smoothly as long as Jimmy got plenty of novocaine for the procedure. Because Jimmy had to get a root canal. O1O2H Meanwhile, Jane was a professor teaching piano to students. In fact, Jane spent the morning sipping coffee and reading a book because none of Jane's students had a lesson that day. O1HO2 In fact, Jane was a professor teaching piano to students. As none of Jane's students had a lesson that day. As a result, Jane spent the morning sipping coffee and reading a book. HO1O2 Because none of Jane's students had a lesson that day. Meantime, Jane was a professor teaching piano to students. As a result, Jane spent the morning sipping coffee and reading a book. HO2O1 If none of Jane's students had a lesson that day. As a result, Jane spent the morning sipping coffee and reading a book after Jane was a professor teaching piano to students. O2O1H Meanwhile, Jane spent the morning sipping coffee and reading a book if Jane was a professor teaching piano to students as none of Jane's students had a lesson that day. O2HO1 In fact, Jane spent the morning sipping coffee and reading a book as long as none of Jane's students had a lesson that day because Jane was a professor teaching piano to students. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards a human-like opendomain chatbot</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Adiwardana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Nemade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/2001.09977</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Abductive and deductive change. Language</title>
		<author>
			<persName><forename type="first">Henning</forename><surname>Andersen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
			<biblScope unit="page" from="765" to="793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity</title>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Cahyawijaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nayeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Wilie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holy</forename><surname>Lovenia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiezheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willy</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quyet</forename><forename type="middle">V</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno>CoRR, abs/2302.04023</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Abductive commonsense reasoning</title>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discourse-aware neural rewards for coherent text generation</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="173" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">COMET: commonsense transformers for automatic knowledge graph construction</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4762" to="4779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Tat Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsha</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Túlio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<idno>GPT-4. CoRR, abs/2303.12712</idno>
		<title level="m">Sparks of artificial general intelligence: Early experiments with</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Make up your mind! adversarial generation of inconsistent natural language explanations</title>
		<author>
			<persName><forename type="first">Oana-Maria</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.382</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="4157" to="4165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discourse-aware prompt for argument impact classification</title>
		<author>
			<persName><forename type="first">Chunkit</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tszho</forename><surname>Chan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3587716.3587743</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 15th International Conference on Machine Learning and Computing, ICMLC &apos;23</title>
		<meeting>the 2023 15th International Conference on Machine Learning and Computing, ICMLC &apos;23<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="165" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Tianqing Fang, Xin Liu, and Yangqiu Song. 2023a. Chatgpt evaluation on sentence level relations: A focus on temporal, causal, and discourse relations</title>
		<author>
			<persName><forename type="first">Chunkit</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2304.14827</idno>
		<idno>CoRR, abs/2304.14827</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discoprompt: Path prediction prompt tuning for implicit discourse relation recognition</title>
		<author>
			<persName><forename type="first">Chunkit</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ginny</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>See</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.findings-acl.4</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2023</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-07-09">2023. July 9-14, 2023</date>
			<biblScope unit="page" from="35" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Story and discourse: Narrative structure in fiction and film</title>
		<author>
			<persName><forename type="first">Seymour</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chatman</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>Cornell University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Vicuna: An opensource chatbot impressing gpt-4</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>with 90%* chatgpt quality</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Scaling instruction-finetuned language models</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2210.11416</idno>
		<idno>CoRR, abs/2210.11416</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning event graph knowledge for abductive reasoning</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL/IJCNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5181" to="5190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hinrich Schütze, and Yoav Goldberg. 2021. Measuring and improving consistency in pretrained language models</title>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nora</forename><surname>Kassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhilasha</forename><surname>Ravichander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00410</idno>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1012" to="1031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reporting bias and knowledge acquisition</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AKBC</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">WARP: Word-level Adversarial ReProgramming</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Hambardzumyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrant</forename><surname>Khachatrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.381</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021-05">May. 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4921" to="4933" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">PTR: prompt tuning with rules for text classification</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR, abs/2105.11259</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deberta: decoding-enhanced bert with disentangled attention</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-05-03">2021. May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>OpenReview. net</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<author>
			<persName><forename type="first">Jerry</forename><forename type="middle">R</forename><surname>Hobbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">E</forename><surname>Stickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><forename type="middle">E</forename><surname>Appelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">A</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interpretation as abduction</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="69" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discourse complements lexical semantics for nonfactoid answer reranking</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/p14-1092</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computer Linguistics</publisher>
			<date type="published" when="2014-06-22">2014. June 22-27, 2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="977" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Lion: Adversarial distillation of closed-source large language model</title>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunkit</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2305.12870</idno>
		<idno>CoRR, abs/2305.12870</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Chatgpt: Jack of all trades, master of none</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kocon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Cichecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliwier</forename><surname>Kaszyca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Kochanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominika</forename><surname>Szydlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Baran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julita</forename><surname>Bielaniewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Gruza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arkadiusz</forename><surname>Janz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamil</forename><surname>Kanclerz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Kocon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bartlomiej</forename><surname>Koptyra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wiktoria</forename><surname>Mieleszczenko-Kowszewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Milkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Oleksy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Piasecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Wojtasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Wozniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Przemyslaw</forename><surname>Kazienko</surname></persName>
		</author>
		<idno>CoRR, abs/2302.10724</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.243</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-07-11">2021. 7-11 November, 2021</date>
			<biblScope unit="page" from="3045" to="3059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A knowledge-level account of abduction</title>
		<author>
			<persName><forename type="first">Hector</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="1061" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Multi-step jailbreaking privacy attacks on chatgpt</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dadi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingshi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2304.05197</idno>
		<idno>CoRR, abs/2304.05197</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">2021a. Interactive model with structural loss for language-based abductive reasoning</title>
		<author>
			<persName><forename type="first">Linhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
		</author>
		<idno>CoRR, abs/2112.00284</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">2021b. UNIMO: towards unified-modal understanding and generation via cross-modal contrastive learning</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guocheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">August 1-6, 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2592" to="2607" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kagnet: Knowledge-aware graph networks for commonsense reasoning</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamin</forename><surname>Liang ; Xinyue Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.353</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2021. August 1-6, 2021. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2829" to="2839" />
		</imprint>
	</monogr>
	<note>EMNLP-IJCNLP</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">2021a. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno>CoRR, abs/2107.13586</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Zhilin Yang, and Jie Tang. 2021b. GPT understands, too. CoRR, abs/2103.10385</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Yangqiu Song, and Xin Jiang. 2021c. Exploring discourse structures for argument impact classification</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiefu</forename><surname>Ou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL/IJCNLP</title>
		<imprint>
			<biblScope unit="page" from="3958" to="3969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">UNICORN on RAINBOW: A universal commonsense reasoning model on a new multitask benchmark</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021-02-02">2021. February 2-9, 2021</date>
			<biblScope unit="page" from="13480" to="13488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Analyzing leakage of personally identifiable information in language models</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Lukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Salem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Tople</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Wutschitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Zanella Béguelin</surname></persName>
		</author>
		<idno>CoRR, abs/2302.00539</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">How additional knowledge can improve natural language commonsense question answering?</title>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyay</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuntal</forename><surname>Kumar Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<idno>CoRR, abs/1909.08855</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A corpus and cloze evaluation for deeper understanding of commonsense stories</title>
		<author>
			<persName><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1098</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The role of coherence in constructing and evaluating abductive explanations</title>
		<author>
			<persName><forename type="first">Tou</forename><surname>Hwee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of the 1990 Spring Symposium on Automated Abduction</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">TR</biblScope>
			<biblScope unit="page" from="90" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Susana</forename><surname>Onega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">García</forename><surname>Landa</surname></persName>
		</author>
		<title level="m">Narratology: an introduction</title>
		<imprint>
			<publisher>Routledge</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">GPT-4 technical report</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<idno>CoRR, abs/2303.08774</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Chatgpt: Optimizing language models for dialogue</title>
		<author>
			<persName><surname>Tb Openai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Social commonsense reasoning with multi-head knowledge attention</title>
		<author>
			<persName><forename type="first">Debjit</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anette</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2969" to="2980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Approaches to abductive reasoning: an overview</title>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Paul</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00849080</idno>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="152" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Collected Papers of Charles Sanders Peirce</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peirce</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1974">1974</date>
			<publisher>Harvard University Press</publisher>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The Penn Discourse TreeBank 2.0</title>
		<author>
			<persName><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)</title>
		<meeting>the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA)</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning how to ask: Querying lms with mixtures of soft prompts</title>
		<author>
			<persName><forename type="first">Guanghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5203" to="5212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Zero: Memory optimizations toward training trillion parameter models</title>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Toward a definition of narrative. The Cambridge Companion to Narrative, 22. Timo Schick and Hinrich Schütze. 2021. Exploiting cloze-questions for few-shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Marie-Laure</forename><surname>Ryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="255" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan</title>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-10">2018. July 10-15, 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4603" to="4611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Chatgpt: The end of online exam integrity?</title>
		<author>
			<persName><forename type="first">Teo</forename><surname>Susnjak</surname></persName>
		</author>
		<idno>CoRR, abs/2212.09292</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target="https://github.com/tatsu-lab/stanford_alpaca" />
		<title level="m">Stanford alpaca: An instruction-following llama model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Narrative theories and narrative discourse</title>
		<author>
			<persName><forename type="first">Slávka</forename><surname>Tomaščíková</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the Transilvania University of Braşov</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">51</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Selfconsistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2203.11171</idno>
		<idno>CoRR, abs/2203.11171</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Factual probing is [MASK]: learning vs. learning to recall</title>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5017" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">L2r 2 : Leveraging ranking for abductive reasoning</title>
		<author>
			<persName><forename type="first">Yunchang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1961" to="1964" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
