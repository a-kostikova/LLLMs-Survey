<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge Injection with Perturbation-based Constrained Attention Network for Word Sense Disambiguation</title>
				<funder ref="#_aScANZv">
					<orgName type="full">SCAT, JKA, Kajima Foundation</orgName>
				</funder>
				<funder ref="#_xFms2R5">
					<orgName type="full">JSPS KAKENHI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Fumiyo</forename><surname>Fukumoto</surname></persName>
							<email>fukumoto@yamanashi.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Interdisciplinary Graduate School</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<addrLine>of Yamanashi 4-3-11</addrLine>
									<postCode>400-8511</postCode>
									<settlement>Takeda, Kofu</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shou</forename><surname>Asakawa</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Engineering</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<addrLine>of Yamanashi 4-3-11</addrLine>
									<postCode>400-8511</postCode>
									<settlement>Takeda, Kofu</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge Injection with Perturbation-based Constrained Attention Network for Word Sense Disambiguation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">78CB5B0CB25C7B0A6F293FACC4EA648A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised Word Sense Disambiguation (WSD) has been studied intensively for over three decades. However, disentangling diverse contexts is still a challenging problem. This paper addresses the problem and proposes a Perturbation-based constrained attention network (Pconan) for injecting lexical knowledge derived from the WordNet. The Pconan allows modeling beneficial dependencies between the segments/words within the input sequence with the mask-attention technique. We incorporate a perturbation method into our model to mitigate the overfitting problem resulting from intensive learning. The experimental results by using a benchmark dataset show that our method is comparable to the SOTA WSD methods. Our source codes are available online 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Computational lexicons such as WordNet <ref type="bibr" target="#b10">(George A. Miller and Miller, 1990)</ref> and ACQUILEX <ref type="bibr" target="#b8">(Edward, 1991)</ref> have been popular knowledge resources for NLP tasks. There is a large body of WSD work based on neural networks that leverage rich information derived from these resources <ref type="bibr">(Luo et al., 2018b;</ref><ref type="bibr" target="#b25">Vial et al., 2019;</ref><ref type="bibr" target="#b14">Kumar and Talukdar, 2019;</ref><ref type="bibr" target="#b13">Huang et al., 2019;</ref><ref type="bibr" target="#b5">Blevins and Zettlemoyer, 2020;</ref><ref type="bibr" target="#b3">Bevilacqua and Navigli, 2020;</ref><ref type="bibr" target="#b6">Conia and Navigli, 2021)</ref>. They demonstrated that the external knowledge base is beneficial to disambiguate senses. However, it is often the case that the inputs are long sequences. It hampers WSD attempts with external knowledge. Several authors have attempted to alleviate the issue. <ref type="bibr" target="#b5">(Blevins and Zettlemoyer, 2020)</ref> independently embedded the target word with its surrounding contexts and the dictionary definition of each sense. <ref type="bibr" target="#b3">(Bevilacqua and Navigli, 2020)</ref> extended <ref type="bibr" target="#b5">(Blevins and Zettlemoyer, 2020)</ref> method and integrated relational knowledge into the architecture through a simple additional 1 https://github.com/fukumoto-lab/Pconan sparse dot product operation. Their results by using a benchmark dataset were beyond 80%.</p><p>The attention mechanism is also one of the major techniques to capture long-term dependencies on their sequence <ref type="bibr" target="#b24">(Vaswani et al., 2017)</ref>. <ref type="bibr">(Luo et al., 2018a)</ref> introduced a co-attention mechanism to generate co-dependent representations to capture both word-and sentence-level information. Their assumption is that lexical knowledge such as gloss sentences and context sentences can help each other to highlight the important words within these sentences, while the sense definition candidates do not all at once take into account during the training process. Several authors focused on the issue <ref type="bibr" target="#b26">(Wang and Wang, 2021)</ref>. <ref type="bibr">(Barba et al., 2021b)</ref> proposed a joint-learning that learns the input context and target word definitions jointly. Subsequently, <ref type="bibr">(Barba et al., 2021a)</ref> attempted to process the disambiguation of a target word to be conditioned not only on its context but also on the explicit senses assigned to the surrounding words, while their model has not leveraged external lexical knowledge.</p><p>Inspired by the previous work mentioned above, we propose a method to inject lexical knowledge, i.e. example sentences from WordNet to effectively learn a context sentence and lexical knowledge simultaneously. Our model called Perturbationbased constrained attention network (Pconan) allows the modeling of dependencies between the segments/words within the input sequence with the mask-attention technique. The technique makes it possible to concentrate on learning beneficial dependencies only, entirely discarding the others. However, this causes an overfitting problem, especially when the available training data is limited. To alleviate this issue, the Pconan utilizes the perturbation technique <ref type="bibr" target="#b21">(Sato et al., 2019)</ref>. More specifically, we add noises to the training data and the model learns sense distinctions of the same word by using these noisy data to assign the correct label. The social and intellectual &lt;d&gt; world &lt;/d&gt; .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[CLS]</head><p>[SEP] &lt;def&gt; people in ... as a whole.</p><p>&lt;def&gt; the world of scholarship and science. &lt;def&gt; people in ... shared interest .</p><p>&lt;def&gt; he is a hero in the eyes of the public. &lt;def&gt; belonging to the modern era; since the Middle Ages.</p><p>[SEP] &lt;def&gt; he is a hero in the eyes of the public.  Let SI tw(i) be a sense inventory, tw(i) be a current, i-th target word appeared in the context sentence c.</p><p>Let also</p><formula xml:id="formula_0">D tw(i) = &lt;def&gt;tw(i) g 1 1 • • • tw(i) g 1 |g 1 | &lt;def&gt;es(i) s 1 1 • • • es(i) s 1 |s 1 | &lt;def&gt;tw(i) g N 1 • • • tw(i) g N |g N | &lt;def&gt; es(i) s N 1 • • • es(i) s N</formula><p>|s N | be the CDs for tw(i), along with ESs, where tw(i) g k j and es(i) s k j be the j-th word of the k-th CD g k , and ES s k (1 ≤ k ≤ N ), respectively. N is the total number of CDs and &lt;def&gt; stands for the start of each segment. Let also D = s 1 ,• • • , s i-1 be a sequence concatenating the context definitions of the senses previously assigned to w</p><formula xml:id="formula_1">1 ,• • • , w i-1 .</formula><p>For a given c including &lt;d&gt;tw(i)&lt;/d&gt;, we create an input sequence</p><formula xml:id="formula_2">[CLS] c [SEP] D tw(i) D [SEP].</formula><p>Here, the inputs to each encoder are padded with DEBERTa-specific start and end symbols: [CLS] and [SEP] <ref type="bibr" target="#b12">(He et al., 2021)</ref>. The goal of the WSD task is for the input sequence, to find the correct definition g ∈ SI tw(i) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Constrained Attention Network</head><p>Our model applies (1) identifying keywords, and (2) transferring information to learn relevant contextual features for WSD. The top of Figure 1 illustrates an example of the input sequence X, i.e.</p><formula xml:id="formula_3">[CLS] c [SEP] D tw(i) D [SEP].</formula><p>For the input sequence</p><formula xml:id="formula_4">X = [CLS] c [SEP] D tw(i) D [SEP],</formula><p>we apply the so-called hard attention technique <ref type="bibr" target="#b29">(Xu et al., 2015;</ref><ref type="bibr" target="#b22">Shen et al., 2018)</ref> that a model concentrates solely on learning beneficial dependencies to identify keywords in the sequence, entirely discarding the others. The middle picture of Figure <ref type="figure" target="#fig_1">1</ref> illustrates masked attention for a bidimensional matrix. The words aligned on the horizontal axis are heads, and those aligned on the vertical axis are dependents. As illustrated in Figure <ref type="figure" target="#fig_1">1</ref>, we discard some segment pairs, each of which consists of a head and dependence on the sequence by masking them as these pairs are not semantically related to each other and do not include keywords that are beneficial to identify the sense of the target word. in Figure <ref type="figure" target="#fig_1">1</ref>). The output is a sequence of words with attention weights. In Figure <ref type="figure" target="#fig_1">1</ref>, keywords having high weight values are marked in red. From the result of keyword identification, for each candidate definition, we created a sequence for the target candidate definition starting from &lt;def&gt; as follows:</p><p>• Context sentence segment perceived as immediately before the CD segment. ((i) in Figure <ref type="figure" target="#fig_1">1</ref>)</p><p>• &lt;def&gt; of the target CD has two branches. One is that it witnesses other CDs, their ES, and context definitions. ((ii) in Figure <ref type="figure" target="#fig_1">1</ref>) Another is that its ES. Relative position of the special token &lt;def&gt; is set to 1. ((iii) in Figure <ref type="figure" target="#fig_1">1</ref>)</p><p>As shown in "(2) Transferring information" of Figure <ref type="figure" target="#fig_1">1</ref>, the structure provides a method to leverage the relative positions of &lt;def&gt; including keywords representation to learn a model more accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Architecture</head><p>Figure <ref type="figure">2</ref> illustrates an overview of our model. Let E = [e 1 , e 2 , • • • , e L ] ∈ R d×L refers to the concatenation of word/token embeddings for the input sequence X, and E rpm ∈ R d×L be relative position matrix of X. Here, e k and its relative position is obtained by DEBERTa encoding. d refers to the dimension of embedding, and L is the number of words/tokens in X. We further utilize a special symbol in X so that the model can capture the difference between the context sentence and others. Let also r k ∈ R d be a perturbation vector for the k-th word x k in the input X. The perturbed input embedding êk is computed based on the stochastic gradient descent as follows:</p><formula xml:id="formula_5">êk = e k + r k , r k = ϵ c k ||c k || , c k = ▽ êk L 1 (θ),<label>(1)</label></formula><p>where ϵ refers to a hyperparameter that controls the norm of the perturbation and L 1 (θ) indicates cross-entropy loss which is given by: </p><formula xml:id="formula_6">L 1 (θ) = - M i=1 N i j=1 y ij log ŷij , (<label>2</label></formula><formula xml:id="formula_7">)</formula><p>where M is the total number of target words in the training data, and N i is the word sense number of the i-th target word, y ij and ŷij are true and predicted probability of the i-th target word that belongs to the j-th candidate definition. As shown in Figure <ref type="figure">2</ref>, for each embedding e k , we apply Eq.( <ref type="formula" target="#formula_5">1</ref>) and obtain perturbed sequence. Let Ê</p><formula xml:id="formula_8">= [ê 1 , ê2 , • • • , êL ] ∈ R d×L</formula><p>be the concatenation of perturbed sequence. Our constrained attention networks aim to learn relevant contextual keywords for WSD, and finally output attention weights A for the inputs, Ê and E rpm . We further obtain Â by applying the mask-attention procedure to A. Ê is linearly projected and we obtain V c . We multiply V c and Â by matrix multiplication. Keyword information is transferred by this operation. The result is fed into a feed-forward network, combined with layer normalization and residual connection. Each encoder layer takes the output of the previous layer as input and the number of layers is N . We obtain the matrix</p><formula xml:id="formula_9">H = [h 1 , h 2 , • • • , h L ] ∈ R d×L as</formula><p>an output of the encoder. Each &lt;def&gt; vector that corresponds to the start of the candidate definition is extracted from the matrix H, passed to the fully connected layer and finally, we obtain the probability score ŷij by the softmax function. The final loss L(θ) obtained by our model is given by:</p><formula xml:id="formula_10">L(θ) = L 1 (θ) + αL 2 (θ), L 2 (θ) = 1 |D| D KL(p(•|X; θ)||p(•|X, r; θ)),</formula><p>(3) where D refers to the number of training instances. α indicates a hyperparameter. KL(p||q) denotes KL-divergence between distributions p and q, and r shows a concatenated vector of r k for all x k (1 ≤ k ≤ L). We train the whole architecture to minimize L(θ). Similar to <ref type="bibr">(Barba et al., 2021a)</ref> approach, at training time, we use teacher forcing on the context definitions, and at prediction time, we use a greedy decoding strategy and the model deems g as the most likely definition for a current target word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We performed the experiments on English allwords fine-grained WSD datasets (Alessandro Raganato, 2017b), using SemCor <ref type="bibr" target="#b11">(George A. Miller and Bunker, 1993)</ref> as the training corpus. The datasets are Senseval/SemEval data consisting of Senseval-2 (SE2) <ref type="bibr" target="#b7">(Edmonds and Cotton, 2001)</ref>, Senseval-3 (SE3) <ref type="bibr" target="#b23">(Snyder and Palmer, 2004)</ref>, SemEval-07 (SE07) <ref type="bibr" target="#b20">(Sameer Pradhan and Palmer, 2007)</ref>, SemEval-13 (SE13) (Roberto <ref type="bibr" target="#b19">Navigli and Vannella, 2013)</ref>, and SemEval-15 (SE15) <ref type="bibr" target="#b18">(Moro and Navigli, 2015)</ref>. Similar to other related work, we chose SE07 as the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model settings and evaluation metrics</head><p>We utilized the hyperparameters with the best performance on SE07 as follows: The dimension of word embedding d was 1,024. The number of maximum words per batch was 1,536. The gradient accumulation and the maximum number of steps were 8.0 and 25,000, respectively. The number of layer N of DEBERTa was 24 and the learning rate was 3e-6. The initial perturbation was set to 1e-2 and the ϵ value in Eq.( <ref type="formula" target="#formula_5">1</ref>) was 3e-6. α in Eq. ( <ref type="formula">3</ref>) was set to 1.0. We used Rectified Adam as optimizer <ref type="bibr" target="#b27">(Weijie Liu, 2020)</ref>. The experiments were conducted by using Pytorch on Nvidia GeForce RTX A6000 (48GB memory). We used the F1-score following (Alessandro Raganato, 2017b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison Models</head><p>We compared our model with the SOTA methods; MSF-SemCor as a frequency based approach, SVC <ref type="bibr" target="#b25">(Vial et al., 2019)</ref>, GlossBERT <ref type="bibr" target="#b13">(Huang et al., 2019)</ref>, ARES (Bianca Scarlini, 2020), EWISER <ref type="bibr" target="#b3">(Bevilacqua and Navigli, 2020)</ref>, BEM <ref type="bibr" target="#b5">(Blevins and Zettlemoyer, 2020)</ref>, WMLC <ref type="bibr" target="#b6">(Conia and Navigli, 2021)</ref>, HCAN <ref type="bibr">(Luo et al., 2018a)</ref>, and KELESC <ref type="bibr" target="#b30">(Zhang et al., 2022)</ref> as a knowledge source integration approach, SACE <ref type="bibr" target="#b26">(Wang and Wang, 2021)</ref>, ES-CHER <ref type="bibr">(Barba et al., 2021b)</ref>, and ConSec <ref type="bibr">(Barba et al., 2021a)</ref> as a joint learning approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>The results are summarized in Table <ref type="table" target="#tab_6">2</ref>. The performance of joint-learning approaches was better than those of frequency-based and knowledge source integration approaches in all test sets and partof-speech (POS) patterns. This indicates that the model learned the input context and target word definitions jointly are effective for disambiguation. Our model was statistically significant compared with the second-best method for test sets and POS patterns except for SE3, 15, Adj, and Adv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablation study</head><p>We conducted ablation studies to empirically examine our mask-attention technique and perturbation (Prtb). Table <ref type="table" target="#tab_7">3</ref> shows the results. When we did not utilize the mask-attention and perturbation procedures, the F1-score was 82.1% which is no significant difference compared with ConSec (82.0%) even though ESs are injected. When we applied the mask-attention technique, the improvement was 0.5% at maximum, 82.6%. Among masked attentions, there is also no statistically significant difference between the masked attention (a) context sentence and ES pairs (82.5%) and the combination of (b)∼ (d) (82.3%). However, we gained 0.6% improvement by using (a) ∼ (d) and further gained 0.4% improvement by perturbation. From these observations, we can conclude that the perturbation approach helps the mask-attention procedure to boost the WSD task performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Qualitative analysis of errors</head><p>We performed an error analysis to provide feedback for further improvement of our method. The number of errors for each POS was 590 noun words, 420 for a verb, 120 for an adjective, and 38 for an adverb, 1,168 words in all. The average senses for these POS words were 6.7 for nouns, 12.2 for verbs, 5.9 for adjectives, and 5.2 for adverbs. We randomly picked up 100 words from 1,168 and found that there are mainly two types of errors:</p><p>1. Sense distribution: When the sense distribution of the target word in the training data is unbalanced, most of the target words tend to be assigned to the sense of having much training instances. This was the most frequent error type and 51 words were classified into this type.</p><p>2. The similarity between candidate definitions: When words that appear one candidate definition are semantically similar or the same as those of other candidate definitions, it is difficult to model beneficial dependencies to identify keywords. For example, in Figure <ref type="figure" target="#fig_3">3</ref>, as "move forward" appears in both candidate definitions and example sentence, only a few words such as "car" and "seat" are clues to predict beneficial dependencies which causes an error. 26 words were classified into this type.</p><p>We focused on example sentences extracted from WordNet as lexical knowledge. <ref type="bibr" target="#b25">(Vial et al., 2019;</ref><ref type="bibr" target="#b6">Conia and Navigli, 2021)</ref> utilized the semantic relationships between senses such as synonymy, hypernymy, and hyponymy derived from the Word-  Net and reported that the external knowledge contributes to improving WED performance. This is definitely worth trying with Pconan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We presented WSD approach for injecting lexical knowledg from the WordNet with perturbationbased constrained attention network. The comparative results with the SOTA WSD methods showed the effectiveness of our method. Future work will include: (i) evaluating our model by using other lexical knowledge such as the semantic relationship between senses, (ii) investigating other perturbation techniques <ref type="bibr" target="#b9">(Gal and Ghahramani, 2016;</ref><ref type="bibr" target="#b26">Wang and Wang, 2021)</ref> to improve the performance, and (iii) applying methods <ref type="bibr" target="#b15">(Liu et al., 2020;</ref><ref type="bibr" target="#b28">Xiong et al., 2021)</ref> to reduce the overall self-attention complexity for further advantages in efficacy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>a</head><label></label><figDesc>&lt;def&gt; the world of scholarship and science .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Identifying keyword and transferring information</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>[</head><label></label><figDesc>Context sentence] Skilled ringers use their wrists to &lt;d&gt; advance &lt;/d&gt; or retard the next swing so that one bell can swap places with another in the following change. [Candidate sense &amp; definition &amp; example sentence] advance#1 &lt;def&gt; move forward, also in the metaphorical sense. &lt;def&gt; Times marches on. advance#5 &lt;def&gt; cause to move forward &lt;def&gt; Can you move the car seat forward?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example with similar context definitions: The correct sense in &lt;d&gt; advance &lt;d&gt; is advance#5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>The social and intellectual &lt;d&gt; world &lt;/d&gt; . &lt;def&gt; belonging</head><label></label><figDesc>to the modern era; since the Middle Ages.</figDesc><table><row><cell>&lt;def&gt; people in ... shared interest .</cell></row><row><cell>&lt;def&gt; people in ... as a whole.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Words with attention weights (2) Transferring information &lt;def&gt; The social and intellectual &lt;d&gt; world &lt;/d&gt; .</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">people in ... shared interest .</cell><cell>&lt;def&gt; people in ... as a whole.</cell><cell>&lt;def&gt; ... public.</cell><cell>&lt;def&gt; ... Ages.</cell><cell>[SEP]</cell></row><row><cell>[CLS]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[SEP]</cell><cell>&lt;def&gt;</cell><cell>1</cell><cell>2 .... 13</cell><cell>14 15</cell><cell>23</cell><cell>24</cell><cell>25 ... 28 26 27 28</cell><cell>29 ... 39 40</cell><cell>41 ... 51 52</cell><cell>53</cell></row><row><cell>-10</cell><cell>-9</cell><cell>-8</cell><cell>-7</cell><cell>-6</cell><cell>-5</cell><cell>-4</cell><cell>-3 -2 -1</cell><cell>0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>(1) Identifying keyword The social and intellectual &lt;d&gt; world &lt;/d&gt; . &lt;def&gt; people</head><label></label><figDesc>in ... shared interest .</figDesc><table><row><cell>Context sentence</cell><cell></cell><cell>Candidate definitions &amp; Example sentence</cell></row><row><cell>[CLS]</cell><cell>[SEP]</cell><cell>&lt;def&gt; the world of scholarship and science.</cell></row><row><cell></cell><cell>&lt;def&gt; people in ... as a whole.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>&lt;def&gt; he</head><label></label><figDesc>is a hero in the eyes of the public. &lt;def&gt; belonging to the modern era; since the Middle Ages.</figDesc><table><row><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7 8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12... 23</cell><cell>24 25</cell><cell>26</cell><cell cols="2">27 28 29</cell><cell>30</cell><cell>31</cell><cell>32 33</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[SEP]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>34</cell><cell cols="3">35 36 ... 39 40 41 42</cell><cell cols="5">43 44 45 46 47 48 49 50 51 52 53 54</cell><cell>55</cell><cell>56</cell><cell>57 58</cell><cell>59</cell><cell>60 61 62 63</cell><cell>64</cell><cell>65 66</cell><cell>67</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Candidate definitions &amp; Example sentence</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Context definitions</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Masked attention</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>:ƒ;</cell><cell></cell><cell>:ƒ;</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>:";</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">:…;</cell><cell>: †;</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>:";</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">:…; : †;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">[CLS]</cell><cell>[SEP]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>:‹‹;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>:‹;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">the world of scholarship and science .</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">:‹‹‹;</cell><cell>16 17 18</cell><cell>19</cell><cell>20</cell><cell>21</cell><cell>22</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Table1shows pairs that we masked. For example, (b) pairs of candidate definitions and example sentences which do not correspond to each other are masked (purple box Masked attention between head and dependent.</figDesc><table><row><cell>Head</cell><cell>Dependent</cell></row><row><cell>(a) Context sentence</cell><cell>Example sentence</cell></row><row><cell cols="2">(b) Candidate definition Different example sentence</cell></row><row><cell>(c) Example sentence</cell><cell>Different candidate definition</cell></row><row><cell>(d) Example sentence</cell><cell>Different example sentence</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison: The best score is in boldface and the second best is underlined. * denotes the method (if any) whose score is not statistically significant compared to the best one. We used a t-test, p-value &lt; 0.01.</figDesc><table><row><cell></cell><cell></cell><cell>Dev Set</cell><cell></cell><cell cols="2">Test Sets</cell><cell></cell><cell cols="4">Concatenation of all Datasets</cell></row><row><cell></cell><cell>Model</cell><cell>SE7</cell><cell>SE2</cell><cell>SE3</cell><cell cols="4">SE13 SE15 Noun Verb Adj</cell><cell>Adv</cell><cell>ALL</cell></row><row><cell></cell><cell>MFS-SemCor</cell><cell>54.5</cell><cell>65.6</cell><cell>66.0</cell><cell>63.8</cell><cell>67.1</cell><cell>67.7</cell><cell>49.8 73.1</cell><cell>80.5</cell><cell>65.5</cell></row><row><cell></cell><cell>SVC (hypernyms)</cell><cell>69.5</cell><cell>77.5</cell><cell>77.4</cell><cell>76.0</cell><cell>78.3</cell><cell>79.6</cell><cell>65.9 79.5</cell><cell>85.5</cell><cell>76.7</cell></row><row><cell></cell><cell>GlossBERT</cell><cell>72.5</cell><cell>77.7</cell><cell>75.2</cell><cell>76.1</cell><cell>80.4</cell><cell>79.8</cell><cell>67.1 79.6</cell><cell>87.4</cell><cell>77.0</cell></row><row><cell></cell><cell>ARES</cell><cell>71.0</cell><cell>78.0</cell><cell>77.1</cell><cell>78.7</cell><cell>75.0</cell><cell>80.6</cell><cell>68.3 80.5</cell><cell>83.5</cell><cell>77.9</cell></row><row><cell></cell><cell>EWISER</cell><cell>71.0</cell><cell>78.9</cell><cell>78.4</cell><cell>78.9</cell><cell>79.3</cell><cell>81.7</cell><cell>66.3 81.2</cell><cell>85.8</cell><cell>78.3</cell></row><row><cell>SemCor</cell><cell>BEM WMLC HCAN</cell><cell>74.5 72.2 -</cell><cell>79.4 78.4 72.8</cell><cell>77.4 77.8 70.3</cell><cell>79.7 76.7 68.5</cell><cell>81.7 78.2 72.8</cell><cell>81.4 80.1 72.7</cell><cell>68.5 83.0 67.0 80.5 58.2 77.4</cell><cell>87.9 86.2 84.1</cell><cell>79.0 77.6 71.1</cell></row><row><cell></cell><cell>KELESC</cell><cell>76.7</cell><cell>82.2</cell><cell>78.1</cell><cell>82.2</cell><cell>83.0</cell><cell>84.3</cell><cell>69.4 84.0</cell><cell>86.7</cell><cell>81.2</cell></row><row><cell></cell><cell>SACE</cell><cell>76.3</cell><cell>82.4</cell><cell>81.1</cell><cell>82.5</cell><cell>83.7</cell><cell>84.1</cell><cell>72.2 86.4</cell><cell>89.0</cell><cell>81.9</cell></row><row><cell></cell><cell>ESCHER</cell><cell>76.3</cell><cell>81.7</cell><cell>77.8</cell><cell>82.2</cell><cell>83.2</cell><cell>83.9</cell><cell>69.3 83.8</cell><cell>86.7</cell><cell>80.7</cell></row><row><cell></cell><cell>ConSec</cell><cell>77.4</cell><cell>82.3</cell><cell>79.9</cell><cell>83.2</cell><cell>85.2</cell><cell>85.4</cell><cell>70.8 84.0</cell><cell>87.3</cell><cell>82.0</cell></row><row><cell></cell><cell>Pconan</cell><cell>79.8</cell><cell cols="2">83.8 81.1  *</cell><cell>83.9</cell><cell>84.7  *</cell><cell>85.6</cell><cell cols="3">73.8 84.8 89.0  *  83.0</cell></row><row><cell cols="2">Model</cell><cell></cell><cell>ALL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ConSec</cell><cell></cell><cell>82.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">w/o Prtb &amp; w/o (a), (b), (c), and (d) 82.1  *</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">w/o Prtb &amp; w/o (b), (c), and (d)</cell><cell>82.5  *</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">w/o Prtb &amp; w/o (a)</cell><cell></cell><cell>82.3  *</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">w/o Prtb</cell><cell></cell><cell>82.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Pconan</cell><cell></cell><cell>83.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Ablation test over the Pconan components: "w/o Prtb" refers to the result without pertubation. * denotes the method whose score is not statistically significant compared to the baseline, ConSec.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank anonymous reviewers for their comments and suggestions. This work is supported by <rs type="funder">SCAT, JKA, Kajima Foundation</rs>'s <rs type="programName">Support Program</rs>, and <rs type="funder">JSPS KAKENHI</rs> No.<rs type="grantNumber">23H03402</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_aScANZv">
					<orgName type="program" subtype="full">Support Program</orgName>
				</org>
				<org type="funding" xml:id="_xFms2R5">
					<idno type="grant-number">23H03402</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural sequence learning models for word sense disambiguation</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><forename type="middle">Delli</forename><surname>Bovi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1156" to="1167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2021a. ConSeC: Word sense disambiguation as continuous sense comprehension</title>
		<author>
			<persName><forename type="first">Edoardo</forename><surname>Barba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Procopio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<biblScope unit="page" from="1492" to="1503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2021b. Esc: Redesigning wsd with extractive sense comprehension</title>
		<author>
			<persName><forename type="first">Edonardo</forename><surname>Barba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Pasini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navigli</forename><surname>Roberto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on the North American Chappter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference on the North American Chappter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<biblScope unit="page" from="4661" to="4672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Breaking through the 80% glass ceiling: Raising the state of the art in word sense disambiguation by incorporating knowledge graph information</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2854" to="2864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">With more contexts comes better performance: Contextualized sense embeddings for all-round word sense disambiguation</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bianca</forename><surname>Scarlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Pasini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3528" to="3539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Moving down the long tail of word sense disambiguation with gloss-informed bi-encoders</title>
		<author>
			<persName><forename type="first">Terra</forename><surname>Blevins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Association for Computational Linguistics</title>
		<meeting>the 58th Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1006" to="1017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Framing Word Sense Disambiguation as a multi-label problem for model-agnostic knowledge integration</title>
		<author>
			<persName><forename type="first">Simone</forename><surname>Conia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3269" to="3275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Senseval2: Overview</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Edmonds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Cotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SENSEVAL2 Second International Workshop on Evaluating Word Sense Disambiguation Systems</title>
		<meeting>SENSEVAL2 Second International Workshop on Evaluating Word Sense Disambiguation Systems</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lexical issues in natural language processing</title>
		<author>
			<persName><forename type="first">Briscoe</forename><surname>Edward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Natural Language and Speech</title>
		<meeting>the Symposium on Natural Language and Speech</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="36" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1027" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Introduction to wordnet: An online lexical database</title>
		<author>
			<persName><forename type="first">Christiane</forename><forename type="middle">D</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gross George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Beckwith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="235" to="244" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A semantic concordance</title>
		<author>
			<persName><forename type="first">Randee</forename><surname>Tengi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Leacokc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">T</forename><surname>Bunker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Human Language Technology</title>
		<meeting>the workshop on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="303" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deberta: decoding-enhanced bert with disentangled attention</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GlossBERT: BERT for word sense disambiguation with gloss knowledge</title>
		<author>
			<persName><forename type="first">Luyao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3507" to="3512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Zero-shot word sense disambiguation using sense definition embeddings</title>
		<author>
			<persName><forename type="first">Sharmistha</forename><surname>Jat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><forename type="middle">Saxena</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sawan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5670" to="5681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Learning Representations</title>
		<meeting>the Eighth International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>ICLR 2020</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Leveraging gloss knowledge in neural word sense disambiguation by hierarchical co-attention</title>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexue</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaolin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1402" to="1411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Incorporating glosses into neural word sense disambiguation</title>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaolin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2473" to="2482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semeval2015 task 13: Multilingual all-words sense disambiguation and entity linking</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2015">2015. SemEval 2015</date>
			<biblScope unit="page" from="288" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semeval-2013 task 12: Multilingual word sense disambiguation</title>
		<author>
			<persName><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Vannella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Semantic Evaluation</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2013">2013. SemEval 2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="222" to="231" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (*SEM)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semeval-2007 task-17: English lexical sample, srl and all words</title>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Dligach Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)</title>
		<meeting>the Fourth International Workshop on Semantic Evaluations (SemEval-2007)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Effective adversarial regularization for neural machine translation</title>
		<author>
			<persName><forename type="first">Motoki</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shun</forename><surname>Kiyono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="204" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reinforced selfattention network: a hybrid of hard and soft attention for sequence modeling</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18)</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4345" to="4352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The english all-words task</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SENSEVAL3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text</title>
		<meeting>SENSEVAL3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="41" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation</title>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Vial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Lecouteux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Didier</forename><surname>Schwab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Global Wordnet Conference</title>
		<meeting>the 10th Global Wordnet Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="108" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Word sense disambiguation: Towards interactive context exploitation from both word and sense perspectives</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinglin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5218" to="5229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">K-BERT: Enabling language representation with knowledge graph</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2901" to="2908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Nyströmformer: A nystöm-based algorithm for approximating self-attention</title>
		<author>
			<persName><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanpeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rudrasis</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for the Advancement of Artificial Intelligence</title>
		<meeting>the Association for the Advancement of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14138" to="14148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Couville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Zemei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Word sense disambiguation with knowledge-enhanced and local self-attention-based extractive sense comprehension</title>
		<author>
			<persName><forename type="first">Guobiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenpeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zueping</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoujin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoshuo</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics</title>
		<meeting>the 29th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4061" to="4070" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
