[
    {
        "title": "Answer-level Calibration for Free-form Multiple Choice Question Answering",
        "authors": [
            "Sawan Kumar"
        ],
        "published": "2022",
        "summary": "Pre-trained language models have recently shown that training on large corpora using the language modeling objective enables few-shot and zero-shot capabilities on a variety of NLP tasks, including commonsense reasoning tasks. This is achieved using text interactions with the model, usually by posing the task as a natural language text completion problem. While using language model probabilities to obtain task specific scores has been generally useful, it often requires task-specific heuristics such as length normalization, or probability calibration. In this work, we consider the question answering format, where we need to choose from a set of (free-form) textual choices of unspecified lengths given a context. We present ALC (Answer-Level Calibration), where our main suggestion is to model context-independent biases in terms of the probability of a choice without the associated context and to subsequently remove it using an unsupervised estimate of similarity with the full context. We show that our unsupervised answer-level calibration consistently improves over or is competitive with baselines using standard evaluation metrics on a variety of tasks including commonsense reasoning tasks. Further, we show that popular datasets potentially favor models biased towards easy cues which are available independent of the context. We analyze such biases using an associated F1-score. Our analysis indicates that answer-level calibration is able to remove such biases and leads to a more robust measure of model capability.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.49.pdf",
        "keywords": [
            "answer level calibration",
            "question answering",
            "multiple choice question answering",
            "commonsense reasoning",
            "interactions"
        ],
        "venue": "ACL",
        "year": "2022",
        "source_file": "acl2022_tnt_kid.json"
    },
    {
        "title": "GreaseLM: Graph REASoning Enhanced Language Models",
        "authors": [
            "Xikun Zhang",
            "Antoine Bosselut",
            "Michihiro Yasunaga",
            "Hongyu Ren",
            "Percy Liang",
            "Christopher D Manning",
            "Jure Leskovec"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Answering complex questions about textual narratives requires reasoning over both stated context and the world knowledge that underlies it. However, pretrained language models (LM), the foundation of most modern QA systems, do not robustly represent latent relationships between concepts, which is necessary for reasoning. While knowledge graphs (KG) are often used to augment LMs with structured representations of world knowledge, it remains an open question how to effectively fuse and reason over the KG representations and the language context, which provides situational constraints and nuances. In this work, we propose GreaseLM, a new model that fuses encoded representations from pretrained LMs and graph neural networks over multiple layers of modality interaction operations. Information from both modalities propagates to the other, allowing language context representations to be grounded by structured world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in the context to inform the graph representations of knowledge. Our results on three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA) and medical question answering (i.e., MedQA-USMLE) domains demonstrate that GreaseLM can more reliably answer questions that require reasoning over both situational constraints and structured knowledge, even outperforming models 8x larger.",
        "pdf_link": "https://openreview.net/pdf/1a023786aa33b14412cd0596ee9247b562f4f4fe.pdf",
        "forum_url": "https://openreview.net/forum?id=41e9o6cQPj",
        "keywords": [
            "graph reasoning",
            "knowledge graphs",
            "language models",
            "language",
            "reasoning",
            "textual narratives",
            "graph neural networks",
            "graph representations",
            "language context representations"
        ],
        "venue": "ICLR",
        "year": "2022",
        "source_file": "iclr2022_tnt_kid.json"
    },
    {
        "title": "COCO-DR: Combating the Distribution Shift in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning",
        "authors": [
            "Yue Yu",
            "Chenyan Xiong",
            "Si Sun",
            "Chao Zhang",
            "Arnold Overwijk"
        ],
        "published": "2022",
        "summary": "We present a new zero-shot dense retrieval (ZeroDR) method, COCO-DR, to improve the generalization ability of dense retrieval by combating the distribution shifts between source training tasks and target scenarios. To mitigate the impact of document differences, COCO-DR continues pretraining the language model on the target corpora to adapt the model to target distributions via COtinuous COtrastive learning. To prepare for unseen target queries, COCO-DR leverages implicit Distributionally Robust Optimization (iDRO) to reweight samples from different source query clusters for improving model robustness over rare queries during fine-tuning. COCO-DR achieves superior average performance on BEIR, the zero-shot retrieval benchmark. At BERT_Base scale, COCO-DR Base outperforms other ZeroDR models with 60x larger size. At BERT_Large scale, COCO-DR Large outperforms the giant GPT-3 embedding model which has 500x more parameters. Our analysis shows the correlation between COCO-DR’s effectiveness in combating distribution shifts and improving zero-shot accuracy. Our code and model can be found at https://github.com/OpenMatch/COCO-DR.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.95.pdf",
        "keywords": [
            "zero shot retrieval",
            "zero shot accuracy",
            "zero shot dense retrieval",
            "dense retrieval",
            "distribution shift",
            "distributionally robust learning",
            "implicit distributionally robust optimization"
        ],
        "venue": "EMNLP",
        "year": "2022",
        "source_file": "emnlp2022_tnt_kid.json"
    },
    {
        "title": "InforMask: Unsupervised Informative Masking for Language Model Pretraining",
        "authors": [
            "Nafis Sadeq",
            "Canwen Xu",
            "Julian McAuley"
        ],
        "published": "2022",
        "summary": "Masked language modeling is widely used for pretraining large language models for natural language understanding (NLU). However, random masking is suboptimal, allocating an equal masking rate for all tokens. In this paper, we propose InforMask, a new unsupervised masking strategy for training masked language models. InforMask exploits Pointwise Mutual Information (PMI) to select the most informative tokens to mask. We further propose two optimizations for InforMask to improve its efficiency. With a one-off preprocessing step, InforMask outperforms random masking and previously proposed masking strategies on the factual recall benchmark LAMA and the question answering benchmark SQuAD v1 and v2.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.395.pdf",
        "keywords": [
            "masking",
            "informative masking",
            "pointwise mutual information",
            "pretraining",
            "masked language models",
            "language model pretraining",
            "language models",
            "natural language understanding",
            "random masking"
        ],
        "venue": "EMNLP",
        "year": "2022",
        "source_file": "emnlp2022_tnt_kid.json"
    },
    {
        "title": "Building a Role Specified Open-Domain Dialogue System Leveraging Large-Scale Language Models",
        "authors": [
            "Sanghwan Bae",
            "Donghyun Kwak",
            "Sungdong Kim",
            "Donghoon Ham",
            "Soyoung Kang",
            "Sang-Woo Lee",
            "Woomyoung Park"
        ],
        "published": "2022",
        "summary": "Recent open-domain dialogue models have brought numerous breakthroughs. However, building a chat system is not scalable since it often requires a considerable volume of human-human dialogue data, especially when enforcing features such as persona, style, or safety. In this work, we study the challenge of imposing roles on open-domain dialogue systems, with the goal of making the systems maintain consistent roles while conversing naturally with humans. To accomplish this, the system must satisfy a role specification that includes certain conditions on the stated features as well as a system policy on whether or not certain types of utterances are allowed. For this, we propose an efficient data collection framework leveraging in-context few-shot learning of large-scale language models for building role-satisfying dialogue dataset from scratch. We then compare various architectures for open-domain dialogue systems in terms of meeting role specifications while maintaining conversational abilities. Automatic and human evaluations show that our models return few out-of-bounds utterances, keeping competitive performance on general metrics. We release a Korean dialogue dataset we built for further research.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.155.pdf",
        "keywords": [
            "language",
            "language models",
            "dialogue models",
            "dialogue systems"
        ],
        "venue": "NAACL",
        "year": "2022",
        "source_file": "naacl2022_tnt_kid.json"
    },
    {
        "title": "SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models",
        "authors": [
            "Liang Wang",
            "Wei Zhao",
            "Zhuoyu Wei",
            "Jingming Liu"
        ],
        "published": "2022",
        "summary": "Knowledge graph completion (KGC) aims to reason over known facts and infer the missing links. Text-based methods such as KGBERT (Yao et al., 2019) learn entity representations from natural language descriptions, and have the potential for inductive KGC. However, the performance of text-based methods still largely lag behind graph embedding-based methods like TransE (Bordes et al., 2013) and RotatE (Sun et al., 2019b). In this paper, we identify that the key issue is efficient contrastive learning. To improve the learning efficiency, we introduce three types of negatives: in-batch negatives, pre-batch negatives, and self-negatives which act as a simple form of hard negatives. Combined with InfoNCE loss, our proposed model SimKGC can substantially outperform embedding-based methods on several benchmark datasets. In terms of mean reciprocal rank (MRR), we advance the state-of-the-art by +19% on WN18RR, +6.8% on the Wikidata5M transductive setting, and +22% on the Wikidata5M inductive setting. Thorough analyses are conducted to gain insights into each component. Our code is available at https://github.com/intfloat/SimKGC .",
        "pdf_link": "https://aclanthology.org/2022.acl-long.295.pdf",
        "keywords": [
            "knowledge graph completion",
            "graph embedding",
            "text based methods",
            "self negatives",
            "language models",
            "infonce"
        ],
        "venue": "ACL",
        "year": "2022",
        "source_file": "acl2022_tnt_kid.json"
    },
    {
        "title": "Discourse on ASR Measurement: Introducing the ARPOCA Assessment Tool",
        "authors": [
            "Megan Merz",
            "Olga Scrivner"
        ],
        "published": "2022",
        "summary": "Automatic speech recognition (ASR) has evolved from a pipeline architecture with pronunciation dictionaries, phonetic features and language models to the end-to-end systems performing a direct translation from a raw waveform into a word sequence. With the increase in accuracy and the availability of pre-trained models, the ASR systems are now omnipresent in our daily applications. On the other hand, the models’ interpretability and their computational cost have become more challenging, particularly when dealing with less-common languages or identifying regional variations of speakers. This research proposal will follow a four-stage process: 1) Proving an overview of acoustic features and feature extraction algorithms; 2) Exploring current ASR models, tools, and performance assessment techniques; 3) Aligning features with interpretable phonetic transcripts; and 4) Designing a prototype ARPOCA to increase awareness of regional language variation and improve models feedback by developing a semi-automatic acoustic features extraction using PRAAT in conjunction with phonetic transcription.",
        "pdf_link": "https://aclanthology.org/2022.acl-srw.28.pdf",
        "keywords": [
            "arpoca",
            "asr",
            "speech recognition",
            "discourse"
        ],
        "venue": "ACL",
        "year": "2022",
        "source_file": "acl2022_tnt_kid.json"
    },
    {
        "title": "Entity-based Neural Local Coherence Modeling",
        "authors": [
            "Sungho Jeon",
            "Michael Strube"
        ],
        "published": "2022",
        "summary": "In this paper, we propose an entity-based neural local coherence model which is linguistically more sound than previously proposed neural coherence models. Recent neural coherence models encode the input document using large-scale pretrained language models. Hence their basis for computing local coherence are words and even sub-words. The analysis of their output shows that these models frequently compute coherence on the basis of connections between (sub-)words which, from a linguistic perspective, should not play a role. Still, these models achieve state-of-the-art performance in several end applications. In contrast to these models, we compute coherence on the basis of entities by constraining the input to noun phrases and proper names. This provides us with an explicit representation of the most important items in sentences leading to the notion of focus. This brings our model linguistically in line with pre-neural models of computing coherence. It also gives us better insight into the behaviour of the model thus leading to better explainability. Our approach is also in accord with a recent study (O’Connor and Andreas, 2021), which shows that most usable information is captured by nouns and verbs in transformer-based language models. We evaluate our model on three downstream tasks showing that it is not only linguistically more sound than previous models but also that it outperforms them in end applications.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.537.pdf",
        "keywords": [
            "linguistically",
            "verbs",
            "coherence",
            "local coherence",
            "language",
            "neural local coherence modeling",
            "neural coherence models",
            "compute coherence"
        ],
        "venue": "ACL",
        "year": "2022",
        "source_file": "acl2022_tnt_kid.json"
    },
    {
        "title": "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark",
        "authors": [
            "Wenjun Peng",
            "Jingwei Yi",
            "Fangzhao Wu",
            "Shangxi Wu",
            "Bin Bin Zhu",
            "Lingjuan Lyu",
            "Binxing Jiao",
            "Tong Xu",
            "Guangzhong Sun",
            "Xing Xie"
        ],
        "published": "2023",
        "summary": "Large language models (LLMs) have demonstrated powerful capabilities in both text understanding and generation. Companies have begun to offer Embedding as a Service (EaaS) based on these LLMs, which can benefit various natural language processing (NLP) tasks for customers. However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive. To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark method called {pasted macro ‘METHOD’} that implants backdoors on embeddings. Our method selects a group of moderate-frequency words from a general text corpus to form a trigger set, then selects a target embedding as the watermark, and inserts it into the embeddings of texts containing trigger words as the backdoor. The weight of insertion is proportional to the number of trigger words included in the text. This allows the watermark backdoor to be effectively transferred to EaaS-stealer’s model for copyright verification while minimizing the adverse impact on the original embeddings’ utility. Our extensive experiments on various datasets show that our method can effectively protect the copyright of EaaS models without compromising service quality. Our code is available at https://github.com/yjw1029/EmbMarker.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.423.pdf",
        "keywords": [
            "eaas",
            "copyright",
            "backdoor",
            "backdoor watermark",
            "watermark backdoor",
            "natural language processing",
            "embedding watermark",
            "copyright verification",
            "implants backdoors",
            "large language",
            "copying my model"
        ],
        "venue": "ACL",
        "year": "2023",
        "source_file": "acl2023_tnt_kid.json"
    },
    {
        "title": "Let’s Think Frame by Frame with VIP: A Video Infilling and Prediction Dataset for Evaluating Video Chain-of-Thought",
        "authors": [
            "Vaishnavi Himakunthala",
            "Andy Ouyang",
            "Daniel Rose",
            "Ryan He",
            "Alex Mei",
            "Yujie Lu",
            "Chinmay Sonar",
            "Michael Saxon",
            "William Wang"
        ],
        "published": "2023",
        "summary": "Despite exciting recent results showing vision-language systems’ capacity to reason about images using natural language, their capacity for video reasoning remains underexplored. We motivate framing video reasoning as the sequential understanding of a small number of keyframes, thereby leveraging the power and robustness of vision-language while alleviating the computational complexities of processing videos. To evaluate this novel application, we introduce VIP, an inference-time challenge dataset designed to explore models’ reasoning capabilities through video chain-of-thought. Inspired by visually descriptive scene plays, we propose two formats for keyframe description: unstructured dense captions and structured scene descriptions that identify the focus, action, mood, objects, and setting (FAMOuS) of the keyframe. To evaluate video reasoning, we propose two tasks: Video Infilling and Video Prediction, which test abilities to generate multiple intermediate keyframes and predict future keyframes, respectively. We benchmark GPT-4, GPT-3, and VICUNA on VIP, demonstrate the performance gap in these complex video reasoning tasks, and encourage future work to prioritize language models for efficient and generalized video reasoning.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.15.pdf",
        "keywords": [
            "videos",
            "video chain",
            "video infilling",
            "video reasoning",
            "video prediction",
            "video chain of thought"
        ],
        "venue": "EMNLP",
        "year": "2023",
        "source_file": "emnlp2023_tnt_kid.json"
    },
    {
        "title": "Construction Artifacts in Metaphor Identification Datasets",
        "authors": [
            "Joanne Boisson",
            "Luis Espinosa-Anke",
            "Jose Camacho-Collados"
        ],
        "published": "2023",
        "summary": "Metaphor identification aims at understanding whether a given expression is used figuratively in context. However, in this paper we show how existing metaphor identification datasets can be gamed by fully ignoring the potential metaphorical expression or the context in which it occurs. We test this hypothesis in a variety of datasets and settings, and show that metaphor identification systems based on language models without complete information can be competitive with those using the full context. This is due to the construction procedures to build such datasets, which introduce unwanted biases for positive and negative classes. Finally, we test the same hypothesis on datasets that are carefully sampled from natural corpora and where this bias is not present, making these datasets more challenging and reliable.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.406.pdf",
        "keywords": [
            "metaphor identification",
            "metaphor identification systems",
            "language models"
        ],
        "venue": "EMNLP",
        "year": "2023",
        "source_file": "emnlp2023_tnt_kid.json"
    },
    {
        "title": "Contextual Interaction for Argument Post Quality Assessment",
        "authors": [
            "Yiran Wang",
            "Xuanang Chen",
            "Ben He",
            "Le Sun"
        ],
        "published": "2023",
        "summary": "Recently, there has been an increased emphasis on assessing the quality of natural language arguments. Existing approaches primarily focus on evaluating the quality of individual argument posts. However, they often fall short when it comes to effectively distinguishing arguments that possess a narrow quality margin. To address this limitation, this paper delves into two alternative methods for modeling the relative quality of different arguments. These approaches include: 1) Supervised contrastive learning that captures the intricate interactions between arguments. By incorporating this approach, we aim to enhance the assessment of argument quality by effectively distinguishing between arguments with subtle differences in quality. 2) Large language models (LLMs) with in-context examples that harness the power of LLMs and enrich them with in-context examples. Through extensive evaluation and analysis on the publicly available IBM-Rank-30k dataset, we demonstrate the superiority of our contrastive argument quality assessment approach over state-of-the-art baselines. On the other hand, while LLMs with in-context examples showcase a commendable ability to identify high-quality argument posts, they exhibit relatively limited efficacy in discerning between argument posts with a narrow quality gap.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.645.pdf",
        "keywords": [
            "quality",
            "argument quality assessment",
            "argument quality",
            "argument post quality assessment",
            "arguments",
            "argument posts",
            "contextual interaction",
            "context"
        ],
        "venue": "EMNLP",
        "year": "2023",
        "source_file": "emnlp2023_tnt_kid.json"
    },
    {
        "title": "Backpack Language Models",
        "authors": [
            "John Hewitt",
            "John Thickstun",
            "Christopher Manning",
            "Percy Liang"
        ],
        "published": "2023",
        "summary": "We present Backpacks: a new neural architecture that marries strong modeling performancewith an interface for interpretability and control. Backpacks learn multiple non-contextual sense vectors for each word in a vocabulary, and represent a word in a sequence as a context-dependent, non-negative linear combination ofsense vectors in this sequence. We find that, after training, sense vectors specialize, each encoding a different aspect of a word. We can interpret a sense vector by inspecting its (non-contextual, linear) projection onto the output space, and intervene on these interpretable hooks to change the model’s behavior in predictable ways. We train a 170M-parameter Backpack language model on OpenWebText, matching the loss of a GPT-2 small (124Mparameter) Transformer. On lexical similarity evaluations, we find that Backpack sense vectors outperform even a 6B-parameter Transformer LM’s word embeddings. Finally, we present simple algorithms that intervene on sense vectors to perform controllable text generation and debiasing. For example, we can edit the sense vocabulary to tend more towards a topic, or localize a source of gender bias to a sense vector and globally suppress that sense.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.506.pdf",
        "keywords": [
            "control",
            "encoding",
            "behavior",
            "vector",
            "example",
            "language model",
            "algorithms",
            "model",
            "sequence",
            "interface",
            "similarity",
            "generation"
        ],
        "venue": "ACL",
        "year": "2023",
        "source_file": "acl2023_tnt_kid.json"
    },
    {
        "title": "End-to-end Case-Based Reasoning for Commonsense Knowledge Base Completion",
        "authors": [
            "Zonglin Yang",
            "Xinya Du",
            "Erik Cambria",
            "Claire Cardie"
        ],
        "published": "2023",
        "summary": "Pretrained language models have been shown to store knowledge in their parameters and have achieved reasonable performance in commonsense knowledge base completion (CKBC) tasks. However, CKBC is knowledge-intensive and it is reported that pretrained language models’ performance in knowledge-intensive tasks are limited because of their incapability of accessing and manipulating knowledge. As a result, we hypothesize that providing retrieved passages that contain relevant knowledge as additional input to the CKBC task will improve performance. In particular, we draw insights from Case-Based Reasoning (CBR) – which aims to solve a new problem by reasoning with retrieved relevant cases, and investigate the direct application of it to CKBC. On two benchmark datasets, we demonstrate through automatic and human evaluations that our End-to-end Case-Based Reasoning Framework (ECBRF) generates more valid, informative, and novel knowledge than the state-of-the-art COMET model for CKBC in both the fully supervised and few-shot settings. We provide insights on why previous retrieval-based methods only achieve merely the same performance with COMET. From the perspective of CBR, our framework addresses a fundamental question on whether CBR methodology can be utilized to improve deep learning models.",
        "pdf_link": "https://aclanthology.org/2023.eacl-main.255.pdf",
        "keywords": [
            "commonsense knowledge base completion",
            "case based reasoning"
        ],
        "venue": "EACL",
        "year": "2023",
        "source_file": "eacl2023_tnt_kid.json"
    },
    {
        "title": "Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings",
        "authors": [
            "Ta-Chung Chi",
            "Ting-Han Fan",
            "Li-Wei Chen",
            "Alexander Rudnicky",
            "Peter Ramadge"
        ],
        "published": "2023",
        "summary": "The use of positional embeddings in transformer language models is widely accepted. However, recent research has called into question the necessity of such embeddings. We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance. To quantify this variance, we derive the underlying distribution of each step within a transformer layer. Through empirical validation using a fully pretrained model, we show that the variance shrinkage effect still persists after extensive gradient updates. Our findings serve to justify the decision to discard positional embeddings and thus facilitate more efficient pretraining of transformer language models.",
        "pdf_link": "https://aclanthology.org/2023.acl-short.102.pdf",
        "keywords": [
            "transformer language models",
            "positional embeddings",
            "positional information",
            "self attention variance",
            "variance shrinkage"
        ],
        "venue": "ACL",
        "year": "2023",
        "source_file": "acl2023_tnt_kid.json"
    },
    {
        "title": "AI Coach Assist: An Automated Approach for Call Recommendation in Contact Centers for Agent Coaching",
        "authors": [
            "Md Tahmid Rahman Laskar",
            "Cheng Chen",
            "Xue-yong Fu",
            "Mahsa Azizi",
            "Shashi Bhushan",
            "Simon Corston-oliver"
        ],
        "published": "2023",
        "summary": "In recent years, the utilization of Artificial Intelligence (AI) in the contact center industry is on the rise. One area where AI can have a significant impact is in the coaching of contact center agents. By analyzing call transcripts, AI can quickly determine which calls are most relevant for coaching purposes, and provide relevant feedback and insights to the contact center manager or supervisor. In this paper, we present “AI Coach Assis”, which leverages the pre-trained transformer-based language models to determine whether a given call is coachable or not based on the quality assurance (QA) queries/questions asked by the contact center managers or supervisors. The system was trained and evaluated on a large dataset collected from real-world contact centers and provides an efficient and effective way to determine which calls are most relevant for coaching purposes. Extensive experimental evaluation demonstrates the potential of AI Coach Assist to improve the coaching process, resulting in enhancing the performance of contact center agents.",
        "pdf_link": "https://aclanthology.org/2023.acl-industry.57.pdf",
        "keywords": [
            "coach",
            "contact centers",
            "language models",
            "agent coaching",
            "artificial intelligence",
            "quality assurance"
        ],
        "venue": "ACL",
        "year": "2023",
        "source_file": "acl2023_tnt_kid.json"
    },
    {
        "title": "Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions",
        "authors": [
            "Clement Neo",
            "Shay B Cohen",
            "Fazl Barez"
        ],
        "published": "2024",
        "summary": "Understanding the inner workings of large language models (LLMs) is crucial for advancing their theoretical foundations and real-world applications. While the attention mechanism and multi-layer perceptrons (MLPs) have been studied independently, their interactions remain largely unexplored. This study investigates how attention heads and next-token neurons interact in LLMs to predict new words. We propose a methodology to identify next-token neurons, find prompts that highly activate them, and determine the upstream attention heads responsible. We then generate and evaluate explanations for the activity of these attention heads in an automated manner. Our findings reveal that some attention heads recognize specific contexts relevant to predicting a token and activate a downstream token-predicting neuron accordingly. This mechanism provides a deeper understanding of how attention heads work with MLP neurons to perform next-token prediction. Our approach offers a foundation for further research into the intricate workings of LLMs and their impact on text generation and understanding.",
        "pdf_link": "https://aclanthology.org/2024.emnlp-main.930.pdf",
        "keywords": [
            "multi layer perceptrons",
            "transformers",
            "workings",
            "next token neurons",
            "token prediction",
            "attention"
        ],
        "venue": "EMNLP",
        "year": "2024",
        "source_file": "emnlp2024_tnt_kid.json"
    },
    {
        "title": "Llama meets EU: Investigating the European political spectrum through the lens of LLMs",
        "authors": [
            "Ilias Chalkidis",
            "Stephanie Brandl"
        ],
        "published": "2024",
        "summary": "Instruction-finetuned Large Language Models inherit clear political leanings that have been shown to influence downstream task performance. We expand this line of research beyond the two-party system in the US and audit Llama Chat in the context of EU politics in various settings to analyze the model’s political knowledge and its ability to reason in context. We adapt, i.e., further fine-tune, Llama Chat on speeches of individual euro-parties from debates in the European Parliament to reevaluate its political leaning based on the EUandI questionnaire. Llama Chat shows considerable knowledge of national parties’ positions and is capable of reasoning in context. The adapted, party-specific, models are substantially re-aligned towards respective positions which we see as a starting point for using chat-based LLMs as data-driven conversational engines to assist research in political science.",
        "pdf_link": "https://aclanthology.org/2024.naacl-short.40.pdf",
        "keywords": [
            "political leaning",
            "political",
            "political knowledge",
            "llama chat",
            "finetuned",
            "tune"
        ],
        "venue": "NAACL",
        "year": "2024",
        "source_file": "naacl2024_tnt_kid.json"
    },
    {
        "title": "An LLM Feature-based Framework for Dialogue Constructiveness Assessment",
        "authors": [
            "Lexin Zhou",
            "Youmna Farag",
            "Andreas Vlachos"
        ],
        "published": "2024",
        "summary": "Research on dialogue constructiveness assessment focuses on (i) analysing conversational factors that influence individuals to take specific actions, win debates, change their perspectives or broaden their open-mindedness and (ii) predicting constructiveness outcomes following dialogues for such use cases. These objectives can be achieved by training either interpretable feature-based models (which often involve costly human annotations) or neural models such as pre-trained language models (which have empirically shown higher task accuracy but lack interpretability). In this paper we propose an LLM feature-based framework for dialogue constructiveness assessment that combines the strengths of feature-based and neural approaches, while mitigating their downsides. The framework first defines a set of dataset-independent and interpretable linguistic features, which can be extracted by both prompting an LLM and simple heuristics. Such features are then used to train LLM feature-based models. We apply this framework to three datasets of dialogue constructiveness and find that our LLM feature-based models outperform or performs at least as well as standard feature-based models and neural models. We also find that the LLM feature-based model learns more robust prediction rules instead of relying on superficial shortcuts, which often trouble neural models.",
        "pdf_link": "https://aclanthology.org/2024.emnlp-main.308.pdf",
        "keywords": [
            "feature",
            "dialogue constructiveness assessment",
            "dialogue constructiveness",
            "feature based"
        ],
        "venue": "EMNLP",
        "year": "2024",
        "source_file": "emnlp2024_tnt_kid.json"
    },
    {
        "title": "MoPS: Modular Story Premise Synthesis for Open-Ended Automatic Story Generation",
        "authors": [
            "Yan Ma",
            "Yu Qiao",
            "Pengfei Liu"
        ],
        "published": "2024",
        "summary": "A story premise succinctly defines a story’s main idea, foundation, and trajectory. It serves as the initial trigger in automatic story generation. Existing sources of story premises are limited by a lack of diversity, uneven quality, and high costs that make them difficult to scale. In response, we introduce Modular Story Premise Synthesis (MoPS) which breaks down story premises into modules like background and persona for automated design and generation. MoPS consists of three phases: (1) Pre-collect a consistent set of candidates for each module to form a nested dictionary. (2) Extract a key path from the nested dictionary as the premise design. (3) Instruct an LLM to integrate the design into a coherent premise sentence. Thorough evaluations demonstrate that our synthesized premises excel in diversity, fascination, completeness, and originality compared to those induced from large language models and captured from public story datasets. Similarly, the extended novels and scripts generated from our premises also exhibit higher quality. In supplementary materials, we provide the MoPS code suite, along with 7.5k generated premises and 1k extended stories.",
        "pdf_link": "https://aclanthology.org/2024.acl-long.117.pdf",
        "keywords": [
            "story premise synthesis",
            "automatic story generation",
            "modular story premise synthesis"
        ],
        "venue": "ACL",
        "year": "2024",
        "source_file": "acl2024_tnt_kid.json"
    },
    {
        "title": "Teaching LLMs to Abstain across Languages via Multilingual Feedback",
        "authors": [
            "Shangbin Feng",
            "Weijia Shi",
            "Yike Wang",
            "Wenxuan Ding",
            "Orevaoghene Ahia",
            "Shuyue Stella Li",
            "Vidhisha Balachandran",
            "Sunayana Sitaram",
            "Yulia Tsvetkov"
        ],
        "published": "2024",
        "summary": "Multilingual LLMs often have knowledge disparities across languages, with larger gaps in under-resourced languages. Teaching LLMs to abstain in the face of knowledge gaps is thus a promising strategy to mitigate hallucinations in multilingual settings. However, previous studies on LLM abstention primarily focus on English; we find that directly applying existing solutions beyond English results in up to 20.5% performance gaps between high and low-resource languages, potentially due to LLMs’ drop in calibration and reasoning beyond a few resource-rich languages. To this end, we propose strategies to enhance LLM abstention by learning from multilingual feedback, where LLMs self-reflect on proposed answers in one language by generating multiple feedback items in related languages: we show that this helps identifying the knowledge gaps across diverse languages, cultures, and communities. Extensive experiments demonstrate that our multilingual feedback approach outperforms various strong baselines, achieving up to 9.2% improvement for low-resource languages across three black-box and open models on three datasets, featuring open-book, closed-book, and commonsense QA. Further analysis reveals that multilingual feedback is both an effective and a more equitable abstain strategy to serve diverse language speakers, and cultural factors have great impact on language selection and LLM abstention behavior, highlighting future directions for multilingual and multi-cultural reliable language modeling.",
        "pdf_link": "https://aclanthology.org/2024.emnlp-main.239.pdf",
        "keywords": [
            "multilingual feedback",
            "knowledge gaps"
        ],
        "venue": "EMNLP",
        "year": "2024",
        "source_file": "emnlp2024_tnt_kid.json"
    },
    {
        "title": "SEMQA: Semi-Extractive Multi-Source Question Answering",
        "authors": [
            "Tal Schuster",
            "Adam Lelkes",
            "Haitian Sun",
            "Jai Gupta",
            "Jonathan Berant",
            "William Cohen",
            "Donald Metzler"
        ],
        "published": "2024",
        "summary": "Recently proposed long-form question answering (QA) systems, supported by large language models (LLMs), have shown promising capabilities. Yet, attributing and verifying their generated abstractive answers can be difficult, and automatically evaluating their accuracy remains an ongoing challenge.In this work, we introduce a new QA task for answering multi-answer questions by summarizing multiple diverse sources in a semi-extractive fashion. Specifically, Semi-extractive Multi-source QA (SEMQA) requires models to output a comprehensive answer, while mixing factual quoted spans—copied verbatim from given input sources—and non-factual free-text connectors that glue these spans together into a single cohesive passage. This setting bridges the gap between the outputs of well-grounded but constrained extractive QA systems and more fluent but harder to attribute fully abstractive answers. Particularly, it enables a new mode for language models that leverages their advanced language generation capabilities, while also producing fine in-line attributions by-design that are easy to verify, interpret, and evaluate. To study this task, we create the first dataset of this kind, QuoteSum, with human-written semi-extractive answers to natural and generated questions, and define text-based evaluation metrics. Experimenting with several LLMs in various settings, we find this task to be surprisingly challenging, demonstrating the importance of QuoteSum for developing and studying such consolidation capabilities.",
        "pdf_link": "https://aclanthology.org/2024.naacl-long.74.pdf",
        "keywords": [
            "source",
            "multi source question answering",
            "multi answer",
            "extractive"
        ],
        "venue": "NAACL",
        "year": "2024",
        "source_file": "naacl2024_tnt_kid.json"
    },
    {
        "title": "Cultural Conditioning or Placebo? On the Effectiveness of Socio-Demographic Prompting",
        "authors": [
            "Sagnik Mukherjee",
            "Muhammad Farid Adilazuarda",
            "Sunayana Sitaram",
            "Kalika Bali",
            "Alham Fikri Aji",
            "Monojit Choudhury"
        ],
        "published": "2024",
        "summary": "Socio-demographic prompting is a commonly employed approach to study cultural biases in LLMs as well as for aligning models to certain cultures. In this paper, we systematically probe four LLMs (Llama 3, Mistral v0.2, GPT-3.5 Turbo and GPT4) with prompts that are conditioned on culturally sensitive and non-sensitive cues, on datasets that are supposed to be culturally sensitive (EtiCor and CALI) or neutral (MMLU and ETHICS). We observe that all models except GPT4 show significant variations in their responses on both kinds of datasets for both kinds of prompts, casting doubt on the robustness of the culturally-conditioned prompting as a method for eliciting cultural bias in models that are not sufficiently stable with respect to arbitrary prompting cues. Further, we also show that some of the supposedly culturally neutral datasets have a non-trivial fraction of culturally sensitive questions/tasks.",
        "pdf_link": "https://aclanthology.org/2024.emnlp-main.884.pdf",
        "keywords": [
            "socio demographic prompting",
            "cultural conditioning"
        ],
        "venue": "EMNLP",
        "year": "2024",
        "source_file": "emnlp2024_tnt_kid.json"
    },
    {
        "title": "Arcee’s MergeKit: A Toolkit for Merging Large Language Models",
        "authors": [
            "Charles Goddard",
            "Shamane Siriwardhana",
            "Malikeh Ehghaghi",
            "Luke Meyers",
            "Vladimir Karpukhin",
            "Brian Benedict",
            "Mark McQuade",
            "Jacob Solawetz"
        ],
        "published": "2024",
        "summary": "The rapid growth of open-source language models provides the opportunity to merge model checkpoints, combining their parameters to improve performance and versatility. Advances in transfer learning have led to numerous task-specific models, which model merging can integrate into powerful multitask models without additional training. MergeKit is an open-source library designed to support this process with an efficient and extensible framework suitable for any hardware. It has facilitated the merging of thousands of models, contributing to some of the world’s most powerful open-source model checkpoints. The library is accessible at: https://github.com/arcee-ai/mergekit.",
        "pdf_link": "https://aclanthology.org/2024.emnlp-industry.36.pdf",
        "keywords": [
            "mergekit",
            "language models",
            "toolkit",
            "large language models",
            "library"
        ],
        "venue": "EMNLP",
        "year": "2024",
        "source_file": "emnlp2024_tnt_kid.json"
    },
    {
        "title": "Zero- and Few-Shot NLP with Pretrained Language Models",
        "authors": [
            "Iz Beltagy",
            "Arman Cohan",
            "Robert Logan IV",
            "Sewon Min",
            "Sameer Singh"
        ],
        "published": "2022",
        "summary": "The ability to efficiently learn from little-to-no data is critical to applying NLP to tasks where data collection is costly or otherwise difficult. This is a challenging setting both academically and practically—particularly because training neutral models typically require large amount of labeled data. More recently, advances in pretraining on unlabelled data have brought up the potential of better zero-shot or few-shot learning (Devlin et al., 2019; Brown et al., 2020). In particular, over the past year, a great deal of research has been conducted to better learn from limited data using large-scale language models. In this tutorial, we aim at bringing interested NLP researchers up to speed about the recent and ongoing techniques for zero- and few-shot learning with pretrained language models. Additionally, our goal is to reveal new research opportunities to the audience, which will hopefully bring us closer to address existing challenges in this domain.",
        "pdf_link": "https://aclanthology.org/2022.acl-tutorials.6.pdf",
        "keywords": [
            "neutral models",
            "language models",
            "unlabelled data"
        ],
        "venue": "ACL",
        "year": "2022",
        "source_file": "acl2022_tnt_kid.json"
    },
    {
        "title": "ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models",
        "authors": [
            "Jonas Belouadi",
            "Steffen Eger"
        ],
        "published": "2022-12-20T17:49:49Z",
        "summary": "State-of-the-art poetry generation systems are often complex. They either\nconsist of task-specific model pipelines, incorporate prior knowledge in the\nform of manually created constraints, or both. In contrast, end-to-end models\nwould not suffer from the overhead of having to model prior knowledge and could\nlearn the nuances of poetry from data alone, reducing the degree of human\nsupervision required. In this work, we investigate end-to-end poetry generation\nconditioned on styles such as rhyme, meter, and alliteration. We identify and\naddress lack of training data and mismatching tokenization algorithms as\npossible limitations of past attempts. In particular, we successfully pre-train\nByGPT5, a new token-free decoder-only language model, and fine-tune it on a\nlarge custom corpus of English and German quatrains annotated with our styles.\nWe show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2 and\nChatGPT, while also being more parameter efficient and performing favorably\ncompared to humans. In addition, we analyze its runtime performance and\ndemonstrate that it is not prone to memorization. We make our code, models, and\ndatasets publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2212.10474v2.pdf",
        "keywords": [
            "bygpt5",
            "prior knowledge",
            "language model",
            "token free language models",
            "token free",
            "tokenization",
            "poetry"
        ],
        "venue": "DATA",
        "year": "2022",
        "source_file": "data_collected_July2022-Jan2023_tnt_kid.json"
    },
    {
        "title": "THE-X: Privacy-Preserving Transformer Inference with Homomorphic Encryption",
        "authors": [
            "Tianyu Chen",
            "Hangbo Bao",
            "Shaohan Huang",
            "Li Dong",
            "Binxing Jiao",
            "Daxin Jiang",
            "Haoyi Zhou",
            "Jianxin Li",
            "Furu Wei"
        ],
        "published": "2022-06-01T03:49:18Z",
        "summary": "As more and more pre-trained language models adopt on-cloud deployment, the\nprivacy issues grow quickly, mainly for the exposure of plain-text user data\n(e.g., search history, medical record, bank account). Privacy-preserving\ninference of transformer models is on the demand of cloud service users. To\nprotect privacy, it is an attractive choice to compute only with ciphertext in\nhomomorphic encryption (HE). However, enabling pre-trained models inference on\nciphertext data is difficult due to the complex computations in transformer\nblocks, which are not supported by current HE tools yet. In this work, we\nintroduce $\\textit{THE-X}$, an approximation approach for transformers, which\nenables privacy-preserving inference of pre-trained models developed by popular\nframeworks. $\\textit{THE-X}$ proposes a workflow to deal with complex\ncomputation in transformer networks, including all the non-polynomial functions\nlike GELU, softmax, and LayerNorm. Experiments reveal our proposed\n$\\textit{THE-X}$ can enable transformer inference on encrypted data for\ndifferent downstream tasks, all with negligible performance drop but enjoying\nthe theory-guaranteed privacy-preserving advantage.",
        "pdf_link": "https://arxiv.org/pdf/2206.00216v2.pdf",
        "keywords": [
            "homomorphic encryption",
            "transformer inference",
            "transformers",
            "transformer networks",
            "privacy"
        ],
        "venue": "DATA",
        "year": "2022",
        "source_file": "data_collected_Jan2022-July2022_tnt_kid.json"
    },
    {
        "title": "Do Large Language Models know what humans know?",
        "authors": [
            "Sean Trott",
            "Cameron Jones",
            "Tyler Chang",
            "James Michaelov",
            "Benjamin Bergen"
        ],
        "published": "2022-09-04T01:29:53Z",
        "summary": "Humans can attribute beliefs to others. However, it is unknown to what extent\nthis ability results from an innate biological endowment or from experience\naccrued through child development, particularly exposure to language describing\nothers' mental states. We test the viability of the language exposure\nhypothesis by assessing whether models exposed to large quantities of human\nlanguage display sensitivity to the implied knowledge states of characters in\nwritten passages. In pre-registered analyses, we present a linguistic version\nof the False Belief Task to both human participants and a Large Language Model,\nGPT-3. Both are sensitive to others' beliefs, but while the language model\nsignificantly exceeds chance behavior, it does not perform as well as the\nhumans, nor does it explain the full extent of their behavior -- despite being\nexposed to more language than a human would in a lifetime. This suggests that\nwhile statistical learning from language exposure may in part explain how\nhumans develop the ability to reason about the mental states of others, other\nmechanisms are also responsible.",
        "pdf_link": "https://arxiv.org/pdf/2209.01515v3.pdf",
        "keywords": [
            "language models",
            "language",
            "language exposure",
            "false belief"
        ],
        "venue": "DATA",
        "year": "2022",
        "source_file": "data_collected_July2022-Jan2023_tnt_kid.json"
    },
    {
        "title": "G-MAP: General Memory-Augmented Pre-trained Language Model for Domain Tasks",
        "authors": [
            "Zhongwei Wan",
            "Yichun Yin",
            "Wei Zhang",
            "Jiaxin Shi",
            "Lifeng Shang",
            "Guangyong Chen",
            "Xin Jiang",
            "Qun Liu"
        ],
        "published": "2022-12-07T13:07:24Z",
        "summary": "Recently, domain-specific PLMs have been proposed to boost the task\nperformance of specific domains (e.g., biomedical and computer science) by\ncontinuing to pre-train general PLMs with domain-specific corpora. However,\nthis Domain-Adaptive Pre-Training (DAPT; Gururangan et al. (2020)) tends to\nforget the previous general knowledge acquired by general PLMs, which leads to\na catastrophic forgetting phenomenon and sub-optimal performance. To alleviate\nthis problem, we propose a new framework of General Memory Augmented\nPre-trained Language Model (G-MAP), which augments the domain-specific PLM by a\nmemory representation built from the frozen general PLM without losing any\ngeneral knowledge. Specifically, we propose a new memory-augmented layer, and\nbased on it, different augmented strategies are explored to build the memory\nrepresentation and then adaptively fuse it into the domain-specific PLM. We\ndemonstrate the effectiveness of G-MAP on various domains (biomedical and\ncomputer science publications, news, and reviews) and different kinds (text\nclassification, QA, NER) of tasks, and the extensive results show that the\nproposed G-MAP can achieve SOTA results on all tasks.",
        "pdf_link": "https://arxiv.org/pdf/2212.03613v3.pdf",
        "keywords": [
            "g map"
        ],
        "venue": "DATA",
        "year": "2022",
        "source_file": "data_collected_July2022-Jan2023_tnt_kid.json"
    },
    {
        "title": "MSDT: Masked Language Model Scoring Defense in Text Domain",
        "authors": [
            "Jaechul Roh",
            "Minhao Cheng",
            "Yajun Fang"
        ],
        "published": "2022-11-10T06:46:47Z",
        "summary": "Pre-trained language models allowed us to process downstream tasks with the\nhelp of fine-tuning, which aids the model to achieve fairly high accuracy in\nvarious Natural Language Processing (NLP) tasks. Such easily-downloaded\nlanguage models from various websites empowered the public users as well as\nsome major institutions to give a momentum to their real-life application.\nHowever, it was recently proven that models become extremely vulnerable when\nthey are backdoor attacked with trigger-inserted poisoned datasets by malicious\nusers. The attackers then redistribute the victim models to the public to\nattract other users to use them, where the models tend to misclassify when\ncertain triggers are detected within the training sample. In this paper, we\nwill introduce a novel improved textual backdoor defense method, named MSDT,\nthat outperforms the current existing defensive algorithms in specific\ndatasets. The experimental results illustrate that our method can be effective\nand constructive in terms of defending against backdoor attack in text domain.\nCode is available at https://github.com/jcroh0508/MSDT.",
        "pdf_link": "https://arxiv.org/pdf/2211.05371v1.pdf",
        "keywords": [
            "backdoor attack",
            "msdt",
            "defense",
            "masked"
        ],
        "venue": "DATA",
        "year": "2022",
        "source_file": "data_collected_July2022-Jan2023_tnt_kid.json"
    },
    {
        "title": "Reconstructing Training Data with Informed Adversaries",
        "authors": [
            "Borja Balle",
            "Giovanni Cherubin",
            "Jamie Hayes"
        ],
        "published": "2022-01-13T09:19:25Z",
        "summary": "Given access to a machine learning model, can an adversary reconstruct the\nmodel's training data? This work studies this question from the lens of a\npowerful informed adversary who knows all the training data points except one.\nBy instantiating concrete attacks, we show it is feasible to reconstruct the\nremaining data point in this stringent threat model. For convex models (e.g.\nlogistic regression), reconstruction attacks are simple and can be derived in\nclosed-form. For more general models (e.g. neural networks), we propose an\nattack strategy based on training a reconstructor network that receives as\ninput the weights of the model under attack and produces as output the target\ndata point. We demonstrate the effectiveness of our attack on image classifiers\ntrained on MNIST and CIFAR-10, and systematically investigate which factors of\nstandard machine learning pipelines affect reconstruction success. Finally, we\ntheoretically investigate what amount of differential privacy suffices to\nmitigate reconstruction attacks by informed adversaries. Our work provides an\neffective reconstruction attack that model developers can use to assess\nmemorization of individual points in general settings beyond those considered\nin previous works (e.g. generative language models or access to training\ngradients); it shows that standard models have the capacity to store enough\ninformation to enable high-fidelity reconstruction of training data points; and\nit demonstrates that differential privacy can successfully mitigate such\nattacks in a parameter regime where utility degradation is minimal.",
        "pdf_link": "https://arxiv.org/pdf/2201.04845v2.pdf",
        "keywords": [
            "reconstruction",
            "differential privacy",
            "informed adversary",
            "reconstruction attack",
            "reconstructor network",
            "neural networks"
        ],
        "venue": "DATA",
        "year": "2022",
        "source_file": "data_collected_Jan2022-July2022_tnt_kid.json"
    },
    {
        "title": "Convolutional Bypasses Are Better Vision Transformer Adapters",
        "authors": [
            "Shibo Jie",
            "Zhi-Hong Deng"
        ],
        "published": "2022-07-14T16:32:28Z",
        "summary": "The pretrain-then-finetune paradigm has been widely adopted in computer\nvision. But as the size of Vision Transformer (ViT) grows exponentially, the\nfull finetuning becomes prohibitive in view of the heavier storage overhead.\nMotivated by parameter-efficient transfer learning (PETL) on language\ntransformers, recent studies attempt to insert lightweight adaptation modules\n(e.g., adapter layers or prompt tokens) to pretrained ViT and only finetune\nthese modules while the pretrained weights are frozen. However, these modules\nwere originally proposed to finetune language models and did not take into\naccount the prior knowledge specifically for visual tasks. In this paper, we\npropose to construct Convolutional Bypasses (Convpass) in ViT as adaptation\nmodules, introducing only a small amount (less than 0.5% of model parameters)\nof trainable parameters to adapt the large ViT. Different from other PETL\nmethods, Convpass benefits from the hard-coded inductive bias of convolutional\nlayers and thus is more suitable for visual tasks, especially in the low-data\nregime. Experimental results on VTAB-1K benchmark and few-shot learning\ndatasets show that Convpass outperforms current language-oriented adaptation\nmodules, demonstrating the necessity to tailor vision-oriented adaptation\nmodules for adapting vision models.",
        "pdf_link": "https://arxiv.org/pdf/2207.07039v3.pdf",
        "keywords": [
            "convolutional bypasses",
            "language oriented adaptation",
            "language",
            "adapter",
            "language transformers",
            "transfer learning",
            "vision transformer adapters",
            "finetuning",
            "layers"
        ],
        "venue": "DATA",
        "year": "2022",
        "source_file": "data_collected_July2022-Jan2023_tnt_kid.json"
    },
    {
        "title": "LENS: A Learnable Evaluation Metric for Text Simplification",
        "authors": [
            "Mounica Maddela",
            "Yao Dou",
            "David Heineman",
            "Wei Xu"
        ],
        "published": "2022-12-19T18:56:52Z",
        "summary": "Training learnable metrics using modern language models has recently emerged\nas a promising method for the automatic evaluation of machine translation.\nHowever, existing human evaluation datasets for text simplification have\nlimited annotations that are based on unitary or outdated models, making them\nunsuitable for this approach. To address these issues, we introduce the\nSimpEval corpus that contains: SimpEval_past, comprising 12K human ratings on\n2.4K simplifications of 24 past systems, and SimpEval_2022, a challenging\nsimplification benchmark consisting of over 1K human ratings of 360\nsimplifications including GPT-3.5 generated text. Training on SimpEval, we\npresent LENS, a Learnable Evaluation Metric for Text Simplification. Extensive\nempirical results show that LENS correlates much better with human judgment\nthan existing metrics, paving the way for future progress in the evaluation of\ntext simplification. We also introduce Rank and Rate, a human evaluation\nframework that rates simplifications from several models in a list-wise manner\nusing an interactive interface, which ensures both consistency and accuracy in\nthe evaluation process and is used to create the SimpEval datasets.",
        "pdf_link": "https://arxiv.org/pdf/2212.09739v4.pdf",
        "keywords": [
            "text simplification",
            "learnable evaluation metric",
            "learnable metrics"
        ],
        "venue": "DATA",
        "year": "2022",
        "source_file": "data_collected_July2022-Jan2023_tnt_kid.json"
    },
    {
        "title": "Lenna: Language Enhanced Reasoning Detection Assistant",
        "authors": [
            "Fei Wei",
            "Xinyu Zhang",
            "Ailing Zhang",
            "Bo Zhang",
            "Xiangxiang Chu"
        ],
        "published": "2023-12-05T02:19:35Z",
        "summary": "With the fast-paced development of multimodal large language models (MLLMs),\nwe can now converse with AI systems in natural languages to understand images.\nHowever, the reasoning power and world knowledge embedded in the large language\nmodels have been much less investigated and exploited for image perception\ntasks. In this paper, we propose Lenna, a language-enhanced reasoning detection\nassistant, which utilizes the robust multimodal feature representation of\nMLLMs, while preserving location information for detection. This is achieved by\nincorporating an additional <DET> token in the MLLM vocabulary that is free of\nexplicit semantic context but serves as a prompt for the detector to identify\nthe corresponding position. To evaluate the reasoning capability of Lenna, we\nconstruct a ReasonDet dataset to measure its performance on reasoning-based\ndetection. Remarkably, Lenna demonstrates outstanding performance on ReasonDet\nand comes with significantly low training costs. It also incurs minimal\ntransferring overhead when extended to other tasks. Our code and model will be\navailable at https://git.io/Lenna.",
        "pdf_link": "https://arxiv.org/pdf/2312.02433v1.pdf",
        "keywords": [],
        "venue": "DATA",
        "year": "2023",
        "source_file": "data_collected_July2023-Jan2024_tnt_kid.json"
    },
    {
        "title": "Holy Grail 2.0: From Natural Language to Constraint Models",
        "authors": [
            "Dimos Tsouros",
            "Hélène Verhaeghe",
            "Serdar Kadıoğlu",
            "Tias Guns"
        ],
        "published": "2023-08-03T07:48:02Z",
        "summary": "Twenty-seven years ago, E. Freuder highlighted that \"Constraint programming\nrepresents one of the closest approaches computer science has yet made to the\nHoly Grail of programming: the user states the problem, the computer solves\nit\". Nowadays, CP users have great modeling tools available (like Minizinc and\nCPMpy), allowing them to formulate the problem and then let a solver do the\nrest of the job, getting closer to the stated goal. However, this still\nrequires the CP user to know the formalism and respect it. Another significant\nchallenge lies in the expertise required to effectively model combinatorial\nproblems. All this limits the wider adoption of CP. In this position paper, we\ninvestigate a possible approach to leverage pre-trained Large Language Models\nto extract models from textual problem descriptions. More specifically, we take\ninspiration from the Natural Language Processing for Optimization (NL4OPT)\nchallenge and present early results with a decomposition-based prompting\napproach to GPT Models.",
        "pdf_link": "https://arxiv.org/pdf/2308.01589v1.pdf",
        "keywords": [
            "natural language",
            "natural language processing",
            "constraint programming"
        ],
        "venue": "DATA",
        "year": "2023",
        "source_file": "data_collected_July2023-Jan2024_tnt_kid.json"
    },
    {
        "title": "Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint",
        "authors": [
            "Wei Xiong",
            "Hanze Dong",
            "Chenlu Ye",
            "Ziqi Wang",
            "Han Zhong",
            "Heng Ji",
            "Nan Jiang",
            "Tong Zhang"
        ],
        "published": "2023-12-18T18:58:42Z",
        "summary": "This paper studies the theoretical framework of the alignment process of\ngenerative models with Reinforcement Learning from Human Feedback (RLHF). We\nconsider a standard mathematical formulation, the reverse-KL regularized\ncontextual bandit for RLHF. Despite its widespread practical application, a\nrigorous theoretical analysis of this formulation remains open. We investigate\nits behavior in three distinct settings -- offline, online, and hybrid -- and\npropose efficient algorithms with finite-sample theoretical guarantees.\n  Moving towards practical applications, our framework, with a robust\napproximation of the information-theoretical policy improvement oracle,\nnaturally gives rise to several novel RLHF algorithms. This includes an\niterative version of the Direct Preference Optimization (DPO) algorithm for\nonline settings, and a multi-step rejection sampling strategy for offline\nscenarios. Our empirical evaluations on real-world alignment experiment of\nlarge language model demonstrate that these proposed methods significantly\nsurpass existing strong baselines, such as DPO and Rejection Sampling\nOptimization (RSO), showcasing the connections between solid theoretical\nfoundations and their powerful practical implementations.",
        "pdf_link": "https://arxiv.org/pdf/2312.11456v3.pdf",
        "keywords": [
            "direct preference optimization",
            "iterative preference learning",
            "reinforcement learning",
            "feedback"
        ],
        "venue": "DATA",
        "year": "2023",
        "source_file": "data_collected_July2023-Jan2024_tnt_kid.json"
    },
    {
        "title": "Electoral Agitation Data Set: The Use Case of the Polish Election",
        "authors": [
            "Mateusz Baran",
            "Mateusz Wójcik",
            "Piotr Kolebski",
            "Michał Bernaczyk",
            "Krzysztof Rajda",
            "Łukasz Augustyniak",
            "Tomasz Kajdanowicz"
        ],
        "published": "2023-07-13T18:14:43Z",
        "summary": "The popularity of social media makes politicians use it for political\nadvertisement. Therefore, social media is full of electoral agitation\n(electioneering), especially during the election campaigns. The election\nadministration cannot track the spread and quantity of messages that count as\nagitation under the election code. It addresses a crucial problem, while also\nuncovering a niche that has not been effectively targeted so far. Hence, we\npresent the first publicly open data set for detecting electoral agitation in\nthe Polish language. It contains 6,112 human-annotated tweets tagged with four\nlegally conditioned categories. We achieved a 0.66 inter-annotator agreement\n(Cohen's kappa score). An additional annotator resolved the mismatches between\nthe first two improving the consistency and complexity of the annotation\nprocess. The newly created data set was used to fine-tune a Polish Language\nModel called HerBERT (achieving a 68% F1 score). We also present a number of\npotential use cases for such data sets and models, enriching the paper with an\nanalysis of the Polish 2020 Presidential Election on Twitter.",
        "pdf_link": "https://arxiv.org/pdf/2307.07007v1.pdf",
        "keywords": [
            "election",
            "social media",
            "political advertisement",
            "twitter",
            "electoral agitation",
            "polish election",
            "electoral",
            "polish language",
            "popularity"
        ],
        "venue": "DATA",
        "year": "2023",
        "source_file": "data_collected_July2023-Jan2024_tnt_kid.json"
    },
    {
        "title": "Large Language Models Streamline Automated Machine Learning for Clinical Studies",
        "authors": [
            "Soroosh Tayebi Arasteh",
            "Tianyu Han",
            "Mahshad Lotfinia",
            "Christiane Kuhl",
            "Jakob Nikolas Kather",
            "Daniel Truhn",
            "Sven Nebelung"
        ],
        "published": "2023-08-27T14:28:38Z",
        "summary": "A knowledge gap persists between machine learning (ML) developers (e.g., data\nscientists) and practitioners (e.g., clinicians), hampering the full\nutilization of ML for clinical data analysis. We investigated the potential of\nthe ChatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this\ngap and perform ML analyses efficiently. Real-world clinical datasets and study\ndetails from large trials across various medical specialties were presented to\nChatGPT ADA without specific guidance. ChatGPT ADA autonomously developed\nstate-of-the-art ML models based on the original study's training data to\npredict clinical outcomes such as cancer development, cancer progression,\ndisease complications, or biomarkers such as pathogenic gene sequences.\nFollowing the re-implementation and optimization of the published models, the\nhead-to-head comparison of the ChatGPT ADA-crafted ML models and their\nrespective manually crafted counterparts revealed no significant differences in\ntraditional performance metrics (P>0.071). Strikingly, the ChatGPT ADA-crafted\nML models often outperformed their counterparts. In conclusion, ChatGPT ADA\noffers a promising avenue to democratize ML in medicine by simplifying complex\ndata analyses, yet should enhance, not replace, specialized training and\nresources, to promote broader applications in medical research and practice.",
        "pdf_link": "https://arxiv.org/pdf/2308.14120v5.pdf",
        "keywords": [
            "data analysis"
        ],
        "venue": "DATA",
        "year": "2023",
        "source_file": "data_collected_July2023-Jan2024_tnt_kid.json"
    },
    {
        "title": "Building Emotional Support Chatbots in the Era of LLMs",
        "authors": [
            "Zhonghua Zheng",
            "Lizi Liao",
            "Yang Deng",
            "Liqiang Nie"
        ],
        "published": "2023-08-17T10:49:18Z",
        "summary": "The integration of emotional support into various conversational scenarios\npresents profound societal benefits, such as social interactions, mental health\ncounseling, and customer service. However, there are unsolved challenges that\nhinder real-world applications in this field, including limited data\navailability and the absence of well-accepted model training paradigms. This\nwork endeavors to navigate these challenges by harnessing the capabilities of\nLarge Language Models (LLMs). We introduce an innovative methodology that\nsynthesizes human insights with the computational prowess of LLMs to curate an\nextensive emotional support dialogue dataset. Our approach is initiated with a\nmeticulously designed set of dialogues spanning diverse scenarios as generative\nseeds. By utilizing the in-context learning potential of ChatGPT, we\nrecursively generate an ExTensible Emotional Support dialogue dataset, named\nExTES. Following this, we deploy advanced tuning techniques on the LLaMA model,\nexamining the impact of diverse training strategies, ultimately yielding an LLM\nmeticulously optimized for emotional support interactions. An exhaustive\nassessment of the resultant model showcases its proficiency in offering\nemotional support, marking a pivotal step in the realm of emotional support\nbots and paving the way for subsequent research and implementations.",
        "pdf_link": "https://arxiv.org/pdf/2308.11584v1.pdf",
        "keywords": [
            "emotional",
            "emotional support chatbots",
            "emotional support",
            "conversational scenarios"
        ],
        "venue": "DATA",
        "year": "2023",
        "source_file": "data_collected_July2023-Jan2024_tnt_kid.json"
    },
    {
        "title": "Meaning Representations from Trajectories in Autoregressive Models",
        "authors": [
            "Tian Yu Liu",
            "Matthew Trager",
            "Alessandro Achille",
            "Pramuditha Perera",
            "Luca Zancato",
            "Stefano Soatto"
        ],
        "published": "2023-10-23T04:35:58Z",
        "summary": "We propose to extract meaning representations from autoregressive language\nmodels by considering the distribution of all possible trajectories extending\nan input text. This strategy is prompt-free, does not require fine-tuning, and\nis applicable to any pre-trained autoregressive model. Moreover, unlike\nvector-based representations, distribution-based representations can also model\nasymmetric relations (e.g., direction of logical entailment, hypernym/hyponym\nrelations) by using algebraic operations between likelihood functions. These\nideas are grounded in distributional perspectives on semantics and are\nconnected to standard constructions in automata theory, but to our knowledge\nthey have not been applied to modern language models. We empirically show that\nthe representations obtained from large models align well with human\nannotations, outperform other zero-shot and prompt-free methods on semantic\nsimilarity tasks, and can be used to solve more complex entailment and\ncontainment tasks that standard embeddings cannot handle. Finally, we extend\nour method to represent data from different modalities (e.g., image and text)\nusing multimodal autoregressive models. Our code is available at:\nhttps://github.com/tianyu139/meaning-as-trajectories",
        "pdf_link": "https://arxiv.org/pdf/2310.18348v3.pdf",
        "keywords": [
            "trajectories",
            "meaning representations",
            "semantics",
            "language"
        ],
        "venue": "DATA",
        "year": "2023",
        "source_file": "data_collected_July2023-Jan2024_tnt_kid.json"
    },
    {
        "title": "Can large language models generate salient negative statements?",
        "authors": [
            "Hiba Arnaout",
            "Simon Razniewski"
        ],
        "published": "2023-05-26T09:13:59Z",
        "summary": "We examine the ability of large language models (LLMs) to generate salient\n(interesting) negative statements about real-world entities; an emerging\nresearch topic of the last few years. We probe the LLMs using zero- and k-shot\nunconstrained probes, and compare with traditional methods for negation\ngeneration, i.e., pattern-based textual extractions and knowledge-graph-based\ninferences, as well as crowdsourced gold statements. We measure the correctness\nand salience of the generated lists about subjects from different domains. Our\nevaluation shows that guided probes do in fact improve the quality of generated\nnegatives, compared to the zero-shot variant. Nevertheless, using both prompts,\nLLMs still struggle with the notion of factuality of negatives, frequently\ngenerating many ambiguous statements, or statements with negative keywords but\na positive meaning.",
        "pdf_link": "https://arxiv.org/pdf/2305.16755v2.pdf",
        "keywords": [
            "salient negative statements",
            "language models",
            "large language models",
            "graph"
        ],
        "venue": "DATA",
        "year": "2023",
        "source_file": "data_collected_Jan2023-July2023_tnt_kid.json"
    },
    {
        "title": "A Refer-and-Ground Multimodal Large Language Model for Biomedicine",
        "authors": [
            "Xiaoshuang Huang",
            "Haifeng Huang",
            "Lingdong Shen",
            "Yehui Yang",
            "Fangxin Shang",
            "Junwei Liu",
            "Jia Liu"
        ],
        "published": "2024-06-26T07:56:17Z",
        "summary": "With the rapid development of multimodal large language models (MLLMs),\nespecially their capabilities in visual chat through refer and ground\nfunctionalities, their significance is increasingly recognized. However, the\nbiomedical field currently exhibits a substantial gap in this area, primarily\ndue to the absence of a dedicated refer and ground dataset for biomedical\nimages. To address this challenge, we devised the Med-GRIT-270k dataset. It\ncomprises 270k question-and-answer pairs and spans eight distinct medical\nimaging modalities. Most importantly, it is the first dedicated to the\nbiomedical domain and integrating refer and ground conversations. The key idea\nis to sample large-scale biomedical image-mask pairs from medical segmentation\ndatasets and generate instruction datasets from text using chatGPT.\nAdditionally, we introduce a Refer-and-Ground Multimodal Large Language Model\nfor Biomedicine (BiRD) by using this dataset and multi-task instruction\nlearning. Extensive experiments have corroborated the efficacy of the\nMed-GRIT-270k dataset and the multi-modal, fine-grained interactive\ncapabilities of the BiRD model. This holds significant reference value for the\nexploration and development of intelligent biomedical assistants.",
        "pdf_link": "https://arxiv.org/pdf/2406.18146v2.pdf",
        "keywords": [
            "visual chat",
            "language model",
            "multi task instruction learning"
        ],
        "venue": "DATA",
        "year": "2024",
        "source_file": "data_collected_Apr2024_Nov2024_tnt_kid.json"
    },
    {
        "title": "WorldMedQA-V: a multilingual, multimodal medical examination dataset for multimodal language models evaluation",
        "authors": [
            "João Matos",
            "Shan Chen",
            "Siena Placino",
            "Yingya Li",
            "Juan Carlos Climent Pardo",
            "Daphna Idan",
            "Takeshi Tohyama",
            "David Restrepo",
            "Luis F. Nakayama",
            "Jose M. M. Pascual-Leone",
            "Guergana Savova",
            "Hugo Aerts",
            "Leo A. Celi",
            "A. Ian Wong",
            "Danielle S. Bitterman",
            "Jack Gallifant"
        ],
        "published": "2024-10-16T16:31:24Z",
        "summary": "Multimodal/vision language models (VLMs) are increasingly being deployed in\nhealthcare settings worldwide, necessitating robust benchmarks to ensure their\nsafety, efficacy, and fairness. Multiple-choice question and answer (QA)\ndatasets derived from national medical examinations have long served as\nvaluable evaluation tools, but existing datasets are largely text-only and\navailable in a limited subset of languages and countries. To address these\nchallenges, we present WorldMedQA-V, an updated multilingual, multimodal\nbenchmarking dataset designed to evaluate VLMs in healthcare. WorldMedQA-V\nincludes 568 labeled multiple-choice QAs paired with 568 medical images from\nfour countries (Brazil, Israel, Japan, and Spain), covering original languages\nand validated English translations by native clinicians, respectively. Baseline\nperformance for common open- and closed-source models are provided in the local\nlanguage and English translations, and with and without images provided to the\nmodel. The WorldMedQA-V benchmark aims to better match AI systems to the\ndiverse healthcare environments in which they are deployed, fostering more\nequitable, effective, and representative applications.",
        "pdf_link": "https://arxiv.org/pdf/2410.12722v1.pdf",
        "keywords": [
            "multimodal language models",
            "multimodal vision language models",
            "multimodal benchmarking",
            "medical examination dataset"
        ],
        "venue": "DATA",
        "year": "2024",
        "source_file": "data_collected_Apr2024_Nov2024_tnt_kid.json"
    },
    {
        "title": "Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?",
        "authors": [
            "Branislav Pecher",
            "Ivan Srba",
            "Maria Bielikova"
        ],
        "published": "2024-02-20T08:38:24Z",
        "summary": "When solving a task with limited labelled data, researchers can either use a\ngeneral large language model without further update, or use the few examples to\ntune a specialised smaller model. When enough labels are available, the\nspecialised models outperform the general ones on many NLP tasks. In this work,\nwe aim to investigate how many labelled samples are required for the\nspecialised models to achieve this superior performance, while taking the\nresults variance into consideration. Observing the behaviour of prompting,\nin-context learning, fine-tuning and instruction-tuning, identifying their\nbreak-even points when increasing number of labelled training samples across\nthree tasks of varying complexity, we find that the specialised models often\nneed only few samples ($100-1000$) to be on par or better than the general\nones. At the same time, the amount of required labelled data strongly depends\non the task complexity and results variance.",
        "pdf_link": "https://arxiv.org/pdf/2402.12819v1.pdf",
        "keywords": [
            "prompting",
            "tune",
            "fine tuning",
            "labelled",
            "instruction tuning",
            "language model",
            "context learning"
        ],
        "venue": "DATA",
        "year": "2024",
        "source_file": "data_collected_Jan2024-31March2024_tnt_kid.json"
    },
    {
        "title": "Comprehensive Study on German Language Models for Clinical and Biomedical Text Understanding",
        "authors": [
            "Ahmad Idrissi-Yaghir",
            "Amin Dada",
            "Henning Schäfer",
            "Kamyar Arzideh",
            "Giulia Baldini",
            "Jan Trienes",
            "Max Hasin",
            "Jeanette Bewersdorff",
            "Cynthia S. Schmidt",
            "Marie Bauer",
            "Kaleb E. Smith",
            "Jiang Bian",
            "Yonghui Wu",
            "Jörg Schlötterer",
            "Torsten Zesch",
            "Peter A. Horn",
            "Christin Seifert",
            "Felix Nensa",
            "Jens Kleesiek",
            "Christoph M. Friedrich"
        ],
        "published": "2024-04-08T17:24:04Z",
        "summary": "Recent advances in natural language processing (NLP) can be largely\nattributed to the advent of pre-trained language models such as BERT and\nRoBERTa. While these models demonstrate remarkable performance on general\ndatasets, they can struggle in specialized domains such as medicine, where\nunique domain-specific terminologies, domain-specific abbreviations, and\nvarying document structures are common. This paper explores strategies for\nadapting these models to domain-specific requirements, primarily through\ncontinuous pre-training on domain-specific data. We pre-trained several German\nmedical language models on 2.4B tokens derived from translated public English\nmedical data and 3B tokens of German clinical data. The resulting models were\nevaluated on various German downstream tasks, including named entity\nrecognition (NER), multi-label classification, and extractive question\nanswering. Our results suggest that models augmented by clinical and\ntranslation-based pre-training typically outperform general domain models in\nmedical contexts. We conclude that continuous pre-training has demonstrated the\nability to match or even exceed the performance of clinical models trained from\nscratch. Furthermore, pre-training on clinical data or leveraging translated\ntexts have proven to be reliable methods for domain adaptation in medical NLP\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2404.05694v2.pdf",
        "keywords": [
            "language models",
            "named entity recognition",
            "german medical language models",
            "german language models",
            "translated",
            "natural language processing",
            "domain adaptation"
        ],
        "venue": "DATA",
        "year": "2024",
        "source_file": "data_collected_Apr2024_Nov2024_tnt_kid.json"
    },
    {
        "title": "Understanding Likelihood Over-optimisation in Direct Alignment Algorithms",
        "authors": [
            "Zhengyan Shi",
            "Sander Land",
            "Acyr Locatelli",
            "Matthieu Geist",
            "Max Bartolo"
        ],
        "published": "2024-10-15T15:14:22Z",
        "summary": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimisation\n(DPO) and Identity Preference Optimisation (IPO), have emerged as alternatives\nto online Reinforcement Learning from Human Feedback (RLHF) algorithms such as\nProximal Policy Optimisation (PPO) for aligning language models to human\npreferences, without the need for explicit reward modelling. These methods\ngenerally aim to increase the likelihood of generating better (preferred)\ncompletions while discouraging worse (non-preferred) ones, while staying close\nto the original model's behaviour. In this work, we explore the relationship\nbetween completion likelihood and model performance in state-of-the-art DAAs,\nand identify a critical issue of likelihood over-optimisation. Contrary to\nexpectations, we find that higher likelihood of better completions and larger\nmargins between better and worse completion likelihoods do not necessarily lead\nto better performance, and may even degrade it. Our analysis reveals that while\nhigher likelihood correlates with better memorisation of factual knowledge\npatterns, a slightly lower completion likelihood tends to improve output\ndiversity, thus leading to better generalisation to unseen scenarios. Moreover,\nwe identify two key indicators that signal when over-optimised output diversity\nbegins to harm performance: Decreasing Entropy over Top-k Tokens and\nDiminishing Top-k Probability Mass. Our experimental results validate that\nthese indicators are reliable signs of declining performance under different\nregularisations, helping prevent over-optimisation and improve alignment with\nhuman preferences.",
        "pdf_link": "https://arxiv.org/pdf/2410.11677v2.pdf",
        "keywords": [
            "likelihood",
            "completion likelihood",
            "direct preference optimisation",
            "preference optimisation",
            "direct alignment algorithms",
            "dpo",
            "proximal"
        ],
        "venue": "DATA",
        "year": "2024",
        "source_file": "data_collected_Apr2024_Nov2024_tnt_kid.json"
    },
    {
        "title": "Mind the Uncertainty in Human Disagreement: Evaluating Discrepancies between Model Predictions and Human Responses in VQA",
        "authors": [
            "Jian Lan",
            "Diego Frassinelli",
            "Barbara Plank"
        ],
        "published": "2024-09-17T13:44:25Z",
        "summary": "Large vision-language models frequently struggle to accurately predict\nresponses provided by multiple human annotators, particularly when those\nresponses exhibit human uncertainty. In this study, we focus on the Visual\nQuestion Answering (VQA) task, and we comprehensively evaluate how well the\nstate-of-the-art vision-language models correlate with the distribution of\nhuman responses. To do so, we categorize our samples based on their levels\n(low, medium, high) of human uncertainty in disagreement (HUD) and employ not\nonly accuracy but also three new human-correlated metrics in VQA, to\ninvestigate the impact of HUD. To better align models with humans, we also\nverify the effect of common calibration and human calibration. Our results show\nthat even BEiT3, currently the best model for this task, struggles to capture\nthe multi-label distribution inherent in diverse human responses. Additionally,\nwe observe that the commonly used accuracy-oriented calibration technique\nadversely affects BEiT3's ability to capture HUD, further widening the gap\nbetween model predictions and human distributions. In contrast, we show the\nbenefits of calibrating models towards human distributions for VQA, better\naligning model confidence with human uncertainty. Our findings highlight that\nfor VQA, the consistent alignment between human responses and model predictions\nis understudied and should become the next crucial target of future studies.",
        "pdf_link": "https://arxiv.org/pdf/2410.02773v1.pdf",
        "keywords": [
            "hud",
            "disagreement",
            "visual question answering",
            "uncertainty",
            "discrepancies"
        ],
        "venue": "DATA",
        "year": "2024",
        "source_file": "data_collected_Apr2024_Nov2024_tnt_kid.json"
    },
    {
        "title": "Exploring LLM-based Data Annotation Strategies for Medical Dialogue Preference Alignment",
        "authors": [
            "Chengfeng Dou",
            "Ying Zhang",
            "Zhi Jin",
            "Wenpin Jiao",
            "Haiyan Zhao",
            "Yongqiang Zhao",
            "Zhengwei Tao"
        ],
        "published": "2024-10-05T10:29:19Z",
        "summary": "This research examines the use of Reinforcement Learning from AI Feedback\n(RLAIF) techniques to improve healthcare dialogue models, with the aim of\ntackling the challenges of preference-aligned data annotation while reducing\nthe reliance on medical experts. We argue that the primary challenges in\ncurrent RLAIF research for healthcare are the limitations of automated\nevaluation methods and the difficulties in accurately representing physician\npreferences. To address these challenges, we present a new evaluation framework\nbased on standardized patient examinations. This framework is designed to\nobjectively assess the effectiveness of large language models (LLMs) in guiding\nusers and following instructions, enabling a comprehensive comparison across\ndifferent models. Furthermore, our investigation of effective ways to express\nphysician preferences using Constitutional AI algorithms highlighted the\nparticular effectiveness of flowcharts. Utilizing this finding, we introduce an\ninnovative agent-based approach for annotating preference data. This approach\nautonomously creates medical dialogue flows tailored to the patient's\ncondition, demonstrates strong generalization abilities, and reduces the need\nfor expert involvement. Our results show that the agent-based approach\noutperforms existing RLAIF annotation methods in standardized patient\nexaminations and surpasses current open source medical dialogue LLMs in various\ntest scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2410.04112v1.pdf",
        "keywords": [
            "annotation",
            "data annotation",
            "preference aligned data annotation",
            "medical dialogue preference alignment",
            "medical dialogue",
            "reinforcement learning",
            "healthcare dialogue models",
            "patient"
        ],
        "venue": "DATA",
        "year": "2024",
        "source_file": "data_collected_Apr2024_Nov2024_tnt_kid.json"
    },
    {
        "title": "OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching",
        "authors": [
            "Zhangcheng Qiang",
            "Kerry Taylor",
            "Weiqing Wang",
            "Jing Jiang"
        ],
        "published": "2024-09-21T06:49:34Z",
        "summary": "Hallucinations of large language models (LLMs) commonly occur in\ndomain-specific downstream tasks, with no exception in ontology matching (OM).\nThe prevalence of using LLMs for OM raises the need for benchmarks to better\nunderstand LLM hallucinations. The OAEI-LLM dataset is an extended version of\nthe Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate\nLLM-specific hallucinations in OM tasks. We outline the methodology used in\ndataset construction and schema extension, and provide examples of potential\nuse cases.",
        "pdf_link": "https://arxiv.org/pdf/2409.14038v3.pdf",
        "keywords": [
            "ontology",
            "ontology matching",
            "language model hallucinations"
        ],
        "venue": "DATA",
        "year": "2024",
        "source_file": "data_collected_Apr2024_Nov2024_tnt_kid.json"
    },
    {
        "title": "When Does Differentially Private Learning Not Suffer in High Dimensions?",
        "authors": [
            "Xuechen Li",
            "Daogao Liu",
            "Tatsunori Hashimoto",
            "Huseyin A. Inan",
            "Janardhan Kulkarni",
            "Yin Tat Lee",
            "Abhradeep Guha Thakurta"
        ],
        "published": "2022-07-01T02:36:51Z",
        "summary": "Large pretrained models can be privately fine-tuned to achieve performance\napproaching that of non-private models. A common theme in these results is the\nsurprising observation that high-dimensional models can achieve favorable\nprivacy-utility trade-offs. This seemingly contradicts known results on the\nmodel-size dependence of differentially private convex learning and raises the\nfollowing research question: When does the performance of differentially\nprivate learning not degrade with increasing model size? We identify that the\nmagnitudes of gradients projected onto subspaces is a key factor that\ndetermines performance. To precisely characterize this for private convex\nlearning, we introduce a condition on the objective that we term\n\\emph{restricted Lipschitz continuity} and derive improved bounds for the\nexcess empirical and population risks that are dimension-independent under\nadditional conditions. We empirically show that in private fine-tuning of large\nlanguage models, gradients obtained during fine-tuning are mostly controlled by\na few principal components. This behavior is similar to conditions under which\nwe obtain dimension-independent bounds in convex settings. Our theoretical and\nempirical results together provide a possible explanation for recent successes\nin large-scale private fine-tuning. Code to reproduce our results can be found\nat\n\\url{https://github.com/lxuechen/private-transformers/tree/main/examples/classification/spectral_analysis}.",
        "pdf_link": "https://arxiv.org/pdf/2207.00160v4.pdf",
        "keywords": [
            "differentially private learning",
            "private convex learning",
            "fine tuning"
        ],
        "venue": "DATA",
        "year": "2022",
        "source_file": "data_collected_July2022-Jan2023_tnt_kid.json"
    }
]