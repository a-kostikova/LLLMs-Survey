{
    "prompt1": "Evaluate the following paper's title and abstract to determine if it discusses Large Language Models (LLMs) and whether it addresses their limitations. If it does, indicate specific evidence from the abstract or title that points to the limitations and identify the domain of the limitation where possible. Note that LMs and LLMs include pre-trained transformer-based models, foundational models, and multimodal models integrating language with other modalities. Include all kinds of language models but exclude other, more general models.\n\n        Please answer in the following format by providing the rating and a brief evidence for each abstract. Please do not give the respective Explanations, only the evidence found in the abstract.\nAnswer in the following way:\nDoes it talk about LLMs: [Yes/No]\nRate Limitations of LLMs: [0-5]\nEvidence: [the evidence text in the abstract or title].\nTitle: {title}\nPaper: {summary}",
    "prompt2": "Evaluate the following paper's title and abstract to determine if it discusses Large Language Models (LLMs) and whether it addresses their limitations. If it does, indicate specific evidence from the abstract or title that points to the limitations and identify the domain of the limitation where possible. Note that LMs and LLMs include pre-trained transformer-based models, foundational models, and multimodal models integrating language with other modalities. Include all kinds of language models but exclude other, more general models.\n\n        Rate the depth of LLM limitations discussion (0–5) using the following scale:\n\n        0: No discussion of LLMs at all.\n        1: Discusses LLMs but does not mention any limitations of LLMs.\n        2: The abstract mentions one or more limitations of LLMs in passing or as a minor detail. The limitations are not explained, elaborated, or analyzed further and are primarily used to justify the paper's goals, methods, or contributions.\n        3: The abstract discusses one or two limitations of LLMs in moderate detail. These limitations are important but are not the primary focus of the abstract. The discussion provides some analysis, examples, or implications, but the abstract emphasizes the solution, methodology, or results more than the limitations.\n        4: The abstract dedicates significant attention to one or more limitations of LLMs, making them a major focus. The limitations are described in detail, with examples, analysis, or experimental evidence. While other aspects (e.g., solutions or results) may be discussed, the limitations play an equally or more important role in the narrative.\n        5: Entirely focused on LLM limitations and challenges.\n        \n        Please answer in the following format by providing the rating and a brief evidence for each abstract. Please do not give the respective Explanations, only the evidence found in the abstract.\nAnswer in the following way:\nDoes it talk about LLMs: [Yes/No]\nRate Limitations of LLMs: [0-5]\nEvidence: [the evidence text in the abstract or title].\nTitle: {title}\nPaper: {summary}",
    "prompt3": "Evaluate the following paper's title and abstract to determine if it discusses Large Language Models (LLMs) and whether it addresses their limitations. If it does, indicate specific evidence from the abstract or title that points to the limitations and identify the domain of the limitation where possible. Note that LMs and LLMs include pre-trained transformer-based models, foundational models, and multimodal models integrating language with other modalities. Include all kinds of language models but exclude other, more general models.\n\n        Rate the depth of LLM limitations discussion (0–5) using the following scale:\n\n        0: No discussion of LLMs at all.\n        1: Discusses LLMs but does not mention any limitations of LLMs.\n        2: The abstract mentions one or more limitations of LLMs in passing or as a minor detail. The limitations are not explained, elaborated, or analyzed further and are primarily used to justify the paper's goals, methods, or contributions.\n        3: The abstract discusses one or two limitations of LLMs in moderate detail. These limitations are important but are not the primary focus of the abstract. The discussion provides some analysis, examples, or implications, but the abstract emphasizes the solution, methodology, or results more than the limitations.\n        4: The abstract dedicates significant attention to one or more limitations of LLMs, making them a major focus. The limitations are described in detail, with examples, analysis, or experimental evidence. While other aspects (e.g., solutions or results) may be discussed, the limitations play an equally or more important role in the narrative.\n        5: Entirely focused on LLM limitations and challenges.\n        \n        Please look at the following examples alongside the explanations on why decided the respective ratings and rate the other abstracts from 0 to 5 accordingly by following the same logic as below: \n\n        **Example Output 0:**\n        Title: Simultaneous Selection and Adaptation of Source Data via Four-Level Optimization\n        Paper: \"In many NLP applications, to mitigate data deficiency in a target task, source data is collected to help with target model training. Existing transfer learning methods either select a subset of source examples that are close to the target domain or try to adapt all source examples into the target domain, then use selected or adapted source examples to train the target model. These methods either incur significant information loss or bear the risk that after adaptation, source examples which are originally already in the target domain may be outside the target domain. To address the limitations of these methods, we propose a four-level optimization based framework which simultaneously selects and adapts source data. Our method can automatically identify in-domain and out-of-domain source examples and apply example-specific processing methods: selection for in-domain examples and adaptation for out-of-domain examples. Experiments on various datasets demonstrate the effectiveness of our proposed method.\"\n        Does it talk about LLMs: No\n        Rate Limitations of LLMs: 0\n        Evidence: No evidence of discussion of limitations of LLMs.\n\n        Output Explanation: This paper should be rated with 0 since this paper does not talk about LLMs or any language model at all.\n\n        **Example Output 1:**\n        Title: SPAE Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs\n        Paper: \"In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%\"\n        Does it talk about LLMs: Yes\n        Rate Limitations of LLMs: 1\n        Evidence: \"In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos.\"\n        \n        Output Explanation: This paper should be rated with 1 since even though it talks about LLMs, it does not mention any explicit limitation of the models in the abstract. \n        \n        **Example Output 2:**\n        Title: Large Language Models for Conducting Advanced Text Analytics Information Systems Research\n        Paper: \"The exponential growth of digital content has generated massive textual datasets, necessitating advanced analytical approaches. Large Language Models (LLMs) have emerged as tools capable of processing and extracting insights from massive unstructured textual datasets. However, how to leverage LLMs for text-based Information Systems (IS) research is currently unclear. To assist IS research in understanding how to operationalize LLMs, we propose a Text Analytics for Information Systems Research (TAISR) framework. Our proposed framework provides detailed recommendations grounded in IS and LLM literature on how to conduct meaningful text-based IS research. We conducted three case studies in business intelligence using our TAISR framework to demonstrate its application across several IS research contexts. We also outline potential challenges and limitations in adopting LLMs for IS. By offering a systematic approach and evidence of its utility, our TAISR framework contributes to future IS research streams looking to incorporate powerful LLMs for text analytics.\"\n        Does it talk about LLMs: Yes\n        Rate Limitations of LLMs: 2\n        Evidence: \"We also outline potential challenges and limitations in adopting LLMs for IS.\"\n        \n        Output Explanation: This abstract mentions just briefly one limitation of the Large Language Models without going into detail and focuses on other topics.\n        \n        **Example Output 3:**\n        Title: Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning\n        Paper: \"In recent years, Large Language Models (LLMs) have shown remarkable performance in generating human-like text, proving to be a valuable asset across various applications. However, adapting these models to incorporate new, out-of-domain knowledge remains a challenge, particularly for facts and events that occur after the model’s knowledge cutoff date. This paper investigates the effectiveness of Supervised Fine-Tuning (SFT) as a method for knowledge injection in LLMs, specifically focusing on the domain of recent sporting events. We compare different dataset generation strategies -- token-based and fact-based scaling -- to create training data that helps the model learn new information. Our experiments on GPT-4 demonstrate that while token-based scaling can lead to improvements in Q&A accuracy, it may not provide uniform coverage of new knowledge. Fact-based scaling, on the other hand, offers a more systematic approach to ensure even coverage across all facts. We present a novel dataset generation process that leads to more effective knowledge ingestion through SFT, and our results show considerable performance improvements in Q&A tasks related to out-of-domain knowledge. This study contributes to the understanding of domain adaptation for LLMs and highlights the potential of SFT in enhancing the factuality of LLM responses in specific knowledge domains.\"\n        Does it talk about LLMs: Yes\n        Rate Limitations of LLMs: 3\n        Evidence: \"However, adapting these models to incorporate new, out-of-domain knowledge remains a challenge, particularly for facts and events that occur after the model’s knowledge cutoff date.”; “Our experiments on GPT-4 demonstrate that while token-based scaling can lead to improvements in Q&A accuracy, it may not provide uniform coverage of new knowledge.”\n        \n        Output Explanation: The limitation (difficulty in adapting to new knowledge) is mentioned but not explored in depth (e.g., why LLMs struggle with knowledge injection, etc.). The primary focus of the paper is on the proposed solution. \n        \n        **Example Output 4:**\n        Title: Red Teaming Language Model Detectors with Language Models\n        Paper: \"The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users. To prevent the potentially deceptive usage of LLMs, recent works have proposed algorithms to detect LLM-generated text and protect LLMs. In this paper, we investigate the robustness and reliability of these LLM detectors under adversarial attacks. We study two types of attack strategies: 1) replacing certain words in an LLM's output with their synonyms given the context; 2) automatically searching for an instructional prompt to alter the writing style of the generation. In both strategies, we leverage an auxiliary LLM to generate the word replacements or the instructional prompt. Different from previous works, we consider a challenging setting where the auxiliary LLM can also be protected by a detector. Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems.\"\n        Does it talk about LLMs: Yes\n        Rate Limitations of LLMs: 4\n        Evidence: \"The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users.”; \"Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems.\"\n        \n        Output Explanation: The paper extensively analyzes vulnerabilities in LLM detection systems under adversarial attacks. Limitation is a major focus, but mixed with technical contributions and experiments.\n        \n        **Example Output 5:**\n        Title: LLMs for Relational Reasoning: How Far are We?\n        Paper: \"Large language models (LLMs) have revolutionized many areas (e.g. natural language processing, software engineering, etc.) by achieving state-of-the-art performance on extensive downstream tasks. Aiming to achieve robust and general artificial intelligence, there has been a surge of interest in investigating the reasoning ability of the LLMs. Whereas the textual and numerical reasoning benchmarks adopted by previous works are rather shallow and simple, it is hard to conclude that the LLMs possess strong reasoning ability by merely achieving positive results on these benchmarks. Recent efforts have demonstrated that the LLMs are poor at solving sequential decision-making problems that require common-sense planning by evaluating their performance on the reinforcement learning benchmarks. In this work, we conduct an in-depth assessment of several state-of-the-art LLMs’ reasoning ability based on the inductive logic programming (ILP) benchmark, which is broadly recognized as a representative and challenging measurement for evaluating logic program induction/synthesis systems as it requires inducing strict cause-effect logic to achieve robust deduction on independent and identically distributed (IID) and out-of-distribution (OOD) test samples. Our evaluations illustrate that compared with the neural program induction systems which are much smaller in model size, the state-of-the-art LLMs are much poorer in terms of reasoning ability by achieving much lower performance and generalization using either natural language prompting or truth-value matrix prompting.\"\n        Does it talk about LLMs: Yes\n        Rate Limitations of LLMs: 5\n        Evidence: \"Whereas the textual and numerical reasoning benchmarks adopted by previous works are rather shallow and simple, it is hard to conclude that the LLMs possess strong reasoning ability by merely achieving positive results on these benchmarks. Recent efforts have demonstrated that the LLMs are poor at solving sequential decision-making problems that require common-sense planning by evaluating their performance on the reinforcement learning benchmarks.”; “Our evaluations illustrate that compared with the neural program induction systems which are much smaller in model size, the state-of-the-art LLMs are much poorer in terms of reasoning ability by achieving much lower performance and generalization using either natural language prompting or truth-value matrix prompting.”\n        \n        Output Explanation: The paper’s primary focus is a detailed exploration of LLMs’ reasoning limitations, rather than just using these limitations to motivate a solution or another contribution.\n        \n        Please answer in the following format by providing the rating and a brief evidence for each abstract. Please do not give the respective Explanations, only the evidence found in the abstract. \n        \n        Answer in the following way:\n        Does it talk about LLMs: [Yes/No]\n        Rate Limitations of LLMs: [0-5]\n        Evidence: [the evidence text in the abstract or title].\n        \n        Title: {title}\n        Paper: {summary}"
}
