[
    {
        "title": "Context-Aware Sparse Deep Coordination Graphs",
        "authors": [
            "Tonghan Wang",
            "Liang Zeng",
            "Weijun Dong",
            "Qianlan Yang",
            "Yang Yu",
            "Chongjie Zhang"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Learning sparse coordination graphs adaptive to the coordination dynamics among agents is a long-standing problem in cooperative multi-agent learning. This paper studies this problem and proposes a novel method using the variance of payoff functions to construct context-aware sparse coordination topologies. We theoretically consolidate our method by proving that the smaller the variance of payoff functions is, the less likely action selection will change after removing the corresponding edge. Moreover, we propose to learn action representations to effectively reduce the influence of payoff functions' estimation errors on graph construction. To empirically evaluate our method, we present the Multi-Agent COordination (MACO) benchmark by collecting classic coordination problems in the literature, increasing their difficulty, and classifying them into different types. We carry out a case study and experiments on the MACO and StarCraft II micromanagement benchmark to demonstrate the dynamics of sparse graph learning, the influence of graph sparseness, and the learning performance of our method.",
        "pdf_link": "https://openreview.net/pdf/a4e94260f9a234c7a8d59eb139b8f31b32a87673.pdf",
        "forum_url": "https://openreview.net/forum?id=wQfgfb8VKTn"
    },
    {
        "title": "8-bit Optimizers via Block-wise Quantization",
        "authors": [
            "Tim Dettmers",
            "Mike Lewis",
            "Sam Shleifer",
            "Luke Zettlemoyer"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Stateful optimizers maintain gradient statistics over time, e.g., the exponentially smoothed sum (SGD with momentum) or squared sum (Adam) of past gradient values. This state can be used to accelerate optimization significantly, compared to plain stochastic gradient descent, but uses memory that might otherwise be allocated to model parameters, thereby limiting the maximum size of models trained in practice. In this paper, we develop the first optimizers that use 8-bit statistics while maintaining the performance levels of using 32-bit optimizer states. To overcome the resulting computational, quantization, and stability challenges, we develop block-wise dynamic quantization. Block-wise quantization divides input tensors into smaller blocks that are independently quantized. Each block is processed in parallel across cores, yielding faster optimization and high precision quantization. To maintain stability and performance, we combine block-wise quantization with two additional changes: (1) dynamic quantization, a form of non-linear optimization that is precise for both large and small magnitude values, and (2) a stable embedding layer to reduce gradient variance that comes from the highly non-uniform distribution of input tokens in language models. As a result, our 8-bit optimizers maintain 32-bit performance with a small fraction of the memory footprint on a range of tasks, including 1.5B parameter language modeling, GLUE finetuning, ImageNet classification, WMT'14 machine translation, MoCo v2 contrastive ImageNet pretraining+finetuning, and RoBERTa pretraining, without changes to the original optimizer hyperparameters. We open-source our 8-bit optimizers as a drop-in replacement that only requires a two-line code change.",
        "pdf_link": "https://openreview.net/pdf/eae16788bf15e102fb9f104d044c6dff582683f4.pdf",
        "forum_url": "https://openreview.net/forum?id=shpkpVXzo3h"
    },
    {
        "title": "CoBERL: Contrastive BERT for Reinforcement Learning",
        "authors": [
            "Andrea Banino",
            "Adria Puigdomenech Badia",
            "Jacob C Walker",
            "Tim Scholtes",
            "Jovana Mitrovic",
            "Charles Blundell"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Many reinforcement learning (RL) agents require a large amount of experience to solve tasks. We propose Contrastive BERT for RL (COBERL), an agent that combines a new contrastive loss and a hybrid LSTM-transformer architecture to tackle the challenge of improving data efficiency. COBERL enables efficient and robust learning from pixels across a wide variety of domains. We use bidirectional masked prediction in combination with a generalization of a recent contrastive method to learn better representations for RL, without the need of hand engineered data augmentations. We find that COBERL consistently improves data efficiency across the full Atari suite, a set of control tasks and a challenging 3D environment, and often it also increases final score performance.",
        "pdf_link": "https://openreview.net/pdf/c833364a7435330b3ee8e71a2020d1172e9d3380.pdf",
        "forum_url": "https://openreview.net/forum?id=sRZ3GhmegS"
    },
    {
        "title": "Language modeling via stochastic processes",
        "authors": [
            "Rose E Wang",
            "Esin Durmus",
            "Noah Goodman",
            "Tatsunori Hashimoto"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Modern language models can generate high-quality short texts. However, they often meander or are incoherent when generating longer texts. These issues arise from the next-token-only language modeling objective. To address these issues, we introduce Time Control (TC), a language model that implicitly plans via a latent stochastic process. TC does this by learning a representation which maps the dynamics of how text changes in a document to the dynamics of a stochastic process of interest. Using this representation, the language model can generate text by first implicitly generating a document plan via a stochastic process, and then generating text that is consistent with this latent plan. Compared to domain-specific methods and fine-tuning GPT2 across a variety of text domains, TC improves performance on text infilling and discourse coherence. On long text generation settings, TC preserves the text structure both in terms of ordering (up to +40% better) and text length consistency (up to +17% better).  Human evaluators also prefer TC's output 28.6% more than the baselines.",
        "pdf_link": "https://openreview.net/pdf/ceeec650a60b1f87ad4dda26ecd02c9df0e3ed9d.pdf",
        "forum_url": "https://openreview.net/forum?id=pMQwKL1yctf"
    },
    {
        "title": "Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation",
        "authors": [
            "Alex Rogozhnikov"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Tensor computations underlie modern scientific computing and deep learning.\nA number of tensor frameworks emerged varying in execution model, hardware support, memory management, model definition, etc.\nHowever, tensor operations in all frameworks follow the same paradigm.\nRecent neural network architectures demonstrate demand for higher expressiveness of tensor operations.\nThe current paradigm is not suited to write readable, reliable, or easy-to-modify code for multidimensional tensor manipulations. \nMoreover, some commonly used operations do not provide sufficient checks and can break a tensor structure.\nThese mistakes are elusive as no tools or tests can detect them.\nIndependently, API discrepancies complicate code transfer between frameworks.\nWe propose einops notation: a uniform and generic way to manipulate tensor structure, that significantly improves code readability and flexibility by focusing on the structure of input and output tensors.\nWe implement einops notation in a Python package that efficiently supports multiple widely used frameworks and provides framework-independent minimalist API for tensor manipulations.",
        "pdf_link": "https://openreview.net/pdf/d568f6e36eaa377888611b8e0d84076777edc330.pdf",
        "forum_url": "https://openreview.net/forum?id=oapKSVM2bcj"
    },
    {
        "title": "Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers",
        "authors": [
            "Ruihan Yang",
            "Minghao Zhang",
            "Nicklas Hansen",
            "Huazhe Xu",
            "Xiaolong Wang"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "We propose to address quadrupedal locomotion tasks using Reinforcement Learning (RL) with a Transformer-based model that learns to combine proprioceptive information and high-dimensional depth sensor inputs. While learning-based locomotion has made great advances using RL, most methods still rely on domain randomization for training blind agents that generalize to challenging terrains. Our key insight is that proprioceptive states only offer contact measurements for immediate reaction, whereas an agent equipped with visual sensory observations can learn to proactively maneuver environments with obstacles and uneven terrain by anticipating changes in the environment many steps ahead. In this paper, we introduce LocoTransformer, an end-to-end RL method that leverages both proprioceptive states and visual observations for locomotion control. We evaluate our method in challenging simulated environments with different obstacles and uneven terrain. We transfer our learned policy from simulation to a real robot by running it indoor and in-the-wild with unseen obstacles and terrain. Our method not only significantly improves over baselines, but also achieves far better generalization performance, especially when transferred to the real robot. Our project page with videos is at https://rchalyang.github.io/LocoTransformer/.",
        "pdf_link": "https://openreview.net/pdf/44a74037c4089730c34de558a6d679925f866a56.pdf",
        "forum_url": "https://openreview.net/forum?id=nhnJ3oo6AB"
    },
    {
        "title": "Learning Long-Term Reward Redistribution via Randomized Return Decomposition",
        "authors": [
            "Zhizhou Ren",
            "Ruihan Guo",
            "Yuan Zhou",
            "Jian Peng"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Many practical applications of reinforcement learning require agents to learn from sparse and delayed rewards. It challenges the ability of agents to attribute their actions to future outcomes. In this paper, we consider the problem formulation of episodic reinforcement learning with trajectory feedback. It refers to an extreme delay of reward signals, in which the agent can only obtain one reward signal at the end of each trajectory. A popular paradigm for this problem setting is learning with a designed auxiliary dense reward function, namely proxy reward, instead of sparse environmental signals. Based on this framework, this paper proposes a novel reward redistribution algorithm, randomized return decomposition (RRD), to learn a proxy reward function for episodic reinforcement learning. We establish a surrogate problem by Monte-Carlo sampling that scales up least-squares-based reward redistribution to long-horizon problems. We analyze our surrogate loss function by connection with existing methods in the literature, which illustrates the algorithmic properties of our approach. In experiments, we extensively evaluate our proposed method on a variety of benchmark tasks with episodic rewards and demonstrate substantial improvement over baseline algorithms.",
        "pdf_link": "https://openreview.net/pdf/9d37ef647d9bba0c4b6fe1976563d07da74d311e.pdf",
        "forum_url": "https://openreview.net/forum?id=lpkGn3k2YdD"
    },
    {
        "title": "The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design",
        "authors": [
            "Yoav Levine",
            "Noam Wies",
            "Daniel Jannai",
            "Dan Navon",
            "Yedid Hoshen",
            "Amnon Shashua"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Pretraining Neural Language Models (NLMs) over a large corpus involves chunking the text into training examples, which are contiguous text segments of sizes processable by the neural architecture. We highlight a bias introduced by this common practice: we prove that the pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segments that appeared in different training examples. This intuitive result has a twofold role. First, it formalizes the motivation behind a broad line of recent successful NLM training heuristics, proposed for the pretraining and fine-tuning stages, which do not necessarily appear related at first glance. Second, our result clearly indicates further improvements to be made in NLM pretraining for the benefit of Natural Language Understanding tasks. As an example, we propose ``kNN-Pretraining\": we show that including semantically related non-neighboring sentences in the same pretraining example yields improved sentence representations and open domain question answering abilities.\tThis theoretically motivated degree of freedom for pretraining example design indicates new training schemes for self-improving representations. ",
        "pdf_link": "https://openreview.net/pdf/da15c2bd56f71f3d484f0da8f8b25aea19884b0a.pdf",
        "forum_url": "https://openreview.net/forum?id=lnEaqbTJIRz"
    },
    {
        "title": "Finetuned Language Models are Zero-Shot Learners",
        "authors": [
            "Jason Wei",
            "Maarten Bosma",
            "Vincent Zhao",
            "Kelvin Guu",
            "Adams Wei Yu",
            "Brian Lester",
            "Nan Du",
            "Andrew M. Dai",
            "Quoc V Le"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning—finetuning language models on a collection of datasets described via instructions—substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",
        "pdf_link": "https://openreview.net/pdf/16b50405ab1e3ac1e2f76190ee62a48c496c568d.pdf",
        "forum_url": "https://openreview.net/forum?id=gEZrGCozdqR"
    },
    {
        "title": "Learning more skills through optimistic exploration",
        "authors": [
            "DJ Strouse",
            "Kate Baumli",
            "David Warde-Farley",
            "Volodymyr Mnih",
            "Steven Stenberg Hansen"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Unsupervised skill learning objectives (Eysenbach et al., 2019; Gregor et al., 2016) allow agents to learn rich repertoires of behavior in the absence of extrinsic rewards. They work by simultaneously training a policy to produce distinguishable latent-conditioned trajectories, and a discriminator to evaluate distinguishability by trying to infer latents from trajectories. The hope is for the agent to explore and master the environment by encouraging each skill (latent) to reliably reach different states. However, an inherent exploration problem lingers: when a novel state is actually encountered, the discriminator will necessarily not have seen enough training data to produce accurate and confident skill classifications, leading to low intrinsic reward for the agent and effective penalization of the sort of exploration needed to actually maximize the objective. To combat this inherent pessimism towards exploration, we derive an information gain auxiliary objective that involves training an ensemble of discriminators and rewarding the policy for their disagreement. Our objective directly estimates the epistemic uncertainty that comes from the discriminator not having seen enough training examples, thus providing an intrinsic reward more tailored to the true objective compared to pseudocount-based methods (Burda et al., 2019). We call this exploration bonus discriminator disagreement intrinsic reward, or DISDAIN. We demonstrate empirically that DISDAIN improves skill learning both in a tabular grid world (Four Rooms) and the 57 games of the Atari Suite (from pixels). Thus, we encourage researchers to treat pessimism with DISDAIN.",
        "pdf_link": "https://openreview.net/pdf/fd77361933d33f5982e69d08631cf6222a3c48ce.pdf",
        "forum_url": "https://openreview.net/forum?id=cU8rknuhxc"
    },
    {
        "title": "Large Language Models Can Be Strong Differentially Private Learners",
        "authors": [
            "Xuechen Li",
            "Florian Tramer",
            "Percy Liang",
            "Tatsunori Hashimoto"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Differentially Private (DP) learning has seen limited success for building large deep learning models of text, and straightforward attempts at applying Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have resulted in large performance drops and high computational overhead.\nWe show that this performance drop can be mitigated with (1) the use of large pretrained language models; (2) non-standard hyperparameters that suit DP optimization; and (3) fine-tuning objectives which are aligned with the pretraining procedure.\nWith the above, we obtain NLP models that outperform state-of-the-art DP-trained models under the same privacy budget and strong non-private baselines---by directly fine-tuning pretrained models with DP optimization on moderately-sized corpora. \nTo address the computational challenge of running DP-SGD with large Transformers, we propose a memory saving technique that allows clipping in DP-SGD to run without instantiating per-example gradients for any linear layer in the model. \nThe technique enables privately training Transformers with almost the same memory cost as non-private training at a modest run-time overhead. \nContrary to conventional wisdom that DP optimization fails at learning high-dimensional models (due to noise that scales with dimension) empirical results reveal that private learning with pretrained language models tends to not suffer from dimension-dependent performance degradation.\nCode to reproduce results can be found at https://github.com/lxuechen/private-transformers.\n",
        "pdf_link": "https://openreview.net/pdf/d88e1e721c4085b8a6403837f45b8c483ad0225b.pdf",
        "forum_url": "https://openreview.net/forum?id=bVuP3ltATMz"
    },
    {
        "title": "Bootstrapped Meta-Learning",
        "authors": [
            "Sebastian Flennerhag",
            "Yannick Schroecker",
            "Tom Zahavy",
            "Hado van Hasselt",
            "David Silver",
            "Satinder Singh"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Meta-learning empowers artificial intelligence to increase its efficiency by learning how to learn. Unlocking this potential involves overcoming a challenging meta-optimisation problem. We propose an algorithm that tackles this problem by letting the meta-learner teach itself. The algorithm first bootstraps a target from the meta-learner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. Focusing on meta-learning with gradients, we establish conditions that guarantee performance improvements and show that metric can be used to control meta-optimisation. Meanwhile, the bootstrapping mechanism can extend the effective meta-learning horizon without requiring backpropagation through all updates. We achieve a new state-of-the art for model-free agents on the Atari ALE benchmark and demonstrate that it yields both performance and efficiency gains in multi-task meta-learning. Finally, we explore how bootstrapping opens up new possibilities and find that it can meta-learn efficient exploration in an epsilon-greedy Q-learning agent - without backpropagating through the update rule.",
        "pdf_link": "https://openreview.net/pdf/0eccd48eddcbf9cfc77b50cb0e97fb58937aee70.pdf",
        "forum_url": "https://openreview.net/forum?id=b-ny3x071E5"
    },
    {
        "title": "Generative Planning for Temporally Coordinated Exploration in Reinforcement Learning",
        "authors": [
            "Haichao Zhang",
            "Wei Xu",
            "Haonan Yu"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Standard model-free reinforcement learning algorithms optimize a policy that generates the action to be taken in the current time step in order to maximize expected future return. While flexible, it faces difficulties arising from the inefficient exploration due to its single step nature. In this work, we present Generative Planning method (GPM), which can generate actions not only for the current step, but also for a number of future steps (thus termed as generative planning). This brings several benefits to GPM. Firstly,  since GPM is trained by maximizing value, the plans generated from it can be regarded as intentional action sequences for reaching high value regions. GPM can therefore leverage its generated multi-step plans for temporally coordinated exploration towards high value regions, which is potentially more effective than a sequence of actions generated by perturbing each action at single step level, whose consistent movement decays exponentially with the number of exploration steps. Secondly, starting from a crude initial plan generator, GPM can refine it to be adaptive to the task, which, in return, benefits future explorations. This is potentially more effective than commonly used action-repeat strategy, which is non-adaptive in its form of plans. Additionally, since the multi-step plan can be interpreted as the intent of the agent from now to a span of time period into the future, it offers a more informative and intuitive signal for interpretation. Experiments are conducted on several benchmark environments and the results demonstrated its effectiveness compared with several baseline methods.",
        "pdf_link": "https://openreview.net/pdf/0e68ff1fa269567c6c6101685f2f721afcc5d0aa.pdf",
        "forum_url": "https://openreview.net/forum?id=YZHES8wIdE"
    },
    {
        "title": "Planning in Stochastic Environments with a Learned Model",
        "authors": [
            "Ioannis Antonoglou",
            "Julian Schrittwieser",
            "Sherjil Ozair",
            "Thomas K Hubert",
            "David Silver"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Model-based reinforcement learning has proven highly successful. However, learning a model in isolation from its use during planning is problematic in complex environments. To date, the most effective techniques have instead combined value-equivalent model learning with powerful tree-search methods. This approach is exemplified by MuZero, which has achieved state-of-the-art performance in a wide range of domains, from board games to visually rich environments, with discrete and continuous action spaces, in online and offline settings. However, previous instantiations of this approach were limited to the use of deterministic models. This limits their performance in environments that are inherently stochastic, partially observed, or so large and complex that they appear stochastic to a finite agent. In this paper we extend this approach to learn and plan with stochastic models. Specifically, we introduce a new algorithm, Stochastic MuZero, that learns a stochastic model incorporating afterstates, and uses this model to perform a stochastic tree search. Stochastic MuZero matched or exceeded the state of the art in a set of canonical single and multi-agent environments, including 2048 and backgammon, while maintaining the same performance as standard MuZero in the game of Go.",
        "pdf_link": "https://openreview.net/pdf/f49fc80947707469997960f573102cea38cafb0f.pdf",
        "forum_url": "https://openreview.net/forum?id=X6D9bAHhBQ1"
    },
    {
        "title": "Transform2Act: Learning a Transform-and-Control Policy for Efficient Agent Design",
        "authors": [
            "Ye Yuan",
            "Yuda Song",
            "Zhengyi Luo",
            "Wen Sun",
            "Kris M. Kitani"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "An agent's functionality is largely determined by its design, i.e., skeletal structure and joint attributes (e.g., length, size, strength). However, finding the optimal agent design for a given function is extremely challenging since the problem is inherently combinatorial and the design space is prohibitively large. Additionally, it can be costly to evaluate each candidate design which requires solving for its optimal controller. To tackle these problems, our key idea is to incorporate the design procedure of an agent into its decision-making process. Specifically, we learn a conditional policy that, in an episode, first applies a sequence of transform actions to modify an agent's skeletal structure and joint attributes, and then applies control actions under the new design. To handle a variable number of joints across designs, we use a graph-based policy where each graph node represents a joint and uses message passing with its neighbors to output joint-specific actions. Using policy gradient methods, our approach enables joint optimization of agent design and control as well as experience sharing across different designs, which improves sample efficiency substantially.  Experiments show that our approach, Transform2Act, outperforms prior methods significantly in terms of convergence speed and final performance. Notably, Transform2Act can automatically discover plausible designs similar to giraffes, squids, and spiders. Code and videos are available at https://sites.google.com/view/transform2act.",
        "pdf_link": "https://openreview.net/pdf/511a5c95afacad18125605721a8d1e530c07018b.pdf",
        "forum_url": "https://openreview.net/forum?id=UcDUxjPYWSr"
    },
    {
        "title": "Memorizing Transformers",
        "authors": [
            "Yuhuai Wu",
            "Markus Norman Rabe",
            "DeLesley Hutchins",
            "Christian Szegedy"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights.  \nWe instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate $k$NN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. \nOn benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time.",
        "pdf_link": "https://openreview.net/pdf/33d84d1024126d6a7d4098f2f3beffdbe7057caa.pdf",
        "forum_url": "https://openreview.net/forum?id=TrjbxzRcnf-"
    },
    {
        "title": "Provably Filtering Exogenous Distractors using Multistep Inverse Dynamics",
        "authors": [
            "Yonathan Efroni",
            "Dipendra Misra",
            "Akshay Krishnamurthy",
            "Alekh Agarwal",
            "John Langford"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Many real-world applications of reinforcement learning (RL) require the agent to deal with high-dimensional observations such as those generated from a megapixel camera. Prior work has addressed such problems with representation learning, through which the agent can provably extract endogenous, latent state information from raw observations and subsequently plan efficiently. However, such approaches can fail in the presence of temporally correlated noise in the observations, a phenomenon that is common in practice. We initiate the formal study of latent state discovery in the presence of such exogenous noise sources by proposing a new model, the Exogenous Block MDP (EX-BMDP), for rich observation RL. We start by establishing several negative results, by highlighting failure cases of prior representation learning based approaches. Then, we introduce the Predictive Path Elimination (PPE) algorithm, that learns a generalization of inverse dynamics and is provably sample and computationally efficient in EX-BMDPs when the endogenous state dynamics are near deterministic. The sample complexity of PPE depends polynomially on the size of the latent endogenous state space while not directly depending on the size of the observation space, nor the exogenous state space. We provide experiments on challenging exploration problems which show that our approach works empirically. ",
        "pdf_link": "https://openreview.net/pdf/310151127bcaaee206f6987dfe48a6f9a49ae848.pdf",
        "forum_url": "https://openreview.net/forum?id=RQLLzMCefQu"
    },
    {
        "title": "Vision-Based Manipulators Need to Also See from Their Hands",
        "authors": [
            "Kyle Hsu",
            "Moo Jin Kim",
            "Rafael Rafailov",
            "Jiajun Wu",
            "Chelsea Finn"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "We study how the choice of visual perspective affects learning and generalization in the context of physical manipulation from raw sensor observations. Compared with the more commonly used global third-person perspective, a hand-centric (eye-in-hand) perspective affords reduced observability, but we find that it consistently improves training efficiency and out-of-distribution generalization. These benefits hold across a variety of learning algorithms, experimental settings, and distribution shifts, and for both simulated and real robot apparatuses. However, this is only the case when hand-centric observability is sufficient; otherwise, including a third-person perspective is necessary for learning, but also harms out-of-distribution generalization. To mitigate this, we propose to regularize the third-person information stream via a variational information bottleneck. On six representative manipulation tasks with varying hand-centric observability adapted from the Meta-World benchmark, this results in a state-of-the-art reinforcement learning agent operating from both perspectives improving its out-of-distribution generalization on every task. While some practitioners have long put cameras in the hands of robots, our work systematically analyzes the benefits of doing so and provides simple and broadly applicable insights for improving end-to-end learned vision-based robotic manipulation.",
        "pdf_link": "https://openreview.net/pdf/bf5308ad68220347e7cbf2dcbedbf7bb4e0a21b1.pdf",
        "forum_url": "https://openreview.net/forum?id=RJkAHKp7kNZ"
    },
    {
        "title": "Reinforcement Learning under a Multi-agent Predictive State Representation Model: Method and Theory",
        "authors": [
            "Zhi Zhang",
            "Zhuoran Yang",
            "Han Liu",
            "Pratap Tokekar",
            "Furong Huang"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "We study reinforcement learning for partially observable multi-agent systems where each agent only has access to its own observation and reward and aims to maximize its cumulative rewards. To handle partial observations, we propose graph-assisted predictive state representations (GAPSR), a scalable multi-agent representation learning framework that leverages the agent connectivity graphs to aggregate local representations computed by each agent. In addition, our representations are readily able to incorporate dynamic interaction graphs and kernel space embeddings of the predictive states, and thus have strong flexibility and representation power. \nBased on GAPSR, we propose an end-to-end  MARL algorithm that simultaneously infers the predictive representations and uses the representations as the input of a policy optimization algorithm. Empirically, we demonstrate the efficacy of the proposed algorithm provided on both a MAMuJoCo robotic learning experiment and a multi-agent particle learning environment.",
        "pdf_link": "https://openreview.net/pdf/abd7a0683441b4eb75fb4381e8ac583f2bff2b90.pdf",
        "forum_url": "https://openreview.net/forum?id=PLDOnFoVm4"
    },
    {
        "title": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models",
        "authors": [
            "Beidi Chen",
            "Tri Dao",
            "Kaizhao Liang",
            "Jiaming Yang",
            "Zhao Song",
            "Atri Rudra",
            "Christopher Re"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Overparameterized neural networks generalize well but are expensive to train. Ideally one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model components. The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is difficult and expensive. To address this, our main insight is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. As butterfly matrices are not hardware efficient, we propose simple variants of butterfly (block and flat) to take advantage of modern hardware. Our method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically validate that Pixelated Butterfly is $3\\times$ faster than Butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs. On the ImageNet classification and WikiText-103 language modeling tasks, our sparse models train up to 2.3$\\times$ faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 small with no drop in accuracy.",
        "pdf_link": "https://openreview.net/pdf/ee0e47a9502622a5bc9e044424d6f3217c00bdf4.pdf",
        "forum_url": "https://openreview.net/forum?id=Nfl-iXa-y7R"
    },
    {
        "title": "Learning Altruistic Behaviours in Reinforcement Learning without External Rewards",
        "authors": [
            "Tim Franzmeyer",
            "Mateusz Malinowski",
            "Joao F. Henriques"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Can artificial agents learn to assist others in achieving their goals without knowing what those goals are? Generic reinforcement learning agents could be trained to behave altruistically towards others by rewarding them for altruistic behaviour, i.e., rewarding them for benefiting other agents in a given situation. Such an approach assumes that other agents' goals are known so that the altruistic agent can cooperate in achieving those goals. However, explicit knowledge of other agents' goals is often difficult to acquire. In the case of human agents, their goals and preferences may be difficult to express fully; they might be ambiguous or even contradictory. Thus, it is beneficial to develop agents that do not depend on external supervision and learn altruistic behaviour in a task-agnostic manner. We propose to act altruistically towards other agents by giving them more choice and allowing them to achieve their goals better. Some concrete examples include opening a door for others or safeguarding them to pursue their objectives without interference. We formalize this concept and propose an altruistic agent that learns to increase the choices another agent has by preferring to maximize the number of states that the other agent can reach in its future. We evaluate our approach in three different multi-agent environments where another agent's success depends on altruistic behaviour. Finally, we show that our unsupervised agents can perform comparably to agents explicitly trained to work cooperatively, in some cases even outperforming them.",
        "pdf_link": "https://openreview.net/pdf/1af1674dc962e470709ff0dba09d61b75acd8daa.pdf",
        "forum_url": "https://openreview.net/forum?id=KxbhdyiPHE"
    },
    {
        "title": "COptiDICE: Offline Constrained Reinforcement Learning via Stationary Distribution Correction Estimation",
        "authors": [
            "Jongmin Lee",
            "Cosmin Paduraru",
            "Daniel J Mankowitz",
            "Nicolas Heess",
            "Doina Precup",
            "Kee-Eung Kim",
            "Arthur Guez"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "We consider the offline constrained reinforcement learning (RL) problem, in which the agent aims to compute a policy that maximizes expected return while satisfying given cost constraints, learning only from a pre-collected dataset. This problem setting is appealing in many real-world scenarios, where direct interaction with the environment is costly or risky, and where the resulting policy should comply with safety constraints. However, it is challenging to compute a policy that guarantees satisfying the cost constraints in the offline RL setting, since the off-policy evaluation inherently has an estimation error. In this paper, we present an offline constrained RL algorithm that optimizes the policy in the space of the stationary distribution. Our algorithm, COptiDICE, directly estimates the stationary distribution corrections of the optimal policy with respect to returns, while constraining the cost upper bound, with the goal of yielding a cost-conservative policy for actual constraint satisfaction. Experimental results show that COptiDICE attains better policies in terms of constraint satisfaction and return-maximization, outperforming baseline algorithms.",
        "pdf_link": "https://openreview.net/pdf/072227698fafd08a9854a8816e3cc5d5a8eb5754.pdf",
        "forum_url": "https://openreview.net/forum?id=FLA55mBee6Q"
    },
    {
        "title": "Increasing the Cost of Model Extraction with Calibrated Proof of Work",
        "authors": [
            "Adam Dziedzic",
            "Muhammad Ahmad Kaleem",
            "Yu Shen Lu",
            "Nicolas Papernot"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "In model extraction attacks, adversaries can steal a machine learning model exposed via a public API by repeatedly querying it and adjusting their own model based on obtained predictions. To prevent model stealing, existing defenses focus on detecting malicious queries, truncating, or distorting outputs, thus necessarily introducing a tradeoff between robustness and model utility for legitimate users. Instead, we propose to impede model extraction by requiring users to complete a proof-of-work before they can read the model's predictions. This deters attackers by greatly increasing (even up to 100x) the computational effort needed to leverage query access for model extraction. Since we calibrate the effort required to complete the proof-of-work to each query, this only introduces a slight overhead for regular users (up to 2x). To achieve this, our calibration applies tools from differential privacy to measure the information revealed by a query. Our method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen.",
        "pdf_link": "https://openreview.net/pdf/c75fb6727a40e11e23ec143778cd6e178a0681f3.pdf",
        "forum_url": "https://openreview.net/forum?id=EAy7C1cgE1L"
    },
    {
        "title": "Latent Variable Sequential Set Transformers for Joint Multi-Agent Motion Prediction",
        "authors": [
            "Roger Girgis",
            "Florian Golemo",
            "Felipe Codevilla",
            "Martin Weiss",
            "Jim Aldon D'Souza",
            "Samira Ebrahimi Kahou",
            "Felix Heide",
            "Christopher Pal"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Robust multi-agent trajectory prediction is essential for the safe control of robotic systems. A major challenge is to efficiently learn a representation that approximates the true joint distribution of contextual, social, and temporal information to enable planning. We propose Latent Variable Sequential Set Transformers which are encoder-decoder architectures that generate scene-consistent multi-agent trajectories. We refer to these architectures as “AutoBots”. The encoder is a stack of interleaved temporal and social multi-head self-attention (MHSA) modules which alternately perform equivariant processing across the temporal and social dimensions. The decoder employs learnable seed parameters in combination with temporal and social MHSA modules allowing it to perform inference over the\nentire future scene in a single forward pass efficiently. AutoBots can produce either the trajectory of one ego-agent or a distribution over the future trajectories for all agents in the scene. For the single-agent prediction case, our model achieves top results on the global nuScenes vehicle motion prediction leaderboard, and produces strong results on the Argoverse vehicle prediction challenge. In the multi-agent setting, we evaluate on the synthetic partition of TrajNet++ dataset to showcase the model’s socially-consistent predictions. We also demonstrate our model on general sequences of sets and provide illustrative experiments modelling the sequential structure of the multiple strokes that make up symbols in the Omniglot data. A distinguishing feature of AutoBots is that all models are trainable on a\nsingle desktop GPU (1080 Ti) in under 48h.",
        "pdf_link": "https://openreview.net/pdf/1ab1260f39e79ac98b52759c8221374f595af7aa.pdf",
        "forum_url": "https://openreview.net/forum?id=Dup_dDqkZC5"
    },
    {
        "title": "GNN-LM: Language Modeling based on Global Contexts via GNN",
        "authors": [
            "Yuxian Meng",
            "Shi Zong",
            "Xiaoya Li",
            "Xiaofei Sun",
            "Tianwei Zhang",
            "Fei Wu",
            "Jiwei Li"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Inspired by the notion that \"it to copy is easier than to memorize\", in this work, we introduce GNN-LM, which extends vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla  LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. The code can be found at https://github.com/ShannonAI/GNN-LM.",
        "pdf_link": "https://openreview.net/pdf/5a848353a24b880cebcd40d2e65796e794c0ff1d.pdf",
        "forum_url": "https://openreview.net/forum?id=BS49l-B5Bql"
    },
    {
        "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
        "authors": [
            "Victor Sanh",
            "Albert Webson",
            "Colin Raffel",
            "Stephen Bach",
            "Lintang Sutawika",
            "Zaid Alyafeai",
            "Antoine Chaffin",
            "Arnaud Stiegler",
            "Arun Raja",
            "Manan Dey",
            "M Saiful Bari",
            "Canwen Xu",
            "Urmish Thakker",
            "Shanya Sharma Sharma",
            "Eliza Szczechla",
            "Taewoon Kim",
            "Gunjan Chhablani",
            "Nihal Nayak",
            "Debajyoti Datta",
            "Jonathan Chang",
            "Mike Tian-Jian Jiang",
            "Han Wang",
            "Matteo Manica",
            "Sheng Shen",
            "Zheng Xin Yong",
            "Harshit Pandey",
            "Rachel Bawden",
            "Thomas Wang",
            "Trishala Neeraj",
            "Jos Rozen",
            "Abheesht Sharma",
            "Andrea Santilli",
            "Thibault Fevry",
            "Jason Alan Fries",
            "Ryan Teehan",
            "Teven Le Scao",
            "Stella Biderman",
            "Leo Gao",
            "Thomas Wolf",
            "Alexander M Rush"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models’ pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely unseen tasks specified in natural language. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several datasets, often outperforming models 16× its size. Further, our model attains strong performance on a subset of tasks from the BIG-Bench benchmark, outperforming models 6× its size. All trained models are available at https://github.com/bigscience-workshop/t-zero, and all prompts are available at https://github.com/bigscience-workshop/promptsource.",
        "pdf_link": "https://openreview.net/pdf/bec425b93713482f8e2de5d1d15b66ff95a47026.pdf",
        "forum_url": "https://openreview.net/forum?id=9Vrb9D0WI4"
    },
    {
        "title": "Possibility Before Utility: Learning And Using Hierarchical Affordances",
        "authors": [
            "Robby Costales",
            "Shariq Iqbal",
            "Fei Sha"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Reinforcement learning algorithms struggle on tasks with complex hierarchical dependency structures. Humans and other intelligent agents do not waste time assessing the utility of every high-level action in existence, but instead only consider ones they deem possible in the first place. By focusing only on what is feasible, or \"afforded'', at the present moment, an agent can spend more time both evaluating the utility of and acting on what matters. To this end, we present Hierarchical Affordance Learning (HAL), a method that learns a model of hierarchical affordances in order to prune impossible subtasks for more effective learning. Existing works in hierarchical reinforcement learning provide agents with structural representations of subtasks but are not affordance-aware, and by grounding our definition of hierarchical affordances in the present state, our approach is more flexible than the multitude of approaches that ground their subtask dependencies in a symbolic history. While these logic-based methods often require complete knowledge of the subtask hierarchy, our approach is able to utilize incomplete and varying symbolic specifications. Furthermore, we demonstrate that relative to non-affordance-aware methods, HAL agents are better able to efficiently learn complex tasks, navigate environment stochasticity, and acquire diverse skills in the absence of extrinsic supervision---all of which are hallmarks of human learning.",
        "pdf_link": "https://openreview.net/pdf/f4b5c96c2948ff7fcea521e9713644691c27bab2.pdf",
        "forum_url": "https://openreview.net/forum?id=7b4zxUnrO2N"
    },
    {
        "title": "Learning Hierarchical Structures with Differentiable Nondeterministic Stacks",
        "authors": [
            "Brian DuSell",
            "David Chiang"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Learning hierarchical structures in sequential data -- from simple algorithmic patterns to natural language -- in a reliable, generalizable way remains a challenging problem for neural language models. Past work has shown that recurrent neural networks (RNNs) struggle to generalize on held-out algorithmic or syntactic patterns without supervision or some inductive bias. To remedy this, many papers have explored augmenting RNNs with various differentiable stacks, by analogy with finite automata and pushdown automata (PDAs). In this paper, we improve the performance of our recently proposed Nondeterministic Stack RNN (NS-RNN), which uses a differentiable data structure that simulates a nondeterministic PDA, with two important changes. First, the model now assigns unnormalized positive weights instead of probabilities to stack actions, and we provide an analysis of why this improves training. Second, the model can directly observe the state of the underlying PDA. Our model achieves lower cross-entropy than all previous stack RNNs on five context-free language modeling tasks (within 0.05 nats of the information-theoretic lower bound), including a task on which the NS-RNN previously failed to outperform a deterministic stack RNN baseline. Finally, we propose a restricted version of the NS-RNN that incrementally processes infinitely long sequences, and we present language modeling results on the Penn Treebank.",
        "pdf_link": "https://openreview.net/pdf/bfc2ff0a81fd70d01a09a0cb018dddf36e401060.pdf",
        "forum_url": "https://openreview.net/forum?id=5LXw_QplBiF"
    },
    {
        "title": "GreaseLM: Graph REASoning Enhanced Language Models",
        "authors": [
            "Xikun Zhang",
            "Antoine Bosselut",
            "Michihiro Yasunaga",
            "Hongyu Ren",
            "Percy Liang",
            "Christopher D Manning",
            "Jure Leskovec"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Answering complex questions about textual narratives requires reasoning over both stated context and the world knowledge that underlies it. However, pretrained language models (LM), the foundation of most modern QA systems, do not robustly represent latent relationships between concepts, which is necessary for reasoning. While knowledge graphs (KG) are often used to augment LMs with structured representations of world knowledge, it remains an open question how to effectively fuse and reason over the KG representations and the language context, which provides situational constraints and nuances. In this work, we propose GreaseLM, a new model that fuses encoded representations from pretrained LMs and graph neural networks over multiple layers of modality interaction operations. Information from both modalities propagates to the other, allowing language context representations to be grounded by structured world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in the context to inform the graph representations of knowledge. Our results on three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA) and medical question answering (i.e., MedQA-USMLE) domains demonstrate that GreaseLM can more reliably answer questions that require reasoning over both situational constraints and structured knowledge, even outperforming models 8x larger.",
        "pdf_link": "https://openreview.net/pdf/1a023786aa33b14412cd0596ee9247b562f4f4fe.pdf",
        "forum_url": "https://openreview.net/forum?id=41e9o6cQPj"
    },
    {
        "title": "The Information Geometry of Unsupervised Reinforcement Learning",
        "authors": [
            "Benjamin Eysenbach",
            "Ruslan Salakhutdinov",
            "Sergey Levine"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "How can a reinforcement learning (RL) agent prepare to solve downstream tasks if those tasks are not known a priori? One approach is unsupervised skill discovery, a class of algorithms that learn a set of policies without access to a reward function. Such algorithms bear a close resemblance to representation learning algorithms (e.g., contrastive learning) in supervised learning, in that both are pretraining algorithms that maximize some approximation to a mutual information objective. While prior work has shown that the set of skills learned by such methods can accelerate downstream RL tasks, prior work offers little analysis into whether these skill learning algorithms are optimal, or even what notion of optimality would be appropriate to apply to them. In this work, we show that unsupervised skill discovery algorithms based on mutual information maximization do not learn skills that are optimal for every possible reward function. However, we show that the distribution over skills provides an optimal initialization minimizing regret against adversarially-chosen reward functions, assuming a certain type of adaptation procedure. Our analysis also provides a geometric perspective on these skill learning methods.",
        "pdf_link": "https://openreview.net/pdf/4709236cdf10497a057511e94fe99f87770c5bf6.pdf",
        "forum_url": "https://openreview.net/forum?id=3wU2UX0voE"
    },
    {
        "title": "Towards a Unified View of Parameter-Efficient Transfer Learning",
        "authors": [
            "Junxian He",
            "Chunting Zhou",
            "Xuezhe Ma",
            "Taylor Berg-Kirkpatrick",
            "Graham Neubig"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Fine-tuning large pretrained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the parameters of the pretrained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pretrained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks.",
        "pdf_link": "https://openreview.net/pdf/859577d9cab3bf7833fe6fd6ea3a66c3b424c6bb.pdf",
        "forum_url": "https://openreview.net/forum?id=0RDcd5Axok"
    },
    {
        "title": "Finite-Time Convergence and Sample Complexity of Multi-Agent Actor-Critic Reinforcement Learning with Average Reward",
        "authors": [
            "FNU Hairi",
            "Jia Liu",
            "Songtao Lu"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "In this paper, we establish the first finite-time convergence result of the actor-critic algorithm for fully decentralized multi-agent reinforcement learning (MARL) problems with average reward. \nIn this problem, a set of $N$ agents work cooperatively to maximize the global average reward through interacting with their neighbors over a communication network.\nWe consider a practical MARL setting, where the rewards and actions of each agent are only known to itself, and the knowledge of joint actions of the agents is not assumed. \nToward this end, we propose a mini-batch Markovian sampled fully decentralized actor-critic algorithm and analyze its finite-time convergence and sample complexity.\nWe show that the sample complexity of this algorithm is $\\mathcal{O}(N^{2}/\\epsilon^{2}\\log(N/\\epsilon))$.\nInterestingly, this sample complexity bound matches that of the state-of-the-art single-agent actor-critic algorithms for reinforcement learning. ",
        "pdf_link": "https://openreview.net/pdf/88771b44b2b7e3d534226c24b2a2e7d9739fc960.pdf",
        "forum_url": "https://openreview.net/forum?id=04pGUg0-pdZ"
    },
    {
        "title": "FP-DETR: Detection Transformer Advanced by Fully Pre-training",
        "authors": [
            "Wen Wang",
            "Yang Cao",
            "Jing Zhang",
            "Dacheng Tao"
        ],
        "published": "iclr 2022 poster",
        "summary": "Large-scale pre-training has proven to be effective for visual representation learning on downstream tasks, especially for improving robustness and generalization. However, the recently developed detection transformers only employ pre-training on its backbone while leaving the key component, i.e., a 12-layer transformer, being trained from scratch, which prevents the model from above benefits. This separated training paradigm is mainly caused by the discrepancy between the upstream and downstream tasks. To mitigate the issue, we propose FP-DETR, a new method that Fully Pre-Trains an encoder-only transformer and smoothly fine-tunes it for object detection via a task adapter. Inspired by the success of textual prompts in NLP, we treat query positional embeddings as visual prompts to help the model attend to the target area (prompting) and recognize the object. To this end, we propose the task adapter which leverages self-attention to model the contextual relation between object query embedding. Experiments on the challenging COCO dataset demonstrate that our FP-DETR achieves competitive performance. Moreover, it enjoys better robustness to common corruptions and generalization to small-size datasets than state-of-the-art detection transformers. Code will be made publicly available at $\\url{https://github.com/encounter1997/FP-DETR}$.",
        "pdf_link": "https://openreview.net/pdf/e9eb5c626773238a576771113c31684900fad1b7.pdf",
        "forum_url": "https://openreview.net/forum?id=yjMQuLLcGWK",
        "source": "iclr2022"
    },
    {
        "title": "OntoProtein: Protein Pretraining With Gene Ontology Embedding",
        "authors": [
            "Ningyu Zhang",
            "Zhen Bi",
            "Xiaozhuan Liang",
            "Siyuan Cheng",
            "Haosen Hong",
            "Shumin Deng",
            "Qiang Zhang",
            "Jiazhang Lian",
            "Huajun Chen"
        ],
        "published": "iclr 2022 poster",
        "summary": "Self-supervised protein language models have proved their effectiveness in learning the proteins representations. With the increasing computational power, current protein language models pre-trained with millions of diverse sequences can advance the parameter scale from million-level to billion-level and achieve remarkable improvement. However, those prevailing approaches rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better protein representations. We argue that informative biology knowledge in KGs can enhance protein representation with external knowledge. In this work, we propose OntoProtein, the first general framework that makes use of structure in GO (Gene Ontology) into protein pre-training models. We construct a novel large-scale knowledge graph that consists of GO and its related proteins, and gene annotation texts or protein sequences describe all nodes in the graph. We propose novel contrastive learning with knowledge-aware negative sampling to jointly optimize the knowledge graph and protein embedding during pre-training.  Experimental results show that OntoProtein can surpass state-of-the-art methods with pre-trained protein language models in TAPE benchmark and yield better performance compared with baselines in protein-protein interaction and protein function prediction.",
        "pdf_link": "https://openreview.net/pdf/45d0f729743dae28d50e8c7e62050d0d95afbf43.pdf",
        "forum_url": "https://openreview.net/forum?id=yfe1VMYAXa4",
        "source": "iclr2022"
    },
    {
        "title": "Communication-Efficient Actor-Critic Methods for Homogeneous Markov Games",
        "authors": [
            "Dingyang Chen",
            "Yile Li",
            "Qi Zhang"
        ],
        "published": "iclr 2022 poster",
        "summary": "Recent success in cooperative multi-agent reinforcement learning (MARL) relies on centralized training and policy sharing. Centralized training eliminates the issue of non-stationarity MARL yet induces large communication costs, and policy sharing is empirically crucial to efficient learning in certain tasks yet lacks theoretical justification. In this paper, we formally characterize a subclass of cooperative Markov games where agents exhibit a certain form of homogeneity such that policy sharing provably incurs no suboptimality. This enables us to develop the first consensus-based decentralized actor-critic method where the consensus update is applied to both the actors and the critics while ensuring convergence. We also develop practical algorithms based on our decentralized actor-critic method to reduce the communication cost during training, while still yielding policies comparable with centralized training.",
        "pdf_link": "https://openreview.net/pdf/53792786d99df961509a8372e2fe61fafba57a92.pdf",
        "forum_url": "https://openreview.net/forum?id=xy_2w3J3kH",
        "source": "iclr2022"
    },
    {
        "title": "Cross-Domain Imitation Learning via Optimal Transport",
        "authors": [
            "Arnaud Fickinger",
            "Samuel Cohen",
            "Stuart Russell",
            "Brandon Amos"
        ],
        "published": "iclr 2022 poster",
        "summary": "Cross-domain imitation learning studies how to leverage expert demonstrations of one agent to train an imitation agent with a different embodiment or morphology. Comparing trajectories and stationary distributions between the expert and imitation agents is challenging because they live on different systems that may not even have the same dimensionality. We propose Gromov-Wasserstein Imitation Learning (GWIL), a method for cross-domain imitation that uses the Gromov-Wasserstein distance to align and compare states between the different spaces of the agents. Our theory formally characterizes the scenarios where GWIL preserves optimality, revealing its possibilities and limitations. We demonstrate the effectiveness of GWIL in non-trivial continuous control domains ranging from simple rigid transformation of the expert domain to arbitrary transformation of the state-action space.",
        "pdf_link": "https://openreview.net/pdf/0e1aa9f9ddcbcd903dbecbe6f034779d158d2260.pdf",
        "forum_url": "https://openreview.net/forum?id=xP3cPq2hQC",
        "source": "iclr2022"
    },
    {
        "title": "Towards Continual Knowledge Learning of Language Models",
        "authors": [
            "Joel Jang",
            "Seonghyeon Ye",
            "Sohee Yang",
            "Joongbo Shin",
            "Janghoon Han",
            "Gyeonghun KIM",
            "Stanley Jungkyu Choi",
            "Minjoon Seo"
        ],
        "published": "iclr 2022 poster",
        "summary": "Large Language Models (LMs) are known to encode world knowledge in their parameters as they pretrain on a vast amount of web corpus, which is often utilized for performing knowledge-dependent downstream tasks such as question answering, fact-checking, and open dialogue. In real-world scenarios, the world knowledge stored in the LMs can quickly become outdated as the world changes, but it is non-trivial to avoid catastrophic forgetting and reliably acquire new knowledge while preserving invariant knowledge. To push the community towards better maintenance of ever-changing LMs, we formulate a new continual learning (CL) problem called Continual Knowledge Learning (CKL). We construct a new benchmark and metric to quantify the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. We adopt applicable recent methods from literature to create several strong baselines. Through extensive experiments, we find that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously. By highlighting the critical causes of knowledge forgetting, we show that CKL is a challenging and important problem that helps us better understand and train ever-changing LMs.",
        "pdf_link": "https://openreview.net/pdf/873039fe2b300e8ad8b1dd3a34054bad1386f1a1.pdf",
        "forum_url": "https://openreview.net/forum?id=vfsRB5MImo9",
        "source": "iclr2022"
    },
    {
        "title": "Knowledge Infused Decoding",
        "authors": [
            "Ruibo Liu",
            "Guoqing Zheng",
            "Shashank Gupta",
            "Radhika Gaonkar",
            "Chongyang Gao",
            "Soroush Vosoughi",
            "Milad Shokouhi",
            "Ahmed Hassan Awadallah"
        ],
        "published": "iclr 2022 poster",
        "summary": "Pre-trained language models (LMs) have been shown to memorize a substantial amount of knowledge from the pre-training corpora; however, they are still limited in recalling factually correct knowledge given a certain context. Hence. they tend to suffer from counterfactual or hallucinatory generation when used in knowledge-intensive natural language generation (NLG) tasks. Recent remedies to this problem focus on modifying either the pre-training or task fine-tuning objectives to incorporate knowledge, which normally require additional costly training or architecture modification of LMs for practical applications.\n\nWe present Knowledge Infused Decoding (KID)---a novel decoding algorithm for generative LMs, which dynamically infuses external knowledge into each step of the LM decoding. Specifically, we maintain a local knowledge memory based on the current context, interacting with a dynamically created external knowledge trie, and continuously update the local memory as a knowledge-aware constraint to guide decoding via reinforcement learning. On six diverse knowledge-intensive NLG tasks, task-agnostic LMs (e.g., GPT-2 and BART) armed with KID outperform many task-optimized state-of-the-art models, and show particularly strong performance in few-shot scenarios over seven related knowledge-infusion techniques. Human evaluation confirms KID's ability to generate more relevant and factual language for the input context when compared with multiple baselines. Finally, KID also alleviates exposure bias and provides stable generation quality when generating longer sequences.",
        "pdf_link": "https://openreview.net/pdf/a95e766c5c2193002a679156d0b1d1b582238b50.pdf",
        "forum_url": "https://openreview.net/forum?id=upnDJ7itech",
        "source": "iclr2022"
    },
    {
        "title": "Language model compression with weighted low-rank factorization",
        "authors": [
            "Yen-Chang Hsu",
            "Ting Hua",
            "Sungen Chang",
            "Qian Lou",
            "Yilin Shen",
            "Hongxia Jin"
        ],
        "published": "iclr 2022 poster",
        "summary": "Factorizing a large matrix into small matrices is a popular strategy for model compression. Singular value decomposition (SVD) plays a vital role in this compression strategy, approximating a learned matrix with fewer parameters. However, SVD minimizes the squared error toward reconstructing the original matrix without gauging the importance of the parameters, potentially giving a larger reconstruction error for those who affect the task accuracy more. In other words, the optimization objective of SVD is not aligned with the trained model's task accuracy. We analyze this previously unexplored problem, make observations, and address it by introducing Fisher information to weigh the importance of parameters affecting the model prediction. This idea leads to our method: Fisher-Weighted SVD (FWSVD). Although the factorized matrices from our approach do not result in smaller reconstruction errors, we find that our resulting task accuracy is much closer to the original model's performance. We perform analysis with the transformer-based language models, showing our weighted SVD largely alleviates the mismatched optimization objectives and can maintain model performance with a higher compression rate. Our method can directly compress a task-specific model while achieving better performance than other compact model strategies requiring expensive model pre-training. Moreover, the evaluation of compressing an already compact model shows our method can further reduce 9% to 30% parameters with an insignificant impact on task accuracy.",
        "pdf_link": "https://openreview.net/pdf/a5edead703a518eda031d7e25734d372b8287883.pdf",
        "forum_url": "https://openreview.net/forum?id=uPv9Y3gmAI5",
        "source": "iclr2022"
    },
    {
        "title": "Learning to Map for Active Semantic Goal Navigation",
        "authors": [
            "Georgios Georgakis",
            "Bernadette Bucher",
            "Karl Schmeckpeper",
            "Siddharth Singh",
            "Kostas Daniilidis"
        ],
        "published": "iclr 2022 poster",
        "summary": "We consider the problem of object goal navigation in unseen environments. Solving this problem requires learning of contextual semantic priors, a challenging endeavour given the spatial and semantic variability of indoor environments. Current methods learn to implicitly encode these priors through goal-oriented navigation policy functions operating on spatial representations that are limited to the agent's observable areas. In this work, we propose a novel framework that actively learns to generate semantic maps outside the field of view of the agent and leverages the uncertainty over the semantic classes in the unobserved areas to decide on long term goals. We demonstrate that through this spatial prediction strategy, we are able to learn semantic priors in scenes that can be leveraged in unknown environments. Additionally, we show how different objectives can be defined by balancing exploration with exploitation during searching for semantic targets. Our method is validated in the visually realistic environments of the Matterport3D dataset and show improved results on object goal navigation over competitive baselines.",
        "pdf_link": "https://openreview.net/pdf/8097afd8a3e6d7c824f59390ca5a9cee0530bbd1.pdf",
        "forum_url": "https://openreview.net/forum?id=swrMQttr6wN",
        "source": "iclr2022"
    },
    {
        "title": "Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators",
        "authors": [
            "Yu Meng",
            "Chenyan Xiong",
            "Payal Bajaj",
            "saurabh tiwary",
            "Paul N. Bennett",
            "Jiawei Han",
            "Xia Song"
        ],
        "published": "iclr 2022 poster",
        "summary": "We present a new framework AMOS that pretrains text encoders with an Adversarial learning curriculum via a Mixture Of Signals from multiple auxiliary generators. Following ELECTRA-style pretraining, the main encoder is trained as a discriminator to detect replaced tokens generated by auxiliary masked language models (MLMs). Different from ELECTRA which trains one MLM as the generator, we jointly train multiple MLMs of different sizes to provide training signals at various levels of difficulty. To push the discriminator to learn better with challenging replaced tokens, we learn mixture weights over the auxiliary MLMs' outputs to maximize the discriminator loss by backpropagating the gradient from the discriminator via Gumbel-Softmax. For better pretraining efficiency, we propose a way to assemble multiple MLMs into one unified auxiliary model. AMOS outperforms ELECTRA and recent state-of-the-art pretrained models by about 1 point on the GLUE benchmark for BERT base-sized models.",
        "pdf_link": "https://openreview.net/pdf/4127a755f1e5ee998e6423f7a8d734f9e88b8cab.pdf",
        "forum_url": "https://openreview.net/forum?id=sX3XaHwotOg",
        "source": "iclr2022"
    },
    {
        "title": "Towards General Function Approximation in Zero-Sum Markov Games",
        "authors": [
            "Baihe Huang",
            "Jason D. Lee",
            "Zhaoran Wang",
            "Zhuoran Yang"
        ],
        "published": "iclr 2022 poster",
        "summary": "This paper considers two-player zero-sum finite-horizon Markov games with simultaneous moves. The study focuses on the challenging settings where the value\nfunction or the model is parameterized by general function classes. Provably efficient\nalgorithms for both decoupled and coordinated settings are developed. In the decoupled setting where the agent controls a single player and plays against an arbitrary opponent, we propose a new model-free algorithm. The sample complexity is governed by the Minimax Eluder dimension—a new dimension of the function class in Markov games. As a special case, this method improves the state-of-the-art algorithm\nby a $\\sqrt{d}$ factor in the regret when the reward function and transition kernel are parameterized with d-dimensional linear features. In the coordinated setting where both\nplayers are controlled by the agent, we propose a model-based algorithm and a model-free algorithm. In the model-based algorithm, we prove that sample complexity can\nbe bounded by a generalization of Witness rank to Markov games. The model-free\nalgorithm enjoys a  $\\sqrt{K}$-regret upper bound where $K$ is the number of episodes. Our\nalgorithms are based on new techniques of alternate optimism",
        "pdf_link": "https://openreview.net/pdf/89164a5698b4ced1396254451108620fc52d5bc1.pdf",
        "forum_url": "https://openreview.net/forum?id=sA4qIu3zv6v",
        "source": "iclr2022"
    },
    {
        "title": "Autonomous Learning of Object-Centric Abstractions for High-Level Planning",
        "authors": [
            "Steven James",
            "Benjamin Rosman",
            "George Konidaris"
        ],
        "published": "iclr 2022 poster",
        "summary": "We propose a method for autonomously learning an object-centric representation of a continuous and high-dimensional environment that is suitable for planning. Such representations can immediately be transferred between tasks that share the same types of objects, resulting in agents that require fewer samples to learn a model of a new task. We first demonstrate our approach on a 2D crafting domain consisting of numerous objects where the agent learns a compact, lifted representation that generalises across objects. We then apply it to a series of Minecraft tasks to learn object-centric representations and object types - directly from pixel data - that can be leveraged to solve new tasks quickly. The resulting learned representations enable the use of a task-level planner, resulting in an agent capable of transferring learned representations to form complex, long-term plans.",
        "pdf_link": "https://openreview.net/pdf/a9a9310c7615055c5fd767dd5c8bde623331a28b.pdf",
        "forum_url": "https://openreview.net/forum?id=rrWeE9ZDw_",
        "source": "iclr2022"
    },
    {
        "title": "Proof Artifact Co-Training for Theorem Proving with Language Models",
        "authors": [
            "Jesse Michael Han",
            "Jason Rute",
            "Yuhuai Wu",
            "Edward Ayers",
            "Stanislas Polu"
        ],
        "published": "iclr 2022 poster",
        "summary": "Labeled data for imitation learning of theorem proving in large libraries of formalized mathematics is scarce as such libraries require years of concentrated effort by human specialists to be built. This is particularly challenging when applying large Transformer language models to tactic prediction, because the scaling of performance with respect to model size is quickly disrupted in the data-scarce, easily-overfitted regime.  We propose PACT (Proof Artifact Co-Training), a general methodology for extracting abundant self-supervised data from kernel-level proof terms for joint training alongside the usual tactic prediction objective.  We apply this methodology to Lean,an interactive proof assistant which hosts some of the most sophisticated formalized mathematics to date. We instrument Lean with a neural theorem prover driven by a Transformer language model and show that PACT improves theorem proving success rate on a held-out suite of test theorems from 32% to 48%.",
        "pdf_link": "https://openreview.net/pdf/452a6f034546f7c5332394391a04b08aafdba0f7.pdf",
        "forum_url": "https://openreview.net/forum?id=rpxJc9j04U",
        "source": "iclr2022"
    },
    {
        "title": "In a Nutshell, the Human Asked for This: Latent Goals for Following Temporal Specifications",
        "authors": [
            "Borja G. León",
            "Murray Shanahan",
            "Francesco Belardinelli"
        ],
        "published": "iclr 2022 poster",
        "summary": "We address the problem of building agents whose goal is to learn to execute out-of distribution (OOD) multi-task instructions expressed in temporal logic (TL) by using deep reinforcement learning (DRL). Recent works provided evidence that the agent's neural architecture is a key feature when DRL agents are learning to solve OOD tasks in TL. Yet, the studies on this topic are still in their infancy. In this work, we propose a new deep learning configuration with inductive biases that lead agents to generate latent representations of their current goal, yielding a stronger generalization performance. We use these latent-goal networks within a neuro-symbolic framework that executes multi-task formally-defined instructions and contrast the performance of the proposed neural networks against employing different state-of-the-art (SOTA) architectures when generalizing to unseen instructions in OOD environments. ",
        "pdf_link": "https://openreview.net/pdf/29122e59291a932f64f32109f73dba7f83c9bedd.pdf",
        "forum_url": "https://openreview.net/forum?id=rUwm9wCjURV",
        "source": "iclr2022"
    },
    {
        "title": "GPT-Critic: Offline Reinforcement Learning for End-to-End Task-Oriented Dialogue Systems",
        "authors": [
            "Youngsoo Jang",
            "Jongmin Lee",
            "Kee-Eung Kim"
        ],
        "published": "iclr 2022 poster",
        "summary": "Training a task-oriented dialogue agent can be naturally formulated as offline reinforcement learning (RL) problem, where the agent aims to learn a conversational strategy to achieve user goals, only from a dialogue corpus. It is very challenging in terms of RL since the natural language action space is astronomical, while feasible (syntactically and semantically correct) actions are very sparse. Thus, standard RL methods easily fail and generate responses diverging from human language, even when fine-tuning a powerful pre-trained language model. In this paper, we introduce GPT-Critic, an offline RL method for task-oriented dialogue. GPT-Critic is built upon GPT-2, fine-tuning the language model through behavior cloning of the critic-guided self-generated sentences. GPT-Critic is essentially free from the issue of diverging from human language since it learns from the sentences sampled from the pre-trained language model. In the experiments, we demonstrate that our algorithm outperforms the state-of-the-art in the task-oriented dialogue benchmarks including MultiWOZ 2.0 and ConvLab.",
        "pdf_link": "https://openreview.net/pdf/c1d0299a479595ff9da923e14398de97f2382394.pdf",
        "forum_url": "https://openreview.net/forum?id=qaxhBG1UUaS",
        "source": "iclr2022"
    },
    {
        "title": "Learning to Complete Code with Sketches",
        "authors": [
            "Daya Guo",
            "Alexey Svyatkovskiy",
            "Jian Yin",
            "Nan Duan",
            "Marc Brockschmidt",
            "Miltiadis Allamanis"
        ],
        "published": "iclr 2022 poster",
        "summary": "Code completion is usually cast as a language modelling problem, i.e., continuing an input in a left-to-right fashion. However, in practice, some parts of the completion (e.g., string literals) may be very hard to predict, whereas subsequent parts directly follow from the context.\nTo handle this, we instead consider the scenario of generating code completions with \"holes\" inserted in places where a model is uncertain. We develop Grammformer, a Transformer-based model that guides the code generation by the programming language grammar, and compare it to a variety of more standard sequence models.\n\nWe train the models on code completion for C# and Python given partial code context. To evaluate models, we consider both ROUGE as well as a new metric RegexAcc that measures success of generating completions matching long outputs with as few holes as possible.\nIn our experiments, Grammformer generates 10-50% more accurate completions compared to traditional generative models and 37-50% longer sketches compared to sketch-generating baselines trained with similar techniques.",
        "pdf_link": "https://openreview.net/pdf/721a38d7a69acd79d164ead05430e5912c4c0721.pdf",
        "forum_url": "https://openreview.net/forum?id=q79uMSC6ZBT",
        "source": "iclr2022"
    },
    {
        "title": "It Takes Four to Tango: Multiagent Self Play for Automatic Curriculum Generation",
        "authors": [
            "Yuqing Du",
            "Pieter Abbeel",
            "Aditya Grover"
        ],
        "published": "iclr 2022 poster",
        "summary": "We are interested in training general-purpose reinforcement learning agents that can solve a wide variety of goals. Training such agents efficiently requires automatic generation of a goal curriculum. This is challenging as it requires (a) exploring goals of increasing difficulty, while ensuring that the agent (b) is exposed to a diverse set of goals in a sample efficient manner and (c) does not catastrophically forget previously solved goals. We propose Curriculum Self Play (CuSP), an automated goal generation framework that seeks to satisfy these desiderata by virtue of a multi-player game with 4 agents. We extend the asymmetric curricula learning in PAIRED (Dennis et al., 2020) to a symmetrized game that carefully balances cooperation and competition between two off-policy student learners and two regret-maximizing teachers. CuSP additionally introduces entropic goal coverage and accounts for the non-stationary nature of the students, allowing us to automatically induce a curriculum that balances progressive exploration with anti-catastrophic exploitation. We demonstrate that our method succeeds at generating an effective curricula of goals for a range of control tasks, outperforming other methods at zero-shot test-time generalization to novel out-of-distribution goals.",
        "pdf_link": "https://openreview.net/pdf/68a6237e79699c723ce9c9c39537422391df3e2b.pdf",
        "forum_url": "https://openreview.net/forum?id=q4tZR1Y-UIs",
        "source": "iclr2022"
    },
    {
        "title": "Vector-quantized Image Modeling with Improved VQGAN",
        "authors": [
            "Jiahui Yu",
            "Xin Li",
            "Jing Yu Koh",
            "Han Zhang",
            "Ruoming Pang",
            "James Qin",
            "Alexander Ku",
            "Yuanzhong Xu",
            "Jason Baldridge",
            "Yonghui Wu"
        ],
        "published": "iclr 2022 poster",
        "summary": "Pretraining language models with next-token prediction on massive text corpora has delivered phenomenal zero-shot, few-shot, transfer learning and multi-tasking capabilities on both generative and discriminative language tasks. Motivated by this success, we explore a Vector-quantized Image Modeling (VIM) approach that involves pretraining a Transformer to predict rasterized image tokens autoregressively. The discrete image tokens are encoded from a learned Vision-Transformer-based VQGAN (ViT-VQGAN). We first propose multiple improvements over vanilla VQGAN from architecture to codebook learning, yielding better efficiency and reconstruction fidelity. The improved ViT-VQGAN further improves vector-quantized image modeling tasks, including unconditional, class-conditioned image generation and unsupervised representation learning. When trained on ImageNet at 256x256 resolution, we achieve Inception Score (IS) of 175.1 and Fr'echet Inception Distance (FID) of 4.17, a dramatic improvement over the vanilla VQGAN, which obtains 70.6 and 17.04 for IS and FID, respectively. Based on ViT-VQGAN and unsupervised pretraining, we further evaluate the pretrained Transformer by averaging intermediate features, similar to Image GPT (iGPT). This ImageNet-pretrained VIM-L significantly beats iGPT-L on linear-probe accuracy from 60.3% to 73.2% for a similar model size. ViM-L also outperforms iGPT-XL which is trained with extra web image data and larger model size.",
        "pdf_link": "https://openreview.net/pdf/63ce2022df1b6352ef17e394bb00cd416cf9497c.pdf",
        "forum_url": "https://openreview.net/forum?id=pfNyExj7z2",
        "source": "iclr2022"
    },
    {
        "title": "Decentralized Learning for Overparameterized Problems: A Multi-Agent Kernel Approximation Approach",
        "authors": [
            "Prashant Khanduri",
            "Haibo Yang",
            "Mingyi Hong",
            "Jia Liu",
            "Hoi To Wai",
            "Sijia Liu"
        ],
        "published": "iclr 2022 poster",
        "summary": "This work develops a novel framework for communication-efficient distributed learning where the models to be learned are overparameterized. We focus on a class of kernel learning problems (which includes the popular neural tangent kernel (NTK) learning as a special case) and propose a novel {\\it multi-agent kernel approximation} technique that allows the agents to distributedly estimate the full kernel function, and subsequently perform decentralized optimization, without directly exchanging any local data or parameters. The proposed framework is a significant departure from the classical consensus-based approaches, because the agents do not exchange problem parameters, and no consensus is required. We analyze the optimization and the generalization performance of the proposed framework for the $\\ell_2$ loss. We show that with $M$ agents and $N$ total samples when certain generalized inner-product kernels (resp. the random features kernel) are used, each agent needs to communicate $\\mathcal{O}\\big({N^2}/{M}\\big)$ bits (resp. $\\mathcal{O}\\big(N \\sqrt{N}/M \\big)$ real values) to achieve minimax optimal generalization performance. We validate the theoretical results on 90 UCI benchmarking datasets (with average data size $N \\approx 1000$) and show that each agent needs to share a total of $200N/M$ bits (resp. $3N/M$ real values) to closely match the performance of the centralized algorithms, and these numbers are independent of parameter and feature dimensions. ",
        "pdf_link": "https://openreview.net/pdf/d2d0106daf104b8b9c7f3868ca69a3a81cd09902.pdf",
        "forum_url": "https://openreview.net/forum?id=oj2yn1Q4Ett",
        "source": "iclr2022"
    },
    {
        "title": "Know Thyself: Transferable Visual Control Policies Through Robot-Awareness",
        "authors": [
            "Edward S. Hu",
            "Kun Huang",
            "Oleh Rybkin",
            "Dinesh Jayaraman"
        ],
        "published": "iclr 2022 poster",
        "summary": "Training visual control policies from scratch on a new robot typically requires generating large amounts of robot-specific data. How might we leverage data previously collected on another robot to reduce or even completely remove this need for robot-specific data? We propose a \"robot-aware control\" paradigm that achieves this by exploiting readily available knowledge about the robot. We then instantiate this in a robot-aware model-based RL policy by training modular dynamics models that couple a transferable, robot-aware world dynamics module with a robot-specific, potentially analytical, robot dynamics module. This also enables us to set up visual planning costs that separately consider the robot agent and the world. Our experiments on tabletop manipulation tasks with simulated and real robots demonstrate that these plug-in improvements dramatically boost the transferability of visual model-based RL policies, even permitting zero-shot transfer of visual manipulation skills onto new robots. Project website: https://www.seas.upenn.edu/~hued/rac",
        "pdf_link": "https://openreview.net/pdf/f710bc40ef26e24b70f811b0f03ad548956e27ae.pdf",
        "forum_url": "https://openreview.net/forum?id=o0ehFykKVtr",
        "source": "iclr2022"
    },
    {
        "title": "Capturing Structural Locality in Non-parametric Language Models",
        "authors": [
            "Frank F. Xu",
            "Junxian He",
            "Graham Neubig",
            "Vincent Josua Hellendoorn"
        ],
        "published": "iclr 2022 poster",
        "summary": "Structural locality is a ubiquitous feature of real-world datasets, wherein data points are organized into local hierarchies. Some examples include topical clusters in text or project hierarchies in source code repositories. In this paper, we explore utilizing this structural locality within non-parametric language models, which generate sequences that reference retrieved examples from an external source. We propose a simple yet effective approach for adding locality information into such models by adding learned parameters that improve the likelihood of retrieving examples from local neighborhoods. Experiments on two different domains, Java source code and Wikipedia text, demonstrate that locality features improve model efficacy over models without access to these features, with interesting differences. We also perform an analysis of how and where locality features contribute to improving performance and why the traditionally used contextual similarity metrics alone are not enough to grasp the locality structure.\n",
        "pdf_link": "https://openreview.net/pdf/05677eb0d7fca88dd7c4c6cbefa73f6ae430ad68.pdf",
        "forum_url": "https://openreview.net/forum?id=nnU3IUMJmN",
        "source": "iclr2022"
    },
    {
        "title": "Autonomous Reinforcement Learning: Formalism and Benchmarking",
        "authors": [
            "Archit Sharma",
            "Kelvin Xu",
            "Nikhil Sardana",
            "Abhishek Gupta",
            "Karol Hausman",
            "Sergey Levine",
            "Chelsea Finn"
        ],
        "published": "iclr 2022 poster",
        "summary": "Reinforcement learning (RL) provides a naturalistic framing for learning through trial and error, which is appealing both because of its simplicity and effectiveness and because of its resemblance to how humans and animals acquire skills through experience. However, real-world embodied learning, such as that performed by humans and animals, is situated in a continual, non-episodic world, whereas common benchmark tasks in RL are episodic, with the environment resetting between trials to provide the agent with multiple attempts. This discrepancy presents a major challenge when we attempt to take RL algorithms developed for episodic simulated environments and run  them on real-world platforms, such as robots. In this paper, we aim to address this discrepancy by laying out a framework for Autonomous Reinforcement Learning (ARL): reinforcement learning where the agent not only learns through its own experience, but also contends with lack of human supervision to reset between trials. We introduce a simulated benchmark EARL based on this framework, containing a set of diverse and challenging simulated tasks reflective of the hurdles introduced to learning when only a minimal reliance on extrinsic intervention can be assumed. We show that standard approaches to episodic RL and existing approaches struggle as interventions are minimized, underscoring the need for developing new algorithms for reinforcement learning with a greater focus on autonomy.",
        "pdf_link": "https://openreview.net/pdf/f572a28516f88b10b825a32cd24ba9922c1d015e.pdf",
        "forum_url": "https://openreview.net/forum?id=nkaba3ND7B5",
        "source": "iclr2022"
    },
    {
        "title": "LoRA: Low-Rank Adaptation of Large Language Models",
        "authors": [
            "Edward J Hu",
            "yelong shen",
            "Phillip Wallis",
            "Zeyuan Allen-Zhu",
            "Yuanzhi Li",
            "Shean Wang",
            "Lu Wang",
            "Weizhu Chen"
        ],
        "published": "iclr 2022 poster",
        "summary": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible.\nUsing GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by a factor of 10,000 and the GPU memory requirement by a factor of 3. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.",
        "pdf_link": "https://openreview.net/pdf/5a54aed5265cb0399c62848f44e84c4a617a354b.pdf",
        "forum_url": "https://openreview.net/forum?id=nZeVKeeFYf9",
        "source": "iclr2022"
    },
    {
        "title": "Policy Smoothing for Provably Robust Reinforcement Learning",
        "authors": [
            "Aounon Kumar",
            "Alexander Levine",
            "Soheil Feizi"
        ],
        "published": "iclr 2022 poster",
        "summary": "The study of provable adversarial robustness for deep neural networks (DNNs) has mainly focused on $\\textit{static}$ supervised learning tasks such as image classification. However, DNNs have been used extensively in real-world $\\textit{adaptive}$ tasks such as reinforcement learning (RL), making such systems vulnerable to adversarial attacks as well. Prior works in provable robustness in RL seek to certify the behaviour of the victim policy at every time-step against a non-adaptive adversary using methods developed for the static setting. But in the real world, an RL adversary can infer the defense strategy used by the victim agent by observing the states, actions, etc. from previous time-steps and adapt itself to produce stronger attacks in future steps (e.g., by focusing more on states critical to the agent's performance). We present an efficient procedure, designed specifically to defend against an adaptive RL adversary, that can directly certify the total reward without requiring the policy to be robust at each time-step. Focusing on randomized smoothing based defenses, our main theoretical contribution is to prove an $\\textit{adaptive version}$ of the Neyman-Pearson Lemma -- a key lemma for smoothing-based certificates -- where the adversarial perturbation at a particular time can be a stochastic function of current and previous observations and states as well as previous actions. Building on this result, we propose $\\textit{policy smoothing}$ where the agent adds a Gaussian noise to its observation at each time-step before passing it through the policy function. Our robustness certificates guarantee that the final total reward obtained by policy smoothing remains above a certain threshold, even though the actions at intermediate time-steps may change under the attack. We show that our certificates are $\\textit{tight}$ by constructing a worst-case scenario that achieves the bounds derived in our analysis. Our experiments on various environments like Cartpole, Pong, Freeway and Mountain Car show that our method can yield meaningful robustness guarantees in practice.\n",
        "pdf_link": "https://openreview.net/pdf/b1ed375c6d8559126ca3c590cf47feff1ae81aeb.pdf",
        "forum_url": "https://openreview.net/forum?id=mwdfai8NBrJ",
        "source": "iclr2022"
    },
    {
        "title": "Towards Model Agnostic Federated Learning Using Knowledge Distillation",
        "authors": [
            "Andrei Afonin",
            "Sai Praneeth Karimireddy"
        ],
        "published": "iclr 2022 poster",
        "summary": "Is it possible to design an universal API for federated learning using which an ad-hoc group of data-holders (agents) collaborate with each other and perform federated learning? Such an API would necessarily need to be model-agnostic i.e. make no assumption about the model architecture being used by the agents, and also cannot rely on having representative public data at hand. Knowledge distillation (KD) is the obvious tool of choice to design such protocols. However, surprisingly, we show that most natural KD-based federated learning protocols have poor performance.\n    \n    To investigate this, we propose a new theoretical framework, Federated Kernel ridge regression, which can capture both model heterogeneity as well as data heterogeneity. Our analysis shows that the degradation is largely due to a fundamental limitation of knowledge distillation under data heterogeneity. We further validate our framework by analyzing and designing new protocols based on KD. Their performance on real world experiments using neural networks, though still unsatisfactory, closely matches our theoretical predictions. ",
        "pdf_link": "https://openreview.net/pdf/f344afa0b3f559c81737bef7821131844aa1feb1.pdf",
        "forum_url": "https://openreview.net/forum?id=lQI_mZjvBxj",
        "source": "iclr2022"
    },
    {
        "title": "Learning to Schedule Learning rate with Graph Neural Networks",
        "authors": [
            "Yuanhao Xiong",
            "Li-Cheng Lan",
            "Xiangning Chen",
            "Ruochen Wang",
            "Cho-Jui Hsieh"
        ],
        "published": "iclr 2022 poster",
        "summary": "Recent decades have witnessed great development of stochastic optimization in training deep neural networks. Learning rate scheduling is one of the most important factors that influence the performance of stochastic optimizers like Adam. Traditional methods seek to find a relatively proper scheduling among a limited number of pre-defined rules and might not accommodate a particular target problem. Instead, we propose a novel Graph-Network-based Scheduler (GNS), aiming at learning a specific scheduling mechanism without restrictions to existing principles. By constructing a directed graph for the underlying neural network of the target problem, GNS encodes current dynamics with a graph message passing network and trains an agent to control the learning rate accordingly via reinforcement learning. The proposed scheduler can capture the intermediate layer information while being able to generalize to problems of varying scales. Besides, an efficient reward collection procedure is leveraged to speed up training. We evaluate our framework on benchmarking datasets, Fashion-MNIST and CIFAR10 for image classification, and GLUE for language understanding. GNS shows consistent improvement over popular baselines when training CNN and Transformer models. Moreover, GNS demonstrates great generalization to different datasets and network structures.",
        "pdf_link": "https://openreview.net/pdf/70093080d93ffa224e3b7bb6cfa481cad374f005.pdf",
        "forum_url": "https://openreview.net/forum?id=k7efTb0un9z",
        "source": "iclr2022"
    },
    {
        "title": "Cross-Lingual Transfer with Class-Weighted Language-Invariant Representations",
        "authors": [
            "Ruicheng Xian",
            "Heng Ji",
            "Han Zhao"
        ],
        "published": "iclr 2022 poster",
        "summary": "Recent advances in neural modeling have produced deep multilingual language models capable of extracting cross-lingual knowledge from non-parallel texts and enabling zero-shot downstream transfer. While their success is often attributed to shared representations, quantitative analyses are limited. Towards a better understanding, through empirical analyses, we show that the invariance of feature representations across languages—an effect of shared representations—strongly correlates with transfer performance. We also observe that distributional shifts in class priors between source and target language task data negatively affect performance, a largely overlooked issue that could cause negative transfer with existing unsupervised approaches. Based on these findings, we propose and evaluate a method for unsupervised transfer, called importance-weighted domain alignment (IWDA), that performs representation alignment with prior shift estimation and correction using unlabeled target language task data. Experiments demonstrate its superiority under large prior shifts, and show further performance gains when combined with existing semi-supervised learning techniques.",
        "pdf_link": "https://openreview.net/pdf/c75daaf7c5ca8ed7ab01e92c4cc16d55f5d6aff5.pdf",
        "forum_url": "https://openreview.net/forum?id=k7-s5HSSPE5",
        "source": "iclr2022"
    },
    {
        "title": "Visual Correspondence Hallucination",
        "authors": [
            "Hugo Germain",
            "Vincent Lepetit",
            "Guillaume Bourmaud"
        ],
        "published": "iclr 2022 poster",
        "summary": "Given a pair of partially overlapping source and target images and a keypoint in the source image, the keypoint's correspondent in the target image can be either visible, occluded or outside the field of view. Local feature matching methods are only able to identify the correspondent's location when it is visible, while humans can also hallucinate its location when it is occluded or outside the field of view through geometric reasoning.  In this paper, we bridge this gap by training a network to output a peaked probability distribution over the correspondent's location, regardless of this correspondent being visible, occluded, or outside the field of view.  We experimentally demonstrate that this network is indeed able to hallucinate correspondences on pairs of images captured in scenes that were not seen at training-time.  We also apply this network to an absolute camera pose estimation problem and find it is significantly more robust than state-of-the-art local feature matching-based competitors.",
        "pdf_link": "https://openreview.net/pdf/6374bf841920e8781ce8a4dbbf3877649df2ae13.pdf",
        "forum_url": "https://openreview.net/forum?id=jaLDP8Hp_gc",
        "source": "iclr2022"
    },
    {
        "title": "GNN is a Counter? Revisiting GNN for Question Answering",
        "authors": [
            "Kuan Wang",
            "Yuyu Zhang",
            "Diyi Yang",
            "Le Song",
            "Tao Qin"
        ],
        "published": "iclr 2022 poster",
        "summary": "Question Answering (QA) has been a long-standing research topic in AI and NLP fields, and a wealth of studies has been conducted to attempt to equip QA systems with human-level reasoning capability. To approximate the complicated human reasoning process, state-of-the-art QA systems commonly use pre-trained language models (LMs) to access knowledge encoded in LMs together with elaborately designed modules based on Graph Neural Networks (GNNs) to perform reasoning over knowledge graphs (KGs). However, many problems remain open regarding the reasoning functionality of these GNN-based modules. Can these GNN-based modules really perform a complex reasoning process? Are they under- or over-complicated for QA? To open the black box of GNN and investigate these problems, we dissect state-of-the-art GNN modules for QA and analyze their reasoning capability. We discover that even a very simple graph neural counter can outperform all the existing GNN modules on CommonsenseQA and OpenBookQA, two popular QA benchmark datasets which heavily rely on knowledge-aware reasoning. Our work reveals that existing knowledge-aware GNN modules may only carry out some simple reasoning such as counting. It remains a challenging open problem to build comprehensive reasoning modules for knowledge-powered QA.",
        "pdf_link": "https://openreview.net/pdf/b4b3e479c999df701d9e74120833e9a3c1edb8f3.pdf",
        "forum_url": "https://openreview.net/forum?id=hzmQ4wOnSb",
        "source": "iclr2022"
    },
    {
        "title": "Continuously Discovering Novel Strategies via Reward-Switching Policy Optimization",
        "authors": [
            "Zihan Zhou",
            "Wei Fu",
            "Bingliang Zhang",
            "Yi Wu"
        ],
        "published": "iclr 2022 poster",
        "summary": "We present Reward-Switching Policy Optimization (RSPO), a paradigm to discover diverse strategies in complex RL environments by iteratively finding novel policies that are both locally optimal and sufficiently different from existing ones. To encourage the learning policy to consistently converge towards a previously undiscovered local optimum, RSPO switches between extrinsic and intrinsic rewards via a trajectory-based novelty measurement during the optimization process. When a sampled trajectory is sufficiently distinct, RSPO performs standard policy optimization with extrinsic rewards. For trajectories with high likelihood under existing policies, RSPO utilizes an intrinsic diversity reward to promote exploration. Experiments show that RSPO is able to discover a wide spectrum of strategies in a variety of domains, ranging from single-agent navigation tasks and MuJoCo control to multi-agent stag-hunt games and the StarCraft II Multi-Agent Challenge.",
        "pdf_link": "https://openreview.net/pdf/a123c43ffc76a63f5bbe2eccdf4bba5a4233b9ed.pdf",
        "forum_url": "https://openreview.net/forum?id=hcQHRHKfN_",
        "source": "iclr2022"
    },
    {
        "title": "Illiterate DALL-E Learns to Compose",
        "authors": [
            "Gautam Singh",
            "Fei Deng",
            "Sungjin Ahn"
        ],
        "published": "iclr 2022 poster",
        "summary": "Although DALL-E has shown an impressive ability of composition-based systematic generalization in image generation, it requires the dataset of text-image pairs and the compositionality is provided by the text. In contrast, object-centric representation models like the Slot Attention model learn composable representations without the text prompt. However, unlike DALL-E, its ability to systematically generalize for zero-shot generation is significantly limited. In this paper, we propose a simple but novel slot-based autoencoding architecture, called SLATE, for combining the best of both worlds: learning object-centric representations that allow systematic generalization in zero-shot image generation without text. As such, this model can also be seen as an illiterate DALL-E model. Unlike the pixel-mixture decoders of existing object-centric representation models, we propose to use the Image GPT decoder conditioned on the slots for capturing complex interactions among the slots and pixels. In experiments, we show that this simple and easy-to-implement architecture not requiring a text prompt achieves significant improvement in in-distribution and out-of-distribution (zero-shot) image generation and qualitatively comparable or better slot-attention structure than the models based on mixture decoders.",
        "pdf_link": "https://openreview.net/pdf/bcb5e847c6cefafd402be4829f49227cf6b457c7.pdf",
        "forum_url": "https://openreview.net/forum?id=h0OYV0We3oh",
        "source": "iclr2022"
    },
    {
        "title": "Iterated Reasoning with Mutual Information in Cooperative and Byzantine Decentralized Teaming",
        "authors": [
            "Sachin G Konan",
            "Esmaeil Seraj",
            "Matthew Gombolay"
        ],
        "published": "iclr 2022 poster",
        "summary": "Information sharing is key in building team cognition and enables coordination and cooperation. High-performing human teams also benefit from acting strategically with hierarchical levels of iterated communication and rationalizability, meaning a human agent can reason about the actions of their teammates in their decision-making. Yet, the majority of prior work in Multi-Agent Reinforcement Learning (MARL) does not support iterated rationalizability and only encourage inter-agent communication, resulting in a suboptimal equilibrium cooperation strategy. In this work, we show that reformulating an agent's policy to be conditional on the policies of its neighboring teammates inherently maximizes Mutual Information (MI) lower-bound when optimizing under Policy Gradient (PG). Building on the idea of decision-making under bounded rationality and cognitive hierarchy theory, we show that our modified PG approach not only maximizes local agent rewards but also implicitly reasons about MI between agents without the need for any explicit ad-hoc regularization terms. Our approach, InfoPG, outperforms baselines in learning emergent collaborative behaviors and sets the state-of-the-art in decentralized cooperative MARL tasks. Our experiments validate the utility of InfoPG by achieving higher sample efficiency and significantly larger cumulative reward in several complex cooperative multi-agent domains.",
        "pdf_link": "https://openreview.net/pdf/6aa48ac42e21539265419c747bb0aedda2b1992d.pdf",
        "forum_url": "https://openreview.net/forum?id=giBFoa-uS12",
        "source": "iclr2022"
    },
    {
        "title": "Global Convergence of Multi-Agent Policy Gradient in Markov Potential Games",
        "authors": [
            "Stefanos Leonardos",
            "Will Overman",
            "Ioannis Panageas",
            "Georgios Piliouras"
        ],
        "published": "iclr 2022 poster",
        "summary": "Potential games are arguably one of the most important and widely studied classes of normal form games. They define the archetypal setting of multi-agent coordination in which all agents utilities are perfectly aligned via a common potential function. Can this intuitive framework be transplanted in the setting of Markov games? What are the similarities and differences between multi-agent coordination with and without state dependence? To answer these questions, we study a natural class of Markov Potential Games (MPGs) that generalize prior attempts at capturing complex stateful multi-agent coordination. Counter-intuitively, insights from normal-form potential games do not carry over as MPGs involve settings where state-games can be zero-sum games. In the opposite direction, Markov games where every state-game is a potential game are not necessarily MPGs. Nevertheless, MPGs showcase standard desirable properties such as the existence of deterministic Nash policies. In our main technical result, we prove convergence of independent policy gradient and its stochastic counterpart to Nash policies (polynomially fast in the approximation error) by adapting recent gradient dominance property arguments developed for single-agent Markov decision processes to multi-agent learning settings. \n",
        "pdf_link": "https://openreview.net/pdf/91e8010784f5d9c14cf26f0a2e86f8b46808d3ae.pdf",
        "forum_url": "https://openreview.net/forum?id=gfwON7rAm4",
        "source": "iclr2022"
    },
    {
        "title": "Mapping Language Models to Grounded Conceptual Spaces",
        "authors": [
            "Roma Patel",
            "Ellie Pavlick"
        ],
        "published": "iclr 2022 poster",
        "summary": "A fundamental criticism of text-only language models (LMs) is their lack of grounding---that is, the ability to tie a word for which they have learned a representation, to its actual use in the world. However, despite this limitation, large pre-trained LMs have been shown to have a remarkable grasp of the conceptual structure of language, as demonstrated by their ability to answer questions, generate fluent text, or make inferences about entities, objects, and properties that they have never physically observed. In this work we investigate the extent to which the rich conceptual structure that LMs learn indeed reflects the conceptual structure of the non-linguistic world---which is something that LMs have never observed. We do this by testing whether the LMs can learn to map an entire conceptual domain (e.g., direction or colour) onto a grounded world representation given only a small number of examples. For example, we show a model what the word ``left\" means using a textual depiction of a grid world, and assess how well it can generalise to related concepts, for example, the word ``right\", in a similar grid world. We investigate a range of generative language models of varying sizes (including GPT-2 and GPT-3), and see that although the smaller models struggle to perform this mapping, the largest model can not only learn to ground the concepts that it is explicitly taught, but appears to generalise to several instances of unseen concepts as well. Our results suggest an alternative means of building grounded language models: rather than learning grounded representations ``from scratch'', it is possible that large text-only models learn a sufficiently rich conceptual structure that could allow them to be grounded in a data-efficient way.",
        "pdf_link": "https://openreview.net/pdf/e21987f87f4a8c8ec30cd2613232d4f27014f121.pdf",
        "forum_url": "https://openreview.net/forum?id=gJcEM8sxHK",
        "source": "iclr2022"
    },
    {
        "title": "How Did the Model Change? Efficiently Assessing Machine Learning API Shifts ",
        "authors": [
            "Lingjiao Chen",
            "Matei Zaharia",
            "James Zou"
        ],
        "published": "iclr 2022 poster",
        "summary": "ML prediction APIs from providers like Amazon and Google have made it simple to use ML in applications. A challenge for users is that such APIs continuously change over time as the providers update models, and changes can happen silently without users knowing. It is thus important to monitor when and how much the MLAPIs’ performance shifts. To provide detailed change assessment, we model MLAPI shifts as confusion matrix differences, and propose a principled algorithmic framework, MASA, to provably assess these shifts efficiently given a sample budget constraint.MASAemploys an upper-confidence bound based approach to adaptively determine on which data point to query the ML API to estimate shifts. Empirically, we observe significant ML API shifts from 2020 to 2021 among 12 out of 36 applications using commercial APIs from Google, Microsoft, Amazon, and other providers. These real-world shifts include both improvements and reductions in accuracy. Extensive experiments show that MASA can estimate such API shifts more accurately than standard approaches given the same budget",
        "pdf_link": "https://openreview.net/pdf/acadda3ff8e7686ab4e0b73fd64ba0ef89f2134a.pdf",
        "forum_url": "https://openreview.net/forum?id=gFDFKC4gHL4",
        "source": "iclr2022"
    },
    {
        "title": "Structure-Aware Transformer Policy for Inhomogeneous Multi-Task Reinforcement Learning",
        "authors": [
            "Sunghoon Hong",
            "Deunsol Yoon",
            "Kee-Eung Kim"
        ],
        "published": "iclr 2022 poster",
        "summary": "Modular Reinforcement Learning, where the agent is assumed to be morphologically structured as a graph, for example composed of limbs and joints, aims to learn a policy that is transferable to a structurally similar but different agent. Compared to traditional Multi-Task Reinforcement Learning, this promising approach allows us to cope with inhomogeneous tasks where the state and action space dimensions differ across tasks. Graph Neural Networks are a natural model for representing the pertinent policies, but a recent work has shown that their multi-hop message passing mechanism is not ideal for conveying important information to other modules and thus a transformer model without morphological information was proposed. In this work, we argue that the morphological information is still very useful and propose a transformer policy model that effectively encodes such information. Specifically, we encode the morphological information in terms of the traversal-based positional embedding and the graph-based relational embedding. We empirically show that the morphological information is crucial for modular reinforcement learning, substantially outperforming prior state-of-the-art methods on multi-task learning as well as transfer learning settings with different state and action space dimensions.",
        "pdf_link": "https://openreview.net/pdf/111d5058b0200075159b27c0969addf7f1a2a871.pdf",
        "forum_url": "https://openreview.net/forum?id=fy_XRVHqly",
        "source": "iclr2022"
    },
    {
        "title": "Pretrained Language Model in Continual Learning: A Comparative Study",
        "authors": [
            "Tongtong Wu",
            "Massimo Caccia",
            "Zhuang Li",
            "Yuan-Fang Li",
            "Guilin Qi",
            "Gholamreza Haffari"
        ],
        "published": "iclr 2022 poster",
        "summary": "Continual learning (CL) is a  setting in which a model learns from a stream of incoming data while avoiding to forget previously learned knowledge. Pre-trained language models (PLMs) have been successfully employed in continual learning of different natural language problems. With the rapid development of many continual learning methods and PLMs, understanding and disentangling their interactions become essential for continued improvement of continual learning performance. In this paper, we thoroughly compare the continual learning performance over the combination of 5 PLMs and 4 CL approaches on 3 benchmarks in 2 typical incremental settings. Our extensive experimental analyses reveal interesting performance differences across PLMs and across CL methods. Furthermore, our representativeness probing analyses dissect PLMs’ performance characteristics in a layer-wise and task-wise manner, uncovering the extent to which their inner layers suffer from forgetting, and the effect of different CL approaches on each layer. Finally, our observations and analyses open up a number of important research questions that will inform and guide the design of effective continual learning techniques.",
        "pdf_link": "https://openreview.net/pdf/eecf8ea887fe5b42c7b3b93b2fb938fed766f7e2.pdf",
        "forum_url": "https://openreview.net/forum?id=figzpGMrdD",
        "source": "iclr2022"
    },
    {
        "title": "Scale Efficiently: Insights from Pretraining and Finetuning Transformers",
        "authors": [
            "Yi Tay",
            "Mostafa Dehghani",
            "Jinfeng Rao",
            "William Fedus",
            "Samira Abnar",
            "Hyung Won Chung",
            "Sharan Narang",
            "Dani Yogatama",
            "Ashish Vaswani",
            "Donald Metzler"
        ],
        "published": "iclr 2022 poster",
        "summary": "There remain many open questions pertaining to the scaling behaviour of Transformer architectures. These scaling decisions and findings can be critical, as training runs often come with an associated computational cost which have both financial and/or environmental impact. The goal of this paper is to present scaling insights from pretraining and finetuning Transformers. While Kaplan et al. presents a comprehensive study of the scaling behaviour of Transformer language models, the scope is only on the upstream (pretraining) loss. Therefore, it is still unclear if these set of findings transfer to downstream task within the context of the pretrain-finetune paradigm. The key findings of this paper are as follows: (1) we show that aside from only the model size, model shape matters for downstream fine-tuning, (2) scaling protocols operate differently at different compute regions, (3) widely adopted T5-base and T5-large sizes are Pareto-inefficient. To this end, we present improved scaling protocols whereby our redesigned models achieve similar downstream fine-tuning quality while having 50\\% fewer parameters and training 40\\% faster compared to the widely adopted T5-base model. We publicly release over 100 pretrained checkpoints of different T5 configurations to facilitate future research and analysis.",
        "pdf_link": "https://openreview.net/pdf/bd048936d58108f06fb49d610e86a9a5dcb9b281.pdf",
        "forum_url": "https://openreview.net/forum?id=f2OYVDyfIB",
        "source": "iclr2022"
    },
    {
        "title": "Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners",
        "authors": [
            "Ningyu Zhang",
            "Luoqiu Li",
            "Xiang Chen",
            "Shumin Deng",
            "Zhen Bi",
            "Chuanqi Tan",
            "Fei Huang",
            "Huajun Chen"
        ],
        "published": "iclr 2022 poster",
        "summary": "Large-scale pre-trained language models have contributed significantly to natural language processing by demonstrating remarkable abilities as few-shot learners. However, their effectiveness depends mainly on scaling the model parameters and prompt design, hindering their implementation in most real-world applications. This study proposes a novel pluggable, extensible, and efficient approach named DifferentiAble pRompT (DART), which can convert small language models into better few-shot learners. The main principle behind this approach involves reformulating potential natural language processing tasks into the task of a pre-trained language model and differentially optimizing the prompt template as well as the target label with backpropagation. Furthermore, the proposed approach can be: (i) Plugged to any pre-trained language models; (ii) Extended to widespread classification tasks. A comprehensive evaluation of standard NLP tasks demonstrates that the proposed approach achieves a better few-shot performance.",
        "pdf_link": "https://openreview.net/pdf/d0850eb512cbbd2b27f6ff0ab0edf71861cd4829.pdf",
        "forum_url": "https://openreview.net/forum?id=ek9a0qIafW",
        "source": "iclr2022"
    },
    {
        "title": "On Robust Prefix-Tuning for Text Classification",
        "authors": [
            "Zonghan Yang",
            "Yang Liu"
        ],
        "published": "iclr 2022 poster",
        "summary": "Recently, prefix-tuning has gained increasing attention as a parameter-efficient finetuning method for large-scale pretrained language models. The method keeps the pretrained models fixed and only updates the prefix token parameters for each downstream task. Despite being lightweight and modular, prefix-tuning still lacks robustness to textual adversarial attacks. However, most currently developed defense techniques necessitate auxiliary model update and storage, which inevitably hamper the modularity and low storage of prefix-tuning. In this work, we propose a robust prefix-tuning framework that preserves the efficiency and modularity of prefix-tuning. The core idea of our framework is leveraging the layerwise activations of the language model by correctly-classified training data as the standard for additional prefix finetuning. During the test phase, an extra batch-level prefix is tuned for each batch and added to the original prefix for robustness enhancement. Extensive experiments on three text classification benchmarks show that our framework substantially improves robustness over several strong baselines against five textual attacks of different types while maintaining comparable accuracy on clean texts. We also interpret our robust prefix-tuning framework from the optimal control perspective and pose several directions for future research.",
        "pdf_link": "https://openreview.net/pdf/02dfcabb44949137b40a59f94715b5caa4b12231.pdf",
        "forum_url": "https://openreview.net/forum?id=eBCmOocUejf",
        "source": "iclr2022"
    },
    {
        "title": "One After Another: Learning Incremental Skills for a Changing World",
        "authors": [
            "Nur Muhammad Mahi Shafiullah",
            "Lerrel Pinto"
        ],
        "published": "iclr 2022 poster",
        "summary": "Reward-free, unsupervised discovery of skills is an attractive alternative to the bottleneck of hand-designing rewards in environments where task supervision is scarce or expensive. However, current skill pre-training methods, like many RL techniques, make a fundamental assumption -- stationary environments during training. Traditional methods learn all their skills simultaneously, which makes it difficult for them to both quickly adapt to changes in the environment, and to not forget earlier skills after such adaptation. On the other hand, in an evolving or expanding environment, skill learning must be able to adapt fast to new environment situations while not forgetting previously learned skills. These two conditions make it difficult for classic skill discovery to do well in an evolving environment. In this work, we propose a new framework for skill discovery, where skills are learned one after another in an incremental fashion. This framework allows newly learned skills to adapt to new environment or agent dynamics, while the fixed old skills ensure the agent doesn't forget a learned skill. We demonstrate experimentally that in both evolving and static environments, incremental skills significantly outperform current state-of-the-art skill discovery methods on both skill quality and the ability to solve downstream tasks. Videos for learned skills and code are made public on https://notmahi.github.io/disk\n",
        "pdf_link": "https://openreview.net/pdf/1ee5373bdf14fb52571018799bfe53c696d99868.pdf",
        "forum_url": "https://openreview.net/forum?id=dg79moSRqIo",
        "source": "iclr2022"
    },
    {
        "title": "Objects in Semantic Topology",
        "authors": [
            "Shuo Yang",
            "Peize Sun",
            "Yi Jiang",
            "Xiaobo Xia",
            "Ruiheng Zhang",
            "Zehuan Yuan",
            "Changhu Wang",
            "Ping Luo",
            "Min Xu"
        ],
        "published": "iclr 2022 poster",
        "summary": "A more realistic object detection paradigm, Open-World Object Detection, has arised increasing research interests in the community recently. A qualified open-world object detector can not only identify objects of known categories, but also discover unknown objects, and incrementally learn to categorize them when their annotations progressively arrive. Previous works rely on independent modules to recognize unknown categories and perform incremental learning, respectively. In this paper, we provide a unified perspective: Semantic Topology. During the life-long learning of an open-world object detector, all object instances from the same category are assigned to their corresponding pre-defined node in the semantic topology, including the `unknown' category. This constraint builds up discriminative feature representations and consistent relationships among objects, thus enabling the detector to distinguish unknown objects out of the known categories, as well as making learned features of known objects undistorted when learning new categories incrementally. Extensive experiments demonstrate that semantic topology, either randomly-generated or derived from a well-trained language model, could outperform the current state-of-the-art open-world object detectors by a large margin, e.g., the absolute open-set error (the number of unknown instances that are wrongly labeled as known) is reduced from 7832 to 2546, exhibiting the inherent superiority of semantic topology on open-world object detection.",
        "pdf_link": "https://openreview.net/pdf/ff70332fa4b027995f092ed696137154488aa5fc.pdf",
        "forum_url": "https://openreview.net/forum?id=d5SCUJ5t1k",
        "source": "iclr2022"
    },
    {
        "title": "Group-based Interleaved Pipeline Parallelism for Large-scale DNN Training",
        "authors": [
            "PengCheng Yang",
            "Xiaoming Zhang",
            "Wenpeng Zhang",
            "Ming Yang",
            "Hong Wei"
        ],
        "published": "iclr 2022 poster",
        "summary": "The recent trend of using large-scale deep neural networks (DNN) to boost performance has propelled the development of the parallel pipelining technique for efficient DNN training, which has resulted in the development of several prominent pipelines such as GPipe, PipeDream, and PipeDream-2BW. However, the current leading pipeline PipeDream-2BW still suffers from two major drawbacks, i.e., the excessive memory redundancy and the delayed weight updates across all stages. In this work, we propose a novel pipeline named WPipe, which achieves better memory efficiency and fresher weight updates. WPipe uses a novel pipelining scheme that divides model partitions into two groups. It moves the forward pass of the next period of weight updates to the front of the backward pass of the current period of weight updates in the first group, retains the order in the second group, and updates each group alternatively. This scheme can eliminate half of the delayed gradients and memory redundancy compared to PipeDream-2BW. The experiments, which train large BERT language models, show that compared to PipeDream-2BW, WPipe achieves $1.4\\times$ acceleration and reduces the memory footprint by 36%, without nearly sacrificing any final model accuracy.",
        "pdf_link": "https://openreview.net/pdf/2322466e5de76b982eaeca16cda0dc1dfd2f5563.pdf",
        "forum_url": "https://openreview.net/forum?id=cw-EmNq5zfD",
        "source": "iclr2022"
    },
    {
        "title": "A Fine-Tuning Approach to Belief State Modeling",
        "authors": [
            "Samuel Sokota",
            "Hengyuan Hu",
            "David J Wu",
            "J Zico Kolter",
            "Jakob Nicolaus Foerster",
            "Noam Brown"
        ],
        "published": "iclr 2022 poster",
        "summary": "We investigate the challenge of modeling the belief state of a partially observable Markov system, given sample-access to its dynamics model. This problem setting is often approached using parametric sequential generative modeling methods. However, these methods do not leverage any additional computation at inference time to increase their accuracy. Moreover, applying these methods to belief state modeling in certain multi-agent settings would require passing policies into the belief model---at the time of writing, there have been no successful demonstrations of this. Toward addressing these shortcomings, we propose an inference-time improvement framework for parametric sequential generative modeling methods called belief fine-tuning (BFT). BFT leverages approximate dynamic programming in the form of fine-tuning to determine the model parameters at each time step. It can improve the accuracy of the belief model at test time because it specializes the model to the space of local observations. Furthermore, because this specialization occurs after the action or policy has already been decided, BFT does not require the belief model to process it as input. As a result of the latter point, BFT enables, for the first time, approximate public belief state search in imperfect-information games where the number of possible information states is too large to track tabularly. We exhibit these findings on large-scale variants of the benchmark game Hanabi.",
        "pdf_link": "https://openreview.net/pdf/944fbcd08f3183592995e10fc1c179afad1bfc67.pdf",
        "forum_url": "https://openreview.net/forum?id=ckZY7DGa7FQ",
        "source": "iclr2022"
    },
    {
        "title": "Evaluating Distributional Distortion in Neural Language Modeling",
        "authors": [
            "Benjamin LeBrun",
            "Alessandro Sordoni",
            "Timothy J. O'Donnell"
        ],
        "published": "iclr 2022 poster",
        "summary": "A fundamental characteristic of natural language is the high rate at which speakers produce novel expressions. Because of this novelty, a heavy-tail of rare events accounts for a significant amount of the total probability mass of distributions in language (Baayen, 2001). Standard language modeling metrics such as perplexity quantify the performance of language models (LM) in aggregate.  As a result, we have relatively little understanding of whether neural LMs accurately estimate the probability of sequences in this heavy-tail of rare events. To address this gap, we develop a controlled evaluation scheme which uses generative models trained on natural data as artificial languages from which we can exactly compute sequence probabilities. Training LMs on generations from these artificial languages, we compare the sequence-level probability estimates given by LMs to the true probabilities in the target language. Our experiments reveal that LSTM and Transformer language models (i) systematically underestimate the probability of sequences drawn from the target language, and (ii) do so more severely for less-probable sequences. Investigating where this probability mass went, (iii) we find that LMs tend to overestimate the probability of ill formed (perturbed) sequences. In addition, we find that this underestimation behaviour (iv) is weakened, but not eliminated by greater amounts of training data, and (v) is exacerbated for target distributions with lower entropy.",
        "pdf_link": "https://openreview.net/pdf/c22ea9d1df97b96c390eb350b4c09eb8e2388128.pdf",
        "forum_url": "https://openreview.net/forum?id=bTteFbU99ye",
        "source": "iclr2022"
    },
    {
        "title": "RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning",
        "authors": [
            "Xiaojian Ma",
            "Weili Nie",
            "Zhiding Yu",
            "Huaizu Jiang",
            "Chaowei Xiao",
            "Yuke Zhu",
            "Song-Chun Zhu",
            "Anima Anandkumar"
        ],
        "published": "iclr 2022 poster",
        "summary": "Reasoning about visual relationships is central to how humans interpret the visual world. This task remains challenging for current deep learning algorithms since it requires addressing three key technical problems jointly: 1) identifying object entities and their properties, 2) inferring semantic relations between pairs of entities, and 3) generalizing to novel object-relation combinations, i.e., systematic generalization. In this work, we use vision transformers (ViTs) as our base model for visual reasoning and make better use of concepts defined as object entities and their relations to improve the reasoning ability of ViTs. Specifically, we introduce a novel concept-feature dictionary to allow flexible image feature retrieval at training time with concept keys. This dictionary enables two new concept-guided auxiliary tasks: 1) a global task for promoting relational reasoning, and 2) a local task for facilitating semantic object-centric correspondence learning. To examine the systematic generalization of visual reasoning models, we introduce systematic splits for the standard HICO and GQA benchmarks. We show the resulting model, Concept-guided Vision Transformer (or RelViT for short) significantly outperforms prior approaches on HICO and GQA by 16% and 13% in the original split, and by 43% and 18% in the systematic split. Our ablation analyses also reveal our model's compatibility with multiple ViT variants and robustness to hyper-parameters.",
        "pdf_link": "https://openreview.net/pdf/9ae93c86cdada9dfdeef3cf2c7ee77363fe51c7b.pdf",
        "forum_url": "https://openreview.net/forum?id=afoV8W3-IYp",
        "source": "iclr2022"
    },
    {
        "title": "Generalisation in Lifelong Reinforcement Learning through Logical Composition ",
        "authors": [
            "Geraud Nangue Tasse",
            "Steven James",
            "Benjamin Rosman"
        ],
        "published": "iclr 2022 poster",
        "summary": "We leverage logical composition in reinforcement learning to create a framework that enables an agent to autonomously determine whether a new task can be immediately solved using its existing abilities, or whether a task-specific skill should be learned. In the latter case, the proposed algorithm also enables the agent to learn the new task faster by generating an estimate of the optimal policy. Importantly, we provide two main theoretical results: we bound the performance of the transferred policy on a new task, and we give bounds on the necessary and sufficient number of tasks that need to be learned throughout an agent's lifetime to generalise over a distribution. We verify our approach in a series of experiments, where we perform transfer learning both after learning a set of base tasks, and after learning an arbitrary set of tasks. We also demonstrate that, as a side effect of our transfer learning approach, an agent can produce an interpretable Boolean expression of its understanding of the current task. Finally, we demonstrate our approach in the full lifelong setting where an agent receives tasks from an unknown distribution. Starting from scratch, an agent is able to quickly generalise over the task distribution after learning only a few tasks, which are sub-logarithmic in the size of the task space.",
        "pdf_link": "https://openreview.net/pdf/89cb79a9b9bb6a9a833a7a8ae73c8c5a87792970.pdf",
        "forum_url": "https://openreview.net/forum?id=ZOcX-eybqoL",
        "source": "iclr2022"
    },
    {
        "title": "Case-based reasoning for better generalization in textual reinforcement learning",
        "authors": [
            "Mattia Atzeni",
            "Shehzaad Zuzar Dhuliawala",
            "Keerthiram Murugesan",
            "Mrinmaya Sachan"
        ],
        "published": "iclr 2022 poster",
        "summary": "Text-based games (TBG) have emerged as promising environments for driving research in grounded language understanding and studying problems like generalization and sample efficiency. Several deep reinforcement learning (RL) methods with varying architectures and learning schemes have been proposed for TBGs. However, these methods fail to generalize efficiently, especially under distributional shifts. In a departure from deep RL approaches, in this paper, we propose a general method inspired by case-based reasoning to train agents and generalize out of the training distribution. The case-based reasoner collects instances of positive experiences from the agent's interaction with the world and later reuses the collected experiences to act efficiently. The method can be used in conjunction with any existing on-policy neural agent introduced in the literature for TBGs. Our experiments show that the proposed approach consistently improves existing methods, obtains good out-of-distribution generalization and achieves new state-of-the-art results on widely used environments.",
        "pdf_link": "https://openreview.net/pdf/426ad0e0a618d416dbdc8a2fbaa7f29661e1f920.pdf",
        "forum_url": "https://openreview.net/forum?id=ZDaSIkWT-AP",
        "source": "iclr2022"
    },
    {
        "title": "Cross-Trajectory Representation Learning for Zero-Shot Generalization in RL",
        "authors": [
            "Bogdan Mazoure",
            "Ahmed M Ahmed",
            "R Devon Hjelm",
            "Andrey Kolobov",
            "Patrick MacAlpine"
        ],
        "published": "iclr 2022 poster",
        "summary": "A highly desirable property of a reinforcement learning (RL) agent -- and a major difficulty for deep RL approaches -- is the ability to generalize policies learned on a few tasks over a high-dimensional observation space to similar tasks not seen during training. Many promising approaches to this challenge consider RL as a process of training two functions simultaneously: a complex nonlinear encoder that maps high-dimensional observations to a latent representation space, and a simple linear policy over this space. We posit that a superior encoder for zero-shot generalization in RL can be trained by using solely an auxiliary SSL objective if the training process encourages the encoder to map behaviorally similar observations to similar representations, as reward-based signal can cause overfitting in the encoder (Raileanu et al., 2021). We propose Cross-Trajectory Representation Learning (CTRL), a method that runs within an RL agent and conditions its encoder to recognize behavioral similarity in observations by applying a novel SSL objective to pairs of trajectories from the agent's policies. CTRL can be viewed as having the same effect as inducing a pseudo-bisimulation metric but, crucially, avoids the use of rewards and associated overfitting risks. Our experiments ablate various components of CTRL and demonstrate that in combination with PPO it achieves better generalization performance on the challenging Procgen benchmark suite (Cobbe et al., 2020).  ",
        "pdf_link": "https://openreview.net/pdf/64c9cd96c4583a048d9effe87d02766bc94a2e07.pdf",
        "forum_url": "https://openreview.net/forum?id=XOh5x-vxsrV",
        "source": "iclr2022"
    },
    {
        "title": "Dealing with Non-Stationarity in MARL via Trust-Region Decomposition",
        "authors": [
            "Wenhao Li",
            "Xiangfeng Wang",
            "Bo Jin",
            "Junjie Sheng",
            "Hongyuan Zha"
        ],
        "published": "iclr 2022 poster",
        "summary": "Non-stationarity is one thorny issue in cooperative multi-agent reinforcement learning (MARL). One of the reasons is the policy changes of agents during the learning process. Some existing works have discussed various consequences caused by non-stationarity with several kinds of measurement indicators. This makes the objectives or goals of existing algorithms are inevitably inconsistent and disparate. In this paper, we introduce a novel notion, the $\\delta$-$stationarity$ measurement, to explicitly measure the non-stationarity of a policy sequence, which can be further proved to be bounded by the KL-divergence of consecutive joint policies. A straightforward but highly non-trivial way is to control the joint policies' divergence, which is difficult to estimate accurately by imposing the trust-region constraint on the joint policy. Although it has lower computational complexity to decompose the joint policy and impose trust-region constraints on the factorized policies, simple policy factorization like mean-field approximation will lead to more considerable policy divergence, which can be considered as the trust-region decomposition dilemma. We model the joint policy as a pairwise Markov random field and propose a trust-region decomposition network (TRD-Net) based on message passing to estimate the joint policy divergence more accurately. The Multi-Agent Mirror descent policy algorithm with Trust region decomposition, called MAMT, is established by adjusting the trust-region of the local policies adaptively in an end-to-end manner. MAMT can approximately constrain the consecutive joint policies' divergence to satisfy $\\delta$-stationarity and alleviate the non-stationarity problem. Our method can bring noticeable and stable performance improvement compared with baselines in cooperative tasks of different complexity.",
        "pdf_link": "https://openreview.net/pdf/533ea53e5b32c81e14b06b4528f54a68836c63a0.pdf",
        "forum_url": "https://openreview.net/forum?id=XHUxf5aRB3s",
        "source": "iclr2022"
    },
    {
        "title": "Scene Transformer: A unified architecture for predicting future trajectories of multiple agents",
        "authors": [
            "Jiquan Ngiam",
            "Vijay Vasudevan",
            "Benjamin Caine",
            "Zhengdong Zhang",
            "Hao-Tien Lewis Chiang",
            "Jeffrey Ling",
            "Rebecca Roelofs",
            "Alex Bewley",
            "Chenxi Liu",
            "Ashish Venugopal",
            "David J Weiss",
            "Benjamin Sapp",
            "Zhifeng Chen",
            "Jonathon Shlens"
        ],
        "published": "iclr 2022 poster",
        "summary": "Predicting the motion of multiple agents is necessary for planning in dynamic environments. This task is challenging for autonomous driving since agents (e.g., vehicles and pedestrians) and their associated behaviors may be diverse and influence one another. Most prior work have focused on predicting independent futures for each agent based on all past motion, and planning against these independent predictions. However, planning against independent predictions can make it challenging to represent the future interaction possibilities between different agents, leading to sub-optimal planning. In this work, we formulate a model for predicting the behavior of all agents jointly, producing consistent futures that account for interactions between agents. Inspired by recent language modeling approaches, we use a masking strategy as the query to our model, enabling one to invoke a single model to predict agent behavior in many ways, such as potentially conditioned on the goal or full future trajectory of the autonomous vehicle or the behavior of other agents in the environment. Our model architecture employs attention to combine features across road elements, agent interactions, and time steps. We evaluate our approach on autonomous driving datasets for both marginal and joint motion prediction, and achieve state of the art performance across two popular datasets. Through combining a scene-centric approach, agent permutation equivariant model, and a sequence masking strategy, we show that our model can unify a variety of motion prediction tasks from joint motion predictions to conditioned prediction.",
        "pdf_link": "https://openreview.net/pdf/92f191f2cdcf1389ed2d3dce901833dc5fc6deaf.pdf",
        "forum_url": "https://openreview.net/forum?id=Wm3EA5OlHsG",
        "source": "iclr2022"
    },
    {
        "title": "Measuring CLEVRness: Black-box Testing of Visual Reasoning Models",
        "authors": [
            "Spyridon Mouselinos",
            "Henryk Michalewski",
            "Mateusz Malinowski"
        ],
        "published": "iclr 2022 poster",
        "summary": "How can we measure the reasoning capabilities of intelligence systems? Visual question answering provides a convenient framework for testing the model's abilities by interrogating the model through questions about the scene. However, despite scores of various visual QA datasets and architectures, which sometimes yield even a super-human performance, the question of whether those architectures can actually reason remains open to debate.\nTo answer this, we extend the visual question answering framework and propose the following behavioral test in the form of a two-player game. We consider black-box neural models of CLEVR. These models are trained on a diagnostic dataset benchmarking reasoning. Next, we train an adversarial player that re-configures the scene to fool the CLEVR model. We show that CLEVR models, which otherwise could perform at a ``human-level'', can easily be fooled by our agent. Our results \nput in doubt whether data-driven approaches can do reasoning without exploiting the numerous biases that are often present in those datasets. Finally, we also propose a controlled experiment measuring the efficiency of such models to learn and perform reasoning.",
        "pdf_link": "https://openreview.net/pdf/3eb3b1766e7d5addbbef3045662e6ad378427ae1.pdf",
        "forum_url": "https://openreview.net/forum?id=UtGtoS4CYU",
        "source": "iclr2022"
    },
    {
        "title": "SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning",
        "authors": [
            "Jongjin Park",
            "Younggyo Seo",
            "Jinwoo Shin",
            "Honglak Lee",
            "Pieter Abbeel",
            "Kimin Lee"
        ],
        "published": "iclr 2022 poster",
        "summary": "Preference-based reinforcement learning (RL) has shown potential for teaching agents to perform the target tasks without a costly, pre-defined reward function by learning the reward with a supervisor’s preference between the two agent behaviors. However, preference-based learning often requires a large amount of human feedback, making it difficult to apply this approach to various applications. This data-efficiency problem, on the other hand, has been typically addressed by using unlabeled samples or data augmentation techniques in the context of supervised learning. Motivated by the recent success of these approaches, we present SURF, a semi-supervised reward learning framework that utilizes a large amount of unlabeled samples with data augmentation. In order to leverage unlabeled samples for reward learning, we infer pseudo-labels of the unlabeled samples based on the confidence of the preference predictor. To further improve the label-efficiency of reward learning, we introduce a new data augmentation that temporally crops consecutive subsequences from the original behaviors. Our experiments demonstrate that our approach significantly improves the feedback-efficiency of the state-of-the-art preference-based method on a variety of locomotion and robotic manipulation tasks.",
        "pdf_link": "https://openreview.net/pdf/2fe39fd19cc40472a98221890fb4cfd5f924e6c7.pdf",
        "forum_url": "https://openreview.net/forum?id=TfhfZLQ2EJO",
        "source": "iclr2022"
    },
    {
        "title": "Exploring extreme parameter compression for pre-trained language models",
        "authors": [
            "Benyou Wang",
            "Yuxin Ren",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu"
        ],
        "published": "iclr 2022 poster",
        "summary": "Recent work explored the potential of large-scale Transformer-based pre-trained models, especially Pre-trained Language Models (PLMs) in natural language processing. This raises many concerns from various perspectives, e.g.,  financial costs and carbon emissions. \nCompressing PLMs like BERT with negligible performance loss for faster inference and cheaper deployment has attracted much attention. In this work, we aim to explore larger compression ratios for PLMs, among which tensor decomposition is a potential but under-investigated one. By comparing existing decomposition methods, Tucker decomposition is found to be parameter-efficient for compression.  Two decomposition and reconstruction protocols are further proposed to improve the effectiveness and efficiency of Tucker decomposition in parameter compression.\nOur compressed BERT with ${1}/{7}$ parameters in Transformer layers performs on-par with,  sometimes slightly better than the original BERT in GLUE benchmark. A tiny version achieves  96.7\\%  performance of  BERT-base with $ {1}/{48} $ encoder parameters (i.e., less than 2M parameters excluding the embedding layer) and  \\textbf{$2.7 \\times$} faster on inference. To show that the proposed method is orthogonal to existing compression methods like knowledge distillation, we also explore the benefit of the proposed method on a distilled BERT. ",
        "pdf_link": "https://openreview.net/pdf/dab5dd8e405bdd89ffafedef9f081622e45d0c61.pdf",
        "forum_url": "https://openreview.net/forum?id=RftryyYyjiG",
        "source": "iclr2022"
    },
    {
        "title": "An Explanation of In-context Learning as Implicit Bayesian Inference",
        "authors": [
            "Sang Michael Xie",
            "Aditi Raghunathan",
            "Percy Liang",
            "Tengyu Ma"
        ],
        "published": "iclr 2022 poster",
        "summary": "Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.",
        "pdf_link": "https://openreview.net/pdf/34aa1a37f7afecc48d15dbab476f32a40db2fe1a.pdf",
        "forum_url": "https://openreview.net/forum?id=RdJVFCHjUMI",
        "source": "iclr2022"
    },
    {
        "title": "THOMAS: Trajectory Heatmap Output with learned Multi-Agent Sampling",
        "authors": [
            "Thomas Gilles",
            "Stefano Sabatini",
            "Dzmitry Tsishkou",
            "Bogdan Stanciulescu",
            "Fabien Moutarde"
        ],
        "published": "iclr 2022 poster",
        "summary": "In this paper, we propose THOMAS, a joint multi-agent trajectory prediction framework allowing for an efficient and consistent prediction of multi-agent multi-modal trajectories. We present a unified model architecture for simultaneous agent future heatmap estimation, in which we leverage hierarchical and sparse image generation for fast and memory-efficient inference. We propose a learnable trajectory recombination model that takes as input a set of predicted trajectories for each agent and outputs its consistent reordered recombination. This recombination module is able to realign the initially independent modalities so that they do no collide and are coherent with each other.  We report our results on the Interaction multi-agent prediction challenge and rank $1^{st}$ on the online test leaderboard.",
        "pdf_link": "https://openreview.net/pdf/a8ce9facf1e0dfc642c02f9849f5b7910589efad.pdf",
        "forum_url": "https://openreview.net/forum?id=QDdJhACYrlX",
        "source": "iclr2022"
    },
    {
        "title": "Bandit Learning with Joint Effect of Incentivized Sampling, Delayed Sampling Feedback, and Self-Reinforcing User Preferences",
        "authors": [
            "Tianchen Zhou",
            "Jia Liu",
            "Chaosheng Dong",
            "Yi Sun"
        ],
        "published": "iclr 2022 poster",
        "summary": "In this paper, we consider a new multi-armed bandit (MAB) framework motivated by three common complications in online recommender systems in practice: (i) the platform (learning agent) cannot sample an intended product directly and has to incentivize customers to select this product (e.g., promotions and coupons); (ii) customer feedbacks are often received later than their selection times; and (iii) customer preferences among products are influenced and reinforced by historical feedbacks. From the platform's perspective, the goal of the MAB framework is to maximize total reward without incurring excessive incentive costs. A major challenge of this MAB framework is that the loss of information caused by feedback delay complicates both user preference evolution and arm incentivizing decisions, both of which are already highly non-trivial even by themselves. Toward this end, we first propose a policy called ``UCB-Filtering-with-Delayed-Feedback'' (UCB-FDF) policy for this new MAB framework. In our analysis, we consider delayed feedbacks that can have either arm-independent or arm-dependent distributions. In both cases, we allow unbounded support for the random delays, i.e., the random delay can be infinite. We show that the delay impacts in both cases can still be upper bounded by an additive penalty on both the regret and total incentive costs. This further implies that logarithmic regret and incentive cost growth rates are achievable under this new MAB framework. Experimental results corroborate our theoretical analysis on both regret and incentive costs.\n",
        "pdf_link": "https://openreview.net/pdf/d654e6eeca4a1faa806d427c0b60d31e7648ad5f.pdf",
        "forum_url": "https://openreview.net/forum?id=Q83vFlie_Pr",
        "source": "iclr2022"
    },
    {
        "title": "Differentially Private Fine-tuning of Language Models",
        "authors": [
            "Da Yu",
            "Saurabh Naik",
            "Arturs Backurs",
            "Sivakanth Gopi",
            "Huseyin A Inan",
            "Gautam Kamath",
            "Janardhan Kulkarni",
            "Yin Tat Lee",
            "Andre Manoel",
            "Lukas Wutschitz",
            "Sergey Yekhanin",
            "Huishuai Zhang"
        ],
        "published": "iclr 2022 poster",
        "summary": "We give simpler, sparser, and faster algorithms for differentially private fine-tuning of large-scale pre-trained language models, which achieve the state-of-the-art privacy versus utility tradeoffs on many standard NLP tasks. We propose a meta-framework for this problem, inspired by the recent success of highly parameter-efficient methods for fine-tuning. Our experiments show that differentially private adaptations of these approaches outperform previous private algorithms in three important dimensions: utility, privacy, and the computational and memory cost of private training. On many commonly studied datasets, the utility of private models approaches that of non-private models. For example, on the MNLI dataset we achieve an accuracy of $87.8\\%$ using RoBERTa-Large and $83.5\\%$ using RoBERTa-Base with a privacy budget of $\\epsilon = 6.7$. In comparison, absent privacy constraints, RoBERTa-Large achieves an accuracy of $90.2\\%$. Our findings are similar for natural language generation when privately fine-tuning GPT-2. Our experiments also show that larger models are better suited for private fine-tuning: while they are well known to achieve superior accuracy non-privately, we find that they also better maintain their accuracy when privacy is introduced.",
        "pdf_link": "https://openreview.net/pdf/a7f73a09ee1b6071d23daf0142ff02e4926db6e9.pdf",
        "forum_url": "https://openreview.net/forum?id=Q42f0dfjECO",
        "source": "iclr2022"
    },
    {
        "title": "Maximum Entropy RL (Provably) Solves Some Robust RL Problems",
        "authors": [
            "Benjamin Eysenbach",
            "Sergey Levine"
        ],
        "published": "iclr 2022 poster",
        "summary": "Many potential applications of reinforcement learning (RL) require guarantees that the agent will perform well in the face of disturbances to the dynamics or reward function. In this paper, we prove theoretically that maximum entropy (MaxEnt) RL maximizes a lower bound on a robust RL objective, and thus can be used to learn policies that are robust to some disturbances in the dynamics and the reward function. While this capability of MaxEnt RL has been observed empirically in prior work, to the best of our knowledge our work provides the first rigorous proof and theoretical characterization of the MaxEnt RL robust set. While a number of prior robust RL algorithms have been designed to handle similar disturbances to the reward function or dynamics, these methods typically require additional moving parts and hyperparameters on top of a base RL algorithm. In contrast, our results suggest that MaxEnt RL by itself is robust to certain disturbances, without requiring any additional modifications. While this does not imply that MaxEnt RL is the best available robust RL method, MaxEnt RL is a simple robust RL method with appealing formal guarantees.",
        "pdf_link": "https://openreview.net/pdf/3fb82fdb60ea0802001d004d0b6951f5226c3ff0.pdf",
        "forum_url": "https://openreview.net/forum?id=PtSAD3caaA2",
        "source": "iclr2022"
    },
    {
        "title": "CoMPS: Continual Meta Policy Search",
        "authors": [
            "Glen Berseth",
            "Zhiwei Zhang",
            "Grace Zhang",
            "Chelsea Finn",
            "Sergey Levine"
        ],
        "published": "iclr 2022 poster",
        "summary": "We develop a new continual meta-learning method to address challenges in sequential multi-task learning. In this setting, the agent's goal is to achieve high reward over any sequence of tasks quickly. Prior meta-reinforcement learning algorithms have demonstrated promising results in accelerating the acquisition of new tasks. However, they require access to all tasks during training. Beyond simply transferring past experience to new tasks, our goal is to devise continual reinforcement learning algorithms that learn to learn, using their experience on previous tasks to learn new tasks more quickly. We introduce a new method, continual meta-policy search (CoMPS), that removes this limitation by meta-training in an incremental fashion, over each task in a sequence, without revisiting prior tasks. CoMPS continuously repeats two subroutines: learning a new task using RL and using the experience from RL to perform completely offline meta-learning to prepare for subsequent task learning. We find that CoMPS outperforms prior continual learning and off-policy meta-reinforcement methods on several sequences of challenging continuous control tasks.",
        "pdf_link": "https://openreview.net/pdf/3ca19a998e5306a18adb13bd58c7b521611ff1f2.pdf",
        "forum_url": "https://openreview.net/forum?id=PVJ6j87gOHz",
        "source": "iclr2022"
    },
    {
        "title": "You Mostly Walk Alone: Analyzing Feature Attribution in Trajectory Prediction",
        "authors": [
            "Osama Makansi",
            "Julius Von Kügelgen",
            "Francesco Locatello",
            "Peter Vincent Gehler",
            "Dominik Janzing",
            "Thomas Brox",
            "Bernhard Schölkopf"
        ],
        "published": "iclr 2022 poster",
        "summary": "Predicting the future trajectory of a moving agent can be easy when the past trajectory continues smoothly but is challenging when complex interactions with other agents are involved. Recent deep learning approaches for trajectory prediction show promising performance and partially attribute this to successful reasoning about agent-agent interactions.  However, it remains unclear which features such black-box models actually learn to use for making predictions. This paper proposes a procedure that quantifies the contributions of different cues to model performance based on a variant of Shapley values. Applying this procedure to state-of-the-art trajectory prediction methods on standard benchmark datasets shows that they are, in fact, unable to reason about interactions. Instead, the past trajectory of the target is the only feature used for predicting its future. For a task with richer social interaction patterns, on the other hand, the tested models do pick up such interactions to a certain extent, as quantified by our feature attribution method. We discuss the limits of the proposed method and its links to causality.",
        "pdf_link": "https://openreview.net/pdf/c2255d7b6f3e4ca52c2e54c74469ac86211d02ca.pdf",
        "forum_url": "https://openreview.net/forum?id=POxF-LEqnF",
        "source": "iclr2022"
    },
    {
        "title": "HTLM: Hyper-Text Pre-Training and Prompting of Language Models",
        "authors": [
            "Armen Aghajanyan",
            "Dmytro Okhonko",
            "Mike Lewis",
            "Mandar Joshi",
            "Hu Xu",
            "Gargi Ghosh",
            "Luke Zettlemoyer"
        ],
        "published": "iclr 2022 poster",
        "summary": "We introduce HTLM, a hyper-text language model trained on a large-scale web crawl. Modeling hyper-text has a number of advantages: (1) it is easily gathered at scale, (2) it provides rich document-level and end-task-adjacent supervision (e.g. 'class' and 'id' attributes often encode document category information), and (3) it allows for new structured prompting that follows the established semantics of HTML (e.g. to do zero-shot summarization by infilling '<title>' tags for a webpage that contains the input text).  We show that pretraining with a BART-style denoising loss directly on simplified HTML provides highly effective transfer for a wide range of end tasks and supervision levels. HTLM matches or exceeds the performance of comparably sized text-only LMs for zero-shot prompting and fine-tuning for classification benchmarks, while also setting new state-of-the-art performance levels for zero-shot summarization. We also find that hyper-text prompts provide more value to HTLM, in terms of data efficiency, than plain text prompts do for existing LMs, and that HTLM is highly effective at auto-prompting itself, by simply generating the most likely hyper-text formatting for any available training data. We will release all code and models to support future HTLM research. ",
        "pdf_link": "https://openreview.net/pdf/565914339be7ddb2453523852e836e1fe3ce3b8b.pdf",
        "forum_url": "https://openreview.net/forum?id=P-pPW1nxf1r",
        "source": "iclr2022"
    },
    {
        "title": "Task-Induced Representation Learning",
        "authors": [
            "Jun Yamada",
            "Karl Pertsch",
            "Anisha Gunjal",
            "Joseph J Lim"
        ],
        "published": "iclr 2022 poster",
        "summary": "In this work, we evaluate the effectiveness of representation learning approaches for decision making in visually complex environments. Representation learning is essential for effective reinforcement learning (RL) from high-dimensional in- puts. Unsupervised representation learning approaches based on reconstruction, prediction or contrastive learning have shown substantial learning efficiency gains. Yet, they have mostly been evaluated in clean laboratory or simulated settings. In contrast, real environments are visually complex and contain substantial amounts of clutter and distractors. Unsupervised representations will learn to model such distractors, potentially impairing the agent’s learning efficiency. In contrast, an alternative class of approaches, which we call task-induced representation learning, leverages task information such as rewards or demonstrations from prior tasks to focus on task-relevant parts of the scene and ignore distractors. We investi- gate the effectiveness of unsupervised and task-induced representation learning approaches on four visually complex environments, from Distracting DMControl to the CARLA driving simulator. For both, RL and imitation learning, we find that representation learning generally improves sample efficiency on unseen tasks even in visually complex scenes and that task-induced representations can double learning efficiency compared to unsupervised alternatives.",
        "pdf_link": "https://openreview.net/pdf/6ffc4d2e51676a89b8a8dc75c13ac8b37bc4d6ad.pdf",
        "forum_url": "https://openreview.net/forum?id=OzyXtIZAzFv",
        "source": "iclr2022"
    },
    {
        "title": "Pareto Policy Pool for Model-based Offline Reinforcement Learning",
        "authors": [
            "Yijun Yang",
            "Jing Jiang",
            "Tianyi Zhou",
            "Jie Ma",
            "Yuhui Shi"
        ],
        "published": "iclr 2022 poster",
        "summary": "Online reinforcement learning (RL) can suffer from poor exploration, sparse reward, insufficient data, and overhead caused by inefficient interactions between an immature policy and a complicated environment. Model-based offline RL instead trains an environment model using a dataset of pre-collected experiences so online RL methods can learn in an offline manner by solely interacting with the model. However, the uncertainty and accuracy of the environment model can drastically vary across different state-action pairs so the RL agent may achieve high model return but perform poorly in the true environment. Unlike previous works that need to carefully tune the trade-off between the model return and uncertainty in a single objective, we study a bi-objective formulation for model-based offline RL that aims at producing a pool of diverse policies on the Pareto front performing different levels of trade-offs, which provides the flexibility to select the best policy for each realistic environment from the pool. Our method, ''Pareto policy pool (P3)'', does not need to tune the trade-off weight but can produce policies allocated at different regions of the Pareto front. For this purpose, we develop an efficient algorithm that solves multiple bi-objective optimization problems with distinct constraints defined by reference vectors targeting diverse regions of the Pareto front. We theoretically prove that our algorithm can converge to the targeted regions. In order to obtain more Pareto optimal policies without linearly increasing the cost, we leverage the achieved policies as initialization to find more Pareto optimal policies in their neighborhoods. On the D4RL benchmark for offline RL, P3 substantially outperforms several recent baseline methods over multiple tasks, especially when the quality of pre-collected experiences is low.",
        "pdf_link": "https://openreview.net/pdf/811177d23b2117fa0be0cc22952e7c1e3325bf59.pdf",
        "forum_url": "https://openreview.net/forum?id=OqcZu8JIIzS",
        "source": "iclr2022"
    },
    {
        "title": "Contextualized Scene Imagination for Generative Commonsense Reasoning",
        "authors": [
            "PeiFeng Wang",
            "Jonathan Zamora",
            "Junfeng Liu",
            "Filip Ilievski",
            "Muhao Chen",
            "Xiang Ren"
        ],
        "published": "iclr 2022 poster",
        "summary": "Humans use natural language to compose common concepts from their environment into plausible, day-to-day scene descriptions. However, such generative commonsense reasoning (GCSR) skills are lacking in state-of-the-art text generation methods. Descriptive sentences about arbitrary concepts generated by neural text generation models (e.g., pre-trained text-to-text Transformers) are often grammatically fluent but may not correspond to human common sense, largely due to their lack of mechanisms to capture concept relations, to identify implicit concepts, and to perform generalizable reasoning about unseen concept compositions. In this paper, we propose an Imagine-and-Verbalize (I\\&V) method, which learns to imagine a relational scene knowledge graph (SKG) with relations between the input concepts, and leverage the SKG as a constraint when generating a plausible scene description. We collect and harmonize a set of knowledge resources from different domains and modalities, providing a rich auxiliary supervision signal for I\\&V. The experiments demonstrate the effectiveness of I\\&V in improving language models on both concept-to-sentence and concept-to-story generation tasks, while enabling the model to learn well from fewer task examples and generate SKGs that make common sense to human annotators.",
        "pdf_link": "https://openreview.net/pdf/a66e1b12b2211131a44463611c8c272c21decbfb.pdf",
        "forum_url": "https://openreview.net/forum?id=Oh1r2wApbPv",
        "source": "iclr2022"
    },
    {
        "title": "Topological Experience Replay",
        "authors": [
            "Zhang-Wei Hong",
            "Tao Chen",
            "Yen-Chen Lin",
            "Joni Pajarinen",
            "Pulkit Agrawal"
        ],
        "published": "iclr 2022 poster",
        "summary": "State-of-the-art deep Q-learning methods update Q-values using state transition tuples sampled from the experience replay buffer. This strategy often randomly samples or prioritizes data sampling based on measures such as the temporal difference (TD) error. Such sampling strategies can be inefficient at learning Q-function since a state's correct Q-value preconditions on the accurate successor states' Q-value. Disregarding such a successor's value dependency leads to useless updates and even learning wrong values.\nTo expedite Q-learning, we maintain states' dependency by organizing the agent's experience into a graph. Each edge in the graph represents a transition between two connected states. We perform value backups via a breadth-first search that expands vertices in the graph starting from the set of terminal states successively moving backward. We empirically show that our method is substantially more data-efficient than several baselines on a diverse range of goal-reaching tasks. Notably, the proposed method also outperforms baselines that consume more batches of training experience. ",
        "pdf_link": "https://openreview.net/pdf/67478db1775a09947be98e8721cebb8b1e453692.pdf",
        "forum_url": "https://openreview.net/forum?id=OXRZeMmOI7a",
        "source": "iclr2022"
    },
    {
        "title": "IFR-Explore: Learning Inter-object Functional Relationships in 3D Indoor Scenes",
        "authors": [
            "QI LI",
            "Kaichun Mo",
            "Yanchao Yang",
            "Hang Zhao",
            "Leonidas Guibas"
        ],
        "published": "iclr 2022 poster",
        "summary": "Building embodied intelligent agents that can interact with 3D indoor environments has received increasing research attention in recent years. While most works focus on single-object or agent-object visual functionality and affordances, our work proposes to study a novel, underexplored, kind of visual relations that is also important to perceive and model -- inter-object functional relationships (e.g., a switch on the wall turns on or off the light, a remote control operates the TV). Humans often spend no effort or only a little to infer these relationships, even when entering a new room, by using our strong prior knowledge (e.g., we know that buttons control electrical devices) or using only a few exploratory interactions in cases of uncertainty (e.g., multiple switches and lights in the same room). In this paper, we take the first step in building AI system learning inter-object functional relationships in 3D indoor environments with key technical contributions of modeling prior knowledge by training over large-scale scenes and designing interactive policies for effectively exploring the training scenes and quickly adapting to novel test scenes. We create a new dataset based on the AI2Thor and PartNet datasets and perform extensive experiments that prove the effectiveness of our proposed method.",
        "pdf_link": "https://openreview.net/pdf/683c0979083ae6f6094f9293b8fdc7408ff7b1c6.pdf",
        "forum_url": "https://openreview.net/forum?id=OT3mLgR8Wg8",
        "source": "iclr2022"
    },
    {
        "title": "TAPEX: Table Pre-training via Learning a Neural SQL Executor",
        "authors": [
            "Qian Liu",
            "Bei Chen",
            "Jiaqi Guo",
            "Morteza Ziyadi",
            "Zeqi Lin",
            "Weizhu Chen",
            "Jian-Guang Lou"
        ],
        "published": "iclr 2022 poster",
        "summary": "Recent progress in language model pre-training has achieved a great success via leveraging large-scale unstructured textual data. However, it is still a challenge to apply pre-training on structured tabular data due to the absence of large-scale high-quality tabular data. In this paper, we propose TAPEX to show that table pre-training can be achieved by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries and their execution outputs. TAPEX addresses the data scarcity challenge via guiding the language model to mimic a SQL executor on the diverse, large-scale and high-quality synthetic corpus. We evaluate TAPEX on four benchmark datasets. Experimental results demonstrate that TAPEX outperforms previous table pre-training approaches by a large margin and achieves new state-of-the-art results on all of them. This includes the improvements on the weakly-supervised WikiSQL denotation accuracy to 89.5% (+2.3%), the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the SQA denotation accuracy to 74.5% (+3.5%), and the TabFact accuracy to 84.2% (+3.2%). To our knowledge, this is the first work to exploit table pre-training via synthetic executable programs and to achieve new state-of-the-art results on various downstream tasks. Our code can be found at https://github.com/microsoft/Table-Pretraining.",
        "pdf_link": "https://openreview.net/pdf/9abc11a326d0ad12abb958697c1ab8e0a585e62b.pdf",
        "forum_url": "https://openreview.net/forum?id=O50443AsCP",
        "source": "iclr2022"
    },
    {
        "title": "Sound Adversarial Audio-Visual Navigation",
        "authors": [
            "Yinfeng Yu",
            "Wenbing Huang",
            "Fuchun Sun",
            "Changan Chen",
            "Yikai Wang",
            "Xiaohong Liu"
        ],
        "published": "iclr 2022 poster",
        "summary": "Audio-visual navigation task requires an agent to find a sound source in a realistic, unmapped 3D environment by utilizing egocentric audio-visual observations. Existing audio-visual navigation works assume a clean environment that solely contains the target sound, which, however, would not be suitable in most real-world applications due to the unexpected sound noise or intentional interference. In this work, we design an acoustically complex environment in which, besides the target sound, there exists a sound attacker playing a zero-sum game with the agent. More specifically, the attacker can move and change the volume and category of the sound to make the agent suffer from finding the sounding object while the agent tries to dodge the attack and navigate to the goal under the intervention. Under certain constraints to the attacker, we can improve the robustness of the agent towards unexpected sound attacks in audio-visual navigation. For better convergence, we develop a joint training mechanism by employing the property of a centralized critic with decentralized actors. Experiments on two real-world 3D scan datasets, Replica, and Matterport3D, verify the effectiveness and the robustness of the agent trained under our designed environment when transferred to the clean environment or the one containing sound attackers with random policy. Project: https://yyf17.github.io/SAAVN .",
        "pdf_link": "https://openreview.net/pdf/892cdd541646cc28a0880494951fbd89079c2a3d.pdf",
        "forum_url": "https://openreview.net/forum?id=NkZq4OEYN-",
        "source": "iclr2022"
    },
    {
        "title": "Normalization of Language Embeddings for Cross-Lingual Alignment",
        "authors": [
            "Prince Osei Aboagye",
            "Yan Zheng",
            "Chin-Chia Michael Yeh",
            "Junpeng Wang",
            "Wei Zhang",
            "Liang Wang",
            "Hao Yang",
            "Jeff Phillips"
        ],
        "published": "iclr 2022 poster",
        "summary": "Learning a good transfer function to map the word vectors from two languages into a shared cross-lingual word vector space plays a crucial role in cross-lingual NLP. It is useful in translation tasks and important in allowing complex models built on a high-resource language like English to be directly applied on an aligned low resource language.  While Procrustes and other techniques can align language models with some success, it has recently been identified that structural differences (for instance, due to differing word frequency) create different profiles for various monolingual embedding. When these profiles differ across languages, it correlates with how well languages can align and their performance on cross-lingual downstream tasks.  In this work, we develop a very general language embedding normalization procedure, building and subsuming various previous approaches, which removes these structural profiles across languages without destroying their intrinsic meaning.  We demonstrate that meaning is retained and alignment is improved on similarity, translation, and cross-language classification tasks.  Our proposed normalization clearly outperforms all prior approaches like centering and vector normalization on each task and with each alignment approach. ",
        "pdf_link": "https://openreview.net/pdf/1cdf6da2d049db967f45c6454ba03f40aa1e3849.pdf",
        "forum_url": "https://openreview.net/forum?id=Nh7CtbyoqV5",
        "source": "iclr2022"
    },
    {
        "title": "BAM: Bayes with Adaptive Memory",
        "authors": [
            "Josue Nassar",
            "Jennifer Rogers Brennan",
            "Ben Evans",
            "Kendall Lowrey"
        ],
        "published": "iclr 2022 poster",
        "summary": "Online learning via Bayes' theorem allows new data to be continuously integrated into an agent's current beliefs. However, a naive application of Bayesian methods in non-stationary environments leads to slow adaptation and results in state estimates that may converge confidently to the wrong parameter value. A common solution when learning in changing environments is to discard/downweight past data; however, this simple mechanism of \"forgetting\" fails to account for the fact that many real-world environments involve revisiting similar states. We propose a new framework, Bayes with Adaptive Memory (BAM), that takes advantage of past experience by allowing the agent to choose which past observations to remember and which to forget. We demonstrate that BAM generalizes many popular Bayesian update rules for non-stationary environments. Through a variety of experiments, we demonstrate the ability of BAM to continuously adapt in an ever-changing world.",
        "pdf_link": "https://openreview.net/pdf/570533db54609ccaa349705c913de9ad4439ee1d.pdf",
        "forum_url": "https://openreview.net/forum?id=NdOoQnYPj_",
        "source": "iclr2022"
    },
    {
        "title": "Synchromesh: Reliable Code Generation from Pre-trained Language Models",
        "authors": [
            "Gabriel Poesia",
            "Alex Polozov",
            "Vu Le",
            "Ashish Tiwari",
            "Gustavo Soares",
            "Christopher Meek",
            "Sumit Gulwani"
        ],
        "published": "iclr 2022 poster",
        "summary": "Large pre-trained language models have been used to generate code, providing a flexible interface for synthesizing programs from natural language specifications. However, they often violate syntactic and semantic rules of their output language, limiting their practical usability. In this paper, we propose Synchromesh: a framework for substantially improving the reliability of pre-trained models for code generation. Synchromesh comprises two components. First, it retrieves few-shot examples from a training bank using Target Similarity Tuning (TST), a novel method for semantic example selection. TST learns to recognize utterances that describe similar target programs despite of differences in surface natural language features. Then, Synchromesh feeds the examples to a pre-trained language model and samples programs using Constrained Semantic Decoding (CSD): a general framework for constraining the output to a set of valid programs in the target language. CSD leverages constraints on partial outputs to sample complete correct programs, and needs neither re-training nor fine-tuning of the language model. We evaluate our methods by synthesizing code from natural language descriptions using GPT-3 and Codex in three real-world languages: SQL queries, Vega-Lite visualizations and SMCalFlow programs. These domains showcase rich constraints that CSD is able to enforce, including syntax, scoping and typing rules. Across all languages, we observe complementary gains from CSD and TST in prediction accuracy and in effectively preventing parsing, type and run-time errors.",
        "pdf_link": "https://openreview.net/pdf/6ff098333e70afec46ebe0a90baf01256cacc43c.pdf",
        "forum_url": "https://openreview.net/forum?id=KmtVD97J43e",
        "source": "iclr2022"
    },
    {
        "title": "Node Feature Extraction by Self-Supervised Multi-scale Neighborhood Prediction",
        "authors": [
            "Eli Chien",
            "Wei-Cheng Chang",
            "Cho-Jui Hsieh",
            "Hsiang-Fu Yu",
            "Jiong Zhang",
            "Olgica Milenkovic",
            "Inderjit S Dhillon"
        ],
        "published": "iclr 2022 poster",
        "summary": "Learning on graphs has attracted significant attention in the learning community due to numerous real-world applications. In particular, graph neural networks (GNNs), which take \\emph{numerical} node features and graph structure as inputs, have been shown to achieve state-of-the-art performance on various graph-related learning tasks. Recent works exploring the correlation between numerical node features and graph structure via self-supervised learning have paved the way for further performance improvements of GNNs. However, methods used for extracting numerical node features from \\emph{raw data} are still \\emph{graph-agnostic} within standard GNN pipelines. This practice is sub-optimal as it prevents one from fully utilizing potential correlations between graph topology and node attributes. To mitigate this issue, we propose a new self-supervised learning framework, Graph Information Aided Node feature exTraction (GIANT). GIANT makes use of the eXtreme Multi-label Classification (XMC) formalism, which is crucial for fine-tuning the language model based on graph information, and scales to large datasets. We also provide a theoretical analysis that justifies the use of XMC over link prediction and motivates integrating XR-Transformers, a powerful method for solving XMC problems, into the GIANT framework. We demonstrate the superior performance of GIANT over the standard GNN pipeline on Open Graph Benchmark datasets: For example, we improve the accuracy of the top-ranked method GAMLP from $68.25\\%$ to $69.67\\%$, SGC from $63.29\\%$ to $66.10\\%$ and MLP from $47.24\\%$ to $61.10\\%$ on the ogbn-papers100M dataset by leveraging GIANT.",
        "pdf_link": "https://openreview.net/pdf/431334fb15f28e23e02e4a1cd1513fef6cacd2b0.pdf",
        "forum_url": "https://openreview.net/forum?id=KJggliHbs8",
        "source": "iclr2022"
    },
    {
        "title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models",
        "authors": [
            "Alexander Pan",
            "Kush Bhatia",
            "Jacob Steinhardt"
        ],
        "published": "iclr 2022 poster",
        "summary": "Reward hacking---where RL agents exploit gaps in misspecified proxy rewards---has been widely observed, but not yet systematically studied. To understand reward hacking, we construct four RL environments with different misspecified rewards. We investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, and observation space noise. Typically, more capable agents are able to better exploit reward misspecifications, causing them to attain higher proxy reward and lower true reward. Moreover, we find instances of \\emph{phase transitions}: capability thresholds at which the agent's behavior qualitatively shifts, leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To encourage further research on reward misspecification, address this, we propose an anomaly detection task for aberrant policies and offer several baseline detectors.",
        "pdf_link": "https://openreview.net/pdf/772b3ddc867fd13a9c98fb15c99f94d5b68ae558.pdf",
        "forum_url": "https://openreview.net/forum?id=JYtwGwIL7ye",
        "source": "iclr2022"
    },
    {
        "title": "Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep RL",
        "authors": [
            "Yanchao Sun",
            "Ruijie Zheng",
            "Yongyuan Liang",
            "Furong Huang"
        ],
        "published": "iclr 2022 poster",
        "summary": "Evaluating the worst-case performance of a reinforcement learning (RL) agent under the strongest/optimal adversarial perturbations on state observations (within some constraints) is crucial for understanding the robustness of RL agents. However, finding the optimal adversary is challenging, in terms of both whether we can find the optimal attack and how efficiently we can find it. Existing works on adversarial RL either use heuristics-based methods that may not find the strongest adversary, or directly train an RL-based adversary by treating the agent as a part of the environment, which can find the optimal adversary but may become intractable in a large state space. \nThis paper introduces a novel attacking method to find the optimal attacks through collaboration between a designed function named \"actor\" and an RL-based learner named \"director'\". The actor crafts state perturbations for a given policy perturbation direction, and the director learns to propose the best policy perturbation directions. Our proposed algorithm, PA-AD, is theoretically optimal and significantly more efficient than prior RL-based works in environments with large state spaces. Empirical results show that our proposed PA-AD universally outperforms state-of-the-art attacking methods in various Atari and MuJoCo environments. By applying PA-AD to adversarial training, we achieve state-of-the-art empirical robustness in multiple tasks under strong adversaries.",
        "pdf_link": "https://openreview.net/pdf/b11335ea1d1d4ca95531723261e11735e0550bc4.pdf",
        "forum_url": "https://openreview.net/forum?id=JM2kFbJvvI",
        "source": "iclr2022"
    },
    {
        "title": "A First-Occupancy Representation for Reinforcement Learning",
        "authors": [
            "Ted Moskovitz",
            "Spencer R Wilson",
            "Maneesh Sahani"
        ],
        "published": "iclr 2022 poster",
        "summary": "Both animals and artificial agents benefit from state representations that support rapid transfer of learning across tasks and which enable them to efficiently traverse their environments to reach rewarding states.  The successor representation (SR), which measures the expected cumulative, discounted state occupancy under a fixed policy, enables efficient transfer to different reward structures in an otherwise constant Markovian environment and has been hypothesized to underlie aspects of biological behavior and neural activity.  However, in the real world, rewards may only be available for consumption once, may shift location, or agents may simply aim to reach goal states as rapidly as possible without the constraint of artificially imposed task horizons. In such cases, the most behaviorally-relevant representation would carry information about when the agent was likely to first reach states of interest, rather than how often it should expect to visit them over a potentially infinite time span.  To reflect such demands, we introduce the first-occupancy representation (FR), which measures the expected temporal discount to the first time a state is accessed.  We demonstrate that the FR facilitates exploration, the selection of efficient paths to desired states, allows the agent, under certain conditions, to plan provably optimal trajectories defined by a sequence of subgoals, and induces similar behavior to animals avoiding threatening stimuli.",
        "pdf_link": "https://openreview.net/pdf/46abdff2d131f44012d855cdd93c0fa7034d601a.pdf",
        "forum_url": "https://openreview.net/forum?id=JBAZe2yN6Ub",
        "source": "iclr2022"
    },
    {
        "title": "Improving Non-Autoregressive Translation Models Without Distillation",
        "authors": [
            "Xiao Shi Huang",
            "Felipe Perez",
            "Maksims Volkovs"
        ],
        "published": "iclr 2022 poster",
        "summary": "Transformer-based autoregressive (AR) machine translation models have achieved significant performance improvements, nearing human-level accuracy on some languages. The AR framework translates one token at a time which can be time consuming, especially for long sequences. To accelerate inference, recent work has been exploring non-autoregressive (NAR) approaches that translate blocks of tokens in parallel. Despite significant progress, leading NAR models still lag behind their AR counterparts, and only become competitive when trained with distillation. In this paper we investigate possible reasons behind this performance gap, namely, the indistinguishability of tokens, and mismatch between training and inference. We then propose the Conditional Masked Language Model with Correction (CMLMC) that addresses these problems. Empirically, we show that CMLMC achieves state-of-the-art NAR performance when trained on raw data without distillation and approaches AR performance on multiple datasets. Full code for this work will be released at the time of publication.",
        "pdf_link": "https://openreview.net/pdf/fe5e18c9939f10295c39693c81d77b03816cad63.pdf",
        "forum_url": "https://openreview.net/forum?id=I2Hw58KHp8O",
        "source": "iclr2022"
    },
    {
        "title": "Multi-Agent MDP Homomorphic Networks",
        "authors": [
            "Elise van der Pol",
            "Herke van Hoof",
            "Frans A Oliehoek",
            "Max Welling"
        ],
        "published": "iclr 2022 poster",
        "summary": "This paper introduces Multi-Agent MDP Homomorphic Networks, a class of networks that allows distributed execution using only local information, yet is able to share experience between global symmetries in the joint state-action space of cooperative multi-agent systems. In cooperative multi-agent systems, complex symmetries arise between different configurations of the agents and their local observations. For example, consider a group of agents navigating: rotating the state globally results in a permutation of the optimal joint policy. Existing work on symmetries in single agent reinforcement learning can only be generalized to the fully centralized setting, because such approaches rely on the global symmetry in the full state-action spaces, and these can result in correspondences across agents. To encode such symmetries while still allowing distributed execution we propose a factorization that decomposes global symmetries into local transformations. Our proposed factorization allows for distributing the computation that enforces global symmetries over local agents and local interactions. We introduce a multi-agent equivariant policy network based on this factorization. We show empirically on symmetric multi-agent problems that globally symmetric distributable policies improve data efficiency compared to non-equivariant baselines.",
        "pdf_link": "https://openreview.net/pdf/3a8f28592a8f20859b54c37f57cb659f7b0664fa.pdf",
        "forum_url": "https://openreview.net/forum?id=H7HDG--DJF0",
        "source": "iclr2022"
    },
    {
        "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision",
        "authors": [
            "Zirui Wang",
            "Jiahui Yu",
            "Adams Wei Yu",
            "Zihang Dai",
            "Yulia Tsvetkov",
            "Yuan Cao"
        ],
        "published": "iclr 2022 poster",
        "summary": "With recent progress in joint modeling of visual and textual representations, Vision-Language Pretraining (VLP) has achieved impressive performance on many multimodal downstream tasks. However, the requirement for expensive annotations including clean image captions and regional labels limits the scalability of existing approaches, and complicates the pretraining procedure with the introduction of multiple dataset-specific objectives. In this work, we relax these constraints and present a minimalist pretraining framework, named Simple Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training complexity by exploiting large-scale weak supervision, and is trained end-to-end with a single prefix language modeling objective. Without utilizing extra data or task-specific customization, the resulting model significantly outperforms previous pretraining methods and achieves new state-of-the-art results on a wide range of discriminative and generative vision-language benchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE (+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score). Furthermore, we demonstrate that SimVLM acquires strong generalization and transfer ability, enabling zero-shot behavior including open-ended visual question answering and cross-modality transfer.",
        "pdf_link": "https://openreview.net/pdf/6212c3fec8ad1418306ef5cebb5e0b76abbc4c26.pdf",
        "forum_url": "https://openreview.net/forum?id=GUrhfTuf_3",
        "source": "iclr2022"
    },
    {
        "title": "Procedural generalization by planning with self-supervised world models",
        "authors": [
            "Ankesh Anand",
            "Jacob C Walker",
            "Yazhe Li",
            "Eszter Vértes",
            "Julian Schrittwieser",
            "Sherjil Ozair",
            "Theophane Weber",
            "Jessica B Hamrick"
        ],
        "published": "iclr 2022 poster",
        "summary": "One of the key promises of model-based reinforcement learning is the ability to generalize using an internal model of the world to make predictions in novel environments and tasks. However, the generalization ability of model-based agents is not well understood because existing work has focused on model-free agents when benchmarking generalization. Here, we explicitly measure the generalization ability of model-based agents in comparison to their model-free counterparts. We focus our analysis on MuZero (Schrittwieser et al., 2020), a powerful model-based agent, and evaluate its performance on both procedural and task generalization. We identify three factors of procedural generalization---planning, self-supervised representation learning, and procedural data diversity---and show that by combining these techniques, we achieve state-of-the art generalization performance and data efficiency on Procgen (Cobbe et al., 2019). However, we find that these factors do not always provide the same benefits for the task generalization benchmarks in Meta-World (Yu et al., 2019), indicating that transfer remains a challenge and may require different approaches than procedural generalization. Overall, we suggest that building generalizable agents requires moving beyond the single-task, model-free paradigm and towards self-supervised model-based agents that are trained in rich, procedural, multi-task environments.",
        "pdf_link": "https://openreview.net/pdf/aff2220dd1aea0f7b326e525fa1ed3f25efe2eb8.pdf",
        "forum_url": "https://openreview.net/forum?id=FmBegXJToY",
        "source": "iclr2022"
    },
    {
        "title": "Wish you were here: Hindsight Goal Selection for long-horizon dexterous manipulation",
        "authors": [
            "Todor Davchev",
            "Oleg Olegovich Sushkov",
            "Jean-Baptiste Regli",
            "Stefan Schaal",
            "Yusuf Aytar",
            "Markus Wulfmeier",
            "Jon Scholz"
        ],
        "published": "iclr 2022 poster",
        "summary": "Complex sequential tasks in continuous-control settings often require agents to successfully traverse a set of ``narrow passages'' in their state space. Solving such tasks with a sparse reward in a sample-efficient manner poses a challenge to modern reinforcement learning (RL) due to the associated long-horizon nature of the problem and the lack of sufficient positive signal during learning. \nVarious tools have been applied to address this challenge. When available, large sets of demonstrations can guide agent exploration. Hindsight relabelling on the other hand does not require additional sources of information. However, existing strategies explore based on task-agnostic goal distributions, which can render the solution of long-horizon tasks impractical. In this work, we extend hindsight relabelling mechanisms to guide exploration along task-specific distributions implied by a small set of successful demonstrations. We evaluate the approach on four complex, single and dual arm, robotics manipulation tasks against strong suitable baselines. The method requires far fewer demonstrations to solve all tasks and achieves a significantly higher overall performance as task complexity increases. Finally, we investigate the robustness of the proposed solution with respect to the quality of input representations and the number of demonstrations.",
        "pdf_link": "https://openreview.net/pdf/524d4c3cacc5ff7803cd7061b33991511fee7db7.pdf",
        "forum_url": "https://openreview.net/forum?id=FKp8-pIRo3y",
        "source": "iclr2022"
    },
    {
        "title": "Causal Contextual Bandits with Targeted Interventions",
        "authors": [
            "Chandrasekar Subramanian",
            "Balaraman Ravindran"
        ],
        "published": "iclr 2022 poster",
        "summary": "We study a contextual bandit setting where the learning agent has the ability to perform interventions on targeted subsets of the population, apart from possessing qualitative causal side-information. This novel formalism captures intricacies in real-world scenarios such as software product experimentation where targeted experiments can be conducted. However, this fundamentally changes the set of options that the agent has, compared to standard contextual bandit settings, necessitating new techniques. This is also the first work that integrates causal side-information in a contextual bandit setting, where the agent aims to learn a policy that maps contexts to arms (as opposed to just identifying one best arm). We propose a new algorithm, which we show empirically performs better than baselines on experiments that use purely synthetic data and on real world-inspired experiments. We also prove a bound on regret that theoretically guards performance.",
        "pdf_link": "https://openreview.net/pdf/b77f8b2b6b86bc81bdb006750510124a95769c01.pdf",
        "forum_url": "https://openreview.net/forum?id=F5Em8ASCosV",
        "source": "iclr2022"
    },
    {
        "title": "Trust Region Policy Optimisation in Multi-Agent Reinforcement Learning",
        "authors": [
            "Jakub Grudzien Kuba",
            "Ruiqing Chen",
            "Muning Wen",
            "Ying Wen",
            "Fanglei Sun",
            "Jun Wang",
            "Yaodong Yang"
        ],
        "published": "iclr 2022 poster",
        "summary": "Trust region methods rigorously enabled reinforcement learning (RL) agents to learn monotonically improving policies, leading to superior performance on a variety of tasks. Unfortunately, when it comes to multi-agent reinforcement learning (MARL),  the property of monotonic improvement may not simply apply; this is because agents, even in cooperative games, could have conflicting directions of policy updates. As a result, achieving a guaranteed improvement on the joint policy where each agent acts individually remains an open challenge. In this paper, we extend the theory of trust region learning to MARL. Central to our findings are the multi-agent advantage decomposition lemma and the sequential policy update scheme. Based on these, we develop Heterogeneous-Agent Trust Region Policy Optimisation (HATPRO) and Heterogeneous-Agent Proximal Policy Optimisation (HAPPO) algorithms. Unlike many existing MARL algorithms, HATRPO/HAPPO do not need agents to share parameters, nor do they need any restrictive assumptions on decomposibility of the joint value function. Most importantly, we justify in theory the monotonic improvement property of HATRPO/HAPPO. We evaluate the proposed methods on a series of Multi-Agent MuJoCo and StarCraftII tasks. Results show that HATRPO and HAPPO significantly outperform strong baselines such as IPPO, MAPPO and MADDPG on all tested tasks, thereby establishing a new state of the art. ",
        "pdf_link": "https://openreview.net/pdf/3909fb38c37d6d25dca74d884b891baf99754ff3.pdf",
        "forum_url": "https://openreview.net/forum?id=EcGGFkNTxdJ",
        "source": "iclr2022"
    },
    {
        "title": "Policy Gradients Incorporating the Future",
        "authors": [
            "David Venuto",
            "Elaine Lau",
            "Doina Precup",
            "Ofir Nachum"
        ],
        "published": "iclr 2022 poster",
        "summary": "Reasoning about the future -- understanding how decisions in the present time affect outcomes in the future -- is one of the central challenges for reinforcement learning (RL), especially in highly-stochastic or partially observable environments. While predicting the future directly is hard, in this work we introduce a method that allows an agent to ``look into the future'' without explicitly predicting it. Namely, we propose to allow an agent, during its training on past experience, to observe what \\emph{actually} happened in the future at that time, while enforcing an information bottleneck to avoid the agent overly relying on this privileged information. Coupled with recent advances in variational inference and a latent-variable autoregressive model, this gives our agent the ability to utilize rich and \\emph{useful} information about the future trajectory dynamics in addition to the present. Our method, Policy Gradients Incorporating the Future (PGIF), is easy to implement and versatile, being applicable to virtually any policy gradient algorithm. We apply our proposed method to a number of off-the-shelf RL algorithms and show that PGIF is able to achieve higher reward faster in a variety of online and offline RL domains, as well as sparse-reward and partially observable environments. ",
        "pdf_link": "https://openreview.net/pdf/1bc7c8d13a1713bf5e86827cb71b07a2d36496ad.pdf",
        "forum_url": "https://openreview.net/forum?id=EHaUTlm2eHg",
        "source": "iclr2022"
    },
    {
        "title": "P-Adapters: Robustly Extracting Factual Information from Language Models with Diverse Prompts",
        "authors": [
            "Benjamin Newman",
            "Prafulla Kumar Choubey",
            "Nazneen Rajani"
        ],
        "published": "iclr 2022 poster",
        "summary": "Recent work (e.g. LAMA (Petroni et al., 2019)) has found that the quality of the factual information extracted from Large Language Models (LLMs) depends on the prompts used to query them. This inconsistency is problematic because different users will query LLMs for the same information using different wording, but should receive the same, accurate responses regardless. In this work we aim to address this shortcoming by introducing P-Adapters: lightweight models that sit between the embedding layer and first attention layer of LLMs. They take LLM embeddings as input and output continuous prompts that are used to query the LLM. Additionally, we investigate Mixture of Experts (MoE) models that learn a set of continuous prompts (the \"experts\") and select one to query the LLM. These require a separate classifier trained on human-annotated data to map natural language prompts to the continuous ones. P-Adapters perform comparably to the more complex MoE models in extracting factual information from BERT and RoBERTa while eliminating the need for additional annotations. P-Adapters show between 12-26% absolute improvement in precision and 36-50% absolute improvement in consistency over a baseline of just using natural language queries alone. Finally, we investigate what makes P-Adapters successful and conclude that a significant factor is access to the LLM's embeddings of the original natural language prompt, particularly the subject of the entity pair being queried.",
        "pdf_link": "https://openreview.net/pdf/8e2c7114bf23dadb13338c9b0dcf063a536ff1b3.pdf",
        "forum_url": "https://openreview.net/forum?id=DhzIU48OcZh",
        "source": "iclr2022"
    },
    {
        "title": "Inverse Online Learning: Understanding Non-Stationary and Reactionary Policies",
        "authors": [
            "Alex Chan",
            "Alicia Curth",
            "Mihaela van der Schaar"
        ],
        "published": "iclr 2022 poster",
        "summary": "Human decision making is well known to be imperfect and the ability to analyse such processes individually is crucial when attempting to aid or improve a decision-maker's ability to perform a task, e.g. to alert them to potential biases or oversights on their part. To do so, it is necessary to develop interpretable representations of how agents make decisions and how this process changes over time as the agent learns online in reaction to the accrued experience. To then understand the decision-making processes underlying a set of observed trajectories, we cast the policy inference problem as the inverse to this online learning problem. By interpreting actions within a potential outcomes framework, we introduce a meaningful mapping based on agents choosing an action they believe to have the greatest treatment effect. We introduce a practical algorithm for retrospectively estimating such perceived effects, alongside the process through which agents update them, using a novel architecture built upon an expressive family of deep state-space models. Through application to the analysis of UNOS organ donation acceptance decisions, we demonstrate that our approach can bring valuable insights into the factors that govern decision processes and how they change over time. ",
        "pdf_link": "https://openreview.net/pdf/dc7c6c1afc5a4d35d2f621c9684cf86f140adf79.pdf",
        "forum_url": "https://openreview.net/forum?id=DYypjaRdph2",
        "source": "iclr2022"
    },
    {
        "title": "Actor-Critic Policy Optimization in a Large-Scale Imperfect-Information Game",
        "authors": [
            "Haobo Fu",
            "Weiming Liu",
            "Shuang Wu",
            "Yijia Wang",
            "Tao Yang",
            "Kai Li",
            "Junliang Xing",
            "Bin Li",
            "Bo Ma",
            "QIANG FU",
            "Yang Wei"
        ],
        "published": "iclr 2022 poster",
        "summary": "The deep policy gradient method has demonstrated promising results in many large-scale games, where the agent learns purely from its own experience. Yet, policy gradient methods with self-play suffer convergence problems to a Nash Equilibrium (NE) in multi-agent situations. Counterfactual regret minimization (CFR) has a convergence guarantee to a NE in 2-player zero-sum games, but it usually needs domain-specific abstractions to deal with large-scale games.  Inheriting merits from both methods, in this paper we extend the actor-critic algorithm framework in deep reinforcement learning to tackle a large-scale 2-player zero-sum imperfect-information game, 1-on-1 Mahjong, whose information set size and game length are much larger than poker. The proposed algorithm, named Actor-Critic Hedge (ACH), modifies the policy optimization objective from originally maximizing the discounted returns to minimizing a type of weighted cumulative counterfactual regret. This modification is achieved by approximating the regret via a deep neural network and minimizing the regret via generating self-play policies using Hedge. ACH is theoretically justified as it is derived from a neural-based weighted CFR, for which we prove the convergence to a NE under certain conditions. Experimental results on the proposed 1-on-1 Mahjong benchmark and benchmarks from the literature demonstrate that ACH outperforms related state-of-the-art methods. Also, the agent obtained by ACH defeats a human champion in 1-on-1 Mahjong.",
        "pdf_link": "https://openreview.net/pdf/6fe3b02efc57f5d0f92998d2d9f16fbf729ade8f.pdf",
        "forum_url": "https://openreview.net/forum?id=DTXZqTNV5nW",
        "source": "iclr2022"
    },
    {
        "title": "Environment Predictive Coding for Visual Navigation",
        "authors": [
            "Santhosh Kumar Ramakrishnan",
            "Tushar Nagarajan",
            "Ziad Al-Halah",
            "Kristen Grauman"
        ],
        "published": "iclr 2022 poster",
        "summary": "We introduce environment predictive coding, a self-supervised approach to learn environment-level representations for embodied agents. In contrast to prior work on self-supervised learning for individual images, we aim to encode a 3D environment using a series of images observed by an agent moving in it. We learn these representations via a masked-zone prediction task, which segments an agent’s trajectory into zones and then predicts features of randomly masked zones, conditioned on the agent’s camera poses. This explicit spatial conditioning encourages learning representations that capture the geometric and semantic regularities of 3D environments. We learn such representations on a collection of video walkthroughs and demonstrate successful transfer to multiple downstream navigation tasks. Our experiments on the real-world scanned 3D environments of Gibson and Matterport3D show that our method obtains 2 - 6× higher sample-efﬁciency and up to 57% higher performance over standard image-representation learning.",
        "pdf_link": "https://openreview.net/pdf/a4d281fcacaf03321d8029271263ac6207b29784.pdf",
        "forum_url": "https://openreview.net/forum?id=DBiQQYWykyy",
        "source": "iclr2022"
    },
    {
        "title": "LIGS: Learnable Intrinsic-Reward Generation Selection for Multi-Agent Learning ",
        "authors": [
            "David Henry Mguni",
            "Taher Jafferjee",
            "Jianhong Wang",
            "Nicolas Perez-Nieves",
            "Oliver Slumbers",
            "Feifei Tong",
            "Yang Li",
            "Jiangcheng Zhu",
            "Yaodong Yang",
            "Jun Wang"
        ],
        "published": "iclr 2022 poster",
        "summary": "Efficient exploration is important for reinforcement learners (RL) to achieve high rewards. In multi-agent systems, coordinated exploration and behaviour is critical for agents to jointly achieve optimal outcomes. In this paper, we introduce a new general framework for improving coordination and performance of multi-agent reinforcement learners (MARL). Our framework, named Learnable Intrinsic-Reward Generation Selection algorithm (LIGS) introduces an adaptive learner, Generator that observes the agents and learns to construct intrinsic rewards online that coordinate the agents’ joint exploration and joint behaviour. Using a novel combination of reinforcement learning (RL) and switching controls, LIGS determines the best states to learn to add intrinsic rewards which leads to a highly efficient learning process. LIGS can subdivide complex tasks making them easier to solve and enables systems of RL agents to quickly solve environments with sparse rewards. LIGS can seamlessly adopt existing multi-agent RL algorithms and our theory shows that it ensures convergence to joint policies that deliver higher system performance. We demonstrate the superior performance of the LIGS framework in challenging tasks in Foraging and StarCraft II and show LIGS is capable of tackling tasks previously unsolvable by MARL methods.",
        "pdf_link": "https://openreview.net/pdf/e236eaa5e5c72be5b36faf18dc589c2d09c9f470.pdf",
        "forum_url": "https://openreview.net/forum?id=CpTuR2ECuW",
        "source": "iclr2022"
    },
    {
        "title": "Learning Synthetic Environments and Reward Networks for Reinforcement Learning",
        "authors": [
            "Fabio Ferreira",
            "Thomas Nierhoff",
            "Andreas Sälinger",
            "Frank Hutter"
        ],
        "published": "iclr 2022 poster",
        "summary": "We introduce Synthetic Environments (SEs) and Reward Networks (RNs), represented by neural networks, as proxy environment models for training Reinforcement Learning (RL) agents. We show that an agent, after being trained exclusively on the SE, is able to solve the corresponding real environment. While an SE acts as a full proxy to a real environment by learning about its state dynamics and rewards, an RN is a partial proxy that learns to augment or replace rewards. We use bi-level optimization to evolve SEs and RNs: the inner loop trains the RL agent, and the outer loop trains the parameters of the SE / RN via an evolution strategy. We evaluate our proposed new concept on a broad range of RL algorithms and classic control environments. In a one-to-one comparison, learning an SE proxy requires more interactions with the real environment than training agents only on the real environment. However, once such an SE has been learned, we do not need any interactions with the real environment to train new agents. Moreover, the learned SE proxies allow us to train agents with fewer interactions while maintaining the original task performance. Our empirical results suggest that SEs achieve this result by learning informed representations that bias the agents towards relevant states. Moreover, we find that these proxies are robust against hyperparameter variation and can also transfer to unseen agents.",
        "pdf_link": "https://openreview.net/pdf/706b0f26d6790fba3e56e1ae94751cce8f4a0789.pdf",
        "forum_url": "https://openreview.net/forum?id=C1_esHN6AVn",
        "source": "iclr2022"
    },
    {
        "title": "Lipschitz-constrained Unsupervised Skill Discovery",
        "authors": [
            "Seohong Park",
            "Jongwook Choi",
            "Jaekyeom Kim",
            "Honglak Lee",
            "Gunhee Kim"
        ],
        "published": "iclr 2022 poster",
        "summary": "We study the problem of unsupervised skill discovery, whose goal is to learn a set of diverse and useful skills with no external reward. There have been a number of skill discovery methods based on maximizing the mutual information (MI) between skills and states. However, we point out that their MI objectives usually prefer static skills to dynamic ones, which may hinder the application for downstream tasks. To address this issue, we propose Lipschitz-constrained Skill Discovery (LSD), which encourages the agent to discover more diverse, dynamic, and far-reaching skills. Another benefit of LSD is that its learned representation function can be utilized for solving goal-following downstream tasks even in a zero-shot manner — i.e., without further training or complex planning. Through experiments on various MuJoCo robotic locomotion and manipulation environments, we demonstrate that LSD outperforms previous approaches in terms of skill diversity, state space coverage, and performance on seven downstream tasks including the challenging task of following multiple goals on Humanoid. Our code and videos are available at https://shpark.me/projects/lsd/.",
        "pdf_link": "https://openreview.net/pdf/8651c40702367a7edf4361ffff2a8a4cd82b9cba.pdf",
        "forum_url": "https://openreview.net/forum?id=BGvt0ghNgA",
        "source": "iclr2022"
    },
    {
        "title": "Trans-Encoder: Unsupervised sentence-pair modelling through self- and mutual-distillations",
        "authors": [
            "Fangyu Liu",
            "Yunlong Jiao",
            "Jordan Massiah",
            "Emine Yilmaz",
            "Serhii Havrylov"
        ],
        "published": "iclr 2022 poster",
        "summary": "In NLP, a large volume of tasks involve pairwise comparison between two sequences (e.g. sentence similarity and paraphrase identification). Predominantly, two formulations are used for sentence-pair tasks: bi-encoders and cross-encoders. Bi-encoders produce fixed-dimensional sentence representations and are computationally efficient, however, they usually underperform cross-encoders. Cross-encoders can leverage their attention heads to exploit inter-sentence interactions for better performance but they require task fine-tuning and are computationally more expensive. In this paper, we present a completely unsupervised sentence representation model termed as Trans-Encoder that combines the two learning paradigms into an iterative joint framework to simultaneously learn enhanced bi- and cross-encoders. Specifically, on top of a pre-trained Language Model (PLM), we start with converting it to an unsupervised bi-encoder, and then alternate between the bi- and cross-encoder task formulations. In each alternation, one task formulation will produce pseudo-labels which are used as learning signals for the other task formulation. We then propose an extension to conduct such self-distillation approach on multiple PLMs in parallel and use the average of their pseudo-labels for mutual distillation. Trans-Encoder creates, to the best of our knowledge, the first completely unsupervised cross-encoder and also a state-of-the-art unsupervised bi-encoder for sentence similarity. Both the bi-encoder and cross-encoder formulations of Trans-Encoder outperform recently proposed state-of-the-art unsupervised sentence encoders such as Mirror-BERT and SimCSE by up to 5% on the sentence similarity benchmarks.",
        "pdf_link": "https://openreview.net/pdf/3a6f31c7903c67d5e431aafcd98d91be36443b10.pdf",
        "forum_url": "https://openreview.net/forum?id=AmUhwTOHgm",
        "source": "iclr2022"
    },
    {
        "title": "A Reduction-Based Framework for Conservative Bandits and Reinforcement Learning",
        "authors": [
            "Yunchang Yang",
            "Tianhao Wu",
            "Han Zhong",
            "Evrard Garcelon",
            "Matteo Pirotta",
            "Alessandro Lazaric",
            "Liwei Wang",
            "Simon Shaolei Du"
        ],
        "published": "iclr 2022 poster",
        "summary": "We study bandits and reinforcement learning (RL) subject to a conservative constraint where the agent is asked to perform at least as well as a given baseline policy. This setting is particular relevant in real-world domains including digital marketing, healthcare, production, finance, etc. In this paper, we present a reduction-based framework for conservative bandits and RL, in which our core technique is to calculate the necessary and sufficient budget obtained from running the baseline policy. For lower bounds, we improve the existing lower bound for conservative multi-armed bandits and obtain new lower bounds for conservative linear bandits, tabular RL and low-rank MDP, through a black-box reduction that turns a certain lower bound in the nonconservative setting into a new lower bound in the conservative setting.  For upper bounds, in multi-armed bandits, linear bandits and tabular RL, our new upper bounds tighten or match existing ones with significantly simpler analyses. We also obtain a new upper bound for conservative low-rank MDP.",
        "pdf_link": "https://openreview.net/pdf/559947894a1c34a3506ac8af2c0fe052117babd0.pdf",
        "forum_url": "https://openreview.net/forum?id=AcrlgZ9BKed",
        "source": "iclr2022"
    },
    {
        "title": "Is High Variance Unavoidable in RL? A Case Study in Continuous Control",
        "authors": [
            "Johan Bjorck",
            "Carla P Gomes",
            "Kilian Q Weinberger"
        ],
        "published": "iclr 2022 poster",
        "summary": "Reinforcement learning (RL) experiments have notoriously high variance, and minor details can have disproportionately large effects on measured outcomes. This is problematic for creating reproducible research and also serves as an obstacle when applying RL to sensitive real-world applications. In this paper, we investigate causes for this perceived instability. To allow for an in-depth analysis, we focus on a specifically popular setup with high variance -- continuous control from pixels with an actor-critic agent. In this setting, we demonstrate that poor outlier runs which completely fail to learn are an important source of variance, but that weight initialization and initial exploration are not at fault. We show that one cause for these outliers is unstable network parametrization which leads to saturating nonlinearities. We investigate several fixes to this issue and find that simply normalizing penultimate features is surprisingly effective. For sparse tasks, we also find that partially disabling clipped double Q-learning decreases variance. By combining fixes we significantly decrease variances, lowering the average standard deviation across 21 tasks by a factor >3 for a state-of-the-art agent. This demonstrates that the perceived variance is not necessarily inherent to RL. Instead, it may be addressed via simple modifications and we argue that developing low-variance agents is an important goal for the RL community.",
        "pdf_link": "https://openreview.net/pdf/231cc619ca2bc7ec04038f562613fdfa22ef8634.pdf",
        "forum_url": "https://openreview.net/forum?id=9xhgmsNVHu",
        "source": "iclr2022"
    },
    {
        "title": "miniF2F: a cross-system benchmark for formal Olympiad-level mathematics",
        "authors": [
            "Kunhao Zheng",
            "Jesse Michael Han",
            "Stanislas Polu"
        ],
        "published": "iclr 2022 poster",
        "summary": "We present $\\textsf{miniF2F}$, a dataset of formal Olympiad-level mathematics problems statements intended to provide a unified cross-system benchmark for neural theorem proving. The $\\textsf{miniF2F}$ benchmark currently targets Metamath, Lean, Isabelle (partially) and HOL Light (partially) and consists of 488 problem statements drawn from the AIME, AMC, and the International Mathematical Olympiad (IMO), as well as material from high-school and undergraduate mathematics courses. We report baseline results using GPT-f, a neural theorem prover based on GPT-3 and provide an analysis of its performance. We intend for $\\textsf{miniF2F}$ to be a community-driven effort and hope that our benchmark will help spur advances in neural theorem proving.",
        "pdf_link": "https://openreview.net/pdf/74334724110d37109013ea669f8ffec08b3498a1.pdf",
        "forum_url": "https://openreview.net/forum?id=9ZPegFuFTFv",
        "source": "iclr2022"
    },
    {
        "title": "Learning Transferable Reward for Query Object Localization with Policy Adaptation",
        "authors": [
            "Tingfeng Li",
            "Shaobo Han",
            "Martin Renqiang Min",
            "Dimitris N. Metaxas"
        ],
        "published": "iclr 2022 poster",
        "summary": "We propose a reinforcement learning based approach to query object localization, for which an agent is trained to localize objects of interest specified by a small exemplary set. We learn a transferable reward signal formulated using the exemplary set by ordinal metric learning. Our proposed method enables test-time policy adaptation to new environments where the reward signals are not readily available, and outperforms fine-tuning approaches that are limited to annotated images. In addition, the transferable reward allows repurposing the trained agent from one specific class to another class. Experiments on corrupted MNIST, CU-Birds, and COCO datasets demonstrate the effectiveness of our approach.",
        "pdf_link": "https://openreview.net/pdf/5b72a5cbe8d019baa19ccef469da73414589de18.pdf",
        "forum_url": "https://openreview.net/forum?id=92tYQiil17",
        "source": "iclr2022"
    },
    {
        "title": "On the Pitfalls of Analyzing Individual Neurons in Language Models",
        "authors": [
            "Omer Antverg",
            "Yonatan Belinkov"
        ],
        "published": "iclr 2022 poster",
        "summary": "While many studies have shown that linguistic information is encoded in hidden word representations, few have studied individual neurons, to show how and in which neurons it is encoded.\nAmong these, the common approach is to use an external probe to rank neurons according to their relevance to some linguistic attribute, and to evaluate the obtained ranking using the same probe that produced it.\nWe show two pitfalls in this methodology:\n    1. It confounds distinct factors: probe quality and ranking quality.\n    We separate them and draw conclusions on each.\n    2. It focuses on encoded information, rather than information that is used by the model.\n    We show that these are not the same.\nWe compare two recent ranking methods and a simple one we introduce, and evaluate them with regard to both of these aspects.",
        "pdf_link": "https://openreview.net/pdf/48b5f2da77098455f23b9b9c4bd9b7a6cd9712e8.pdf",
        "forum_url": "https://openreview.net/forum?id=8uz0EWPQIMu",
        "source": "iclr2022"
    },
    {
        "title": "Constructing a Good Behavior Basis for Transfer using Generalized Policy Updates",
        "authors": [
            "Safa Alver",
            "Doina Precup"
        ],
        "published": "iclr 2022 poster",
        "summary": "We study the problem of learning a good set of policies, so that when combined together, they can solve a wide variety of unseen reinforcement learning tasks with no or very little new data. Specifically, we consider the framework of generalized policy evaluation and improvement, in which the rewards for all tasks of interest are assumed to be expressible as a linear combination of a fixed set of features. We show theoretically that, under certain assumptions, having access to a specific set of diverse policies, which we call a set of independent policies, can allow for instantaneously achieving high-level performance on all possible downstream tasks which are typically more complex than the ones on which the agent was trained. Based on this theoretical analysis, we propose a simple algorithm that iteratively constructs this set of policies. In addition to empirically validating our theoretical results, we compare our approach with recently proposed diverse policy set construction methods and show that, while others fail, our approach is able to build a behavior basis that enables instantaneous transfer to all possible downstream tasks. We also show empirically that having access to a set of independent policies can better bootstrap the learning process on downstream tasks where the new reward function cannot be described as a linear combination of the features. Finally, we demonstrate how this policy set can be useful in a lifelong reinforcement learning setting.",
        "pdf_link": "https://openreview.net/pdf/99cd11d061fb209a0dffb177bc34b326b5d4d7a6.pdf",
        "forum_url": "https://openreview.net/forum?id=7IWGzQ6gZ1D",
        "source": "iclr2022"
    },
    {
        "title": "Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis--Hastings",
        "authors": [
            "Kartik Goyal",
            "Chris Dyer",
            "Taylor Berg-Kirkpatrick"
        ],
        "published": "iclr 2022 poster",
        "summary": "While recent work has shown that scores from models trained by the ubiquitous masked language modeling (MLM) objective effectively discriminate probable from improbable sequences, it is still an open question if these MLMs specify a principled probability distribution over the space of possible sequences. In this paper, we interpret MLMs as energy-based sequence models and propose two energy parametrizations derivable from the trained MLMs. In order to draw samples correctly from these models, we develop a tractable sampling scheme based on the Metropolis--Hastings Monte Carlo algorithm. In our approach, samples are proposed from the same masked conditionals used for training the masked language models, and they are accepted or rejected based on their energy values according to the target distribution. We validate the effectiveness of the proposed parametrizations by exploring the quality of samples drawn from these energy-based models for both open-ended unconditional generation and a conditional generation task of machine translation. We theoretically and empirically justify our sampling algorithm by showing that the masked conditionals on their own do not yield a Markov chain whose stationary distribution is that of our target distribution, and our approach generates higher quality samples than other recently proposed undirected generation approaches (Wang et al., 2019, Ghazvininejad et al., 2019).",
        "pdf_link": "https://openreview.net/pdf/dfdc7212f0c035baaec71e0d9d64317aec15492b.pdf",
        "forum_url": "https://openreview.net/forum?id=6PvWo1kEvlT",
        "source": "iclr2022"
    },
    {
        "title": "When Can We Learn General-Sum Markov Games with a Large Number of Players Sample-Efficiently?",
        "authors": [
            "Ziang Song",
            "Song Mei",
            "Yu Bai"
        ],
        "published": "iclr 2022 poster",
        "summary": "Multi-agent reinforcement learning has made substantial empirical progresses in solving games with a large number of players. However, theoretically, the best known sample complexity for finding a Nash equilibrium in general-sum games scales exponentially in the number of players due to the size of the joint action space, and there is a matching exponential lower bound. This paper investigates what learning goals admit better sample complexities in the setting of $m$-player general-sum Markov games with $H$ steps, $S$ states, and $A_i$ actions per player. First, we design algorithms for learning an $\\epsilon$-Coarse Correlated Equilibrium (CCE) in $\\widetilde{\\mathcal{O}}(H^5S\\max_{i\\le m} A_i / \\epsilon^2)$ episodes, and an $\\epsilon$-Correlated Equilibrium (CE) in $\\widetilde{\\mathcal{O}}(H^6S\\max_{i\\le m} A_i^2 / \\epsilon^2)$ episodes. This is the first line of results for learning CCE and CE with sample complexities polynomial in $\\max_{i\\le m} A_i$. Our algorithm for learning CE integrates an adversarial bandit subroutine which minimizes a weighted swap regret, along with several novel designs in the outer loop. Second, we consider the important special case of Markov Potential Games, and design an algorithm that learns an $\\epsilon$-approximate Nash equilibrium within $\\widetilde{\\mathcal{O}}(S\\sum_{i\\le m} A_i / \\epsilon^3)$ episodes (when only highlighting the dependence on $S$, $A_i$, and $\\epsilon$), which only depends linearly in $\\sum_{i\\le m} A_i$ and significantly improves over the existing efficient algorithm in the $\\epsilon$ dependence. Overall, our results shed light on what equilibria or structural assumptions on the game may enable sample-efficient learning with many players.",
        "pdf_link": "https://openreview.net/pdf/4897aa8eae5efc42c6f0c8f6dcc2f6ce0989de42.pdf",
        "forum_url": "https://openreview.net/forum?id=6MmiS0HUJHR",
        "source": "iclr2022"
    },
    {
        "title": "HyAR: Addressing Discrete-Continuous Action Reinforcement Learning via Hybrid Action Representation",
        "authors": [
            "Boyan Li",
            "Hongyao Tang",
            "YAN ZHENG",
            "Jianye HAO",
            "Pengyi Li",
            "Zhen Wang",
            "Zhaopeng Meng",
            "LI Wang"
        ],
        "published": "iclr 2022 poster",
        "summary": "Discrete-continuous hybrid action space is a natural setting in many practical problems, such as robot control and game AI. However, most previous Reinforcement Learning (RL) works only demonstrate the success in controlling with either discrete or continuous action space, while seldom take into account the hybrid action space. One naive way to address hybrid action RL is to convert the hybrid action space into a unified homogeneous action space by discretization or continualization, so that conventional RL algorithms can be applied. However, this ignores the underlying structure of hybrid action space and also induces the scalability issue and additional approximation difficulties, thus leading to degenerated results. In this paper, we propose Hybrid Action Representation (HyAR) to learn a compact and decodable latent representation space for the original hybrid action space. HyAR constructs the latent space and embeds the dependence between discrete action and continuous parameter via an embedding table and conditional Variantional Auto-Encoder (VAE). To further improve the effectiveness, the action representation is trained to be semantically smooth through unsupervised environmental dynamics prediction. Finally, the agent then learns its policy with conventional DRL algorithms in the learned representation space and interacts with the environment by decoding the hybrid action embeddings to the original action space. We evaluate HyAR in a variety of environments with discrete-continuous action space. The results demonstrate the superiority of HyAR when compared with previous baselines, especially for high-dimensional action spaces.",
        "pdf_link": "https://openreview.net/pdf/21005c7fdccb6d12eff7a0a9deba69320225bd53.pdf",
        "forum_url": "https://openreview.net/forum?id=64trBbOhdGU",
        "source": "iclr2022"
    },
    {
        "title": "On the role of population heterogeneity in emergent communication",
        "authors": [
            "Mathieu Rita",
            "Florian Strub",
            "Jean-Bastien Grill",
            "Olivier Pietquin",
            "Emmanuel Dupoux"
        ],
        "published": "iclr 2022 poster",
        "summary": "Populations have often been perceived as a structuring component for language to emerge and evolve: the larger the population, the more systematic the language. While this observation is widespread in the sociolinguistic literature, it has not been reproduced in computer simulations with neural agents. In this paper, we thus aim to clarify this apparent contradiction. We explore emergent language properties by varying agent population size in the speaker-listener Lewis Game. After reproducing the experimental paradox, we challenge the simulation assumption that the agent community is homogeneous. We first investigate how speaker-listener asymmetry alters language structure to examine two potential diversity factors: training speed and network capacity. We find out that emergent language properties are only altered by the relative difference of factors between speaker and listener, and not by their absolute values. From then, we leverage this observation to control population heterogeneity without introducing confounding factors. We finally show that introducing such training speed heterogeneities naturally sort out the initial paradox: larger simulated communities start developing more systematic and structured languages.",
        "pdf_link": "https://openreview.net/pdf/58631dde9cf5c1e786259a90eb129ea9f0e5b5b3.pdf",
        "forum_url": "https://openreview.net/forum?id=5Qkd7-bZfI",
        "source": "iclr2022"
    },
    {
        "title": "Learning a subspace of policies for online adaptation in Reinforcement Learning",
        "authors": [
            "Jean-Baptiste Gaya",
            "Laure Soulier",
            "Ludovic Denoyer"
        ],
        "published": "iclr 2022 poster",
        "summary": "Deep Reinforcement Learning (RL) is mainly studied in a setting where the training and the testing environments are similar. But in many practical applications, these environments may differ. For instance, in control systems, the robot(s) on which a policy is learned might differ from the robot(s) on which a policy will run. It can be caused by different internal factors (e.g., calibration issues, system attrition, defective modules) or also by external changes (e.g., weather conditions). There is  a need to develop RL methods that generalize well to variations of the training conditions. In this article, we consider the simplest yet hard to tackle generalization setting where the test environment is unknown at train time, forcing the agent to adapt to the system's new dynamics. This online adaptation process can be computationally expensive (e.g., fine-tuning) and cannot rely on meta-RL techniques since there is just a single train environment. To do so, we propose an approach where we learn a subspace of policies within the parameter space. This subspace contains an infinite number of policies that are trained to solve the training environment while having different parameter values. As a consequence, two policies in that subspace process information differently and exhibit different behaviors when facing variations of the train environment. Our experiments carried out over a large variety of benchmarks compare our approach with baselines, including diversity-based methods. In comparison, our approach is simple to tune, does not need any extra component  (e.g., discriminator) and learns policies able to gather a high reward on unseen environments.",
        "pdf_link": "https://openreview.net/pdf/2845a7c71512ebc0d2961233b0ec523e86dd65a8.pdf",
        "forum_url": "https://openreview.net/forum?id=4Muj-t_4o4",
        "source": "iclr2022"
    },
    {
        "title": "Learning Continuous Environment Fields via Implicit Functions",
        "authors": [
            "Xueting Li",
            "Shalini De Mello",
            "Xiaolong Wang",
            "Ming-Hsuan Yang",
            "Jan Kautz",
            "Sifei Liu"
        ],
        "published": "iclr 2022 poster",
        "summary": "   We propose a novel scene representation that encodes reaching distance -- the distance between any position in the scene to a goal along a feasible trajectory. We demonstrate that this environment field representation can directly guide the dynamic behaviors of agents in 2D mazes or 3D indoor scenes. Our environment field is a continuous representation and learned via a neural implicit function using discretely sampled training data. We showcase its application for agent navigation in 2D mazes, and human trajectory prediction in 3D indoor environments. To produce physically plausible and natural trajectories for humans, we additionally learn a generative model that predicts regions where humans commonly appear, and enforce the environment field to be defined within such regions. Extensive experiments demonstrate that the proposed method can generate both feasible and plausible trajectories efficiently and accurately.",
        "pdf_link": "https://openreview.net/pdf/bb9cdfc2e84adef6cb9610f21715cf048acaeaa4.pdf",
        "forum_url": "https://openreview.net/forum?id=3ILxkQ7yElm",
        "source": "iclr2022"
    },
    {
        "title": "ToM2C: Target-oriented Multi-agent Communication and Cooperation with Theory of Mind",
        "authors": [
            "Yuanfei Wang",
            "fangwei zhong",
            "Jing Xu",
            "Yizhou Wang"
        ],
        "published": "iclr 2022 poster",
        "summary": "Being able to predict the mental states of others is a key factor to effective social interaction. It is also crucial for distributed multi-agent systems, where agents are required to communicate and cooperate. In this paper, we introduce such an important social-cognitive skill, i.e. Theory of Mind (ToM), to build socially intelligent agents who are able to communicate and cooperate effectively to accomplish challenging tasks. With ToM, each agent is capable of inferring the mental states and intentions of others according to its (local) observation. Based on the inferred states, the agents decide \"when'' and with \"whom'' to share their intentions. With the information observed, inferred, and received, the agents decide their sub-goals and reach a consensus among the team. In the end, the low-level executors independently take primitive actions to accomplish the sub-goals. We demonstrate the idea in two typical target-oriented multi-agent tasks: cooperative navigation and multi-sensor target coverage. The experiments show that the proposed model not only outperforms the state-of-the-art methods on reward and communication efficiency, but also shows good generalization across different scales of the environment.\n",
        "pdf_link": "https://openreview.net/pdf/a18a759fefcf7c83cb3cd488541d2df743059f5b.pdf",
        "forum_url": "https://openreview.net/forum?id=2t7CkQXNpuq",
        "source": "iclr2022"
    },
    {
        "title": "Should We Be Pre-training? An Argument for End-task Aware Training as an Alternative",
        "authors": [
            "Lucio M. Dery",
            "Paul Michel",
            "Ameet Talwalkar",
            "Graham Neubig"
        ],
        "published": "iclr 2022 poster",
        "summary": "In most settings of practical concern, machine learning practitioners know in advance what end-task they wish to boost with auxiliary tasks. However, widely used methods for leveraging auxiliary data like pre-training and its continued-pretraining variant are end-task agnostic: they rarely, if ever, exploit knowledge of the target task. We study replacing end-task agnostic continued training of pre-trained language models with end-task aware training of said models. We argue that for sufficiently important end-tasks, the benefits of leveraging auxiliary data in a task-aware fashion can justify forgoing the traditional approach of obtaining generic, end-task agnostic representations as with (continued) pre-training. On three different low-resource NLP tasks from two domains, we demonstrate that  multi-tasking the end-task and auxiliary objectives results in significantly better downstream task performance than the widely-used task-agnostic continued pre-training paradigm of Gururangan et al. (2020).\nWe next introduce an online meta-learning algorithm that learns  a set of multi-task weights to better balance among our multiple auxiliary objectives, achieving further improvements on end-task performance and data efficiency.",
        "pdf_link": "https://openreview.net/pdf/691ed677722bace4faaff6140d7c50eac5c22122.pdf",
        "forum_url": "https://openreview.net/forum?id=2bO2x8NAIMB",
        "source": "iclr2022"
    },
    {
        "title": "Benchmarking the Spectrum of Agent Capabilities",
        "authors": [
            "Danijar Hafner"
        ],
        "published": "iclr 2022 poster",
        "summary": "Evaluating the general abilities of intelligent agents requires complex simulation environments. Existing benchmarks typically evaluate only one narrow task per environment, requiring researchers to perform expensive training runs on many different environments. We introduce Crafter, an open world survival game with visual inputs that evaluates a wide range of general abilities within a single environment. Agents either learn from the provided reward signal or through intrinsic objectives and are evaluated by semantically meaningful achievements that can be unlocked during each episode, such as discovering resources and crafting tools. Consistently unlocking all achievements requires strong generalization, deep exploration, and long-term reasoning. We experimentally verify that Crafter is of appropriate difficulty to drive future research and provide baselines scores of reward agents and unsupervised agents. Furthermore, we observe sophisticated behaviors emerging from maximizing the reward signal, such as building tunnel systems, bridges, houses, and plantations. We hope that Crafter will accelerate research progress by quickly evaluating a wide spectrum of abilities.",
        "pdf_link": "https://openreview.net/pdf/116a18888b3fb460e882ec2b844128223e3b17ca.pdf",
        "forum_url": "https://openreview.net/forum?id=1W0z96MFEoH",
        "source": "iclr2022"
    },
    {
        "title": "Online Ad Hoc Teamwork under Partial Observability",
        "authors": [
            "Pengjie Gu",
            "Mengchen Zhao",
            "Jianye Hao",
            "Bo An"
        ],
        "published": "iclr 2022 poster",
        "summary": "Autonomous agents often need to work together as a team to accomplish complex cooperative tasks. Due to privacy and other realistic constraints, agents might need to collaborate with previously unknown teammates on the fly. This problem is known as ad hoc teamwork, which remains a core research challenge. Prior works usually rely heavily on strong assumptions like full observability, fixed and predefined teammates' types. This paper relaxes these assumptions with a novel reinforcement learning framework called ODITS, which allows the autonomous agent to adapt to arbitrary teammates in an online fashion. Instead of limiting teammates into a finite set of predefined types, ODITS automatically learns latent variables of teammates' behaviors to infer how to cooperate with new teammates effectively. To overcome partial observability, we introduce an information-based regularizer to derive proxy representations of the learned variables from local observations. Extensive experimental results show that ODITS significantly outperforms various baselines in widely used ad hoc teamwork tasks.",
        "pdf_link": "https://openreview.net/pdf/69ebe98356c87ca06fb980028a882b8bf4ea9078.pdf",
        "forum_url": "https://openreview.net/forum?id=18Ys0-PzyPI",
        "source": "iclr2022"
    },
    {
        "title": "Learning Graphon Mean Field Games and Approximate Nash Equilibria",
        "authors": [
            "Kai Cui",
            "Heinz Koeppl"
        ],
        "published": "iclr 2022 poster",
        "summary": "Recent advances at the intersection of dense large graph limits and mean field games have begun to enable the scalable analysis of a broad class of dynamical sequential games with large numbers of agents. So far, results have been largely limited to graphon mean field systems with continuous-time diffusive or jump dynamics, typically without control and with little focus on computational methods. We propose a novel discrete-time formulation for graphon mean field games as the limit of non-linear dense graph Markov games with weak interaction. On the theoretical side, we give extensive and rigorous existence and approximation properties of the graphon mean field solution in sufficiently large systems. On the practical side we provide general learning schemes for graphon mean field equilibria by either introducing agent equivalence classes or reformulating the graphon mean field system as a classical mean field system. By repeatedly finding a regularized optimal control solution and its generated mean field, we successfully obtain plausible approximate Nash equilibria in otherwise infeasible large dense graph games with many agents. Empirically, we are able to demonstrate on a number of examples that the finite-agent behavior comes increasingly close to the mean field behavior for our computed equilibria as the graph or system size grows, verifying our theory. More generally, we successfully apply policy gradient reinforcement learning in conjunction with sequential Monte Carlo methods.",
        "pdf_link": "https://openreview.net/pdf/b4f2ad24930753086ac1a6b4fea2f45e13771c21.pdf",
        "forum_url": "https://openreview.net/forum?id=0sgntlpKDOz",
        "source": "iclr2022"
    },
    {
        "title": "Efficient Learning of Safe Driving Policy via Human-AI Copilot Optimization",
        "authors": [
            "Quanyi Li",
            "Zhenghao Peng",
            "Bolei Zhou"
        ],
        "published": "iclr 2022 poster",
        "summary": "Human intervention is an effective way to inject human knowledge into the training loop of reinforcement learning, which can bring fast learning and ensured training safety. Given the very limited budget of human intervention, it remains challenging to design when and how human expert interacts with the learning agent in the training. In this work, we develop a novel human-in-the-loop learning method called Human-AI Copilot Optimization (HACO).To allow the agent's sufficient exploration in the risky environments while ensuring the training safety, the human expert can take over the control and demonstrate how to avoid probably dangerous situations or trivial behaviors. The proposed HACO then effectively utilizes the data both from the trial-and-error exploration and human's partial demonstration to train a high-performing agent. HACO extracts proxy state-action values from partial human demonstration and optimizes the agent to improve the proxy values meanwhile reduce the human interventions. The experiments show that HACO achieves a substantially high sample efficiency in the safe driving benchmark. HACO can train agents to drive in unseen traffic scenarios with a handful of human intervention budget and achieve high safety and generalizability, outperforming both reinforcement learning and imitation learning baselines with a large margin. Code and demo video are included in the supplementary materials.",
        "pdf_link": "https://openreview.net/pdf/c0b165aabfc0cf4dea07b0341e17033a3bc5722b.pdf",
        "forum_url": "https://openreview.net/forum?id=0cgU-BZp2ky",
        "source": "iclr2022"
    },
    {
        "title": "Fast Model Editing at Scale",
        "authors": [
            "Eric Mitchell",
            "Charles Lin",
            "Antoine Bosselut",
            "Chelsea Finn",
            "Christopher D Manning"
        ],
        "published": "iclr 2022 poster",
        "summary": "While large pre-trained models have enabled impressive results on a variety of downstream tasks, the largest existing models still make errors, and even accurate predictions may become outdated over time. Because detecting all such failures at training time is impossible, enabling both developers and end users of such models to correct inaccurate outputs while leaving the model otherwise intact is desirable. However, the distributed, black-box nature of the representations learned by large neural networks makes producing such targeted edits difficult. If presented with only a single problematic input and new desired output, fine-tuning approaches tend to overfit; other editing algorithms are either computationally infeasible or simply ineffective when applied to very large models. To enable easy post-hoc editing at scale, we propose Model Editor Networks using Gradient Decomposition (MEND), a collection of small auxiliary editing networks that use a single desired input-output pair to make fast, local edits to a pre-trained model's behavior. MEND learns to transform the gradient obtained by standard fine-tuning, using a low-rank decomposition of the gradient to make the parameterization of this transformation tractable. MEND can be trained on a single GPU in less than a day even for 10 billion+ parameter models; once trained MEND enables rapid application of new edits to the pre-trained model. Our experiments with T5, GPT, BERT, and BART models show that MEND is the only approach to model editing that effectively edits the behavior of models with more than 10 billion parameters. Code available at https://sites.google.com/view/mend-editing.",
        "pdf_link": "https://openreview.net/pdf/f647f6c18613fc91a31c3ef0b98dbe3b782d01f8.pdf",
        "forum_url": "https://openreview.net/forum?id=0DcZxeWfOPt",
        "source": "iclr2022"
    },
    {
        "title": "Non-Parallel Text Style Transfer with Self-Parallel Supervision",
        "authors": [
            "Ruibo Liu",
            "Chongyang Gao",
            "Chenyan Jia",
            "Guangxuan Xu",
            "Soroush Vosoughi"
        ],
        "published": "iclr 2022 poster",
        "summary": "The performance of existing text style transfer models is severely limited by the non-parallel datasets on which the models are trained. In non-parallel datasets, no direct mapping exists between sentences of the source and target style; the style transfer models thus only receive weak supervision of the target sentences during training, which often leads the model to discard too much style-independent information, or utterly fail to transfer the style.\n\nIn this work, we propose LaMer, a novel text style transfer framework based on large-scale language models. LaMer first mines the roughly parallel expressions in the non-parallel datasets with scene graphs, and then employs MLE training, followed by imitation learning refinement, to leverage the intrinsic parallelism within the data. On two benchmark tasks (sentiment & formality transfer) and a newly proposed challenging task (political stance transfer), our model achieves qualitative advances in transfer accuracy, content preservation, and fluency. Further empirical and human evaluations demonstrate that our model not only makes training more efficient, but also generates more readable and diverse expressions than previous models.",
        "pdf_link": "https://openreview.net/pdf/7858e341aa92c11991455a43e9a78c35ee4655a2.pdf",
        "forum_url": "https://openreview.net/forum?id=-TSe5o7STVR",
        "source": "iclr2022"
    }
]