[
    {
        "title": "Benchmarking Large Language Models for News Summarization",
        "authors": [
            "Tianyi Zhang",
            "Faisal Ladhak",
            "Esin Durmus",
            "Percy Liang",
            "Kathleen McKeown",
            "Tatsunori B. Hashimoto"
        ],
        "published": "2024",
        "summary": "Large language models (LLMs) have shown promise for automatic summarization but the reasons behind their successes are poorly understood. By conducting a human evaluation on ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations. First, we find instruction tuning, not model size, is the key to the LLM’s zero-shot summarization capability. Second, existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance. To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers. Despite major stylistic differences such as the amount of paraphrasing, we find that LLM summaries are judged to be on par with human written summaries.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.3.pdf"
    },
    {
        "title": "mGPT: Few-Shot Learners Go Multilingual",
        "authors": [
            "Oleh Shliazhko",
            "Alena Fenogenova",
            "Maria Tikhonova",
            "Anastasia Kozlova",
            "Vladislav Mikhailov",
            "Tatiana Shavrina"
        ],
        "published": "2024",
        "summary": "This paper introduces mGPT, a multilingual variant of GPT-3, pretrained on 61 languages from 25 linguistically diverse language families using Wikipedia and the C4 Corpus. We detail the design and pretraining procedure. The models undergo an intrinsic and extrinsic evaluation: language modeling in all languages, downstream evaluation on cross-lingual NLU datasets and benchmarks in 33 languages, and world knowledge probing in 23 languages. The in-context learning abilities are on par with the contemporaneous language models while covering a larger number of languages, including underrepresented and low-resource languages of the Commonwealth of Independent States and the indigenous peoples in Russia. The source code and the language models are publicly available under the MIT license.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.4.pdf"
    },
    {
        "title": "Cultural Adaptation of Recipes",
        "authors": [
            "Yong Cao",
            "Yova Kementchedjhieva",
            "Ruixiang Cui",
            "Antonia Karamolegkou",
            "Li Zhou",
            "Megan Dare",
            "Lucia Donatelli",
            "Daniel Hershcovich"
        ],
        "published": "2024",
        "summary": "Building upon the considerable advances in Large Language Models (LLMs), we are now equipped to address more sophisticated tasks demanding a nuanced understanding of cross-cultural contexts. A key example is recipe adaptation, which goes beyond simple translation to include a grasp of ingredients, culinary techniques, and dietary preferences specific to a given culture. We introduce a new task involving the translation and cultural adaptation of recipes between Chinese- and English-speaking cuisines. To support this investigation, we present CulturalRecipes, a unique dataset composed of automatically paired recipes written in Mandarin Chinese and English. This dataset is further enriched with a human-written and curated test set. In this intricate task of cross-cultural recipe adaptation, we evaluate the performance of various methods, including GPT-4 and other LLMs, traditional machine translation, and information retrieval techniques. Our comprehensive analysis includes both automatic and human evaluation metrics. While GPT-4 exhibits impressive abilities in adapting Chinese recipes into English, it still lags behind human expertise when translating English recipes into Chinese. This underscores the multifaceted nature of cultural adaptations. We anticipate that these insights will significantly contribute to future research on culturally aware language models and their practical application in culturally diverse contexts.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.5.pdf"
    },
    {
        "title": "Lost in the Middle: How Language Models Use Long Contexts",
        "authors": [
            "Nelson F. Liu",
            "Kevin Lin",
            "John Hewitt",
            "Ashwin Paranjape",
            "Michele Bevilacqua",
            "Fabio Petroni",
            "Percy Liang"
        ],
        "published": "2024",
        "summary": "While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.9.pdf"
    },
    {
        "title": "Red Teaming Language Model Detectors with Language Models",
        "authors": [
            "Zhouxing Shi",
            "Yihan Wang",
            "Fan Yin",
            "Xiangning Chen",
            "Kai-Wei Chang",
            "Cho-Jui Hsieh"
        ],
        "published": "2024",
        "summary": "The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users. To prevent the potentially deceptive usage of LLMs, recent work has proposed algorithms to detect LLM-generated text and protect LLMs. In this paper, we investigate the robustness and reliability of these LLM detectors under adversarial attacks. We study two types of attack strategies: 1) replacing certain words in an LLM’s output with their synonyms given the context; 2) automatically searching for an instructional prompt to alter the writing style of the generation. In both strategies, we leverage an auxiliary LLM to generate the word replacements or the instructional prompt. Different from previous works, we consider a challenging setting where the auxiliary LLM can also be protected by a detector. Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems. Code is available at https://github.com/shizhouxing/LLM-Detector-Robustness.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.10.pdf"
    },
    {
        "title": "Exploring Human-Like Translation Strategy with Large Language Models",
        "authors": [
            "Zhiwei He",
            "Tian Liang",
            "Wenxiang Jiao",
            "Zhuosheng Zhang",
            "Yujiu Yang",
            "Rui Wang",
            "Zhaopeng Tu",
            "Shuming Shi",
            "Xing Wang"
        ],
        "published": "2024",
        "summary": "Large language models (LLMs) have demonstrated impressive capabilities in general scenarios, exhibiting a level of aptitude that approaches, in some aspects even surpasses, human-level intelligence. Among their numerous skills, the translation abilities of LLMs have received considerable attention. Compared to typical machine translation that focuses solely on source-to-target mapping, LLM-based translation can potentially mimic the human translation process, which might take preparatory steps to ensure high-quality translation. This work explores this possibility by proposing the MAPS framework, which stands for Multi-Aspect Prompting and Selection. Specifically, we enable LLMs first to analyze the given source sentence and induce three aspects of translation-related knowledge (keywords, topics, and relevant demonstrations) to guide the final translation process. Moreover, we employ a selection mechanism based on quality estimation to filter out noisy and unhelpful knowledge. Both automatic (3 LLMs × 11 directions × 2 automatic metrics) and human evaluation (preference study and MQM) demonstrate the effectiveness of MAPS. Further analysis shows that by mimicking the human translation process, MAPS reduces various translation errors such as hallucination, ambiguity, mistranslation, awkward style, untranslated text, and omission. Source code is available at https://github.com/zwhe99/MAPS-mt.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.13.pdf"
    },
    {
        "title": "Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering",
        "authors": [
            "Dingmin Wang",
            "Qiuyuan Huang",
            "Matthew Jackson",
            "Jianfeng Gao"
        ],
        "published": "2024",
        "summary": "An open-domain question answering (QA) system usually follows a retrieve-then-read paradigm, in which a retriever is used to retrieve relevant passages from a large corpus, and then a reader generates answers based on the retrieved passages and the original question. In this paper, we propose a simple and novel mutual learning framework to improve the performance of retrieve-then-read-style models via an intermediate module named the knowledge selector, which we train with reinforcement learning. The key benefits of our proposed intermediate module are: 1) no requirement for additional annotated question-passage pairs; 2) improvements in both retrieval and QA performance, as well as computational efficiency, compared to prior competitive retrieve-then-read models; 3) with no finetuning, improvement in the zero-shot performance of large-scale pre-trained language models, e.g., ChatGPT, by encapsulating the input with relevant knowledge without violating the input length constraint.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.14.pdf"
    },
    {
        "title": "Evaluating the Ripple Effects of Knowledge Editing in Language Models",
        "authors": [
            "Roi Cohen",
            "Eden Biran",
            "Ori Yoran",
            "Amir Globerson",
            "Mor Geva"
        ],
        "published": "2024",
        "summary": "Modern language models capture a large body of factual knowledge. However, some facts can be incorrectly induced or become obsolete over time, resulting in factually incorrect generations. This has led to the development of various editing methods that allow updating facts encoded by the model. Evaluation of these methods has primarily focused on testing whether an individual fact has been successfully injected, and if similar predictions for other subjects have not changed. Here we argue that such evaluation is limited, since injecting one fact (e.g., “Jack Depp is the son of Johnny Depp”) introduces a “ripple effect” in the form of additional facts that the model needs to update (e.g., “Jack Depp is the sibling of Lily-Rose Depp”). To address this, we propose novel evaluation criteria that consider the implications of an edit on related facts. Using these criteria, we then construct RippleEdits, a diagnostic benchmark of 5K factual edits, capturing various types of ripple effects. We evaluate prominent editing methods on RippleEdits, showing that they fail to introduce consistent changes in the model’s knowledge. In addition, we find that a simple in-context editing baseline obtains the best scores on our benchmark, suggesting a promising research direction for model editing.1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.16.pdf"
    },
    {
        "title": "The Impact of Word Splitting on the Semantic Content of Contextualized Word Representations",
        "authors": [
            "Aina Garí Soler",
            "Matthieu Labeau",
            "Chloé Clavel"
        ],
        "published": "2024",
        "summary": "When deriving contextualized word representations from language models, a decision needs to be made on how to obtain one for out-of-vocabulary (OOV) words that are segmented into subwords. What is the best way to represent these words with a single vector, and are these representations of worse quality than those of in-vocabulary words? We carry out an intrinsic evaluation of embeddings from different models on semantic similarity tasks involving OOV words. Our analysis reveals, among other interesting findings, that the quality of representations of words that are split is often, but not always, worse than that of the embeddings of known words. Their similarity values, however, must be interpreted with caution.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.17.pdf"
    },
    {
        "title": "Large Language Models Enable Few-Shot Clustering",
        "authors": [
            "Vijay Viswanathan",
            "Kiril Gashteovski",
            "Kiril Gashteovski",
            "Carolin Lawrence",
            "Tongshuang Wu",
            "Graham Neubig"
        ],
        "published": "2024",
        "summary": "Unlike traditional unsupervised clustering, semi-supervised clustering allows users to provide meaningful structure to the data, which helps the clustering algorithm to match the user’s intent. Existing approaches to semi-supervised clustering require a significant amount of feedback from an expert to improve the clusters. In this paper, we ask whether a large language model (LLM) can amplify an expert’s guidance to enable query-efficient, few-shot semi-supervised text clustering. We show that LLMs are surprisingly effective at improving clustering. We explore three stages where LLMs can be incorporated into clustering: before clustering (improving input features), during clustering (by providing constraints to the clusterer), and after clustering (using LLMs post-correction). We find that incorporating LLMs in the first two stages routinely provides significant improvements in cluster quality, and that LLMs enable a user to make trade-offs between cost and accuracy to produce desired clusters. We release our code and LLM prompts for the public to use.1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.18.pdf"
    },
    {
        "title": "JustiLM: Few-shot Justification Generation for Explainable Fact-Checking of Real-world Claims",
        "authors": [
            "Fengzhu Zeng",
            "Wei Gao"
        ],
        "published": "2024",
        "summary": "Justification is an explanation that supports the veracity assigned to a claim in fact-checking. However, the task of justification generation has been previously oversimplified as summarization of a fact-check article authored by fact-checkers. Therefore, we propose a realistic approach to generate justification based on retrieved evidence. We present a new benchmark dataset called ExClaim (for Explainable fact-checking of real-world Claims), and introduce JustiLM, a novel few-shot Justification generation based on retrieval-augmented Language Model by using fact-check articles as an auxiliary resource during training only. Experiments show that JustiLM achieves promising performance in justification generation compared to strong baselines, and can also enhance veracity classification with a straightforward extension.1 Code and dataset are released at https://github.com/znhy1024/JustiLM.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.19.pdf"
    },
    {
        "title": "Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation",
        "authors": [
            "Lukas Edman",
            "Gabriele Sarti",
            "Antonio Toral",
            "Gertjan van Noord",
            "Arianna Bisazza"
        ],
        "published": "2024",
        "summary": "Pretrained character-level and byte-level language models have been shown to be competitive with popular subword models across a range of Natural Language Processing tasks. However, there has been little research on their effectiveness for neural machine translation (NMT), particularly within the popular pretrain-then-finetune paradigm. This work performs an extensive comparison across multiple languages and experimental conditions of character- and subword-level pretrained models (ByT5 and mT5, respectively) on NMT. We show the effectiveness of character-level modeling in translation, particularly in cases where fine-tuning data is limited. In our analysis, we show how character models’ gains in translation quality are reflected in better translations of orthographically similar words and rare words. While evaluating the importance of source texts in driving model predictions, we highlight word-level patterns within ByT5, suggesting an ability to modulate word-level and character-level information during generation. We conclude by assessing the efficiency tradeoff of byte models, suggesting their usage in non-time-critical scenarios to boost translation quality.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.22.pdf"
    },
    {
        "title": "Geographic Adaptation of Pretrained Language Models",
        "authors": [
            "Valentin Hofmann",
            "Goran Glavaš",
            "Nikola Ljubešić",
            "Janet B. Pierrehumbert",
            "Hinrich Schütze"
        ],
        "published": "2024",
        "summary": "While pretrained language models (PLMs) have been shown to possess a plethora of linguistic knowledge, the existing body of research has largely neglected extralinguistic knowledge, which is generally difficult to obtain by pretraining on text alone. Here, we contribute to closing this gap by examining geolinguistic knowledge, i.e., knowledge about geographic variation in language. We introduce geoadaptation, an intermediate training step that couples language modeling with geolocation prediction in a multi-task learning setup. We geoadapt four PLMs, covering language groups from three geographic areas, and evaluate them on five different tasks: fine-tuned (i.e., supervised) geolocation prediction, zero-shot (i.e., unsupervised) geolocation prediction, fine-tuned language identification, zero-shot language identification, and zero-shot prediction of dialect features. Geoadaptation is very successful at injecting geolinguistic knowledge into the PLMs: The geoadapted PLMs consistently outperform PLMs adapted using only language modeling (by especially wide margins on zero-shot prediction tasks), and we obtain new state-of-the-art results on two benchmarks for geolocation prediction and language identification. Furthermore, we show that the effectiveness of geoadaptation stems from its ability to geographically retrofit the representation space of the PLMs.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.23.pdf"
    },
    {
        "title": "ConvoSense: Overcoming Monotonous Commonsense Inferences for Conversational AI",
        "authors": [
            "Sarah E. Finch",
            "Jinho D. Choi"
        ],
        "published": "2024",
        "summary": "Mastering commonsense understanding and reasoning is a pivotal skill essential for conducting engaging conversations. While there have been several attempts to create datasets that facilitate commonsense inferences in dialogue contexts, existing datasets tend to lack in-depth details, restate information already present in the conversation, and often fail to capture the multifaceted nature of commonsense reasoning. In response to these limitations, we compile a new synthetic dataset for commonsense reasoning in dialogue contexts using GPT, ℂonvoSense, that boasts greater contextual novelty, offers a higher volume of inferences per example, and substantially enriches the detail conveyed by the inferences. Our dataset contains over 500,000 inferences across 12,000 dialogues with 10 popular inference types, which empowers the training of generative commonsense models for dialogue that are superior in producing plausible inferences with high novelty when compared to models trained on the previous datasets. To the best of our knowledge, ℂonvoSense is the first of its kind to provide such a multitude of novel inferences at such a large scale.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.26.pdf"
    },
    {
        "title": "Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies",
        "authors": [
            "Liangming Pan",
            "Michael Saxon",
            "Wenda Xu",
            "Deepak Nathani",
            "Xinyi Wang",
            "William Yang Wang"
        ],
        "published": "2024",
        "summary": "While large language models (LLMs) have shown remarkable effectiveness in various NLP tasks, they are still prone to issues such as hallucination, unfaithful reasoning, and toxicity. A promising approach to rectify these flaws is correcting LLMs with feedback, where the LLM itself is prompted or guided with feedback to fix problems in its own output. Techniques leveraging automated feedback—either produced by the LLM itself (self-correction) or some external system—are of particular interest as they make LLM-based solutions more practical and deployable with minimal human intervention. This paper provides an exhaustive review of the recent advances in correcting LLMs with automated feedback, categorizing them into training-time, generation-time, and post-hoc approaches. We also identify potential challenges and future directions in this emerging field.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.27.pdf"
    },
    {
        "title": "KoBBQ: Korean Bias Benchmark for Question Answering",
        "authors": [
            "Jiho Jin",
            "Jiseon Kim",
            "Nayeon Lee",
            "Haneul Yoo",
            "Alice Oh",
            "Hwaran Lee"
        ],
        "published": "2024",
        "summary": "Warning: This paper contains examples of stereotypes and biases. The Bias Benchmark for Question Answering (BBQ) is designed to evaluate social biases of language models (LMs), but it is not simple to adapt this benchmark to cultural contexts other than the US because social biases depend heavily on the cultural context. In this paper, we present KoBBQ, a Korean bias benchmark dataset, and we propose a general framework that addresses considerations for cultural adaptation of a dataset. Our framework includes partitioning the BBQ dataset into three classes—Simply-Transferred (can be used directly after cultural translation), Target-Modified (requires localization in target groups), and Sample-Removed (does not fit Korean culture)—and adding four new categories of bias specific to Korean culture. We conduct a large-scale survey to collect and validate the social biases and the targets of the biases that reflect the stereotypes in Korean culture. The resulting KoBBQ dataset comprises 268 templates and 76,048 samples across 12 categories of social bias. We use KoBBQ to measure the accuracy and bias scores of several state-of-the-art multilingual LMs. The results clearly show differences in the bias of LMs as measured by KoBBQ and a machine-translated version of BBQ, demonstrating the need for and utility of a well-constructed, culturally aware social bias benchmark.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.28.pdf"
    },
    {
        "title": "AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning",
        "authors": [
            "Han Zhou",
            "Xingchen Wan",
            "Ivan Vulić",
            "Anna Korhonen"
        ],
        "published": "2024",
        "summary": "Large pretrained language models are widely used in downstream NLP tasks via task- specific fine-tuning, but such procedures can be costly. Recently, Parameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task performance while updating much fewer parameters than full model fine-tuning (FFT). However, it is non-trivial to make informed design choices on the PEFT configurations, such as their architecture, the number of tunable parameters, and even the layers in which the PEFT modules are inserted. Consequently, it is highly likely that the current, manually designed configurations are suboptimal in terms of their performance-efficiency trade-off. Inspired by advances in neural architecture search, we propose AutoPEFT for automatic PEFT configuration selection: We first design an expressive configuration search space with multiple representative PEFT modules as building blocks. Using multi-objective Bayesian optimization in a low-cost setup, we then discover a Pareto-optimal set of configurations with strong performance-cost trade-offs across different numbers of parameters that are also highly transferable across different tasks. Empirically, on GLUE and SuperGLUE tasks, we show that AutoPEFT-discovered configurations significantly outperform existing PEFT methods and are on par or better than FFT without incurring substantial training efficiency costs.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.29.pdf"
    },
    {
        "title": "Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap",
        "authors": [
            "Michael Staniek",
            "Raphael Schumann",
            "Maike Züfle",
            "Stefan Riezler"
        ],
        "published": "2024",
        "summary": "We present Text-to-OverpassQL, a task designed to facilitate a natural language interface for querying geodata from OpenStreetMap (OSM). The Overpass Query Language (OverpassQL) allows users to formulate complex database queries and is widely adopted in the OSM ecosystem. Generating Overpass queries from natural language input serves multiple use-cases. It enables novice users to utilize OverpassQL without prior knowledge, assists experienced users with crafting advanced queries, and enables tool-augmented large language models to access information stored in the OSM database. In order to assess the performance of current sequence generation models on this task, we propose OverpassNL,1 a dataset of 8,352 queries with corresponding natural language inputs. We further introduce task specific evaluation metrics and ground the evaluation of the Text-to-OverpassQL task by executing the queries against the OSM database. We establish strong baselines by finetuning sequence-to-sequence models and adapting large language models with in-context examples. The detailed evaluation reveals strengths and weaknesses of the considered learning strategies, laying the foundations for further research into the Text-to-OverpassQL task.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.31.pdf"
    },
    {
        "title": "Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions",
        "authors": [
            "Jiahuan Li",
            "Hao Zhou",
            "Shujian Huang",
            "Shanbo Cheng",
            "Jiajun Chen"
        ],
        "published": "2024",
        "summary": "Large-scale pretrained language models (LLMs), such as ChatGPT and GPT4, have shown strong abilities in multilingual translation, without being explicitly trained on parallel corpora. It is intriguing how the LLMs obtain their ability to carry out translation instructions for different languages. In this paper, we present a detailed analysis by finetuning a multilingual pretrained language model, XGLM-7.5B, to perform multilingual translation following given instructions. Firstly, we show that multilingual LLMs have stronger translation abilities than previously demonstrated. For a certain language, the translation performance depends on its similarity to English and the amount of data used in the pretraining phase. Secondly, we find that LLMs’ ability to carry out translation instructions relies on the understanding of translation instructions and the alignment among different languages. With multilingual finetuning with translation instructions, LLMs could learn to perform the translation task well even for those language pairs unseen during the instruction tuning phase.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.32.pdf"
    },
    {
        "title": "Improving Probability-based Prompt Selection Through Unified Evaluation and Analysis",
        "authors": [
            "Sohee Yang",
            "Jonghyeon Kim",
            "Joel Jang",
            "Seonghyeon Ye",
            "Hyunji Lee",
            "Minjoon Seo"
        ],
        "published": "2024",
        "summary": "Previous work in prompt engineering for large language models has introduced different gradient-free probability-based prompt selection methods that aim to choose the optimal prompt among the candidates for a given task but have failed to provide a comprehensive and fair comparison between each other. In this paper, we propose a unified framework to interpret and evaluate the existing probability-based prompt selection methods by performing extensive experiments on 13 common and diverse NLP tasks. We find that each of the existing methods can be interpreted as some variant of the method that maximizes mutual information between the input and the predicted output (MI). Utilizing this finding, we develop several other combinatorial variants of MI and increase the effectiveness of the oracle prompt selection method from 87.79% to 94.98%, measured as the ratio of the performance of the selected prompt to that of the optimal oracle prompt. Furthermore, considering that all the methods rely on the output probability distribution of the model that might be biased, we propose a novel calibration method called Calibration by Marginalization (CBM) that is orthogonal to the existing methods and helps increase the prompt selection effectiveness of the best method to 96.85%, achieving 99.44% of the oracle prompt F1 without calibration.1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.37.pdf"
    },
    {
        "title": "Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering",
        "authors": [
            "Vaibhav Adlakha",
            "Parishad BehnamGhader",
            "Xing Han Lu",
            "Nicholas Meade",
            "Siva Reddy"
        ],
        "published": "2024",
        "summary": "Instruction-following models are attractive alternatives to fine-tuned approaches for question answering (QA). By simply prepending relevant documents and an instruction to their input, these models can be adapted to various information domains and tasks without additional training. However, these models tend to produce verbose responses with supplementary information, which makes traditional QA metrics like exact match (EM) and F1 unreliable for accurately quantifying model performance. In this work, we evaluate instruction-following models along two fronts: 1) how well they satisfy user’s information need (correctness), and 2) whether they disseminate information supported by the provided knowledge (faithfulness). Guided by human evaluation and analysis, we highlight the shortcomings of traditional metrics for both correctness and faithfulness and propose simple token-overlap metrics that correlate highly with human judgments. Our analysis reveals that for correctness, instruction-following models perform comparably to models specifically fine-tuned for that task. However, they struggle to accurately judge the relevance of the provided knowledge and often hallucinate in their responses. We hope our work encourages more holistic evaluation of instruction-following models for QA. Our code and human annotation data is available at https://github.com/McGill-NLP/instruct-qa.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.38.pdf"
    },
    {
        "title": "The Ethics of Automating Legal Actors",
        "authors": [
            "Josef Valvoda",
            "Alec Thompson",
            "Ryan Cotterell",
            "Simone Teufel"
        ],
        "published": "2024",
        "summary": "The introduction of large public legal datasets has brought about a renaissance in legal NLP. Many of these datasets are composed of legal judgments—the product of judges deciding cases. Since ML algorithms learn to model the data they are trained on, several legal NLP models are models of judges. While some have argued for the automation of judges, in this position piece, we argue that automating the role of the judge raises difficult ethical challenges, in particular for common law legal systems. Our argument follows from the social role of the judge in actively shaping the law, rather than merely applying it. Since current NLP models are too far away from having the facilities necessary for this task, they should not be used to automate judges. Furthermore, even in the case that the models could achieve human-level capabilities, there would still be remaining ethical concerns inherent in the automation of the legal process.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.39.pdf"
    },
    {
        "title": "Scope Ambiguities in Large Language Models",
        "authors": [
            "Gaurav Kamath",
            "Sebastian Schuster",
            "Sowmya Vajjala",
            "Siva Reddy"
        ],
        "published": "2024",
        "summary": "Sentences containing multiple semantic operators with overlapping scope often create ambiguities in interpretation, known as scope ambiguities. These ambiguities offer rich insights into the interaction between semantic structure and world knowledge in language processing. Despite this, there has been little research into how modern large language models treat them. In this paper, we investigate how different versions of certain autoregressive language models—GPT-2, GPT-3/3.5, Llama 2, and GPT-4—treat scope ambiguous sentences, and compare this with human judgments. We introduce novel datasets that contain a joint total of almost 1,000 unique scope-ambiguous sentences, containing interactions between a range of semantic operators, and annotated for human judgments. Using these datasets, we find evidence that several models (i) are sensitive to the meaning ambiguity in these sentences, in a way that patterns well with human judgments, and (ii) can successfully identify human-preferred readings at a high level of accuracy (over 90% in some cases).1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.41.pdf"
    },
    {
        "title": "Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias",
        "authors": [
            "Itay Itzhak",
            "Gabriel Stanovsky",
            "Nir Rosenfeld",
            "Yonatan Belinkov"
        ],
        "published": "2024",
        "summary": "Recent studies show that instruction tuning (IT) and reinforcement learning from human feedback (RLHF) improve the abilities of large language models (LMs) dramatically. While these tuning methods can help align models with human objectives and generate high-quality text, not much is known about their potential adverse effects. In this work, we investigate the effect of IT and RLHF on decision making and reasoning in LMs, focusing on three cognitive biases—the decoy effect, the certainty effect, and the belief bias—all of which are known to influence human decision-making and reasoning. Our findings highlight the presence of these biases in various models from the GPT-3, Mistral, and T5 families. Notably, we find a stronger presence of biases in models that have undergone instruction tuning, such as Flan-T5, Mistral-Instruct, GPT3.5, and GPT4. Our work constitutes a step toward comprehending cognitive biases in instruction-tuned LMs, which is crucial for the development of more reliable and unbiased language models.1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.43.pdf"
    },
    {
        "title": "Beyond Boundaries: A Human-like Approach for Question Answering over Structured and Unstructured Information Sources",
        "authors": [
            "Jens Lehmann",
            "Dhananjay Bhandiwad",
            "Preetam Gattogi",
            "Sahar Vahdati"
        ],
        "published": "2024",
        "summary": "Answering factual questions from heterogenous sources, such as graphs and text, is a key capacity of intelligent systems. Current approaches either (i) perform question answering over text and structured sources as separate pipelines followed by a merge step or (ii) provide an early integration, giving up the strengths of particular information sources. To solve this problem, we present “HumanIQ”, a method that teaches language models to dynamically combine retrieved information by imitating how humans use retrieval tools. Our approach couples a generic method for gathering human demonstrations of tool use with adaptive few-shot learning for tool augmented models. We show that HumanIQ confers significant benefits, including i) reducing the error rate of our strongest baseline (GPT-4) by over 50% across 3 benchmarks, (ii) improving human preference over responses from vanilla GPT-4 (45.3% wins, 46.7% ties, 8.0% loss), and (iii) outperforming numerous task-specific baselines.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.44.pdf"
    },
    {
        "title": "Comparing Humans and Large Language Models on an Experimental Protocol Inventory for Theory of Mind Evaluation (EPITOME)",
        "authors": [
            "Cameron R. Jones",
            "Sean Trott",
            "Benjamin Bergen"
        ],
        "published": "2024",
        "summary": "We address a growing debate about the extent to which large language models (LLMs) produce behavior consistent with Theory of Mind (ToM) in humans. We present EPITOME: a battery of six experiments that tap diverse ToM capacities, including belief attribution, emotional inference, and pragmatic reasoning. We elicit a performance baseline from human participants for each task. We use the dataset to ask whether distributional linguistic information learned by LLMs is sufficient to explain ToM in humans. We compare performance of five LLMs to a baseline of responses from human comprehenders. Results are mixed. LLMs display considerable sensitivity to mental states and match human performance in several tasks. Yet, they commit systematic errors in others, especially those requiring pragmatic reasoning on the basis of mental state information. Such uneven performance indicates that human-level ToM may require resources beyond distributional information.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.45.pdf"
    },
    {
        "title": "Revisiting Meta-evaluation for Grammatical Error Correction",
        "authors": [
            "Masamune Kobayashi",
            "Masato Mita",
            "Mamoru Komachi"
        ],
        "published": "2024",
        "summary": "Metrics are the foundation for automatic evaluation in grammatical error correction (GEC), with their evaluation of the metrics (meta-evaluation) relying on their correlation with human judgments. However, conventional meta-evaluations in English GEC encounter several challenges, including biases caused by inconsistencies in evaluation granularity and an outdated setup using classical systems. These problems can lead to misinterpretation of metrics and potentially hinder the applicability of GEC techniques. To address these issues, this paper proposes SEEDA, a new dataset for GEC meta-evaluation. SEEDA consists of corrections with human ratings along two different granularities: edit-based and sentence-based, covering 12 state-of-the-art systems including large language models, and two human corrections with different focuses. The results of improved correlations by aligning the granularity in the sentence-level meta-evaluation suggest that edit-based metrics may have been underestimated in existing studies. Furthermore, correlations of most metrics decrease when changing from classical to neural systems, indicating that traditional metrics are relatively poor at evaluating fluently corrected sentences with many edits.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.47.pdf"
    },
    {
        "title": "Decision-Oriented Dialogue for Human-AI Collaboration",
        "authors": [
            "Jessy Lin",
            "Nicholas Tomlin",
            "Jacob Andreas",
            "Jason Eisner"
        ],
        "published": "2024",
        "summary": "We describe a class of tasks called decision-oriented dialogues, in which AI assistants such as large language models (LMs) must collaborate with one or more humans via natural language to help them make complex decisions. We formalize three domains in which users face everyday decisions: (1) choosing an assignment of reviewers to conference papers, (2) planning a multi-step itinerary in a city, and (3) negotiating travel plans for a group of friends. In each of these settings, AI assistants and users have disparate abilities that they must combine to arrive at the best decision: Assistants can access and process large amounts of information, while users have preferences and constraints external to the system. For each task, we build a dialogue environment where agents receive a reward based on the quality of the final decision they reach. We evaluate LMs in self-play and in collaboration with humans and find that they fall short compared to human assistants, achieving much lower rewards despite engaging in longer dialogues. We highlight a number of challenges models face in decision-oriented dialogues, ranging from goal-directed behavior to reasoning and optimization, and release our environments as a testbed for future work.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.50.pdf"
    },
    {
        "title": "State of What Art? A Call for Multi-Prompt LLM Evaluation",
        "authors": [
            "Moran Mizrahi",
            "Guy Kaplan",
            "Dan Malkin",
            "Rotem Dror",
            "Dafna Shahaf",
            "Gabriel Stanovsky"
        ],
        "published": "2024",
        "summary": "Recent advances in LLMs have led to an abundance of evaluation benchmarks, which typically rely on a single instruction template per task. We create a large-scale collection of instruction paraphrases and comprehensively analyze the brittleness introduced by single-prompt evaluations across 6.5M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks. We find that different instruction templates lead to very different performance, both absolute and relative. Instead, we propose a set of diverse metrics on multiple instruction paraphrases, specifically tailored for different use cases (e.g., LLM vs. downstream development), ensuring a more reliable and meaningful assessment of LLM capabilities. We show that our metrics provide new insights into the strengths and limitations of current LLMs.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.52.pdf"
    },
    {
        "title": "xcomet: Transparent Machine Translation Evaluation through Fine-grained Error Detection",
        "authors": [
            "Nuno M. Guerreiro",
            "Ricardo Rei",
            "Daan van Stigt",
            "Luisa Coheur",
            "Pierre Colombo",
            "André F. T. Martins"
        ],
        "published": "2024",
        "summary": "Widely used learned metrics for machine translation evaluation, such as Comet and Bleurt, estimate the quality of a translation hypothesis by providing a single sentence-level score. As such, they offer little insight into translation errors (e.g., what are the errors and what is their severity). On the other hand, generative large language models (LLMs) are amplifying the adoption of more granular strategies to evaluation, attempting to detail and categorize translation errors. In this work, we introduce xcomet, an open-source learned metric designed to bridge the gap between these approaches. xcomet integrates both sentence-level evaluation and error span detection capabilities, exhibiting state-of-the-art performance across all types of evaluation (sentence-level, system-level, and error span detection). Moreover, it does so while highlighting and categorizing error spans, thus enriching the quality assessment. We also provide a robustness analysis with stress tests, and show that xcomet is largely capable of identifying localized critical errors and hallucinations.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.54.pdf"
    },
    {
        "title": "Do LLMs Exhibit Human-like Response Biases? A Case Study in Survey Design",
        "authors": [
            "Lindia Tjuatja",
            "Valerie Chen",
            "Tongshuang Wu",
            "Ameet Talwalkwar",
            "Graham Neubig"
        ],
        "published": "2024",
        "summary": "One widely cited barrier to the adoption of LLMs as proxies for humans in subjective tasks is their sensitivity to prompt wording—but interestingly, humans also display sensitivities to instruction changes in the form of response biases. We investigate the extent to which LLMs reflect human response biases, if at all. We look to survey design, where human response biases caused by changes in the wordings of “prompts” have been extensively explored in social psychology literature. Drawing from these works, we design a dataset and framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior, particularly in models that have undergone RLHF. Furthermore, even if a model shows a significant change in the same direction as humans, we find that they are sensitive to perturbations that do not elicit significant changes in humans. These results highlight the pitfalls of using LLMs as human proxies, and underscore the need for finer-grained characterizations of model behavior.1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.56.pdf"
    },
    {
        "title": "Do Multi-Document Summarization Models Synthesize?",
        "authors": [
            "Jay DeYoung",
            "Stephanie C. Martinez",
            "Iain J. Marshall",
            "Byron C. Wallace"
        ],
        "published": "2024",
        "summary": "Multi-document summarization entails producing concise synopses of collections of inputs. For some applications, the synopsis should accurately synthesize inputs with respect to a key aspect, e.g., a synopsis of film reviews written about a particular movie should reflect the average critic consensus. As a more consequential example, narrative summaries that accompany biomedical systematic reviews of clinical trial results should accurately summarize the potentially conflicting results from individual trials. In this paper we ask: To what extent do modern multi-document summarization models implicitly perform this sort of synthesis? We run experiments over opinion and evidence synthesis datasets using a suite of summarization models, from fine-tuned transformers to GPT-4. We find that existing models partially perform synthesis, but imperfectly: Even the best performing models are over-sensitive to changes in input ordering and under-sensitive to changes in input compositions (e.g., ratio of positive to negative reviews). We propose a simple, general, effective method for improving model synthesis capabilities by generating an explicitly diverse set of candidate outputs, and then selecting from these the string best aligned with the expected aggregate measure for the inputs, or abstaining when the model produces no good candidate.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.58.pdf"
    },
    {
        "title": "ARN: Analogical Reasoning on Narratives",
        "authors": [
            "Zhivar Sourati",
            "Filip Ilievski",
            "Pia Sommerauer",
            "Yifan Jiang"
        ],
        "published": "2024",
        "summary": "As a core cognitive skill that enables the transferability of information across domains, analogical reasoning has been extensively studied for both humans and computational models. However, while cognitive theories of analogy often focus on narratives and study the distinction between surface, relational, and system similarities, existing work in natural language processing has a narrower focus as far as relational analogies between word pairs. This gap brings a natural question: can state-of-the-art large language models (LLMs) detect system analogies between narratives? To gain insight into this question and extend word-based relational analogies to relational system analogies, we devise a comprehensive computational framework that operationalizes dominant theories of analogy, using narrative elements to create surface and system mappings. Leveraging the interplay between these mappings, we create a binary task and benchmark for Analogical Reasoning on Narratives (ARN), covering four categories of far (cross-domain)/near (within-domain) analogies and disanalogies. We show that while all LLMs can largely recognize near analogies, even the largest ones struggle with far analogies in a zero-shot setting, with GPT4.0 scoring below random. Guiding the models through solved examples and Chain-of-Thought reasoning enhances their analogical reasoning ability. Yet, since even in the few-shot setting, the best model only performs halfway between random and humans, ARN opens exciting directions for computational analogical reasoners.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.59.pdf"
    },
    {
        "title": "Are Language Models More Like Libraries or Like Librarians? Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs",
        "authors": [
            "Harvey Lederman",
            "Kyle Mahowald"
        ],
        "published": "2024",
        "summary": "Are LLMs cultural technologies like photocopiers or printing presses, which transmit information but cannot create new content? A challenge for this idea, which we call bibliotechnism, is that LLMs generate novel text. We begin with a defense of bibliotechnism, showing how even novel text may inherit its meaning from original human-generated text. We then argue that bibliotechnism faces an independent challenge from examples in which LLMs generate novel reference, using new names to refer to new entities. Such examples could be explained if LLMs were not cultural technologies but had beliefs, desires, and intentions. According to interpretationism in the philosophy of mind, a system has such attitudes if and only if its behavior is well explained by the hypothesis that it does. Interpretationists may hold that LLMs have attitudes, and thus have a simple solution to the novel reference problem. We emphasize, however, that interpretationism is compatible with very simple creatures having attitudes and differs sharply from views that presuppose these attitudes require consciousness, sentience, or intelligence (topics about which we make no claims).",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.60.pdf"
    },
    {
        "title": "Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation",
        "authors": [
            "Cyril Chhun",
            "Fabian M. Suchanek",
            "Chloé Clavel"
        ],
        "published": "2024",
        "summary": "Storytelling is an integral part of human experience and plays a crucial role in social interactions. Thus, Automatic Story Evaluation (ASE) and Generation (ASG) could benefit society in multiple ways, but they are challenging tasks which require high-level human abilities such as creativity, reasoning, and deep understanding. Meanwhile, Large Language Models (LLMs) now achieve state-of-the-art performance on many NLP tasks. In this paper, we study whether LLMs can be used as substitutes for human annotators for ASE. We perform an extensive analysis of the correlations between LLM ratings, other automatic measures, and human annotations, and we explore the influence of prompting on the results and the explainability of LLM behaviour. Most notably, we find that LLMs outperform current automatic measures for system-level evaluation but still struggle at providing satisfactory explanations for their answers.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.62.pdf"
    },
    {
        "title": "How Often Are Errors in Natural Language Reasoning Due to Paraphrastic Variability?",
        "authors": [
            "Neha Srikanth",
            "Marine Carpuat",
            "Rachel Rudinger"
        ],
        "published": "2024",
        "summary": "Large language models have been shown to behave inconsistently in response to meaning-preserving paraphrastic inputs. At the same time, researchers evaluate the knowledge and reasoning abilities of these models with test evaluations that do not disaggregate the effect of paraphrastic variability on performance. We propose a metric, PC, for evaluating the paraphrastic consistency of natural language reasoning models based on the probability of a model achieving the same correctness on two paraphrases of the same problem. We mathematically connect this metric to the proportion of a model’s variance in correctness attributable to paraphrasing. To estimate PC, we collect ParaNlu, a dataset of 7,782 human-written and validated paraphrased reasoning problems constructed on top of existing benchmark datasets for defeasible and abductive natural language inference.1 Using ParaNlu, we measure the paraphrastic consistency of several model classes and show that consistency dramatically increases with pretraining but not fine-tuning. All models tested exhibited room for improvement in paraphrastic consistency.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.63.pdf"
    },
    {
        "title": "Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization",
        "authors": [
            "George Chrysostomou",
            "Zhixue Zhao",
            "Miles Williams",
            "Nikolaos Aletras"
        ],
        "published": "2024",
        "summary": "Despite the remarkable performance of generative large language models (LLMs) on abstractive summarization, they face two significant challenges: their considerable size and tendency to hallucinate. Hallucinations are concerning because they erode reliability and raise safety issues. Pruning is a technique that reduces model size by removing redundant weights, enabling more efficient sparse inference. Pruned models yield downstream task performance comparable to the original, making them ideal alternatives when operating on a limited budget. However, the effect that pruning has upon hallucinations in abstractive summarization with LLMs has yet to be explored. In this paper, we provide an extensive empirical study across five summarization datasets, two state-of-the-art pruning methods, and five instruction-tuned LLMs. Surprisingly, we find that hallucinations are less prevalent from pruned LLMs than the original models. Our analysis suggests that pruned models tend to depend more on the source document for summary generation. This leads to a higher lexical overlap between the generated summary and the source document, which could be a reason for the reduction in hallucination risk.1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.64.pdf"
    },
    {
        "title": "Retrieval-Pretrained Transformer: Long-range Language Modeling with Self-retrieval",
        "authors": [
            "Ohad Rubin",
            "Jonathan Berant"
        ],
        "published": "2024",
        "summary": "Retrieval-augmented language models (LMs) have received much attention recently. However, typically the retriever is not trained jointly as a native component of the LM, but added post-hoc to an already-pretrained LM, which limits the ability of the LM and the retriever to adapt to one another. In this work, we propose the Retrieval-Pretrained Transformer (RPT), an architecture and training procedure for jointly training a retrieval-augmented LM from scratch and applying it to the task of modeling long texts. Given a recently generated text chunk in a long document, the LM computes query representations, which are then used to retrieve earlier chunks in the document, located potentially tens of thousands of tokens before. Information from retrieved chunks is fused into the LM representations to predict the next target chunk. We train the retriever component with a semantic objective, where the goal is to retrieve chunks that increase the probability of the next chunk, according to a reference LM. We evaluate RPT on four long-range language modeling tasks, spanning books, code, and mathematical writing, and demonstrate that RPT improves retrieval quality and subsequently perplexity across the board compared to strong baselines.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.66.pdf"
    },
    {
        "title": "Retrieval-style In-context Learning for Few-shot Hierarchical Text Classification",
        "authors": [
            "Huiyao Chen",
            "Yu Zhao",
            "Zulong Chen",
            "Mengjia Wang",
            "Liangyue Li",
            "Meishan Zhang",
            "Min Zhang"
        ],
        "published": "2024",
        "summary": "Hierarchical text classification (HTC) is an important task with broad applications, and few-shot HTC has gained increasing interest recently. While in-context learning (ICL) with large language models (LLMs) has achieved significant success in few-shot learning, it is not as effective for HTC because of the expansive hierarchical label sets and extremely ambiguous labels. In this work, we introduce the first ICL-based framework with LLM for few-shot HTC. We exploit a retrieval database to identify relevant demonstrations, and an iterative policy to manage multi-layer hierarchical labels. Particularly, we equip the retrieval database with HTC label-aware representations for the input texts, which is achieved by continual training on a pretrained language model with masked language modeling (MLM), layer-wise classification (CLS, specifically for HTC), and a novel divergent contrastive learning (DCL, mainly for adjacent semantically similar labels) objective. Experimental results on three benchmark datasets demonstrate superior performance of our method, and we can achieve state-of-the-art results in few-shot HTC.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.67.pdf"
    },
    {
        "title": "Do Vision and Language Models Share Concepts? A Vector Space Alignment Study",
        "authors": [
            "Jiaang Li",
            "Yova Kementchedjhieva",
            "Constanza Fierro",
            "Anders Søgaard"
        ],
        "published": "2024",
        "summary": "Large-scale pretrained language models (LMs) are said to “lack the ability to connect utterances to the world” (Bender and Koller, 2020), because they do not have “mental models of the world” (Mitchell and Krakauer, 2023). If so, one would expect LM representations to be unrelated to representations induced by vision models. We present an empirical evaluation across four families of LMs (BERT, GPT-2, OPT, and LLaMA-2) and three vision model architectures (ResNet, SegFormer, and MAE). Our experiments show that LMs partially converge towards representations isomorphic to those of vision models, subject to dispersion, polysemy, and frequency. This has important implications for both multi-modal processing and the LM understanding debate (Mitchell and Krakauer, 2023).1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.68.pdf"
    },
    {
        "title": "Assessing the Role of Context in Chat Translation Evaluation: Is Context Helpful and Under What Conditions?",
        "authors": [
            "Sweta Agrawal",
            "Amin Farajian",
            "Patrick Fernandes",
            "Ricardo Rei",
            "André F. T. Martins"
        ],
        "published": "2024",
        "summary": "Despite the recent success of automatic metrics for assessing translation quality, their application in evaluating the quality of machine-translated chats has been limited. Unlike more structured texts like news, chat conversations are often unstructured, short, and heavily reliant on contextual information. This poses questions about the reliability of existing sentence-level metrics in this domain as well as the role of context in assessing the translation quality. Motivated by this, we conduct a meta-evaluation of existing automatic metrics, primarily designed for structured domains such as news, to assess the quality of machine-translated chats. We find that reference-free metrics lag behind reference-based ones, especially when evaluating translation quality in out-of-English settings. We then investigate how incorporating conversational contextual information in these metrics for sentence-level evaluation affects their performance. Our findings show that augmenting neural learned metrics with contextual information helps improve correlation with human judgments in the reference-free scenario and when evaluating translations in out-of-English settings. Finally, we propose a new evaluation metric, Context-MQM, that utilizes bilingual context with a large language model (LLM) and further validate that adding context helps even for LLM-based evaluation metrics.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.69.pdf"
    },
    {
        "title": "Reading Subtext: Evaluating Large Language Models on Short Story Summarization with Writers",
        "authors": [
            "Melanie Subbiah",
            "Sean Zhang",
            "Lydia B. Chilton",
            "Kathleen McKeown"
        ],
        "published": "2024",
        "summary": "We evaluate recent Large Language Models (LLMs) on the challenging task of summarizing short stories, which can be lengthy, and include nuanced subtext or scrambled timelines. Importantly, we work directly with authors to ensure that the stories have not been shared online (and therefore are unseen by the models), and to obtain informed evaluations of summary quality using judgments from the authors themselves. Through quantitative and qualitative analysis grounded in narrative theory, we compare GPT-4, Claude-2.1, and LLama-2-70B. We find that all three models make faithfulness mistakes in over 50% of summaries and struggle with specificity and interpretation of difficult subtext. We additionally demonstrate that LLM ratings and other automatic metrics for summary quality do not correlate well with the quality ratings from the writers.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.71.pdf"
    },
    {
        "title": "L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models",
        "authors": [
            "Ansong Ni",
            "Pengcheng Yin",
            "Yilun Zhao",
            "Martin Riddell",
            "Troy Feng",
            "Rui Shen",
            "Stephen Yin",
            "Ye Liu",
            "Semih Yavuz",
            "Caiming Xiong",
            "Shafiq Joty",
            "Yingbo Zhou",
            "Dragomir Radev",
            "Arman Cohan",
            "Arman Cohan"
        ],
        "published": "2024",
        "summary": "Recently, large language models (LLMs), especially those that are pretrained on code, have demonstrated strong capabilities in generating programs from natural language inputs. Despite promising results, there is a notable lack of a comprehensive evaluation of these models’ language-to-code generation capabilities. Existing studies often focus on specific tasks, model architectures, or learning paradigms, leading to a fragmented understanding of the overall landscape. In this work, we present L2CEval, a systematic evaluation of the language-to-code generation capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing, math reasoning, and Python programming, analyzing the factors that potentially affect their performance, such as model size, pretraining data, instruction tuning, and different prompting methods. In addition, we assess confidence calibration, and conduct human evaluations to identify typical failures across different tasks and models. L2CEval offers a comprehensive understanding of the capabilities and limitations of LLMs in language-to-code generation. We release the evaluation framework1 and all model outputs, hoping to lay the groundwork for further future research. All future evaluations (e.g., LLaMA-3, StarCoder2, etc) will be updated on the project website: https://l2c-eval.github.io/.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.72.pdf"
    },
    {
        "title": "NoviCode: Generating Programs from Natural Language Utterances by Novices",
        "authors": [
            "Asaf Achi Mordechai",
            "Yoav Goldberg",
            "Reut Tsarfaty"
        ],
        "published": "2024",
        "summary": "Current Text-to-Code models demonstrate impressive capabilities in generating executable code from natural language snippets. However, current studies focus on technical instructions and programmer-oriented language, and it is an open question whether these models can effectively translate natural language descriptions given by non-technical users and express complex goals, to an executable program that contains an intricate flow—composed of API access and control structures as loops, conditions, and sequences. To unlock the challenge of generating a complete program from a plain non-technical description we present NoviCode, a novel NL Programming task, which takes as input an API and a natural language description by a novice non-programmer, and provides an executable program as output. To assess the efficacy of models on this task, we provide a novel benchmark accompanied by test suites wherein the generated program code is assessed not according to their form, but according to their functional execution. Our experiments show that, first, NoviCode is indeed a challenging task in the code synthesis domain, and that generating complex code from non-technical instructions goes beyond the current Text-to-Code paradigm. Second, we show that a novel approach wherein we align the NL utterances with the compositional hierarchical structure of the code, greatly enhances the performance of LLMs on this task, compared with the end-to-end Text-to-Code counterparts.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.73.pdf"
    },
    {
        "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
        "authors": [
            "Tyler A. Chang",
            "Zhuowen Tu",
            "Benjamin K. Bergen"
        ],
        "published": "2024",
        "summary": "How do language models learn to make predictions during pre-training? To study this, we extract learning curves from five autoregressive English language model pre-training runs, for 1M unseen tokens in context. We observe that the language models generate short repetitive phrases before learning to generate longer and more coherent text. We also find that individual tokens often exhibit sudden increases or decreases in loss that are surprisingly consistent across pre-training runs. To better understand these fluctuations, we quantify the final surprisal, within-run variability, age of acquisition, forgettability, and cross-run variability of learning curves for individual tokens in context. More frequent tokens reach lower final surprisals, exhibit less variability within and across pre-training runs, are learned earlier, and are less likely to be “forgotten” during pre-training. Higher n-gram probabilities further accentuate these effects. Independent of the target token, shorter and more frequent contexts correlate with marginally more stable and quickly acquired predictions. Based on our results, we argue for the existence of sequential learning dependencies between different model capabilities, and we characterize language model learning as early n-gram learning before gradual refinement of tail n-gram predictions.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.74.pdf"
    },
    {
        "title": "Beyond Prompt Brittleness: Evaluating the Reliability and Consistency of Political Worldviews in LLMs",
        "authors": [
            "Tanise Ceron",
            "Neele Falk",
            "Ana Barić",
            "Dmitry Nikolaev",
            "Sebastian Padó"
        ],
        "published": "2024",
        "summary": "Due to the widespread use of large language models (LLMs), we need to understand whether they embed a specific “worldview” and what these views reflect. Recent studies report that, prompted with political questionnaires, LLMs show left-liberal leanings (Feng et al., 2023; Motoki et al., 2024). However, it is as yet unclear whether these leanings are reliable (robust to prompt variations) and whether the leaning is consistent across policies and political leaning. We propose a series of tests which assess the reliability and consistency of LLMs’ stances on political statements based on a dataset of voting-advice questionnaires collected from seven EU countries and annotated for policy issues. We study LLMs ranging in size from 7B to 70B parameters and find that their reliability increases with parameter count. Larger models show overall stronger alignment with left-leaning parties but differ among policy programs: They show a (left-wing) positive stance towards environment protection, social welfare state, and liberal society but also (right-wing) law and order, with no consistent preferences in the areas of foreign policy and migration.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.76.pdf"
    },
    {
        "title": "When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs",
        "authors": [
            "Ryo Kamoi",
            "Yusen Zhang",
            "Nan Zhang",
            "Jiawei Han",
            "Rui Zhang"
        ],
        "published": "2024",
        "summary": "Self-correction is an approach to improving responses from large language models (LLMs) by refining the responses using LLMs during inference. Prior work has proposed various self-correction frameworks using different sources of feedback, including self-evaluation and external feedback. However, there is still no consensus on the question of when LLMs can correct their own mistakes, as recent studies also report negative results. In this work, we critically survey broad papers and discuss the conditions required for successful self-correction. We first find that prior studies often do not define their research questions in detail and involve impractical frameworks or unfair evaluations that over-evaluate self-correction. To tackle these issues, we categorize research questions in self-correction research and provide a checklist for designing appropriate experiments. Our critical survey based on the newly categorized research questions shows that (1) no prior work demonstrates successful self-correction with feedback from prompted LLMs, except for studies in tasks that are exceptionally suited for self-correction, (2) self-correction works well in tasks that can use reliable external feedback, and (3) large-scale fine-tuning enables self-correction.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.78.pdf"
    },
    {
        "title": "Interactive Machine Teaching by Labeling Rules and Instances",
        "authors": [
            "Giannis Karamanolakis",
            "Daniel Hsu",
            "Luis Gravaano"
        ],
        "published": "2024",
        "summary": "Weakly supervised learning aims to reduce the cost of labeling data by using expert-designed labeling rules. However, existing methods require experts to design effective rules in a single shot, which is difficult in the absence of proper guidance and tooling. Therefore, it is still an open question whether experts should spend their limited time writing rules or instead providing instance labels via active learning. In this paper, we investigate how to exploit an expert’s limited time to create effective supervision. First, to develop practical guidelines for rule creation, we conduct an exploratory analysis of diverse collections of existing expert-designed rules and find that rule precision is more important than coverage across datasets. Second, we compare rule creation to individual instance labeling via active learning and demonstrate the importance of both across 6 datasets. Third, we propose an interactive learning framework, INTERVAL, that achieves efficiency by automatically extracting candidate rules based on rich patterns (e.g., by prompting a language model), and effectiveness by soliciting expert feedback on both candidate rules and individual instances. Across 6 datasets, INTERVAL outperforms state-of-the-art weakly supervised approaches by 7% in F1. Furthermore, it requires as few as 10 queries for expert feedback to reach F1 values that existing active learning methods cannot match even with 100 queries.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.79.pdf",
        "source": "tacl2024"
    },
    {
        "title": "Conformal Prediction for Natural Language Processing: A Survey",
        "authors": [
            "Margarida Campos",
            "António Farinhas",
            "Chrysoula Zerva",
            "Mário A. T. Figueiredo",
            "André F. T. Martins"
        ],
        "published": "2024",
        "summary": "The rapid proliferation of large language models and natural language processing (NLP) applications creates a crucial need for uncertainty quantification to mitigate risks such as Hallucinations and to enhance decision-making reliability in critical applications. Conformal prediction is emerging as a theoretically sound and practically useful framework, combining flexibility with strong statistical guarantees. Its model-agnostic and distribution-free nature makes it particularly promising to address the current shortcomings of NLP systems that stem from the absence of uncertainty quantification. This paper provides a comprehensive survey of conformal prediction techniques, their guarantees, and existing applications in NLP, pointing to directions for future research and open challenges.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.82.pdf",
        "source": "tacl2024"
    },
    {
        "title": "FINCH: Prompt-guided Key-Value Cache Compression for Large Language Models",
        "authors": [
            "Giulio Corallo",
            "Paolo Papotti"
        ],
        "published": "2024",
        "summary": "Recent large language model applications, such as Retrieval-Augmented Generation and chatbots, have led to an increased need to process longer input contexts. However, this requirement is hampered by inherent limitations. Architecturally, models are constrained by a context window defined during training. Additionally, processing extensive texts requires substantial GPU memory. We propose a novel approach, Finch, to compress the input context by leveraging the pre-trained model weights of the self-attention. Given a prompt and a long text, Finch iteratively identifies the most relevant Key (K) and Value (V) pairs over chunks of the text conditioned on the prompt. Only such pairs are stored in the KV cache, which, within the space constrained by the context window, ultimately contains a compressed version of the long text. Our proposal enables models to consume large inputs even with high compression (up to 93x) while preserving semantic integrity without the need for fine-tuning.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.83.pdf",
        "source": "tacl2024"
    },
    {
        "title": "Hierarchical Indexing for Retrieval-Augmented Opinion Summarization",
        "authors": [
            "Tom Hosking",
            "Hao Tang",
            "Mirella Lapata"
        ],
        "published": "2024",
        "summary": "We propose a method for unsupervised abstractive opinion summarization, that combines the attributability and scalability of extractive approaches with the coherence and fluency of Large Language Models (LLMs). Our method, HIRO, learns an index structure that maps sentences to a path through a semantically organized discrete hierarchy. At inference time, we populate the index and use it to identify and retrieve clusters of sentences containing popular opinions from input reviews. Then, we use a pretrained LLM to generate a readable summary that is grounded in these extracted evidential clusters. The modularity of our approach allows us to evaluate its efficacy at each stage. We show that HIRO learns an encoding space that is more semantically structured than prior work, and generates summaries that are more representative of the opinions in the input reviews. Human evaluation confirms that HIRO generates significantly more coherent, detailed, and accurate summaries.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.84.pdf",
        "source": "tacl2024"
    },
    {
        "title": "A Survey on Model Compression for Large Language Models",
        "authors": [
            "Xunyu Zhu",
            "Jian Li",
            "Yong Liu",
            "Can Ma",
            "Weiping Wang"
        ],
        "published": "2024",
        "summary": "Large Language Models (LLMs) have transformed natural language processing tasks successfully. Yet, their large size and high computational needs pose challenges for practical use, especially in resource-limited settings. Model compression has emerged as a key research area to address these challenges. This paper presents a survey of model compression techniques for LLMs. We cover methods like quantization, pruning, and knowledge distillation, highlighting recent advancements. We also discuss benchmarking strategies and evaluation metrics crucial for assessing compressed LLMs. This survey offers valuable insights for researchers and practitioners, aiming to enhance efficiency and real-world applicability of LLMs while laying a foundation for future advancements.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.85.pdf",
        "source": "tacl2024"
    },
    {
        "title": "Rescue Conversations from Dead-ends: Efficient Exploration for Task-oriented Dialogue Policy Optimization",
        "authors": [
            "Yangyang Zhao",
            "Mehdi Dastani",
            "Jinchuan Long",
            "Zhenyu Wang",
            "Shihan Wang"
        ],
        "published": "2024",
        "summary": "Training a task-oriented dialogue policy using deep reinforcement learning is promising but requires extensive environment exploration. The amount of wasted invalid exploration makes policy learning inefficient. In this paper, we define and argue that dead-end states are important reasons for invalid exploration. When a conversation enters a dead-end state, regardless of the actions taken afterward, it will continue in a dead-end trajectory until the agent reaches a termination state or maximum turn. We propose a Dead-end Detection and Resurrection (DDR) method that detects dead-end states in an efficient manner and provides a rescue action to guide and correct the exploration direction. To prevent dialogue policies from repeating errors, DDR also performs dialogue data augmentation by adding relevant experiences that include dead-end states and penalties into the experience pool. We first validate the dead-end detection reliability and then demonstrate the effectiveness and generality of the method across various domains through experiments on four public dialogue datasets.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.86.pdf",
        "source": "tacl2024"
    },
    {
        "title": "Filtered Corpus Training (FiCT) Shows that Language Models Can Generalize from Indirect Evidence",
        "authors": [
            "Abhinav Patil",
            "Jaap Jumelet",
            "Yu Ying Chiu",
            "Andy Lapastora",
            "Peter Shen",
            "Lexie Wang",
            "Clevis Willrich",
            "Shane Steinert-Threlkeld"
        ],
        "published": "2024",
        "summary": "This paper introduces Filtered Corpus Training, a method that trains language models (LMs) on corpora with certain linguistic constructions filtered out from the training data, and uses it to measure the ability of LMs to perform linguistic generalization on the basis of indirect evidence. We apply the method to both LSTM and Transformer LMs (of roughly comparable size), developing filtered corpora that target a wide range of linguistic phenomena. Our results show that while transformers are better qua LMs (as measured by perplexity), both models perform equally and surprisingly well on linguistic generalization measures, suggesting that they are capable of generalizing from indirect evidence.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.87.pdf",
        "source": "tacl2024"
    },
    {
        "title": "Holmes ⌕ A Benchmark to Assess the Linguistic Competence of Language Models",
        "authors": [
            "Andreas Waldis",
            "Yotam Perlitz",
            "Leshem Choshen",
            "Yufang Hou",
            "Iryna Gurevych"
        ],
        "published": "2024",
        "summary": "We introduce Holmes, a new benchmark designed to assess language models’ (LMs’) linguistic competence—their unconscious understanding of linguistic phenomena. Specifically, we use classifier-based probing to examine LMs’ internal representations regarding distinct linguistic phenomena (e.g., part-of-speech tagging). As a result, we meet recent calls to disentangle LMs’ linguistic competence from other cognitive abilities, such as following instructions in prompting-based evaluations. Composing Holmes, we review over 270 probing studies and include more than 200 datasets to assess syntax, morphology, semantics, reasoning, and discourse phenomena. Analyzing over 50 LMs reveals that, aligned with known trends, their linguistic competence correlates with model size. However, surprisingly, model architecture and instruction tuning also significantly influence performance, particularly in morphology and syntax. Finally, we propose FlashHolmes, a streamlined version that reduces the computation load while maintaining high-ranking precision.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.88.pdf",
        "source": "tacl2024"
    },
    {
        "title": "TabVer: Tabular Fact Verification with Natural Logic",
        "authors": [
            "Rami Aly",
            "Andreas Vlachos"
        ],
        "published": "2024",
        "summary": "Fact verification on tabular evidence incentivizes the use of symbolic reasoning models where a logical form is constructed (e.g., a LISP-style program), providing greater verifiability than fully neural approaches. However, these logical forms typically rely on well-formed tables, restricting their use in many scenarios. An emerging symbolic reasoning paradigm for textual evidence focuses on natural logic inference, which constructs proofs by modeling set-theoretic relations between a claim and its evidence in natural language. This approach provides flexibility and transparency but is less compatible with tabular evidence since the relations do not extend to arithmetic functions. We propose a set-theoretic interpretation of numerals and arithmetic functions in the context of natural logic, enabling the integration of arithmetic expressions in deterministic proofs. We leverage large language models to generate arithmetic expressions by generating questions about salient parts of a claim which are answered by executing appropriate functions on tables. In a few-shot setting on FEVEROUS, we achieve an accuracy of 71.4, outperforming both fully neural and symbolic reasoning models by 3.4 points. When evaluated on TabFact without any further training, our method remains competitive with an accuracy lead of 0.5 points.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.89.pdf",
        "source": "tacl2024"
    },
    {
        "title": "Toward Robust RALMs: Revealing the Impact of Imperfect Retrieval on Retrieval-Augmented Language Models",
        "authors": [
            "Seong-Il Park",
            "Jay-Yoon Lee"
        ],
        "published": "2024",
        "summary": "Retrieval Augmented Language Models (RALMs) have gained significant attention for their ability to generate accurate answers and improve efficiency. However, RALMs are inherently vulnerable to imperfect information due to their reliance on the imperfect retriever or knowledge source. We identify three common scenarios—unanswerable, adversarial, conflicting—where retrieved document sets can confuse RALMs with plausible real-world examples. We present the first comprehensive investigation to assess how well RALMs detect and handle such problematic scenarios. Among these scenarios, to systematically examine adversarial robustness we propose a new adversarial attack method, Generative model-based ADVersarial attack (GenADV) and a novel metric Robustness under Additional Document (RAD). Our findings reveal that RALMs often fail to identify the unanswerability or contradiction of a document set, which frequently leads to hallucinations. Moreover, we show that the addition of an adversary significantly degrades RALM’s performance, with the model becoming even more vulnerable when the two scenarios overlap (adversarial+ unanswerable). Our research identifies critical areas for assessing and enhancing the robustness of RALMs, laying the foundation for the development of more robust models.1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.91.pdf",
        "source": "tacl2024"
    },
    {
        "title": "IndoCulture: Exploring Geographically Influenced Cultural Commonsense Reasoning Across Eleven Indonesian Provinces",
        "authors": [
            "Fajri Koto",
            "Rahmad Mahendra",
            "Nurul Aisyah",
            "Timothy Baldwin"
        ],
        "published": "2024",
        "summary": "Although commonsense reasoning is greatly shaped by cultural and geographical factors, previous studies have predominantly centered on cultures grounded in the English language, potentially resulting in an Anglocentric bias. In this paper, we introduce IndoCulture, aimed at understanding the influence of geographical factors on language model reasoning ability, with a specific emphasis on the diverse cultures found within eleven Indonesian provinces. In contrast to prior work that has relied on templates (Yin et al., 2022) and online scrapping (Fung et al., 2024), we create IndoCulture by asking local people to manually develop a cultural context and plausible options, across a set of predefined topics. Evaluation of 27 language models reveals several insights: (1) the open-weight Llama–3 is competitive with GPT–4, while other open-weight models struggle, with accuracies below 50%; (2) there is a general pattern of models generally performing better for some provinces, such as Bali and West Java, and less well for others; and (3) the inclusion of location context enhances performance, especially for larger models like GPT–4, emphasizing the significance of geographical context in commonsense reasoning.1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.92.pdf",
        "source": "tacl2024"
    },
    {
        "title": "Deuce: Dual-diversity Enhancement and Uncertainty-awareness for Cold-start Active Learning",
        "authors": [
            "Jiaxin Guo",
            "C. L. Philip Chen",
            "Shuzhen Li",
            "Tong Zhang"
        ],
        "published": "2024",
        "summary": "Cold-start active learning (CSAL) selects valuable instances from an unlabeled dataset for manual annotation. It provides high-quality data at a low annotation cost for label-scarce text classification. However, existing CSAL methods overlook weak classes and hard representative examples, resulting in biased learning. To address these issues, this paper proposes a novel dual-diversity enhancing and uncertainty-aware (Deuce) framework for CSAL. Specifically, Deuce leverages a pretrained language model (PLM) to efficiently extract textual representations, class predictions, and predictive uncertainty. Then, it constructs a Dual-Neighbor Graph (DNG) to combine information on both textual diversity and class diversity, ensuring a balanced data distribution. It further propagates uncertainty information via density-based clustering to select hard representative instances. Deuce performs well in selecting class-balanced and hard representative data by dual-diversity and informativeness. Experiments on six NLP datasets demonstrate the superiority and efficiency of Deuce.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.94.pdf",
        "source": "tacl2024"
    },
    {
        "title": "Robust Pronoun Fidelity with English LLMs: Are they Reasoning, Repeating, or Just Biased?",
        "authors": [
            "Vagrant Gautam",
            "Eileen Bingert",
            "Dawei Zhu",
            "Anne Lauscher",
            "Dietrich Klakow"
        ],
        "published": "2024",
        "summary": "Robust, faithful, and harm-free pronoun use for individuals is an important goal for language model development as their use increases, but prior work tends to study only one or two of these characteristics at a time. To measure progress towards the combined goal, we introduce the task of pronoun fidelity: Given a context introducing a co-referring entity and pronoun, the task is to reuse the correct pronoun later. We present RUFF, a carefully designed dataset of over 5 million instances to measure robust pronoun fidelity in English, and we evaluate 37 model variants from nine popular families, across architectures (encoder-only, decoder-only, and encoder-decoder) and scales (11M-70B parameters). When an individual is introduced with a pronoun, models can mostly faithfully reuse this pronoun in the next sentence, but they are significantly worse with she/her/her, singular they, and neopronouns. Moreover, models are easily distracted by non-adversarial sentences discussing other people; even one sentence with a distractor pronoun causes accuracy to drop on average by 34 percentage points. Our results show that pronoun fidelity is not robust, in a simple, naturalistic setting where humans achieve nearly 100% accuracy. We encourage researchers to bridge the gaps we find and to carefully evaluate reasoning in settings where superficial repetition might inflate perceptions of model performance.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.95.pdf",
        "source": "tacl2024"
    }
]