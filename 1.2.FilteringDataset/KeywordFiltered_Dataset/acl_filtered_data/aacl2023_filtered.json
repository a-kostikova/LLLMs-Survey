[
    {
        "title": "Toward Unified Controllable Text Generation via Regular Expression Instruction",
        "authors": [
            "Xin Zheng",
            "Hongyu Lin",
            "Xianpei Han",
            "Le Sun"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.1.pdf",
        "summary": "Controllable text generation is a fundamental aspect of natural language generation, with numerous methods proposed for different constraint types. However, these approaches often require significant architectural or decoding modifications, making them challenging to apply to additional constraints or resolve different constraint combinations. To address this, our paper introduces Regular Expression Instruction (REI), which utilizes an instructionbased mechanism to fully exploit regular expressions' advantages to uniformly model diverse constraints. Specifically, our REI supports all popular fine-grained controllable generation constraints, i.e., lexical, positional, and length, as well as their complex combinations, via regular expression-style instructions. Our method only requires fine-tuning on mediumscale language models or few-shot, in-context learning on large language models, and requires no further adjustment when applied to various constraint combinations. Experiments demonstrate that our straightforward approach yields high success rates and adaptability to various constraints while maintaining competitiveness in automatic metrics and outperforming most previous baselines. 1"
    },
    {
        "title": "MQAG: Multiple-choice Question Answering and Generation for Assessing Information Consistency in Summarization",
        "authors": [
            "Potsawee Manakul",
            "Adian Liusie",
            "Mark Gales"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.4.pdf",
        "summary": "State-of-the-art summarization systems can generate highly fluent summaries. These summaries, however, may contain factual inconsistencies and/or information not present in the source. Hence, an important component of assessing the quality of summaries is to determine whether there is information consistency between the source and the summary. Existing approaches are typically based on lexical matching or representation-based methods. In this work, we introduce an alternative scheme based on standard information-theoretic measures in which the information present in the source and summary is directly compared. We propose a Multiple-choice Question Answering and Generation framework, MQAG, which approximates the information consistency by computing the expected statistical distance between summary and source answer distributions over automatically generated multiple-choice questions. This approach exploits multiple-choice answer probabilities, as predicted answer distributions can be compared. We conduct experiments on four summary evaluation datasets: QAG-CNNDM/XSum, XSum-Hallucination, Podcast Assessment, and SummEval. Experiments show that MQAG, using models trained on SQuAD or RACE, outperforms existing evaluation methods on the majority of tasks. 1"
    },
    {
        "title": "SYNC: A Structurally Guided Hard Negative Curricula for Generalizable Neural Code Search",
        "authors": [
            "Atharva Naik",
            "Soumitra Das",
            "Jyothi Vedurada",
            "Somak Aditya"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.7.pdf",
        "summary": "In neural code search, a Transformers-based pre-trained language model (such as Code-BERT) is used to embed both the query (NL) and the code snippet (PL) into a joint representation space; which is used to retrieve the relevant PLs satisfying the query. These models often make mistakes such as retrieving snippets with incorrect data types, and incorrect method names or signatures. The generalization ability beyond training data is also limited (as the code retrieval datasets vary in the ways NL-PL pairs are collected). In this work, we propose a novel contrastive learning technique (SYNC) that enables efficient finetuning of code LMs with soft and hard negatives, where the hard negatives are constructed using a set of structure-aware AST-based perturbations; targeted towards possible syntactic and semantic variations. Our method achieves significant improvements in retrieval performance for three code LMs (CodeBERT, GraphCode-BERT, UniXCoder) over four Python code retrieval datasets. We also open source our code for reproducibility 1 ."
    },
    {
        "title": "On a Benefit of Masked Language Model Pretraining: Robustness to Simplicity Bias",
        "authors": [
            "Ting-Rui Chiang"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.8.pdf",
        "summary": "Despite the success of pretrained masked language models (MLM), why MLM pretraining is useful is still a question not fully answered. In this work we theoretically and empirically show that MLM pretraining makes models robust to lexicon-level spurious features, partly answering the question. Our explanation is that MLM pretraining may alleviate problems brought by simplicity bias (Shah et al., 2020), which refers to the phenomenon that a deep model tends to rely excessively on simple features. In NLP tasks, those simple features could be token-level features whose spurious association with the label can be learned easily. We show that MLM pretraining makes learning from the context easier. Thus, pretrained models are less likely to rely excessively on a single token. We also explore the theoretical explanations of MLM's efficacy in causal settings. Compared with Wei et al. (2021), we achieve similar results with milder assumptions. Finally, we close the gap between our theories and real-world practices by conducting experiments on real-world tasks."
    },
    {
        "title": "Conversation Style Transfer using Few-Shot Learning",
        "authors": [
            "Shamik Roy",
            "Raphael Shu",
            "Nikolaos Pappas",
            "Elman Mansimov",
            "Yi Zhang",
            "Saab Mansour",
            "Dan Roth"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.9.pdf",
        "summary": "Conventional text style transfer approaches focus on sentence-level style transfer without considering contextual information, and the style is described with attributes (e.g., formality). When applying style transfer in conversations such as task-oriented dialogues, existing approaches suffer from these limitations as context can play an important role and the style attributes are often difficult to define in conversations. In this paper, we introduce conversation style transfer as a few-shot learning problem, where the model learns to perform style transfer by observing only a few example dialogues in the target style. We propose a novel in-context learning approach to solve the task with stylefree dialogues as a pivot. Human evaluation shows that by incorporating multi-turn context, the model is able to match the target style while having better appropriateness and semantic correctness compared to utterance/sentence-level style transfer. Additionally, we show that conversation style transfer can also benefit downstream tasks. For example, in multi-domain intent classification tasks, the F1 scores improve after transferring the style of training data to match the style of the test data."
    },
    {
        "title": "Human-Like Distractor Response in Vision-Language Model",
        "authors": [
            "Xiaonan Xu",
            "Haoshuo Chen"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.12.pdf",
        "summary": "Previous studies exploring the human-like capabilities of machine-learning models have primarily focused on pure language models. Limited attention has been given to investigating whether models exhibit human-like behavior when performing tasks that require the integration of visual and language information. In this study, we investigate the impact of tags of semantic, phonological, and bilingual features on the visual question-answering task performance of an unsupervised model. Our findings reveal its similarities with the influence of distractors in the picture-naming task (known as the picture-word-interference paradigm) observed in human experiments: 1) Semanticallyrelated tags have a more negative effect on task performance compared to unrelated tags, indicating a more robust competition between visual and tag information which are semantically closer to each other when generating an answer. 2) Even presenting a partial section (wordpiece) of the originally detected tag significantly improves task performance, with the portion that plays a lesser role in determining the overall meaning of the original tag leading to a more pronounced improvement. 3) Tags in two languages that refer to the same meaning exhibit a symmetrical-like effect on performance in balanced bilingual models. Datasets and code of this project are released at https: //github.com/NLPbelllabs/PWI"
    },
    {
        "title": "Zero-shot Triplet Extraction by Template Infilling",
        "authors": [
            "Bosung Kim",
            "Hayate Iso",
            "Nikita Bhutani",
            "Estevam Hruschka",
            "Ndapa Nakashole",
            "Tom Mitchell"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.18.pdf",
        "summary": "The task of triplet extraction aims to extract pairs of entities and their corresponding relations from unstructured text. Most existing methods train an extraction model on training data involving specific target relations, and are incapable of extracting new relations that were not observed at training time. Generalizing the model to unseen relations typically requires fine-tuning on synthetic training data which is often noisy and unreliable. We show that by reducing triplet extraction to a template infilling task over a pre-trained language model (LM), we can equip the extraction model with zero-shot learning capabilities and eliminate the need for additional training data. We propose a novel framework, ZETT (ZEro-shot Triplet extraction by Template infilling), that aligns the task objective to the pre-training objective of generative transformers to generalize to unseen relations. Experiments on FewRel and Wiki-ZSL datasets demonstrate that ZETT shows consistent and stable performance, outperforming previous state-of-the-art methods, even when using automatically generated templates. 1"
    },
    {
        "title": "Faithful Chain-of-Thought Reasoning",
        "authors": [
            "Qing Lyu",
            "Shreya Havaldar",
            "Adam Stein",
            "Li Zhang",
            "Delip Rao",
            "Eric Wong",
            "Marianna Apidianaki",
            "Chris Callison-Burch"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.20.pdf",
        "summary": "While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query → symbolic reasoning chain) and Problem Solving (reasoning chain → answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning, 5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong synergy between faithfulness and accuracy. 1"
    },
    {
        "title": "Assessment of Pre-Trained Models Across Languages and Grammars",
        "authors": [
            "Alberto Muñoz-Ortiz",
            "David Vilares",
            "Carlos Gómez-Rodríguez"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.23.pdf",
        "summary": "We present an approach for assessing how multilingual large language models (LLMs) learn syntax in terms of multi-formalism syntactic structures. We aim to recover constituent and dependency structures by casting parsing as sequence labeling. To do so, we select a few LLMs and study them on 13 diverse UD treebanks for dependency parsing and 10 treebanks for constituent parsing. Our results show that: (i) the framework is consistent across encodings, (ii) pre-trained word vectors do not favor constituency representations of syntax over dependencies, (iii) sub-word tokenization is needed to represent syntax, in contrast to character-based models, and (iv) occurrence of a language in the pretraining data is more important than the amount of task data when recovering syntax from the word vectors."
    },
    {
        "title": "24-bit Languages",
        "authors": [
            "Yiran Wang",
            "Taro Watanabe",
            "Masao Utiyama",
            "Yuji Matsumoto"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.27.pdf",
        "summary": "We propose a contrastive hashing method to compress and interpret the contextual representation of pre-trained language models into binary codes. Unlike previous work that generates token-level tags, our method narrows the representation bottleneck to codes with only 24 bits, retaining task-relevant information in a more interpretable and fine-grained format without sacrificing performance (in most cases). We provide experiments and discussions on various structured prediction tasks, such as part-ofspeech tagging, named entity recognition, and constituency parsing, to demonstrate the effectiveness and interpretability of our method."
    },
    {
        "title": "DisCGen: A Framework for Discourse-Informed Counterspeech Generation",
        "authors": [
            "Sabit Hassan",
            "Malihe Alikhani"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.28.pdf",
        "summary": "Counterspeech can be an effective method for battling hateful content on social media. Automated counterspeech generation can aid in this process. Generated counterspeech, however, can be viable only when grounded in the context of topic, audience and sensitivity as these factors influence both the efficacy and appropriateness. In this work, we propose a novel framework based on theories of discourse to study the inferential links that connect counter speeches to the hateful comment. Within this framework, we propose: i) a taxonomy of counterspeech derived from discourse frameworks, and ii) discourse-informed prompting strategies for generating contextually-grounded counterspeech. To construct and validate this framework, we present a process for collecting an in-the-wild dataset of counterspeech from Reddit. Using this process, we manually annotate a dataset of 3.9k Reddit comment pairs for the presence of hatespeech and counterspeech 1. The positive pairs are annotated for 10 classes in our proposed taxonomy. We annotate these pairs with paraphrased counterparts to remove offensiveness and first-person references. We show that by using our dataset and framework, large language models can generate contextually-grounded counterspeech informed by theories of discourse. According to our human evaluation, our approaches can act as a safeguard against critical failures of discourseagnostic models."
    },
    {
        "title": "Interactive-Chain-Prompting: Ambiguity Resolution for Crosslingual Conditional Generation with Interaction",
        "authors": [
            "Jonathan Pilault",
            "Xavier Garcia",
            "Arthur Bražinskas",
            "Orhan Firat"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.31.pdf",
        "summary": "Crosslingual conditional generation (e.g., machine translation) has long enjoyed the benefits of scaling. Nonetheless, there are still issues that scale alone may not overcome. A source query in one language, for instance, may yield several translation options in another language without any extra context. Only one translation could be acceptable however, depending on the translator's preferences and goals. Choosing the incorrect option might significantly affect translation usefulness and quality. We propose a novel method interactive-chain prompting-a series of question, answering and generation intermediate steps between a Translator model and a User model-that reduces translations into a list of subproblems addressing ambiguities and then resolving such subproblems before producing the final text to be translated. To check ambiguity resolution capabilities and evaluate translation quality, we create a dataset exhibiting different linguistic phenomena which leads to ambiguities at inference for four languages. To encourage further exploration in this direction, we release all datasets. We note that interactive-chain prompting, using eight interactions as exemplars, consistently surpasses prompt-based methods with direct access to background information to resolve ambiguities."
    },
    {
        "title": "Attacking Open-domain Question Answering by Injecting Misinformation",
        "authors": [
            "Liangming Pan",
            "Wenhu Chen",
            "Min-Yen Kan",
            "William Yang Wang"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.35.pdf",
        "summary": "With a rise in false, inaccurate, and misleading information in propaganda, news, and social media, real-world Question Answering (QA) systems face the challenges of synthesizing and reasoning over misinformation-polluted contexts to derive correct answers. This urgency gives rise to the need to make QA systems robust to misinformation, a topic previously unexplored. We study the risk of misinformation to QA models by investigating the sensitivity of open-domain QA models to corpus pollution with misinformation documents. We curate both human-written and model-generated false documents that we inject into the evidence corpus of QA models, and assess the impact on the performance of these systems. Experiments show that QA models are vulnerable to even small amounts of evidence contamination brought by misinformation, with large absolute performance drops on all models. Misinformation attack brings more threat when fake documents are produced at scale by neural models or the attacker targets on hacking specific questions of interest. To defend against such a threat, we discuss the necessity of building a misinformation-aware QA system that integrates question-answering and misinformation detection in a joint fashion."
    },
    {
        "title": "Smoothing Entailment Graphs with Language Models",
        "authors": [
            "Nick McKenna",
            "Tianyi Li",
            "Mark Johnson",
            "Mark Steedman"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.37.pdf",
        "summary": "The diversity and Zipfian frequency distribution of natural language predicates in corpora leads to sparsity in Entailment Graphs (EGs) built by Open Relation Extraction (ORE). EGs are computationally efficient and explainable models of natural language inference, but as symbolic models, they fail if a novel premise or hypothesis vertex is missing at test-time. We present theory and methodology for overcoming such sparsity in symbolic models. First, we introduce a theory of optimal smoothing of EGs by constructing transitive chains. We then demonstrate an efficient, open-domain, and unsupervised smoothing method using an off-the-shelf Language Model to find approximations of missing premise predicates. This improves recall by 25.1 and 16.3 percentage points on two difficult directional entailment datasets, while raising average precision and maintaining model explainability. Further, in a QA task we show that EG smoothing is most useful for answering questions with lesser supporting text, where missing premise predicates are more costly. Finally, controlled experiments with WordNet confirm our theory and show that hypothesis smoothing is difficult, but possible in principle. 1"
    },
    {
        "title": "FastRAT: Fast and Efficient Cross-lingual Text-to-SQL Semantic Parsing",
        "authors": [
            "Pavlos Vougiouklis",
            "Nikos Papasarantopoulos",
            "Danna Zheng",
            "David Tuckey",
            "Chenxin Diao",
            "Zhili Shen",
            "Jeff Pan"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.38.pdf",
        "summary": "Recent advances of large pre-trained language models have motivated significant breakthroughs in various Text-to-SQL tasks. However, a number of challenges inhibit the deployment of SQL parsers in commercial applications. In this paper, we focus on two such challenges: decoding speed and multilingual input, and introduce FastRAT, a model that includes (i) a decoder-free framework to quickly generate SQL queries from natural language questions based on SQL Semantic Predictions, (ii) a cross-lingual multi-task pre-training scheme, and (iii) a method, based on distant supervision, to extend a semantic parser to new languages. We apply FastRAT on CSpider and Spider, two challenging zero-shot semantic parsing benchmarks. Our system achieves an average of 10x decoding speedup over a set of competitive baselines based on auto-or semi-autoregressive decoding. In the cross-lingual CSpider dataset, our approach achieves an exact query match accuracy score of 61.3, outperforming the relevant competition. In the monolingual task, it maintains competitive performance by exhibiting < 5% accuracy drop compared to disproportionately slower solutions."
    },
    {
        "title": "ProMap: Effective Bilingual Lexicon Induction via Language Model Prompting",
        "authors": [
            "Abdellah El Mekki",
            "Muhammad Abdul-Mageed",
            "ElMoatez Billah Nagoudi",
            "Ismail Berrada",
            "Ahmed Khoumsi"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.39.pdf",
        "summary": "Bilingual Lexicon Induction (BLI), where words are translated between two languages, is an important NLP task. While noticeable progress on BLI in rich resource languages using static word embeddings has been achieved. The word translation performance can be further improved by incorporating information from contextualized word embeddings. In this paper, we introduce ProMap, a novel approach for BLI that leverages the power of prompting pretrained multilingual and multidialectal language models to address these challenges. To overcome the employment of subword tokens in these models, ProMap relies on an effective padded prompting of language models with a seed dictionary that achieves good performance when used independently. We also demonstrate the effectiveness of ProMap in re-ranking results from other BLI methods such as with aligned static word embeddings. When evaluated on both rich-resource and low-resource languages, ProMap consistently achieves stateof-the-art results. Furthermore, ProMap enables strong performance in few-shot scenarios (even with less than 10 training examples), making it a valuable tool for low-resource language translation. Overall, we believe our method offers both exciting and promising direction for BLI in general and low-resource languages in particular. ProMap code and data are available at https://github.com/4mekki4/promap."
    },
    {
        "title": "ConDA: Contrastive Domain Adaptation for AI-generated Text Detection",
        "authors": [
            "Amrita Bhattacharjee",
            "Tharindu Kumarage",
            "Raha Moraffah",
            "Huan Liu"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.40.pdf",
        "summary": "Large language models (LLMs) are increasingly being used for generating text in a variety of use cases, including journalistic news articles. Given the potential malicious nature in which these LLMs can be used to generate disinformation at scale, it is important to build effective detectors for such AI-generated text. Given the surge in development of new LLMs, acquiring labeled training data for supervised detectors is a bottleneck. However, there might be plenty of unlabeled text data available, without information on which generator it came from. In this work we tackle this data problem, in detecting AI-generated news text, and frame the problem as an unsupervised domain adaptation task. Here the domains are the different text generators, i.e. LLMs, and we assume we have access to only the labeled source data and unlabeled target data. We develop a Contrastive Domain Adaptation framework, called ConDA, that blends standard domain adaptation techniques with the representation power of contrastive learning to learn domain invariant representations that are effective for the final unsupervised detection task. Our experiments demonstrate the effectiveness of our framework, resulting in average performance gains of 31.7% from the best performing baselines, and within 0.8% margin of a fully supervised detector. All our code and data is available here."
    },
    {
        "title": "MedRedQA for Medical Consumer Question Answering: Dataset, Tasks, and Neural Baselines",
        "authors": [
            "Vincent Nguyen",
            "Sarvnaz Karimi",
            "Maciej Rybinski",
            "Zhenchang Xing"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.42.pdf",
        "summary": "Medical question answering for consumers aims to assist consumers in finding trustworthy and relevant information for their concerns. Although some datasets exist for consumer question answering, they use synthetic questions or present difficult-to-understand answers. We introduce MedRedQA, a large non-factoid English consumer Question Answering (QA) dataset containing 51,000 pairs of consumer questions and their corresponding expert answers. MedRedQA facilitates research that aims to provide consumer-friendly responses to realworld consumer questions. We propose and benchmark three tasks for consumer medical question answering for our dataset, including (1) candidate answer ranking, (2) open-ended answer generation, and (3) answer generation with scientific evidence. Our benchmarking experiments reveal that, for the ranking task, it is feasible to retrieve expert answers within five responses in an oracle retrieval. Though, in an answer generation task, it remains challenging to align the generation toward expert answers. However, our experiments show that including scientific evidence in the prompt may reduce hallucinations in an answer generation setup. 1"
    },
    {
        "title": "Sentiment Aided Graph Attentive Contextualization for Task Oriented Negotiation Dialogue Generation",
        "authors": [
            "Aritra Raut",
            "Sriparna Saha",
            "Anutosh Maitra",
            "Roshni Ramnani"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.44.pdf",
        "summary": "Over the past several years, the demand and popularity of using virtual assistants to finish jobs like service scheduling and online shopping have increased. While keeping the user's request in mind, an effective task-oriented virtual agent must strive to improve the seller's profit. Therefore, in order to achieve the best possible trade-off between the parties, this form of virtual agent has to have strong negotiating abilities. Although current conversational agents are quite good at making fluent sentences, they are still unable to use strategic thinking. In order to more effectively contextualize the choice of the next set of negotiation methods while producing answers, we develop Nego-GAT, an end-to-end negotiation system that includes sentiment information and graph attention embedding into GPT-2. Our selfsupervised model outperforms earlier cuttingedge negotiation models in terms of both the precision of strategy/dialogue act prediction and the caliber of the generated dialogue responses 1 ."
    },
    {
        "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
        "authors": [
            "Yejin Bang",
            "Samuel Cahyawijaya",
            "Nayeon Lee",
            "Wenliang Dai",
            "Dan Su",
            "Bryan Wilie",
            "Holy Lovenia",
            "Ziwei Ji",
            "Tiezheng Yu",
            "Willy Chung",
            "Quyet V. Do",
            "Yan Xu",
            "Pascale Fung"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.45.pdf",
        "summary": "This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets, using 23 data sets covering 8 different common NLP application tasks. We extensively evaluate the multitask, multilingual, and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zeroshot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. ChatGPT suffers from hallucination problems like other LLMs. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e., 8% ROUGE-1 on summarization and 2% ChrF++ on machine translation, in a multi-turn \"prompt engineering\" fashion. We release a code for evaluation set extraction. 1"
    },
    {
        "title": "Benchmarking Procedural Language Understanding for Low-Resource Languages: A Case Study on Turkish",
        "authors": [
            "Arda Uzunoglu",
            "Gözde Şahin"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.52.pdf",
        "summary": "Understanding procedural natural language (e.g., step-by-step instructions) is a crucial step to execution and planning. However, while there are ample corpora and downstream tasks available in English, the field lacks such resources for most languages. To address this gap, we conduct a case study on Turkish procedural texts. We first expand the number of tutorials in Turkish wikiHow from 2,000 to 52,000 using automated translation tools, where the translation quality and loyalty to the original meaning are validated by a team of experts on a random set. Then, we generate several downstream tasks on the corpus, such as linking actions, goal inference, and summarization. To tackle these tasks, we implement strong baseline models via fine-tuning large language-specific models such as TR-BART and BERTurk, as well as multilingual models such as mBART, mT5, and XLM. We find that language-specific models consistently outperform their multilingual models by a significant margin across most procedural language understanding (PLU) tasks. We release our corpus, downstream tasks and the baseline models with https://github.com/ GGLAB-KU/turkish-plu."
    },
    {
        "title": "GrailQA++: A Challenging Zero-Shot Benchmark for Knowledge Base Question Answering",
        "authors": [
            "Ritam Dutt",
            "Sopan Khosla",
            "Vinayshekhar Bannihatti Kumar",
            "Rashmi Gangadharaiah"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.58.pdf",
        "summary": "Most benchmarks designed for question answering over knowledge bases (KBQA) operate with the i.i.d. assumption where one encounters the same schema items during inference as those observed during training. Recently, the GrailQA dataset was established to evaluate zero-shot generalization capabilities of KBQA models as a departure from the i.i.d. assumption. Reasonable performance of current KBQA systems on the zero-shot GrailQA split hints that the field might be moving towards more generalizable systems. In this work, we observe a bias in the GrailQA dataset towards simpler one or two-hop questions, which results in an inaccurate assessment of the aforementioned prowess. We propose GrailQA++, a challenging zero-shot KBQA test set that contains more questions relying on complex reasoning. We leverage the concept of graph isomorphisms to control the complexity of the questions and to ensure that our proposed test set has a fair distribution of simple and complex questions. Existing KBQA models suffer a substantial drop in performance on our constructed new test set as compared to the GrailQA zero-shot split. Our analysis reveals how isomorphisms can be used to understand the complementary strengths of different KBQA models and provide a deeper insight into model mispredictions. Overall, our paper highlights the non-generalizability of existing models and the necessity for designing more challenging benchmarks. Our dataset is available at https://github.com/sopankhosla/ GrailQA-PlusPlus"
    },
    {
        "title": "NusaWrites: Constructing High-Quality Corpora for Underrepresented and Extremely Low-Resource Languages",
        "authors": [
            "Samuel Cahyawijaya",
            "Holy Lovenia",
            "Fajri Koto",
            "Dea Adhista",
            "Emmanuel Dave",
            "Sarah Oktavianti",
            "Salsabil Akbar",
            "Jhonson Lee",
            "Nuur Shadieq",
            "Tjeng Wawan Cenggoro",
            "Hanung Linuwih",
            "Bryan Wilie",
            "Galih Muridan",
            "Genta Winata",
            "David Moeljadi",
            "Alham Fikri Aji",
            "Ayu Purwarianti",
            "Pascale Fung"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.60.pdf",
        "summary": "Democratizing access to natural language processing (NLP) technology is crucial, especially for underrepresented and extremely lowresource languages. Previous research has focused on developing labeled and unlabeled corpora for these languages through online scraping and document translation. While these methods have proven effective and costefficient, we have identified limitations in the resulting corpora, including a lack of lexical diversity and cultural relevance to local communities. To address this gap, we conduct a case study on Indonesian local languages. We compare the effectiveness of online scraping, human translation, and paragraph writing by native speakers in constructing datasets. Our findings demonstrate that datasets generated through paragraph writing by native speakers exhibit superior quality in terms of lexical diversity and cultural content. In addition, we present the NusaWrites benchmark, encompassing 12 underrepresented and extremely lowresource languages spoken by millions of individuals in Indonesia. Our empirical experiment results using existing multilingual large language models emphasize the need to extend these models to more underrepresented languages. We release the NusaWrites dataset 1 and code involved in our experiment at https: //github.com/IndoNLP/nusa-writes."
    },
    {
        "title": "Generation of Korean Offensive Language by Leveraging Large Language Models via Prompt Design",
        "authors": [
            "Jisu Shin",
            "Hoyun Song",
            "Huije Lee",
            "Fitsum Gaim",
            "Jong Park"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.62.pdf",
        "summary": "Warning: This paper contains content that can be offensive or upsetting. The research for detecting offensive language on online platforms has much advanced. However, the majority of these studies have primarily focused on English. Given the unique characteristics of offensive language, where social and cultural contexts significantly influence content understanding, language-specific datasets are essential. Acquiring comprehensive datasets in Korean, a less-resourced language, has mostly relied on human annotations, suffering from inherent limitations in terms of labor intensity and potential annotator bias. Automatic generation of datasets using generative methods offers an alternative approach to address these limitations, yet faces challenges in capturing linguistic and cultural diversities while maintaining native-level fluency. To address these challenges, we introduce a prompt design methodology, Korean Offensive language Machine Generation (K-OMG), using large language models. By manipulating three prompt factors, we find an effective prompt design to generate culturally aligned offensive language with fluent expressions. Experimental results demonstrate the high quality and utility of our automatically generated dataset. Our detailed analysis shows that the proposed approach achieves exceptional fluency in generating texts while effectively incorporating social and cultural diversities."
    },
    {
        "title": "PICK: Polished & Informed Candidate Scoring for Knowledge-Grounded Dialogue Systems",
        "authors": [
            "Bryan Wilie",
            "Yan Xu",
            "Willy Chung",
            "Samuel Cahyawijaya",
            "Holy Lovenia",
            "Pascale Fung"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.63.pdf",
        "summary": "Grounding dialogue response generation on external knowledge is proposed to produce informative and engaging responses. However, current knowledge-grounded dialogue (KGD) systems often fail to align the generated responses with human-preferred qualities due to several issues like hallucination and the lack of coherence. Upon analyzing multiple language model generations, we observe the presence of alternative generated responses within a single decoding process. These alternative responses are more faithful and exhibit a comparable or higher level of relevance to prior conversational turns compared to the optimal responses prioritized by the decoding processes. To address these challenges and driven by these observations, we propose Polished & Informed Candidate Scoring (PICK), a generation re-scoring framework that empowers models to generate faithful and relevant responses without requiring additional labeled data or model tuning. Through comprehensive automatic and human evaluations, we demonstrate the effectiveness of PICK in generating responses that are more faithful while keeping them relevant to the dialogue history. Furthermore, PICK consistently improves the system's performance with both oracle and retrieved knowledge in all decoding strategies. We provide the detailed implementation in 1 ."
    },
    {
        "title": "Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method",
        "authors": [
            "Xuan Zhang",
            "Wei Gao"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.64.pdf",
        "summary": "While large pre-trained language models (LLMs) have shown their impressive capabilities in various NLP tasks, they are still underexplored in the misinformation domain. In this paper, we examine LLMs with in-context learning (ICL) for news claim verification, and find that only with 4-shot demonstration examples, the performance of several prompting methods can be comparable with previous supervised models. To further boost performance, we introduce a Hierarchical Step-by-Step (HiSS) prompting method which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questionsanswering steps progressively. Experiment results on two public misinformation datasets show that HiSS prompting outperforms stateof-the-art fully-supervised approach and strong few-shot ICL-enabled baselines."
    },
    {
        "title": "Retrieval Augmented Generation with Rich Answer Encoding",
        "authors": [
            "Wenyu Huang",
            "Mirella Lapata",
            "Pavlos Vougiouklis",
            "Nikos Papasarantopoulos",
            "Jeff Pan"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.65.pdf",
        "summary": "Knowledge-intensive generation tasks like generative question answering require models to retrieve appropriate passages from external knowledge sources to support answer generation. The generation quality relies heavily on the retrieved passages, which serve as contextual information. State-of-the-art Retrieval Augmented Generation models with marginalized output dominate this area but focus too much on label-relevant passages, rather than question-relevant passages and answers. This work addresses this issue by incorporating rich answer encoding through Dense Knowledge Similarity (DKS) and Retriever as Answer Classifier (RAC). We demonstrate the advantages of our proposed approach in open domain question answering (MSMARCO) and conversation (Wizard of Wikipedia) datasets, reporting both generation and retrieval metrics. In the MSMARCO development set, our best model achieves 12.1% relative improvement 1 on Recall@1 and 4.5% relative improvement on BLEU-4 compared to the baseline model. In the KILT-WoW leaderboard, our best model achieves 8.9% relative improvement on R-Precision and 13.3% relative improvement on KILT-RL compared to the baseline model. Our codes and models are available at https://github.com/hwy9855/rag-ae."
    },
    {
        "title": "Examining Consistency of Visual Commonsense Reasoning based on Person Grounding",
        "authors": [
            "Huiju Kim",
            "Youjin Kang",
            "SangKeun Lee"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.66.pdf",
        "summary": "Given an image depicting multiple individuals, humans are capable of inferring each individual's emotions, intentions, and social norms based on commonsense understanding. However, a machine's ability of commonsense reasoning about distinct individuals in images remains underexplored. In this study, we examine the consistency of visual commonsense reasoning based on person grounding. We introduce a novel test dataset called Visual Commonsense Reasoning-Contrast Sets (VCR-CS) to evaluate whether models can reason about individual people in an image by changing the person tags in the questions and answers. We benchmark various vision-language models on VCR-CS and observe that they fail in consistent commonsense reasoning about different people in one image, showing a performance decrease of up to 31.5%. To mitigate such failures, we propose a multi-task learning framework called Personcentric groundIng eNhanced Tuning (PINT). Our framework enhances a model's ability to perform person-grounded commonsense reasoning by leveraging two novel person-centric pretraining tasks: Image Person-based Text Matching and Person-Masked Language Modeling. The experimental results revealed the effectiveness of PINT by showing the lowest performance degradation on VCR-CS and the improvements in consistency and sensitivity metrics. Our dataset and code are publicly available 1 ."
    },
    {
        "title": "Self-Consistent Narrative Prompts on Abductive Natural Language Inference",
        "authors": [
            "Chunkit Chan",
            "Xin Liu",
            "Tsz Ho Chan",
            "Jiayang Cheng",
            "Yangqiu Song",
            "Ginny Wong",
            "Simon See"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.67.pdf",
        "summary": "Abduction has long been seen as crucial for narrative comprehension and reasoning about everyday situations. The abductive natural language inference (αNLI) task has been proposed, and this narrative text-based task aims to infer the most plausible hypothesis from the candidates given two observations. However, the inter-sentential coherence and the model consistency have not been well exploited in the previous works on this task. In this work, we propose a prompt tuning model α-PACE 1 , which takes self-consistency and intersentential coherence into consideration. Besides, we propose a general self-consistent framework that considers various narrative sequences (e.g., linear narrative and reverse chronology) for guiding the pre-trained language model in understanding the narrative context of input. We conduct extensive experiments and thorough ablation studies to illustrate the necessity and effectiveness of α-PACE. The performance of our method shows significant improvement against extensive competitive baselines."
    },
    {
        "title": "Reranking for Natural Language Generation from Logical Forms: A Study based on Large Language Models",
        "authors": [
            "Levon Haroutunian",
            "Zhuang Li",
            "Lucian Galescu",
            "Philip Cohen",
            "Raj Tumuluri",
            "Gholamreza Haffari"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.69.pdf",
        "summary": "Large language models (LLMs) have demonstrated impressive capabilities in natural language generation. However, their output quality can be inconsistent, posing challenges for generating natural language from logical forms (LFs). This task requires the generated outputs to embody the exact semantics of LFs, without missing any LF semantics or creating any hallucinations. In this work, we tackle this issue by proposing a novel generate-and-rerank approach. Our approach involves initially generating a set of candidate outputs by prompting an LLM and subsequently reranking them using a task-specific reranker model. In addition, we curate a manually collected dataset to evaluate the alignment between different ranking metrics and human judgements. The chosen ranking metrics are utilized to enhance the training and evaluation of the reranker model. By conducting extensive experiments on three diverse datasets, we demonstrate that the candidates selected by our reranker outperform those selected by baseline methods in terms of semantic consistency and fluency, as measured by three comprehensive metrics. Our findings provide strong evidence for the effectiveness of our approach in improving the quality of generated outputs."
    },
    {
        "title": "PACT: Pretraining with Adversarial Contrastive Learning for Text Classification",
        "authors": [
            "Md Tawkat Islam Khondaker",
            "Muhammad Abdul-Mageed",
            "Laks Lakshmanan, V.S."
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-main.71.pdf",
        "summary": "We present PACT (Pretraining with Adversarial Contrastive Learning for Text Classification), a novel self-supervised framework for text classification. Instead of contrasting against inbatch negatives, a popular approach in the literature, PACT mines negatives closer to the anchor representation. PACT operates by endowing the standard pretraining mechanisms of BERT with adversarial contrastive learning objectives, allowing for effective joint optimization of token-and sentence-level pretraining of the BERT model. Our experiments on 13 diverse datasets including token-level, singlesentence, and sentence-pair text classification tasks show that PACT achieves consistent improvements over SOTA baselines. We further show that PACT regularizes both token-level and sentence-level embedding spaces into more uniform representations, thereby alleviating the undesirable anisotropic phenomenon of language models. 1"
    },
    {
        "title": "Self-Augmentation Improves Zero-Shot Cross-Lingual Transfer",
        "authors": [
            "Fei Wang",
            "Kuan-Hao Huang",
            "Kai-Wei Chang",
            "Muhao Chen"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-short.1.pdf",
        "summary": "Zero-shot cross-lingual transfer is a central task in multilingual NLP, allowing models trained in languages with more sufficient training resources to generalize to other low-resource languages. Earlier efforts on this task use parallel corpora, bilingual dictionaries, or other annotated alignment data to improve cross-lingual transferability, which are typically expensive to obtain. In this paper, we propose a simple yet effective method, SALT, to improve the zero-shot cross-lingual transfer of the multilingual pretrained language models without the help of such external data. By incorporating code-switching and embedding mixup with self-augmentation, SALT effectively distills cross-lingual knowledge from the multilingual PLM and enhances its transferability on downstream tasks. Experimental results on XNLI and PAWS-X show that our method is able to improve zero-shot cross-lingual transferability without external data. 1"
    },
    {
        "title": "Learning to Predict Concept Ordering for Common Sense Generation",
        "authors": [
            "Tianhui Zhang",
            "Danushka Bollegala",
            "Bei Peng"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-short.2.pdf",
        "summary": "Prior work has shown that the ordering in which concepts are shown to a commonsense generator plays an important role, affecting the quality of the generated sentence. However, it remains a challenge to determine the optimal ordering of a given set of concepts such that a natural sentence covering all the concepts could be generated from a pretrained generator. To understand the relationship between the ordering of the input concepts and the quality of the generated sentences, we conduct a systematic study considering multiple language models (LMs) and concept ordering strategies. We find that BART-large model consistently outperforms all other LMs considered in this study when fine-tuned using the ordering of concepts as they appear in CommonGen training data as measured using multiple evaluation metrics. Moreover, the larger GPT3-based large language models (LLMs) variants do not necessarily outperform much smaller LMs on this task, even when fine-tuned on task-specific training data. Interestingly, human annotators significantly reorder input concept sets when manually writing sentences covering those concepts, and this ordering provides the best sentence generations independently of the LM used for the generation, outperforming a probabilistic concept ordering baseline. 1"
    },
    {
        "title": "The Impact of Debiasing on the Performance of Language Models in Downstream Tasks is Underestimated",
        "authors": [
            "Masahiro Kaneko",
            "Danushka Bollegala",
            "Naoaki Okazaki"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-short.4.pdf",
        "summary": "Pre-trained language models trained on largescale data have learned serious levels of social biases. Consequently, various methods have been proposed to debias pre-trained models. Debiasing methods need to mitigate only discriminatory bias information from the pretrained models, while retaining information that is useful for the downstream tasks. In previous research, whether useful information is retained has been confirmed by the performance of downstream tasks in debiased pretrained models. On the other hand, it is not clear whether these benchmarks consist of data pertaining to social biases and are appropriate for investigating the impact of debiasing. For example in gender-related social biases, data containing female words (e.g. \"she, female, woman\"), male words (e.g. \"he, male, man\"), and stereotypical words (e.g. \"nurse, doctor, professor\") are considered to be the most affected by debiasing. If there is not much data containing these words in a benchmark dataset for a target task, there is the possibility of erroneously evaluating the effects of debiasing. In this study, we compare the impact of debiasing on performance across multiple downstream tasks using a wide-range of benchmark datasets that containing female, male, and stereotypical words. Experiments show that the effects of debiasing are consistently underestimated across all tasks. Moreover, the effects of debiasing could be reliably evaluated by separately considering instances containing female, male, and stereotypical words than all of the instances in a benchmark dataset."
    },
    {
        "title": "Enhancing Volatility Forecasting in Financial Markets: A General Numeral Attachment Dataset for Understanding Earnings Calls",
        "authors": [
            "Ming-Xuan Shi",
            "Chung-Chi Chen",
            "Hen-Hsen Huang",
            "Hsin-Hsi Chen"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-short.5.pdf",
        "summary": "Volatility, a crucial statistical measure in the financial market, serves as an indicator of financial instrument risk. Accurate volatility capture aids in predicting stock movements and is valuable in derivative trading, such as options trading. While recent research focuses on volatility forecasting using earnings call transcriptions, most approaches rely on end-to-end models that directly process textual or vocal data. However, limited efforts have been made to simulate the reading and comprehension processes of financial professionals, thereby enhancing the capabilities of language models. To address this gap, we propose a general numeral attachment dataset designed to train language models to understand earnings calls with the expertise of professionals. Additionally, we introduce a pre-training process that improves the semantic understanding of earnings calls. Experimental results demonstrate that our pretrained language model enhances the accuracy of 3-day volatility forecasting."
    },
    {
        "title": "Learning a Better Initialization for Soft Prompts via Meta-Learning",
        "authors": [
            "Yukun Huang",
            "Kun Qian",
            "Zhou Yu"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-short.8.pdf",
        "summary": "Prompt tuning (PT) is an effective approach to adapting pre-trained language models to downstream tasks. However, prompt tuning doesn't perform well under few-shot settings due to the poor initialization. So pre-trained prompt tuning (PPT) (Gu et al., 2022) is proposed to adapt prompt tuning to few-shot settings by initializing prompts with source data. We propose Meta-learned Prompt Tuning (MetaPT) to further improve PPT's few-shot learning performance by considering latent structure within the source data. Specifically, we introduce the framework by first clustering source data into different meta-training tasks in an unsupervised manner. Then we leverage these tasks to metatrain prompts with a meta-learning algorithm. Such a process enables prompts to learn a better initialization by discovering commonalities among these meta-training tasks. We evaluate our method on seven downstream sentiment tasks. The results demonstrate that our MetaPT achieves better performance and stability than the state-of-the-art method."
    },
    {
        "title": "Can You Translate for Me? Code-Switched Machine Translation with Large Language Models",
        "authors": [
            "Jyotsana Khatri",
            "Vivek Srivastava",
            "Lovekesh Vig"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-short.10.pdf",
        "summary": "Large language models (LLMs) have shown remarkable performance on a variety of multilingual NLP tasks. Code-switching is one of the most convenient styles of communication in multilingual communities. It is known to present several challenges to the existing language models and task-specific models. In this paper, we evaluate the capability of multilingual LLMs for the code-switched machine translation (CSMT) task in traditional and novel settings and present our insights. We observe that ChatGPT outperforms other LLMs and shows competitive performance to the supervised fine-tuned models. Though promising, ChatGPT shows major limitations, such as high gender bias, stereotypes, and factual inconsistencies. It further demands a multi-dimensional large-scale evaluation of the multilingual LLMs for code-switched languages."
    },
    {
        "title": "Who Are All The Stochastic Parrots Imitating? They Should Tell Us!",
        "authors": [
            "Sagi Shaier",
            "Lawrence Hunter",
            "Katharina Kann"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-short.13.pdf",
        "summary": "Both standalone language models (LMs) as well as LMs within downstream-task systems have been shown to generate statements which are factually untrue. This problem is especially severe for low-resource languages, where training data is scarce and of worse quality than for high-resource languages. In this opinion piece, we argue that LMs in their current state will never be fully trustworthy in critical settings and suggest a possible novel strategy to handle this issue: by building LMs such that can cite their sources-i.e., point a user to the parts of their training data that back up their outputs. We first discuss which current NLP tasks would or would not benefit from such models. We then highlight the expected benefits such models would bring, e.g., quick verifiability of statements. We end by outlining the individual tasks that would need to be solved on the way to developing LMs with the ability to cite. We hope to start a discussion about the field's current approach to building LMs, especially for low-resource languages, and the role of the training data in explaining model generations."
    },
    {
        "title": "Theia: Weakly Supervised Multimodal Event Extraction from Incomplete Data",
        "authors": [
            "Farhad Moghimifar",
            "Fatemeh Shiri",
            "Van Nguyen",
            "Yuan-Fang Li",
            "Gholamreza Haffari"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-short.16.pdf",
        "summary": "Event extraction from multimodal documents is an important yet under-explored problem. One challenge faced by this task is the scarcity of paired image-text datasets, making it difficult to fully exploit the strong representation power of multimodal language models. In this paper, we present Theia, an end-to-end multimodal event extraction framework that can be trained on incomplete data. Specifically, we couple a generation-based event extraction model with a customised image synthesizer that can generate images from text. Our model leverages capabilities of pre-trained visionlanguage models and can be trained on incomplete (i.e. text-only) data. Experimental results on existing multimodal datasets demonstrate the effectiveness of our approach for both synthesising missing data and extracting events over state-of-the-art approaches."
    },
    {
        "title": "Perplexity-Driven Case Encoding Needs Augmentation for CAPITALIZATION Robustness",
        "authors": [
            "Rohit Jain",
            "Huda Khayrallah",
            "Roman Grundkiewicz",
            "Marcin Junczys-Dowmunt"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-short.17.pdf",
        "summary": "Subword segmentation methods are the predominant solution to vocab sparsity in NMT. However, they cannot currently handle capitalization well. We re-encode case to allow the perplexity-driven SPM unigram language model algorithm to learn how to segment capitalization. Since naturally occurring data accurately describes the prevalence of capitalization but underestimates the importance humans ascribe to capitalization robustness, we propose data augmentation to fill this gap. We demonstrate that our proposed method improves translation quality on ALL CAPS, lower cased, and Title Case, while maintaining quality on standard test sets. In contrast to prior work, our proposed method has minimal impact on decoding speed. We release our code: github.com/marian-nmt/sentencepiece."
    },
    {
        "title": "The Language Model, Resources, and Computational Pipelines for the Under-Resourced Iranian Azerbaijani",
        "authors": [
            "Marzia Nouri",
            "Mahsa Amani",
            "Reihaneh Zohrabi",
            "Ehsaneddin Asgari"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-short.19.pdf",
        "summary": "Iranian Azerbaijani is a dialect of the Azerbaijani language spoken by more than 16% of the population in Iran (>14 million). Unfortunately, a lack of computational resources is one of the factors that puts this language and its rich culture at risk of extinction. This work aims to create fundamental natural language processing (NLP) resources and pipelines for the processing and analysis of Iranian Azerbaijani introducing standard datasets and starter models for various NLP tasks such as language modeling, text classification, part-of-speech (POS) tagging, and machine translation. The proposed resources have been curated and preprocessed to facilitate the development of NLP models for Iranian Azerbaijani and provide a strong baseline for further research and development. This study is an example of bridging the gap in NLP for low-resource languages and promoting the advancement of language technologies in underrepresented languages. To the best of our knowledge, for the first time, this paper presents major infrastructures for the processing and analysis of Iranian Azerbaijani, with the ultimate goal of improving communication and information access for millions of individuals. Furthermore, our translation model's online demo is accessible at https://azeri.parsi.ai/."
    },
    {
        "title": "Borderless Azerbaijani Processing: Linguistic Resources and a Transformer-based Approach for Azerbaijani Transliteration",
        "authors": [
            "Reihaneh Zohrabi",
            "Mostafa Masumi",
            "Omid Ghahroodi",
            "Parham AbedAzad",
            "Hamid Beigy",
            "Mohammad Hossein Rohban",
            "Ehsaneddin Asgari"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.ijcnlp-short.20.pdf",
        "summary": "Recent advancements in neural language models have revolutionized natural language understanding. However, many languages still face the risk of being left behind without the benefits of such advancements, potentially leading to their extinction. One such language is Azerbaijani in Iran, which suffers from limited digital resources and a lack of alignment between spoken and written forms. In contrast, Azerbaijani in the Republic of Azerbaijan has seen more resources and is not considered as low-resource as its Iranian counterpart. In this context, our research focuses on the computational progress made in Iranian Azerbaijani language. We propose a transliteration model that leverages an Azerbaijani parallel dataset, effectively bridging the gap between the Latin and Persian scripts. By enabling seamless communication between these two scripts, our model facilitates cultural exchange and serves as a valuable tool for transfer learning. The effectiveness of our approach surpasses traditional rule-based methods, as evidenced by the significant improvements in performance metrics. We observe a minimum 15% increase in BLEU scores and a reduction of at least 1/3 in edit distance. Furthermore, our model's online demo is accessible at https://azeri.parsi.ai/."
    },
    {
        "title": "SPEC5G: A Dataset for 5G Cellular Network Protocol Analysis",
        "authors": [
            "Imtiaz Karim",
            "Kazi Samin Mubasshir",
            "Mirza Masfiqur Rahman",
            "Elisa Bertino"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.findings-ijcnlp.3.pdf",
        "summary": "5G is the 5 th generation state-of-the-art cellular network protocol designed to connect virtually everyone and everything with increased speed and reduced latency. Therefore, its development, analysis, and security are critical. However, all approaches to the 5G protocol development and security analysis, e.g., property extraction, protocol summarization, and semantic analysis of the protocol specifications and implementations are completely manual. To reduce such manual efforts, in this paper, we curate SPEC5G-the first-ever public 5G dataset for NLP research. The dataset contains 3,547,587 sentences with 134M words, from 13094 cellular network specifications and 13 online websites. By leveraging large-scale pre-trained language models that have achieved state-of-the-art results on NLP tasks, we use this dataset for security-related text classification and summarization. Security-related text classification can be used to extract relevant security-related properties for protocol testing. On the other hand, summarization can help developers and practitioners understand the highlevel idea of the protocol, which is itself a daunting task. To ensure the research community can benefit from this work, all the datasets and accompanying codebase are made publicly available 1 ."
    },
    {
        "title": "Learning to Diversify Neural Text Generation via Degenerative Model",
        "authors": [
            "Jimin Hong",
            "ChaeHun Park",
            "Jaegul Choo"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.findings-ijcnlp.6.pdf",
        "summary": "Neural language models often fail to generate diverse and informative texts, limiting their applicability in real-world problems. While previous approaches have proposed to address these issues by identifying and penalizing undesirable behaviors (e.g., repetition, overuse of frequent words) from language models, we propose an alternative approach based on an observation: models primarily learn attributes within examples that are likely to cause degeneration problems. Based on this observation, we propose a new approach to prevent degeneration problems by training two models. Specifically, we first train a model that is designed to amplify undesirable patterns. We then enhance the diversity of the second model by focusing on patterns that the first model fails to learn. Extensive experiments on two tasks, namely language modeling and dialogue generation, demonstrate the effectiveness of our approach."
    },
    {
        "title": "Location Aware Modular Biencoder for Tourism Question Answering",
        "authors": [
            "Haonan Li",
            "Martin Tomko",
            "Timothy Baldwin"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.findings-ijcnlp.9.pdf",
        "summary": "Answering real-world tourism questions that seek Point-of-Interest (POI) recommendations is challenging, as it requires both spatial and non-spatial reasoning, over a large candidate pool. The traditional method of encoding each pair of question and POI becomes inefficient when the number of candidates increases, making it infeasible for real-world applications. To overcome this, we propose treating the QA task as a dense vector retrieval problem, where we encode questions and POIs separately and retrieve the most relevant POIs for a question by utilizing embedding space similarity. We use pretrained language models (PLMs) to encode textual information, and train a location encoder to capture spatial information of POIs. Experiments on a real-world tourism QA dataset demonstrate that our approach is effective, efficient, and outperforms previous methods across all metrics. Enabled by the dense retrieval architecture, we further build a global evaluation baseline, expanding the search space by 20 times compared to previous work. We also explore several factors that impact on the model's performance through follow-up experiments. Our code and model are publicly available at https://github. com/haonan-li/LAMB."
    },
    {
        "title": "Large Language Models and Low-Resource Languages: An Examination of Armenian NLP",
        "authors": [
            "Hayastan Avetisyan",
            "David Broneske"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.findings-ijcnlp.18.pdf",
        "summary": "This paper presents a comprehensive review of Natural Language Processing (NLP) research on Armenian, a language that, despite its rich history and unique linguistic characteristics, is currently low-resource in the field of NLP. We critically synthesize and evaluate various studies in Armenian NLP, highlighting key advancements, challenges, and areas for improvement. A notable aspect of our work is the underlined lack of application of Large Language Models (LLMs) in Armenian NLP, signifying an area of potential exploration and development. Identifying and discussing these challenges and opportunities lays the groundwork for future research directions in Armenian NLP. The emphasis on Armenian also advocates for increased attention to low-resource languages in NLP research, stressing the importance of linguistic diversity and equity. To the best of our knowledge, this is the first paper providing such an extensive review of Armenian NLP, marking a significant contribution to the field."
    },
    {
        "title": "Improving Machine Reading Comprehension through A Simple Masked-Training Scheme",
        "authors": [
            "Xun Yao",
            "Junlong Ma",
            "Xinrong Hu",
            "Jie Yang",
            "Yuan-Fang Li"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.findings-ijcnlp.20.pdf",
        "summary": "Extractive Question Answering (EQA) is a fundamental problem in Natural Language Understanding, aiming at answering given questions via extracting a contiguous sequence or span of words from a passage. Recent work on EQA has achieved promising performance with the help of pre-trained language models, for which Masked Language Modeling (MLM) is usually adopted as a pre-training task to predict masked tokens. This paper revisits MLM and proposes a simple yet effective method to improve the EQA performance, termed the [Mask]-for-Answering method (M4A). Specifically, three masking strategies are first introduced, which produce masked copies of the original passages. Instead of predicting masked tokens as in MLM, both original samples and masked copies are utilized simultaneously for training the EQA model. Importantly, a discrepancy loss is further incorporated to ensure that masked copies remain semantically close to the originals. As such, M4A is able to produce robust embeddings for both original and masked samples and infer correct answers even with masked context. Experimental study on several highly-competitive benchmarks consistently demonstrates the superiority of our proposed method over existing methods. M4A also achieves strong performance in low-resource settings and out-of-domain generalization."
    },
    {
        "title": "My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks",
        "authors": [
            "Tanmay Chavan",
            "Omkar Gokhale",
            "Aditya Kane",
            "Shantanu Patankar",
            "Raviraj Joshi"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.findings-ijcnlp.22.pdf",
        "summary": "The research on code-mixed data is limited due to the unavailability of dedicated code-mixed datasets and pre-trained language models. In this work, we focus on the low-resource Indian language Marathi which lacks any prior work in code-mixing. We present L3Cube-MeCorpus, a large code-mixed Marathi-English (Mr-En) corpus with 10 million social media sentences for pretraining. We also release L3Cube-MeBERT and MeRoBERTa, code-mixed BERT-based transformer models pre-trained on MeCorpus. Furthermore, for benchmarking, we present three supervised datasets MeHate, MeSent, and MeLID for downstream tasks like codemixed Mr-En hate speech detection, sentiment analysis, and language identification respectively. These evaluation datasets individually consist of manually annotated ~12,000 Marathi-English code-mixed tweets. Ablations show that the models trained on this novel corpus significantly outperform the existing state-of-the-art BERT models. This is the first work that presents artifacts for code-mixed Marathi research. All datasets and models are publicly released at https://github.com/ l3cube-pune/MarathiNLP."
    },
    {
        "title": "Template Filling for Controllable Commonsense Reasoning",
        "authors": [
            "Dheeraj Rajagopal",
            "Vivek Khetan",
            "Bogdan Sacaleanu",
            "Anatole Gershman",
            "Andrew E. Fano Fano",
            "Eduard Hovy"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.findings-ijcnlp.23.pdf",
        "summary": "Large-scale sequence-to-sequence models have shown to be adept at both multiple-choice and open-domain commonsense reasoning tasks. However, the current formulations do not provide the ability to control the various attributes of the reasoning chain. To enable better controllability, we propose to study the commonsense reasoning as a template filling task (TemplateCSR)-where the language models fills reasoning templates with the given constraints as control factors. As an approach to TemplateCSR, we (i) propose a dataset of commonsense reasoning templateexpansion pairs for healthcare and well-being domain and (ii) introduce ITO, an instruction fine-tuned sequence-to-sequence model that performs commonsense reasoning across concepts in the template. Our experiments show that our approach outperforms baseline both in generation metrics and factuality metrics. We also present a detailed error analysis on our approach's ability to reliably perform template based commonsense reasoning 1 ."
    },
    {
        "title": "Mitigating Word Bias in Zero-shot Prompt-based Classifiers",
        "authors": [
            "Adian Liusie",
            "Potsawee Manakul",
            "Mark Gales"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.findings-ijcnlp.29.pdf",
        "summary": "Prompt-based classifiers are an attractive approach for zero-shot classification. However, the precise choice of the prompt template and label words can largely influence performance, with semantically equivalent settings often showing notable performance difference. This discrepancy can be partly attributed to word biases, where the classifier may be biased towards classes. To address this problem, it is possible to optimise classification thresholds on a labelled data set, however, this mitigates some of the advantages of prompt-based classifiers. This paper instead approaches this problem by examining the expected marginal probabilities of the classes. Here, probabilities are reweighted to have a uniform prior over classes, in an unsupervised fashion. Further, we draw a theoretical connection between the class priors and the language models' word prior, and offer the ability to set a threshold in a zero-resource fashion. We show that matching class priors correlates strongly with the oracle upper bound performance and demonstrate large consistent performance gains for prompt settings over a range of NLP tasks. 1"
    },
    {
        "title": "Mixing It Up: Inducing Empathy and Politeness using Multiple Behaviour-aware Generators for Conversational Systems",
        "authors": [
            "Mauajama Firdaus",
            "Priyanshu Priya",
            "Asif Ekbal"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.findings-ijcnlp.30.pdf",
        "summary": "Politeness is a key component that can assist in building a strong customer-agent relationship. With the ongoing increase in customer-care systems, it is crucial to have healthy relations with the users providing satisfaction and a better customer experience. In this regard, it is significant to model the different polite behaviors in an agent to help the user in reaching the intended objectives. In our current work, we propose the task of polite behavior-aware generation considering the affective state of the user and the conversational context. We design a Transformer based encoder-decoder framework with three major components i.e., Affective tracker, Behaviour-aware generators, and Polite generator. The affective tracker is a context encoder that captures the contextual information along with the affective information in the utterances; the behavior-aware generators independently attends to the context information to compute behavior-aware polite representations and finally, polite generator generates the final polite response considering the representations from different generators. Experimental results on the CYCCD dataset prove that our approach generates contextually correct and relevant responses compared to the state-of-the-art approaches and the baselines."
    },
    {
        "title": "Few-Shot Adaptation for Parsing Contextual Utterances with LLMs",
        "authors": [
            "Kevin Lin",
            "Patrick Xia",
            "Hao Fang"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.findings-ijcnlp.31.pdf",
        "summary": "We evaluate the ability of semantic parsers based on large language models (LLMs) to handle contextual utterances. In real-world settings, there typically exists only a limited number of annotated contextual utterances due to annotation cost, resulting in an imbalance compared to non-contextual utterances. Therefore, parsers must adapt to contextual utterances with a few training examples. We examine four major paradigms for doing so in conversational semantic parsing i.e., Parse-with-Utterance-History, Parse-with-Reference-Program, Parsethen-Resolve, and Rewrite-then-Parse. To facilitate such cross-paradigm comparisons, we construct SMCalFlow-EventQueries, a subset of contextual examples from SMCalFlow with additional annotations. Experiments with in-context learning and fine-tuning suggest that Rewrite-then-Parse is the most promising paradigm when holistically considering parsing accuracy, annotation cost, and error types."
    },
    {
        "title": "Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study",
        "authors": [
            "Yi Chen",
            "Rui Wang",
            "Haiyun Jiang",
            "Shuming Shi",
            "Ruifeng Xu"
        ],
        "published": "2023",
        "pdf_link": "https://aclanthology.org/2023.findings-ijcnlp.32.pdf",
        "summary": "Evaluating the quality of generated text is a challenging task in NLP, due to the inherent complexity and diversity of text. Recently, large language models (LLMs) have garnered significant attention due to their impressive performance in various tasks. Therefore, we present this paper to investigate the effectiveness of LLMs, especially ChatGPT, and explore ways to optimize their use in assessing text quality. We compared three kinds of referencefree evaluation methods. The experimental results prove that ChatGPT is capable of evaluating text quality effectively from various perspectives without reference and demonstrates superior performance than most existing automatic metrics. In particular, the Explicit Score, which utilizes ChatGPT to generate a numeric score measuring text quality, is the most effective and reliable method among the three exploited approaches. However, directly comparing the quality of two texts may lead to suboptimal results. We believe this paper will provide valuable insights for evaluating text quality with LLMs and have released the used data 1 ."
    }
]