[
    {
        "title": "Revisiting Design Choices in Offline Model Based Reinforcement Learning",
        "authors": [
            "Cong Lu",
            "Philip Ball",
            "Jack Parker-Holder",
            "Michael Osborne",
            "Stephen J. Roberts"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Offline reinforcement learning enables agents to leverage large pre-collected datasets of environment transitions to learn control policies, circumventing the need for potentially expensive or unsafe online data collection. Significant progress has been made recently in offline model-based reinforcement learning, approaches which leverage a learned dynamics model. This typically involves constructing a probabilistic model, and using the model uncertainty to penalize rewards where there is insufficient data, solving for a pessimistic MDP that lower bounds the true MDP. Existing methods, however, exhibit a breakdown between theory and practice, whereby pessimistic return ought to be bounded by the total variation distance of the model from the true dynamics, but is instead implemented through a penalty based on estimated model uncertainty. This has spawned a variety of uncertainty heuristics, with little to no comparison between differing approaches. In this paper, we compare these heuristics, and design novel protocols to investigate their interaction with other hyperparameters, such as the number of models, or imaginary rollout horizon. Using these insights, we show that selecting these key hyperparameters using Bayesian Optimization produces superior configurations that are vastly different to those currently used in existing hand-tuned state-of-the-art methods, and result in drastically stronger performance.",
        "pdf_link": "https://openreview.net/pdf/1fd710c82e5202735a840dcabdf897afa2030b34.pdf",
        "forum_url": "https://openreview.net/forum?id=zz9hXVhf40",
        "keywords": [
            "reinforcement learning",
            "offline reinforcement learning",
            "design choices"
        ]
    },
    {
        "title": "On the relation between statistical learning and perceptual distances",
        "authors": [
            "Alexander Hepburn",
            "Valero Laparra",
            "Raul Santos-Rodriguez",
            "Johannes Ball\u00e9",
            "Jesus Malo"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "It has been demonstrated many times that the behavior of the human visual system is connected to the statistics of natural images. Since machine learning relies on the statistics of training data as well, the above connection has interesting implications when using perceptual distances (which mimic the behavior of the human visual system) as a loss function. In this paper, we aim to unravel the non-trivial relationships between the probability distribution of the data, perceptual distances, and unsupervised machine learning. To this end, we show that perceptual sensitivity is correlated with the probability of an image in its close neighborhood. We also explore the relation between distances induced by autoencoders and the probability distribution of the training data, as well as how these induced distances are correlated with human perception. Finally, we find perceptual distances do not always lead to noticeable gains in performance over Euclidean distance in common image processing tasks, except when data is scarce and the perceptual distance provides regularization. We propose this may be due to a double-counting effect of the image statistics, once in the perceptual distance and once in the training procedure.",
        "pdf_link": "https://openreview.net/pdf/12c717193deff83fed4cdbbc207d6c4ffebad63e.pdf",
        "forum_url": "https://openreview.net/forum?id=zXM0b4hi5_B",
        "keywords": [
            "statistical learning",
            "perceptual distances",
            "unsupervised machine learning",
            "perceptual sensitivity",
            "regularization"
        ]
    },
    {
        "title": "Equivariant Transformers for Neural Network based Molecular Potentials",
        "authors": [
            "Philipp Th\u00f6lke",
            "Gianni De Fabritiis"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "The prediction of quantum mechanical properties is historically plagued by a trade-off between accuracy and speed. Machine learning potentials have previously shown great success in this domain, reaching increasingly better accuracy while maintaining computational efficiency comparable with classical force fields. In this work we propose TorchMD-NET, a novel equivariant Transformer (ET) architecture, outperforming state-of-the-art on MD17, ANI-1, and many QM9 targets in both accuracy and computational efficiency. Through an extensive attention weight analysis, we gain valuable insights into the black box predictor and show differences in the learned representation of conformers versus conformations sampled from molecular dynamics or normal modes. Furthermore, we highlight the importance of datasets including off-equilibrium conformations for the evaluation of molecular potentials.",
        "pdf_link": "https://openreview.net/pdf/3567cf90fbb34ac3013bdb0c392a4d115421dec8.pdf",
        "forum_url": "https://openreview.net/forum?id=zNHzqZ9wrRB",
        "keywords": [
            "molecular potentials",
            "neural network",
            "molecular dynamics",
            "machine learning potentials",
            "equivariant transformers"
        ]
    },
    {
        "title": "Frame Averaging for Invariant and Equivariant Network Design",
        "authors": [
            "Omri Puny",
            "Matan Atzmon",
            "Edward J. Smith",
            "Ishan Misra",
            "Aditya Grover",
            "Heli Ben-Hamu",
            "Yaron Lipman"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Many machine learning tasks involve learning functions that are known to be invariant or equivariant to certain symmetries of the input data. However, it is often challenging to design neural network architectures that respect these symmetries while being expressive and computationally efficient. For example, Euclidean motion invariant/equivariant graph or point cloud neural networks. \nWe introduce Frame Averaging (FA), a highly general purpose and systematic framework for adapting known (backbone) architectures to become invariant or equivariant to new symmetry types. Our framework builds on the well known group averaging operator that guarantees invariance or equivariance but is intractable. In contrast, we observe that for many important classes of symmetries, this operator can be replaced with an averaging operator over a small subset of the group elements, called a frame. We show that averaging over a frame guarantees exact invariance or equivariance while often being much simpler to compute than averaging over the entire group. Furthermore, we prove that FA-based models have maximal expressive power in a broad setting and in general preserve the expressive power of their backbone architectures. Using frame averaging, we propose a new class of universal Graph Neural Networks (GNNs), universal Euclidean motion invariant point cloud networks, and Euclidean motion invariant Message Passing (MP) GNNs. We demonstrate the practical effectiveness of FA on several applications including point cloud normal estimation, beyond $2$-WL graph separation, and $n$-body dynamics prediction, achieving state-of-the-art results in all of these benchmarks.",
        "pdf_link": "https://openreview.net/pdf/d7849f0ef0f911d06889785dc7116564d5342442.pdf",
        "forum_url": "https://openreview.net/forum?id=zIUyj55nXR",
        "keywords": [
            "frame averaging",
            "invariant",
            "neural",
            "neural networks",
            "equivariant network design",
            "group averaging",
            "universal graph neural networks",
            "expressive"
        ]
    },
    {
        "title": "Extending the WILDS Benchmark for Unsupervised Adaptation",
        "authors": [
            "Shiori Sagawa",
            "Pang Wei Koh",
            "Tony Lee",
            "Irena Gao",
            "Sang Michael Xie",
            "Kendrick Shen",
            "Ananya Kumar",
            "Weihua Hu",
            "Michihiro Yasunaga",
            "Henrik Marklund",
            "Sara Beery",
            "Etienne David",
            "Ian Stavness",
            "Wei Guo",
            "Jure Leskovec",
            "Kate Saenko",
            "Tatsunori Hashimoto",
            "Sergey Levine",
            "Chelsea Finn",
            "Percy Liang"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Machine learning systems deployed in the wild are often trained on a source distribution but deployed on a different target distribution. Unlabeled data can be a powerful point of leverage for mitigating these distribution shifts, as it is frequently much more available than labeled data and can often be obtained from distributions beyond the source distribution as well. However, existing distribution shift benchmarks with unlabeled data do not reflect the breadth of scenarios that arise in real-world applications. In this work, we present the WILDS 2.0 update, which extends 8 of the 10 datasets in the WILDS benchmark of distribution shifts to include curated unlabeled data that would be realistically obtainable in deployment. These datasets span a wide range of applications (from histology to wildlife conservation), tasks (classification, regression, and detection), and modalities (photos, satellite images, microscope slides, text, molecular graphs). The update maintains consistency with the original WILDS benchmark by using identical labeled training, validation, and test sets, as well as identical evaluation metrics. We systematically benchmark state-of-the-art methods that use unlabeled data, including domain-invariant, self-training, and self-supervised methods, and show that their success on WILDS is limited. To facilitate method development, we provide an open-source package that automates data loading and contains the model architectures and methods used in this paper. Code and leaderboards are available at https://wilds.stanford.edu.",
        "pdf_link": "https://openreview.net/pdf/16bc69d47c7ff67867bfc50009d6b9fc5043a00f.pdf",
        "forum_url": "https://openreview.net/forum?id=z7p2V6KROOV",
        "keywords": [
            "machine learning",
            "distribution shift benchmarks",
            "unsupervised adaptation",
            "classification",
            "wilds benchmark"
        ]
    },
    {
        "title": "Autoregressive Quantile Flows for Predictive Uncertainty Estimation",
        "authors": [
            "Phillip Si",
            "Allan Bishop",
            "Volodymyr Kuleshov"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Numerous applications of machine learning involve representing probability distributions over high-dimensional data. We propose autoregressive quantile flows, a flexible class of normalizing flow models trained using a novel objective based on proper scoring rules. Our objective does not require calculating computationally expensive determinants of Jacobians during training and supports new types of neural architectures, such as neural autoregressive flows from which it is easy to sample. \n    We leverage these models in quantile flow regression, an approach that parameterizes predictive conditional distributions with flows, resulting in improved probabilistic predictions on tasks such as time series forecasting and object detection.\n    Our novel objective functions and neural flow parameterizations also yield improvements on popular generation and density estimation tasks, and represent a step beyond maximum likelihood learning of flows.",
        "pdf_link": "https://openreview.net/pdf/564438790b7385df4f30b72aafb410555a14f948.pdf",
        "forum_url": "https://openreview.net/forum?id=z1-I6rOKv1S",
        "keywords": [
            "proper scoring rules",
            "autoregressive quantile flows",
            "density estimation",
            "predictive uncertainty estimation",
            "quantile flow regression"
        ]
    },
    {
        "title": "Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks",
        "authors": [
            "Anne Harrington",
            "Arturo Deza"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Recent work suggests that feature constraints in the training datasets of deep neural networks (DNNs) drive robustness to adversarial noise (Ilyas et al., 2019). The representations learned by such adversarially robust networks have also been shown to be more human perceptually-aligned than non-robust networks via image manipulations (Santurkar et al., 2019, Engstrom et al., 2019). Despite appearing closer to human visual perception, it is unclear if the constraints in robust DNN representations match biological constraints found in human vision. Human vision seems to rely on texture-based/summary statistic representations in the periphery, which have been shown to explain phenomena such as crowding (Balas et al., 2009) and performance on visual search tasks (Rosenholtz et al., 2012). To understand how adversarially robust optimizations/representations compare to human vision, we performed a psychophysics experiment using a metamer task similar to Freeman \\& Simoncelli, 2011, Wallis et al., 2016 and Deza et al., 2019 where we evaluated how well human observers could distinguish between images synthesized to match adversarially robust representations compared to non-robust representations and a texture synthesis model of peripheral vision (Texforms a la Long et al., 2018).  We found that the discriminability of robust representation and texture model images decreased to near chance performance as stimuli were presented farther in the periphery.  Moreover, performance on robust and texture-model images showed similar trends within participants, while performance on non-robust representations changed minimally across the visual field.  These results together suggest that (1) adversarially robust representations capture peripheral computation better than non-robust representations and (2) robust representations capture peripheral computation similar to current state-of-the-art texture peripheral vision models. More broadly, our findings support the idea that localized texture summary statistic representations may drive human invariance to adversarial perturbations and that the incorporation of such representations in DNNs could give rise to useful properties like adversarial robustness.",
        "pdf_link": "https://openreview.net/pdf/7f2e10fe0e775d6b9a7ac2d2d46206fcefd3f1ca.pdf",
        "forum_url": "https://openreview.net/forum?id=yeP_zx9vqNm",
        "keywords": [
            "robust",
            "metameric tasks",
            "peripheral vision",
            "adversarial robustness",
            "texture peripheral vision",
            "adversarial",
            "adversarially robust networks",
            "texture synthesis",
            "robust representations",
            "non robust representations",
            "visual perception"
        ]
    },
    {
        "title": "Imbedding Deep Neural Networks",
        "authors": [
            "Andrew Corbett",
            "Dmitry Kangin"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Continuous-depth neural networks, such as Neural ODEs, have refashioned the understanding of residual neural networks in terms of non-linear vector-valued optimal control problems. The common solution is to use the adjoint sensitivity method to replicate a forward-backward pass optimisation problem. We propose a new approach which explicates the network's `depth' as a fundamental variable, thus reducing the problem to a system of forward-facing initial value problems. This new method is based on the principal of `Invariant Imbedding' for which we prove a general solution, applicable to all non-linear, vector-valued optimal control problems with both running and terminal loss.\nOur new architectures provide a tangible tool for inspecting the theoretical--and to a great extent unexplained--properties of network depth. They also constitute a resource of discrete implementations of Neural ODEs comparable to classes of imbedded residual neural networks. Through a series of experiments, we show the competitive performance of the proposed architectures for supervised learning and time series prediction. ",
        "pdf_link": "https://openreview.net/pdf/2696810601b722fa25baf9dd6ed1280d43c1c474.pdf",
        "forum_url": "https://openreview.net/forum?id=yKIAXjkJc2F",
        "keywords": [
            "neural networks",
            "residual neural networks",
            "imbedding",
            "network depth",
            "adjoint sensitivity method",
            "forward backward pass optimisation",
            "optimal control"
        ]
    },
    {
        "title": "On the approximation properties of recurrent encoder-decoder architectures",
        "authors": [
            "Zhong Li",
            "Haotian Jiang",
            "Qianxiao Li"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Encoder-decoder architectures have recently gained popularity in sequence to sequence modelling, featuring in state-of-the-art models such as transformers. However, a mathematical understanding of their working principles still remains limited. In this paper, we study the approximation properties of recurrent encoder-decoder architectures. Prior work established theoretical results for RNNs in the linear setting, where approximation capabilities can be related to smoothness and memory of target temporal relationships. Here, we uncover that the encoder and decoder together form a particular \u201ctemporal product structure\u201d which determines the approximation efficiency. Moreover, the encoder-decoder architecture generalises RNNs with the capability to learn time-inhomogeneous relationships. Our results provide the theoretical understanding of approximation properties of the recurrent encoder-decoder architecture, which precisely characterises, in the considered setting, the types of temporal relationships that can be efficiently learned.",
        "pdf_link": "https://openreview.net/pdf/93c3702cdbb8429d512ae64ed57daf520f84137f.pdf",
        "forum_url": "https://openreview.net/forum?id=xDIvIqQ3DXD",
        "keywords": [
            "encoder",
            "recurrent encoder decoder",
            "recurrent encoder decoder architectures",
            "approximation",
            "approximation properties",
            "approximation efficiency",
            "encoder decoder architectures"
        ]
    },
    {
        "title": "TAMP-S2GCNets: Coupling Time-Aware Multipersistence Knowledge Representation with Spatio-Supra Graph Convolutional Networks for Time-Series Forecasting",
        "authors": [
            "Yuzhou Chen",
            "Ignacio Segovia-Dominguez",
            "Baris Coskunuzer",
            "Yulia Gel"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Graph Neural Networks (GNNs) are proven to be a powerful machinery for learning complex dependencies in multivariate spatio-temporal processes. However, most existing GNNs have inherently static architectures, and as a result, do not explicitly account for time dependencies of the encoded knowledge and are limited in their ability to simultaneously infer latent time-conditioned relations among entities. We postulate that such hidden time-conditioned properties may be captured by the tools of multipersistence, i.e, a emerging machinery in topological data analysis which allows us to quantify dynamics of the data shape along multiple geometric dimensions. \n We make the first step toward integrating the two rising research directions, that is, time-aware deep learning and multipersistence, and propose a new model, Time-Aware Multipersistence Spatio-Supra Graph Convolutional Network (TAMP-S2GCNets). We summarize inherent time-conditioned topological properties of the data as time-aware multipersistence Euler-Poincar\\'e surface and prove its stability. We then construct a supragraph convolution module which simultaneously accounts for the extracted intra- and inter- spatio-temporal dependencies in the data. Our extensive experiments on highway traffic flow, Ethereum token prices, and COVID-19 hospitalizations demonstrate that TAMP-S2GCNets outperforms the state-of-the-art tools in multivariate time series forecasting tasks.",
        "pdf_link": "https://openreview.net/pdf/7e6227c594fe1f760bcc5ecab60120d7439ba995.pdf",
        "forum_url": "https://openreview.net/forum?id=wv6g8fWLX2q",
        "keywords": [
            "time series forecasting",
            "graph neural networks",
            "knowledge representation",
            "coupling time aware",
            "time aware deep learning",
            "spatio temporal processes"
        ]
    },
    {
        "title": "iLQR-VAE : control-based learning of input-driven dynamics with applications to neural data",
        "authors": [
            "Marine Schimel",
            "Ta-Chu Kao",
            "Kristopher T Jensen",
            "Guillaume Hennequin"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Understanding how neural dynamics give rise to behaviour is one of the most fundamental questions in systems neuroscience. To achieve this, a common approach is to record neural populations in behaving animals, and model these data as emanating from a latent dynamical system whose state trajectories can then be related back to behavioural observations via some form of decoding. As recordings are typically performed in localized circuits that form only a part of the wider implicated network, it is important to simultaneously learn the local dynamics and infer any unobserved external input that might drive them. Here, we introduce iLQR-VAE, a novel control-based approach to variational inference in nonlinear dynamical systems, capable of learning both latent dynamics, initial conditions, and ongoing external inputs. As in recent deep learning approaches, our method is based on an input-driven sequential variational autoencoder (VAE). The main novelty lies in the use of the powerful iterative linear quadratic regulator algorithm (iLQR) in the recognition model. Optimization of the standard evidence lower-bound requires differentiating through iLQR solutions, which is made possible by recent advances in differentiable control. Importantly, having the recognition model be implicitly defined by the generative model greatly reduces the number of free parameters and allows for flexible, high-quality inference. This makes it possible for instance to evaluate the model on a single long trial after training on smaller chunks. We demonstrate the effectiveness of iLQR-VAE on a range of synthetic systems, with autonomous as well as input-driven dynamics. We further apply it to neural and behavioural recordings in non-human primates performing two different reaching tasks, and show that iLQR-VAE yields high-quality kinematic reconstructions from the neural data. ",
        "pdf_link": "https://openreview.net/pdf/c4b2a10a835b79e5cbaff71f6577c29236e964b5.pdf",
        "forum_url": "https://openreview.net/forum?id=wRODLDHaAiW",
        "keywords": [
            "input driven dynamics",
            "neural dynamics",
            "neural data",
            "variational inference"
        ]
    },
    {
        "title": "Context-Aware Sparse Deep Coordination Graphs",
        "authors": [
            "Tonghan Wang",
            "Liang Zeng",
            "Weijun Dong",
            "Qianlan Yang",
            "Yang Yu",
            "Chongjie Zhang"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Learning sparse coordination graphs adaptive to the coordination dynamics among agents is a long-standing problem in cooperative multi-agent learning. This paper studies this problem and proposes a novel method using the variance of payoff functions to construct context-aware sparse coordination topologies. We theoretically consolidate our method by proving that the smaller the variance of payoff functions is, the less likely action selection will change after removing the corresponding edge. Moreover, we propose to learn action representations to effectively reduce the influence of payoff functions' estimation errors on graph construction. To empirically evaluate our method, we present the Multi-Agent COordination (MACO) benchmark by collecting classic coordination problems in the literature, increasing their difficulty, and classifying them into different types. We carry out a case study and experiments on the MACO and StarCraft II micromanagement benchmark to demonstrate the dynamics of sparse graph learning, the influence of graph sparseness, and the learning performance of our method.",
        "pdf_link": "https://openreview.net/pdf/a4e94260f9a234c7a8d59eb139b8f31b32a87673.pdf",
        "forum_url": "https://openreview.net/forum?id=wQfgfb8VKTn",
        "keywords": [
            "context aware",
            "multi agent coordination",
            "agent learning",
            "action representations"
        ]
    },
    {
        "title": "Expressiveness and Approximation Properties of Graph Neural Networks",
        "authors": [
            "Floris Geerts",
            "Juan L Reutter"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Characterizing the separation power of graph neural networks (GNNs) provides an understanding of their limitations for graph learning tasks. Results regarding separation power are, however, usually geared at specific GNNs architectures, and tools for understanding arbitrary GNN architectures are generally lacking. We provide an elegant way to easily obtain bounds on the separation power of GNNs in terms of the Weisfeiler-Leman (WL) tests, which have become the yardstick to measure the separation power of GNNs. The crux is to view GNNs as expressions in a procedural tensor language describing the computations in the layers of the GNNs. Then, by a simple analysis of the obtained expressions, in terms of the number of indexes used and the nesting depth of summations, bounds on the separation power in terms of the WL-tests readily follow. We use tensor language to define Higher-Order Message-Passing Neural Networks (or k-MPNNs), a natural extension of MPNNs. Furthermore, the tensor language point of view allows for the derivation of universality results for classes of GNNs in a natural way. Our approach provides a toolbox with which GNN architecture designers can analyze the separation power of their GNNs, without needing to know the intricacies of the WL-tests. We also provide insights in what is needed to boost the separation power of GNNs.",
        "pdf_link": "https://openreview.net/pdf/9d0fe7ff08261aae56611b7f670de9875c2a9cd9.pdf",
        "forum_url": "https://openreview.net/forum?id=wIzUeM3TAU",
        "keywords": [
            "separation power",
            "neural networks",
            "graph neural networks",
            "gnn",
            "nesting depth",
            "universality"
        ]
    },
    {
        "title": "D-CODE: Discovering Closed-form ODEs from Observed Trajectories",
        "authors": [
            "Zhaozhi Qian",
            "Krzysztof Kacprzyk",
            "Mihaela van der Schaar"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "For centuries, scientists have manually designed closed-form ordinary differential equations (ODEs) to model dynamical systems. An automated tool to distill closed-form ODEs from observed trajectories would accelerate the modeling process. Traditionally, symbolic regression is used to uncover a closed-form prediction function $a=f(b)$ with label-feature pairs $(a_i, b_i)$ as training examples. However, an ODE models the time derivative $\\dot{x}(t)$ of a dynamical system, e.g. $\\dot{x}(t) = f(x(t),t)$, and the \"label\" $\\dot{x}(t)$ is usually *not* observed. The existing ways to bridge this gap only perform well for a narrow range of settings with low measurement noise, frequent sampling, and non-chaotic dynamics. In this work, we propose the Discovery of Closed-form ODE framework (D-CODE), which advances symbolic regression beyond the paradigm of supervised learning. D-CODE leverages a novel objective function based on the variational formulation of ODEs to bypass the unobserved time derivative. For formal justification, we prove that this objective is a valid proxy for the estimation error of the true (but unknown) ODE. In the experiments, D-CODE successfully discovered the governing equations of a diverse range of dynamical systems under challenging measurement settings with high noise and infrequent sampling.",
        "pdf_link": "https://openreview.net/pdf/3a0bdc107d197bd18aa7299f8d8f198db2225d03.pdf",
        "forum_url": "https://openreview.net/forum?id=wENMvIsxNN",
        "keywords": [
            "dynamics",
            "dynamical systems",
            "trajectories",
            "d code",
            "odes"
        ]
    },
    {
        "title": "Spanning Tree-based Graph Generation for Molecules",
        "authors": [
            "Sungsoo Ahn",
            "Binghong Chen",
            "Tianzhe Wang",
            "Le Song"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "In this paper, we explore the problem of generating molecules using deep neural networks, which has recently gained much interest in chemistry. To this end, we propose a spanning tree-based graph generation (STGG) framework based on formulating molecular graph generation as a construction of a spanning tree and the residual edges. Such a formulation exploits the sparsity of molecular graphs and allows using compact tree-constructive operations to define the molecular graph connectivity. Based on the intermediate graph structure of the construction process, our framework can constrain its generation to molecular graphs that satisfy the chemical valence rules. We also newly design a Transformer architecture with tree-based relative positional encodings for realizing the tree construction procedure. Experiments on QM9, ZINC250k, and MOSES benchmarks verify the effectiveness of the proposed framework in metrics such as validity, Frechet ChemNet distance, and fragment similarity. We also demonstrate the usefulness of STGG in maximizing penalized LogP value of molecules.",
        "pdf_link": "https://openreview.net/pdf/dcb1134d836d26dc8ef7d83683aa9f5b35964eae.pdf",
        "forum_url": "https://openreview.net/forum?id=w60btE_8T2m",
        "keywords": [
            "graph generation",
            "spanning tree",
            "tree construction",
            "molecular graph connectivity"
        ]
    },
    {
        "title": "Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path",
        "authors": [
            "X.Y. Han",
            "Vardan Papyan",
            "David L. Donoho"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "The recently discovered Neural Collapse (NC) phenomenon occurs pervasively in today's deep net training paradigm of driving cross-entropy (CE) loss towards zero. During NC, last-layer features collapse to their class-means, both classifiers and class-means collapse to the same Simplex Equiangular Tight Frame, and classifier behavior collapses to the nearest-class-mean decision rule. Recent works demonstrated that deep nets trained with mean squared error (MSE) loss perform comparably to those trained with CE. As a preliminary, we empirically establish that NC emerges in such MSE-trained deep nets as well through experiments on three canonical networks and five benchmark datasets. We provide, in a Google Colab notebook, PyTorch code for reproducing MSE-NC and CE-NC: https://colab.research.google.com/github/neuralcollapse/neuralcollapse/blob/main/neuralcollapse.ipynb. The analytically-tractable MSE loss offers more mathematical opportunities than the hard-to-analyze CE loss, inspiring us to leverage MSE loss towards the theoretical investigation of NC. We develop three main contributions: (I) We show a new decomposition of the MSE loss into (A) terms directly interpretable through the lens of NC and which assume the last-layer classifier is exactly the least-squares classifier; and (B) a term capturing the deviation from this least-squares classifier. (II) We exhibit experiments on canonical datasets and networks demonstrating that term-(B) is negligible during training. This motivates us to introduce a new theoretical construct: the central path, where the linear classifier stays MSE-optimal for feature activations throughout the dynamics. (III) By studying renormalized gradient flow along the central path, we derive exact dynamics that predict NC.",
        "pdf_link": "https://openreview.net/pdf/75799bbe466f7240935655cbfaa930c9628a915e.pdf",
        "forum_url": "https://openreview.net/forum?id=w1UbdvWH_R3",
        "keywords": [
            "dynamics",
            "collapse",
            "central path",
            "proximity",
            "mse loss",
            "exact dynamics",
            "neural collapse",
            "cross entropy",
            "tight"
        ]
    },
    {
        "title": "Long Expressive Memory for Sequence Modeling",
        "authors": [
            "T. Konstantin Rusch",
            "Siddhartha Mishra",
            "N. Benjamin Erichson",
            "Michael W. Mahoney"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "We propose a novel method called Long Expressive Memory (LEM) for learning long-term sequential dependencies. LEM is gradient-based, it can efficiently process sequential tasks with very long-term dependencies, and it is sufficiently expressive to be able to learn complicated input-output maps. To derive LEM, we consider a system of multiscale ordinary differential equations, as well as a suitable time-discretization of this system. For LEM, we derive rigorous bounds to show the mitigation of the exploding and vanishing gradients problem, a well-known challenge for gradient-based recurrent sequential learning methods. We also prove that LEM can approximate a large class of dynamical systems to high accuracy. Our empirical results, ranging from image and time-series classification through dynamical systems prediction to speech recognition and language modeling, demonstrate that LEM outperforms state-of-the-art recurrent neural networks, gated recurrent units, and long short-term memory models.",
        "pdf_link": "https://openreview.net/pdf/af3e85f49d797e1d183908208759b1776c72eb5d.pdf",
        "forum_url": "https://openreview.net/forum?id=vwj6aUeocyf",
        "keywords": [
            "sequence modeling",
            "language modeling",
            "expressive memory",
            "long expressive memory"
        ]
    },
    {
        "title": "Sample Efficient Deep Reinforcement Learning via Uncertainty Estimation",
        "authors": [
            "Vincent Mai",
            "Kaustubh Mani",
            "Liam Paull"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "In model-free deep reinforcement learning (RL) algorithms, using noisy value estimates to supervise policy evaluation and optimization is detrimental to the sample efficiency. As this noise is heteroscedastic, its effects can be mitigated using uncertainty-based weights in the optimization process. Previous methods rely on sampled ensembles, which do not capture all aspects of uncertainty. We provide a systematic analysis of the sources of uncertainty in the noisy supervision that occurs in RL, and introduce inverse-variance RL, a Bayesian framework which combines probabilistic ensembles and Batch Inverse Variance weighting. We propose a method whereby two complementary uncertainty estimation methods account for both the Q-value and the environment stochasticity to better mitigate the negative impacts of noisy supervision. Our results show significant improvement in terms of sample efficiency on discrete and continuous control tasks.",
        "pdf_link": "https://openreview.net/pdf/2957fd1597c9d0c85f628e1d53b0aba1a7aa45b1.pdf",
        "forum_url": "https://openreview.net/forum?id=vrW3tvDfOJQ",
        "keywords": [
            "uncertainty estimation",
            "deep reinforcement learning",
            "sample efficiency"
        ]
    },
    {
        "title": "Superclass-Conditional Gaussian Mixture Model For Learning Fine-Grained Embeddings",
        "authors": [
            "Jingchao Ni",
            "Wei Cheng",
            "Zhengzhang Chen",
            "Takayoshi Asakura",
            "Tomoya Soma",
            "Sho Kato",
            "Haifeng Chen"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Learning fine-grained embeddings is essential for extending the generalizability of models pre-trained on \"coarse\" labels (e.g., animals). It is crucial to fields for which fine-grained labeling (e.g., breeds of animals) is expensive, but fine-grained prediction is desirable, such as medicine. The dilemma necessitates adaptation of a \"coarsely\" pre-trained model to new tasks with a few \"finer-grained\" training labels. However, coarsely supervised pre-training tends to suppress intra-class variation, which is vital for cross-granularity adaptation. In this paper, we develop a training framework underlain by a novel superclass-conditional Gaussian mixture model (SCGM). SCGM imitates the generative process of samples from hierarchies of classes through latent variable modeling of the fine-grained subclasses. The framework is agnostic to the encoders and only adds a few distribution related parameters, thus is efficient, and flexible to different domains. The model parameters are learned end-to-end by maximum-likelihood estimation via a principled Expectation-Maximization algorithm. Extensive experiments on benchmark datasets and a real-life medical dataset indicate the effectiveness of our method.",
        "pdf_link": "https://openreview.net/pdf/6eed00839e8f02de0ecba5b42ac5ce892def5ac0.pdf",
        "forum_url": "https://openreview.net/forum?id=vds4SNooOe",
        "keywords": [
            "fine grained embeddings",
            "fine grained prediction",
            "fine grained labeling",
            "latent variable modeling"
        ]
    },
    {
        "title": "Message Passing Neural PDE Solvers",
        "authors": [
            "Johannes Brandstetter",
            "Daniel E. Worrall",
            "Max Welling"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "The numerical solution of partial differential equations (PDEs) is difficult, having led to a century of research so far. Recently, there have been pushes to build neural--numerical hybrid solvers, which piggy-backs the modern trend towards fully end-to-end learned systems. Most works so far can only generalize over a subset of properties to which a generic solver would be faced, including: resolution, topology, geometry, boundary conditions, domain discretization regularity, dimensionality, etc. In this work, we build a solver, satisfying these properties, where all the components are based on neural message passing, replacing all heuristically designed components in the computation graph with backprop-optimized neural function approximators. We show that neural message passing solvers representationally contain some classical methods, such as finite differences, finite volumes, and WENO schemes. In order to encourage stability in training autoregressive models, we put forward a method that is based on the principle of zero-stability, posing stability as a domain adaptation problem. We validate our method on various fluid-like flow problems, demonstrating fast, stable, and accurate performance across different domain topologies, discretization, etc. in 1D and 2D. Our model outperforms state-of-the-art numerical solvers in the low resolution regime in terms of speed, and accuracy.",
        "pdf_link": "https://openreview.net/pdf/dfb3e9b359e53414eb2852a5ed8ed48038f889c0.pdf",
        "forum_url": "https://openreview.net/forum?id=vSix3HPYKSU",
        "keywords": [
            "neural message passing solvers",
            "numerical solvers",
            "domain adaptation problem"
        ]
    },
    {
        "title": "Implicit Bias of Projected Subgradient Method Gives Provable Robust Recovery of Subspaces of Unknown Codimension",
        "authors": [
            "Paris Giampouras",
            "Benjamin David Haeffele",
            "Rene Vidal"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Robust subspace recovery (RSR) is the problem of learning a subspace from sample data points corrupted by outliers. Dual Principal Component Pursuit (DPCP) is a robust subspace recovery method that aims to find a basis for the orthogonal complement of the subspace by minimizing the sum of the distances of the points to the subspaces subject to orthogonality constraints on the basis. Prior work has shown that DPCP can provably recover the correct subspace in the presence of outliers as long as the true dimension of the subspace is known. In this paper, we show that if the orthogonality constraints --adopted in previous DPCP formulations-- are relaxed and random initialization is used instead of spectral one, DPCP can provably recover a subspace of \\emph{unknown dimension}. Specifically, we propose a very simple algorithm based on running multiple instances of a projected sub-gradient descent method (PSGM), with each problem instance seeking to find one vector in the null space of the subspace. We theoretically prove that under mild conditions this approach succeeds with high probability. In particular, we show that 1) all of the problem instances will converge to a vector in the nullspace of the subspace and 2) the ensemble of problem instance solutions will be sufficiently diverse to fully span the nullspace of the subspace thus also revealing its true unknown codimension. We provide empirical results that corroborate our theoretical results and showcase the remarkable implicit rank regularization behavior of the PSGM algorithm that allows us to perform RSR without knowing the subspace dimension",
        "pdf_link": "https://openreview.net/pdf/4590e3755b5109113b2a58fd038313d6a8b091ec.pdf",
        "forum_url": "https://openreview.net/forum?id=vA7doMdgi75",
        "keywords": [
            "orthogonality constraints",
            "projected subgradient method",
            "robust subspace recovery",
            "rank regularization",
            "dual principal component",
            "bias"
        ]
    },
    {
        "title": "A New Perspective on \"How Graph Neural Networks Go Beyond Weisfeiler-Lehman?\"",
        "authors": [
            "Asiri Wijesinghe",
            "Qing Wang"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "We propose a new perspective on designing powerful Graph Neural Networks (GNNs). In a nutshell, this enables a general solution to inject structural properties of graphs into a message-passing aggregation scheme of GNNs. As a theoretical basis, we develop a new hierarchy of local isomorphism on neighborhood subgraphs. Then, we theoretically characterize how message-passing GNNs can be designed to be more expressive than the Weisfeiler Lehman test. To elaborate this characterization, we propose a novel neural model, called GraphSNN, and prove that this model is strictly more expressive than the Weisfeiler Lehman test in distinguishing graph structures. We empirically verify the strength of our model on different graph learning tasks. It is shown that our model consistently improves the state-of-the-art methods on the benchmark tasks without sacrificing computational simplicity and efficiency.",
        "pdf_link": "https://openreview.net/pdf/376e7da3d7f86a2bd40cd51fadfc278e94372443.pdf",
        "forum_url": "https://openreview.net/forum?id=uxgg9o7bI_3",
        "keywords": [
            "graph neural networks",
            "weisfeiler lehman test",
            "graph learning",
            "message passing aggregation",
            "local isomorphism"
        ]
    },
    {
        "title": "Strength of Minibatch Noise in SGD",
        "authors": [
            "Liu Ziyin",
            "Kangqiao Liu",
            "Takashi Mori",
            "Masahito Ueda"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "The noise in stochastic gradient descent (SGD), caused by minibatch sampling, is poorly understood despite its practical importance in deep learning. This work presents the first systematic study of the SGD noise and fluctuations close to a local minimum. We first analyze the SGD noise in linear regression in detail and then derive a general formula for approximating SGD noise in different types of minima. For application, our results (1) provide insight into the stability of training a neural network, (2) suggest that a large learning rate can help generalization by introducing an implicit regularization, (3) explain why the linear learning rate-batchsize scaling law fails at a large learning rate or at a small batchsize and (4) can provide an understanding of how discrete-time nature of SGD affects the recently discovered power-law phenomenon of SGD.",
        "pdf_link": "https://openreview.net/pdf/d9f2b7e88fec2d057ac6f54cbbd1ecdff0f06afb.pdf",
        "forum_url": "https://openreview.net/forum?id=uorVGbWV5sw",
        "keywords": [
            "minibatch sampling",
            "minibatch noise",
            "neural network",
            "noise",
            "gradient descent"
        ]
    },
    {
        "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
        "authors": [
            "Albert Gu",
            "Karan Goel",
            "Christopher Re"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies.  Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps.  A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state matrix \\( A \\), this system could handle long-range dependencies mathematically and empirically.  However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution.  We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths.  Our technique involves conditioning \\( A \\) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel.  S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60\\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.",
        "pdf_link": "https://openreview.net/pdf/a8eedf494f6698cb467c310c59d3ea6488546805.pdf",
        "forum_url": "https://openreview.net/forum?id=uYLFoz1vlAC",
        "keywords": [
            "state spaces",
            "sequence modeling",
            "language modeling",
            "fundamental state space model"
        ]
    },
    {
        "title": "RISP: Rendering-Invariant State Predictor with Differentiable Simulation and Rendering for Cross-Domain Parameter Estimation",
        "authors": [
            "Pingchuan Ma",
            "Tao Du",
            "Joshua B. Tenenbaum",
            "Wojciech Matusik",
            "Chuang Gan"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "This work considers identifying parameters characterizing a physical system's dynamic motion directly from a video whose rendering configurations are inaccessible. Existing solutions require massive training data or lack generalizability to unknown rendering configurations. We propose a novel approach that marries domain randomization and differentiable rendering gradients to address this problem. Our core idea is to train a rendering-invariant state-prediction (RISP) network that transforms image differences into state differences independent of rendering configurations, e.g., lighting, shadows, or material reflectance. To train this predictor, we formulate a new loss on rendering variances using gradients from differentiable rendering. Moreover, we present an efficient, second-order method to compute the gradients of this loss, allowing it to be integrated seamlessly into modern deep learning frameworks. We evaluate our method in rigid-body and deformable-body simulation environments using four tasks: state estimation, system identification, imitation learning, and visuomotor control. We further demonstrate the efficacy of our approach on a real-world example: inferring the state and action sequences of a quadrotor from a video of its motion sequences. Compared with existing methods, our approach achieves significantly lower reconstruction errors and has better generalizability among unknown rendering configurations.",
        "pdf_link": "https://openreview.net/pdf/999353870633727a2d50bc5b4ee873b50401eba7.pdf",
        "forum_url": "https://openreview.net/forum?id=uSE03demja",
        "keywords": [
            "rendering invariant",
            "rendering invariant state prediction",
            "state estimation",
            "cross domain parameter estimation",
            "deformable body simulation"
        ]
    },
    {
        "title": "Non-Transferable Learning: A New Approach for Model Ownership Verification and Applicability Authorization",
        "authors": [
            "Lixu Wang",
            "Shichao Xu",
            "Ruiqi Xu",
            "Xiao Wang",
            "Qi Zhu"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "As Artificial Intelligence as a Service gains popularity, protecting well-trained models as intellectual property is becoming increasingly important. There are two common types of protection methods: ownership verification and usage authorization. In this paper, we propose Non-Transferable Learning (NTL), a novel approach that captures the exclusive data representation in the learned model and restricts the model generalization ability to certain domains. This approach provides effective solutions to both model verification and authorization. Specifically: 1) For ownership verification, watermarking techniques are commonly used but are often vulnerable to sophisticated watermark removal methods. By comparison, our NTL-based ownership verification provides robust resistance to state-of-the-art watermark removal methods, as shown in extensive experiments with 6 removal approaches over the digits, CIFAR10 & STL10, and VisDA datasets. 2) For usage authorization, prior solutions focus on authorizing specific users to access the model, but authorized users can still apply the model to any data without restriction. Our NTL-based authorization approach instead provides data-centric protection, which we call applicability authorization, by significantly degrading the performance of the model on unauthorized data. Its effectiveness is also shown through experiments on aforementioned datasets. ",
        "pdf_link": "https://openreview.net/pdf/cc0b829e495ebd36c4e0dcce6f5d044ad4dce58d.pdf",
        "forum_url": "https://openreview.net/forum?id=tYRrOdSnVUy",
        "keywords": [
            "authorization",
            "applicability authorization",
            "usage authorization",
            "verification",
            "ownership verification",
            "watermarking",
            "artificial intelligence",
            "non transferable learning",
            "model ownership verification"
        ]
    },
    {
        "title": "Natural Posterior Network: Deep Bayesian Predictive Uncertainty for Exponential Family Distributions",
        "authors": [
            "Bertrand Charpentier",
            "Oliver Borchert",
            "Daniel Z\u00fcgner",
            "Simon Geisler",
            "Stephan G\u00fcnnemann"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Uncertainty awareness is crucial to develop reliable machine learning models. In this work, we propose the Natural Posterior Network (NatPN) for fast and high-quality uncertainty estimation for any task where the target distribution belongs to the exponential family. Thus, NatPN finds application for both classification and general regression settings. Unlike many previous approaches, NatPN does not require out-of-distribution (OOD) data at training time. Instead, it leverages Normalizing Flows to fit a single density on a learned low-dimensional and task-dependent latent space. For any input sample, NatPN uses the predicted likelihood to perform a Bayesian update over the target distribution. Theoretically, NatPN assigns high uncertainty far away from training data. Empirically, our extensive experiments on calibration and OOD detection show that NatPN delivers highly competitive performance for classification, regression and count prediction tasks.",
        "pdf_link": "https://openreview.net/pdf/c3ffe01a3bb574ad88e06692cb426a681a7c0f54.pdf",
        "forum_url": "https://openreview.net/forum?id=tV3N0DWMxCg",
        "keywords": [
            "natural posterior network",
            "uncertainty",
            "classification",
            "uncertainty estimation",
            "latent space",
            "prediction",
            "family"
        ]
    },
    {
        "title": "Improved deterministic l2 robustness on CIFAR-10 and CIFAR-100",
        "authors": [
            "Sahil Singla",
            "Surbhi Singla",
            "Soheil Feizi"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Training convolutional neural networks (CNNs) with a strict Lipschitz constraint under the $l_{2}$ norm is useful for provable adversarial robustness, interpretable gradients and stable training. While $1$-Lipschitz CNNs can be designed by enforcing a $1$-Lipschitz constraint on each layer, training such networks requires each layer to have an orthogonal Jacobian matrix (for all inputs) to prevent the gradients from vanishing during backpropagation. A layer with this property is said to be Gradient Norm Preserving (GNP). In this work, we introduce a procedure to certify the robustness of $1$-Lipschitz CNNs by relaxing the orthogonalization of the last linear layer of the network that significantly advances the state of the art for both standard and provable robust accuracies on CIFAR-100 (gains of $4.80\\%$ and $4.71\\%$, respectively). We further boost their robustness by introducing (i) a novel Gradient Norm preserving activation function called the Householder activation function (that includes every $\\mathrm{GroupSort}$ activation) and (ii) a certificate regularization. On CIFAR-10, we achieve significant improvements over prior works in provable robust accuracy ($5.81\\%$) with only a minor drop in standard accuracy ($-0.29\\%$). Code for reproducing all experiments in the paper is available at \\url{https://github.com/singlasahil14/SOC}. ",
        "pdf_link": "https://openreview.net/pdf/666d6306f372e8bad4486c511aded6081ad8c921.pdf",
        "forum_url": "https://openreview.net/forum?id=tD7eCtaSkR",
        "keywords": [
            "cifar 100",
            "robustness",
            "convolutional neural networks",
            "cifar 10",
            "provable robust",
            "provable robust accuracies"
        ]
    },
    {
        "title": "Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions",
        "authors": [
            "Leslie O'Bray",
            "Max Horn",
            "Bastian Rieck",
            "Karsten Borgwardt"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Graph generative models are a highly active branch of machine learning. Given the steady development of new models of ever-increasing complexity, it is necessary to provide a principled way to evaluate and compare them. In this paper, we enumerate the desirable criteria for such a comparison metric and provide an overview of the status quo of graph generative model comparison in use today, which predominantly relies on the maximum mean discrepancy (MMD). We perform a systematic evaluation of MMD in the context of graph generative model comparison, highlighting some of the challenges and pitfalls researchers inadvertently may encounter. After conducting a thorough analysis of the behaviour of MMD on synthetically-generated perturbed graphs as well as on recently-proposed graph generative models, we are able to provide a suitable procedure to mitigate these challenges and pitfalls. We aggregate our findings into a list of practical recommendations for researchers to use when evaluating graph generative models.",
        "pdf_link": "https://openreview.net/pdf/3af218144851b57d7d59c78ea79729bed4f8adba.pdf",
        "forum_url": "https://openreview.net/forum?id=tBtoZYKd9n",
        "keywords": [
            "generative",
            "graph generative",
            "systematic evaluation",
            "graph generative models",
            "evaluation metrics",
            "learning"
        ]
    },
    {
        "title": "Learning Optimal Conformal Classifiers",
        "authors": [
            "David Stutz",
            "Krishnamurthy Dj Dvijotham",
            "Ali Taylan Cemgil",
            "Arnaud Doucet"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Modern deep learning based classifiers show very high accuracy on test data but this does not provide sufficient guarantees for safe deployment, especially in high-stake AI applications such as medical diagnosis. Usually, predictions are obtained without a reliable uncertainty estimate or a formal guarantee. Conformal prediction (CP) addresses these issues by using the classifier's predictions, e.g., its probability estimates, to predict confidence sets containing the true class with a user-specified probability. However, using CP as a separate processing step after training prevents the underlying model from adapting to the prediction of confidence sets. Thus, this paper explores strategies to differentiate through CP during training with the goal of training model with the conformal wrapper end-to-end. In our approach, conformal training (ConfTr), we specifically \"simulate\" conformalization on mini-batches during training. Compared to standard training, ConfTr reduces the average confidence set size (inefficiency) of state-of-the-art CP methods applied after training. Moreover, it allows to \"shape\" the confidence sets predicted at test time, which is difficult for standard CP. On experiments with several datasets, we show ConfTr can influence how inefficiency is distributed across classes, or guide the composition of confidence sets in terms of the included classes, while retaining the guarantees offered by CP.",
        "pdf_link": "https://openreview.net/pdf/9fab4fe6695fabd7230072d001a43466d0e92499.pdf",
        "forum_url": "https://openreview.net/forum?id=t8O-4LKFVx",
        "keywords": [
            "conformal prediction",
            "conformalization"
        ]
    },
    {
        "title": "SOSP: Efficiently Capturing Global Correlations by Second-Order Structured Pruning",
        "authors": [
            "Manuel Nonnenmacher",
            "Thomas Pfeil",
            "Ingo Steinwart",
            "David Reeb"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Pruning neural networks reduces inference time and memory costs. On standard hardware, these benefits will be especially prominent if coarse-grained structures, like feature maps, are pruned. We devise two novel saliency-based methods for second-order structured pruning (SOSP) which include correlations among all structures and layers. Our main method SOSP-H employs an innovative second-order approximation, which enables saliency evaluations by fast Hessian-vector products. SOSP-H thereby scales like a first-order method despite taking into account the full Hessian. We validate SOSP-H by comparing it to our second method SOSP-I that uses a well-established Hessian approximation, and to numerous state-of-the-art methods. While SOSP-H performs on par or better in terms of accuracy, it has clear advantages in terms of scalability and efficiency. This allowed us to scale SOSP-H to large-scale vision tasks, even though it captures correlations across all layers of the network. To underscore the global nature of our pruning methods, we evaluate their performance not only by removing structures from a pretrained network, but also by detecting architectural bottlenecks. We show that our algorithms allow to systematically reveal architectural bottlenecks, which we then remove to further increase the accuracy of the networks.",
        "pdf_link": "https://openreview.net/pdf/27b7b15fa7028267ed543816aadafc963d85b09a.pdf",
        "forum_url": "https://openreview.net/forum?id=t5EmXZ3ZLR",
        "keywords": [
            "saliency",
            "pruned"
        ]
    },
    {
        "title": "What Happens after SGD Reaches Zero Loss? --A Mathematical Framework",
        "authors": [
            "Zhiyuan Li",
            "Tianhao Wang",
            "Sanjeev Arora"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Understanding the implicit bias of Stochastic Gradient Descent (SGD) is one of the key challenges in deep learning, especially for overparametrized models, where the local minimizers of the loss function $L$ can form a manifold. Intuitively, with a sufficiently small learning rate $\\eta$, SGD tracks Gradient Descent (GD) until it gets close to such manifold, where the gradient noise prevents further convergence. In such regime, Blanc et al. (2020) proved that SGD with label noise locally decreases a regularizer-like term, the sharpness of loss, $\\text{tr}[\\nabla^2 L]$. The current paper gives a general framework for such analysis by adapting ideas from Katzenberger (1991). It allows in principle a complete characterization for the regularization effect of SGD around such manifold---i.e., the \"implicit bias\"---using a stochastic differential equation (SDE) describing the limiting dynamics of the parameters, which is determined jointly by the loss function and the noise covariance. This yields some new results: (1) a *global* analysis of the implicit bias valid for $\\eta^{-2}$ steps, in contrast to the local analysis of Blanc et al. (2020) that is only valid for $\\eta^{-1.6}$ steps and (2) allowing *arbitrary* noise covariance. As an application, we show with arbitrary large initialization, label noise SGD can always escape the kernel regime and only requires $O(\\kappa\\ln d)$ samples for learning an $\\kappa$-sparse overparametrized linear model in $\\mathbb{R}^d$ (Woodworth et al., 2020), while GD initialized in the kernel regime requires $\\Omega(d)$ samples. This upper bound is minimax optimal and improves the previous $\\widetilde{O}(\\kappa^2)$ upper bound (HaoChen et al., 2020).",
        "pdf_link": "https://openreview.net/pdf/053e833d0de895f0839c04aabd053b4d678ffb02.pdf",
        "forum_url": "https://openreview.net/forum?id=siCt4xZn5Ve",
        "keywords": [
            "loss function",
            "loss",
            "sharpness",
            "label noise",
            "implicit bias",
            "local analysis",
            "regularization",
            "differential equation"
        ]
    },
    {
        "title": "8-bit Optimizers via Block-wise Quantization",
        "authors": [
            "Tim Dettmers",
            "Mike Lewis",
            "Sam Shleifer",
            "Luke Zettlemoyer"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Stateful optimizers maintain gradient statistics over time, e.g., the exponentially smoothed sum (SGD with momentum) or squared sum (Adam) of past gradient values. This state can be used to accelerate optimization significantly, compared to plain stochastic gradient descent, but uses memory that might otherwise be allocated to model parameters, thereby limiting the maximum size of models trained in practice. In this paper, we develop the first optimizers that use 8-bit statistics while maintaining the performance levels of using 32-bit optimizer states. To overcome the resulting computational, quantization, and stability challenges, we develop block-wise dynamic quantization. Block-wise quantization divides input tensors into smaller blocks that are independently quantized. Each block is processed in parallel across cores, yielding faster optimization and high precision quantization. To maintain stability and performance, we combine block-wise quantization with two additional changes: (1) dynamic quantization, a form of non-linear optimization that is precise for both large and small magnitude values, and (2) a stable embedding layer to reduce gradient variance that comes from the highly non-uniform distribution of input tokens in language models. As a result, our 8-bit optimizers maintain 32-bit performance with a small fraction of the memory footprint on a range of tasks, including 1.5B parameter language modeling, GLUE finetuning, ImageNet classification, WMT'14 machine translation, MoCo v2 contrastive ImageNet pretraining+finetuning, and RoBERTa pretraining, without changes to the original optimizer hyperparameters. We open-source our 8-bit optimizers as a drop-in replacement that only requires a two-line code change.",
        "pdf_link": "https://openreview.net/pdf/eae16788bf15e102fb9f104d044c6dff582683f4.pdf",
        "forum_url": "https://openreview.net/forum?id=shpkpVXzo3h",
        "keywords": [
            "bit optimizer",
            "block wise quantization",
            "quantization",
            "bit statistics",
            "imagenet",
            "8 bit optimizers"
        ]
    },
    {
        "title": "CoBERL: Contrastive BERT for Reinforcement Learning",
        "authors": [
            "Andrea Banino",
            "Adria Puigdomenech Badia",
            "Jacob C Walker",
            "Tim Scholtes",
            "Jovana Mitrovic",
            "Charles Blundell"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Many reinforcement learning (RL) agents require a large amount of experience to solve tasks. We propose Contrastive BERT for RL (COBERL), an agent that combines a new contrastive loss and a hybrid LSTM-transformer architecture to tackle the challenge of improving data efficiency. COBERL enables efficient and robust learning from pixels across a wide variety of domains. We use bidirectional masked prediction in combination with a generalization of a recent contrastive method to learn better representations for RL, without the need of hand engineered data augmentations. We find that COBERL consistently improves data efficiency across the full Atari suite, a set of control tasks and a challenging 3D environment, and often it also increases final score performance.",
        "pdf_link": "https://openreview.net/pdf/c833364a7435330b3ee8e71a2020d1172e9d3380.pdf",
        "forum_url": "https://openreview.net/forum?id=sRZ3GhmegS",
        "keywords": [
            "reinforcement learning"
        ]
    },
    {
        "title": "Responsible Disclosure of Generative Models Using Scalable Fingerprinting",
        "authors": [
            "Ning Yu",
            "Vladislav Skripniuk",
            "Dingfan Chen",
            "Larry S. Davis",
            "Mario Fritz"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Over the past years, deep generative models have achieved a new level of performance. Generated data has become difficult, if not impossible, to be distinguished from real data. While there are plenty of use cases that benefit from this technology, there are also strong concerns on how this new technology can be misused to generate deep fakes and enable misinformation at scale. Unfortunately, current deep fake detection methods are not sustainable, as the gap between real and fake continues to close. In contrast, our work enables a responsible disclosure of such state-of-the-art generative models, that allows model inventors to fingerprint their models, so that the generated samples containing a fingerprint can be accurately detected and attributed to a source. Our technique achieves this by an efficient and scalable ad-hoc generation of a large population of models with distinct fingerprints. Our recommended operation point uses a 128-bit fingerprint which in principle results in more than 10^{38} identifiable models. Experiments show that our method fulfills key properties of a fingerprinting mechanism and achieves effectiveness in deep fake detection and attribution. Code and models are available at https://github.com/ningyu1991/ScalableGANFingerprints.",
        "pdf_link": "https://openreview.net/pdf/e17ae78a19a967b27b17c7545fd71dfff5109784.pdf",
        "forum_url": "https://openreview.net/forum?id=sOK-zS6WHB",
        "keywords": [
            "fingerprinting",
            "fake detection",
            "disclosure",
            "attribution",
            "generative models"
        ]
    },
    {
        "title": "ProtoRes: Proto-Residual Network for Pose Authoring via Learned Inverse Kinematics",
        "authors": [
            "Boris N. Oreshkin",
            "Florent Bocquelet",
            "Felix G. Harvey",
            "Bay Raitt",
            "Dominic Laflamme"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Our work focuses on the development of a learnable neural representation of human pose for advanced AI assisted animation tooling. Specifically, we tackle the problem of constructing a full static human pose based on sparse and variable user inputs (e.g. locations and/or orientations of a subset of body joints). To solve this problem, we propose a novel neural architecture that combines residual connections with prototype encoding of a partially specified pose to create a new complete pose from the learned latent space. We show that our architecture outperforms a baseline based on Transformer, both in terms of accuracy and computational efficiency. Additionally, we develop a user interface to integrate our neural model in Unity, a real-time 3D development platform. Furthermore, we introduce two new datasets representing the static human pose modeling problem, based on high-quality human motion capture data, which will be released publicly along with model code.",
        "pdf_link": "https://openreview.net/pdf/72eadcfe21558f0be18ff071adc50adc3ae85e5e.pdf",
        "forum_url": "https://openreview.net/forum?id=s03AQxehtd_",
        "keywords": [
            "pose authoring",
            "protores",
            "learned inverse kinematics",
            "proto residual"
        ]
    },
    {
        "title": "Optimal Transport for Causal Discovery",
        "authors": [
            "Ruibo Tu",
            "Kun Zhang",
            "Hedvig Kjellstrom",
            "Cheng Zhang"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "To determine causal relationships between two variables, approaches based on Functional Causal Models (FCMs) have been proposed by properly restricting model classes; however, the performance is sensitive to the model assumptions, which makes it difficult to use. In this paper, we provide a novel dynamical-system view of FCMs and propose a new framework for identifying causal direction in the bivariate case. We first show the connection between FCMs and optimal transport, and then study optimal transport under the constraints of FCMs. Furthermore, by exploiting the dynamical interpretation of optimal transport under the FCM constraints, we determine the corresponding underlying dynamical process of the static cause-effect pair data. It provides a new dimension for describing static causal discovery tasks while enjoying more freedom for modeling the quantitative causal influences. In particular, we show that Additive Noise Models (ANMs) correspond to volume-preserving pressureless flows. Consequently, based on their velocity field divergence, we introduce a criterion for determining causal direction. With this criterion, we propose a novel optimal transport-based algorithm for ANMs which is robust to the choice of models and extend it to post-nonlinear models. Our method demonstrated state-of-the-art results on both synthetic and causal discovery benchmark datasets.",
        "pdf_link": "https://openreview.net/pdf/beb079a1e442ec0e13e5da89161a1f30ba96279f.pdf",
        "forum_url": "https://openreview.net/forum?id=qwBK94cP1y",
        "keywords": [
            "causal",
            "functional causal models",
            "optimal transport",
            "causal discovery",
            "fcms",
            "fcm constraints",
            "cause effect pair data",
            "causal direction",
            "causal relationships"
        ]
    },
    {
        "title": "Variational Inference for Discriminative Learning with Generative Modeling of Feature Incompletion",
        "authors": [
            "Kohei Miyaguchi",
            "Takayuki Katsuki",
            "Akira Koseki",
            "Toshiya Iwamori"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "We are concerned with the problem of distributional prediction with incomplete features: The goal is to estimate the distribution of target variables given feature vectors with some of the elements missing. A typical approach to this problem is to perform missing-value imputation and regression, simultaneously or sequentially, which we call the generative approach. Another approach is to perform regression after appropriately encoding missing values into the feature, which we call the discriminative approach. In comparison, the generative approach is more robust to the feature corruption while the discriminative approach is more favorable to maximize the performance of prediction. \nIn this study, we propose a hybrid method to take the best of both worlds. Our method utilizes the black-box variational inference framework so that it can be applied to a wide variety of modern machine learning models, including the variational autoencoders. We also confirmed the effectiveness of the proposed method empirically.\n",
        "pdf_link": "https://openreview.net/pdf/537474668e8264be0d7e7963ad009564621ad25e.pdf",
        "forum_url": "https://openreview.net/forum?id=qnQN4yr6FJz",
        "keywords": [
            "variational inference",
            "discriminative learning",
            "generative modeling",
            "missing value imputation",
            "regression"
        ]
    },
    {
        "title": "Real-Time Neural Voice Camouflage",
        "authors": [
            "Mia Chiquier",
            "Chengzhi Mao",
            "Carl Vondrick"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Automatic speech recognition systems have created exciting possibilities for applications, however they also enable opportunities for systematic eavesdropping.We propose a method to camouflage a person's voice from these systems without inconveniencing the conversation between people in the room. Standard adversarial attacks are not effective in real-time streaming situations because the characteristics of the signal will have changed by the time the attack is executed. We introduce predictive adversarial attacks, which achieves real-time performance by forecasting the attack vector that will be the most effective in the future. Under real-time constraints, our method jams the established speech recognition system DeepSpeech 3.9x more than online projected gradient descent as measured through word error rate, and 6.6x more as measured through character error rate. We furthermore demonstrate our approach is practically effective in realistic environments with complex scene geometries. ",
        "pdf_link": "https://openreview.net/pdf/e2b96a38db73636bfa51d5ee4097373ddda15329.pdf",
        "forum_url": "https://openreview.net/forum?id=qj1IZ-6TInc",
        "keywords": [
            "speech recognition",
            "adversarial attacks",
            "neural voice camouflage"
        ]
    },
    {
        "title": "Learning transferable motor skills with hierarchical latent mixture policies",
        "authors": [
            "Dushyant Rao",
            "Fereshteh Sadeghi",
            "Leonard Hasenclever",
            "Markus Wulfmeier",
            "Martina Zambelli",
            "Giulia Vezzani",
            "Dhruva Tirumala",
            "Yusuf Aytar",
            "Josh Merel",
            "Nicolas Heess",
            "raia hadsell"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "For robots operating in the real world, it is desirable to learn reusable abstract behaviours that can effectively be transferred across numerous tasks and scenarios.\nWe propose an approach to learn skills from data using a hierarchical mixture latent variable model.\nOur method exploits a multi-level hierarchy of both discrete and continuous latent variables, to model a discrete set of abstract high-level behaviours while allowing for variance in how they are executed.\nWe demonstrate in manipulation domains that the method can effectively cluster offline data into distinct, executable behaviours, while retaining the flexibility of a continuous latent variable model.\nThe resulting skills can be transferred to new tasks, unseen objects, and from state to vision-based policies, yielding significantly better sample efficiency and asymptotic performance compared to existing skill- and imitation-based methods.\nWe also perform further analysis showing how and when the skills are most beneficial: they encourage directed exploration to cover large regions of the state space relevant to the task, making them most effective in challenging sparse-reward settings.",
        "pdf_link": "https://openreview.net/pdf/da585a69d336f46f18b80d4a026fd3a7dcb40eae.pdf",
        "forum_url": "https://openreview.net/forum?id=qTHBE7E9iej",
        "keywords": [
            "latent variable model",
            "hierarchical latent mixture policies",
            "transferable motor skills"
        ]
    },
    {
        "title": "Language modeling via stochastic processes",
        "authors": [
            "Rose E Wang",
            "Esin Durmus",
            "Noah Goodman",
            "Tatsunori Hashimoto"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Modern language models can generate high-quality short texts. However, they often meander or are incoherent when generating longer texts. These issues arise from the next-token-only language modeling objective. To address these issues, we introduce Time Control (TC), a language model that implicitly plans via a latent stochastic process. TC does this by learning a representation which maps the dynamics of how text changes in a document to the dynamics of a stochastic process of interest. Using this representation, the language model can generate text by first implicitly generating a document plan via a stochastic process, and then generating text that is consistent with this latent plan. Compared to domain-specific methods and fine-tuning GPT2 across a variety of text domains, TC improves performance on text infilling and discourse coherence. On long text generation settings, TC preserves the text structure both in terms of ordering (up to +40% better) and text length consistency (up to +17% better).  Human evaluators also prefer TC's output 28.6% more than the baselines.",
        "pdf_link": "https://openreview.net/pdf/ceeec650a60b1f87ad4dda26ecd02c9df0e3ed9d.pdf",
        "forum_url": "https://openreview.net/forum?id=pMQwKL1yctf",
        "keywords": [
            "language modeling",
            "stochastic"
        ]
    },
    {
        "title": "IntSGD: Adaptive Floatless Compression of Stochastic Gradients",
        "authors": [
            "Konstantin Mishchenko",
            "Bokun Wang",
            "Dmitry Kovalev",
            "Peter Richt\u00e1rik"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "We propose a family of adaptive integer compression operators for distributed Stochastic Gradient Descent (SGD) that do not communicate a single float. This is achieved by multiplying floating-point vectors with a number known to every device and then rounding to integers. In contrast to the prior work on integer compression for SwitchML by (Sapio et al., 2021), our IntSGD method is provably convergent and computationally cheaper as it estimates the scaling of vectors adaptively. Our theory shows that the iteration complexity of IntSGD matches that of SGD up to constant factors for both convex and non-convex, smooth and non-smooth functions, with and without overparameterization. Moreover, our algorithm can also be tailored for the popular all-reduce primitive and shows promising empirical performance.",
        "pdf_link": "https://openreview.net/pdf/fa05928174931d3f8e117dbba3da063cb29acbd2.pdf",
        "forum_url": "https://openreview.net/forum?id=pFyXqxChZc",
        "keywords": [
            "intsgd",
            "integer compression",
            "floatless compression",
            "adaptive integer compression",
            "stochastic gradient descent",
            "gradients",
            "switchml"
        ]
    },
    {
        "title": "BEiT: BERT Pre-Training of Image Transformers",
        "authors": [
            "Hangbo Bao",
            "Li Dong",
            "Songhao Piao",
            "Furu Wei"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e., image patches (such as 16 x 16 pixels), and visual tokens (i.e., discrete tokens). We first ``tokenize'' the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods.",
        "pdf_link": "https://openreview.net/pdf/1be2cb0e0edf9af45f8ef450b802b459897cec3d.pdf",
        "forum_url": "https://openreview.net/forum?id=p-BhZSz59o4",
        "keywords": [
            "image transformers",
            "image classification",
            "beit",
            "semantic segmentation"
        ]
    },
    {
        "title": "Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation",
        "authors": [
            "Alex Rogozhnikov"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Tensor computations underlie modern scientific computing and deep learning.\nA number of tensor frameworks emerged varying in execution model, hardware support, memory management, model definition, etc.\nHowever, tensor operations in all frameworks follow the same paradigm.\nRecent neural network architectures demonstrate demand for higher expressiveness of tensor operations.\nThe current paradigm is not suited to write readable, reliable, or easy-to-modify code for multidimensional tensor manipulations. \nMoreover, some commonly used operations do not provide sufficient checks and can break a tensor structure.\nThese mistakes are elusive as no tools or tests can detect them.\nIndependently, API discrepancies complicate code transfer between frameworks.\nWe propose einops notation: a uniform and generic way to manipulate tensor structure, that significantly improves code readability and flexibility by focusing on the structure of input and output tensors.\nWe implement einops notation in a Python package that efficiently supports multiple widely used frameworks and provides framework-independent minimalist API for tensor manipulations.",
        "pdf_link": "https://openreview.net/pdf/d568f6e36eaa377888611b8e0d84076777edc330.pdf",
        "forum_url": "https://openreview.net/forum?id=oapKSVM2bcj",
        "keywords": [
            "tensor manipulations",
            "tensor",
            "einops notation",
            "einstein",
            "neural network",
            "readability"
        ]
    },
    {
        "title": "Self-Supervision Enhanced Feature Selection with Correlated Gates",
        "authors": [
            "Changhee Lee",
            "Fergus Imrie",
            "Mihaela van der Schaar"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Discovering relevant input features for predicting a target variable is a key scientific question. However, in many domains, such as medicine and biology, feature selection is confounded by a scarcity of labeled samples coupled with significant correlations among features. In this paper, we propose a novel deep learning approach to feature selection that addresses both challenges simultaneously. First, we pre-train the network using unlabeled samples within a self-supervised learning framework by solving pretext tasks that require the network to learn informative representations from partial feature sets. Then, we fine-tune the pre-trained network to discover relevant features using labeled samples. During both training phases, we explicitly account for the correlation structure of the input features by generating correlated gate vectors from a multivariate Bernoulli distribution. Experiments on multiple real-world datasets including clinical and omics demonstrate that our model discovers relevant features that provide superior prediction performance compared to the state-of-the-art benchmarks in practical scenarios where there is often limited labeled data and high correlations among features.",
        "pdf_link": "https://openreview.net/pdf/eeb0bd632ffa4f23e0076ad9369b3943bbf31efc.pdf",
        "forum_url": "https://openreview.net/forum?id=oDFvtxzPOx",
        "keywords": [
            "self supervision",
            "feature selection",
            "deep learning",
            "gates"
        ]
    },
    {
        "title": "Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers",
        "authors": [
            "Ruihan Yang",
            "Minghao Zhang",
            "Nicklas Hansen",
            "Huazhe Xu",
            "Xiaolong Wang"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "We propose to address quadrupedal locomotion tasks using Reinforcement Learning (RL) with a Transformer-based model that learns to combine proprioceptive information and high-dimensional depth sensor inputs. While learning-based locomotion has made great advances using RL, most methods still rely on domain randomization for training blind agents that generalize to challenging terrains. Our key insight is that proprioceptive states only offer contact measurements for immediate reaction, whereas an agent equipped with visual sensory observations can learn to proactively maneuver environments with obstacles and uneven terrain by anticipating changes in the environment many steps ahead. In this paper, we introduce LocoTransformer, an end-to-end RL method that leverages both proprioceptive states and visual observations for locomotion control. We evaluate our method in challenging simulated environments with different obstacles and uneven terrain. We transfer our learned policy from simulation to a real robot by running it indoor and in-the-wild with unseen obstacles and terrain. Our method not only significantly improves over baselines, but also achieves far better generalization performance, especially when transferred to the real robot. Our project page with videos is at https://rchalyang.github.io/LocoTransformer/.",
        "pdf_link": "https://openreview.net/pdf/44a74037c4089730c34de558a6d679925f866a56.pdf",
        "forum_url": "https://openreview.net/forum?id=nhnJ3oo6AB",
        "keywords": [
            "locomotion",
            "reinforcement learning",
            "quadrupedal locomotion",
            "cross modal transformers",
            "transformer based model",
            "blind agents"
        ]
    },
    {
        "title": "Does your graph need a confidence boost?  Convergent boosted smoothing on graphs with tabular node features",
        "authors": [
            "Jiuhai Chen",
            "Jonas Mueller",
            "Vassilis N. Ioannidis",
            "Soji Adeshina",
            "Yangkun Wang",
            "Tom Goldstein",
            "David Wipf"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Many practical modeling tasks require making predictions using tabular data composed of heterogeneous feature types (e.g., text-based, categorical, continuous, etc.).  In this setting boosted decision trees and related ensembling techniques generally dominate real-world applications involving iid training/test sets.  However, when there are relations between samples and the iid assumption is no longer reasonable, it remains unclear how to incorporate these dependencies within existing boosting pipelines.  To this end, we propose a generalized framework for combining boosted trees and more general model ensembling techniques, with graph propagation layers that share  node/sample information across edges connecting related samples.  And unlike previous efforts to integrate graph-based models with boosting, our approach is anchored to a principled meta loss function such that provable convergence can be guaranteed under relatively mild assumptions. Across a variety of benchmarks involving non-iid graph data with tabular node features, our framework achieves comparable or superior performance.",
        "pdf_link": "https://openreview.net/pdf/1c7555405291429ac38f31042735a6536dd9bfb5.pdf",
        "forum_url": "https://openreview.net/forum?id=nHpzE7DqAnG",
        "keywords": [
            "boosted decision trees",
            "boosted trees",
            "boosting",
            "tabular node features",
            "confidence",
            "smoothing"
        ]
    },
    {
        "title": "Contact Points Discovery for Soft-Body Manipulations with Differentiable Physics",
        "authors": [
            "Sizhe Li",
            "Zhiao Huang",
            "Tao Du",
            "Hao Su",
            "Joshua B. Tenenbaum",
            "Chuang Gan"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Differentiable physics has recently been shown as a powerful tool for solving soft-body manipulation tasks. However, the differentiable physics solver often gets stuck when the initial contact points of the end effectors are sub-optimal or when performing multi-stage tasks that require contact point switching, which often leads to local minima.\nTo address this challenge, we propose a  contact point discovery approach (CPDeform) that guides the stand-alone differentiable physics solver to deform various soft-body plasticines. The key idea of our approach is to integrate optimal transport-based contact points discovery into the differentiable physics solver to overcome the local minima from initial contact points or contact switching.\nOn single-stage tasks, our method can automatically find suitable initial contact points based on transport priorities. On complex multi-stage tasks, we can iteratively switch the contact points of end-effectors based on transport priorities. To evaluate the effectiveness of our method, we introduce PlasticineLab-M that extends the existing differentiable physics benchmark PlasticineLab to seven new challenging multi-stage soft-body manipulation tasks. Extensive experimental results suggest that: 1) on multi-stage tasks that are infeasible for the vanilla differentiable physics solver, our approach discovers contact points that efficiently guide the solver to completion; 2) on tasks where the vanilla solver performs sub-optimally or near-optimally, our contact point discovery method performs better than or on par with the manipulation performance obtained with handcrafted contact points.\n",
        "pdf_link": "https://openreview.net/pdf/21c3d98f02351425ae7c7201ec763c603b24b4c7.pdf",
        "forum_url": "https://openreview.net/forum?id=mmUA7_O9mjY",
        "keywords": [
            "soft body",
            "contact points",
            "differentiable physics",
            "contact switching",
            "contact points discovery",
            "soft body manipulations"
        ]
    },
    {
        "title": "Continual Learning with Filter Atom Swapping",
        "authors": [
            "Zichen Miao",
            "Ze Wang",
            "Wei Chen",
            "Qiang Qiu"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Continual learning has been widely studied in recent years to resolve the catastrophic forgetting of deep neural networks. In this paper, we first enforce a low-rank filter subspace by decomposing convolutional filters within each network layer over a small set of filter atoms. Then, we perform continual learning with filter atom swapping. In other words, we learn for each task a new filter subspace for each convolutional layer, i.e., hundreds of parameters as filter atoms, but keep subspace coefficients shared across tasks. By maintaining a small footprint memory of filter atoms, we can easily archive models for past tasks to avoid forgetting. The effectiveness of this simple scheme for continual learning is illustrated both empirically and theoretically. The proposed atom swapping framework further enables flexible and efficient model ensemble with members selected within a task or across tasks to improve the performance in different continual learning settings. Being validated on multiple benchmark datasets with different convolutional network structures, the proposed method outperforms the state-of-the-art methods in both accuracy and scalability.",
        "pdf_link": "https://openreview.net/pdf/f453901d83c3373b05abe089d047948fa6e10b9b.pdf",
        "forum_url": "https://openreview.net/forum?id=metRpM4Zrcb",
        "keywords": [
            "continual learning",
            "convolutional network",
            "convolutional filters",
            "atom swapping",
            "filter atom swapping",
            "convolutional layer",
            "filter",
            "ensemble"
        ]
    },
    {
        "title": "Boosting Randomized Smoothing with Variance Reduced Classifiers",
        "authors": [
            "Mikl\u00f3s Z. Horv\u00e1th",
            "Mark Niklas Mueller",
            "Marc Fischer",
            "Martin Vechev"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Randomized Smoothing (RS) is a promising method for obtaining robustness certi\ufb01cates by evaluating a base model under noise. In this work, we: (i) theoretically motivate why ensembles are a particularly suitable choice as base models for RS, and (ii) empirically con\ufb01rm this choice, obtaining state-of-the-art results in multiple settings. The key insight of our work is that the reduced variance of ensembles over the perturbations introduced in RS leads to signi\ufb01cantly more consistent classi\ufb01cations for a given input. This, in turn, leads to substantially increased certi\ufb01able radii for samples close to the decision boundary. Additionally, we introduce key optimizations which enable an up to 55-fold decrease in sample complexity of RS for predetermined radii, thus drastically reducing its computational overhead. Experimentally, we show that ensembles of only 3 to 10 classi\ufb01ers consistently improve on their strongest constituting model with respect to their average certi\ufb01ed radius (ACR) by 5% to 21% on both CIFAR10 and ImageNet, achieving a new state-of-the-art ACR of 0.86 and 1.11, respectively. We release all code and models required to reproduce our results at https://github.com/eth-sri/smoothing-ensembles.",
        "pdf_link": "https://openreview.net/pdf/23e2b6f2b0f6bf1e6d39492a2557b0b0357d6fdf.pdf",
        "forum_url": "https://openreview.net/forum?id=mHu2vIds_-b",
        "keywords": [
            "randomized smoothing",
            "smoothing",
            "smoothing ensembles",
            "ensembles",
            "sample",
            "boosting"
        ]
    },
    {
        "title": "Weighted Training for Cross-Task Learning",
        "authors": [
            "Shuxiao Chen",
            "Koby Crammer",
            "Hangfeng He",
            "Dan Roth",
            "Weijie J Su"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "In this paper, we introduce Target-Aware Weighted Training (TAWT), a weighted training algorithm for cross-task learning based on minimizing a representation-based task distance between the source and target tasks. We show that TAWT is easy to implement, is computationally efficient, requires little hyperparameter tuning, and enjoys non-asymptotic learning-theoretic guarantees. The effectiveness of TAWT is corroborated through extensive experiments with BERT on four sequence tagging tasks in natural language processing (NLP), including part-of-speech (PoS) tagging, chunking, predicate detection, and named entity recognition (NER). As a byproduct, the proposed representation-based task distance allows one to reason in a theoretically principled way about several critical aspects of cross-task learning, such as the choice of the source data and the impact of fine-tuning.",
        "pdf_link": "https://openreview.net/pdf/579ed2f74ecc130396039eae33e13de66b8de08b.pdf",
        "forum_url": "https://openreview.net/forum?id=ltM1RMZntpu",
        "keywords": [
            "cross task learning",
            "named entity recognition",
            "task distance",
            "weighted training"
        ]
    },
    {
        "title": "Learning Long-Term Reward Redistribution via Randomized Return Decomposition",
        "authors": [
            "Zhizhou Ren",
            "Ruihan Guo",
            "Yuan Zhou",
            "Jian Peng"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Many practical applications of reinforcement learning require agents to learn from sparse and delayed rewards. It challenges the ability of agents to attribute their actions to future outcomes. In this paper, we consider the problem formulation of episodic reinforcement learning with trajectory feedback. It refers to an extreme delay of reward signals, in which the agent can only obtain one reward signal at the end of each trajectory. A popular paradigm for this problem setting is learning with a designed auxiliary dense reward function, namely proxy reward, instead of sparse environmental signals. Based on this framework, this paper proposes a novel reward redistribution algorithm, randomized return decomposition (RRD), to learn a proxy reward function for episodic reinforcement learning. We establish a surrogate problem by Monte-Carlo sampling that scales up least-squares-based reward redistribution to long-horizon problems. We analyze our surrogate loss function by connection with existing methods in the literature, which illustrates the algorithmic properties of our approach. In experiments, we extensively evaluate our proposed method on a variety of benchmark tasks with episodic rewards and demonstrate substantial improvement over baseline algorithms.",
        "pdf_link": "https://openreview.net/pdf/9d37ef647d9bba0c4b6fe1976563d07da74d311e.pdf",
        "forum_url": "https://openreview.net/forum?id=lpkGn3k2YdD",
        "keywords": [
            "reward redistribution",
            "randomized return decomposition"
        ]
    },
    {
        "title": "The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design",
        "authors": [
            "Yoav Levine",
            "Noam Wies",
            "Daniel Jannai",
            "Dan Navon",
            "Yedid Hoshen",
            "Amnon Shashua"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Pretraining Neural Language Models (NLMs) over a large corpus involves chunking the text into training examples, which are contiguous text segments of sizes processable by the neural architecture. We highlight a bias introduced by this common practice: we prove that the pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segments that appeared in different training examples. This intuitive result has a twofold role. First, it formalizes the motivation behind a broad line of recent successful NLM training heuristics, proposed for the pretraining and fine-tuning stages, which do not necessarily appear related at first glance. Second, our result clearly indicates further improvements to be made in NLM pretraining for the benefit of Natural Language Understanding tasks. As an example, we propose ``kNN-Pretraining\": we show that including semantically related non-neighboring sentences in the same pretraining example yields improved sentence representations and open domain question answering abilities.\tThis theoretically motivated degree of freedom for pretraining example design indicates new training schemes for self-improving representations. ",
        "pdf_link": "https://openreview.net/pdf/da15c2bd56f71f3d484f0da8f8b25aea19884b0a.pdf",
        "forum_url": "https://openreview.net/forum?id=lnEaqbTJIRz",
        "keywords": [
            "pretraining",
            "pretraining example",
            "inductive bias",
            "pretraining neural language models",
            "natural language"
        ]
    },
    {
        "title": "Data-Efficient Graph Grammar Learning for Molecular Generation",
        "authors": [
            "Minghao Guo",
            "Veronika Thost",
            "Beichen Li",
            "Payel Das",
            "Jie Chen",
            "Wojciech Matusik"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "The problem of molecular generation has received significant attention recently. Existing methods are typically based on deep neural networks and require training on large datasets with tens of thousands of samples. In practice, however, the size of class-specific chemical datasets is usually limited (e.g., dozens of samples) due to labor-intensive experimentation and data collection. Another major challenge is to generate only physically synthesizable molecules. This is a non-trivial task for neural network-based generative models since the relevant chemical knowledge can only be extracted and generalized from the limited training data. In this work, we propose a data-efficient generative model that can be learned from datasets with orders of magnitude smaller sizes than common benchmarks. At the heart of this method is a learnable graph grammar that generates molecules from a sequence of production rules. Without any human assistance, these production rules are automatically constructed from training data. Furthermore, additional chemical knowledge can be incorporated into the model by further grammar optimization. Our learned graph grammar yields state-of-the-art results on generating high-quality molecules for three monomer datasets that contain only ${\\sim}20$ samples each. Our approach also achieves remarkable performance in a challenging polymer generation task with $only$ $117$ training samples and is competitive against existing methods using $81$k data points.\n",
        "pdf_link": "https://openreview.net/pdf/c17b0db09f98b3279ad677650f18acbf907883ce.pdf",
        "forum_url": "https://openreview.net/forum?id=l4IHywGq6a",
        "keywords": [
            "graph grammar",
            "generation",
            "molecular generation",
            "learnable graph grammar"
        ]
    },
    {
        "title": "SphereFace2: Binary Classification is All You Need for Deep Face Recognition",
        "authors": [
            "Yandong Wen",
            "Weiyang Liu",
            "Adrian Weller",
            "Bhiksha Raj",
            "Rita Singh"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "State-of-the-art deep face recognition methods are mostly trained with a softmax-based multi-class classification framework. Despite being popular and effective, these methods still have a few shortcomings that limit empirical performance. In this paper, we start by identifying the discrepancy between training and evaluation in the existing multi-class classification framework and then discuss the potential limitations caused by the \"competitive\" nature of softmax normalization. Motivated by these limitations, we propose a novel binary classification training framework, termed SphereFace2. In contrast to existing methods, SphereFace2 circumvents the softmax normalization, as well as the corresponding closed-set assumption. This effectively bridges the gap between training and evaluation, enabling the representations to be improved individually by each binary classification task. Besides designing a specific well-performing loss function, we summarize a few general principles for this \"one-vs-all\" binary classification framework so that it can outperform current competitive methods. Our experiments on popular benchmarks demonstrate that SphereFace2 can consistently outperform state-of-the-art deep face recognition methods.",
        "pdf_link": "https://openreview.net/pdf/32df823cca80df310924963131f7d3535aa80a57.pdf",
        "forum_url": "https://openreview.net/forum?id=l3SDgUh7qZO",
        "keywords": [
            "deep face recognition",
            "softmax normalization",
            "sphereface2"
        ]
    },
    {
        "title": "Variational methods for simulation-based inference",
        "authors": [
            "Manuel Gl\u00f6ckler",
            "Michael Deistler",
            "Jakob H. Macke"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "We present Sequential Neural Variational Inference (SNVI), an approach to perform Bayesian inference in models with intractable likelihoods. SNVI combines likelihood-estimation (or likelihood-ratio-estimation) with variational inference to achieve a scalable simulation-based inference approach. SNVI maintains the flexibility of likelihood(-ratio) estimation to allow arbitrary proposals for simulations, while simultaneously providing a functional estimate of the posterior distribution without requiring MCMC sampling. We present several variants of SNVI and demonstrate that they are substantially more computationally efficient than previous algorithms, without loss of accuracy on benchmark tasks. We apply SNVI to a neuroscience model of the pyloric network in the crab and demonstrate that it can infer the posterior distribution with one order of magnitude fewer simulations than previously reported. SNVI vastly reduces the computational cost of simulation-based inference while maintaining accuracy and flexibility, making it possible to tackle problems that were previously inaccessible.",
        "pdf_link": "https://openreview.net/pdf/7f95763dfe526e3086de5dd3efa1b9b647be0253.pdf",
        "forum_url": "https://openreview.net/forum?id=kZ0UYdhqkNY",
        "keywords": [
            "variational inference",
            "simulation based inference",
            "neural variational inference",
            "variational methods",
            "functional estimate",
            "likelihood"
        ]
    },
    {
        "title": "Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing",
        "authors": [
            "Sai Praneeth Karimireddy",
            "Lie He",
            "Martin Jaggi"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "In Byzantine robust distributed or federated learning, a central server wants to train a machine learning model over data distributed across multiple workers. However, a fraction of these workers may deviate from the prescribed algorithm and send arbitrary messages. While this problem has received significant attention recently, most current defenses assume that the workers have identical data. For realistic cases when the data across workers are heterogeneous (non-iid), we design new attacks which circumvent current defenses, leading to significant loss of performance. We then propose a simple bucketing scheme that adapts existing robust algorithms to heterogeneous datasets at a negligible computational cost. We also theoretically and experimentally validate our approach, showing that combining bucketing with existing robust algorithms is effective against challenging attacks. Our work is the first to establish guaranteed convergence for the non-iid Byzantine robust problem under realistic assumptions.\n",
        "pdf_link": "https://openreview.net/pdf/def52b4b7685b406e3aa64994f6e04b41df63bdb.pdf",
        "forum_url": "https://openreview.net/forum?id=jXKKDEi5vJt",
        "keywords": [
            "bucketing",
            "byzantine robust learning"
        ]
    },
    {
        "title": "Towards Understanding the Data Dependency of Mixup-style Training",
        "authors": [
            "Muthu Chidambaram",
            "Xiang Wang",
            "Yuzheng Hu",
            "Chenwei Wu",
            "Rong Ge"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "In the Mixup training paradigm, a model is trained using convex combinations of data points and their associated labels. Despite seeing very few true data points during training, models trained using Mixup seem to still minimize the original empirical risk and exhibit better generalization and robustness on various tasks when compared to standard training. In this paper, we investigate how these benefits of Mixup training rely on properties of the data in the context of classification. For minimizing the original empirical risk, we compute a closed form for the Mixup-optimal classification, which allows us to construct a simple dataset on which minimizing the Mixup loss leads to learning a classifier that does not minimize the empirical loss on the data. On the other hand, we also give sufficient conditions for Mixup training to also minimize the original empirical risk. For generalization, we characterize the margin of a Mixup classifier, and use this to understand why the decision boundary of a Mixup classifier can adapt better to the full structure of the training data when compared to standard training. In contrast, we also show that, for a large class of linear models and linearly separable datasets, Mixup training leads to learning the same classifier as standard training.",
        "pdf_link": "https://openreview.net/pdf/da2d0598034ce31ec61b7322e921e2900518e9f3.pdf",
        "forum_url": "https://openreview.net/forum?id=ieNJYujcGDO",
        "keywords": [
            "mixup training",
            "empirical risk",
            "classification"
        ]
    },
    {
        "title": "DISCOVERING AND EXPLAINING THE REPRESENTATION BOTTLENECK OF DNNS",
        "authors": [
            "Huiqi Deng",
            "Qihan Ren",
            "Hao Zhang",
            "Quanshi Zhang"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "This paper explores the bottleneck of feature representations of deep neural networks (DNNs), from the perspective of the complexity of interactions between input variables encoded in DNNs. To this end, we focus on the multi-order interaction between input variables, where the order represents the complexity of interactions. We discover that a DNN is more likely to encode both too simple and too complex interactions, but usually fails to learn interactions of intermediate complexity. Such a phenomenon is widely shared by different DNNs for different tasks. This phenomenon indicates a cognition gap between DNNs and humans, and we call it a representation bottleneck. We theoretically prove the underlying reason for the representation bottleneck. Furthermore, we propose losses to encourage/penalize the learning of interactions of specific complexities, and analyze the representation capacities of interactions of different complexities. The code is available at https://github.com/Nebularaid2000/bottleneck.",
        "pdf_link": "https://openreview.net/pdf/e470657e4d47a20411713a973ed0282f87c9f9a9.pdf",
        "forum_url": "https://openreview.net/forum?id=iRCUlgmdfHJ",
        "keywords": [
            "bottleneck",
            "representation bottleneck",
            "feature representations",
            "deep neural networks",
            "complexity",
            "dnns"
        ]
    },
    {
        "title": "MT3: Multi-Task Multitrack Music Transcription",
        "authors": [
            "Joshua P Gardner",
            "Ian Simon",
            "Ethan Manilow",
            "Curtis Hawthorne",
            "Jesse Engel"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Automatic Music Transcription (AMT), inferring musical notes from raw audio, is a challenging task at the core of music understanding. Unlike Automatic Speech Recognition (ASR), which typically focuses on the words of a single speaker, AMT often requires transcribing multiple instruments simultaneously, all while preserving fine-scale pitch and timing information. Further, many AMT datasets are ``low-resource'', as even expert musicians find music transcription difficult and time-consuming. Thus, prior work has focused on task-specific architectures, tailored to the individual instruments of each task. In this work, motivated by the promising results of sequence-to-sequence transfer learning for low-resource Natural Language Processing (NLP), we demonstrate that a general-purpose Transformer model can perform multi-task AMT, jointly transcribing arbitrary combinations of musical instruments across several transcription datasets. We show this unified training framework achieves high-quality transcription results across a range of datasets, dramatically improving performance for low-resource instruments (such as guitar), while preserving strong performance for abundant instruments (such as piano). Finally, by expanding the scope of AMT, we expose the need for more consistent evaluation metrics and better dataset alignment, and provide a strong baseline for this new direction of multi-task AMT.",
        "pdf_link": "https://openreview.net/pdf/d2fbcd8e79c33510066015e1639aa7edbd4a0dac.pdf",
        "forum_url": "https://openreview.net/forum?id=iMSjopcOn0p",
        "keywords": [
            "multi task",
            "automatic music transcription",
            "multi task amt"
        ]
    },
    {
        "title": "Spike-inspired rank coding for fast and accurate recurrent neural networks",
        "authors": [
            "Alan Jeffares",
            "Qinghai Guo",
            "Pontus Stenetorp",
            "Timoleon Moraitis"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Biological spiking neural networks (SNNs) can temporally encode information in their outputs, e.g. in the rank order in which neurons fire, whereas artificial neural networks (ANNs) conventionally do not. As a result, models of SNNs for neuromorphic computing are regarded as potentially more rapid and efficient than ANNs when dealing with temporal input. On the other hand, ANNs are simpler to train, and usually achieve superior performance. Here we show that temporal coding such as rank coding (RC) inspired by SNNs can also be applied to conventional ANNs such as LSTMs, and leads to computational savings and speedups.\nIn our RC for ANNs, we apply backpropagation through time using the standard real-valued activations, but only from a strategically early time step of each sequential input example, decided by a threshold-crossing event. Learning then incorporates naturally also when to produce an output, without other changes to the model or the algorithm. Both the forward and the backward training pass can be significantly shortened by skipping the remaining input sequence after that first event. RC-training also significantly reduces time-to-insight during inference, with a minimal decrease in accuracy. The desired speed-accuracy trade-off is tunable by varying the threshold or a regularization parameter that rewards output entropy. We demonstrate these in two toy problems of sequence classification, and in a temporally-encoded MNIST dataset where our RC model achieves 99.19% accuracy after the first input time-step, outperforming the state of the art in temporal coding with SNNs, as well as in spoken-word classification of Google Speech Commands, outperforming non-RC-trained early inference with LSTMs.",
        "pdf_link": "https://openreview.net/pdf/c1c6fc25f1bbf5574f5a49723ca38dafe70660ca.pdf",
        "forum_url": "https://openreview.net/forum?id=iMH1e5k7n3L",
        "keywords": [
            "recurrent neural networks",
            "temporal coding",
            "spike inspired rank coding"
        ]
    },
    {
        "title": "PAC-Bayes Information Bottleneck",
        "authors": [
            "Zifeng Wang",
            "Shao-Lun Huang",
            "Ercan Engin Kuruoglu",
            "Jimeng Sun",
            "Xi Chen",
            "Yefeng Zheng"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Understanding the source of the superior generalization ability of NNs remains one of the most important problems in ML research. There have been a series of theoretical works trying to derive non-vacuous bounds for NNs. Recently, the compression of information stored in weights (IIW) is proved to play a key role in NNs generalization based on the PAC-Bayes theorem. However, no solution of IIW has ever been provided, which builds a barrier for further investigation of the IIW's property and its potential in practical deep learning. In this paper, we propose an algorithm for the efficient approximation of IIW. Then, we build an IIW-based information bottleneck on the trade-off between accuracy and information complexity of NNs, namely PIB. From PIB, we can empirically identify the fitting to compressing phase transition during NNs' training and the concrete connection between the IIW compression and the generalization. Besides, we verify that IIW is able to explain NNs in broad cases, e.g., varying batch sizes, over-parameterization, and noisy labels. Moreover, we propose an MCMC-based algorithm to sample from the optimal weight posterior characterized by PIB, which fulfills the potential of IIW in enhancing NNs in practice.",
        "pdf_link": "https://openreview.net/pdf/3c2adeb5d32bd783ea4cbafee0397d6e76f81ce7.pdf",
        "forum_url": "https://openreview.net/forum?id=iLHOIDsPv1P",
        "keywords": [
            "information bottleneck",
            "phase transition",
            "information complexity",
            "deep learning"
        ]
    },
    {
        "title": "TRGP: Trust Region Gradient Projection for Continual Learning",
        "authors": [
            "Sen Lin",
            "Li Yang",
            "Deliang Fan",
            "Junshan Zhang"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Catastrophic forgetting is one of the major challenges in continual learning. To address this issue, some existing methods put restrictive constraints on the optimization space of the new task for minimizing the interference to old tasks. However, this may lead to unsatisfactory performance for the new task, especially when the new task is strongly correlated with old tasks. To tackle this challenge, we propose Trust Region Gradient Projection (TRGP) for continual learning to facilitate the forward knowledge transfer based on an efficient characterization of task correlation. Particularly, we introduce a notion of 'trust region' to select the most related old tasks for the new task in a layer-wise and single-shot manner, using the norm of gradient projection onto the subspace spanned by task inputs. Then, a scaled weight projection is proposed to cleverly reuse the frozen weights of the selected old tasks in the trust region through a layer-wise scaling matrix. By jointly optimizing the scaling matrices and the model, where the model is updated along the directions orthogonal to the subspaces of old tasks,  TRGP can effectively prompt knowledge transfer without forgetting. Extensive experiments show that our approach achieves significant improvement over related state-of-the-art methods.",
        "pdf_link": "https://openreview.net/pdf/7291715ef56df6f6a92491a5977eb95a2db1ca86.pdf",
        "forum_url": "https://openreview.net/forum?id=iEvAf8i6JjO",
        "keywords": [
            "trust region gradient projection",
            "continual learning",
            "norm of gradient projection",
            "scaled weight projection"
        ]
    },
    {
        "title": "Poisoning and Backdooring Contrastive Learning",
        "authors": [
            "Nicholas Carlini",
            "Andreas Terzis"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually, and even improves out-of-distribution robustness. We show that this practice makes backdoor and poisoning attacks a significant threat. By poisoning just 0.01% of a dataset (e.g., just 300 images of the 3 million-example Conceptual Captions dataset), we can cause the model to misclassify test images by overlaying a small patch. Targeted poisoning attacks, whereby the model misclassifies a particular test input  with an adversarially-desired label, are even easier requiring control of 0.0001% of the dataset (e.g., just three out of the 3 million images). Our attacks call into question whether training on noisy and uncurated Internet scrapes is desirable.",
        "pdf_link": "https://openreview.net/pdf/abd77f0543a72cd26da355efc5680de233f120af.pdf",
        "forum_url": "https://openreview.net/forum?id=iC4UHbQ01Mp",
        "keywords": [
            "poisoning",
            "backdoor and poisoning",
            "targeted poisoning attacks"
        ]
    },
    {
        "title": "Scalable One-Pass Optimisation of High-Dimensional Weight-Update Hyperparameters by Implicit Differentiation",
        "authors": [
            "Ross M Clarke",
            "Elre Talea Oldewage",
            "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Machine learning training methods depend plentifully and intricately on hyperparameters, motivating automated strategies for their optimisation. Many existing algorithms restart training for each new hyperparameter choice, at considerable computational cost. Some hypergradient-based one-pass methods exist, but these either cannot be applied to arbitrary optimiser hyperparameters (such as learning rates and momenta) or take several times longer to train than their base models. We extend these existing methods to develop an approximate hypergradient-based hyperparameter optimiser which is applicable to any continuous hyperparameter appearing in a differentiable model weight update, yet requires only one training episode, with no restarts. We also provide a motivating argument for convergence to the true hypergradient, and perform tractable gradient-based optimisation of independent learning rates for each model parameter. Our method performs competitively from varied random hyperparameter initialisations on several UCI datasets and Fashion-MNIST (using a one-layer MLP), Penn Treebank (using an LSTM) and CIFAR-10 (using a ResNet-18), in time only 2-3x greater than vanilla training.",
        "pdf_link": "https://openreview.net/pdf/a7f35bc772a0804247c8631982741afe42ec790e.pdf",
        "forum_url": "https://openreview.net/forum?id=hfU7Ka5cfrC",
        "keywords": [
            "hyperparameter",
            "implicit differentiation",
            "update hyperparameters",
            "machine learning",
            "on hyperparameters",
            "scalable one pass optimisation",
            "hypergradient"
        ]
    },
    {
        "title": "Scaling Laws for Neural Machine Translation",
        "authors": [
            "Behrooz Ghorbani",
            "Orhan Firat",
            "Markus Freitag",
            "Ankur Bapna",
            "Maxim Krikun",
            "Xavier Garcia",
            "Ciprian Chelba",
            "Colin Cherry"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "We present an empirical study of scaling properties of encoder-decoder Transformer models used in neural machine translation (NMT). We show that cross-entropy loss as a function of model size follows a certain scaling law. Specifically (i) We propose a formula which describes the scaling behavior of cross-entropy loss as a bivariate function of encoder and decoder size, and show that it gives accurate predictions under a variety of scaling approaches and languages; we show that the total number of parameters alone is not sufficient for such purposes. (ii) We observe different power law exponents when scaling the decoder vs scaling the encoder, and provide recommendations for optimal allocation of encoder/decoder capacity based on this observation. (iii) We also report that the scaling behavior of the model is acutely influenced by composition bias of the train/test sets, which we define as any deviation from naturally generated text (either via machine generated or human translated text). We observe that natural text on the target side enjoys scaling, which manifests as successful reduction of the cross-entropy loss. (iv) Finally, we investigate the relationship between the cross-entropy loss and the quality of the generated translations. We find two different behaviors, depending on the nature of the test data. For test sets which were originally translated from target language to source language, both loss and BLEU score improve as model size increases. In contrast, for test sets originally translated from source language to target language, the loss improves, but the BLEU score stops improving after a certain threshold. We release generated text from all models used in this study.",
        "pdf_link": "https://openreview.net/pdf/dec3d7582a0893c49661157564fdbe66ccc0036f.pdf",
        "forum_url": "https://openreview.net/forum?id=hR_SMu8cxCV",
        "keywords": [
            "translated",
            "neural machine translation",
            "cross entropy",
            "scaling",
            "scaling laws",
            "encoder",
            "cross entropy loss",
            "capacity"
        ]
    },
    {
        "title": "Compositional Training for End-to-End Deep AUC Maximization",
        "authors": [
            "Zhuoning Yuan",
            "Zhishuai Guo",
            "Nitesh Chawla",
            "Tianbao Yang"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Recently, deep AUC maximization (DAM) has achieved great success in different domains (e.g., medical image classification). However, the end-to-end training for deep AUC maximization still remains a challenging problem. Previous studies employ an ad-hoc  two-stage approach that first trains the network by optimizing a traditional  loss (e.g., cross-entropy loss) and then finetunes the network by optimizing an AUC loss. This is because that training a deep neural network from scratch by maximizing an AUC loss usually does not yield a satisfactory performance. This phenomenon can be attributed to the degraded feature representations learned by maximizing the AUC loss from scratch. To address this issue, we propose a novel compositional training framework for end-to-end DAM, namely compositional DAM. The key idea of compositional training is to minimize a compositional objective function, where the outer function corresponds to an AUC loss and the inner function represents  a gradient descent step for minimizing a traditional loss, e.g., the cross-entropy (CE) loss. To optimize the non-standard compositional objective, we propose an efficient and provable stochastic optimization algorithm. The proposed algorithm enhances the capabilities  of  both robust feature learning and robust classifier learning  by alternatively taking a gradient descent step for the CE loss and for the AUC loss in a systematic way.  We conduct extensive empirical studies on imbalanced benchmark and medical image datasets, which unanimously verify the effectiveness of the proposed method.  Our results show that the compositional training approach dramatically improves both the feature representations and the testing AUC score compared with traditional deep learning approaches, and yields better performance than the two-stage approaches for DAM as well. The proposed method is implemented in our open-sourced library LibAUC (https://www.libauc.org) and code is available at https://github.com/Optimization-AI/LibAUC.",
        "pdf_link": "https://openreview.net/pdf/a32a2790db0163f6e6f71daa98631818c4713912.pdf",
        "forum_url": "https://openreview.net/forum?id=gPvB4pdu_Z",
        "keywords": [
            "auc",
            "deep auc maximization",
            "compositional training"
        ]
    },
    {
        "title": "Finetuned Language Models are Zero-Shot Learners",
        "authors": [
            "Jason Wei",
            "Maarten Bosma",
            "Vincent Zhao",
            "Kelvin Guu",
            "Adams Wei Yu",
            "Brian Lester",
            "Nan Du",
            "Andrew M. Dai",
            "Quoc V Le"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning\u2014finetuning language models on a collection of datasets described via instructions\u2014substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",
        "pdf_link": "https://openreview.net/pdf/16b50405ab1e3ac1e2f76190ee62a48c496c568d.pdf",
        "forum_url": "https://openreview.net/forum?id=gEZrGCozdqR",
        "keywords": [
            "instruction tune",
            "finetuned",
            "natural language",
            "language models",
            "zero shot learning",
            "shot performance"
        ]
    },
    {
        "title": "NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning",
        "authors": [
            "Chun-Hao Chang",
            "Rich Caruana",
            "Anna Goldenberg"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Deployment of machine learning models in real high-risk settings (e.g. healthcare) often depends not only on the model's accuracy but also on its fairness, robustness, and interpretability. Generalized Additive Models (GAMs) are a class of interpretable models with a long history of use in these high-risk domains, but they lack desirable features of deep learning such as differentiability and scalability. In this work, we propose a neural GAM (NODE-GAM) and neural GA$^2$M (NODE-GA$^2$M) that scale well and perform better than other GAMs on large datasets, while remaining interpretable compared to other ensemble and deep learning models. We demonstrate that our models find interesting patterns in the data. Lastly, we show that we are able to improve model accuracy via self-supervised pre-training, an improvement that is not possible for non-differentiable GAMs.",
        "pdf_link": "https://openreview.net/pdf/dca44cbebb8f51bf19f423539a640118c674db6f.pdf",
        "forum_url": "https://openreview.net/forum?id=g8NJR6fCCl8",
        "keywords": [
            "learning",
            "deep learning",
            "generalized additive model",
            "ensemble and deep learning",
            "interpretable deep learning",
            "ga"
        ]
    },
    {
        "title": "Properties from mechanisms: an equivariance perspective on identifiable representation learning",
        "authors": [
            "Kartik Ahuja",
            "Jason Hartford",
            "Yoshua Bengio"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "A key goal of unsupervised representation learning is ``inverting'' a data generating process to recover its latent properties.  Existing work that provably achieves this goal relies on strong assumptions on relationships between the latent variables (e.g., independence conditional on auxiliary information). In this paper, we take a very different perspective on the problem and ask,  ``Can we instead identify latent properties by leveraging knowledge of the mechanisms that govern their evolution?'' We provide a complete characterization of the sources of non-identifiability as we vary knowledge about a set of possible mechanisms. In particular, we prove that if we know the exact mechanisms under which the latent properties evolve, then identification can be achieved up to any equivariances that are shared by the underlying mechanisms. We generalize this characterization to settings where we only know some hypothesis class over possible mechanisms, as well as settings where the mechanisms are stochastic. We demonstrate the power of this mechanism-based perspective by showing that we can leverage our results to generalize existing identifiable representation learning results. These results suggest that by exploiting inductive biases on mechanisms, it is possible to design a range of new identifiable representation learning approaches.",
        "pdf_link": "https://openreview.net/pdf/2b2b18f71be3fd973de88f72c37e764d69dde18a.pdf",
        "forum_url": "https://openreview.net/forum?id=g5ynW-jMq4M",
        "keywords": [
            "representation learning",
            "identifiable representation learning",
            "design"
        ]
    },
    {
        "title": "Wiring Up Vision: Minimizing Supervised Synaptic Updates Needed to Produce a Primate Ventral Stream",
        "authors": [
            "Franziska Geiger",
            "Martin Schrimpf",
            "Tiago Marques",
            "James J. DiCarlo"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "After training on large datasets, certain deep neural networks are surprisingly good models of the neural mechanisms of adult primate visual object recognition. Nevertheless, these models are considered poor models of the development of the visual system because they posit millions of sequential, precisely coordinated synaptic updates, each based on a labeled image.  While ongoing research is pursuing the use of unsupervised proxies for labels, we here explore a complementary strategy of reducing the required number of supervised synaptic updates to produce an adult-like ventral visual stream (as judged by the match to V1, V2, V4, IT, and behavior). Such models might require less precise machinery and energy expenditure to coordinate these updates and would thus move us closer to viable neuroscientific hypotheses about how the visual system wires itself up. Relative to standard model training on labeled images in ImageNet, we here demonstrate that the total number of supervised weight updates can be substantially reduced using three complementary strategies: First, we find that only 2% of supervised updates (epochs and images) are needed to achieve 80% of the match to adult ventral stream. Specifically, training benefits predictions of higher visual cortex the most whereas early visual cortex predictions only improve marginally over the course of training. Second, by improving the random distribution of synaptic connectivity, we find that 54% of the brain match can already be achieved \u201cat birth\" (i.e. no training at all). Third, we find that, by training only 5% of model synapses, we can still achieve nearly 80% of the match to the ventral stream. This approach further improves on ImageNet performance over previous attempts in computer vision of minimizing trained components without substantially increasing the relative number of trained parameters. These results reflect first steps in modeling not just primate adult visual processing during inference, but also how the ventral visual stream might be \"wired up\" by evolution (a model's \"birth\" state) and by developmental learning (a model's updates based on visual experience).",
        "pdf_link": "https://openreview.net/pdf/52d6dfe1e693892f0382d80002c972022ced3a49.pdf",
        "forum_url": "https://openreview.net/forum?id=g1SzIRLQXMM",
        "keywords": [
            "primate",
            "proxies",
            "imagenet",
            "primate visual object recognition",
            "inference",
            "supervised",
            "supervised updates",
            "updates",
            "supervised synaptic updates"
        ]
    },
    {
        "title": "Perceiver IO: A General Architecture for Structured Inputs & Outputs",
        "authors": [
            "Andrew Jaegle",
            "Sebastian Borgeaud",
            "Jean-Baptiste Alayrac",
            "Carl Doersch",
            "Catalin Ionescu",
            "David Ding",
            "Skanda Koppula",
            "Daniel Zoran",
            "Andrew Brock",
            "Evan Shelhamer",
            "Olivier J Henaff",
            "Matthew Botvinick",
            "Andrew Zisserman",
            "Oriol Vinyals",
            "Joao Carreira"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain & task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.",
        "pdf_link": "https://openreview.net/pdf/be7bf6b12e6abb37fb7853467cc6ef71ea5a1659.pdf",
        "forum_url": "https://openreview.net/forum?id=fILj7WpI-g",
        "keywords": []
    },
    {
        "title": "On Predicting Generalization using GANs",
        "authors": [
            "Yi Zhang",
            "Arushi Gupta",
            "Nikunj Saunshi",
            "Sanjeev Arora"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Research on generalization bounds for deep networks seeks to give ways to predict test error using just the training dataset and the network parameters. While generalization bounds can give many insights about architecture design, training algorithms etc., what they do not currently do is yield good predictions for actual test error. A recently introduced Predicting Generalization in Deep Learning competition aims to encourage discovery of methods to better predict test error. The current paper investigates a simple idea: can test error be predicted using {\\em synthetic data,} produced using a Generative Adversarial Network (GAN) that was trained on the same training dataset? Upon investigating several GAN models and architectures, we find that this turns out to be the case. \n\nIn fact, using GANs pre-trained on standard datasets, the test error can be predicted without requiring any additional hyper-parameter tuning. This result is surprising because GANs have well-known limitations (e.g. mode collapse) and are known to not learn the data distribution accurately. Yet the generated samples are good enough to substitute for test data. Several additional experiments are presented to explore reasons why GANs do well at this task. In addition to a new approach for predicting generalization, the counter-intuitive phenomena presented in our work may also call for a better understanding of GANs' strengths and limitations.",
        "pdf_link": "https://openreview.net/pdf/c02dd2fa195251e1b7cc85379208fea1bc5f6a53.pdf",
        "forum_url": "https://openreview.net/forum?id=eW5R4Cek6y6",
        "keywords": [
            "gan",
            "generalization bounds",
            "generalization",
            "gan models",
            "test error",
            "predicting generalization",
            "deep learning"
        ]
    },
    {
        "title": "Sampling with Mirrored Stein Operators",
        "authors": [
            "Jiaxin Shi",
            "Chang Liu",
            "Lester Mackey"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "We introduce a new family of particle evolution samplers suitable for constrained domains and non-Euclidean geometries. Stein Variational Mirror Descent and Mirrored Stein Variational Gradient Descent minimize the Kullback-Leibler (KL) divergence to constrained target distributions by evolving particles in a dual space defined by a mirror map. Stein Variational Natural Gradient exploits non-Euclidean geometry to more efficiently minimize the KL divergence to unconstrained targets. We derive these samplers from a new class of mirrored Stein operators and adaptive kernels developed in this work. We demonstrate that these new samplers yield accurate approximations to distributions on the simplex, deliver valid confidence intervals in post-selection inference, and converge more rapidly than prior methods in large-scale unconstrained posterior inference. Finally, we establish the convergence of our new procedures under verifiable conditions on the target distribution.",
        "pdf_link": "https://openreview.net/pdf/5c867e64ede248dde6dfa23bae9c1f0365022fdb.pdf",
        "forum_url": "https://openreview.net/forum?id=eMudnJsb1T5",
        "keywords": [
            "mirrored stein operators",
            "mirror map",
            "sampling",
            "posterior inference"
        ]
    },
    {
        "title": "Analyzing and Improving the Optimization Landscape of Noise-Contrastive Estimation",
        "authors": [
            "Bingbin Liu",
            "Elan Rosenfeld",
            "Pradeep Kumar Ravikumar",
            "Andrej Risteski"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Noise-contrastive estimation (NCE) is a statistically consistent method for learning unnormalized probabilistic models. It has been empirically observed that the choice of the noise distribution is crucial for NCE\u2019s performance. However, such observation has never been made formal or quantitative. In fact, it is not even clear whether the difficulties arising from a poorly chosen noise distribution are statistical or algorithmic in nature.\nIn this work, we formally pinpoint reasons for NCE\u2019s poor performance when an inappropriate noise distribution is used. Namely, we prove these challenges arise due to an ill-behaved (more precisely, flat) loss landscape.\nTo address this, we introduce a variant of NCE called \\emph{eNCE} which uses an exponential loss and for which \\emph{normalized gradient descent} addresses the landscape issues \\emph{provably} when the target and noise distributions are in a given exponential family. ",
        "pdf_link": "https://openreview.net/pdf/6fece6afb2eea04bf01bd8ff7ec9da4a780eb660.pdf",
        "forum_url": "https://openreview.net/forum?id=eBS-3YiaIL-",
        "keywords": [
            "noise contrastive estimation"
        ]
    },
    {
        "title": "ViTGAN: Training GANs with Vision Transformers",
        "authors": [
            "Kwonjoon Lee",
            "Huiwen Chang",
            "Lu Jiang",
            "Han Zhang",
            "Zhuowen Tu",
            "Ce Liu"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Recently, Vision Transformers (ViTs) have shown competitive performance on image recognition while requiring less vision-specific inductive biases. In this paper, we investigate if such performance can be extended to image generation. To this end, we integrate the ViT architecture into generative adversarial networks (GANs). For ViT discriminators, we observe that existing regularization methods for GANs interact poorly with self-attention, causing serious instability during training. To resolve this issue, we introduce several novel regularization techniques for training GANs with ViTs. For ViT generators, we examine architectural choices for latent and pixel mapping layers to faciliate convergence. Empirically, our approach, named ViTGAN, achieves comparable performance to the leading CNN- based GAN models on three datasets: CIFAR-10, CelebA, and LSUN bedroom.",
        "pdf_link": "https://openreview.net/pdf/12b5f82c142cc00c08e950c5a49db1948f14aa54.pdf",
        "forum_url": "https://openreview.net/forum?id=dwg5rXg1WS_",
        "keywords": [
            "vision transformers",
            "gans",
            "regularization",
            "gan models"
        ]
    },
    {
        "title": "Learnability of convolutional neural networks for infinite dimensional input via mixed and anisotropic smoothness",
        "authors": [
            "Sho Okumoto",
            "Taiji Suzuki"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Among a wide range of success of deep learning, convolutional neural networks have been extensively utilized in several tasks such as speech recognition, image processing, and natural language processing, which require inputs with large dimensions.\nSeveral studies have investigated function estimation capability of deep learning, but most of them have assumed that the dimensionality of the input is much smaller than the sample size. \nHowever, for typical data in applications such as those handled by the convolutional neural networks described above, \nthe dimensionality of inputs is relatively high or even infinite. \nIn this paper, we investigate the approximation and estimation errors of the (dilated) convolutional neural networks when the input is infinite dimensional. \nAlthough the approximation and estimation errors of neural networks are affected by the curse of dimensionality in the existing analyses for typical function spaces such as the \\Holder and Besov spaces, we show that, by considering anisotropic smoothness, they can alleviate exponential dependency on the dimensionality but they only depend on the smoothness of the target functions. \nOur theoretical analysis supports the great practical success of convolutional networks.  \nFurthermore, we show that the dilated convolution is advantageous when the smoothness of the target function has a sparse structure.",
        "pdf_link": "https://openreview.net/pdf/634d72e29913c5641e4d1a3370a43676421f5b75.pdf",
        "forum_url": "https://openreview.net/forum?id=dgxFTxuJ50e",
        "keywords": [
            "neural networks",
            "convolutional neural networks",
            "convolutional networks",
            "dilated convolution",
            "and estimation",
            "learnability"
        ]
    },
    {
        "title": "Revisiting Over-smoothing in BERT from the Perspective of Graph",
        "authors": [
            "Han Shi",
            "JIAHUI GAO",
            "Hang Xu",
            "Xiaodan Liang",
            "Zhenguo Li",
            "Lingpeng Kong",
            "Stephen M. S. Lee",
            "James Kwok"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Recently over-smoothing phenomenon of Transformer-based models is observed in both vision and language fields. However, no existing work has delved deeper to further investigate the main cause of this phenomenon. In this work, we make the attempt to analyze the over-smoothing problem from the perspective of graph, where such problem was first discovered and explored. Intuitively, the self-attention matrix can be seen as a normalized adjacent matrix of a corresponding graph. Based on the above connection, we provide some theoretical analysis and find that layer normalization plays a key role in the over-smoothing issue of Transformer-based models. Specifically, if the standard deviation of layer normalization is sufficiently large, the output of Transformer stacks will converge to a specific low-rank subspace and result in over-smoothing. To alleviate the over-smoothing problem, we consider hierarchical fusion strategies, which combine the representations from different layers adaptively to make the output more diverse. Extensive experiment results on various data sets illustrate the effect of our fusion method.",
        "pdf_link": "https://openreview.net/pdf/c6d11173508af7016a361ef28688912c6b0a80a8.pdf",
        "forum_url": "https://openreview.net/forum?id=dUV91uaXm3",
        "keywords": [
            "smoothing"
        ]
    },
    {
        "title": "Equivariant Subgraph Aggregation Networks",
        "authors": [
            "Beatrice Bevilacqua",
            "Fabrizio Frasca",
            "Derek Lim",
            "Balasubramaniam Srinivasan",
            "Chen Cai",
            "Gopinath Balamurugan",
            "Michael M. Bronstein",
            "Haggai Maron"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Message-passing neural networks (MPNNs) are the leading architecture for deep learning on graph-structured data, in large part due to their simplicity and scalability. Unfortunately, it was shown that these architectures are limited in their expressive power. This paper proposes a novel framework called Equivariant Subgraph Aggregation Networks (ESAN) to address this issue. Our main observation is that while two graphs may not be distinguishable by an MPNN, they often contain distinguishable subgraphs. Thus, we propose to represent each graph as a set of subgraphs derived by some predefined policy, and to process it using a suitable equivariant architecture. We develop novel variants of the 1-dimensional Weisfeiler-Leman (1-WL) test for graph isomorphism, and prove lower bounds on the expressiveness of ESAN in terms of these new WL variants. We further prove that our approach increases the expressive power of both MPNNs and more expressive architectures. Moreover, we provide theoretical results that describe how design choices such as the subgraph selection policy and equivariant neural architecture affect our architecture's expressive power. To deal with the increased computational cost, we propose a subgraph sampling scheme, which can be viewed as a stochastic version of our framework. A comprehensive set of experiments on real and synthetic datasets demonstrates that our framework improves the expressive power and overall performance of popular GNN architectures. ",
        "pdf_link": "https://openreview.net/pdf/a9272500150ccf0f8fafbd6cb0a26e71c003663f.pdf",
        "forum_url": "https://openreview.net/forum?id=dFbKQaRk15w",
        "keywords": [
            "subgraph",
            "subgraph aggregation networks",
            "graph isomorphism",
            "graph structured",
            "message passing neural networks",
            "equivariant",
            "expressive"
        ]
    },
    {
        "title": "When should agents explore?",
        "authors": [
            "Miruna Pislar",
            "David Szepesvari",
            "Georg Ostrovski",
            "Diana L Borsa",
            "Tom Schaul"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Exploration remains a central challenge for reinforcement learning (RL). Virtually all existing methods share the feature of a *monolithic* behaviour policy that changes only gradually (at best). In contrast, the exploratory behaviours of animals and humans exhibit a rich diversity, namely including forms of *switching* between modes. This paper presents an initial study of mode-switching, non-monolithic exploration for RL. We investigate different modes to switch between, at what timescales it makes sense to switch, and what signals make for good switching triggers. We also propose practical algorithmic components that make the switching mechanism adaptive and robust, which enables flexibility without an accompanying hyper-parameter-tuning burden. Finally, we report a promising initial study on Atari, using two-mode exploration and switching at sub-episodic time-scales.",
        "pdf_link": "https://openreview.net/pdf/5628a68890630dcf1b41b8c21140287bb6d2e9eb.pdf",
        "forum_url": "https://openreview.net/forum?id=dEwfxt14bca",
        "keywords": [
            "reinforcement learning",
            "mode exploration",
            "exploration",
            "adaptive"
        ]
    },
    {
        "title": "Leveraging Automated Unit Tests for Unsupervised Code Translation",
        "authors": [
            "Baptiste Roziere",
            "Jie Zhang",
            "Francois Charton",
            "Mark Harman",
            "Gabriel Synnaeve",
            "Guillaume Lample"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "With little to no parallel data available for programming languages, unsupervised methods are well-suited to source code translation. However, the majority of unsupervised machine translation approaches rely on back-translation, a method developed in the context of natural language translation and one that inherently involves training on noisy inputs. Unfortunately, source code is highly sensitive to small changes; a single token can result in compilation failures or erroneous programs, unlike natural languages where small inaccuracies may not change the meaning of a sentence. To address this issue, we propose to leverage an automated unit-testing system to filter out invalid translations, thereby creating a fully tested parallel corpus. We found that fine-tuning an unsupervised model with this filtered data set significantly reduces the noise in the translations so-generated, comfortably outperforming the state-of-the-art for all language pairs studied. In particular, for Java\u2192Python and Python\u2192C++ we outperform the best previous methods by more than 16% and 24% respectively, reducing the error rate by more than 35%.",
        "pdf_link": "https://openreview.net/pdf/5afda866e17de3287b9281461435fdc488309beb.pdf",
        "forum_url": "https://openreview.net/forum?id=cmt-6KtR4c4",
        "keywords": [
            "unit tests",
            "language",
            "natural languages",
            "language translation",
            "translation",
            "code translation",
            "unsupervised"
        ]
    },
    {
        "title": "Towards Deployment-Efficient Reinforcement Learning: Lower Bound and Optimality",
        "authors": [
            "Jiawei Huang",
            "Jinglin Chen",
            "Li Zhao",
            "Tao Qin",
            "Nan Jiang",
            "Tie-Yan Liu"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Deployment efficiency is an important criterion for many real-world applications of reinforcement learning (RL). Despite the community's increasing interest, there lacks a formal theoretical formulation for the problem. In this paper, we propose such a formulation for deployment-efficient RL (DE-RL) from an ''optimization with constraints'' perspective: we are interested in exploring an MDP and obtaining a near-optimal policy within minimal \\emph{deployment complexity}, whereas in each deployment the policy can sample a large batch of data. Using finite-horizon linear MDPs as a concrete structural model, we reveal the fundamental limit in achieving deployment efficiency by establishing information-theoretic lower bounds, and provide algorithms that achieve the optimal deployment efficiency. Moreover, our formulation for DE-RL is flexible and can serve as a building block for other practically relevant settings; we give ''Safe DE-RL'' and ''Sample-Efficient DE-RL'' as two examples, which may be worth future investigation.",
        "pdf_link": "https://openreview.net/pdf/a0405b77400aed6d75d6c20a20d9a5335d22d314.pdf",
        "forum_url": "https://openreview.net/forum?id=ccWaPGl9Hq",
        "keywords": [
            "reinforcement learning",
            "deployment efficient reinforcement learning",
            "deployment efficient rl",
            "deployment efficiency",
            "de rl",
            "optimality",
            "bound"
        ]
    },
    {
        "title": "Learning more skills through optimistic exploration",
        "authors": [
            "DJ Strouse",
            "Kate Baumli",
            "David Warde-Farley",
            "Volodymyr Mnih",
            "Steven Stenberg Hansen"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Unsupervised skill learning objectives (Eysenbach et al., 2019; Gregor et al., 2016) allow agents to learn rich repertoires of behavior in the absence of extrinsic rewards. They work by simultaneously training a policy to produce distinguishable latent-conditioned trajectories, and a discriminator to evaluate distinguishability by trying to infer latents from trajectories. The hope is for the agent to explore and master the environment by encouraging each skill (latent) to reliably reach different states. However, an inherent exploration problem lingers: when a novel state is actually encountered, the discriminator will necessarily not have seen enough training data to produce accurate and confident skill classifications, leading to low intrinsic reward for the agent and effective penalization of the sort of exploration needed to actually maximize the objective. To combat this inherent pessimism towards exploration, we derive an information gain auxiliary objective that involves training an ensemble of discriminators and rewarding the policy for their disagreement. Our objective directly estimates the epistemic uncertainty that comes from the discriminator not having seen enough training examples, thus providing an intrinsic reward more tailored to the true objective compared to pseudocount-based methods (Burda et al., 2019). We call this exploration bonus discriminator disagreement intrinsic reward, or DISDAIN. We demonstrate empirically that DISDAIN improves skill learning both in a tabular grid world (Four Rooms) and the 57 games of the Atari Suite (from pixels). Thus, we encourage researchers to treat pessimism with DISDAIN.",
        "pdf_link": "https://openreview.net/pdf/fd77361933d33f5982e69d08631cf6222a3c48ce.pdf",
        "forum_url": "https://openreview.net/forum?id=cU8rknuhxc",
        "keywords": [
            "learning",
            "skill learning",
            "pessimism",
            "optimistic exploration",
            "policy",
            "uncertainty"
        ]
    },
    {
        "title": "Large Language Models Can Be Strong Differentially Private Learners",
        "authors": [
            "Xuechen Li",
            "Florian Tramer",
            "Percy Liang",
            "Tatsunori Hashimoto"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Differentially Private (DP) learning has seen limited success for building large deep learning models of text, and straightforward attempts at applying Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have resulted in large performance drops and high computational overhead.\nWe show that this performance drop can be mitigated with (1) the use of large pretrained language models; (2) non-standard hyperparameters that suit DP optimization; and (3) fine-tuning objectives which are aligned with the pretraining procedure.\nWith the above, we obtain NLP models that outperform state-of-the-art DP-trained models under the same privacy budget and strong non-private baselines---by directly fine-tuning pretrained models with DP optimization on moderately-sized corpora. \nTo address the computational challenge of running DP-SGD with large Transformers, we propose a memory saving technique that allows clipping in DP-SGD to run without instantiating per-example gradients for any linear layer in the model. \nThe technique enables privately training Transformers with almost the same memory cost as non-private training at a modest run-time overhead. \nContrary to conventional wisdom that DP optimization fails at learning high-dimensional models (due to noise that scales with dimension) empirical results reveal that private learning with pretrained language models tends to not suffer from dimension-dependent performance degradation.\nCode to reproduce results can be found at https://github.com/lxuechen/private-transformers.\n",
        "pdf_link": "https://openreview.net/pdf/d88e1e721c4085b8a6403837f45b8c483ad0225b.pdf",
        "forum_url": "https://openreview.net/forum?id=bVuP3ltATMz",
        "keywords": [
            "differentially private",
            "large language models",
            "differentially private stochastic"
        ]
    },
    {
        "title": "Policy improvement by planning with Gumbel",
        "authors": [
            "Ivo Danihelka",
            "Arthur Guez",
            "Julian Schrittwieser",
            "David Silver"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "AlphaZero is a powerful reinforcement learning algorithm based on approximate policy iteration and tree search. However, AlphaZero can fail to improve its policy network, if not visiting all actions at the root of a search tree. To address this issue, we propose a policy improvement algorithm based on sampling actions without replacement. Furthermore, we use the idea of policy improvement to replace the more heuristic mechanisms by which AlphaZero selects and uses actions, both at root nodes and at non-root nodes. Our new algorithms, Gumbel AlphaZero and Gumbel MuZero, respectively without and with model-learning, match the state of the art on Go, chess, and Atari, and significantly improve prior performance when planning with few simulations.",
        "pdf_link": "https://openreview.net/pdf/4f2c0c813d0fbe127329c69b1ba216fbcd95d52c.pdf",
        "forum_url": "https://openreview.net/forum?id=bERaNdoegnO",
        "keywords": [
            "policy improvement",
            "policy network",
            "reinforcement learning",
            "model learning",
            "sampling",
            "tree search"
        ]
    },
    {
        "title": "Bootstrapped Meta-Learning",
        "authors": [
            "Sebastian Flennerhag",
            "Yannick Schroecker",
            "Tom Zahavy",
            "Hado van Hasselt",
            "David Silver",
            "Satinder Singh"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Meta-learning empowers artificial intelligence to increase its efficiency by learning how to learn. Unlocking this potential involves overcoming a challenging meta-optimisation problem. We propose an algorithm that tackles this problem by letting the meta-learner teach itself. The algorithm first bootstraps a target from the meta-learner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. Focusing on meta-learning with gradients, we establish conditions that guarantee performance improvements and show that metric can be used to control meta-optimisation. Meanwhile, the bootstrapping mechanism can extend the effective meta-learning horizon without requiring backpropagation through all updates. We achieve a new state-of-the art for model-free agents on the Atari ALE benchmark and demonstrate that it yields both performance and efficiency gains in multi-task meta-learning. Finally, we explore how bootstrapping opens up new possibilities and find that it can meta-learn efficient exploration in an epsilon-greedy Q-learning agent - without backpropagating through the update rule.",
        "pdf_link": "https://openreview.net/pdf/0eccd48eddcbf9cfc77b50cb0e97fb58937aee70.pdf",
        "forum_url": "https://openreview.net/forum?id=b-ny3x071E5",
        "keywords": [
            "meta learning",
            "meta optimisation",
            "bootstrapped meta learning",
            "artificial intelligence",
            "bootstrapping"
        ]
    },
    {
        "title": "Asymmetry Learning for Counterfactually-invariant Classification in OOD Tasks",
        "authors": [
            "S Chandra Mouli",
            "Bruno Ribeiro"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Generalizing from observed to new related environments (out-of-distribution) is central to the reliability of classifiers. However, most classifiers fail to predict label $Y$ from input $X$ when the change in environment is due a (stochastic) input transformation $T^\\text{te} \\circ X'$ not observed in training, as in training we observe $T^\\text{tr} \\circ X'$, where $X'$ is a hidden variable. This work argues that when the transformations in train $T^\\text{tr}$ and test $T^\\text{te}$ are (arbitrary) symmetry transformations induced by a collection of known $m$ equivalence relations, the task of finding a robust OOD classifier can be defined as finding the simplest causal model that defines a causal connection between the target labels and the symmetry transformations that are associated with label changes. We then propose a new learning paradigm, asymmetry learning, that identifies which symmetries the classifier must break in order to correctly predict $Y$ in both train and test. Asymmetry learning performs a causal model search that, under certain identifiability conditions, finds classifiers that perform equally well in-distribution and out-of-distribution. Finally, we show how to learn counterfactually-invariant representations with asymmetry learning in two physics tasks.",
        "pdf_link": "https://openreview.net/pdf/f15da1dc02ded9aba4a26e8ade750b28429da30f.pdf",
        "forum_url": "https://openreview.net/forum?id=avgclFZ221l",
        "keywords": [
            "asymmetry learning",
            "counterfactually invariant classification",
            "symmetry transformations"
        ]
    },
    {
        "title": "Ab-Initio Potential Energy Surfaces by Pairing GNNs with Neural Wave Functions",
        "authors": [
            "Nicholas Gao",
            "Stephan G\u00fcnnemann"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Solving the Schr\u00f6dinger equation is key to many quantum mechanical properties. However, an analytical solution is only tractable for single-electron systems. Recently, neural networks succeeded at modelling wave functions of many-electron systems. Together with the variational Monte-Carlo (VMC) framework, this led to solutions on par with the best known classical methods. Still, these neural methods require tremendous amounts of computational resources as one has to train a separate model for each molecular geometry. In this work, we combine a Graph Neural Network (GNN) with a neural wave function to simultaneously solve the Schr\u00f6dinger equation for multiple geometries via VMC. This enables us to model continuous subsets of the potential energy surface with a single training pass. Compared to existing state-of-the-art networks, our Potential Energy Surface Network (PESNet) speeds up training for multiple geometries by up to 40 times while matching or surpassing their accuracy. This may open the path to accurate and orders of magnitude cheaper quantum mechanical calculations.",
        "pdf_link": "https://openreview.net/pdf/c55015744159581849683b350d34f68681b90315.pdf",
        "forum_url": "https://openreview.net/forum?id=apv504XsysP",
        "keywords": [
            "potential energy surfaces",
            "wave functions",
            "neural wave functions",
            "energy surface network",
            "neural networks",
            "graph neural network",
            "schr\u00f6dinger equation",
            "pairing gnns",
            "variational monte carlo"
        ]
    },
    {
        "title": "Meta-Learning with Fewer Tasks through Task Interpolation",
        "authors": [
            "Huaxiu Yao",
            "Linjun Zhang",
            "Chelsea Finn"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Meta-learning enables algorithms to quickly learn a newly encountered task with just a few labeled examples by transferring previously learned knowledge. However, the bottleneck of current meta-learning algorithms is the requirement of a large number of meta-training tasks, which may not be accessible in real-world scenarios. To address the challenge that available tasks may not densely sample the space of tasks, we propose to augment the task set through interpolation. By meta-learning with task interpolation (MLTI), our approach effectively generates additional tasks by randomly sampling a pair of tasks and interpolating the corresponding features and labels. Under both gradient-based and metric-based meta-learning settings, our theoretical analysis shows MLTI corresponds to a data-adaptive meta-regularization and further improves the generalization. Empirically, in our experiments on eight datasets from diverse domains including image recognition, pose prediction, molecule property prediction, and medical image classification, we find that the proposed general MLTI framework is compatible with representative meta-learning algorithms and consistently outperforms other state-of-the-art strategies.",
        "pdf_link": "https://openreview.net/pdf/ebbfc5841da414394c96beeba92500546061461a.pdf",
        "forum_url": "https://openreview.net/forum?id=ajXWF7bVR8d",
        "keywords": [
            "meta learning",
            "interpolation",
            "task interpolation"
        ]
    },
    {
        "title": "Geometric and Physical Quantities improve E(3) Equivariant Message Passing",
        "authors": [
            "Johannes Brandstetter",
            "Rob Hesselink",
            "Elise van der Pol",
            "Erik J Bekkers",
            "Max Welling"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Including covariant information, such as position, force, velocity or spin is important in many tasks in computational physics and chemistry. We introduce Steerable E($3$) Equivariant Graph Neural Networks (SEGNNs) that generalise equivariant graph networks, such that node and edge attributes are not restricted to invariant scalars, but can contain covariant information, such as vectors or tensors. Our model, composed of steerable MLPs, is able to incorporate geometric and physical information in both the message and update functions.\nThrough the definition of steerable node attributes, the MLPs provide a new class of activation functions for general use with steerable feature fields. We discuss ours and related work through the lens of equivariant non-linear convolutions, which further allows us to pin-point the successful components of SEGNNs: non-linear message aggregation improves upon classic linear (steerable) point convolutions; steerable messages improve upon recent equivariant graph networks that send invariant messages. We demonstrate the effectiveness of our method on several tasks in computational physics and chemistry and provide extensive ablation studies.",
        "pdf_link": "https://openreview.net/pdf/65770d511d6363ed18177d5f6ad2c21985a7884b.pdf",
        "forum_url": "https://openreview.net/forum?id=_xwr8gOBeV1",
        "keywords": [
            "covariant information",
            "equivariant graph networks",
            "message aggregation",
            "neural networks",
            "passing"
        ]
    },
    {
        "title": "F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization",
        "authors": [
            "Qing Jin",
            "Jian Ren",
            "Richard Zhuang",
            "Sumant Hanumante",
            "Zhengang Li",
            "Zhiyu Chen",
            "Yanzhi Wang",
            "Kaiyuan Yang",
            "Sergey Tulyakov"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Neural network quantization is a promising compression technique to reduce memory footprint and save energy consumption, potentially leading to real-time inference. However, there is a performance gap between quantized and full-precision models. To reduce it, existing quantization approaches require high-precision INT32 or full-precision multiplication during inference for scaling or dequantization. This introduces a noticeable cost in terms of memory, speed, and required energy. To tackle these issues, we present F8Net, a novel quantization framework consisting in only \ufb01xed-point 8-bit multiplication. To derive our method, we \ufb01rst discuss the advantages of \ufb01xed-point multiplication with different formats of \ufb01xed-point numbers and study the statistical behavior of the associated \ufb01xed-point numbers. Second, based on the statistical and algorithmic analysis, we apply different \ufb01xed-point formats for weights and activations of different layers. We introduce a novel algorithm to automatically determine the right format for each layer during training. Third, we analyze a previous quantization algorithm\u2014parameterized clipping activation (PACT)\u2014and reformulate it using \ufb01xed-point arithmetic. Finally, we unify the recently proposed method for quantization \ufb01ne-tuning and our \ufb01xed-point approach to show the potential of our method. We verify F8Net on ImageNet for MobileNet V1/V2 and ResNet18/50. Our approach achieves comparable and better performance, when compared not only to existing quantization techniques with INT32 multiplication or \ufb02oating point arithmetic, but also to the full-precision counterparts, achieving state-of-the-art performance.",
        "pdf_link": "https://openreview.net/pdf/aed69dd0c10990a2c4948e6d230de04c5719fb7d.pdf",
        "forum_url": "https://openreview.net/forum?id=_CfpJazzXT2",
        "keywords": [
            "multiplication",
            "quantized",
            "network quantization",
            "neural network quantization",
            "fixed point arithmetic"
        ]
    },
    {
        "title": "Understanding and Preventing Capacity Loss in Reinforcement Learning",
        "authors": [
            "Clare Lyle",
            "Mark Rowland",
            "Will Dabney"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "The reinforcement learning (RL) problem is rife with sources of non-stationarity that can destabilize or inhibit learning progress.\nWe identify a key mechanism by which this occurs in agents using neural networks as function approximators: \\textit{capacity loss}, whereby networks trained to predict a sequence of target values lose their ability to quickly fit new functions over time.\nWe demonstrate that capacity loss occurs in a broad range of RL agents and environments, and is particularly damaging to learning progress in sparse-reward tasks. We then present a simple regularizer, Initial Feature Regularization (InFeR), that mitigates this phenomenon by regressing a subspace of features towards its value at initialization, improving performance over a state-of-the-art model-free algorithm in the Atari 2600 suite. Finally, we study how this regularization affects different notions of capacity and evaluate other mechanisms by which it may improve performance.",
        "pdf_link": "https://openreview.net/pdf/4d5e54a26ae7558a8d6efeb0bcc7d5f6844aba2a.pdf",
        "forum_url": "https://openreview.net/forum?id=ZkC8wKoLbQ7",
        "keywords": [
            "reinforcement learning",
            "neural networks",
            "regularization",
            "feature regularization",
            "predict",
            "capacity loss"
        ]
    },
    {
        "title": "The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks: an Exact Characterization of Optimal Solutions",
        "authors": [
            "Yifei Wang",
            "Jonathan Lacotte",
            "Mert Pilanci"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "We prove that finding all globally optimal two-layer ReLU neural networks can be performed by solving a convex optimization program with cone constraints. Our analysis is novel, characterizes all optimal solutions, and does not leverage duality-based analysis which was recently used to lift neural network training into convex spaces. Given the set of solutions of our convex optimization program, we show how to construct exactly the entire set of optimal neural networks. We provide a detailed characterization of this optimal set and its invariant transformations. As additional consequences of our convex perspective, (i) we establish that Clarke stationary points found by stochastic gradient descent correspond to the global optimum of a subsampled convex problem (ii) we provide a polynomial-time algorithm for checking if a neural network is a global minimum of the training loss (iii) we provide an explicit construction of a continuous path between any neural network and the global minimum of its sublevel set and (iv) characterize the minimal size of the hidden layer so that the neural network optimization landscape has no spurious valleys.\nOverall, we provide a rich framework for studying the landscape of neural network training loss through convexity.",
        "pdf_link": "https://openreview.net/pdf/9733b1623c23b45535cc2c126e6fb496e55e8049.pdf",
        "forum_url": "https://openreview.net/forum?id=Z7Lk2cQEG8a",
        "keywords": [
            "relu networks",
            "relu neural networks",
            "optimal neural networks",
            "convex optimization",
            "optimization",
            "lift neural network",
            "hidden convex optimization landscape",
            "optimization landscape",
            "regularized",
            "globally optimal",
            "invariant transformations"
        ]
    },
    {
        "title": "Generative Planning for Temporally Coordinated Exploration in Reinforcement Learning",
        "authors": [
            "Haichao Zhang",
            "Wei Xu",
            "Haonan Yu"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Standard model-free reinforcement learning algorithms optimize a policy that generates the action to be taken in the current time step in order to maximize expected future return. While flexible, it faces difficulties arising from the inefficient exploration due to its single step nature. In this work, we present Generative Planning method (GPM), which can generate actions not only for the current step, but also for a number of future steps (thus termed as generative planning). This brings several benefits to GPM. Firstly,  since GPM is trained by maximizing value, the plans generated from it can be regarded as intentional action sequences for reaching high value regions. GPM can therefore leverage its generated multi-step plans for temporally coordinated exploration towards high value regions, which is potentially more effective than a sequence of actions generated by perturbing each action at single step level, whose consistent movement decays exponentially with the number of exploration steps. Secondly, starting from a crude initial plan generator, GPM can refine it to be adaptive to the task, which, in return, benefits future explorations. This is potentially more effective than commonly used action-repeat strategy, which is non-adaptive in its form of plans. Additionally, since the multi-step plan can be interpreted as the intent of the agent from now to a span of time period into the future, it offers a more informative and intuitive signal for interpretation. Experiments are conducted on several benchmark environments and the results demonstrated its effectiveness compared with several baseline methods.",
        "pdf_link": "https://openreview.net/pdf/0e68ff1fa269567c6c6101685f2f721afcc5d0aa.pdf",
        "forum_url": "https://openreview.net/forum?id=YZHES8wIdE",
        "keywords": [
            "temporally coordinated exploration",
            "reinforcement learning",
            "generative planning"
        ]
    },
    {
        "title": "Neural Structured Prediction for Inductive Node Classification",
        "authors": [
            "Meng Qu",
            "Huiyu Cai",
            "Jian Tang"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "This paper studies node classification in the inductive setting, i.e., aiming to learn a model on labeled training graphs and generalize it to infer node labels on unlabeled test graphs. This problem has been extensively studied with graph neural networks (GNNs) by learning effective node representations, as well as traditional structured prediction methods for modeling the structured output of node labels, e.g., conditional random fields (CRFs). In this paper, we present a new approach called the Structured Proxy Network (SPN), which combines the advantages of both worlds. SPN defines flexible potential functions of CRFs with GNNs. However, learning such a model is nontrivial as it involves optimizing a maximin game with high-cost inference. Inspired by the underlying connection between joint and marginal distributions defined by Markov networks, we propose to solve an approximate version of the optimization problem as a proxy, which yields a near-optimal solution, making learning more efficient. Extensive experiments on two settings show that our approach outperforms many competitive baselines.",
        "pdf_link": "https://openreview.net/pdf/df1b628202430dff01a7eeed5b5e5a2e703d1bad.pdf",
        "forum_url": "https://openreview.net/forum?id=YWNAX0caEjI",
        "keywords": [
            "structured proxy network",
            "structured prediction",
            "neural structured prediction",
            "node classification",
            "graph neural networks",
            "inductive node classification",
            "conditional random fields"
        ]
    },
    {
        "title": "Reinforcement Learning with Sparse Rewards using Guidance from Offline Demonstration",
        "authors": [
            "Desik Rengarajan",
            "Gargi Vaidya",
            "Akshay Sarvesh",
            "Dileep Kalathil",
            "Srinivas Shakkottai"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "A major challenge in real-world reinforcement learning (RL) is the sparsity of reward feedback.  Often, what is available is an intuitive but sparse reward function that only indicates whether the task is completed partially or fully.  However, the lack of carefully designed, fine grain feedback implies that most existing RL algorithms fail to learn an acceptable policy in a reasonable time frame.  This is because of the large number of exploration actions that the policy has to perform before it gets any useful feedback that it can learn from.  In this work, we address this challenging problem by developing an algorithm that exploits the offline demonstration data generated by {a sub-optimal behavior policy} for faster and efficient online RL in such sparse reward settings.  The proposed algorithm, which we call the Learning Online with Guidance Offline (LOGO) algorithm, merges a policy improvement step with an additional policy guidance step by using the offline demonstration data.  The key idea is that by obtaining guidance from - not imitating - the offline {data}, LOGO orients its policy in the manner of the sub-optimal {policy}, while yet being able to learn beyond and approach optimality.  We provide a theoretical analysis of our algorithm, and provide a lower bound on the performance improvement in each learning episode.  We also extend our algorithm to the even more challenging incomplete observation setting, where the demonstration data contains only a censored version of the true state observation.  We demonstrate the superior performance of our algorithm over state-of-the-art approaches on a number of  benchmark environments with sparse rewards {and censored state}.  Further, we demonstrate the value of our approach via implementing LOGO on a mobile robot for trajectory tracking and obstacle avoidance, where it shows excellent performance.",
        "pdf_link": "https://openreview.net/pdf/3b163b1af845a3cb05504bfe3d2f3a4a205fe856.pdf",
        "forum_url": "https://openreview.net/forum?id=YJ1WzgMVsMt",
        "keywords": [
            "reinforcement learning",
            "mobile robot",
            "sparse rewards",
            "guidance step",
            "obstacle avoidance",
            "demonstration"
        ]
    },
    {
        "title": "Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement Learning",
        "authors": [
            "Chenjia Bai",
            "Lingxiao Wang",
            "Zhuoran Yang",
            "Zhi-Hong Deng",
            "Animesh Garg",
            "Peng Liu",
            "Zhaoran Wang"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Offline Reinforcement Learning (RL) aims to learn policies from previously collected datasets without exploring the environment. Directly applying off-policy algorithms to offline RL usually fails due to the extrapolation error caused by the out-of-distribution (OOD) actions. Previous methods tackle such problem by penalizing the Q-values of OOD actions or constraining the trained policy to be close to the behavior policy. Nevertheless, such methods typically prevent the generalization of value functions beyond the offline data and also lack precise characterization of OOD data. In this paper, we propose Pessimistic Bootstrapping for offline RL (PBRL), a purely uncertainty-driven offline algorithm without explicit policy constraints. Specifically, PBRL conducts uncertainty quantification via the disagreement of bootstrapped Q-functions, and performs pessimistic updates by penalizing the value function based on the estimated uncertainty. To tackle the extrapolating error, we further propose a novel OOD sampling method. We show that such OOD sampling and pessimistic bootstrapping yields provable uncertainty quantifier in linear MDPs, thus providing the theoretical underpinning for PBRL. Extensive experiments on D4RL benchmark show that PBRL has better performance compared to the state-of-the-art algorithms.",
        "pdf_link": "https://openreview.net/pdf/cbca2d6ef1966129d7e95e65cae5bec0dc19f0ea.pdf",
        "forum_url": "https://openreview.net/forum?id=Y4cs1Z3HnqL",
        "keywords": [
            "bootstrapping",
            "uncertainty",
            "uncertainty quantification",
            "reinforcement learning"
        ]
    },
    {
        "title": "Coordination Among Neural Modules Through a Shared Global Workspace",
        "authors": [
            "Anirudh Goyal",
            "Aniket Rajiv Didolkar",
            "Alex Lamb",
            "Kartikeya Badola",
            "Nan Rosemary Ke",
            "Nasim Rahaman",
            "Jonathan Binas",
            "Charles Blundell",
            "Michael Curtis Mozer",
            "Yoshua Bengio"
        ],
        "published": "ICLR 2022 Oral",
        "summary": " Deep learning has seen a movement away from representing examples with a monolithic hidden state towards a richly structured state. For example, Transformers segment by position, and object-centric architectures decompose images into entities. In all these architectures, interactions between different elements are modeled via pairwise interactions: Transformers make use of self-attention to incorporate information from other positions and object-centric architectures make use of graph neural networks to model interactions among entities.  We consider how to improve on pairwise interactions in terms of global coordination and a coherent, integrated representation that can be used for downstream tasks. In cognitive science, a global workspace architecture has been proposed in which functionally  specialized  components share information through a common, bandwidth-limited communication channel. We explore the use of such a communication channel in the context of deep learning for modeling the structure of complex environments. The proposed method includes a shared workspace through which communication among different specialist modules takes place but due to limits on the communication bandwidth, specialist modules must compete for access. We show that capacity limitations have  a rational basis in that (1) they encourage specialization and compositionality and (2) they facilitate the synchronization of otherwise  independent specialists.\n",
        "pdf_link": "https://openreview.net/pdf/19aac83e8824498df7b9d1e6952523f7c068218b.pdf",
        "forum_url": "https://openreview.net/forum?id=XzTtHjgPDsT",
        "keywords": [
            "deep learning",
            "shared global workspace",
            "neural networks",
            "coordination"
        ]
    },
    {
        "title": "Lossless Compression with Probabilistic Circuits",
        "authors": [
            "Anji Liu",
            "Stephan Mandt",
            "Guy Van den Broeck"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Despite extensive progress on image generation, common deep generative model architectures are not easily applied to lossless compression. For example, VAEs suffer from a compression cost overhead due to their latent variables. This overhead can only be partially eliminated with elaborate schemes such as bits-back coding, often resulting in poor single-sample compression rates. To overcome such problems, we establish a new class of tractable lossless compression models that permit efficient encoding and decoding: Probabilistic Circuits (PCs). These are a class of neural networks involving $|p|$ computational units that support efficient marginalization over arbitrary subsets of the $D$ feature dimensions, enabling efficient arithmetic coding. We derive efficient encoding and decoding schemes that both have time complexity $\\mathcal{O} (\\log(D) \\cdot |p|)$, where a naive scheme would have linear costs in $D$ and $|p|$, making the approach highly scalable. Empirically, our PC-based (de)compression algorithm runs 5-40 times faster than neural compression algorithms that achieve similar bitrates. By scaling up the traditional PC structure learning pipeline, we achieve state-of-the-art results on image datasets such as MNIST. Furthermore, PCs can be naturally integrated with existing neural compression algorithms to improve the performance of these base models on natural image datasets. Our results highlight the potential impact that non-standard learning architectures may have on neural data compression.",
        "pdf_link": "https://openreview.net/pdf/7cccb2cf8c807b3d5eeee9e05f70c8b5ea9ab246.pdf",
        "forum_url": "https://openreview.net/forum?id=X_hByk2-5je",
        "keywords": [
            "probabilistic circuits",
            "lossless compression",
            "compression",
            "decoding",
            "encoding",
            "neural compression",
            "arithmetic coding",
            "neural networks"
        ]
    },
    {
        "title": "EE-Net: Exploitation-Exploration Neural Networks in Contextual Bandits",
        "authors": [
            "Yikun Ban",
            "Yuchen Yan",
            "Arindam Banerjee",
            "Jingrui He"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "In this paper, we propose a novel neural exploration strategy in contextual bandits, EE-Net, distinct from the standard UCB-based and TS-based approaches. Contextual multi-armed bandits have been studied for decades with various applications. To solve the exploitation-exploration tradeoff in bandits, there are three main techniques: epsilon-greedy, Thompson Sampling (TS), and Upper Confidence Bound (UCB). In recent literature, linear contextual bandits have adopted ridge regression to estimate the reward function and combine it with TS or UCB strategies for exploration. However, this line of works explicitly assumes the reward is based on a linear function of arm vectors, which may not be true in real-world datasets. To overcome this challenge, a series of neural bandit algorithms have been proposed, where a neural network is used to learn the underlying reward function and TS or UCB are adapted for exploration. Instead of calculating a large-deviation based statistical bound for exploration like previous methods,  we propose \"EE-Net\", a novel neural-based exploration strategy. In addition to using a neural network (Exploitation network) to learn the reward function, EE-Net uses another neural network (Exploration network) to adaptively learn potential gains compared to the currently estimated reward for exploration. Then, a decision-maker is constructed to combine the outputs from the Exploitation and Exploration networks. We prove that EE-Net can achieve $\\mathcal{O}(\\sqrt{T\\log T})$ regret and show that EE-Net outperforms existing linear and neural contextual bandit baselines on real-world datasets. ",
        "pdf_link": "https://openreview.net/pdf/c6228ff8fe747650e5b549f73f34d2306402b787.pdf",
        "forum_url": "https://openreview.net/forum?id=X_ch3VrNSRg",
        "keywords": [
            "neural network",
            "exploration networks",
            "exploitation exploration neural networks",
            "exploitation network",
            "exploitation exploration",
            "neural bandit algorithms",
            "ridge regression",
            "sampling",
            "confidence bound"
        ]
    },
    {
        "title": "Contrastive Fine-grained Class Clustering via Generative Adversarial Networks",
        "authors": [
            "Yunji Kim",
            "Jung-Woo Ha"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Unsupervised fine-grained class clustering is a practical yet challenging task due to the difficulty of feature representations learning of subtle object details. We introduce C3-GAN, a method that leverages the categorical inference power of InfoGAN with contrastive learning. We aim to learn feature representations that encourage a dataset to form distinct cluster boundaries in the embedding space, while also maximizing the mutual information between the latent code and its image observation. Our approach is to train a discriminator, which is also used for inferring clusters, to optimize the contrastive loss, where image-latent pairs that maximize the mutual information are considered as positive pairs and the rest as negative pairs. Specifically, we map the input of a generator, which was sampled from the categorical distribution, to the embedding space of the discriminator and let them act as a cluster centroid. In this way, C3-GAN succeeded in learning a clustering-friendly embedding space where each cluster is distinctively separable. Experimental results show that C3-GAN achieved the state-of-the-art clustering performance on four fine-grained image datasets, while also alleviating the mode collapse phenomenon. Code is available at https://github.com/naver-ai/c3-gan.",
        "pdf_link": "https://openreview.net/pdf/e0f34f45a561d5470192a2a430af4c5ad2ade5d9.pdf",
        "forum_url": "https://openreview.net/forum?id=XWODe7ZLn8f",
        "keywords": [
            "generative adversarial networks",
            "fine grained class clustering"
        ]
    },
    {
        "title": "Training invariances and the low-rank phenomenon: beyond linear networks",
        "authors": [
            "Thien Le",
            "Stefanie Jegelka"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "The implicit bias induced by the training of neural networks has become a topic of rigorous study. In the limit of gradient flow and gradient descent with appropriate step size, it has been shown that when one trains a deep linear network with logistic or exponential loss on linearly separable data, the weights converge to rank-$1$ matrices. In this paper, we extend this theoretical result to the last few linear layers of the much wider class of nonlinear ReLU-activated feedforward networks containing fully-connected layers and skip connections.  Similar to the linear case, the proof relies on specific local training invariances, sometimes referred to as alignment, which we show to hold for submatrices where neurons are stably-activated in all training examples, and it reflects empirical results in the literature. We also show this is not true in general for the full matrix of ReLU fully-connected layers. Our proof relies on a specific decomposition of the network into a multilinear function and another ReLU network whose weights are constant under a certain parameter directional convergence.",
        "pdf_link": "https://openreview.net/pdf/09cd317c13e66c6829a08a5b598324211771fd4b.pdf",
        "forum_url": "https://openreview.net/forum?id=XEW8CQgArno",
        "keywords": [
            "neural networks",
            "linear network",
            "training",
            "low rank phenomenon"
        ]
    },
    {
        "title": "Learning-Augmented $k$-means Clustering",
        "authors": [
            "Jon C. Ergun",
            "Zhili Feng",
            "Sandeep Silwal",
            "David Woodruff",
            "Samson Zhou"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "$k$-means clustering is a well-studied problem due to its wide applicability. Unfortunately, there exist strong theoretical limits on the performance of any algorithm for the $k$-means problem on worst-case inputs. To overcome this barrier, we consider a scenario where ``advice'' is provided to help perform clustering. Specifically, we consider the $k$-means problem augmented with a predictor that, given any point, returns its cluster label in an approximately optimal clustering up to some, possibly adversarial, error. We present an algorithm whose performance improves along with the accuracy of the predictor, even though na\\\"{i}vely following the accurate predictor can still lead to a high clustering cost. Thus if the predictor is sufficiently accurate, we can retrieve a close to optimal clustering with nearly optimal runtime, breaking known computational barriers for algorithms that do not have access to such advice. We evaluate our algorithms on real datasets and show significant improvements in the quality of clustering.",
        "pdf_link": "https://openreview.net/pdf/aec5563bc92ef6f5f5b441eec312315315b468c9.pdf",
        "forum_url": "https://openreview.net/forum?id=X8cLTHexYyY",
        "keywords": [
            "algorithm",
            "clustering"
        ]
    },
    {
        "title": "Planning in Stochastic Environments with a Learned Model",
        "authors": [
            "Ioannis Antonoglou",
            "Julian Schrittwieser",
            "Sherjil Ozair",
            "Thomas K Hubert",
            "David Silver"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Model-based reinforcement learning has proven highly successful. However, learning a model in isolation from its use during planning is problematic in complex environments. To date, the most effective techniques have instead combined value-equivalent model learning with powerful tree-search methods. This approach is exemplified by MuZero, which has achieved state-of-the-art performance in a wide range of domains, from board games to visually rich environments, with discrete and continuous action spaces, in online and offline settings. However, previous instantiations of this approach were limited to the use of deterministic models. This limits their performance in environments that are inherently stochastic, partially observed, or so large and complex that they appear stochastic to a finite agent. In this paper we extend this approach to learn and plan with stochastic models. Specifically, we introduce a new algorithm, Stochastic MuZero, that learns a stochastic model incorporating afterstates, and uses this model to perform a stochastic tree search. Stochastic MuZero matched or exceeded the state of the art in a set of canonical single and multi-agent environments, including 2048 and backgammon, while maintaining the same performance as standard MuZero in the game of Go.",
        "pdf_link": "https://openreview.net/pdf/f49fc80947707469997960f573102cea38cafb0f.pdf",
        "forum_url": "https://openreview.net/forum?id=X6D9bAHhBQ1",
        "keywords": [
            "planning",
            "reinforcement learning",
            "learn"
        ]
    },
    {
        "title": "Assessing Generalization of SGD via Disagreement",
        "authors": [
            "Yiding Jiang",
            "Vaishnavh Nagarajan",
            "Christina Baek",
            "J Zico Kolter"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "We empirically show that the test error of deep networks can be estimated by training the same architecture on the same training set but with two different runs of Stochastic Gradient Descent (SGD), and then measuring the disagreement rate between the two networks on unlabeled test data. This builds on -- and is a stronger version of -- the observation in Nakkiran&Bansal 20, which requires the runs to be on separate training sets. We further theoretically show that this peculiar phenomenon arises from the well-calibrated nature of ensembles of SGD-trained models. This finding not only provides a simple empirical measure to directly predict the test error using unlabeled test data, but also establishes a new conceptual connection between generalization and calibration.",
        "pdf_link": "https://openreview.net/pdf/c7e974d96a71d3095746891b113427e16a551bb3.pdf",
        "forum_url": "https://openreview.net/forum?id=WvOGCEAQhxl",
        "keywords": [
            "disagreement",
            "test error",
            "generalization",
            "sgd",
            "ensembles of sgd",
            "deep networks"
        ]
    },
    {
        "title": "Sparse Communication via Mixed Distributions",
        "authors": [
            "Ant\u00f3nio Farinhas",
            "Wilker Aziz",
            "Vlad Niculae",
            "Andre Martins"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Neural networks and other machine learning models compute continuous representations, while humans communicate mostly through discrete symbols. Reconciling these two forms of communication is desirable for generating human-readable interpretations or learning discrete latent variable models, while maintaining end-to-end differentiability. Some existing approaches (such as the Gumbel-Softmax transformation) build continuous relaxations that are discrete approximations in the zero-temperature limit, while others (such as sparsemax transformations and the Hard Concrete distribution) produce discrete/continuous hybrids. In this paper, we build rigorous theoretical foundations for these hybrids, which we call \"mixed random variables.'' Our starting point is a new \"direct sum'' base measure defined on the face lattice of the probability simplex. From this measure, we introduce new entropy and Kullback-Leibler divergence functions that subsume the discrete and differential cases and have interpretations in terms of code optimality. Our framework suggests two strategies for representing and sampling mixed random variables, an extrinsic (\"sample-and-project'\u2019) and an intrinsic one (based on face stratification). We experiment with both approaches on an  emergent communication benchmark and on modeling MNIST and Fashion-MNIST data with variational auto-encoders with mixed latent variables.",
        "pdf_link": "https://openreview.net/pdf/f8c966f98befffb0bfbd9af921a4e4dd831d549f.pdf",
        "forum_url": "https://openreview.net/forum?id=WAid50QschI",
        "keywords": [
            "emergent communication",
            "networks"
        ]
    },
    {
        "title": "How to Robustify Black-Box ML Models? A Zeroth-Order Optimization Perspective",
        "authors": [
            "Yimeng Zhang",
            "Yuguang Yao",
            "Jinghan Jia",
            "Jinfeng Yi",
            "Mingyi Hong",
            "Shiyu Chang",
            "Sijia Liu"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "The lack of adversarial robustness has been recognized as an important issue for state-of-the-art machine learning (ML) models, e.g., deep neural networks (DNNs). Thereby, robustifying ML models against adversarial attacks is now a major focus of research. However, nearly all existing defense methods, particularly for robust training, made the white-box assumption that the defender has the access to the details of an ML model (or its surrogate alternatives if available), e.g., its architectures and parameters. Beyond existing works, in this paper we aim to address the problem of black-box defense: How to robustify a black-box model using just input queries and output feedback? Such a problem arises in practical scenarios, where the owner of the predictive model is reluctant to share model information in order to preserve privacy. To this end, we propose a general notion of defensive operation that can be applied to black-box models, and design it through the lens of denoised smoothing (DS), a \ufb01rst-order (FO) certi\ufb01ed defense technique. To allow the design of merely using model queries, we further integrate DS with the zeroth-order (gradient-free) optimization. However, a direct implementation of zeroth-order (ZO) optimization suffers a high variance of gradient estimates, and thus leads to ineffective defense. To tackle this problem, we next propose to prepend an autoencoder (AE) to a given (black-box) model so that DS can be trained using variance-reduced ZO optimization. We term the eventual defense as ZO-AE-DS. In practice, we empirically show that ZO-AE-DS can achieve improved accuracy, certi\ufb01ed robustness, and query complexity over existing baselines. And the effectiveness of our approach is justi\ufb01ed under both image classi\ufb01cation and image reconstruction tasks.",
        "pdf_link": "https://openreview.net/pdf/892f86851a58432caba514751380a19f53458d67.pdf",
        "forum_url": "https://openreview.net/forum?id=W9G_ImpHlQd",
        "keywords": [
            "certified robustness",
            "black box",
            "certified defense technique",
            "optimization",
            "gradient free",
            "first order"
        ]
    },
    {
        "title": "Exploring the Limits of Large Scale Pre-training",
        "authors": [
            "Samira Abnar",
            "Mostafa Dehghani",
            "Behnam Neyshabur",
            "Hanie Sedghi"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Recent developments in large-scale machine learning suggest that by scaling up data, model size and training time properly, one might  observe that improvements in pre-training would transfer favorably to  most downstream tasks. In this work we systematically study this phenomena and establish that, as we increase the upstream accuracy, performance of downstream tasks \\emph{saturates}. In particular, we investigate more than 4800 experiments on Vision Transformers, MLP-Mixers and ResNets with number of parameters ranging from ten million to ten billion, trained on the largest scale of available image data (JFT, ImageNet21K) and evaluated on more than 20 downstream image recognition tasks. We propose a model for downstream performance  that reflects the saturation phenomena and captures the nonlinear relationship in performance of upstream and downstream tasks. Delving deeper to understand the reasons that give rise to these phenomena, we show that the observed saturation behavior is closely related to the way that representations evolve through the layers of the models. We showcase an even more extreme scenario where performance on upstream and downstream are at odds with each other. That is, in order to have a better downstream performance, we need to hurt upstream accuracy.",
        "pdf_link": "https://openreview.net/pdf/a96ab51b85d9ac8937fe9688a023e72c05b91822.pdf",
        "forum_url": "https://openreview.net/forum?id=V3C8p78sDa",
        "keywords": [
            "image recognition",
            "transformers",
            "large scale machine learning",
            "pre training"
        ]
    },
    {
        "title": "MIDI-DDSP: Detailed Control of Musical Performance via Hierarchical Modeling",
        "authors": [
            "Yusong Wu",
            "Ethan Manilow",
            "Yi Deng",
            "Rigel Swavely",
            "Kyle Kastner",
            "Tim Cooijmans",
            "Aaron Courville",
            "Cheng-Zhi Anna Huang",
            "Jesse Engel"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Musical expression requires control of both what notes that are played, and how they are performed. Conventional audio synthesizers provide detailed expressive controls, but at the cost of realism. Black-box neural audio synthesis and concatenative samplers can produce realistic audio, but have few mechanisms for control. In this work, we introduce MIDI-DDSP a hierarchical model of musical instruments that enables both realistic neural audio synthesis and detailed user control. Starting from interpretable Differentiable Digital Signal Processing (DDSP) synthesis parameters, we infer musical notes and high-level properties of their expressive performance (such as timbre, vibrato, dynamics, and articulation). This creates a 3-level hierarchy (notes, performance, synthesis) that affords individuals the option to intervene at each level, or utilize trained priors (performance given notes, synthesis given performance) for creative assistance. Through quantitative experiments and listening tests, we demonstrate that this hierarchy can reconstruct high-fidelity audio, accurately predict performance attributes for a note sequence, independently manipulate the attributes of a given performance,  and as a complete system, generate realistic audio from a novel note sequence. By utilizing an interpretable hierarchy, with multiple levels of granularity, MIDI-DDSP opens the door to assistive tools to empower individuals across a diverse range of musical experience.",
        "pdf_link": "https://openreview.net/pdf/e26b385d95d67af36d02a385047be6f7a0d6f47b.pdf",
        "forum_url": "https://openreview.net/forum?id=UseMOjWENv",
        "keywords": [
            "musical expression",
            "hierarchical modeling",
            "neural audio synthesis",
            "musical performance"
        ]
    },
    {
        "title": "Transform2Act: Learning a Transform-and-Control Policy for Efficient Agent Design",
        "authors": [
            "Ye Yuan",
            "Yuda Song",
            "Zhengyi Luo",
            "Wen Sun",
            "Kris M. Kitani"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "An agent's functionality is largely determined by its design, i.e., skeletal structure and joint attributes (e.g., length, size, strength). However, finding the optimal agent design for a given function is extremely challenging since the problem is inherently combinatorial and the design space is prohibitively large. Additionally, it can be costly to evaluate each candidate design which requires solving for its optimal controller. To tackle these problems, our key idea is to incorporate the design procedure of an agent into its decision-making process. Specifically, we learn a conditional policy that, in an episode, first applies a sequence of transform actions to modify an agent's skeletal structure and joint attributes, and then applies control actions under the new design. To handle a variable number of joints across designs, we use a graph-based policy where each graph node represents a joint and uses message passing with its neighbors to output joint-specific actions. Using policy gradient methods, our approach enables joint optimization of agent design and control as well as experience sharing across different designs, which improves sample efficiency substantially.  Experiments show that our approach, Transform2Act, outperforms prior methods significantly in terms of convergence speed and final performance. Notably, Transform2Act can automatically discover plausible designs similar to giraffes, squids, and spiders. Code and videos are available at https://sites.google.com/view/transform2act.",
        "pdf_link": "https://openreview.net/pdf/511a5c95afacad18125605721a8d1e530c07018b.pdf",
        "forum_url": "https://openreview.net/forum?id=UcDUxjPYWSr",
        "keywords": [
            "control policy"
        ]
    },
    {
        "title": "Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution",
        "authors": [
            "Ananya Kumar",
            "Aditi Raghunathan",
            "Robbie Matthew Jones",
            "Tengyu Ma",
            "Percy Liang"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer---the \"head\"). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (BREEDS-Living17, BREEDS-Entity30, DomainNet, CIFAR $\\to$ STL, CIFAR-10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2% higher accuracy ID but 7% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head---this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1% better ID, 10% better OOD than full fine-tuning).",
        "pdf_link": "https://openreview.net/pdf/5d8a4ae4492042b22b07eabc7a9abcfa517f419c.pdf",
        "forum_url": "https://openreview.net/forum?id=UYneFzXSJWh",
        "keywords": [
            "fine tuning",
            "linear probing",
            "neural network",
            "distribution shift"
        ]
    },
    {
        "title": "EntQA: Entity Linking as Question Answering",
        "authors": [
            "Wenzheng Zhang",
            "Wenyue Hua",
            "Karl Stratos"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "A conventional approach to entity linking is to first find mentions in a given document and then infer their underlying entities in the knowledge base. A well-known limitation of this approach is that it requires finding mentions without knowing their entities, which is unnatural and difficult. We present a new model that does not suffer from this limitation called $\\textbf{EntQA}$, which stands for $\\mbox{\\textbf{Ent}ity}$ linking as $\\mbox{\\textbf{Q}uestion}$ $\\mbox{\\textbf{A}nswering}$. EntQA first proposes candidate entities with a fast retrieval module, and then scrutinizes the document to find mentions of each candidate with a powerful reader module. Our approach combines progress in entity linking with that in open-domain question answering and capitalizes on pretrained models for dense entity retrieval and reading comprehension. Unlike in previous works, we do not rely on a mention-candidates dictionary or large-scale weak supervision. EntQA achieves strong results on the GERBIL benchmarking platform.\n",
        "pdf_link": "https://openreview.net/pdf/644a6b82d054eb9827a94841c9a51dc4ba75e7b4.pdf",
        "forum_url": "https://openreview.net/forum?id=US2rTP5nm_",
        "keywords": [
            "entity linking",
            "as question answering",
            "open domain question answering"
        ]
    },
    {
        "title": "Memorizing Transformers",
        "authors": [
            "Yuhuai Wu",
            "Markus Norman Rabe",
            "DeLesley Hutchins",
            "Christian Szegedy"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights.  \nWe instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate $k$NN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. \nOn benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time.",
        "pdf_link": "https://openreview.net/pdf/33d84d1024126d6a7d4098f2f3beffdbe7057caa.pdf",
        "forum_url": "https://openreview.net/forum?id=TrjbxzRcnf-",
        "keywords": [
            "transformers",
            "finetuned",
            "language models",
            "modeling"
        ]
    },
    {
        "title": "Progressive Distillation for Fast Sampling of Diffusion Models",
        "authors": [
            "Tim Salimans",
            "Jonathan Ho"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Diffusion models have recently shown great promise for generative modeling, outperforming GANs on perceptual quality and autoregressive models at density estimation. A remaining downside is their slow sampling time: generating high quality samples takes many hundreds or thousands of model evaluations. Here we make two contributions to help eliminate this downside: First, we present new parameterizations of diffusion models that provide increased stability when using few sampling steps, compared to models in the literature. Second, we present a method to distill a trained deterministic diffusion sampler, using many steps, into a new diffusion model that takes half as many sampling steps. We then keep progressively applying this distillation procedure to our model, halving the number of required sampling steps each time. On standard image generation benchmarks like CIFAR-10, ImageNet, and LSUN, we start out with (near) state-of-the-art samplers taking 1024 or 8192 steps, and are able to distill down to models taking as little as 4 steps without losing much perceptual quality; achieving, for example, a FID of 3.0 on CIFAR-10 in 4 steps. Finally, we show that the full progressive distillation procedure does not take more time than it takes to train the original model, thus representing an efficient solution for generative modeling using diffusion at both train and test time.",
        "pdf_link": "https://openreview.net/pdf/3b30857a628099896b6123e85d6cf04c59abe77b.pdf",
        "forum_url": "https://openreview.net/forum?id=TIdIXIpzhoI",
        "keywords": [
            "progressive distillation",
            "diffusion",
            "diffusion models",
            "samples"
        ]
    },
    {
        "title": "RotoGrad: Gradient Homogenization in Multitask Learning",
        "authors": [
            "Adri\u00e1n Javaloy",
            "Isabel Valera"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Multitask learning is being increasingly adopted in applications domains like computer vision and reinforcement learning. However, optimally exploiting its advantages remains a major challenge due to the effect of negative transfer. Previous works have tracked down this issue to the disparities in gradient magnitudes and directions across tasks, when optimizing the shared network parameters. While recent work has acknowledged that negative transfer is a two-fold problem, existing approaches fall short as they only focus on either homogenizing the gradient magnitude across tasks; or greedily change the gradient directions, overlooking future conflicts. In this work, we introduce RotoGrad, an algorithm that tackles negative transfer as a whole: it jointly homogenizes gradient magnitudes and directions, while ensuring training convergence. We show that RotoGrad outperforms competing methods in complex problems, including multi-label classification in CelebA and computer vision tasks in the NYUv2 dataset. A Pytorch implementation can be found in https://github.com/adrianjav/rotograd.",
        "pdf_link": "https://openreview.net/pdf/288f1ffa5c44be1b70664610932a5019dd24b6a1.pdf",
        "forum_url": "https://openreview.net/forum?id=T8wHz4rnuGL",
        "keywords": [
            "multitask",
            "gradient homogenization"
        ]
    },
    {
        "title": "Understanding Domain Randomization for Sim-to-real Transfer",
        "authors": [
            "Xiaoyu Chen",
            "Jiachen Hu",
            "Chi Jin",
            "Lihong Li",
            "Liwei Wang"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Reinforcement learning encounters many challenges when applied directly in the real world. Sim-to-real transfer is widely used to transfer the knowledge learned from simulation to the real world. Domain randomization---one of the most popular algorithms for sim-to-real transfer---has been demonstrated to be effective in various tasks in robotics and  autonomous driving. Despite its empirical successes, theoretical understanding on why this simple algorithm works is largely missing. In this paper, we propose a  theoretical framework for sim-to-real transfers, in which the simulator is modeled as a set of MDPs with tunable parameters (corresponding to unknown physical parameters such as friction).  We provide sharp bounds on the sim-to-real gap---the difference between the value of policy returned by domain randomization and the value of an optimal policy for the real world. We prove that sim-to-real transfer can succeed under mild conditions without any real-world training samples. Our theory also highlights the importance of using memory (i.e., history-dependent policies) in domain randomization. Our proof is based on novel techniques that reduce the problem of bounding the sim-to-real gap to the problem of designing efficient learning algorithms for infinite-horizon MDPs, which we believe are of independent interest.",
        "pdf_link": "https://openreview.net/pdf/4e09871da1715277fa6f29c516b944b9e97b0c16.pdf",
        "forum_url": "https://openreview.net/forum?id=T8vZHIRTrY",
        "keywords": [
            "randomization",
            "domain randomization",
            "mdps",
            "reinforcement learning",
            "real transfer"
        ]
    },
    {
        "title": "On the Uncomputability of Partition Functions in Energy-Based Sequence Models",
        "authors": [
            "Chu-Cheng Lin",
            "Arya D. McCarthy"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "In this paper, we argue that energy-based sequence models backed by expressive parametric families can result in uncomputable and inapproximable partition functions. Among other things, this makes model selection--and therefore learning model parameters--not only difficult, but generally _undecidable_. The reason is that there are no good deterministic or randomized estimates of partition functions. Specifically, we exhibit a pathological example where under common assumptions, _no_ useful importance sampling estimates of the partition function can guarantee to have variance bounded below a rational number. As alternatives, we consider sequence model families whose partition functions are computable (if they exist), but at the cost of reduced expressiveness. Our theoretical results suggest that statistical procedures with asymptotic guarantees and sheer (but finite) amounts of compute are not the only things that make sequence modeling work; computability concerns must not be neglected as we consider more expressive model parametrizations.",
        "pdf_link": "https://openreview.net/pdf/886d78756e355f194640b5e7bf0bfbd8482e5623.pdf",
        "forum_url": "https://openreview.net/forum?id=SsPCtEY6yCl",
        "keywords": [
            "uncomputability",
            "partition functions",
            "model selection",
            "energy based sequence models",
            "sequence model families"
        ]
    },
    {
        "title": "Near-Optimal Reward-Free Exploration for Linear Mixture MDPs with Plug-in Solver",
        "authors": [
            "Xiaoyu Chen",
            "Jiachen Hu",
            "Lin Yang",
            "Liwei Wang"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Although model-based reinforcement learning (RL) approaches are considered more sample efficient, existing algorithms are usually relying on sophisticated planning algorithm to couple tightly with the model-learning procedure. Hence the learned models may lack the ability of being re-used with more specialized planners. In this paper we address this issue and provide approaches to learn an RL model efficiently without the guidance of a reward signal. In particular, we take a plug-in solver approach, where we focus on learning a model in the exploration phase and demand that \\emph{any planning algorithm} on the learned model can give a near-optimal policy. Specicially, we focus on the linear mixture MDP setting, where the probability transition matrix is a (unknown) convex combination of a set of existing models. We show that, by establishing a novel exploration algorithm, the plug-in approach learns a model by taking $\\tilde{O}(d^2H^3/\\epsilon^2)$ interactions with the environment and \\emph{any} $\\epsilon$-optimal planner on the model gives an $O(\\epsilon)$-optimal policy on the original model. This sample complexity matches lower bounds for non-plug-in approaches and is \\emph{statistically optimal}. We achieve this result by leveraging a careful maximum total-variance bound using Bernstein inequality and properties specified to linear mixture MDP.",
        "pdf_link": "https://openreview.net/pdf/91c23b5592d358e7283f6fdfe3f0cf6890b65e1c.pdf",
        "forum_url": "https://openreview.net/forum?id=SidzxAb9k30",
        "keywords": [
            "model based reinforcement learning",
            "linear mixture mdps",
            "optimal reward free exploration",
            "plug in solver",
            "inequality"
        ]
    },
    {
        "title": "Looking Back on Learned Experiences  For Class/task Incremental Learning",
        "authors": [
            "Mozhgan PourKeshavarzi",
            "Guoying Zhao",
            "Mohammad Sabokrou"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Classical deep neural networks are limited in their ability to learn from emerging streams of training data. When trained sequentially on new or evolving tasks, their performance degrades sharply, making them inappropriate in real-world use cases. Existing methods tackle it by either storing old data samples or only updating a parameter set of deep neural networks, which, however, demands a large memory budget or spoils the flexibility of models to learn the incremented task distribution. In this paper, we shed light on an on-call transfer set to provide past experiences whenever a new task arises in the data stream. In particular, we propose a Cost-Free Incremental Learning (CF-IL) not only to replay past experiences the model has learned but also to perform this in a cost free manner. Towards this end, we introduced a memory recovery paradigm in which we query the network to synthesize past exemplars whenever a new task emerges. Thus, our method needs no extra memory for data buffering or network growing, besides calls the proposed memory recovery paradigm to provide past exemplars, named a transfer set in order to mitigate catastrophically forgetting the former tasks in the Incremental Learning (IL) setup. Moreover, in contrast with recently proposed methods, the suggested paradigm does not desire a parallel architecture since it only relies on the learner network. Compared to the state-of-the-art data techniques without buffering past data samples, CF-IL demonstrates significantly better performance on the well-known datasets whether a task oracle is available in test time (Task-IL) or not (Class-IL).",
        "pdf_link": "https://openreview.net/pdf/6a7c60c7a81fef15a85782af93ea2e117db6ac6b.pdf",
        "forum_url": "https://openreview.net/forum?id=RxplU3vmBx",
        "keywords": [
            "learned",
            "incremental learning",
            "class task incremental learning"
        ]
    },
    {
        "title": "Provably Filtering Exogenous Distractors using Multistep Inverse Dynamics",
        "authors": [
            "Yonathan Efroni",
            "Dipendra Misra",
            "Akshay Krishnamurthy",
            "Alekh Agarwal",
            "John Langford"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Many real-world applications of reinforcement learning (RL) require the agent to deal with high-dimensional observations such as those generated from a megapixel camera. Prior work has addressed such problems with representation learning, through which the agent can provably extract endogenous, latent state information from raw observations and subsequently plan efficiently. However, such approaches can fail in the presence of temporally correlated noise in the observations, a phenomenon that is common in practice. We initiate the formal study of latent state discovery in the presence of such exogenous noise sources by proposing a new model, the Exogenous Block MDP (EX-BMDP), for rich observation RL. We start by establishing several negative results, by highlighting failure cases of prior representation learning based approaches. Then, we introduce the Predictive Path Elimination (PPE) algorithm, that learns a generalization of inverse dynamics and is provably sample and computationally efficient in EX-BMDPs when the endogenous state dynamics are near deterministic. The sample complexity of PPE depends polynomially on the size of the latent endogenous state space while not directly depending on the size of the observation space, nor the exogenous state space. We provide experiments on challenging exploration problems which show that our approach works empirically. ",
        "pdf_link": "https://openreview.net/pdf/310151127bcaaee206f6987dfe48a6f9a49ae848.pdf",
        "forum_url": "https://openreview.net/forum?id=RQLLzMCefQu",
        "keywords": [
            "distractors",
            "reinforcement learning",
            "predictive path elimination",
            "prior representation learning",
            "inverse",
            "endogenous"
        ]
    },
    {
        "title": "Vision-Based Manipulators Need to Also See from Their Hands",
        "authors": [
            "Kyle Hsu",
            "Moo Jin Kim",
            "Rafael Rafailov",
            "Jiajun Wu",
            "Chelsea Finn"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "We study how the choice of visual perspective affects learning and generalization in the context of physical manipulation from raw sensor observations. Compared with the more commonly used global third-person perspective, a hand-centric (eye-in-hand) perspective affords reduced observability, but we find that it consistently improves training efficiency and out-of-distribution generalization. These benefits hold across a variety of learning algorithms, experimental settings, and distribution shifts, and for both simulated and real robot apparatuses. However, this is only the case when hand-centric observability is sufficient; otherwise, including a third-person perspective is necessary for learning, but also harms out-of-distribution generalization. To mitigate this, we propose to regularize the third-person information stream via a variational information bottleneck. On six representative manipulation tasks with varying hand-centric observability adapted from the Meta-World benchmark, this results in a state-of-the-art reinforcement learning agent operating from both perspectives improving its out-of-distribution generalization on every task. While some practitioners have long put cameras in the hands of robots, our work systematically analyzes the benefits of doing so and provides simple and broadly applicable insights for improving end-to-end learned vision-based robotic manipulation.",
        "pdf_link": "https://openreview.net/pdf/bf5308ad68220347e7cbf2dcbedbf7bb4e0a21b1.pdf",
        "forum_url": "https://openreview.net/forum?id=RJkAHKp7kNZ",
        "keywords": []
    },
    {
        "title": "StyleAlign: Analysis and Applications of Aligned StyleGAN Models",
        "authors": [
            "Zongze Wu",
            "Yotam Nitzan",
            "Eli Shechtman",
            "Dani Lischinski"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "In this paper, we perform an in-depth study of the properties and applications of aligned generative models.\nWe refer to two models as aligned if they share the same architecture, and one of them (the child) is obtained from the other (the parent) via fine-tuning to another domain, a common practice in transfer learning. Several works already utilize some basic properties of aligned StyleGAN models to perform image-to-image translation. Here, we perform the first detailed exploration of model alignment, also focusing on StyleGAN. First, we empirically analyze aligned models and provide answers to important questions regarding their nature. In particular, we find that the child model's latent spaces are semantically aligned with those of the parent, inheriting incredibly rich semantics, even for distant data domains such as human faces and churches. Second, equipped with this better understanding, we leverage aligned models to solve a diverse set of tasks. In addition to image translation, we demonstrate fully automatic cross-domain image morphing. We further show that zero-shot vision tasks may be performed in the child domain, while relying exclusively on supervision in the parent domain. We demonstrate qualitatively and quantitatively that our approach yields state-of-the-art results, while requiring only simple fine-tuning and inversion. ",
        "pdf_link": "https://openreview.net/pdf/a75f48f49713ac38baaaee51cb3273177975f96b.pdf",
        "forum_url": "https://openreview.net/forum?id=Qg2vi4ZbHM9",
        "keywords": [
            "stylegan",
            "aligned stylegan models",
            "aligned models",
            "aligned generative models",
            "image morphing",
            "image translation"
        ]
    },
    {
        "title": "GeoDiff: A Geometric Diffusion Model for Molecular Conformation Generation",
        "authors": [
            "Minkai Xu",
            "Lantao Yu",
            "Yang Song",
            "Chence Shi",
            "Stefano Ermon",
            "Jian Tang"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Predicting molecular conformations from molecular graphs is a fundamental problem in cheminformatics and drug discovery. Recently, significant progress has been achieved with machine learning approaches, especially with deep generative models. Inspired by the diffusion process in classical non-equilibrium thermodynamics where heated particles will diffuse from original states to a noise distribution, in this paper, we propose a novel generative model named GeoDiff for molecular conformation prediction. GeoDiff treats each atom as a particle and learns to directly reverse the diffusion process (i.e., transforming from a noise distribution to stable conformations) as a Markov chain. Modeling such a generation process is however very challenging as the likelihood of conformations should be roto-translational invariant. We theoretically show that Markov chains evolving with equivariant Markov kernels can induce an invariant distribution by design, and further propose building blocks for the Markov kernels to preserve the desirable equivariance property. The whole framework can be efficiently trained in an end-to-end fashion by optimizing a weighted variational lower bound to the (conditional) likelihood. Experiments on multiple benchmarks show that GeoDiff is superior or comparable to existing state-of-the-art approaches, especially on large molecules. ",
        "pdf_link": "https://openreview.net/pdf/d6be0299d7f2d2bf947d450fffe98c8395458c75.pdf",
        "forum_url": "https://openreview.net/forum?id=PzcvxEMzvQC",
        "keywords": [
            "conformations",
            "molecular conformation prediction",
            "geometric diffusion",
            "diffusion",
            "molecular conformation generation",
            "cheminformatics"
        ]
    },
    {
        "title": "Constrained Policy Optimization via Bayesian World Models",
        "authors": [
            "Yarden As",
            "Ilnura Usmanova",
            "Sebastian Curi",
            "Andreas Krause"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Improving sample-efficiency and safety are crucial challenges when deploying reinforcement learning in high-stakes real world applications. We propose LAMBDA, a novel model-based approach for policy optimization in safety critical tasks modeled via constrained Markov decision processes. Our approach utilizes Bayesian world models, and harnesses the resulting uncertainty to maximize optimistic upper bounds on the task objective, as well as pessimistic upper bounds on the safety constraints. We demonstrate LAMBDA's state of the art performance on the Safety-Gym benchmark suite in terms of sample efficiency and constraint violation.",
        "pdf_link": "https://openreview.net/pdf/649d2990399ada19288169dd3031ecbb109a02aa.pdf",
        "forum_url": "https://openreview.net/forum?id=PRZoSmCinhf",
        "keywords": [
            "bayesian world models",
            "policy optimization",
            "constrained policy optimization",
            "reinforcement learning",
            "markov decision processes",
            "sample efficiency"
        ]
    },
    {
        "title": "DR3: Value-Based Deep Reinforcement Learning Requires Explicit Regularization",
        "authors": [
            "Aviral Kumar",
            "Rishabh Agarwal",
            "Tengyu Ma",
            "Aaron Courville",
            "George Tucker",
            "Sergey Levine"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Despite overparameterization, deep networks trained via supervised learning are surprisingly easy to optimize and exhibit excellent generalization. One hypothesis to explain this is that overparameterized deep networks enjoy the benefits of implicit regularization induced by stochastic gradient descent, which favors parsimonious solutions that generalize well on test inputs. It is reasonable to surmise that deep reinforcement learning (RL) methods could also benefit from this effect. In this paper, we discuss how the implicit regularization effect of SGD seen in supervised learning could in fact be harmful in the offline deep RL setting, leading to poor generalization and degenerate feature representations. Our theoretical analysis shows that when existing models of implicit regularization are applied to temporal difference learning, the resulting derived regularizer favors degenerate solutions with excessive aliasing, in stark contrast to the supervised learning case. We back up these findings empirically, showing that feature representations learned by a deep network value function trained via bootstrapping can indeed become degenerate, aliasing the representations for state-action pairs that appear on either side of the Bellman backup. To address this issue, we derive the form of this implicit regularizer and, inspired by this derivation, propose a simple and effective explicit regularizer, called DR3, that counteracts the undesirable effects of this implicit regularizer. When combined with existing offline RL methods, DR3 substantially improves performance and stability, alleviating unlearning in Atari 2600 games, D4RL domains and robotic manipulation from images.",
        "pdf_link": "https://openreview.net/pdf/4682f104a198c9218cf0cdcdbdea5d55d4cf56d8.pdf",
        "forum_url": "https://openreview.net/forum?id=POvMvLi91f",
        "keywords": [
            "regularizer",
            "implicit regularization",
            "optimize",
            "deep reinforcement learning",
            "explicit regularization",
            "temporal difference learning",
            "overparameterization",
            "overparameterized deep networks"
        ]
    },
    {
        "title": "Reinforcement Learning under a Multi-agent Predictive State Representation Model: Method and Theory",
        "authors": [
            "Zhi Zhang",
            "Zhuoran Yang",
            "Han Liu",
            "Pratap Tokekar",
            "Furong Huang"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "We study reinforcement learning for partially observable multi-agent systems where each agent only has access to its own observation and reward and aims to maximize its cumulative rewards. To handle partial observations, we propose graph-assisted predictive state representations (GAPSR), a scalable multi-agent representation learning framework that leverages the agent connectivity graphs to aggregate local representations computed by each agent. In addition, our representations are readily able to incorporate dynamic interaction graphs and kernel space embeddings of the predictive states, and thus have strong flexibility and representation power. \nBased on GAPSR, we propose an end-to-end  MARL algorithm that simultaneously infers the predictive representations and uses the representations as the input of a policy optimization algorithm. Empirically, we demonstrate the efficacy of the proposed algorithm provided on both a MAMuJoCo robotic learning experiment and a multi-agent particle learning environment.",
        "pdf_link": "https://openreview.net/pdf/abd7a0683441b4eb75fb4381e8ac583f2bff2b90.pdf",
        "forum_url": "https://openreview.net/forum?id=PLDOnFoVm4",
        "keywords": [
            "reinforcement learning",
            "predictive state representations",
            "multi agent systems",
            "partially"
        ]
    },
    {
        "title": "VAE Approximation Error: ELBO and Exponential Families",
        "authors": [
            "Alexander Shekhovtsov",
            "Dmitrij Schlesinger",
            "Boris Flach"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "The importance of Variational Autoencoders reaches far beyond standalone generative models -- the approach is also used for learning latent representations and can be generalized to semi-supervised learning. This requires a thorough analysis of their commonly known shortcomings: posterior collapse and approximation errors. This paper analyzes VAE approximation errors caused by the combination of the ELBO objective and encoder models from conditional exponential families, including, but not limited to, commonly used conditionally independent discrete and continuous models.\nWe characterize subclasses of generative models consistent with these encoder families. We show that the ELBO optimizer is pulled away from the likelihood optimizer towards the consistent subset and study this effect experimentally. Importantly, this subset can not be enlarged, and the respective error cannot be decreased, by considering deeper encoder/decoder networks.",
        "pdf_link": "https://openreview.net/pdf/a5374002f74f6bc2b38c0470b1886b02536c628f.pdf",
        "forum_url": "https://openreview.net/forum?id=OIs3SxU5Ynl",
        "keywords": [
            "conditional exponential families",
            "approximation error",
            "vae approximation errors",
            "semi supervised learning",
            "variational"
        ]
    },
    {
        "title": "Learning Pruning-Friendly Networks via Frank-Wolfe: One-Shot, Any-Sparsity, And No Retraining",
        "authors": [
            "Miao Lu",
            "Xiaolong Luo",
            "Tianlong Chen",
            "Wuyang Chen",
            "Dong Liu",
            "Zhangyang Wang"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "We present a novel framework to train a large deep neural network (DNN) for only $\\textit{once}$, which can then be pruned to $\\textit{any sparsity ratio}$ to preserve competitive accuracy $\\textit{without any re-training}$. Conventional methods often require (iterative) pruning followed by re-training, which not only incurs large overhead beyond the original DNN training but also can be sensitive to retraining hyperparameters. Our core idea is to re-cast the DNN training as an explicit $\\textit{pruning-aware}$ process: that is formulated with an auxiliary $K$-sparse polytope constraint, to encourage network weights to lie in a convex hull spanned by $K$-sparse vectors, potentially resulting in more sparse weight matrices. We then leverage a stochastic Frank-Wolfe (SFW) algorithm to solve this new constrained optimization, which naturally leads to sparse weight updates each time. We further note an overlooked fact that existing DNN initializations were derived to enhance SGD training (e.g., avoid gradient explosion or collapse), but was unaligned with the challenges of training with SFW. We hence also present the first learning-based initialization scheme specifically for boosting SFW-based DNN training. Experiments on CIFAR-10 and Tiny-ImageNet datasets demonstrate that our new framework named $\\textbf{SFW-pruning}$ consistently achieves the state-of-the-art performance on various benchmark DNNs over a wide range of pruning ratios. Moreover, SFW-pruning only needs to train once on the same model and dataset, for obtaining arbitrary ratios, while requiring neither iterative pruning nor retraining. All codes will be released to the public. ",
        "pdf_link": "https://openreview.net/pdf/c893710fa491c04dc86547df19635fae45a567c7.pdf",
        "forum_url": "https://openreview.net/forum?id=O1DEtITim__",
        "keywords": [
            "wolfe",
            "sparsity",
            "pruned"
        ]
    },
    {
        "title": "Natural Language Descriptions of Deep Visual Features",
        "authors": [
            "Evan Hernandez",
            "Sarah Schwettmann",
            "David Bau",
            "Teona Bagashvili",
            "Antonio Torralba",
            "Jacob Andreas"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Some neurons in deep networks specialize in recognizing highly specific perceptual, structural, or semantic features of inputs. In computer vision, techniques exist for identifying neurons that respond to individual concept categories like colors, textures, and object classes. But these techniques are limited in scope, labeling only a small subset of neurons and behaviors in any network. Is a richer characterization of neuron-level computation possible? We introduce a procedure (called MILAN, for mutual information-guided linguistic annotation of neurons) that automatically labels neurons with open-ended, compositional, natural language descriptions. Given a neuron, MILAN generates a description by searching for a natural language string that maximizes pointwise mutual information with the image regions in which the neuron is active. MILAN produces fine-grained descriptions that capture categorical, relational, and logical structure in learned features. These descriptions obtain high agreement with human-generated feature descriptions across a diverse set of model architectures and tasks, and can aid in understanding and controlling learned models. We highlight three applications of natural language neuron descriptions. First, we use MILAN for analysis, characterizing the distribution and importance of neurons selective for attribute, category, and relational information in vision models. Second, we use MILAN for auditing, surfacing neurons sensitive to human faces in datasets designed to obscure them. Finally, we use MILAN for editing, improving robustness in an image classifier by deleting neurons sensitive to text features spuriously correlated with class labels.",
        "pdf_link": "https://openreview.net/pdf/842234024e58a8d5073a88b3c04282011b8e20a7.pdf",
        "forum_url": "https://openreview.net/forum?id=NudBMY-tzDr",
        "keywords": [
            "natural language string",
            "natural language neuron",
            "natural language descriptions"
        ]
    },
    {
        "title": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models",
        "authors": [
            "Beidi Chen",
            "Tri Dao",
            "Kaizhao Liang",
            "Jiaming Yang",
            "Zhao Song",
            "Atri Rudra",
            "Christopher Re"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Overparameterized neural networks generalize well but are expensive to train. Ideally one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model components. The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is difficult and expensive. To address this, our main insight is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. As butterfly matrices are not hardware efficient, we propose simple variants of butterfly (block and flat) to take advantage of modern hardware. Our method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically validate that Pixelated Butterfly is $3\\times$ faster than Butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs. On the ImageNet classification and WikiText-103 language modeling tasks, our sparse models train up to 2.3$\\times$ faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 small with no drop in accuracy.",
        "pdf_link": "https://openreview.net/pdf/ee0e47a9502622a5bc9e044424d6f3217c00bdf4.pdf",
        "forum_url": "https://openreview.net/forum?id=Nfl-iXa-y7R",
        "keywords": [
            "sparse models",
            "neural network",
            "sparse model training",
            "sparse training",
            "imagenet classification",
            "butterfly",
            "sparsity"
        ]
    },
    {
        "title": "CycleMLP: A MLP-like Architecture for Dense Prediction",
        "authors": [
            "Shoufa Chen",
            "Enze Xie",
            "Chongjian GE",
            "Runjian Chen",
            "Ding Liang",
            "Ping Luo"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "This paper presents a simple MLP-like architecture, CycleMLP, which is a versatile backbone for visual recognition and dense predictions. As compared to modern MLP architectures, e.g. , MLP-Mixer, ResMLP, and gMLP, whose architectures are correlated to image size and thus are infeasible in object detection and segmentation, CycleMLP has two advantages compared to modern approaches. (1) It can cope\nwith various image sizes. (2) It achieves linear computational complexity to image size by using local windows. In contrast, previous MLPs have $O(N^2)$ computations due to fully spatial connections. We build a family of models which surpass existing MLPs and even state-of-the-art Transformer-based models, e.g. Swin Transformer, while using fewer parameters and FLOPs. We expand the MLP-like models\u2019 applicability, making them a versatile backbone for dense prediction tasks. CycleMLP achieves competitive results on object detection, instance segmentation, and semantic segmentation. In particular, CycleMLP-Tiny outperforms Swin-Tiny by 1.3% mIoU on ADE20K dataset with fewer FLOPs. Moreover, CycleMLP also shows excellent zero-shot robustness on ImageNet-C dataset.",
        "pdf_link": "https://openreview.net/pdf/0ff0f728cbc430b36ea84288793e887e216cff59.pdf",
        "forum_url": "https://openreview.net/forum?id=NMEceG4v69Y",
        "keywords": [
            "dense prediction",
            "cyclemlp",
            "mlp"
        ]
    },
    {
        "title": "Unsupervised Vision-Language Grammar Induction with Shared Structure Modeling",
        "authors": [
            "Bo Wan",
            "Wenjuan Han",
            "Zilong Zheng",
            "Tinne Tuytelaars"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "We introduce a new task, unsupervised vision-language (VL) grammar induction. Given an image-caption pair, the goal is to extract a shared hierarchical structure for both image and language simultaneously.  We argue that such structured output, grounded in both modalities, is a clear step towards the high-level understanding of multimodal information. Besides challenges existing in conventional visually grounded grammar induction tasks, VL grammar induction requires a model to capture contextual semantics and perform a fine-grained alignment. To address these challenges, we propose a novel method, CLIORA, which constructs a shared vision-language constituency tree structure with context-dependent semantics for all possible phrases in different levels of the tree. It computes a matching score between each constituent and image region, trained via contrastive learning.  It integrates two levels of fusion, namely at feature-level and at score-level, so as to allow fine-grained alignment. We introduce a new evaluation metric for VL grammar induction, CCRA, and show a 3.3% improvement over a strong baseline on Flickr30k Entities. We also evaluate our model via two derived tasks, i.e., language grammar induction and phrase grounding, and improve over the state-of-the-art for both.",
        "pdf_link": "https://openreview.net/pdf/5c104842d13e8d6efd55b6d7c04f4373a39eae18.pdf",
        "forum_url": "https://openreview.net/forum?id=N0n_QyQ5lBF",
        "keywords": [
            "grammar induction",
            "language grammar induction",
            "shared structure modeling",
            "shared vision",
            "unsupervised vision language"
        ]
    },
    {
        "title": "On the Optimal Memorization Power of ReLU Neural Networks",
        "authors": [
            "Gal Vardi",
            "Gilad Yehudai",
            "Ohad Shamir"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "We study the memorization power of feedforward ReLU neural networks. We show that such networks can memorize any $N$ points that satisfy a mild separability assumption using $\\tilde{O}\\left(\\sqrt{N}\\right)$ parameters. Known VC-dimension upper bounds imply that memorizing $N$ samples requires $\\Omega(\\sqrt{N})$ parameters, and hence our construction is optimal up to logarithmic factors. We also give a generalized construction for networks with depth bounded by $1 \\leq L \\leq \\sqrt{N}$, for memorizing $N$ samples using $\\tilde{O}(N/L)$ parameters. This bound is also optimal up to logarithmic factors. Our construction uses weights with large bit complexity. We prove that having such a large bit complexity is both necessary and sufficient for memorization with a sub-linear number of parameters.",
        "pdf_link": "https://openreview.net/pdf/6278a5d471f74abf6613e3926eac6ce535669473.pdf",
        "forum_url": "https://openreview.net/forum?id=MkTPtnjeYTV",
        "keywords": [
            "memorize",
            "relu neural networks",
            "feedforward relu neural networks",
            "memorization power",
            "optimal memorization power",
            "separability",
            "vc dimension"
        ]
    },
    {
        "title": "Meta Discovery: Learning to Discover Novel Classes given Very Limited Data",
        "authors": [
            "Haoang Chi",
            "Feng Liu",
            "Wenjing Yang",
            "Long Lan",
            "Tongliang Liu",
            "Bo Han",
            "Gang Niu",
            "Mingyuan Zhou",
            "Masashi Sugiyama"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "In novel class discovery (NCD), we are given labeled data from seen classes and unlabeled data from unseen classes, and we train clustering models for the unseen classes. However, the implicit assumptions behind NCD are still unclear. In this paper, we demystify assumptions behind NCD and find that high-level semantic features should be shared among the seen and unseen classes. Based on this finding, NCD is theoretically solvable under certain assumptions and can be naturally linked to meta-learning that has exactly the same assumption as NCD. Thus, we can empirically solve the NCD problem by meta-learning algorithms after slight modifications. This meta-learning-based methodology significantly reduces the amount of unlabeled data needed for training and makes it more practical, as demonstrated in experiments. The use of very limited data is also justified by the application scenario of NCD: since it is unnatural to label only seen-class data, NCD is sampling instead of labeling in causality. Therefore, unseen-class data should be collected on the way of collecting seen-class data, which is why they are novel and first need to be clustered.",
        "pdf_link": "https://openreview.net/pdf/ce36270eda861ce89f8998343017db1dff96ed19.pdf",
        "forum_url": "https://openreview.net/forum?id=MEpKGLsY8f",
        "keywords": [
            "meta discovery",
            "class discovery",
            "meta learning",
            "very limited data",
            "limited data"
        ]
    },
    {
        "title": "Learning Strides in Convolutional Neural Networks",
        "authors": [
            "Rachid Riad",
            "Olivier Teboul",
            "David Grangier",
            "Neil Zeghidour"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Convolutional neural networks typically contain several downsampling operators, such as strided convolutions or pooling layers, that progressively reduce the resolution of intermediate representations. This provides some shift-invariance while reducing the computational complexity of the whole architecture. A critical hyperparameter of such layers is their stride: the integer factor of downsampling. As strides are not differentiable, finding the best configuration either requires cross-validation or discrete optimization (e.g. architecture search), which rapidly become prohibitive as the search space grows exponentially with the number of downsampling layers. Hence, exploring this search space by gradient descent would allow finding better configurations at a lower computational cost. This work introduces DiffStride, the first downsampling layer with learnable strides. Our layer learns the size of a cropping mask in the Fourier domain, that effectively performs resizing in a differentiable way. Experiments on audio and image classification show the generality and effectiveness of our solution: we use DiffStride as a drop-in replacement to standard downsampling layers and outperform them. In particular, we show that introducing our layer into a ResNet-18 architecture allows keeping consistent high performance on CIFAR10, CIFAR100 and ImageNet even when training starts from poor random stride configurations. Moreover, formulating strides as learnable variables allows us to introduce a regularization term that controls the computational complexity of the architecture. We show how this regularization allows trading off accuracy for efficiency on ImageNet.",
        "pdf_link": "https://openreview.net/pdf/1bc01ea49b5a288387ac5de300847b1d6690d940.pdf",
        "forum_url": "https://openreview.net/forum?id=M752z9FKJP",
        "keywords": [
            "downsampling",
            "convolutional neural networks",
            "strides",
            "learning strides",
            "regularization",
            "resizing"
        ]
    },
    {
        "title": "Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy",
        "authors": [
            "Jiehui Xu",
            "Haixu Wu",
            "Jianmin Wang",
            "Mingsheng Long"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Unsupervised detection of anomaly points in time series is a challenging problem, which requires the model to derive a distinguishable criterion. Previous methods tackle the problem mainly through learning pointwise representation or pairwise association, however, neither is sufficient to reason about the intricate dynamics. Recently, Transformers have shown great power in unified modeling of pointwise representation and pairwise association, and we find that the self-attention weight distribution of each time point can embody rich association with the whole series. Our key observation is that due to the rarity of anomalies, it is extremely difficult to build nontrivial associations from abnormal points to the whole series, thereby, the anomalies' associations shall mainly concentrate on their adjacent time points. This adjacent-concentration bias implies an association-based criterion inherently distinguishable between normal and abnormal points, which we highlight through the Association Discrepancy. Technically, we propose the Anomaly Transformer with a new Anomaly-Attention mechanism to compute the association discrepancy. A minimax strategy is devised to amplify the normal-abnormal distinguishability of the association discrepancy. The Anomaly Transformer achieves state-of-the-art results on six unsupervised time series anomaly detection benchmarks of three applications: service monitoring, space & earth exploration, and water treatment.",
        "pdf_link": "https://openreview.net/pdf/b3974b079de39a5b7e379db64e3fe6b27d3bc07f.pdf",
        "forum_url": "https://openreview.net/forum?id=LzQQ89U1qm_",
        "keywords": [
            "anomaly detection",
            "anomaly transformer",
            "association discrepancy",
            "time series anomaly detection"
        ]
    },
    {
        "title": "When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations",
        "authors": [
            "Xiangning Chen",
            "Cho-Jui Hsieh",
            "Boqing Gong"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Vision Transformers (ViTs) and MLPs signal further efforts on replacing hand-wired features or inductive biases with general-purpose neural architectures. Existing works empower the models by massive data, such as large-scale pre-training and/or repeated strong data augmentations, and still report optimization-related problems (e.g., sensitivity to initialization and learning rates). Hence, this paper investigates ViTs and MLP-Mixers from the lens of loss geometry, intending to improve the models' data efficiency at training and generalization at inference. Visualization and Hessian reveal extremely sharp local minima of converged models. By promoting smoothness with a recently proposed sharpness-aware optimizer, we substantially improve the accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning supervised, adversarial, contrastive, and transfer learning (e.g., +5.3\\% and +11.0\\% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively, with the simple Inception-style preprocessing). We show that the improved smoothness attributes to sparser active neurons in the first few layers. The resultant ViTs outperform ResNets of similar size and throughput when trained from scratch on ImageNet without large-scale pre-training or strong data augmentations. Model checkpoints are available at \\url{https://github.com/google-research/vision_transformer}.",
        "pdf_link": "https://openreview.net/pdf/97b8505eb7034c4bfaee9c7d480a9f605be6fea8.pdf",
        "forum_url": "https://openreview.net/forum?id=LtKcMgGOeLt",
        "keywords": [
            "vision transformers",
            "resnets",
            "massive data",
            "geometry"
        ]
    },
    {
        "title": "Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond",
        "authors": [
            "Chulhee Yun",
            "Shashank Rajput",
            "Suvrit Sra"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "In distributed learning, local SGD (also known as federated averaging) and its simple baseline minibatch SGD are widely studied optimization methods. Most existing analyses of these methods assume independent and unbiased gradient estimates obtained via with-replacement sampling. In contrast, we study shuffling-based variants: minibatch and local Random Reshuffling, which draw stochastic gradients without replacement and are thus closer to practice. For smooth functions satisfying the Polyak-\u0141ojasiewicz condition, we obtain convergence bounds (in the large epoch regime) which show that these shuffling-based variants converge faster than their with-replacement counterparts. Moreover, we prove matching lower bounds showing that our convergence analysis is tight. Finally, we propose an algorithmic modification called synchronized shuffling that leads to convergence rates faster than our lower bounds in near-homogeneous settings.",
        "pdf_link": "https://openreview.net/pdf/1669f6cc32c853b0d69068b7ed1a230ce3f321d0.pdf",
        "forum_url": "https://openreview.net/forum?id=LdlwbBP2mlq",
        "keywords": [
            "shuffling",
            "convergence bounds",
            "synchronized shuffling",
            "federated averaging",
            "distributed learning",
            "minibatch",
            "tight convergence bounds",
            "convergence analysis",
            "epoch"
        ]
    },
    {
        "title": "Iterative Refinement Graph Neural Network for Antibody Sequence-Structure Co-design",
        "authors": [
            "Wengong Jin",
            "Jeremy Wohlwend",
            "Regina Barzilay",
            "Tommi S. Jaakkola"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Antibodies are versatile proteins that bind to pathogens like viruses and stimulate the adaptive immune system. The specificity of antibody binding is determined by complementarity-determining regions (CDRs) at the tips of these Y-shaped proteins. In this paper, we propose a generative model to automatically design the CDRs of antibodies with enhanced binding specificity or neutralization capabilities. Previous generative approaches formulate protein design as a structure-conditioned sequence generation task, assuming the desired 3D structure is given a priori. In contrast, we propose to co-design the sequence and 3D structure of CDRs as graphs. Our model unravels a sequence autoregressively while iteratively refining its predicted global structure. The inferred structure in turn guides subsequent residue choices. For efficiency, we model the conditional dependence between residues inside and outside of a CDR in a coarse-grained manner. Our method achieves superior log-likelihood on the test set and outperforms previous baselines in designing antibodies capable of neutralizing the SARS-CoV-2 virus.\n",
        "pdf_link": "https://openreview.net/pdf/f85e516c2ab137179adf7bda106ae34944694b9d.pdf",
        "forum_url": "https://openreview.net/forum?id=LI2bhrE_2A",
        "keywords": [
            "antibody sequence structure"
        ]
    },
    {
        "title": "Tighter Sparse Approximation Bounds for ReLU Neural Networks",
        "authors": [
            "Carles Domingo-Enrich",
            "Youssef Mroueh"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "A well-known line of work (Barron, 1993; Breiman, 1993; Klusowski & Barron, 2018) provides bounds on the width $n$ of a ReLU two-layer neural network needed to approximate a function $f$ over the ball $\\mathcal{B}_R(\\mathbb{R}^d)$ up to error $\\epsilon$, when the Fourier based quantity $C_f = \\int_{\\mathbb{R}^d} \\|\\xi\\|^2 |\\hat{f}(\\xi)| \\ d\\xi$ is finite. More recently Ongie et al. (2019) used the Radon transform as a tool for analysis of infinite-width ReLU two-layer networks. In particular, they introduce the concept of Radon-based $\\mathcal{R}$-norms and show that a function defined on $\\mathbb{R}^d$ can be represented as an infinite-width two-layer neural network if and only if its $\\mathcal{R}$-norm is finite. In this work, we extend the framework of Ongie et al. (2019) and define similar Radon-based semi-norms ($\\mathcal{R}, \\mathcal{U}$-norms) such that a function admits an infinite-width neural network representation on a bounded open set $\\mathcal{U} \\subseteq \\mathbb{R}^d$ when its $\\mathcal{R}, \\mathcal{U}$-norm is finite. Building on this, we derive sparse (finite-width) neural network approximation bounds that refine those of Breiman (1993); Klusowski & Barron (2018). Finally, we show that infinite-width neural network representations on bounded open sets are not unique and study their structure, providing a functional view of mode connectivity.",
        "pdf_link": "https://openreview.net/pdf/26cbfd341c2fcbea04f1b531de54a26548f1ec2d.pdf",
        "forum_url": "https://openreview.net/forum?id=LBvk4QWIUpm",
        "keywords": [
            "radon transform",
            "networks",
            "approximate",
            "neural network",
            "neural network approximation bounds",
            "width neural network representations",
            "mode connectivity",
            "sparse approximation"
        ]
    },
    {
        "title": "On the Connection between Local Attention and Dynamic Depth-wise Convolution",
        "authors": [
            "Qi Han",
            "Zejia Fan",
            "Qi Dai",
            "Lei Sun",
            "Ming-Ming Cheng",
            "Jiaying Liu",
            "Jingdong Wang"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Vision Transformer (ViT) attains state-of-the-art performance in visual recognition, and the variant, Local Vision Transformer, makes further improvements. The major component in Local Vision Transformer, local attention, performs the attention separately over small local windows. We rephrase local attention as a channel-wise locally-connected layer and analyze it from two network regularization manners, sparse connectivity and weight sharing, as well as dynamic weight computation. We point out that local attention resembles depth-wise convolution and its dynamic variants in sparse connectivity: there is no connection across channels, and each position is connected to the positions within a small local window. The main differences lie in (i) weight sharing - depth-wise convolution shares connection weights (kernel weights) across spatial positions and attention shares the connection weights across channels, and (ii) dynamic weight computation manners - local attention is based on dot-products between pairwise positions in the local window, and dynamic convolution is based on linear projections conducted on the center representation or the globally pooled representation. The connection between local attention and dynamic depth-wise convolution is empirically verified by the ablation study about weight sharing and dynamic weight computation in Local Vision Transformer and (dynamic) depth-wise convolution. We empirically observe that the models based on depth-wise convolution and the dynamic variants with lower computation complexity perform on-par with or slightly better than Swin Transformer, an instance of Local Vision Transformer, for ImageNet classification, COCO object detection and ADE semantic segmentation. Code is available at https://github.com/Atten4Vis/DemystifyLocalViT.",
        "pdf_link": "https://openreview.net/pdf/b5b230d05deb5ca8dcfd87f952bca5621cf5cced.pdf",
        "forum_url": "https://openreview.net/forum?id=L3_SsSNMmy",
        "keywords": [
            "vision transformer",
            "local vision transformer",
            "local attention",
            "dynamic convolution",
            "dynamic",
            "dynamic depth wise convolution",
            "depth wise convolution",
            "imagenet classification"
        ]
    },
    {
        "title": "Learning Altruistic Behaviours in Reinforcement Learning without External Rewards",
        "authors": [
            "Tim Franzmeyer",
            "Mateusz Malinowski",
            "Joao F. Henriques"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Can artificial agents learn to assist others in achieving their goals without knowing what those goals are? Generic reinforcement learning agents could be trained to behave altruistically towards others by rewarding them for altruistic behaviour, i.e., rewarding them for benefiting other agents in a given situation. Such an approach assumes that other agents' goals are known so that the altruistic agent can cooperate in achieving those goals. However, explicit knowledge of other agents' goals is often difficult to acquire. In the case of human agents, their goals and preferences may be difficult to express fully; they might be ambiguous or even contradictory. Thus, it is beneficial to develop agents that do not depend on external supervision and learn altruistic behaviour in a task-agnostic manner. We propose to act altruistically towards other agents by giving them more choice and allowing them to achieve their goals better. Some concrete examples include opening a door for others or safeguarding them to pursue their objectives without interference. We formalize this concept and propose an altruistic agent that learns to increase the choices another agent has by preferring to maximize the number of states that the other agent can reach in its future. We evaluate our approach in three different multi-agent environments where another agent's success depends on altruistic behaviour. Finally, we show that our unsupervised agents can perform comparably to agents explicitly trained to work cooperatively, in some cases even outperforming them.",
        "pdf_link": "https://openreview.net/pdf/1af1674dc962e470709ff0dba09d61b75acd8daa.pdf",
        "forum_url": "https://openreview.net/forum?id=KxbhdyiPHE",
        "keywords": [
            "agent",
            "altruistic agent",
            "reinforcement learning agents",
            "learning altruistic behaviours",
            "reinforcement learning",
            "external rewards"
        ]
    },
    {
        "title": "NASPY: Automated Extraction of Automated Machine Learning Models",
        "authors": [
            "Xiaoxuan Lou",
            "Shangwei Guo",
            "Jiwei Li",
            "Yaoxin Wu",
            "Tianwei Zhang"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "We present NASPY, an end-to-end adversarial framework to extract the networkarchitecture of deep learning models from Neural Architecture Search (NAS). Existing works about model extraction attacks mainly focus on conventional DNN models with very simple operations, or require heavy manual analysis with lots of domain knowledge.  In contrast, NASPY introduces seq2seq models to automatically identify novel and complicated operations (e.g., separable convolution,dilated convolution) from hardware side-channel sequences. We design two models (RNN-CTC and transformer), which can achieve only 3.2% and 11.3% error rates for operation prediction.  We further present methods to recover the model hyper-parameters and topology from the operation sequence .  With these techniques, NASPY is able to extract the complete NAS model architecture with high fidelity and automation, which are rarely analyzed before.",
        "pdf_link": "https://openreview.net/pdf/86d83b714af5dd7905a8be1ae5d3722feeb303ab.pdf",
        "forum_url": "https://openreview.net/forum?id=KhLK0sHMgXK",
        "keywords": [
            "dilated convolution",
            "automated machine learning",
            "neural",
            "extraction"
        ]
    },
    {
        "title": "Comparing Distributions by Measuring Differences that Affect Decision Making",
        "authors": [
            "Shengjia Zhao",
            "Abhishek Sinha",
            "Yutong He",
            "Aidan Perreault",
            "Jiaming Song",
            "Stefano Ermon"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Measuring the discrepancy between two probability distributions is a fundamental problem in machine learning and statistics. We propose a new class of discrepancies based on the optimal loss for a decision task -- two distributions are different if the optimal decision loss is higher on their mixture than on each individual distribution. By suitably choosing the decision task, this generalizes the Jensen-Shannon divergence and the maximum mean discrepancy family. We apply our approach to two-sample tests, and on various benchmarks, we achieve superior test power compared to competing methods. In addition, a modeler can directly specify their preferences when comparing distributions through the decision loss. We apply this property to understanding the effects of climate change on different social and economic activities, evaluating sample quality, and selecting features targeting different decision tasks.",
        "pdf_link": "https://openreview.net/pdf/e99719a7a6796b569cc6afdf6f42024d0df2fbea.pdf",
        "forum_url": "https://openreview.net/forum?id=KB5onONJIAU",
        "keywords": [
            "decision loss",
            "decision making",
            "statistics",
            "probability distributions"
        ]
    },
    {
        "title": "The MultiBERTs: BERT Reproductions for Robustness Analysis",
        "authors": [
            "Thibault Sellam",
            "Steve Yadlowsky",
            "Ian Tenney",
            "Jason Wei",
            "Naomi Saphra",
            "Alexander D'Amour",
            "Tal Linzen",
            "Jasmijn Bastings",
            "Iulia Raluca Turc",
            "Jacob Eisenstein",
            "Dipanjan Das",
            "Ellie Pavlick"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Experiments with pre-trained models such as BERT are often based on a single checkpoint. While the conclusions drawn apply to the artifact tested in the experiment (i.e., the particular instance of the model), it is not always clear whether they hold for the more general procedure which includes the architecture, training data, initialization scheme, and loss function. Recent work has shown that repeating the pre-training process can lead to substantially different performance, suggesting that an alternative strategy is needed to make principled statements about procedures. To enable researchers to draw more robust conclusions, we introduce MultiBERTs, a set of 25 BERT-Base checkpoints, trained with similar hyper-parameters as the original BERT model but differing in random weight initialization and shuffling of training data. We also define the Multi-Bootstrap, a non-parametric bootstrap method for statistical inference designed for settings where there are multiple pre-trained models and limited test data. To illustrate our approach, we present a case study of gender bias in coreference resolution, in which the Multi-Bootstrap lets us measure effects that may not be detected with a single checkpoint. The models and statistical library are available online, along with an additional set of 140 intermediate checkpoints captured during pre-training to facilitate research on learning dynamics.",
        "pdf_link": "https://openreview.net/pdf/2e66adcaa5191446f5006eb5db26da387f29ec18.pdf",
        "forum_url": "https://openreview.net/forum?id=K0E_F0gFDgA",
        "keywords": [
            "bootstrap",
            "multi bootstrap",
            "robust",
            "robustness analysis"
        ]
    },
    {
        "title": "Tackling the Generative Learning Trilemma with Denoising Diffusion GANs",
        "authors": [
            "Zhisheng Xiao",
            "Karsten Kreis",
            "Arash Vahdat"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "A wide variety of deep generative models has been developed in the past decade. Yet, these models often struggle with simultaneously addressing three key requirements including: high sample quality, mode coverage, and fast sampling. We call the challenge imposed by these requirements the generative learning trilemma, as the existing models often trade some of them for others. Particularly, denoising diffusion models have shown impressive sample quality and diversity, but their expensive sampling does not yet allow them to be applied in many real-world applications. In this paper, we argue that slow sampling in these models is fundamentally attributed to the Gaussian assumption in the denoising step which is justified only for small step sizes. To enable denoising with large steps, and hence, to reduce the total number of denoising steps, we propose to model the denoising distribution using a complex multimodal distribution. We introduce denoising diffusion generative adversarial networks (denoising diffusion GANs) that model each denoising step using a multimodal conditional GAN. Through extensive evaluations, we show that denoising diffusion GANs obtain sample quality and diversity competitive with original diffusion models while being 2000$\\times$ faster on the CIFAR-10 dataset. Compared to traditional GANs, our model exhibits better mode coverage and sample diversity. To the best of our knowledge, denoising diffusion GAN is the first model that reduces sampling cost in diffusion models to an extent that allows them to be applied to real-world applications inexpensively.",
        "pdf_link": "https://openreview.net/pdf/6570cfc5a990e77af23d8a4c6b934ac249ba4426.pdf",
        "forum_url": "https://openreview.net/forum?id=JprM0p-q0Co",
        "keywords": [
            "diffusion",
            "denoising diffusion",
            "denoising",
            "denoising diffusion gan",
            "generative learning trilemma",
            "sample"
        ]
    },
    {
        "title": "Path Auxiliary Proposal for MCMC in Discrete Space",
        "authors": [
            "Haoran Sun",
            "Hanjun Dai",
            "Wei Xia",
            "Arun Ramamurthy"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Energy-based Model (EBM) offers a powerful approach for modeling discrete structure, but both inference and learning of EBM are hard as it involves sampling from discrete distributions. Recent work shows Markov Chain Monte Carlo (MCMC) with the informed proposal is a powerful tool for such sampling. However, an informed proposal only allows local updates as it requires evaluating all energy changes in the neighborhood.\nIn this work, we present a path auxiliary algorithm that uses a composition of local moves to efficiently explore large neighborhoods. We also give a fast version of our algorithm that only queries the evaluation of energy function twice for each proposal via linearization of the energy function. Empirically, we show that our path auxiliary algorithms considerably outperform other generic samplers on various discrete models for sampling, inference, and learning. Our method can also be used to train deep EBMs for high-dimensional discrete data.",
        "pdf_link": "https://openreview.net/pdf/03e5897fe57ea115623975dd15807e2738f12501.pdf",
        "forum_url": "https://openreview.net/forum?id=JSR-YDImK95",
        "keywords": [
            "auxiliary",
            "informed proposal",
            "energy based model",
            "path auxiliary proposal",
            "markov chain monte carlo",
            "mcmc"
        ]
    },
    {
        "title": "Universal Approximation Under Constraints is Possible with Transformers",
        "authors": [
            "Anastasis Kratsios",
            "Behnoosh Zamanlooy",
            "Tianlin Liu",
            "Ivan Dokmani\u0107"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Many practical problems need the output of a machine learning model to satisfy a set of constraints, $K$.  Nevertheless, there is no known guarantee that classical neural network architectures can exactly encode constraints while simultaneously achieving universality.  We provide a quantitative constrained universal approximation theorem which guarantees that for any non-convex compact set $K$ and any continuous function $f:\\mathbb{R}^n\\rightarrow K$, there is a probabilistic transformer $\\hat{F}$ whose randomized outputs all lie in $K$ and whose expected output uniformly approximates $f$.  Our second main result is a ``deep neural version'' of Berge's Maximum Theorem (1963).  The result guarantees that given an objective function $L$, a constraint set $K$, and a family of soft constraint sets, there is a probabilistic transformer $\\hat{F}$ that approximately minimizes $L$ and whose outputs belong to $K$; moreover, $\\hat{F}$ approximately satisfies the soft constraints.  Our results imply the first universal approximation theorem for classical transformers with exact convex constraint satisfaction.  They also yield that a chart-free universal approximation theorem for Riemannian manifold-valued functions subject to suitable geodesically convex constraints.",
        "pdf_link": "https://openreview.net/pdf/5b02f8b0bb76868ca513d915646aba6e37d7727e.pdf",
        "forum_url": "https://openreview.net/forum?id=JGO8CvG5S9",
        "keywords": [
            "universal approximation",
            "universal approximation theorem",
            "transformers",
            "soft constraint"
        ]
    },
    {
        "title": "Representation Learning for Online and Offline RL in Low-rank MDPs",
        "authors": [
            "Masatoshi Uehara",
            "Xuezhou Zhang",
            "Wen Sun"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "This work studies the question of Representation Learning in RL: how can we learn a compact low-dimensional representation such that on top of the representation we can perform RL procedures such as exploration and exploitation, in a sample efficient manner. We focus on the low-rank Markov Decision Processes (MDPs) where the transition dynamics correspond to a low-rank transition matrix. Unlike prior works that assume the representation is known (e.g., linear MDPs), here we need to learn the representation for the low-rank MDP. We study both the online RL and offline RL settings. For the online setting, operating with the same computational oracles used in FLAMBE (Agarwal et.al), the state-of-art algorithm for learning representations in low-rank MDPs, we propose an algorithm REP-UCB Upper Confidence Bound driven Representation learning for RL), which significantly improves the sample complexity from $\\widetilde{O}( A^9 d^7 / (\\epsilon^{10} (1-\\gamma)^{22}))$ for FLAMBE to $\\widetilde{O}( A^4 d^4 / (\\epsilon^2 (1-\\gamma)^{2})  )$ with $d$ being the rank of the transition matrix (or dimension of the ground truth representation), $A$ being the number of actions, and $\\gamma$ being the discounted factor. Notably, REP-UCB is simpler than FLAMBE, as it directly balances the interplay between representation learning, exploration, and exploitation, while FLAMBE is an explore-then-commit style approach and has to perform reward-free exploration step-by-step forward in time. For the offline RL setting, we develop an algorithm that leverages pessimism to learn under a partial coverage condition: our algorithm is able to compete against any policy as long as it is covered by the offline distribution.",
        "pdf_link": "https://openreview.net/pdf/5efca5979678883d593d4e2a7fd7c48f0add1015.pdf",
        "forum_url": "https://openreview.net/forum?id=J4iSIR9fhY0",
        "keywords": [
            "sample",
            "offline rl",
            "low rank mdps",
            "mdps",
            "pessimism",
            "online rl",
            "algorithm",
            "markov decision processes",
            "representation learning",
            "exploitation"
        ]
    },
    {
        "title": "Compositional Attention: Disentangling Search and Retrieval",
        "authors": [
            "Sarthak Mittal",
            "Sharath Chandra Raparthy",
            "Irina Rish",
            "Yoshua Bengio",
            "Guillaume Lajoie"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Multi-head, key-value attention is the backbone of transformer-like model architectures which have proven to be widely successful in recent years. This attention mechanism uses multiple parallel key-value attention blocks (called heads), each performing two fundamental computations: (1) search - selection of a relevant entity from a set via query-key interaction, and (2) retrieval - extraction of relevant features from the selected entity via a value matrix. Standard attention heads learn a rigid mapping between search and retrieval. In this work, we first highlight how this static nature of the pairing can potentially: (a) lead to learning of redundant parameters in certain tasks, and (b) hinder generalization. To alleviate this problem, we propose a novel attention mechanism,  called Compositional Attention, that replaces the standard head structure. The proposed mechanism  disentangles search and retrieval and composes them in a dynamic, flexible and context-dependent manner. Through a series of numerical experiments, we show that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings. Through our qualitative analysis, we demonstrate that Compositional Attention leads to dynamic specialization based on the type of retrieval needed. Our proposed mechanism generalizes multi-head attention, allows independent scaling of search and retrieval and is easy to implement in a variety of established network architectures.",
        "pdf_link": "https://openreview.net/pdf/0ac34f592d10eb8cc8a3486322f340c5e0a456ba.pdf",
        "forum_url": "https://openreview.net/forum?id=IwJPj2MBcIa",
        "keywords": [
            "multi head attention",
            "attention",
            "compositional attention",
            "disentangles search"
        ]
    },
    {
        "title": "On Bridging Generic and Personalized Federated Learning for Image Classification",
        "authors": [
            "Hong-You Chen",
            "Wei-Lun Chao"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Federated learning is promising for its capability to collaboratively train models with multiple clients without accessing their data, but vulnerable when clients' data distributions diverge from each other. This divergence further leads to a dilemma: \"Should we prioritize the learned model's generic performance (for future use at the server) or its personalized performance (for each client)?\" These two, seemingly competing goals have divided the community to focus on one or the other, yet in this paper we show that it is possible to approach both at the same time. Concretely, we propose a novel federated learning framework that explicitly decouples a model's dual duties with two prediction tasks. On the one hand, we introduce a family of losses that are robust to non-identical class distributions, enabling clients to train a generic predictor with a consistent objective across them. On the other hand, we formulate the personalized predictor as a lightweight adaptive module that is learned to minimize each client's empirical risk on top of the generic predictor. With this two-loss, two-predictor framework which we name Federated Robust Decoupling (Fed-RoD), the learned model can simultaneously achieve state-of-the-art generic and personalized performance, essentially bridging the two tasks. ",
        "pdf_link": "https://openreview.net/pdf/770f89d1e8fedb42b47cd1ac9c8df3df92f3a178.pdf",
        "forum_url": "https://openreview.net/forum?id=I1hQbx10Kxn",
        "keywords": [
            "federated learning",
            "personalized",
            "personalized federated",
            "image classification"
        ]
    },
    {
        "title": "Churn Reduction via Distillation",
        "authors": [
            "Heinrich Jiang",
            "Harikrishna Narasimhan",
            "Dara Bahri",
            "Andrew Cotter",
            "Afshin Rostamizadeh"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "In real-world systems, models are frequently updated as more data becomes available, and in addition to achieving high accuracy, the goal is to also maintain a low difference in predictions compared to the base model (i.e. predictive churn). If model retraining results in vastly different behavior, then it could cause negative effects in downstream systems, especially if this churn can be avoided with limited impact on model accuracy. In this paper, we show an equivalence between training with distillation using the base model as the teacher and training with an explicit constraint on the predictive churn. We then show that distillation performs strongly for low churn training against a number of recent baselines on a wide range of datasets and model architectures, including fully-connected networks, convolutional networks, and transformers.",
        "pdf_link": "https://openreview.net/pdf/d4c4b0da2bc7b1427e642ebdfc966ac7b142ecd0.pdf",
        "forum_url": "https://openreview.net/forum?id=HbtFCX2PLq0",
        "keywords": [
            "transformers",
            "churn",
            "churn reduction",
            "churn training"
        ]
    },
    {
        "title": "Half-Inverse Gradients for Physical Deep Learning",
        "authors": [
            "Patrick Schnell",
            "Philipp Holl",
            "Nils Thuerey"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Recent works in deep learning have shown that integrating differentiable physics simulators into the training process can greatly improve the quality of results. Although this combination represents a more complex optimization task than usual neural network training, the same gradient-based optimizers are used to minimize the loss function. However, the integrated physics solvers have a profound effect on the gradient flow as manipulating scales in magnitude and direction is an inherent property of many physical processes. Consequently, the gradient flow is often highly unbalanced and creates an environment in which existing gradient-based optimizers perform poorly. In this work, we analyze the characteristics of both physical and neural network optimizations separately to derive a new method based on a half-inversion of the Jacobian. Our approach combines principles of both classical network and physics optimizers to solve the combined optimization task. Compared to state-of-the-art neural network optimizers, our method converges more quickly and to better solutions, which we demonstrate on three complex learning problems involving nonlinear oscillators, the Schroedinger equation and the Poisson problem.",
        "pdf_link": "https://openreview.net/pdf/30d8e38a4f776ba1a0480b58ae7a48fc34a42760.pdf",
        "forum_url": "https://openreview.net/forum?id=HTx7vrlLBEj",
        "keywords": [
            "deep learning",
            "gradient flow",
            "inverse gradients"
        ]
    },
    {
        "title": "Learning the Dynamics of Physical Systems from Sparse Observations with Finite Element Networks",
        "authors": [
            "Marten Lienen",
            "Stephan G\u00fcnnemann"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "We propose a new method for spatio-temporal forecasting on arbitrarily distributed points. Assuming that the observed system follows an unknown partial differential equation, we derive a continuous-time model for the dynamics of the data via the finite element method. The resulting graph neural network estimates the instantaneous effects of the unknown dynamics on each cell in a meshing of the spatial domain. Our model can incorporate prior knowledge via assumptions on the form of the unknown PDE, which induce a structural bias towards learning specific processes. Through this mechanism, we derive a transport variant of our model from the convection equation and show that it improves the transfer performance to higher-resolution meshes on sea surface temperature and gas flow forecasting against baseline models representing a selection of spatio-temporal forecasting methods. A qualitative analysis shows that our model disentangles the data dynamics into their constituent parts, which makes it uniquely interpretable.",
        "pdf_link": "https://openreview.net/pdf/ded4dea1523b16ecaee30b3bb0676b5556db6544.pdf",
        "forum_url": "https://openreview.net/forum?id=HFmAukZ-k-2",
        "keywords": [
            "data dynamics",
            "finite element method",
            "finite element networks",
            "graph neural network",
            "learning the dynamics"
        ]
    },
    {
        "title": "Hybrid Local SGD for Federated Learning with Heterogeneous Communications",
        "authors": [
            "Yuanxiong Guo",
            "Ying Sun",
            "Rui Hu",
            "Yanmin Gong"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Communication is a key bottleneck in federated learning where a large number of edge devices collaboratively learn a model under the orchestration of a central server without sharing their own training data. While local SGD has been proposed to reduce the number of FL rounds and become the algorithm of choice for FL, its total communication cost is still prohibitive when each device needs to communicate with the remote server repeatedly for many times over bandwidth-limited networks. In light of both device-to-device (D2D) and device-to-server (D2S) cooperation opportunities in modern communication networks, this paper proposes a new federated optimization algorithm dubbed hybrid local SGD (HL-SGD) in FL settings where devices are grouped into a set of disjoint clusters with high D2D communication bandwidth. HL-SGD subsumes previous proposed algorithms such as local SGD and gossip SGD and enables us to strike the best balance between model accuracy and runtime. We analyze the convergence of HL-SGD in the presence of heterogeneous data for general nonconvex settings. We also perform extensive experiments and show that the use of hybrid model aggregation via D2D and D2S communications in HL-SGD can largely speed up the training time of federated learning. ",
        "pdf_link": "https://openreview.net/pdf/e765632eb816448e521a3fb0562e39c39651de07.pdf",
        "forum_url": "https://openreview.net/forum?id=H0oaWl6THa",
        "keywords": [
            "federated learning",
            "hybrid model aggregation",
            "sgd",
            "hl sgd",
            "federated optimization",
            "heterogeneous communications"
        ]
    },
    {
        "title": "Independent SE(3)-Equivariant Models for End-to-End Rigid Protein Docking",
        "authors": [
            "Octavian-Eugen Ganea",
            "Xinyuan Huang",
            "Charlotte Bunne",
            "Yatao Bian",
            "Regina Barzilay",
            "Tommi S. Jaakkola",
            "Andreas Krause"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Protein complex formation is a central problem in biology, being involved in most of the cell's processes, and essential for applications, e.g. drug design or protein engineering. We tackle rigid body protein-protein docking, i.e., computationally predicting the 3D structure of a protein-protein complex from the individual unbound structures, assuming no conformational change within the proteins happens during binding. We design a novel pairwise-independent SE(3)-equivariant graph matching network to predict the rotation and translation to place one of the proteins at the right docked position relative to the second protein. We mathematically guarantee a basic principle: the predicted complex is always identical regardless of the initial locations and orientations of the two structures. Our model, named EquiDock, approximates the binding pockets and predicts the docking poses using keypoint matching and alignment, achieved through optimal transport and a differentiable Kabsch algorithm. Empirically, we achieve significant running time improvements and often outperform existing  docking software despite not relying on heavy candidate sampling, structure refinement, or templates.",
        "pdf_link": "https://openreview.net/pdf/4b1b65283103b98ee8f765eb00dca5e4786fcf19.pdf",
        "forum_url": "https://openreview.net/forum?id=GQjaI9mLet",
        "keywords": [
            "keypoint matching",
            "equidock",
            "alignment",
            "docking",
            "predicting"
        ]
    },
    {
        "title": "On Lottery Tickets and Minimal Task Representations in Deep Reinforcement Learning",
        "authors": [
            "Marc Vischer",
            "Robert Tjarko Lange",
            "Henning Sprekeler"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "The lottery ticket hypothesis questions the role of overparameterization in supervised deep learning. But how is the performance of winning lottery tickets affected by the distributional shift inherent to reinforcement learning problems? In this work, we address this question by comparing sparse agents who have to address the non-stationarity of the exploration-exploitation problem with supervised agents trained to imitate an expert. We show that feed-forward networks trained with behavioural cloning compared to reinforcement learning can be pruned to higher levels of sparsity without performance degradation. This suggests that in order to solve the RL-specific distributional shift agents require more degrees of freedom. Using a set of carefully designed baseline conditions, we find that the majority of the lottery ticket effect in both learning paradigms can be attributed to the identified mask rather than the weight initialization. The input layer mask selectively prunes entire input dimensions that turn out to be irrelevant for the task at hand. At a moderate level of sparsity the mask identified by iterative magnitude pruning yields minimal task-relevant representations, i.e., an interpretable inductive bias. Finally, we propose a simple initialization rescaling which promotes the robust identification of sparse task representations in low-dimensional control tasks.",
        "pdf_link": "https://openreview.net/pdf/8e8fd56ca3b46bba9d6db9d68f6fc7df8c828705.pdf",
        "forum_url": "https://openreview.net/forum?id=Fl3Mg_MZR-",
        "keywords": [
            "ticket",
            "lottery tickets",
            "deep learning",
            "pruned"
        ]
    },
    {
        "title": "Amortized Tree Generation for Bottom-up Synthesis Planning and Synthesizable Molecular Design",
        "authors": [
            "Wenhao Gao",
            "Roc\u00edo Mercado",
            "Connor W. Coley"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Molecular design and synthesis planning are two critical steps in the process of molecular discovery that we propose to formulate as a single shared task of conditional synthetic pathway generation. We report an amortized approach to generate synthetic pathways as a Markov decision process conditioned on a target molecular embedding. This approach allows us to conduct synthesis planning in a bottom-up manner and design synthesizable molecules by decoding from optimized conditional codes, demonstrating the potential to solve both problems of design and synthesis simultaneously. The approach leverages neural networks to probabilistically model the synthetic trees, one reaction step at a time, according to reactivity rules encoded in a discrete action space of reaction templates. We train these networks on hundreds of thousands of artificial pathways generated from a pool of purchasable compounds and a list of expert-curated templates. We validate our method with (a) the recovery of molecules using conditional generation, (b) the identification of synthesizable structural analogs, and (c) the optimization of molecular structures given oracle functions relevant to bioactivity and drug discovery.",
        "pdf_link": "https://openreview.net/pdf/9a1a1ce4faf67f9a0f649866c9158c2f0f055db1.pdf",
        "forum_url": "https://openreview.net/forum?id=FRxhHdnxt1",
        "keywords": [
            "synthesis planning",
            "molecular design",
            "bottom up synthesis planning",
            "neural networks",
            "conditional generation",
            "pathways"
        ]
    },
    {
        "title": "Domino: Discovering Systematic Errors with Cross-Modal Embeddings",
        "authors": [
            "Sabri Eyuboglu",
            "Maya Varma",
            "Khaled Kamal Saab",
            "Jean-Benoit Delbrouck",
            "Christopher Lee-Messer",
            "Jared Dunnmon",
            "James Zou",
            "Christopher Re"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Machine learning models that achieve high overall accuracy often make systematic errors on important subsets (or slices) of data. Identifying underperforming slices is particularly challenging when working with high-dimensional inputs (e.g. images, audio), where important slices are often unlabeled. In order to address this issue, recent studies have proposed automated slice discovery methods (SDMs), which leverage learned model representations to mine input data for slices on which a model performs poorly. To be useful to a practitioner, these methods must identify slices that are both underperforming and coherent (i.e. united by a human-understandable concept). However, no quantitative evaluation framework currently exists for rigorously assessing SDMs with respect to these criteria. Additionally, prior qualitative evaluations have shown that SDMs often identify slices that are incoherent. In this work, we address these challenges by first designing a principled evaluation framework that enables a quantitative comparison of SDMs across 1,235 slice discovery settings in three input domains (natural images, medical images, and time-series data).\nThen, motivated by the recent development of powerful cross-modal representation learning approaches, we present Domino, an SDM that leverages cross-modal embeddings and a novel error-aware mixture model to discover and describe coherent slices. We find that Domino accurately identifies 36% of the 1,235 slices in our framework -- a 12 percentage point improvement over prior methods. Further, Domino is the first SDM that can provide natural language descriptions of identified slices, correctly generating the exact name of the slice in 35% of settings. ",
        "pdf_link": "https://openreview.net/pdf/a5ca838a35d810400cfa090453cd85abe02ab6b0.pdf",
        "forum_url": "https://openreview.net/forum?id=FPCMqjI0jXN",
        "keywords": [
            "sdms",
            "cross modal embeddings",
            "errors",
            "cross modal representation learning",
            "error aware mixture",
            "automated slice discovery methods",
            "evaluation"
        ]
    },
    {
        "title": "COptiDICE: Offline Constrained Reinforcement Learning via Stationary Distribution Correction Estimation",
        "authors": [
            "Jongmin Lee",
            "Cosmin Paduraru",
            "Daniel J Mankowitz",
            "Nicolas Heess",
            "Doina Precup",
            "Kee-Eung Kim",
            "Arthur Guez"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "We consider the offline constrained reinforcement learning (RL) problem, in which the agent aims to compute a policy that maximizes expected return while satisfying given cost constraints, learning only from a pre-collected dataset. This problem setting is appealing in many real-world scenarios, where direct interaction with the environment is costly or risky, and where the resulting policy should comply with safety constraints. However, it is challenging to compute a policy that guarantees satisfying the cost constraints in the offline RL setting, since the off-policy evaluation inherently has an estimation error. In this paper, we present an offline constrained RL algorithm that optimizes the policy in the space of the stationary distribution. Our algorithm, COptiDICE, directly estimates the stationary distribution corrections of the optimal policy with respect to returns, while constraining the cost upper bound, with the goal of yielding a cost-conservative policy for actual constraint satisfaction. Experimental results show that COptiDICE attains better policies in terms of constraint satisfaction and return-maximization, outperforming baseline algorithms.",
        "pdf_link": "https://openreview.net/pdf/072227698fafd08a9854a8816e3cc5d5a8eb5754.pdf",
        "forum_url": "https://openreview.net/forum?id=FLA55mBee6Q",
        "keywords": [
            "constrained reinforcement learning",
            "stationary distribution",
            "stationary distribution correction estimation",
            "offline constrained rl"
        ]
    },
    {
        "title": "RelaxLoss: Defending Membership Inference Attacks without Losing Utility",
        "authors": [
            "Dingfan Chen",
            "Ning Yu",
            "Mario Fritz"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "As a long-term threat to the privacy of training data, membership inference attacks (MIAs) emerge ubiquitously in machine learning models.\nExisting works evidence strong connection between the distinguishability of the training and testing loss distributions and the model's vulnerability to MIAs. Motivated by existing results, we propose a novel training framework based on a relaxed loss ($\\textbf{RelaxLoss}$) with a more achievable learning target, which leads to narrowed generalization gap and reduced privacy leakage. RelaxLoss is applicable to any classification model with added benefits of easy implementation and negligible overhead. Through extensive evaluations on five datasets with diverse modalities (images, medical data, transaction records), our approach consistently outperforms state-of-the-art defense mechanisms in terms of resilience against MIAs as well as model utility. Our defense is the first that can withstand a wide range of attacks while preserving (or even improving) the target model's utility.",
        "pdf_link": "https://openreview.net/pdf/e3ac303ac886fc33aba9568f6cb7a74e2c021f00.pdf",
        "forum_url": "https://openreview.net/forum?id=FEDfGWVZYIn",
        "keywords": [
            "membership inference attacks"
        ]
    },
    {
        "title": "Resolving Training Biases via Influence-based Data Relabeling",
        "authors": [
            "Shuming Kong",
            "Yanyan Shen",
            "Linpeng Huang"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "The performance of supervised learning methods easily suffers from the training bias issue caused by train-test distribution mismatch or label noise. Influence function is a  technique that estimates the impacts of a training sample on the model\u2019s predictions. Recent studies on \\emph{data resampling} have employed influence functions to identify \\emph{harmful} training samples that will degrade model's test performance. They have shown that discarding or downweighting the identified harmful training samples is an effective way to resolve training biases. In this work, we move one step forward and propose an influence-based relabeling framework named RDIA for reusing harmful training samples toward better model performance. To achieve this, we use influence functions to estimate how relabeling a training sample would affect model's test performance and further develop a novel relabeling function R. We theoretically prove that applying R to relabel harmful training samples allows the model to achieve lower test loss than simply discarding them for any classification tasks using cross-entropy loss. Extensive experiments on ten real-world datasets demonstrate RDIA outperforms the state-of-the-art data resampling methods and improves model's robustness against label noise. ",
        "pdf_link": "https://openreview.net/pdf/64c51657be7bb5a9efecafe39344c719ccb4d394.pdf",
        "forum_url": "https://openreview.net/forum?id=EskfH0bwNVn",
        "keywords": [
            "affect",
            "bias",
            "test",
            "training sample",
            "training biases",
            "training",
            "influence",
            "supervised learning",
            "label noise",
            "influence function",
            "test distribution mismatch",
            "influence based relabeling",
            "data resampling",
            "data relabeling",
            "classification"
        ]
    },
    {
        "title": "Multi-Stage Episodic Control for Strategic Exploration in Text Games",
        "authors": [
            "Jens Tuyls",
            "Shunyu Yao",
            "Sham M. Kakade",
            "Karthik R Narasimhan"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Text adventure games present unique challenges to reinforcement learning methods due to their combinatorially large action spaces and sparse rewards. The interplay of these two factors is particularly demanding because large action spaces require extensive exploration, while sparse rewards provide limited feedback. This work proposes to tackle the explore-vs-exploit dilemma using a multi-stage approach that explicitly disentangles these two strategies within each episode. Our algorithm, called eXploit-Then-eXplore (XTX), begins each episode using an exploitation policy that imitates a set of promising trajectories from the past, and then switches over to an exploration policy aimed at discovering novel actions that lead to unseen state spaces. This policy decomposition allows us to combine global decisions about which parts of the game space to return to with curiosity-based local exploration in that space, motivated by how a human may approach these games. Our method significantly outperforms prior approaches by 27% and 11% average normalized score over 12 games from the Jericho benchmark (Hausknecht et al., 2020) in both deterministic and stochastic settings, respectively. On the game of Zork1, in particular, XTX obtains a score of 103, more than a 2x improvement over prior methods, and pushes past several known bottlenecks in the game that have plagued previous state-of-the-art methods.",
        "pdf_link": "https://openreview.net/pdf/b67d131e9b9afd599358ce78865538bd83521d24.pdf",
        "forum_url": "https://openreview.net/forum?id=Ek7PSN7Y77z",
        "keywords": [
            "episodic control",
            "exploit then explore",
            "text adventure games",
            "text games",
            "game space",
            "reinforcement learning"
        ]
    },
    {
        "title": "PiCO: Contrastive Label Disambiguation for Partial Label Learning",
        "authors": [
            "Haobo Wang",
            "Ruixuan Xiao",
            "Yixuan Li",
            "Lei Feng",
            "Gang Niu",
            "Gang Chen",
            "Junbo Zhao"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Partial label learning (PLL) is an important problem that allows each training example to be labeled with a coarse candidate set, which well suits many real-world data annotation scenarios with label ambiguity.  Despite the promise, the performance of PLL often lags behind the supervised counterpart. In this work, we bridge the gap by addressing two key research challenges in PLL---representation learning and label disambiguation---in one coherent framework. Specifically, our proposed framework PiCO consists of a contrastive learning module along with a novel class prototype-based label disambiguation algorithm. PiCO produces closely aligned representations for examples from the same classes and facilitates label disambiguation. Theoretically, we show that these two components are mutually beneficial, and can be rigorously justified from an expectation-maximization (EM) algorithm perspective. Extensive experiments demonstrate that PiCO significantly outperforms the current state-of-the-art approaches in PLL and even achieves comparable results to fully supervised learning. Code and data available: https://github.com/hbzju/PiCO.",
        "pdf_link": "https://openreview.net/pdf/f9275b96d741f229db4e61a15ce5f2a499c9ee67.pdf",
        "forum_url": "https://openreview.net/forum?id=EhYjZy6e1gJ",
        "keywords": [
            "label disambiguation",
            "partial label learning",
            "label ambiguity"
        ]
    },
    {
        "title": "Deconstructing the Inductive Biases of Hamiltonian Neural Networks",
        "authors": [
            "Nate Gruver",
            "Marc Anton Finzi",
            "Samuel Don Stanton",
            "Andrew Gordon Wilson"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Physics-inspired neural networks (NNs), such as Hamiltonian or Lagrangian NNs, dramatically outperform other learned dynamics models by leveraging strong inductive biases. These models, however, are challenging to apply to many real world systems, such as those that don\u2019t conserve energy or contain contacts, a common setting for robotics and reinforcement learning. In this paper, we examine the inductive biases that make physics-inspired models successful in practice. We show that, contrary to conventional wisdom, the improved generalization of HNNs is the result of modeling acceleration directly and avoiding artificial complexity from the coordinate system, rather than symplectic structure or energy conservation. We show that by relaxing the inductive biases of these models, we can match or exceed performance on energy-conserving systems while dramatically improving performance on practical, non-conservative systems. We extend this approach to constructing transition models for common Mujoco environments, showing that our model can appropriately balance inductive biases with the flexibility required for model-based control. ",
        "pdf_link": "https://openreview.net/pdf/0c5f008200cc427b5b6c416a2ec60e6adf6df996.pdf",
        "forum_url": "https://openreview.net/forum?id=EDeVYpT42oS",
        "keywords": [
            "inductive biases",
            "hamiltonian neural networks",
            "neural networks",
            "reinforcement learning"
        ]
    },
    {
        "title": "Increasing the Cost of Model Extraction with Calibrated Proof of Work",
        "authors": [
            "Adam Dziedzic",
            "Muhammad Ahmad Kaleem",
            "Yu Shen Lu",
            "Nicolas Papernot"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "In model extraction attacks, adversaries can steal a machine learning model exposed via a public API by repeatedly querying it and adjusting their own model based on obtained predictions. To prevent model stealing, existing defenses focus on detecting malicious queries, truncating, or distorting outputs, thus necessarily introducing a tradeoff between robustness and model utility for legitimate users. Instead, we propose to impede model extraction by requiring users to complete a proof-of-work before they can read the model's predictions. This deters attackers by greatly increasing (even up to 100x) the computational effort needed to leverage query access for model extraction. Since we calibrate the effort required to complete the proof-of-work to each query, this only introduces a slight overhead for regular users (up to 2x). To achieve this, our calibration applies tools from differential privacy to measure the information revealed by a query. Our method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen.",
        "pdf_link": "https://openreview.net/pdf/c75fb6727a40e11e23ec143778cd6e178a0681f3.pdf",
        "forum_url": "https://openreview.net/forum?id=EAy7C1cgE1L",
        "keywords": [
            "calibrate",
            "model extraction",
            "calibrated proof of work",
            "differential privacy",
            "model stealing",
            "measure"
        ]
    },
    {
        "title": "Latent Variable Sequential Set Transformers for Joint Multi-Agent Motion Prediction",
        "authors": [
            "Roger Girgis",
            "Florian Golemo",
            "Felipe Codevilla",
            "Martin Weiss",
            "Jim Aldon D'Souza",
            "Samira Ebrahimi Kahou",
            "Felix Heide",
            "Christopher Pal"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Robust multi-agent trajectory prediction is essential for the safe control of robotic systems. A major challenge is to efficiently learn a representation that approximates the true joint distribution of contextual, social, and temporal information to enable planning. We propose Latent Variable Sequential Set Transformers which are encoder-decoder architectures that generate scene-consistent multi-agent trajectories. We refer to these architectures as \u201cAutoBots\u201d. The encoder is a stack of interleaved temporal and social multi-head self-attention (MHSA) modules which alternately perform equivariant processing across the temporal and social dimensions. The decoder employs learnable seed parameters in combination with temporal and social MHSA modules allowing it to perform inference over the\nentire future scene in a single forward pass efficiently. AutoBots can produce either the trajectory of one ego-agent or a distribution over the future trajectories for all agents in the scene. For the single-agent prediction case, our model achieves top results on the global nuScenes vehicle motion prediction leaderboard, and produces strong results on the Argoverse vehicle prediction challenge. In the multi-agent setting, we evaluate on the synthetic partition of TrajNet++ dataset to showcase the model\u2019s socially-consistent predictions. We also demonstrate our model on general sequences of sets and provide illustrative experiments modelling the sequential structure of the multiple strokes that make up symbols in the Omniglot data. A distinguishing feature of AutoBots is that all models are trainable on a\nsingle desktop GPU (1080 Ti) in under 48h.",
        "pdf_link": "https://openreview.net/pdf/1ab1260f39e79ac98b52759c8221374f595af7aa.pdf",
        "forum_url": "https://openreview.net/forum?id=Dup_dDqkZC5",
        "keywords": [
            "encoder",
            "trajectory prediction"
        ]
    },
    {
        "title": "Omni-Dimensional Dynamic Convolution",
        "authors": [
            "Chao Li",
            "Aojun Zhou",
            "Anbang Yao"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Learning a single static convolutional kernel in each convolutional layer is the common training paradigm of modern Convolutional Neural Networks (CNNs). Instead, recent research in dynamic convolution shows that learning a linear combination of n convolutional kernels weighted with their input-dependent attentions can significantly improve the accuracy of light-weight CNNs, while maintaining efficient inference. However, we observe that existing works endow convolutional kernels with the dynamic property through one dimension (regarding the convolutional kernel number) of the kernel space, but the other three dimensions (regarding the spatial size, the input channel number and the output channel number for each convolutional kernel) are overlooked. Inspired by this, we present Omni-dimensional Dynamic Convolution (ODConv), a more generalized yet elegant dynamic convolution design, to advance this line of research. ODConv leverages a novel multi-dimensional attention mechanism with a parallel strategy to learn complementary attentions for convolutional kernels along all four dimensions of the kernel space at any convolutional layer. As a drop-in replacement of regular convolutions, ODConv can be plugged into many CNN architectures. Extensive experiments on the ImageNet and MS-COCO datasets show that ODConv brings solid accuracy boosts for various prevailing CNN backbones including both light-weight and large ones, e.g., 3.77%~5.71%|1.86%~3.72% absolute top-1 improvements to MobivleNetV2|ResNet family on the ImageNet dataset. Intriguingly, thanks to its improved feature learning ability, ODConv with even one single kernel can compete with or outperform existing dynamic convolution counterparts with multiple kernels, substantially reducing extra parameters. Furthermore, ODConv is also superior to other attention modules for modulating the output features or the convolutional weights. Code and models will be available at https://github.com/OSVAI/ODConv.",
        "pdf_link": "https://openreview.net/pdf/7b2dd41d0729d79f0f22fac00e8ac757b46ff5a9.pdf",
        "forum_url": "https://openreview.net/forum?id=DmpCfq6Mg39",
        "keywords": [
            "convolutions",
            "convolutional kernels",
            "convolutional neural networks",
            "dynamic convolution",
            "imagenet",
            "convolutional weights",
            "convolutional layer"
        ]
    },
    {
        "title": "A Fine-Grained Analysis on Distribution Shift",
        "authors": [
            "Olivia Wiles",
            "Sven Gowal",
            "Florian Stimberg",
            "Sylvestre-Alvise Rebuffi",
            "Ira Ktena",
            "Krishnamurthy Dj Dvijotham",
            "Ali Taylan Cemgil"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Robustness to distribution shifts is critical for deploying machine learning models in the real world. Despite this necessity, there has been little work in defining the underlying mechanisms that cause these shifts and evaluating the robustness of algorithms across multiple, different distribution shifts. To this end, we introduce a framework that enables fine-grained analysis of various distribution shifts. We provide a holistic analysis of current state-of-the-art methods by evaluating 19 distinct methods grouped into five categories across both synthetic and real-world datasets.  Overall, we train more than 85K models. Our experimental framework can be easily extended to include new methods, shifts, and datasets. We find, unlike previous work (Gulrajani & Lopez-Paz, 2021), that progress has been made over a standard ERM baseline; in particular, pretraining and augmentations (learned or heuristic) offer large gains in many cases. However, the best methods are not consistent over different datasets and shifts. We will open source our experimental framework, allowing future work to evaluate new methods over multiple shifts to obtain a more complete picture of a method's effectiveness. \nCode is available at github.com/deepmind/distribution_shift_framework.\n",
        "pdf_link": "https://openreview.net/pdf/6be366539738706234ad0b104ed82361a3c5f6e7.pdf",
        "forum_url": "https://openreview.net/forum?id=Dl4LetuLdyK",
        "keywords": [
            "distribution shift",
            "robustness"
        ]
    },
    {
        "title": "Interpretable Unsupervised Diversity Denoising and Artefact Removal",
        "authors": [
            "Mangal Prakash",
            "Mauricio Delbracio",
            "Peyman Milanfar",
            "Florian Jug"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Image denoising and artefact removal are complex inverse problems admitting multiple valid solutions. Unsupervised diversity restoration, that is, obtaining a diverse set of possible restorations given a corrupted image, is important for ambiguity removal in many applications such as microscopy where paired data for supervised training are often unobtainable. In real world applications, imaging noise and artefacts are typically hard to model, leading to unsatisfactory performance of existing unsupervised approaches. This work presents an interpretable approach for unsupervised and diverse image restoration. To this end, we introduce a capable architecture called Hierarchical DivNoising (HDN) based on hierarchical Variational Autoencoder. We show that HDN learns an interpretable multi-scale representation of artefacts  and we leverage this interpretability to remove imaging artefacts commonly occurring in microscopy data. Our method achieves state-of-the-art results on twelve benchmark image denoising datasets while providing access to a whole distribution of sensibly restored solutions.\nAdditionally, we demonstrate on three real microscopy datasets that HDN removes artefacts without supervision, being the first method capable of doing so while generating multiple plausible restorations all consistent with the given corrupted image.",
        "pdf_link": "https://openreview.net/pdf/3fb037cb33af2bbe50cc241272e4f9313aaf3552.pdf",
        "forum_url": "https://openreview.net/forum?id=DfMqlB0PXjM",
        "keywords": [
            "artefact removal",
            "image denoising",
            "diversity denoising",
            "ambiguity removal",
            "diverse",
            "divnoising"
        ]
    },
    {
        "title": "Learning meta-features for AutoML",
        "authors": [
            "Herilalaina Rakotoarison",
            "Louisot Milijaona",
            "Andry RASOANAIVO",
            "Michele Sebag",
            "Marc Schoenauer"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "This paper tackles the AutoML problem, aimed to automatically select an ML algorithm and its hyper-parameter configuration most appropriate to the dataset at hand. The proposed approach, MetaBu, learns new meta-features via an Optimal Transport procedure, aligning the manually designed \\mf s with the space of distributions on the hyper-parameter configurations. MetaBu meta-features, learned once and for all, induce a topology on the set of datasets that is exploited to define a distribution of promising hyper-parameter configurations amenable to AutoML. Experiments on the OpenML CC-18 benchmark demonstrate that using MetaBu meta-features boosts the performance of state of the art AutoML systems, AutoSklearn (Feurer et al. 2015) and Probabilistic Matrix Factorization (Fusi et al. 2018). Furthermore, the inspection of MetaBu meta-features gives some hints into when an ML algorithm does well. Finally, the topology based on MetaBu meta-features enables to estimate the intrinsic dimensionality of the OpenML benchmark w.r.t. a given ML algorithm or pipeline. The source code is available at https://github.com/luxusg1/metabu.",
        "pdf_link": "https://openreview.net/pdf/2c85e44817e8f27a5434ef26f8089b6dbaec6dab.pdf",
        "forum_url": "https://openreview.net/forum?id=DTkEfj0Ygb8",
        "keywords": [
            "automl",
            "metabu",
            "openml",
            "metabu meta features",
            "ml algorithm"
        ]
    },
    {
        "title": "On the Importance of Firth Bias Reduction in Few-Shot Classification",
        "authors": [
            "Saba Ghaffari",
            "Ehsan Saleh",
            "David Forsyth",
            "Yu-Xiong Wang"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Learning accurate classifiers for novel categories from very few examples, known as few-shot image classification, is a challenging task in statistical machine learning and computer vision. The performance in few-shot classification suffers from the bias in the estimation of classifier parameters; however, an effective underlying bias reduction technique that could alleviate this issue in training few-shot classifiers has been overlooked. In this work, we demonstrate the effectiveness of Firth bias reduction in few-shot classification. Theoretically, Firth bias reduction removes the $O(N^{-1})$ first order term from the small-sample bias of the Maximum Likelihood Estimator. Here we show that the general Firth bias reduction technique simplifies to encouraging uniform class assignment probabilities for multinomial logistic classification, and almost has the same effect in cosine classifiers. We derive an easy-to-implement optimization objective for Firth penalized multinomial logistic and cosine classifiers, which is equivalent to penalizing the cross-entropy loss with a KL-divergence between the predictions and the uniform label distribution. Then, we empirically evaluate that it is consistently effective across the board for few-shot image classification, regardless of (1) the feature representations from different backbones, (2) the number of samples per class, and (3) the number of classes. Furthermore, we demonstrate the effectiveness of Firth bias reduction on cross-domain and imbalanced data settings. Our implementation is available at https://github.com/ehsansaleh/firth_bias_reduction.",
        "pdf_link": "https://openreview.net/pdf/fafc00da00fec3e0c8db049a4e2e14a588fa4aa7.pdf",
        "forum_url": "https://openreview.net/forum?id=DNRADop4ksB",
        "keywords": [
            "firth bias",
            "firth bias reduction",
            "few shot image classification",
            "cosine classifiers",
            "cross entropy loss",
            "imbalanced data"
        ]
    },
    {
        "title": "Controlling Directions Orthogonal to a Classifier",
        "authors": [
            "Yilun Xu",
            "Hao He",
            "Tianxiao Shen",
            "Tommi S. Jaakkola"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "We propose to identify directions invariant to a given classifier so that these directions can be controlled in tasks such as style transfer. While orthogonal decomposition is directly identifiable when the given classifier is linear, we formally define a notion of orthogonality in the non-linear case. We also provide a surprisingly simple method for constructing the orthogonal classifier (a classifier utilizing directions other than those of the given classifier). Empirically, we present three use cases where controlling orthogonal variation is important: style transfer, domain adaptation, and fairness. The orthogonal classifier enables desired style transfer when domains vary in multiple aspects, improves domain adaptation with label shifts and mitigates the unfairness as a predictor. The code is available at https://github.com/Newbeeer/orthogonal_classifier",
        "pdf_link": "https://openreview.net/pdf/259173e97dc2f0da3f9b4879faaede603f13d98a.pdf",
        "forum_url": "https://openreview.net/forum?id=DIjCrlsu6Z",
        "keywords": [
            "domain adaptation",
            "orthogonal classifier",
            "classifier",
            "controlling directions orthogonal to",
            "fairness"
        ]
    },
    {
        "title": "How Do Vision Transformers Work?",
        "authors": [
            "Namuk Park",
            "Songkuk Kim"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "The success of multi-head self-attentions (MSAs) for computer vision is now indisputable. However, little is known about how MSAs work. We present fundamental explanations to help better understand the nature of MSAs. In particular, we demonstrate the following properties of MSAs and Vision Transformers (ViTs): (1) MSAs improve not only accuracy but also generalization by flattening the loss landscapes. Such improvement is primarily attributable to their data specificity, not long-range dependency. On the other hand, ViTs suffer from non-convex losses. Large datasets and loss landscape smoothing methods alleviate this problem; (2) MSAs and Convs exhibit opposite behaviors. For example, MSAs are low-pass filters, but Convs are high-pass filters. Therefore, MSAs and Convs are complementary; (3) Multi-stage neural networks behave like a series connection of small individual models. In addition, MSAs at the end of a stage play a key role in prediction. Based on these insights, we propose AlterNet, a model in which Conv blocks at the end of a stage are replaced with MSA blocks. AlterNet outperforms CNNs not only in large data regimes but also in small data regimes. The code is available at https://github.com/xxxnell/how-do-vits-work.",
        "pdf_link": "https://openreview.net/pdf/e293cbd49c33a4924e5b45932a342361dd9845cf.pdf",
        "forum_url": "https://openreview.net/forum?id=D78Go4hVcxO",
        "keywords": [
            "vision transformers",
            "computer vision",
            "head self attentions",
            "series connection",
            "neural networks"
        ]
    },
    {
        "title": "On Improving Adversarial Transferability of Vision Transformers ",
        "authors": [
            "Muzammal Naseer",
            "Kanchana Ranasinghe",
            "Salman Khan",
            "Fahad Khan",
            "Fatih Porikli"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Vision transformers (ViTs) process input images as sequences of patches via self-attention; a radically different architecture than convolutional neural networks (CNNs).  This makes it interesting to study the adversarial feature space of ViT models and their transferability. In particular, we observe that adversarial patterns found via conventional adversarial attacks show very \\emph{low} black-box transferability even for large ViT models. We show that this phenomenon is only due to the sub-optimal attack procedures that do not leverage the true representation potential of ViTs. A deep ViT is composed of multiple blocks, with a consistent architecture comprising of self-attention and feed-forward layers, where each block is capable of independently producing a class token. Formulating an attack using only the last class token (conventional approach) does not directly leverage the discriminative information stored in the earlier tokens, leading to poor adversarial transferability of ViTs.Using the compositional nature of ViT models, we enhance transferability of existing attacks by introducing two novel strategies specific to the architecture of ViT models.  \\emph{(i) Self-Ensemble:} We propose a method to find multiple discriminative pathways by dissecting a single ViT model into an ensemble of networks. This allows explicitly utilizing class-specific information at each ViT block. \\emph{(ii) Token Refinement:} We then propose to refine the tokens to further enhance the discriminative capacity at each block of ViT.Our token refinement systematically combines the class tokens with structural information preserved within the patch tokens. An adversarial attack when applied to such refined tokens within the ensemble of classifiers found in a single vision transformer has significantly higher transferability and thereby brings out the true generalization potential of the ViT's adversarial space. Code: https://t.ly/hBbW.",
        "pdf_link": "https://openreview.net/pdf/b40e4df19f3e58593a885adc8809af1ba9864da8.pdf",
        "forum_url": "https://openreview.net/forum?id=D6nH3719vZy",
        "keywords": [
            "transferability",
            "adversarial transferability",
            "convolutional neural networks",
            "vision transformers"
        ]
    },
    {
        "title": "Score-Based Generative Modeling with Critically-Damped Langevin Diffusion",
        "authors": [
            "Tim Dockhorn",
            "Arash Vahdat",
            "Karsten Kreis"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Score-based generative models (SGMs) have demonstrated remarkable synthesis quality. SGMs rely on a diffusion process that gradually perturbs the data towards a tractable distribution, while the generative model learns to denoise. The complexity of this denoising task is, apart from the data distribution itself, uniquely determined by the diffusion process. We argue that current SGMs employ overly simplistic diffusions, leading to unnecessarily complex denoising processes, which limit generative modeling performance. Based on connections to statistical mechanics, we propose a novel critically-damped Langevin diffusion (CLD) and show that CLD-based SGMs achieve superior performance. CLD can be interpreted as running a joint diffusion in an extended space, where the auxiliary variables can be considered \"velocities\" that are coupled to the data variables as in Hamiltonian dynamics. We derive a novel score matching objective for CLD and show that the model only needs to learn the score function of the conditional distribution of the velocity given data, an easier task than learning scores of the data directly. We also derive a new sampling scheme for efficient synthesis from CLD-based diffusion models. We find that CLD outperforms previous SGMs in synthesis quality for similar network architectures and sampling compute budgets. We show that our novel sampler for CLD significantly outperforms solvers such as Euler\u2013Maruyama. Our framework provides new insights into score-based denoising diffusion models and can be readily used for high-resolution image synthesis. Project page and code: https://nv-tlabs.github.io/CLD-SGM.",
        "pdf_link": "https://openreview.net/pdf/b0abf219a709e60170ee3d79e0068b682e8696dd.pdf",
        "forum_url": "https://openreview.net/forum?id=CzceR82CYc",
        "keywords": [
            "critically damped langevin diffusion",
            "score matching",
            "denoising diffusion",
            "generative modeling",
            "hamiltonian dynamics",
            "score function"
        ]
    },
    {
        "title": "Transition to Linearity of Wide Neural Networks is an Emerging Property of Assembling Weak Models",
        "authors": [
            "Chaoyue Liu",
            "Libin Zhu",
            "Misha Belkin"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Wide neural networks with linear output layer have been shown to be near-linear, and to have near-constant neural tangent kernel (NTK), in a region containing the optimization path of gradient descent. These findings seem counter-intuitive since in general neural networks are highly complex models. Why does a linear structure emerge when the neural networks become wide? \nIn this work, we provide a new perspective on this \"transition to linearity\" by considering a neural network as an assembly model recursively built from a set of sub-models corresponding to individual neurons. In this view, we show that the linearity of wide neural networks is, in fact, an emerging property of assembling a large number of diverse ``weak'' sub-models, none of which dominate the assembly. ",
        "pdf_link": "https://openreview.net/pdf/d4e2a924c1b35ff107a22198b053db66c9f1ae3d.pdf",
        "forum_url": "https://openreview.net/forum?id=CyKHoKyvgnp",
        "keywords": [
            "wide neural networks",
            "neural",
            "neural networks"
        ]
    },
    {
        "title": "Scarf: Self-Supervised Contrastive Learning using Random Feature Corruption",
        "authors": [
            "Dara Bahri",
            "Heinrich Jiang",
            "Yi Tay",
            "Donald Metzler"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Self-supervised contrastive representation learning has proved incredibly successful in the vision and natural language domains, enabling state-of-the-art performance with orders of magnitude less labeled data. However, such methods are domain-specific and little has been done to leverage this technique on real-world \\emph{tabular} datasets. We propose \\textsc{Scarf}, a simple, widely-applicable technique for contrastive learning, where views are formed by corrupting a random subset of features. When applied to pre-train deep neural networks on the 69 real-world, tabular classification datasets from the OpenML-CC18 benchmark, \\textsc{Scarf} not only improves classification accuracy in the fully-supervised setting but does so also in the presence of label noise and in the semi-supervised setting where only a fraction of the available training data is labeled. We show that \\textsc{Scarf} complements existing strategies and outperforms alternatives like autoencoders. We conduct comprehensive ablations, detailing the importance of a range of factors.",
        "pdf_link": "https://openreview.net/pdf/b63986fb73f0b81b950ec3c4c84a0977ad6bdee1.pdf",
        "forum_url": "https://openreview.net/forum?id=CuV_qYkmKb3",
        "keywords": [
            "self supervised"
        ]
    },
    {
        "title": "Generalized Decision Transformer for Offline Hindsight Information Matching",
        "authors": [
            "Hiroki Furuta",
            "Yutaka Matsuo",
            "Shixiang Shane Gu"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "How to extract as much learning signal from each trajectory data has been a key problem in reinforcement learning (RL), where sample inefficiency has posed serious challenges for practical applications. Recent works have shown that using expressive policy function approximators and conditioning on future trajectory information -- such as future states in hindsight experience replay (HER) or returns-to-go in Decision Transformer (DT) -- enables efficient learning of multi-task policies, where at times online RL is fully replaced by offline behavioral cloning (BC), e.g. sequence modeling. We demonstrate that all these approaches are doing hindsight information matching (HIM) -- training policies that can output the rest of trajectory that matches some statistics of future state information. We present Generalized Decision Transformer (GDT) for solving any HIM problem, and show how different choices for the feature function and the anti-causal aggregator not only recover DT as a special case, but also lead to novel Categorical DT (CDT) and Bi-directional DT (BDT) for matching different statistics of the future. For evaluating CDT and BDT, we define offline multi-task state-marginal matching (SMM) and imitation learning (IL) as two generic HIM problems, propose a Wasserstein distance loss as a metric for both, and empirically study them on MuJoCo continuous control benchmarks. Categorical DT, which simply replaces anti-causal summation with anti-causal binning in DT, enables arguably the first effective offline multi-task SMM algorithm that generalizes well to unseen (and even synthetic) multi-modal reward or state-feature distributions. Bi-directional DT, which uses an anti-causal second transformer as the aggregator, can learn to model any statistics of the future and outperforms DT variants in offline multi-task IL, i.e. one-shot IL. Our generalized formulations from HIM and GDT greatly expand the role of powerful sequence modeling architectures in modern RL.",
        "pdf_link": "https://openreview.net/pdf/86d7058e78842b10462a9f0e0311ca3040adfe97.pdf",
        "forum_url": "https://openreview.net/forum?id=CAjxVodl_v",
        "keywords": [
            "reinforcement learning",
            "sequence modeling",
            "behavioral cloning",
            "generalized decision transformer",
            "information matching",
            "offline hindsight information matching",
            "categorical dt"
        ]
    },
    {
        "title": "Dynamics-Aware Comparison of Learned Reward Functions",
        "authors": [
            "Blake Wulfe",
            "Logan Michael Ellis",
            "Jean Mercat",
            "Rowan Thomas McAllister",
            "Adrien Gaidon"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "The ability to learn reward functions plays an important role in enabling the deployment of intelligent agents in the real world. However, $\\textit{comparing}$ reward functions, for example as a means of evaluating reward learning methods, presents a challenge. Reward functions are typically compared by considering the behavior of optimized policies, but this approach conflates deficiencies in the reward function with those of the policy search algorithm used to optimize it. To address this challenge, Gleave et al. (2020) propose the Equivalent-Policy Invariant Comparison (EPIC) distance. EPIC avoids policy optimization, but in doing so requires computing reward values at transitions that may be impossible under the system dynamics. This is problematic for learned reward functions because it entails evaluating them outside of their training distribution, resulting in inaccurate reward values that we show can render EPIC ineffective at comparing rewards. To address this problem, we propose the Dynamics-Aware Reward Distance (DARD), a new reward pseudometric. DARD uses an approximate transition model of the environment to transform reward functions into a form that allows for comparisons that are invariant to reward shaping while only evaluating reward functions on transitions close to their training distribution. Experiments in simulated physical domains demonstrate that DARD enables reliable reward comparisons without policy optimization and is significantly more predictive than baseline methods of downstream policy performance when dealing with learned reward functions.",
        "pdf_link": "https://openreview.net/pdf/14a7ecb3498b71a8fba347a8d3438e054084f561.pdf",
        "forum_url": "https://openreview.net/forum?id=CALFyKVs87",
        "keywords": [
            "reward functions",
            "learned reward functions",
            "reward learning",
            "reward shaping",
            "reward distance",
            "policy optimization"
        ]
    },
    {
        "title": "Probabilistic Implicit Scene Completion",
        "authors": [
            "Dongsu Zhang",
            "Changwoon Choi",
            "Inbum Park",
            "Young Min Kim"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "We propose a probabilistic shape completion method extended to the continuous geometry of large-scale 3D scenes. Real-world scans of 3D scenes suffer from a considerable amount of missing data cluttered with unsegmented objects. The problem of shape completion is inherently ill-posed, and high-quality result requires scalable solutions that consider multiple possible outcomes. We employ the Generative Cellular Automata that learns the multi-modal distribution and transform the formulation to process large-scale continuous geometry. The local continuous shape is incrementally generated as a sparse voxel embedding, which contains the latent code for each occupied cell. We formally derive that our training objective for the sparse voxel embedding maximizes the variational lower bound of the complete shape distribution and therefore our progressive generation constitutes a valid generative model. Experiments show that our model successfully generates diverse plausible scenes faithful to the input, especially when the input suffers from a significant amount of missing data. We also demonstrate that our approach outperforms deterministic models even in less ambiguous cases with a small amount of missing data, which infers that probabilistic formulation is crucial for high-quality geometry completion on input scans exhibiting any levels of completeness.",
        "pdf_link": "https://openreview.net/pdf/d9aba3c3efb0ad334f7b6a755f29a0b39bc05867.pdf",
        "forum_url": "https://openreview.net/forum?id=BnQhMqDfcKG",
        "keywords": [
            "shape completion",
            "probabilistic shape completion",
            "probabilistic implicit scene completion",
            "geometry completion",
            "generative cellular automata"
        ]
    },
    {
        "title": "EViT: Expediting Vision Transformers via Token Reorganizations",
        "authors": [
            "Youwei Liang",
            "Chongjian GE",
            "Zhan Tong",
            "Yibing Song",
            "Jue Wang",
            "Pengtao Xie"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Vision Transformers (ViTs) take all the image patches as tokens and construct multi-head self-attention (MHSA) among them. Complete leverage of these image tokens brings redundant computations since not all the tokens are attentive in MHSA. Examples include that tokens containing semantically meaningless or distractive image backgrounds do not positively contribute to the ViT predictions. In this work, we propose to reorganize image tokens during the feed-forward process of ViT models, which is integrated into ViT during training. For each forward inference, we identify the attentive image tokens between MHSA and FFN (i.e., feed-forward network) modules, which is guided by the corresponding class token attention. Then, we reorganize image tokens by preserving attentive image tokens and fusing inattentive ones to expedite subsequent MHSA and FFN computations. To this end, our method EViT improves ViTs from two perspectives. First, under the same amount of input image tokens, our method reduces MHSA and FFN computation for efficient inference. For instance, the inference speed of DeiT-S is increased by 50% while its recognition accuracy is decreased by only 0.3% for ImageNet classification. Second, by maintaining the same computational cost, our method empowers ViTs to take more image tokens as input for recognition accuracy improvement, where the image tokens are from higher resolution images. An example is that we improve the recognition accuracy of DeiT-S by 1% for ImageNet classification at the same computational cost of a vanilla DeiT-S. Meanwhile, our method does not introduce more parameters to ViTs. Experiments on the standard benchmarks show the effectiveness of our method. The code is available at https://github.com/youweiliang/evit",
        "pdf_link": "https://openreview.net/pdf/feb0c5a2e1c1fc63509c2e528ca07aa95aea2d5e.pdf",
        "forum_url": "https://openreview.net/forum?id=BjyvwnXXVn_",
        "keywords": [
            "reorganizations",
            "vision transformers",
            "expediting vision transformers",
            "head self attention",
            "image tokens"
        ]
    },
    {
        "title": "GNN-LM: Language Modeling based on Global Contexts via GNN",
        "authors": [
            "Yuxian Meng",
            "Shi Zong",
            "Xiaoya Li",
            "Xiaofei Sun",
            "Tianwei Zhang",
            "Fei Wu",
            "Jiwei Li"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Inspired by the notion that \"it to copy is easier than to memorize\", in this work, we introduce GNN-LM, which extends vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla  LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. The code can be found at https://github.com/ShannonAI/GNN-LM.",
        "pdf_link": "https://openreview.net/pdf/5a848353a24b880cebcd40d2e65796e794c0ff1d.pdf",
        "forum_url": "https://openreview.net/forum?id=BS49l-B5Bql",
        "keywords": [
            "gnn",
            "gnn lm",
            "global contexts",
            "lm",
            "language modeling",
            "model",
            "graph neural networks"
        ]
    },
    {
        "title": "Scalable Sampling for Nonsymmetric Determinantal Point Processes",
        "authors": [
            "Insu Han",
            "Mike Gartrell",
            "Jennifer Gillenwater",
            "Elvis Dohmatob",
            "amin karbasi"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "A determinantal point process (DPP) on a collection of $M$ items is a model, parameterized by a symmetric kernel matrix, that assigns a probability to every subset of those items.  Recent work shows that removing the kernel symmetry constraint, yielding nonsymmetric DPPs (NDPPs), can lead to significant predictive performance gains for machine learning applications. However, existing work leaves open the question of scalable NDPP sampling. There is only one known DPP sampling algorithm, based on Cholesky decomposition, that can directly apply to NDPPs as well. Unfortunately, its runtime is cubic in $M$, and thus does not scale to large item collections. In this work, we first note that this algorithm can be transformed into a linear-time one for kernels with low-rank structure.  Furthermore, we develop a scalable sublinear-time rejection sampling algorithm by constructing a novel proposal distribution.  Additionally, we show that imposing certain structural constraints on the NDPP kernel enables us to bound the rejection rate in a way that depends only on the kernel rank. In our experiments we compare the speed of all of these samplers for a variety of real-world tasks.",
        "pdf_link": "https://openreview.net/pdf/b38ff838b862c1f5918c345f4322281132fa0715.pdf",
        "forum_url": "https://openreview.net/forum?id=BB4e8Atc1eR",
        "keywords": [
            "sampling",
            "determinantal point processes",
            "scalable sampling",
            "symmetry",
            "rank"
        ]
    },
    {
        "title": "Understanding the Role of Self Attention for Efficient Speech Recognition",
        "authors": [
            "Kyuhong Shim",
            "Jungwook Choi",
            "Wonyong Sung"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Self-attention (SA) is a critical component of Transformer neural networks that have succeeded in automatic speech recognition (ASR). In this paper, we analyze the role of SA in Transformer-based ASR models for not only understanding the mechanism of improved recognition accuracy but also lowering the computational complexity. We reveal that SA performs two distinct roles: phonetic and linguistic localization. Especially, we show by experiments that phonetic localization in the lower layers extracts phonologically meaningful features from speech and reduces the phonetic variance in the utterance for proper linguistic localization in the upper layers. From this understanding, we discover that attention maps can be reused as long as their localization capability is preserved. To evaluate this idea, we implement the layer-wise attention map reuse on real GPU platforms and achieve up to 1.96 times speedup in inference and 33% savings in training time with noticeably improved ASR performance for the challenging benchmark on LibriSpeech dev/test-other dataset.\n",
        "pdf_link": "https://openreview.net/pdf/d785690241796686225be6fa4f299ba32712c574.pdf",
        "forum_url": "https://openreview.net/forum?id=AvcfxqRy4Y",
        "keywords": [
            "self attention",
            "speech recognition",
            "efficient speech recognition",
            "transformer"
        ]
    },
    {
        "title": "Emergent Communication at Scale",
        "authors": [
            "Rahma Chaabouni",
            "Florian Strub",
            "Florent Altch\u00e9",
            "Eugene Tarassov",
            "Corentin Tallec",
            "Elnaz Davoodi",
            "Kory Wallace Mathewson",
            "Olivier Tieleman",
            "Angeliki Lazaridou",
            "Bilal Piot"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Emergent communication aims for a better understanding of human language evolution and building more efficient representations. We posit that reaching these goals will require scaling up, in contrast to a significant amount of literature that focuses on setting up small-scale problems to tease out desired properties of the emergent languages. We focus on three independent aspects to scale up, namely the dataset, task complexity, and population size. We provide a first set of results for large populations solving complex tasks on realistic large-scale datasets, as well as an easy-to-use codebase to enable further experimentation. In more complex tasks and datasets, we find that RL training can become unstable, but responds well to established stabilization techniques.\nWe also identify the need for a different metric than topographic similarity, which does not correlate with the generalization performances when working with natural images. In this context, we probe ease-of-learnability and transfer methods to assess emergent languages. Finally, we observe that larger populations do not induce robust emergent protocols with high generalization performance, leading us to explore different ways to leverage population, through voting and imitation learning. ",
        "pdf_link": "https://openreview.net/pdf/89135b1aa6a72139d3332a71501a3d5d792156a8.pdf",
        "forum_url": "https://openreview.net/forum?id=AUGBfDIV9rL",
        "keywords": [
            "emergent communication",
            "emergent languages",
            "task complexity",
            "population size",
            "codebase",
            "evolution",
            "scale"
        ]
    },
    {
        "title": "POETREE: Interpretable Policy Learning with Adaptive Decision Trees",
        "authors": [
            "Aliz\u00e9e Pace",
            "Alex Chan",
            "Mihaela van der Schaar"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Building models of human decision-making from observed behaviour is critical to better understand, diagnose and support real-world policies such as clinical care. As established policy learning approaches remain focused on imitation performance, they fall short of explaining the demonstrated decision-making process. Policy Extraction through decision Trees (POETREE) is a novel framework for interpretable policy learning, compatible with fully-offline and partially-observable clinical decision environments -- and builds probabilistic tree policies determining physician actions based on patients' observations and medical history. Fully-differentiable tree architectures are grown incrementally during optimization to adapt their complexity to the modelling task, and learn a representation of patient history through recurrence, resulting in decision tree policies that adapt over time with patient information. This policy learning method outperforms the state-of-the-art on real and synthetic medical datasets, both in terms of understanding, quantifying and evaluating observed behaviour as well as in accurately replicating it -- with potential to improve future decision support systems.",
        "pdf_link": "https://openreview.net/pdf/18b6779789f76ccc8b7ff5e0a77ec0fa2a5d4057.pdf",
        "forum_url": "https://openreview.net/forum?id=AJsI-ymaKn_",
        "keywords": [
            "decision trees",
            "adaptive decision trees",
            "policy learning",
            "policy extraction",
            "decision making",
            "interpretable policy learning"
        ]
    },
    {
        "title": "DEPTS: Deep Expansion Learning for Periodic Time Series Forecasting",
        "authors": [
            "Wei Fan",
            "Shun Zheng",
            "Xiaohan Yi",
            "Wei Cao",
            "Yanjie Fu",
            "Jiang Bian",
            "Tie-Yan Liu"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Periodic time series (PTS) forecasting plays a crucial role in a variety of industries to foster critical tasks, such as early warning, pre-planning, resource scheduling, etc. However, the complicated dependencies of the PTS signal on its inherent periodicity as well as the sophisticated composition of various periods hinder the performance of PTS forecasting. In this paper, we introduce a deep expansion learning framework, DEPTS, for PTS forecasting. DEPTS starts with a decoupled formulation by introducing the periodic state as a hidden variable, which stimulates us to make two dedicated modules to tackle the aforementioned two challenges. First, we develop an expansion module on top of residual learning to perform a layer-by-layer expansion of those complicated dependencies. Second, we introduce a periodicity module with a parameterized periodic function that holds sufficient capacity to capture diversified periods. Moreover, our two customized modules also have certain interpretable capabilities, such as attributing the forecasts to either local momenta or global periodicity and characterizing certain core periodic properties, e.g., amplitudes and frequencies. Extensive experiments on both synthetic data and real-world data demonstrate the effectiveness of DEPTS on handling PTS. In most cases, DEPTS achieves significant improvements over the best baseline. Specifically, the error reduction can even reach up to 20% for a few cases. All codes for this paper are publicly available.",
        "pdf_link": "https://openreview.net/pdf/cd132957a26c075bcbe5f26a96995eea829b38e0.pdf",
        "forum_url": "https://openreview.net/forum?id=AJAR-JgNw__",
        "keywords": [
            "periodic time series",
            "periodic time series forecasting",
            "forecasting",
            "deep expansion learning",
            "depts",
            "residual learning"
        ]
    },
    {
        "title": "Task Relatedness-Based Generalization Bounds for Meta Learning",
        "authors": [
            "Jiechao Guan",
            "Zhiwu Lu"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Supposing the $n$ training tasks and the new task are sampled from the same environment, traditional meta learning theory derives an error bound on the expected loss over the new task in terms of the empirical training loss, uniformly over the set of all hypothesis spaces. However, there is still little research on how the relatedness of these tasks can affect the full utilization of all $mn$ training data (with $m$ examples per task). In this paper, we propose to address this problem by defining a new notion of task relatedness according to the existence of the bijective transformation between two tasks. A novel generalization bound of $\\mathcal{O}(\\frac{1}{\\sqrt{mn}})$ for meta learning is thus derived by exploiting the proposed task relatedness. Moreover, when investigating a special branch of meta learning that involves representation learning with deep neural networks, we establish spectrally-normalized bounds for both classification and regression problems. Finally, we demonstrate that the relatedness requirement between two tasks is satisfied when the sample space possesses the completeness and separability properties, validating the rationality and applicability of our proposed task-relatedness measure.",
        "pdf_link": "https://openreview.net/pdf/ebbc100be110ad3e6a5f5491100b967847d22082.pdf",
        "forum_url": "https://openreview.net/forum?id=A3HHaEdqAJL",
        "keywords": [
            "generalization bounds",
            "bounds",
            "task relatedness",
            "meta learning",
            "classification",
            "error bound",
            "utilization",
            "task relatedness measure",
            "regression",
            "empirical training loss"
        ]
    },
    {
        "title": "SGD Can Converge to Local Maxima",
        "authors": [
            "Liu Ziyin",
            "Botao Li",
            "James B Simon",
            "Masahito Ueda"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Previous works on stochastic gradient descent (SGD) often focus on its success. In this work, we construct worst-case optimization problems illustrating that, when not in the regimes that the previous works often assume, SGD can exhibit many strange and potentially undesirable behaviors. Specifically, we construct landscapes and data distributions such that (1) SGD converges to local maxima, (2) SGD escapes saddle points arbitrarily slowly, (3) SGD prefers sharp minima over flat ones, and (4) AMSGrad converges to local maxima. We also realize results in a minimal neural network-like example. Our results highlight the importance of simultaneously analyzing the minibatch sampling, discrete-time updates rules, and realistic landscapes to understand the role of SGD in deep learning.",
        "pdf_link": "https://openreview.net/pdf/c054a998fd46a7c8498a497ba6b856c2e0532b6b.pdf",
        "forum_url": "https://openreview.net/forum?id=9XhPLAjjRB",
        "keywords": [
            "local maxima"
        ]
    },
    {
        "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
        "authors": [
            "Victor Sanh",
            "Albert Webson",
            "Colin Raffel",
            "Stephen Bach",
            "Lintang Sutawika",
            "Zaid Alyafeai",
            "Antoine Chaffin",
            "Arnaud Stiegler",
            "Arun Raja",
            "Manan Dey",
            "M Saiful Bari",
            "Canwen Xu",
            "Urmish Thakker",
            "Shanya Sharma Sharma",
            "Eliza Szczechla",
            "Taewoon Kim",
            "Gunjan Chhablani",
            "Nihal Nayak",
            "Debajyoti Datta",
            "Jonathan Chang",
            "Mike Tian-Jian Jiang",
            "Han Wang",
            "Matteo Manica",
            "Sheng Shen",
            "Zheng Xin Yong",
            "Harshit Pandey",
            "Rachel Bawden",
            "Thomas Wang",
            "Trishala Neeraj",
            "Jos Rozen",
            "Abheesht Sharma",
            "Andrea Santilli",
            "Thibault Fevry",
            "Jason Alan Fries",
            "Ryan Teehan",
            "Teven Le Scao",
            "Stella Biderman",
            "Leo Gao",
            "Thomas Wolf",
            "Alexander M Rush"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models\u2019 pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely unseen tasks specified in natural language. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several datasets, often outperforming models 16\u00d7 its size. Further, our model attains strong performance on a subset of tasks from the BIG-Bench benchmark, outperforming models 6\u00d7 its size. All trained models are available at https://github.com/bigscience-workshop/t-zero, and all prompts are available at https://github.com/bigscience-workshop/promptsource.",
        "pdf_link": "https://openreview.net/pdf/bec425b93713482f8e2de5d1d15b66ff95a47026.pdf",
        "forum_url": "https://openreview.net/forum?id=9Vrb9D0WI4",
        "keywords": [
            "multitask",
            "implicit multitask"
        ]
    },
    {
        "title": "Representational Continuity for Unsupervised Continual Learning",
        "authors": [
            "Divyam Madaan",
            "Jaehong Yoon",
            "Yuanchun Li",
            "Yunxin Liu",
            "Sung Ju Hwang"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Continual learning (CL) aims to learn a sequence of tasks without forgetting the previously acquired knowledge. However, recent CL advances are restricted to supervised continual learning (SCL) scenarios. Consequently, they are not scalable to real-world applications where the data distribution is often biased and unannotated. In this work, we focus on unsupervised continual learning (UCL), where we learn the feature representations on an unlabelled sequence of tasks and show that reliance on annotated data is not necessary for continual learning. We conduct a systematic study analyzing the learned feature representations and show that unsupervised visual representations are surprisingly more robust to catastrophic forgetting, consistently achieve better performance, and generalize better to out-of-distribution tasks than SCL. Furthermore, we find that UCL achieves a smoother loss landscape through qualitative analysis of the learned representations and learns meaningful feature representations. Additionally, we propose Lifelong Unsupervised Mixup (LUMP), a simple yet effective technique that interpolates between the current task and previous tasks' instances to alleviate catastrophic forgetting for unsupervised representations. ",
        "pdf_link": "https://openreview.net/pdf/947f2c6dc3cd63a83d402bf9cbaddf42e362709e.pdf",
        "forum_url": "https://openreview.net/forum?id=9Hrka5PA7LW",
        "keywords": [
            "continual learning",
            "unsupervised continual learning",
            "supervised continual learning"
        ]
    },
    {
        "title": "Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme",
        "authors": [
            "Vadim Popov",
            "Ivan Vovk",
            "Vladimir Gogoryan",
            "Tasnima Sadekova",
            "Mikhail Sergeevich Kudinov",
            "Jiansheng Wei"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Voice conversion is a common speech synthesis task which can be solved in different ways depending on a particular real-world scenario. The most challenging one often referred to as one-shot many-to-many voice conversion consists in copying target voice from only one reference utterance in the most general case when both source and target speakers do not belong to the training dataset. We present a scalable high-quality solution based on diffusion probabilistic modeling and demonstrate its superior quality compared to state-of-the-art one-shot voice conversion approaches. Moreover, focusing on real-time applications, we investigate general principles which can make diffusion models faster while keeping synthesis quality at a high level. As a result, we develop a novel Stochastic Differential Equations solver suitable for various diffusion model types and generative tasks as shown through empirical studies and justify it by theoretical analysis.",
        "pdf_link": "https://openreview.net/pdf/468145b46e459c5ba69e7017b6ef4eaece277e94.pdf",
        "forum_url": "https://openreview.net/forum?id=8c50f-DoWAu",
        "keywords": [
            "voice conversion",
            "fast maximum likelihood sampling",
            "speech synthesis",
            "diffusion"
        ]
    },
    {
        "title": "Label Encoding for Regression Networks",
        "authors": [
            "Deval Shah",
            "Zi Yu Xue",
            "Tor Aamodt"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Deep neural networks are used for a wide range of regression problems. However, there exists a significant gap in accuracy between specialized approaches and generic direct regression in which a network is trained by minimizing the squared or absolute error of output labels. Prior work has shown that solving a regression problem with a set of binary classifiers can improve accuracy by utilizing well-studied binary classification algorithms. We introduce binary-encoded labels (BEL), which generalizes the application of binary classification to regression by providing a framework for considering arbitrary multi-bit values when encoding target values. We identify desirable properties of suitable encoding and decoding functions used for the conversion between real-valued and binary-encoded labels based on theoretical and empirical study. These properties highlight a tradeoff between classification error probability and error-correction capabilities of label encodings. BEL can be combined with off-the-shelf task-specific feature extractors and trained end-to-end. We propose a series of sample encoding, decoding, and training loss functions for BEL and demonstrate they result in lower error than direct regression and specialized approaches while being suitable for a diverse set of regression problems, network architectures, and evaluation metrics. BEL achieves state-of-the-art accuracies for several regression benchmarks. Code is available at https://github.com/ubc-aamodt-group/BEL_regression.\n",
        "pdf_link": "https://openreview.net/pdf/f9f19a0a8f967030ba6a2061f7e7c78524762ef7.pdf",
        "forum_url": "https://openreview.net/forum?id=8WawVDdKqlL",
        "keywords": [
            "label encoding",
            "regression",
            "classification",
            "regression networks"
        ]
    },
    {
        "title": "Relational Multi-Task Learning: Modeling Relations between Data and Tasks",
        "authors": [
            "Kaidi Cao",
            "Jiaxuan You",
            "Jure Leskovec"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "A key assumption in multi-task learning is that at the inference time the multi-task model only has access to a given data point but not to the data point\u2019s labels from other tasks. This presents an opportunity to extend multi-task learning to utilize data point\u2019s labels from other auxiliary tasks, and this way improves performance on the new task. Here we introduce a novel relational multi-task learning setting where we leverage data point labels from auxiliary tasks to make more accurate predictions on the new task. We develop MetaLink, where our key innovation is to build a knowledge graph that connects data points and tasks and thus allows us to leverage labels from auxiliary tasks. The knowledge graph consists of two types of nodes: (1) data nodes, where node features are data embeddings computed by the neural network, and (2) task nodes, with the last layer\u2019s weights for each task as node features. The edges in this knowledge graph capture data-task relationships, and the edge label captures the label of a data point on a particular task. Under MetaLink, we reformulate the new task as a link label prediction problem between a data node and a task node. The MetaLink framework provides flexibility to model knowledge transfer from auxiliary task labels to the task of interest. We evaluate MetaLink on 6 benchmark datasets in both biochemical and vision domains. Experiments demonstrate that MetaLink can successfully utilize the relations among different tasks, outperforming the state-of-the-art methods under the proposed relational multi-task learning setting, with up to 27% improvement in ROC AUC.",
        "pdf_link": "https://openreview.net/pdf/2fc25baa1e108a6b1e91d4388bfae9d417400e3f.pdf",
        "forum_url": "https://openreview.net/forum?id=8Py-W8lSUgy",
        "keywords": [
            "metalink",
            "multi task learning",
            "multi task model",
            "relational multi task learning",
            "knowledge graph"
        ]
    },
    {
        "title": "AdaRL: What, Where, and How to Adapt in Transfer Reinforcement Learning",
        "authors": [
            "Biwei Huang",
            "Fan Feng",
            "Chaochao Lu",
            "Sara Magliacane",
            "Kun Zhang"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "One practical challenge in reinforcement learning (RL) is how to make quick adaptations when faced with new environments. In this paper, we propose a principled framework for adaptive RL, called AdaRL, that adapts reliably and efficiently to changes across domains with a few samples from the target domain, even in partially observable environments. Specifically, we leverage a parsimonious graphical representation that characterizes structural relationships over variables in the RL system. Such graphical representations provide a compact way to encode what and where the changes across domains are, and furthermore inform us with a minimal set of changes that one has to consider for the purpose of policy adaptation. We show that by explicitly leveraging this compact representation to encode changes, we can efficiently adapt the policy to the target domain, in which only a few samples are needed and further policy optimization is avoided. We illustrate the efficacy of AdaRL through a series of experiments that vary factors in the observation, transition and reward functions for Cartpole and Atari games.",
        "pdf_link": "https://openreview.net/pdf/d3061c36db2595696e3c5444edf46fe3a2f665e9.pdf",
        "forum_url": "https://openreview.net/forum?id=8H5bpVwvt5",
        "keywords": [
            "reinforcement learning",
            "adaptation",
            "transfer reinforcement learning",
            "adaptive rl"
        ]
    },
    {
        "title": "Learning Causal Models from Conditional Moment Restrictions by Importance Weighting",
        "authors": [
            "Masahiro Kato",
            "Masaaki Imaizumi",
            "Kenichiro McAlinn",
            "Shota Yasui",
            "Haruo Kakehi"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "We consider learning causal relationships under conditional moment restrictions. Unlike causal inference under unconditional moment restrictions, conditional moment restrictions pose serious challenges for causal inference. To address this issue, we propose a method that transforms conditional moment restrictions to unconditional moment restrictions through importance weighting using a conditional density ratio estimator. Then, using this transformation, we propose a method that successfully estimate a parametric or nonparametric functions defined under the conditional moment restrictions. We analyze the estimation error and provide a bound on the structural function, providing theoretical support for our proposed method. In experiments, we confirm the soundness of our proposed method.",
        "pdf_link": "https://openreview.net/pdf/84e1728a94b82499ea64a0d48409de2dfc9115ee.pdf",
        "forum_url": "https://openreview.net/forum?id=7twQI5VnC8",
        "keywords": [
            "importance weighting",
            "restrictions",
            "causal inference",
            "conditional moment restrictions",
            "conditional density ratio estimator",
            "learning causal models",
            "unconditional moment restrictions"
        ]
    },
    {
        "title": "Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters",
        "authors": [
            "Qiang Meng",
            "Feng Zhou",
            "Hainan Ren",
            "Tianshu Feng",
            "Guochao Liu",
            "Yuanqing Lin"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "The growing public concerns on data privacy in face recognition can be partly relieved by the federated learning (FL) paradigm. However, conventional FL methods usually perform poorly  due to the particularity of the task, \\textit{i.e.},  broadcasting class centers among clients is essential for recognition performances but leads to privacy leakage. To resolve the privacy-utility paradox, this work proposes PrivacyFace, a framework largely improves the federated learning face recognition via communicating auxiliary and privacy-agnostic information among clients. PrivacyFace mainly consists of two components: First, a practical Differentially Private Local Clustering (DPLC) mechanism is proposed to distill sanitized clusters from local class centers. Second, a consensus-aware recognition loss subsequently encourages global consensuses among clients, which ergo leads to more discriminative features. The proposed schemes are mathematically proved to be differential private, introduce a lightweight overhead as well as yield prominent performance boosts (\\textit{e.g.}, +9.63\\% and +10.26\\% for TAR@FAR=1e-4 on IJB-B and IJB-C respectively). Extensive experiments and ablation studies on a large-scale dataset have demonstrated the efficacy and practicability of our method.  ",
        "pdf_link": "https://openreview.net/pdf/6e32e9b384f0d8b5260a2a95ad7645cebf50046f.pdf",
        "forum_url": "https://openreview.net/forum?id=7l1IjZVddDW",
        "keywords": [
            "face recognition",
            "federated learning face recognition",
            "federated learning",
            "privacy"
        ]
    },
    {
        "title": "A General Analysis of Example-Selection for Stochastic Gradient Descent",
        "authors": [
            "Yucheng Lu",
            "Si Yi Meng",
            "Christopher De Sa"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Training example order in SGD has long been known to affect convergence rate. Recent results show that accelerated rates are possible in a variety of cases for permutation-based sample orders, in which each example from the training set is used once before any example is reused. In this paper, we develop a broad condition on the sequence of examples used by SGD that is sufficient to prove tight convergence rates in both strongly convex and non-convex settings. We show that our approach suffices to recover, and in some cases improve upon, previous state-of-the-art analyses for four known example-selection schemes: (1) shuffle once, (2) random reshuffling, (3) random reshuffling with data echoing, and (4) Markov Chain Gradient Descent. Motivated by our theory, we propose two new example-selection approaches. First, using quasi-Monte-Carlo methods, we achieve unprecedented accelerated convergence rates for learning with data augmentation. Second, we greedily choose a fixed scan-order to minimize the metric used in our condition and show that we can obtain more accurate solutions from the same number of epochs of SGD. We conclude by empirically demonstrating the utility of our approach for both convex linear-model and deep learning tasks. Our code is available at: https://github.com/EugeneLYC/qmc-ordering.",
        "pdf_link": "https://openreview.net/pdf/f12e4d11665b4464f75c539a335e09880c3e3472.pdf",
        "forum_url": "https://openreview.net/forum?id=7gWSJrP3opB",
        "keywords": [
            "example selection",
            "deep learning",
            "convergence",
            "data augmentation",
            "markov chain gradient descent"
        ]
    },
    {
        "title": "Possibility Before Utility: Learning And Using Hierarchical Affordances",
        "authors": [
            "Robby Costales",
            "Shariq Iqbal",
            "Fei Sha"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Reinforcement learning algorithms struggle on tasks with complex hierarchical dependency structures. Humans and other intelligent agents do not waste time assessing the utility of every high-level action in existence, but instead only consider ones they deem possible in the first place. By focusing only on what is feasible, or \"afforded'', at the present moment, an agent can spend more time both evaluating the utility of and acting on what matters. To this end, we present Hierarchical Affordance Learning (HAL), a method that learns a model of hierarchical affordances in order to prune impossible subtasks for more effective learning. Existing works in hierarchical reinforcement learning provide agents with structural representations of subtasks but are not affordance-aware, and by grounding our definition of hierarchical affordances in the present state, our approach is more flexible than the multitude of approaches that ground their subtask dependencies in a symbolic history. While these logic-based methods often require complete knowledge of the subtask hierarchy, our approach is able to utilize incomplete and varying symbolic specifications. Furthermore, we demonstrate that relative to non-affordance-aware methods, HAL agents are better able to efficiently learn complex tasks, navigate environment stochasticity, and acquire diverse skills in the absence of extrinsic supervision---all of which are hallmarks of human learning.",
        "pdf_link": "https://openreview.net/pdf/f4b5c96c2948ff7fcea521e9713644691c27bab2.pdf",
        "forum_url": "https://openreview.net/forum?id=7b4zxUnrO2N",
        "keywords": [
            "affordances",
            "hierarchical affordances",
            "hierarchical affordance learning",
            "reinforcement learning",
            "learning"
        ]
    },
    {
        "title": "Continual Learning with Recursive Gradient Optimization",
        "authors": [
            "Hao Liu",
            "Huaping Liu"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Learning multiple tasks sequentially without forgetting previous knowledge, called Continual Learning(CL), remains a long-standing challenge for neural networks. Most existing methods rely on additional network capacity or data replay. In contrast, we introduce a novel approach which we refer to as Recursive Gradient Optimization(RGO). RGO is composed of an iteratively updated optimizer that modifies the gradient to minimize forgetting without data replay and a virtual Feature Encoding Layer(FEL) that represents different long-term structures with only task descriptors. Experiments demonstrate that RGO has significantly better performance on popular continual classification benchmarks when compared to the baselines and achieves new state-of-the-art performance on 20-split-CIFAR100(82.22%) and 20-split-miniImageNet(72.63%). With higher average accuracy than Single-Task Learning(STL), this method is flexible and reliable to provide continual learning capabilities for learning models that rely on gradient descent.",
        "pdf_link": "https://openreview.net/pdf/19ab5aa0cb46a13fe9b97dab46913da24e89d2fb.pdf",
        "forum_url": "https://openreview.net/forum?id=7YDLgf9_zgm",
        "keywords": [
            "continual learning",
            "recursive gradient optimization",
            "continual classification",
            "feature encoding layer"
        ]
    },
    {
        "title": "Understanding over-squashing and bottlenecks on graphs via curvature",
        "authors": [
            "Jake Topping",
            "Francesco Di Giovanni",
            "Benjamin Paul Chamberlain",
            "Xiaowen Dong",
            "Michael M. Bronstein"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as a factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This phenomenon, referred to as 'over-squashing', has been heuristically attributed to graph bottlenecks where the number of $k$-hop neighbors grows rapidly with $k$. We provide a precise description of the over-squashing phenomenon in GNNs and analyze how it arises from bottlenecks in the graph. For this purpose, we introduce a new edge-based combinatorial curvature and prove that negatively curved edges are responsible for the over-squashing issue. We also propose and experimentally test a  curvature-based graph rewiring method to alleviate the over-squashing.",
        "pdf_link": "https://openreview.net/pdf/f6b974eac8792a0d8d59633044276dabbf9d01c9.pdf",
        "forum_url": "https://openreview.net/forum?id=7UmjRGzp-A",
        "keywords": [
            "over squashing",
            "curvature",
            "message passing",
            "networks"
        ]
    },
    {
        "title": "$\\mathrm{SO}(2)$-Equivariant Reinforcement Learning",
        "authors": [
            "Dian Wang",
            "Robin Walters",
            "Robert Platt"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Equivariant neural networks enforce symmetry within the structure of their convolutional layers, resulting in a substantial improvement in sample efficiency when learning an equivariant or invariant function. Such models are applicable to robotic manipulation learning which can often be formulated as a rotationally symmetric problem. This paper studies equivariant model architectures in the context of $Q$-learning and actor-critic reinforcement learning. We identify equivariant and invariant characteristics of the optimal $Q$-function and the optimal policy and propose equivariant DQN and SAC algorithms that leverage this structure. We present experiments that demonstrate that our equivariant versions of DQN and SAC can be significantly more sample efficient than competing algorithms on an important class of robotic manipulation problems.",
        "pdf_link": "https://openreview.net/pdf/9f58959cef1dc2c685298e532713a5104f2df44b.pdf",
        "forum_url": "https://openreview.net/forum?id=7F9cOhdvfk_",
        "keywords": [
            "equivariant",
            "reinforcement learning",
            "equivariant neural networks",
            "sample efficient",
            "efficiency",
            "actor critic reinforcement learning",
            "robotic"
        ]
    },
    {
        "title": "Programmatic Reinforcement Learning without Oracles",
        "authors": [
            "Wenjie Qiu",
            "He Zhu"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Deep reinforcement learning (RL) has led to encouraging successes in many challenging control tasks. However, a deep RL model lacks interpretability due to the difficulty of identifying how the model's control logic relates to its network structure. Programmatic policies structured in more interpretable representations emerge as a promising solution. Yet two shortcomings remain: First, synthesizing programmatic policies requires optimizing over the discrete and non-differentiable search space of program architectures. Previous works are suboptimal because they only enumerate program architectures greedily guided by a pretrained RL oracle. Second, these works do not exploit compositionality, an important programming concept, to reuse and compose primitive functions to form a complex function for new tasks. Our first contribution is a programmatically interpretable RL framework that conducts program architecture search on top of a continuous relaxation of the architecture space defined by programming language grammar rules. Our algorithm allows policy architectures to be learned with policy parameters via bilevel optimization using efficient policy-gradient methods, and thus does not require a pretrained oracle. Our second contribution is improving programmatic policies to support compositionality by integrating primitive functions learned to grasp task-agnostic skills as a composite program to solve novel RL problems. Experiment results demonstrate that our algorithm excels in discovering optimal programmatic policies that are highly interpretable. The code of this work is available at https://github.com/RU-Automated-Reasoning-Group/pi-PRL.",
        "pdf_link": "https://openreview.net/pdf/92dbdb48fe9a64f9e46e509762a9443b84450f68.pdf",
        "forum_url": "https://openreview.net/forum?id=6Tk2noBdvxt",
        "keywords": [
            "oracles",
            "reinforcement learning",
            "architectures",
            "language"
        ]
    },
    {
        "title": "Open-Set Recognition: A Good Closed-Set Classifier is All You Need",
        "authors": [
            "Sagar Vaze",
            "Kai Han",
            "Andrea Vedaldi",
            "Andrew Zisserman"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "The ability to identify whether or not a test sample belongs to one of the semantic classes in a classifier's training set is critical to practical deployment of the model. This task is termed open-set recognition (OSR) and has received significant attention in recent years. In this paper, we first demonstrate that the ability of a classifier to make the 'none-of-above' decision is highly correlated with its accuracy on the closed-set classes. We find that this relationship holds across loss objectives and architectures, and further demonstrate the trend both on the standard OSR benchmarks as well as on a large-scale ImageNet evaluation. Second, we use this correlation to boost the performance of the maximum softmax probability OSR 'baseline' by improving its closed-set accuracy, and with this strong baseline achieve state-of-the-art on a number of OSR benchmarks. Similarly, we boost the performance of the existing state-of-the-art method by improving its closed-set accuracy, but the resulting discrepancy with the strong baseline is marginal. Our third contribution is to present the 'Semantic Shift Benchmark' (SSB), which better respects the task of detecting semantic novelty, as opposed to low-level distributional shifts as tackled by neighbouring machine learning fields. On this new evaluation, we again demonstrate that there is negligible difference between the strong baseline and the existing state-of-the-art. Code available at: https://github.com/sgvaze/osr_closed_set_all_you_need.",
        "pdf_link": "https://openreview.net/pdf/a9e422d293a936fe65575b5e1ea6a86549b84bca.pdf",
        "forum_url": "https://openreview.net/forum?id=5hLP5JY9S2d",
        "keywords": [
            "open set recognition",
            "closed set accuracy",
            "closed set classifier"
        ]
    },
    {
        "title": "Learning Hierarchical Structures with Differentiable Nondeterministic Stacks",
        "authors": [
            "Brian DuSell",
            "David Chiang"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Learning hierarchical structures in sequential data -- from simple algorithmic patterns to natural language -- in a reliable, generalizable way remains a challenging problem for neural language models. Past work has shown that recurrent neural networks (RNNs) struggle to generalize on held-out algorithmic or syntactic patterns without supervision or some inductive bias. To remedy this, many papers have explored augmenting RNNs with various differentiable stacks, by analogy with finite automata and pushdown automata (PDAs). In this paper, we improve the performance of our recently proposed Nondeterministic Stack RNN (NS-RNN), which uses a differentiable data structure that simulates a nondeterministic PDA, with two important changes. First, the model now assigns unnormalized positive weights instead of probabilities to stack actions, and we provide an analysis of why this improves training. Second, the model can directly observe the state of the underlying PDA. Our model achieves lower cross-entropy than all previous stack RNNs on five context-free language modeling tasks (within 0.05 nats of the information-theoretic lower bound), including a task on which the NS-RNN previously failed to outperform a deterministic stack RNN baseline. Finally, we propose a restricted version of the NS-RNN that incrementally processes infinitely long sequences, and we present language modeling results on the Penn Treebank.",
        "pdf_link": "https://openreview.net/pdf/bfc2ff0a81fd70d01a09a0cb018dddf36e401060.pdf",
        "forum_url": "https://openreview.net/forum?id=5LXw_QplBiF",
        "keywords": [
            "recurrent neural networks",
            "pushdown automata",
            "differentiable stacks",
            "differentiable nondeterministic stacks",
            "nondeterministic pda",
            "differentiable data structure",
            "language modeling",
            "learning hierarchical structures"
        ]
    },
    {
        "title": "Understanding Latent Correlation-Based Multiview Learning and Self-Supervision: An Identifiability Perspective",
        "authors": [
            "Qi Lyu",
            "Xiao Fu",
            "Weiran Wang",
            "Songtao Lu"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Multiple views of data, both naturally acquired (e.g., image and audio) and artificially produced (e.g., via adding different noise to data samples), have proven useful in enhancing representation learning. Natural views are often handled by multiview analysis tools, e.g., (deep) canonical correlation analysis [(D)CCA], while the artificial ones are frequently used in self-supervised learning (SSL) paradigms, e.g., BYOL and Barlow Twins. Both types of approaches often involve learning neural feature extractors such that the embeddings of data exhibit high cross-view correlations. Although intuitive, the effectiveness of correlation-based neural embedding is mostly empirically validated. \nThis work aims to understand latent correlation maximization-based deep multiview learning from a latent component identification viewpoint. An intuitive generative model of multiview data is adopted, where the views are different nonlinear mixtures of shared and private components. Since the shared components are view/distortion-invariant, representing the data using such components is believed to reveal the identity of the samples effectively and robustly. Under this model, latent correlation maximization is shown to guarantee the extraction of the shared components across views (up to certain ambiguities). In addition, it is further shown that the private information in each view can be provably disentangled from the shared using proper regularization design. A finite sample analysis, which has been rare in nonlinear mixture identifiability study, is also presented. The theoretical results and newly designed regularization are tested on a series of tasks. ",
        "pdf_link": "https://openreview.net/pdf/a4489765925696bd54b9091ab12640cebd4b76ce.pdf",
        "forum_url": "https://openreview.net/forum?id=5FUq05QRc5b",
        "keywords": [
            "identifiability",
            "latent correlation",
            "regularization",
            "self supervision",
            "self supervised learning",
            "latent component identification",
            "multiview learning",
            "distortion invariant",
            "cross view correlations",
            "representation learning"
        ]
    },
    {
        "title": "Continuous-Time Meta-Learning with Forward Mode Differentiation",
        "authors": [
            "Tristan Deleu",
            "David Kanaa",
            "Leo Feng",
            "Giancarlo Kerg",
            "Yoshua Bengio",
            "Guillaume Lajoie",
            "Pierre-Luc Bacon"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Drawing inspiration from gradient-based meta-learning methods with infinitely small gradient steps, we introduce Continuous-Time Meta-Learning (COMLN), a meta-learning algorithm where adaptation follows the dynamics of a gradient vector field. Specifically, representations of the inputs are meta-learned such that a task-specific linear classifier is obtained as a solution of an ordinary differential equation (ODE). Treating the learning process as an ODE offers the notable advantage that the length of the trajectory is now continuous, as opposed to a fixed and discrete number of gradient steps. As a consequence, we can optimize the amount of adaptation necessary to solve a new task using stochastic gradient descent, in addition to learning the initial conditions as is standard practice in gradient-based meta-learning. Importantly, in order to compute the exact meta-gradients required for the outer-loop updates, we  devise an efficient algorithm based on forward mode differentiation, whose memory requirements do not scale with the length of the learning trajectory, thus allowing longer adaptation in constant memory. We provide analytical guarantees for the stability of COMLN, we show empirically its efficiency in terms of runtime and memory usage, and we illustrate its effectiveness on a range of few-shot image classification problems.",
        "pdf_link": "https://openreview.net/pdf/68b03565ea73b881a10643d2e81e4ace23821ef2.pdf",
        "forum_url": "https://openreview.net/forum?id=57PipS27Km",
        "keywords": [
            "forward mode differentiation",
            "meta learning",
            "continuous time meta learning"
        ]
    },
    {
        "title": "Self-supervised Learning is More Robust to Dataset Imbalance",
        "authors": [
            "Hong Liu",
            "Jeff Z. HaoChen",
            "Adrien Gaidon",
            "Tengyu Ma"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Self-supervised learning (SSL) is a scalable way to learn general visual representations since it learns without labels. However, large-scale unlabeled datasets in the wild often have long-tailed label distributions, where we know little about the behavior of SSL. In this work, we systematically investigate self-supervised learning under dataset imbalance. First, we find via extensive experiments that off-the-shelf self-supervised representations are already more robust to class imbalance than supervised representations. The performance gap between balanced and imbalanced pre-training with SSL is significantly smaller than the gap with supervised learning, across sample sizes, for both in-domain and, especially, out-of-domain evaluation. Second, towards understanding the robustness of SSL, we hypothesize that SSL learns richer features from frequent data: it may learn label-irrelevant-but-transferable features that help classify the rare classes and downstream tasks. In contrast, supervised learning has no incentive to learn features irrelevant to the labels from frequent examples. We validate this hypothesis with semi-synthetic experiments as well as rigorous mathematical analyses on a simplified setting. Third, inspired by the theoretical insights, we devise a re-weighted regularization technique that  consistently improves the SSL representation quality on imbalanced datasets with several evaluation criteria, closing the small gap between balanced and imbalanced datasets with the same number of examples.",
        "pdf_link": "https://openreview.net/pdf/8dbaf8d4a30f70cb8b4967ee6b1814c513bc92e6.pdf",
        "forum_url": "https://openreview.net/forum?id=4AZz9osqrar",
        "keywords": [
            "self supervised",
            "self supervised learning",
            "supervised learning",
            "dataset imbalance"
        ]
    },
    {
        "title": "Linking Emergent and Natural Languages via Corpus Transfer",
        "authors": [
            "Shunyu Yao",
            "Mo Yu",
            "Yang Zhang",
            "Karthik R Narasimhan",
            "Joshua B. Tenenbaum",
            "Chuang Gan"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "The study of language emergence aims to understand how human languages are shaped by perceptual grounding and communicative intent. Computational approaches to emergent communication (EC) predominantly consider referential games in limited domains and analyze the learned protocol within the game framework. As a result, it remains unclear how the emergent languages from these settings connect to natural languages or provide benefits in real-world language processing tasks, where statistical models trained on large text corpora dominate. In this work, we propose a novel way to establish such a link by corpus transfer, i.e. pretraining on a corpus of emergent language for downstream natural language tasks, which is in contrast to prior work that directly transfers speaker and listener parameters. Our approach showcases non-trivial transfer benefits for two different tasks \u2013 language modeling and image captioning. For example, in a low-resource setup (modeling 2 million natural language tokens), pre-training on an emergent language corpus with just 2 million tokens reduces model perplexity by 24.6% on average across ten natural languages. We also introduce a novel metric to predict the transferability of an emergent language by translating emergent messages to natural language captions grounded on the same images. We find that our translation-based metric highly correlates with the downstream performance on modeling natural languages (for instance $\\rho = 0.83$ on Hebrew), while topographic similarity, a popular metric in previous works, shows surprisingly low correlation ($\\rho = 0.003$), hinting that simple properties like attribute disentanglement from synthetic domains might not capture the full complexities of natural language. Our findings also indicate potential benefits of moving language emergence forward with natural language resources and models.",
        "pdf_link": "https://openreview.net/pdf/1a6c8cd5798d4d7939b303c278024594ccda6968.pdf",
        "forum_url": "https://openreview.net/forum?id=49A1Y6tRhaq",
        "keywords": [
            "emergent",
            "emergent languages",
            "emergent communication",
            "natural languages",
            "referential games"
        ]
    },
    {
        "title": "Explanations of Black-Box Models based on Directional Feature Interactions",
        "authors": [
            "Aria Masoomi",
            "Davin Hill",
            "Zhonghui Xu",
            "Craig P Hersh",
            "Edwin K. Silverman",
            "Peter J. Castaldi",
            "Stratis Ioannidis",
            "Jennifer Dy"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "As machine learning algorithms are deployed ubiquitously to a variety of domains, it is imperative to make these often black-box models transparent.  Several recent works explain black-box models by capturing the most influential features for prediction per instance; such explanation methods are univariate, as they characterize importance per feature.  We extend univariate explanation to a higher-order; this enhances explainability, as bivariate methods can capture feature interactions in black-box models, represented as a directed graph.  Analyzing this graph enables us to discover groups of features that are equally important (i.e., interchangeable), while the notion of directionality allows us to identify the most influential features.  We apply our bivariate method on Shapley value explanations, and experimentally demonstrate the ability of directional explanations to discover feature interactions. We show the superiority of our method against state-of-the-art on CIFAR10, IMDB, Census, Divorce, Drug, and gene data.  ",
        "pdf_link": "https://openreview.net/pdf/861d5f0b89fc65fc7bd1c9d4a41c92e697f76061.pdf",
        "forum_url": "https://openreview.net/forum?id=45Mr7LeKR9",
        "keywords": [
            "feature interactions",
            "directional feature interactions",
            "explanations",
            "directional explanations",
            "univariate explanation",
            "explainability"
        ]
    },
    {
        "title": "Graph-Augmented Normalizing Flows for Anomaly Detection of Multiple Time Series",
        "authors": [
            "Enyan Dai",
            "Jie Chen"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Anomaly detection is a widely studied task for a broad variety of data types; among them, multiple time series appear frequently in applications, including for example, power grids and traffic networks. Detecting anomalies for multiple time series, however, is a challenging subject, owing to the intricate interdependencies among the constituent series. We hypothesize that anomalies occur in low density regions of a distribution and explore the use of normalizing flows for unsupervised anomaly detection, because of their superior quality in density estimation. Moreover, we propose a novel flow model by imposing a Bayesian network among constituent series. A Bayesian network is a directed acyclic graph (DAG) that models causal relationships; it factorizes the joint probability of the series into the product of easy-to-evaluate conditional probabilities. We call such a graph-augmented normalizing flow approach GANF and propose joint estimation of the DAG with flow parameters. We conduct extensive experiments on real-world datasets and demonstrate the effectiveness of GANF for density estimation, anomaly detection, and identification of time series distribution drift.",
        "pdf_link": "https://openreview.net/pdf/b66a4e577ef3e2caf4cb8c17ee02ca6b15eaca26.pdf",
        "forum_url": "https://openreview.net/forum?id=45L_dgP48Vd",
        "keywords": [
            "anomaly detection",
            "bayesian network",
            "density estimation",
            "time series",
            "multiple time series",
            "augmented normalizing",
            "graph augmented normalizing flows"
        ]
    },
    {
        "title": "GreaseLM: Graph REASoning Enhanced Language Models",
        "authors": [
            "Xikun Zhang",
            "Antoine Bosselut",
            "Michihiro Yasunaga",
            "Hongyu Ren",
            "Percy Liang",
            "Christopher D Manning",
            "Jure Leskovec"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Answering complex questions about textual narratives requires reasoning over both stated context and the world knowledge that underlies it. However, pretrained language models (LM), the foundation of most modern QA systems, do not robustly represent latent relationships between concepts, which is necessary for reasoning. While knowledge graphs (KG) are often used to augment LMs with structured representations of world knowledge, it remains an open question how to effectively fuse and reason over the KG representations and the language context, which provides situational constraints and nuances. In this work, we propose GreaseLM, a new model that fuses encoded representations from pretrained LMs and graph neural networks over multiple layers of modality interaction operations. Information from both modalities propagates to the other, allowing language context representations to be grounded by structured world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in the context to inform the graph representations of knowledge. Our results on three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA) and medical question answering (i.e., MedQA-USMLE) domains demonstrate that GreaseLM can more reliably answer questions that require reasoning over both situational constraints and structured knowledge, even outperforming models 8x larger.",
        "pdf_link": "https://openreview.net/pdf/1a023786aa33b14412cd0596ee9247b562f4f4fe.pdf",
        "forum_url": "https://openreview.net/forum?id=41e9o6cQPj",
        "keywords": [
            "graph reasoning",
            "knowledge graphs",
            "language models",
            "language",
            "reasoning",
            "textual narratives",
            "graph neural networks",
            "graph representations",
            "language context representations"
        ]
    },
    {
        "title": "Value Gradient weighted Model-Based Reinforcement Learning",
        "authors": [
            "Claas A Voelcker",
            "Victor Liao",
            "Animesh Garg",
            "Amir-massoud Farahmand"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Model-based reinforcement learning (MBRL) is a sample efficient technique to obtain control policies, yet unavoidable modeling errors often lead performance deterioration. The model in MBRL is often solely fitted to reconstruct dynamics, state observations in particular, while the impact of model error on the policy is not captured by the training objective. This leads to a mismatch between the intended goal of MBRL, enabling good policy and value learning, and the target of the loss function employed in practice, future state prediction. Naive intuition would suggest that value-aware model learning would fix this problem and, indeed, several solutions to this objective mismatch problem have been proposed based on theoretical analysis. However, they tend to be inferior in practice to commonly used maximum likelihood (MLE) based approaches. In this paper we propose the Value-gradient weighted Model Learning (VaGraM), a novel method for value-aware model learning which improves the performance of MBRL in challenging settings, such as small model capacity and the presence of distracting state dimensions. We analyze both MLE and value-aware approaches and demonstrate how they fail to account for exploration and the behavior of function approximation when learning value-aware models and highlight the additional goals that must be met to stabilize optimization in the deep learning setting. We verify our analysis by showing that our loss function is able to achieve high returns on the Mujoco benchmark suite while being more robust than maximum likelihood based approaches.\n",
        "pdf_link": "https://openreview.net/pdf/d924f4fd00b558974bf7f10d5b94c179c583225b.pdf",
        "forum_url": "https://openreview.net/forum?id=4-D6CZkRXxI",
        "keywords": [
            "reinforcement learning",
            "model learning",
            "model based reinforcement learning",
            "function"
        ]
    },
    {
        "title": "The Information Geometry of Unsupervised Reinforcement Learning",
        "authors": [
            "Benjamin Eysenbach",
            "Ruslan Salakhutdinov",
            "Sergey Levine"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "How can a reinforcement learning (RL) agent prepare to solve downstream tasks if those tasks are not known a priori? One approach is unsupervised skill discovery, a class of algorithms that learn a set of policies without access to a reward function. Such algorithms bear a close resemblance to representation learning algorithms (e.g., contrastive learning) in supervised learning, in that both are pretraining algorithms that maximize some approximation to a mutual information objective. While prior work has shown that the set of skills learned by such methods can accelerate downstream RL tasks, prior work offers little analysis into whether these skill learning algorithms are optimal, or even what notion of optimality would be appropriate to apply to them. In this work, we show that unsupervised skill discovery algorithms based on mutual information maximization do not learn skills that are optimal for every possible reward function. However, we show that the distribution over skills provides an optimal initialization minimizing regret against adversarially-chosen reward functions, assuming a certain type of adaptation procedure. Our analysis also provides a geometric perspective on these skill learning methods.",
        "pdf_link": "https://openreview.net/pdf/4709236cdf10497a057511e94fe99f87770c5bf6.pdf",
        "forum_url": "https://openreview.net/forum?id=3wU2UX0voE",
        "keywords": [
            "reinforcement learning",
            "information geometry",
            "mutual information maximization",
            "optimal"
        ]
    },
    {
        "title": "R5: Rule Discovery with Reinforced and Recurrent Relational Reasoning",
        "authors": [
            "Shengyao Lu",
            "Bang Liu",
            "Keith G Mills",
            "SHANGLING JUI",
            "Di Niu"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Systematicity, i.e., the ability to recombine known parts and rules to form new sequences while reasoning over relational data, is critical to machine intelligence. A model with strong systematicity is able to train on small-scale tasks and generalize to large-scale tasks. In this paper, we propose R5, a relational reasoning framework based on reinforcement learning that reasons over relational graph data and explicitly mines underlying compositional logical rules from observations. R5 has strong systematicity and being robust to noisy data. It consists of a policy value network equipped with Monte Carlo Tree Search to perform recurrent relational prediction and a backtrack rewriting mechanism for rule mining. By alternately applying the two components, R5 progressively learns a set of explicit rules from data and performs explainable and generalizable relation prediction. We conduct extensive evaluations on multiple datasets. Experimental results show that R5 outperforms various embedding-based and rule induction baselines on relation prediction tasks while achieving a high recall rate in discovering ground truth rules. ",
        "pdf_link": "https://openreview.net/pdf/08116e08b73b5c728213b5d350ddbbcf4154bb9f.pdf",
        "forum_url": "https://openreview.net/forum?id=2eXhNpHeW6E",
        "keywords": [
            "rule discovery",
            "systematicity",
            "recurrent",
            "recurrent relational reasoning",
            "reinforcement learning",
            "generalizable relation prediction",
            "rule mining",
            "generalize"
        ]
    },
    {
        "title": "Escaping limit cycles: Global convergence for constrained nonconvex-nonconcave minimax problems",
        "authors": [
            "Thomas Pethick",
            "Puya Latafat",
            "Panos Patrinos",
            "Olivier Fercoq",
            "Volkan Cevher"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "This paper introduces a new extragradient-type algorithm for a class of nonconvex-nonconcave minimax problems. It is well-known that finding a local solution for general minimax problems is computationally intractable. This observation has recently motivated the study of structures sufficient for convergence of first order methods in the more general setting of variational inequalities when the so-called weak Minty variational inequality (MVI) holds. This problem class captures non-trivial structures as we demonstrate with examples, for which a large family of existing algorithms provably converge to limit cycles. Our results require a less restrictive parameter range in the weak MVI compared to what is previously known, thus extending the applicability of our scheme. The proposed algorithm is applicable to constrained and regularized problems, and involves an adaptive stepsize allowing for potentially larger stepsizes. Our scheme also converges globally even in settings where the underlying operator exhibits limit cycles.",
        "pdf_link": "https://openreview.net/pdf/2b1d67d765bf24cb72760db6f0bcaff4cefc8032.pdf",
        "forum_url": "https://openreview.net/forum?id=2_vhkAMARk",
        "keywords": [
            "convergence",
            "global convergence",
            "limit cycles",
            "minimax problems",
            "variational inequalities",
            "escaping limit cycles",
            "weak minty variational inequality",
            "first order methods"
        ]
    },
    {
        "title": "Adversarial Support Alignment",
        "authors": [
            "Shangyuan Tong",
            "Timur Garipov",
            "Yang Zhang",
            "Shiyu Chang",
            "Tommi S. Jaakkola"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "We study the problem of aligning the supports of distributions. Compared to the existing work on distribution alignment, support alignment does not require the densities to be matched. We propose symmetric support difference as a divergence measure to quantify the mismatch between supports. We show that select discriminators (e.g. discriminator trained for Jensen-Shannon divergence) are able to map support differences as support differences in their one-dimensional output space. Following this result, our method aligns supports by minimizing a symmetrized relaxed optimal transport cost in the discriminator 1D space via an adversarial process. Furthermore, we show that our approach can be viewed as a limit of existing notions of alignment by increasing transportation assignment tolerance. We quantitatively evaluate the method across domain adaptation tasks with shifts in label distributions. Our experiments show that the proposed method is more robust against these shifts than other alignment-based baselines.",
        "pdf_link": "https://openreview.net/pdf/baf81164820438550e81b120efdf1f7f96cd349d.pdf",
        "forum_url": "https://openreview.net/forum?id=26gKg6x-ie",
        "keywords": [
            "aligns",
            "support alignment",
            "distribution alignment",
            "adversarial support alignment"
        ]
    },
    {
        "title": "Learning Multimodal VAEs through Mutual Supervision",
        "authors": [
            "Tom Joy",
            "Yuge Shi",
            "Philip Torr",
            "Tom Rainforth",
            "Sebastian M Schmon",
            "Siddharth N"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Multimodal VAEs seek to model the joint distribution over heterogeneous data (e.g.\\ vision, language), whilst also capturing a shared representation across such modalities. Prior work has typically combined information from the modalities by reconciling idiosyncratic representations directly in the recognition model through explicit products, mixtures, or other such factorisations. Here we introduce a novel alternative, the MEME, that avoids such explicit combinations by repurposing semi-supervised VAEs to combine information between modalities implicitly through mutual supervision. This formulation naturally allows learning from partially-observed data where some modalities can be entirely missing---something that most existing approaches either cannot handle, or do so to a limited extent. We demonstrate that MEME outperforms baselines on standard metrics across both partial and complete observation schemes on the MNIST-SVHN (image--image) and CUB (image--text) datasets. We also contrast the quality of the representations learnt by mutual supervision against standard approaches and observe interesting trends in its ability to capture relatedness between data.",
        "pdf_link": "https://openreview.net/pdf/6e3f8005627ed71e89af82b1e6d063771b707c3e.pdf",
        "forum_url": "https://openreview.net/forum?id=1xXvPrAshao",
        "keywords": [
            "mutual supervision"
        ]
    },
    {
        "title": "Filtered-CoPhy: Unsupervised Learning of Counterfactual Physics in Pixel Space",
        "authors": [
            "Steeven JANNY",
            "Fabien Baradel",
            "Natalia Neverova",
            "Madiha Nadri",
            "Greg Mori",
            "Christian Wolf"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Learning causal relationships in high-dimensional data (images, videos) is a hard task, as they are often defined on low dimensional manifolds and must be extracted from complex signals dominated by appearance, lighting, textures and also spurious correlations in the data. We present a method for learning counterfactual reasoning of physical processes in pixel space, which requires the prediction of the impact of interventions on initial conditions. Going beyond the identification of structural relationships, we deal with the challenging problem of forecasting raw video over long horizons. Our method does not require the knowledge or supervision of any ground truth positions or other object or scene properties. Our model learns and acts on a suitable hybrid latent representation based on a combination of dense features, sets of 2D keypoints and an additional latent vector per keypoint. We show that this better captures the dynamics of physical processes than purely dense or sparse representations. We introduce a new challenging and carefully designed counterfactual benchmark for predictions in pixel space and outperform strong baselines in physics-inspired ML and video prediction.",
        "pdf_link": "https://openreview.net/pdf/cbd75b662eaa377753b892113b221d062f26511e.pdf",
        "forum_url": "https://openreview.net/forum?id=1L0C5ROtFp",
        "keywords": [
            "counterfactual reasoning",
            "cophy",
            "counterfactual physics",
            "unsupervised learning"
        ]
    },
    {
        "title": "Source-Free Adaptation to Measurement Shift via Bottom-Up Feature Restoration",
        "authors": [
            "Cian Eastwood",
            "Ian Mason",
            "Chris Williams",
            "Bernhard Sch\u00f6lkopf"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Source-free domain adaptation (SFDA) aims to adapt a model trained on labelled data in a source domain to unlabelled data in a target domain without access to the source-domain data during adaptation. Existing methods for SFDA leverage entropy-minimization techniques which: (i) apply only to classification; (ii) destroy model calibration; and (iii) rely on the source model achieving a good level of feature-space class-separation in the target domain. We address these issues for a particularly pervasive type of domain shift called measurement shift which can be resolved by restoring the source features rather than extracting new ones. In particular, we propose Feature Restoration (FR) wherein we: (i) store a lightweight and flexible approximation of the feature distribution under the source data; and (ii) adapt the feature-extractor such that the approximate feature distribution under the target data realigns with that saved on the source. We additionally propose a bottom-up training scheme which boosts performance, which we call Bottom-Up Feature Restoration (BUFR). On real and synthetic data, we demonstrate that BUFR outperforms existing SFDA methods in terms of accuracy, calibration, and data efficiency, while being less reliant on the performance of the source model in the target domain.\n",
        "pdf_link": "https://openreview.net/pdf/aeb9d0c4c2be949d33ea83ddad8547e536405144.pdf",
        "forum_url": "https://openreview.net/forum?id=1JDiK_TbV4S",
        "keywords": [
            "feature restoration",
            "source free domain adaptation",
            "source free adaptation",
            "domain shift",
            "measurement shift",
            "bottom up feature restoration"
        ]
    },
    {
        "title": "Unifying Likelihood-free Inference with Black-box Optimization and Beyond",
        "authors": [
            "Dinghuai Zhang",
            "Jie Fu",
            "Yoshua Bengio",
            "Aaron Courville"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Black-box optimization formulations for biological sequence design have drawn recent attention due to their promising potential impact on the pharmaceutical industry. In this work, we propose to unify two seemingly distinct worlds: likelihood-free inference and black-box optimization, under one probabilistic framework. In tandem, we provide a recipe for constructing various sequence design methods based on this framework. We show how previous optimization approaches can be \"reinvented\" in our framework, and further propose new probabilistic black-box optimization algorithms. Extensive experiments on sequence design application illustrate the benefits of the proposed methodology.",
        "pdf_link": "https://openreview.net/pdf/e2ec346ff6de5e9270bf7e826ba6ff87f1b8055b.pdf",
        "forum_url": "https://openreview.net/forum?id=1HxTO6CTkz",
        "keywords": [
            "black box optimization",
            "likelihood free inference"
        ]
    },
    {
        "title": "Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models",
        "authors": [
            "Fan Bao",
            "Chongxuan Li",
            "Jun Zhu",
            "Bo Zhang"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Diffusion probabilistic models (DPMs) represent a class of powerful generative models. Despite their success, the inference of DPMs is expensive since it generally needs to iterate over thousands of timesteps. A key problem in the inference is to estimate the variance in each timestep of the reverse process. In this work, we present a surprising result that both the optimal reverse variance and the corresponding optimal KL divergence of a DPM have analytic forms w.r.t. its score function. Building upon it, we propose \\textit{Analytic-DPM}, a training-free inference framework that estimates the analytic forms of the variance and KL divergence using the Monte Carlo method and a pretrained score-based model. Further, to correct the potential bias caused by the score-based model, we derive both lower and upper bounds of the optimal variance and clip the estimate for a better result. Empirically, our analytic-DPM improves the log-likelihood of various DPMs, produces high-quality samples, and meanwhile enjoys a $20\\times$ to $80\\times$ speed up.",
        "pdf_link": "https://openreview.net/pdf/541cdc9e000367bb0bd3fc42201573ed434094c8.pdf",
        "forum_url": "https://openreview.net/forum?id=0xiJLKH-ufZ",
        "keywords": [
            "dpm",
            "variance",
            "diffusion probabilistic models",
            "reverse variance",
            "optimal reverse variance",
            "analytic dpm",
            "analytic estimate"
        ]
    },
    {
        "title": "Towards a Unified View of Parameter-Efficient Transfer Learning",
        "authors": [
            "Junxian He",
            "Chunting Zhou",
            "Xuezhe Ma",
            "Taylor Berg-Kirkpatrick",
            "Graham Neubig"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Fine-tuning large pretrained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the parameters of the pretrained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pretrained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks.",
        "pdf_link": "https://openreview.net/pdf/859577d9cab3bf7833fe6fd6ea3a66c3b424c6bb.pdf",
        "forum_url": "https://openreview.net/forum?id=0RDcd5Axok",
        "keywords": [
            "transfer learning",
            "efficient transfer learning",
            "parameter efficient transfer learning"
        ]
    },
    {
        "title": "Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting",
        "authors": [
            "Shizhan Liu",
            "Hang Yu",
            "Cong Liao",
            "Jianguo Li",
            "Weiyao Lin",
            "Alex X. Liu",
            "Schahram Dustdar"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "Accurate prediction of the future given the past based on time series data is of paramount importance, since it opens the door for decision making and risk management ahead of time. In practice, the challenge is to build a flexible but parsimonious model that can capture a wide range of temporal dependencies. In this paper, we propose Pyraformer by exploring the multiresolution representation of the time series. Specifically, we introduce the pyramidal attention module (PAM) in which the inter-scale tree structure summarizes features at different resolutions and the intra-scale neighboring connections model the temporal dependencies of different ranges. Under mild conditions, the maximum length of the signal traversing path in Pyraformer is a constant (i.e., $\\mathcal O(1)$) with regard to the sequence length $L$, while its time and space complexity scale linearly with $L$. Extensive numerical results show that Pyraformer typically achieves the highest prediction accuracy in both single-step and long-range forecasting tasks with the least amount of time and memory consumption, especially when the sequence is long.",
        "pdf_link": "https://openreview.net/pdf/2ac159853cd001bbca6a8a12da497c8013914b31.pdf",
        "forum_url": "https://openreview.net/forum?id=0EXmFzUn5I",
        "keywords": [
            "pyramidal attention",
            "pyraformer"
        ]
    },
    {
        "title": "Finite-Time Convergence and Sample Complexity of Multi-Agent Actor-Critic Reinforcement Learning with Average Reward",
        "authors": [
            "FNU Hairi",
            "Jia Liu",
            "Songtao Lu"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "In this paper, we establish the first finite-time convergence result of the actor-critic algorithm for fully decentralized multi-agent reinforcement learning (MARL) problems with average reward. \nIn this problem, a set of $N$ agents work cooperatively to maximize the global average reward through interacting with their neighbors over a communication network.\nWe consider a practical MARL setting, where the rewards and actions of each agent are only known to itself, and the knowledge of joint actions of the agents is not assumed. \nToward this end, we propose a mini-batch Markovian sampled fully decentralized actor-critic algorithm and analyze its finite-time convergence and sample complexity.\nWe show that the sample complexity of this algorithm is $\\mathcal{O}(N^{2}/\\epsilon^{2}\\log(N/\\epsilon))$.\nInterestingly, this sample complexity bound matches that of the state-of-the-art single-agent actor-critic algorithms for reinforcement learning. ",
        "pdf_link": "https://openreview.net/pdf/88771b44b2b7e3d534226c24b2a2e7d9739fc960.pdf",
        "forum_url": "https://openreview.net/forum?id=04pGUg0-pdZ",
        "keywords": [
            "sample complexity",
            "reinforcement learning",
            "actor critic reinforcement learning",
            "finite time convergence",
            "actor critic",
            "average reward"
        ]
    },
    {
        "title": "Online Hyperparameter Meta-Learning with Hypergradient Distillation",
        "authors": [
            "Hae Beom Lee",
            "Hayeon Lee",
            "JaeWoong Shin",
            "Eunho Yang",
            "Timothy Hospedales",
            "Sung Ju Hwang"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "Many gradient-based meta-learning methods assume a set of parameters that do not participate in inner-optimization, which can be considered as hyperparameters. Although such hyperparameters can be optimized using the existing gradient-based hyperparameter optimization (HO) methods, they suffer from the following issues. Unrolled differentiation methods do not scale well to high-dimensional hyperparameters or horizon length, Implicit Function Theorem (IFT) based methods are restrictive for online optimization, and short horizon approximations suffer from short horizon bias. In this work, we propose a novel HO method that can overcome these limitations, by approximating the second-order term with knowledge distillation. Specifically, we parameterize a single Jacobian-vector product (JVP) for each HO step and minimize the distance from the true second-order term. Our method allows online optimization and also is scalable to the hyperparameter dimension and the horizon length. We demonstrate the effectiveness of our method on three different meta-learning methods and two benchmark datasets.",
        "pdf_link": "https://openreview.net/pdf/e1a0b8a3fc38e7d1b03adaf5cd0f110f66568bab.pdf",
        "forum_url": "https://openreview.net/forum?id=01AMRlen9wJ",
        "keywords": [
            "hypergradient distillation",
            "meta learning",
            "online hyperparameter meta learning",
            "hyperparameters",
            "optimization",
            "online optimization"
        ]
    },
    {
        "title": "Fairness in Representation for Multilingual NLP: Insights from Controlled Experiments on Conditional Language Modeling",
        "authors": [
            "Ada Wan"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "We perform systematically and fairly controlled experiments with the 6-layer Transformer to investigate  the hardness in conditional-language-modeling languages which have been traditionally considered morphologically rich (AR and RU) and poor (ZH). We evaluate through statistical comparisons across 30 possible language directions from the 6 languages of the United Nations Parallel Corpus across 5 data sizes on 3 representation levels --- character, byte, and word. Results show that performance is relative to the representation granularity of each of the languages, not to the language as a whole. On the character and byte levels, we are able to eliminate statistically significant performance disparity, hence demonstrating that a language cannot be intrinsically hard. The disparity that mirrors the morphological complexity hierarchy is shown to be a byproduct of word segmentation. Evidence from data statistics, along with the fact that word segmentation is qualitatively indeterminate, renders a decades-long debate on morphological complexity (unless it is being intentionally modeled in a word-based, meaning-driven context) irrelevant in the context of computing. The intent of our work is to help effect more objectivity and adequacy in evaluation as well as fairness and inclusivity in experimental setup in the area of language and computing so to uphold diversity in Machine Learning and Artificial Intelligence research. Multilinguality is real and relevant in computing not due to canonical, structural linguistic concepts such as morphology or \"words\" in our minds, but rather standards related to internationalization and localization, such as character encoding --- something which has thus far been sorely overlooked in our discourse and curricula. ",
        "pdf_link": "https://openreview.net/pdf/1bc81aec7b25823dcaab95c24c1c5c0779bd3c7c.pdf",
        "forum_url": "https://openreview.net/forum?id=-llS6TiOew",
        "keywords": [
            "conditional language modeling",
            "word segmentation",
            "fairness"
        ]
    },
    {
        "title": "SHINE: SHaring the INverse Estimate from the forward pass for bi-level optimization and implicit models",
        "authors": [
            "Zaccharie Ramzi",
            "Florian Mannel",
            "Shaojie Bai",
            "Jean-Luc Starck",
            "Philippe Ciuciu",
            "Thomas Moreau"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "In recent years, implicit deep learning has emerged as a method to increase the depth of deep neural networks. While their training is memory-efficient, they are still significantly slower to train than their explicit counterparts. In Deep Equilibrium Models~(DEQs), the training is performed as a bi-level problem, and its computational complexity is partially driven by the iterative inversion of a huge Jacobian matrix. In this paper, we propose a novel strategy to tackle this computational bottleneck from which many bi-level problems suffer. The main idea is to use the quasi-Newton matrices from the forward pass to efficiently approximate the inverse Jacobian matrix in the direction needed for the gradient computation. We provide a theorem that motivates using our method with the original forward algorithms. In addition, by modifying these forward algorithms, we further provide theoretical guarantees that our method asymptotically estimates the true implicit gradient. We empirically study this approach in many settings, ranging from hyperparameter optimization to large Multiscale DEQs applied to CIFAR and ImageNet. We show that it reduces the computational cost of the backward pass by up to two orders of magnitude. All this is achieved while retaining the excellent performance of the original models in hyperparameter optimization and on CIFAR, and giving encouraging and competitive results on ImageNet.",
        "pdf_link": "https://openreview.net/pdf/d20ddecab60635b82dae5e9ac637ccb24c8038fc.pdf",
        "forum_url": "https://openreview.net/forum?id=-ApAkox5mp",
        "keywords": [
            "bi level optimization",
            "hyperparameter optimization",
            "implicit models"
        ]
    },
    {
        "title": "Hyperparameter Tuning with Renyi Differential Privacy",
        "authors": [
            "Nicolas Papernot",
            "Thomas Steinke"
        ],
        "published": "ICLR 2022 Oral",
        "summary": "For many differentially private algorithms, such as the prominent noisy stochastic gradient descent (DP-SGD), the analysis needed to bound the privacy leakage of a single training run is well understood. However, few studies have reasoned about the privacy leakage resulting from the multiple training runs needed to fine tune the value of the training algorithm\u2019s hyperparameters. In this work, we first illustrate how simply setting hyperparameters based on non-private training runs can leak private information. Motivated by this observation, we then provide privacy guarantees for hyperparameter search procedures within the framework of Renyi Differential Privacy. Our results improve and extend the work of Liu and Talwar (STOC 2019). Our analysis supports our previous observation that tuning hyperparameters does indeed leak private information, but we prove that, under certain assumptions, this leakage is modest, as long as each candidate training run needed to select hyperparameters is itself differentially private.",
        "pdf_link": "https://openreview.net/pdf/8832d0e112b9fd6c5c8f0be8a093625e4de6e337.pdf",
        "forum_url": "https://openreview.net/forum?id=-70L8lpp9DF",
        "keywords": [
            "hyperparameters",
            "differentially private",
            "privacy",
            "differentially private algorithms",
            "hyperparameter tuning",
            "tuning hyperparameters",
            "hyperparameter search",
            "renyi differential privacy",
            "privacy leakage"
        ]
    }
]