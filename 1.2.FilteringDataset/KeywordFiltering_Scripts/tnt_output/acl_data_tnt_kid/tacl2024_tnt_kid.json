[
    {
        "title": "AmbiFC: Fact-Checking Ambiguous Claims with Evidence",
        "authors": [
            "Max Glockner",
            "Ieva Stali\u016bnait\u0117",
            "James Thorne",
            "Gisela Vallejo",
            "Andreas Vlachos",
            "Iryna Gurevych"
        ],
        "published": "2024",
        "summary": "Automated fact-checking systems verify claims against evidence to predict their veracity. In real-world scenarios, the retrieved evidence may not unambiguously support or refute the claim and yield conflicting but valid interpretations. Existing fact-checking datasets assume that the models developed with them predict a single veracity label for each claim, thus discouraging the handling of such ambiguity. To address this issue we present AmbiFC,1 a fact-checking dataset with 10k claims derived from real-world information needs. It contains fine-grained evidence annotations of 50k passages from 5k Wikipedia pages. We analyze the disagreements arising from ambiguity when comparing claims against evidence in AmbiFC, observing a strong correlation of annotator disagreement with linguistic phenomena such as underspecification and probabilistic reasoning. We develop models for predicting veracity handling this ambiguity via soft labels, and find that a pipeline that learns the label distribution for sentence-level evidence selection and veracity prediction yields the best performance. We compare models trained on different subsets of AmbiFC and show that models trained on the ambiguous instances perform better when faced with the identified linguistic phenomena.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.1.pdf",
        "keywords": [
            "veracity",
            "ambiguity",
            "fact checking dataset",
            "automated fact checking systems",
            "soft labels"
        ]
    },
    {
        "title": "Language Varieties of Italy: Technology Challenges and Opportunities",
        "authors": [
            "Alan Ramponi"
        ],
        "published": "2024",
        "summary": "Italy is characterized by a one-of-a-kind linguistic diversity landscape in Europe, which implicitly encodes local knowledge, cultural traditions, artistic expressions, and history of its speakers. However, most local languages and dialects in Italy are at risk of disappearing within a few generations. The NLP community has recently begun to engage with endangered languages, including those of Italy. Yet, most efforts assume that these varieties are under-resourced language monoliths with an established written form and homogeneous functions and needs, and thus highly interchangeable with each other and with high-resource, standardized languages. In this paper, we introduce the linguistic context of Italy and challenge the default machine-centric assumptions of NLP for Italy\u2019s language varieties. We advocate for a shift in the paradigm from machine-centric to speaker-centric NLP, and provide recommendations and opportunities for work that prioritizes languages and their speakers over technological advances. To facilitate the process, we finally propose building a local community towards responsible, participatory efforts aimed at supporting vitality of languages and dialects of Italy.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.2.pdf",
        "keywords": [
            "italy",
            "varieties",
            "language",
            "standardized languages",
            "technology challenges"
        ]
    },
    {
        "title": "Benchmarking Large Language Models for News Summarization",
        "authors": [
            "Tianyi Zhang",
            "Faisal Ladhak",
            "Esin Durmus",
            "Percy Liang",
            "Kathleen McKeown",
            "Tatsunori B. Hashimoto"
        ],
        "published": "2024",
        "summary": "Large language models (LLMs) have shown promise for automatic summarization but the reasons behind their successes are poorly understood. By conducting a human evaluation on ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations. First, we find instruction tuning, not model size, is the key to the LLM\u2019s zero-shot summarization capability. Second, existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance. To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers. Despite major stylistic differences such as the amount of paraphrasing, we find that LLM summaries are judged to be on par with human written summaries.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.3.pdf",
        "keywords": [
            "summarization",
            "perform",
            "large language models",
            "benchmarking large language models"
        ]
    },
    {
        "title": "mGPT: Few-Shot Learners Go Multilingual",
        "authors": [
            "Oleh Shliazhko",
            "Alena Fenogenova",
            "Maria Tikhonova",
            "Anastasia Kozlova",
            "Vladislav Mikhailov",
            "Tatiana Shavrina"
        ],
        "published": "2024",
        "summary": "This paper introduces mGPT, a multilingual variant of GPT-3, pretrained on 61 languages from 25 linguistically diverse language families using Wikipedia and the C4 Corpus. We detail the design and pretraining procedure. The models undergo an intrinsic and extrinsic evaluation: language modeling in all languages, downstream evaluation on cross-lingual NLU datasets and benchmarks in 33 languages, and world knowledge probing in 23 languages. The in-context learning abilities are on par with the contemporaneous language models while covering a larger number of languages, including underrepresented and low-resource languages of the Commonwealth of Independent States and the indigenous peoples in Russia. The source code and the language models are publicly available under the MIT license.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.4.pdf",
        "keywords": [
            "language models"
        ]
    },
    {
        "title": "Cultural Adaptation of Recipes",
        "authors": [
            "Yong Cao",
            "Yova Kementchedjhieva",
            "Ruixiang Cui",
            "Antonia Karamolegkou",
            "Li Zhou",
            "Megan Dare",
            "Lucia Donatelli",
            "Daniel Hershcovich"
        ],
        "published": "2024",
        "summary": "Building upon the considerable advances in Large Language Models (LLMs), we are now equipped to address more sophisticated tasks demanding a nuanced understanding of cross-cultural contexts. A key example is recipe adaptation, which goes beyond simple translation to include a grasp of ingredients, culinary techniques, and dietary preferences specific to a given culture. We introduce a new task involving the translation and cultural adaptation of recipes between Chinese- and English-speaking cuisines. To support this investigation, we present CulturalRecipes, a unique dataset composed of automatically paired recipes written in Mandarin Chinese and English. This dataset is further enriched with a human-written and curated test set. In this intricate task of cross-cultural recipe adaptation, we evaluate the performance of various methods, including GPT-4 and other LLMs, traditional machine translation, and information retrieval techniques. Our comprehensive analysis includes both automatic and human evaluation metrics. While GPT-4 exhibits impressive abilities in adapting Chinese recipes into English, it still lags behind human expertise when translating English recipes into Chinese. This underscores the multifaceted nature of cultural adaptations. We anticipate that these insights will significantly contribute to future research on culturally aware language models and their practical application in culturally diverse contexts.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.5.pdf",
        "keywords": [
            "adaptation",
            "recipe adaptation",
            "recipes",
            "ingredients",
            "cultural adaptation",
            "cuisines"
        ]
    },
    {
        "title": "Metric-Free Learning Network with Dual Relations Propagation for Few-Shot Aspect Category Sentiment Analysis",
        "authors": [
            "Shiman Zhao",
            "Yutao Xie",
            "Wei Chen",
            "Tengjiao Wang",
            "Jiahui Yao",
            "Jiabin Zheng"
        ],
        "published": "2024",
        "summary": "Few-shot Aspect Category Sentiment Analysis (ACSA) is a crucial task for aspect-based sentiment analysis, which aims to detect sentiment polarity for a given aspect category in a sentence with limited data. However, few-shot learning methods focus on distance metrics between the query and support sets to classify queries, heavily relying on aspect distributions in the embedding space. Thus, they suffer from overlapping distributions of aspect embeddings caused by irrelevant sentiment noise among sentences with multiple sentiment aspects, leading to misclassifications. To solve the above issues, we propose a metric-free method for few-shot ACSA, which models the associated relations among the aspects of support and query sentences by Dual Relations Propagation (DRP), addressing the passive effect of overlapping distributions. Specifically, DRP uses the dual relations (similarity and diversity) among the aspects of support and query sentences to explore intra-cluster commonality and inter-cluster uniqueness for alleviating sentiment noise and enhancing aspect features. Additionally, the dual relations are transformed from support-query to class-query to promote query inference by learning class knowledge. Experiments show that we achieve convincing performance on few-shot ACSA, especially an average improvement of 2.93% accuracy and 2.10% F1 score in the 3-way 1-shot setting.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.6.pdf",
        "keywords": [
            "dual relations propagation",
            "dual relations",
            "aspect category sentiment analysis",
            "sentiment polarity",
            "aspect based sentiment analysis",
            "metric free learning network"
        ]
    },
    {
        "title": "Addressing the Binning Problem in Calibration Assessment through Scalar Annotations",
        "authors": [
            "Zhengping Jiang",
            "Anqi Liu",
            "Benjamin Van Durme"
        ],
        "published": "2024",
        "summary": "Computational linguistics models commonly target the prediction of discrete\u2014categorical\u2014labels. When assessing how well-calibrated these model predictions are, popular evaluation schemes require practitioners to manually determine a binning scheme: grouping labels into bins to approximate true label posterior. The problem is that these metrics are sensitive to binning decisions. We consider two solutions to the binning problem that apply at the stage of data annotation: collecting either distributed (redundant) labels or direct scalar value assignment. In this paper, we show that although both approaches address the binning problem by evaluating instance-level calibration, direct scalar assignment is significantly more cost-effective. We provide theoretical analysis and empirical evidence to support our proposal for dataset creators to adopt scalar annotation protocols to enable a higher-quality assessment of model calibration.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.7.pdf",
        "keywords": [
            "calibrated",
            "binning",
            "calibration assessment",
            "assessment",
            "model calibration",
            "annotation"
        ]
    },
    {
        "title": "An Energy-based Model for Word-level AutoCompletion in Computer-aided Translation",
        "authors": [
            "Cheng Yang",
            "Guoping Huang",
            "Mo Yu",
            "Zhirui Zhang",
            "Siheng Li",
            "Mingming Yang",
            "Shuming Shi",
            "Yujiu Yang",
            "Lemao Liu"
        ],
        "published": "2024",
        "summary": "Word-level AutoCompletion (WLAC) is a rewarding yet challenging task in Computer-aided Translation. Existing work addresses this task through a classification model based on a neural network that maps the hidden vector of the input context into its corresponding label (i.e., the candidate target word is treated as a label). Since the context hidden vector itself does not take the label into account and it is projected to the label through a linear classifier, the model cannot sufficiently leverage valuable information from the source sentence as verified in our experiments, which eventually hinders its overall performance. To alleviate this issue, this work proposes an energy-based model for WLAC, which enables the context hidden vector to capture crucial information from the source sentence. Unfortunately, training and inference suffer from efficiency and effectiveness challenges, therefore we employ three simple yet effective strategies to put our model into practice. Experiments on four standard benchmarks demonstrate that our reranking-based approach achieves substantial improvements (about 6.07%) over the previous state-of-the-art model. Further analyses show that each strategy of our approach contributes to the final performance.1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.8.pdf",
        "keywords": [
            "autocompletion",
            "computer aided translation",
            "word level autocompletion"
        ]
    },
    {
        "title": "Lost in the Middle: How Language Models Use Long Contexts",
        "authors": [
            "Nelson F. Liu",
            "Kevin Lin",
            "John Hewitt",
            "Ashwin Paranjape",
            "Michele Bevilacqua",
            "Fabio Petroni",
            "Percy Liang"
        ],
        "published": "2024",
        "summary": "While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.9.pdf",
        "keywords": [
            "context",
            "long contexts",
            "long context language models",
            "language models",
            "long context models",
            "language models use",
            "relevant information",
            "use"
        ]
    },
    {
        "title": "Red Teaming Language Model Detectors with Language Models",
        "authors": [
            "Zhouxing Shi",
            "Yihan Wang",
            "Fan Yin",
            "Xiangning Chen",
            "Kai-Wei Chang",
            "Cho-Jui Hsieh"
        ],
        "published": "2024",
        "summary": "The prevalence and strong capability of large language models (LLMs) present significant safety and ethical risks if exploited by malicious users. To prevent the potentially deceptive usage of LLMs, recent work has proposed algorithms to detect LLM-generated text and protect LLMs. In this paper, we investigate the robustness and reliability of these LLM detectors under adversarial attacks. We study two types of attack strategies: 1) replacing certain words in an LLM\u2019s output with their synonyms given the context; 2) automatically searching for an instructional prompt to alter the writing style of the generation. In both strategies, we leverage an auxiliary LLM to generate the word replacements or the instructional prompt. Different from previous works, we consider a challenging setting where the auxiliary LLM can also be protected by a detector. Experiments reveal that our attacks effectively compromise the performance of all detectors in the study with plausible generations, underscoring the urgent need to improve the robustness of LLM-generated text detection systems. Code is available at https://github.com/shizhouxing/LLM-Detector-Robustness.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.10.pdf",
        "keywords": [
            "language models"
        ]
    },
    {
        "title": "Text Attribute Control via Closed-Loop Disentanglement",
        "authors": [
            "Lei Sha",
            "Thomas Lukasiewicz"
        ],
        "published": "2024",
        "summary": "Changing an attribute of a text without changing the content usually requires first disentangling the text into irrelevant attributes and content representations. After that, in the inference phase, the representation of one attribute is tuned to a different value, expecting that the corresponding attribute of the text can also be changed accordingly. The usual way of disentanglement is to add some constraints on the latent space of an encoder-decoder architecture, including adversarial-based constraints and mutual-information-based constraints. However, previous semi-supervised processes of attribute change are usually not enough to guarantee the success of attribute change and content preservation. In this paper, we propose a novel approach to achieve a robust control of attributes while enhancing content preservation. In this approach, we use a semi-supervised contrastive learning method to encourage the disentanglement of attributes in latent spaces. Differently from previous works, we re-disentangle the reconstructed sentence and compare the re-disentangled latent space with the original latent space, which makes a closed-loop disentanglement process. This also helps content preservation. In addition, the contrastive learning method is also able to replace the role of minimizing mutual information and adversarial training in the disentanglement process, which alleviates the computation cost. We conducted experiments on three text datasets, including the Yelp Service review dataset, the Amazon Product review dataset, and the GoEmotions dataset. The experimental results show the effectiveness of our model.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.11.pdf",
        "keywords": [
            "mutual information",
            "disentanglement",
            "content preservation",
            "text attribute control",
            "closed loop disentanglement",
            "disentanglement process"
        ]
    },
    {
        "title": "Unifying Structured Data as Graph for Data-to-Text Pre-Training",
        "authors": [
            "Shujie Li",
            "Liang Li",
            "Ruiying Geng",
            "Min Yang",
            "Binhua Li",
            "Guanghu Yuan",
            "Wanwei He",
            "Shao Yuan",
            "Can Ma",
            "Fei Huang",
            "Yongbin Li"
        ],
        "published": "2024",
        "summary": "Data-to-text (D2T) generation aims to transform structured data into natural language text. Data-to-text pre-training has proved to be powerful in enhancing D2T generation and yields impressive performance. However, previous pre-training methods either oversimplified structured data into a sequence without considering input structures or designed training objectives tailored for a specific data structure (e.g., table or knowledge graph). In this paper, we unify different types of structured data (i.e., table, key-value data, knowledge graph) into the graph format and cast different D2T generation tasks as graph-to-text generation. To effectively exploit the structural information of the input graph, we propose a structure-enhanced pre-training method for D2T generation by designing a structure-enhanced Transformer. Concretely, we devise a position matrix for the Transformer, encoding relative positional information of connected nodes in the input graph. In addition, we propose a new attention matrix to incorporate graph structures into the original Transformer by taking the available explicit connectivity structure into account. Extensive experiments on six benchmark datasets show the effectiveness of our model. Our source codes are available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/unid2t.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.12.pdf",
        "keywords": [
            "graph",
            "knowledge graph",
            "pre training",
            "text pre training",
            "data",
            "structured data"
        ]
    },
    {
        "title": "Exploring Human-Like Translation Strategy with Large Language Models",
        "authors": [
            "Zhiwei He",
            "Tian Liang",
            "Wenxiang Jiao",
            "Zhuosheng Zhang",
            "Yujiu Yang",
            "Rui Wang",
            "Zhaopeng Tu",
            "Shuming Shi",
            "Xing Wang"
        ],
        "published": "2024",
        "summary": "Large language models (LLMs) have demonstrated impressive capabilities in general scenarios, exhibiting a level of aptitude that approaches, in some aspects even surpasses, human-level intelligence. Among their numerous skills, the translation abilities of LLMs have received considerable attention. Compared to typical machine translation that focuses solely on source-to-target mapping, LLM-based translation can potentially mimic the human translation process, which might take preparatory steps to ensure high-quality translation. This work explores this possibility by proposing the MAPS framework, which stands for Multi-Aspect Prompting and Selection. Specifically, we enable LLMs first to analyze the given source sentence and induce three aspects of translation-related knowledge (keywords, topics, and relevant demonstrations) to guide the final translation process. Moreover, we employ a selection mechanism based on quality estimation to filter out noisy and unhelpful knowledge. Both automatic (3 LLMs \u00d7 11 directions \u00d7 2 automatic metrics) and human evaluation (preference study and MQM) demonstrate the effectiveness of MAPS. Further analysis shows that by mimicking the human translation process, MAPS reduces various translation errors such as hallucination, ambiguity, mistranslation, awkward style, untranslated text, and omission. Source code is available at https://github.com/zwhe99/MAPS-mt.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.13.pdf",
        "keywords": [
            "language models",
            "translation",
            "large language models",
            "human translation process",
            "maps"
        ]
    },
    {
        "title": "Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering",
        "authors": [
            "Dingmin Wang",
            "Qiuyuan Huang",
            "Matthew Jackson",
            "Jianfeng Gao"
        ],
        "published": "2024",
        "summary": "An open-domain question answering (QA) system usually follows a retrieve-then-read paradigm, in which a retriever is used to retrieve relevant passages from a large corpus, and then a reader generates answers based on the retrieved passages and the original question. In this paper, we propose a simple and novel mutual learning framework to improve the performance of retrieve-then-read-style models via an intermediate module named the knowledge selector, which we train with reinforcement learning. The key benefits of our proposed intermediate module are: 1) no requirement for additional annotated question-passage pairs; 2) improvements in both retrieval and QA performance, as well as computational efficiency, compared to prior competitive retrieve-then-read models; 3) with no finetuning, improvement in the zero-shot performance of large-scale pre-trained language models, e.g., ChatGPT, by encapsulating the input with relevant knowledge without violating the input length constraint.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.14.pdf",
        "keywords": [
            "mutual learning",
            "open domain question answering",
            "mutual learning framework"
        ]
    },
    {
        "title": "Explicitly Representing Syntax Improves Sentence-to-Layout Prediction of Unexpected Situations",
        "authors": [
            "Wolf Nuyts",
            "Ruben Cartuyvels",
            "Marie-Francine Moens"
        ],
        "published": "2024",
        "summary": "Recognizing visual entities in a natural language sentence and arranging them in a 2D spatial layout require a compositional understanding of language and space. This task of layout prediction is valuable in text-to-image synthesis as it allows localized and controlled in-painting of the image. In this comparative study it is shown that we can predict layouts from language representations that implicitly or explicitly encode sentence syntax, if the sentences mention similar entity-relationships to the ones seen during training. To test compositional understanding, we collect a test set of grammatically correct sentences and layouts describing compositions of entities and relations that unlikely have been seen during training. Performance on this test set substantially drops, showing that current models rely on correlations in the training data and have difficulties in understanding the structure of the input sentences. We propose a novel structural loss function that better enforces the syntactic structure of the input sentence and show large performance gains in the task of 2D spatial layout prediction conditioned on text. The loss has the potential to be used in other generation tasks where a tree-like structure underlies the conditioning modality. Code, trained models, and the USCOCO evaluation set are available via Github.1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.15.pdf",
        "keywords": [
            "unexpected situations",
            "layout prediction"
        ]
    },
    {
        "title": "Evaluating the Ripple Effects of Knowledge Editing in Language Models",
        "authors": [
            "Roi Cohen",
            "Eden Biran",
            "Ori Yoran",
            "Amir Globerson",
            "Mor Geva"
        ],
        "published": "2024",
        "summary": "Modern language models capture a large body of factual knowledge. However, some facts can be incorrectly induced or become obsolete over time, resulting in factually incorrect generations. This has led to the development of various editing methods that allow updating facts encoded by the model. Evaluation of these methods has primarily focused on testing whether an individual fact has been successfully injected, and if similar predictions for other subjects have not changed. Here we argue that such evaluation is limited, since injecting one fact (e.g., \u201cJack Depp is the son of Johnny Depp\u201d) introduces a \u201cripple effect\u201d in the form of additional facts that the model needs to update (e.g., \u201cJack Depp is the sibling of Lily-Rose Depp\u201d). To address this, we propose novel evaluation criteria that consider the implications of an edit on related facts. Using these criteria, we then construct RippleEdits, a diagnostic benchmark of 5K factual edits, capturing various types of ripple effects. We evaluate prominent editing methods on RippleEdits, showing that they fail to introduce consistent changes in the model\u2019s knowledge. In addition, we find that a simple in-context editing baseline obtains the best scores on our benchmark, suggesting a promising research direction for model editing.1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.16.pdf",
        "keywords": [
            "ripple",
            "knowledge editing",
            "language models",
            "ripple effects"
        ]
    },
    {
        "title": "The Impact of Word Splitting on the Semantic Content of Contextualized Word Representations",
        "authors": [
            "Aina Gar\u00ed Soler",
            "Matthieu Labeau",
            "Chlo\u00e9 Clavel"
        ],
        "published": "2024",
        "summary": "When deriving contextualized word representations from language models, a decision needs to be made on how to obtain one for out-of-vocabulary (OOV) words that are segmented into subwords. What is the best way to represent these words with a single vector, and are these representations of worse quality than those of in-vocabulary words? We carry out an intrinsic evaluation of embeddings from different models on semantic similarity tasks involving OOV words. Our analysis reveals, among other interesting findings, that the quality of representations of words that are split is often, but not always, worse than that of the embeddings of known words. Their similarity values, however, must be interpreted with caution.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.17.pdf",
        "keywords": [
            "word splitting",
            "words",
            "semantic similarity"
        ]
    },
    {
        "title": "Large Language Models Enable Few-Shot Clustering",
        "authors": [
            "Vijay Viswanathan",
            "Kiril Gashteovski",
            "Kiril Gashteovski",
            "Carolin Lawrence",
            "Tongshuang Wu",
            "Graham Neubig"
        ],
        "published": "2024",
        "summary": "Unlike traditional unsupervised clustering, semi-supervised clustering allows users to provide meaningful structure to the data, which helps the clustering algorithm to match the user\u2019s intent. Existing approaches to semi-supervised clustering require a significant amount of feedback from an expert to improve the clusters. In this paper, we ask whether a large language model (LLM) can amplify an expert\u2019s guidance to enable query-efficient, few-shot semi-supervised text clustering. We show that LLMs are surprisingly effective at improving clustering. We explore three stages where LLMs can be incorporated into clustering: before clustering (improving input features), during clustering (by providing constraints to the clusterer), and after clustering (using LLMs post-correction). We find that incorporating LLMs in the first two stages routinely provides significant improvements in cluster quality, and that LLMs enable a user to make trade-offs between cost and accuracy to produce desired clusters. We release our code and LLM prompts for the public to use.1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.18.pdf",
        "keywords": [
            "semi supervised text clustering",
            "semi supervised clustering",
            "clustering",
            "large language models",
            "few shot clustering"
        ]
    },
    {
        "title": "JustiLM: Few-shot Justification Generation for Explainable Fact-Checking of Real-world Claims",
        "authors": [
            "Fengzhu Zeng",
            "Wei Gao"
        ],
        "published": "2024",
        "summary": "Justification is an explanation that supports the veracity assigned to a claim in fact-checking. However, the task of justification generation has been previously oversimplified as summarization of a fact-check article authored by fact-checkers. Therefore, we propose a realistic approach to generate justification based on retrieved evidence. We present a new benchmark dataset called ExClaim (for Explainable fact-checking of real-world Claims), and introduce JustiLM, a novel few-shot Justification generation based on retrieval-augmented Language Model by using fact-check articles as an auxiliary resource during training only. Experiments show that JustiLM achieves promising performance in justification generation compared to strong baselines, and can also enhance veracity classification with a straightforward extension.1 Code and dataset are released at https://github.com/znhy1024/JustiLM.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.19.pdf",
        "keywords": [
            "justification",
            "fact checking",
            "justification generation",
            "few shot justification",
            "veracity",
            "world"
        ]
    },
    {
        "title": "To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation",
        "authors": [
            "Jiaming Luo",
            "Colin Cherry",
            "George Foster"
        ],
        "published": "2024",
        "summary": "We conduct a large-scale fine-grained comparative analysis of machine translations (MTs) against human translations (HTs) through the lens of morphosyntactic divergence. Across three language pairs and two types of divergence defined as the structural difference between the source and the target, MT is consistently more conservative than HT, with less morphosyntactic diversity, more convergent patterns, and more one-to-one alignments. Through analysis on different decoding algorithms, we attribute this discrepancy to the use of beam search that biases MT towards more convergent patterns. This bias is most amplified when the convergent pattern appears around 50% of the time in training data. Lastly, we show that for a majority of morphosyntactic divergences, their presence in HT is correlated with decreased MT performance, presenting a greater challenge for MT systems.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.20.pdf",
        "keywords": [
            "diverge",
            "morphosyntactic divergence",
            "machine translation",
            "morphosyntactic diversity",
            "human translation",
            "convergent pattern",
            "convergent"
        ]
    },
    {
        "title": "What Do Self-Supervised Speech Models Know About Words?",
        "authors": [
            "Ankita Pasad",
            "Chung-Ming Chien",
            "Shane Settle",
            "Karen Livescu"
        ],
        "published": "2024",
        "summary": "Many self-supervised speech models (S3Ms) have been introduced over the last few years, improving performance and data efficiency on various speech tasks. However, these empirical successes alone do not give a complete picture of what is learned during pre-training. Recent work has begun analyzing how S3Ms encode certain properties, such as phonetic and speaker information, but we still lack a proper understanding of knowledge encoded at the word level and beyond. In this work, we use lightweight analysis methods to study segment-level linguistic properties\u2014word identity, boundaries, pronunciation, syntactic features, and semantic features\u2014encoded in S3Ms. We present a comparative study of layer-wise representations from ten S3Ms and find that (i) the frame-level representations within each word segment are not all equally informative, and (ii) the pre-training objective and model size heavily influence the accessibility and distribution of linguistic information across layers. We also find that on several tasks\u2014word discrimination, word segmentation, and semantic sentence similarity\u2014S3Ms trained with visual grounding outperform their speech-only counterparts. Finally, our task-based analyses demonstrate improved performance on word segmentation and acoustic word discrimination while using simpler methods than prior work.1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.21.pdf",
        "keywords": [
            "word segmentation",
            "self supervised speech",
            "speech",
            "visual grounding",
            "syntactic features",
            "pronunciation",
            "self supervised speech models",
            "s3ms",
            "boundaries"
        ]
    },
    {
        "title": "Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation",
        "authors": [
            "Lukas Edman",
            "Gabriele Sarti",
            "Antonio Toral",
            "Gertjan van Noord",
            "Arianna Bisazza"
        ],
        "published": "2024",
        "summary": "Pretrained character-level and byte-level language models have been shown to be competitive with popular subword models across a range of Natural Language Processing tasks. However, there has been little research on their effectiveness for neural machine translation (NMT), particularly within the popular pretrain-then-finetune paradigm. This work performs an extensive comparison across multiple languages and experimental conditions of character- and subword-level pretrained models (ByT5 and mT5, respectively) on NMT. We show the effectiveness of character-level modeling in translation, particularly in cases where fine-tuning data is limited. In our analysis, we show how character models\u2019 gains in translation quality are reflected in better translations of orthographically similar words and rare words. While evaluating the importance of source texts in driving model predictions, we highlight word-level patterns within ByT5, suggesting an ability to modulate word-level and character-level information during generation. We conclude by assessing the efficiency tradeoff of byte models, suggesting their usage in non-time-critical scenarios to boost translation quality.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.22.pdf",
        "keywords": [
            "translation",
            "byte models",
            "subword models",
            "translation quality",
            "machine translation",
            "natural language processing",
            "character",
            "character level translations"
        ]
    },
    {
        "title": "Geographic Adaptation of Pretrained Language Models",
        "authors": [
            "Valentin Hofmann",
            "Goran Glava\u0161",
            "Nikola Ljube\u0161i\u0107",
            "Janet B. Pierrehumbert",
            "Hinrich Sch\u00fctze"
        ],
        "published": "2024",
        "summary": "While pretrained language models (PLMs) have been shown to possess a plethora of linguistic knowledge, the existing body of research has largely neglected extralinguistic knowledge, which is generally difficult to obtain by pretraining on text alone. Here, we contribute to closing this gap by examining geolinguistic knowledge, i.e., knowledge about geographic variation in language. We introduce geoadaptation, an intermediate training step that couples language modeling with geolocation prediction in a multi-task learning setup. We geoadapt four PLMs, covering language groups from three geographic areas, and evaluate them on five different tasks: fine-tuned (i.e., supervised) geolocation prediction, zero-shot (i.e., unsupervised) geolocation prediction, fine-tuned language identification, zero-shot language identification, and zero-shot prediction of dialect features. Geoadaptation is very successful at injecting geolinguistic knowledge into the PLMs: The geoadapted PLMs consistently outperform PLMs adapted using only language modeling (by especially wide margins on zero-shot prediction tasks), and we obtain new state-of-the-art results on two benchmarks for geolocation prediction and language identification. Furthermore, we show that the effectiveness of geoadaptation stems from its ability to geographically retrofit the representation space of the PLMs.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.23.pdf",
        "keywords": [
            "prediction",
            "language models",
            "geolocation prediction",
            "language identification",
            "geoadapted",
            "pretrained language models",
            "geographic adaptation"
        ]
    },
    {
        "title": "Do Text Simplification Systems Preserve Meaning? A Human Evaluation via Reading Comprehension",
        "authors": [
            "Sweta Agrawal",
            "Marine Carpuat"
        ],
        "published": "2024",
        "summary": "Automatic text simplification (TS) aims to automate the process of rewriting text to make it easier for people to read. A pre-requisite for TS to be useful is that it should convey information that is consistent with the meaning of the original text. However, current TS evaluation protocols assess system outputs for simplicity and meaning preservation without regard for the document context in which output sentences occur and for how people understand them. In this work, we introduce a human evaluation framework to assess whether simplified texts preserve meaning using reading comprehension questions. With this framework, we conduct a thorough human evaluation of texts by humans and by nine automatic systems. Supervised systems that leverage pre-training knowledge achieve the highest scores on the reading comprehension tasks among the automatic controllable TS systems. However, even the best-performing supervised system struggles with at least 14% of the questions, marking them as \u201cunanswerable\u201d based on simplified content. We further investigate how existing TS evaluation metrics and automatic question-answering systems approximate the human judgments we obtained.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.24.pdf",
        "keywords": [
            "text",
            "human evaluation",
            "automatic text simplification",
            "simplified content",
            "reading comprehension",
            "comprehension",
            "text simplification systems",
            "question answering systems"
        ]
    },
    {
        "title": "Simultaneous Selection and Adaptation of Source Data via Four-Level Optimization",
        "authors": [
            "Pengtao Xie",
            "Xingchen Zhao",
            "Xuehai He"
        ],
        "published": "2024",
        "summary": "In many NLP applications, to mitigate data deficiency in a target task, source data is collected to help with target model training. Existing transfer learning methods either select a subset of source examples that are close to the target domain or try to adapt all source examples into the target domain, then use selected or adapted source examples to train the target model. These methods either incur significant information loss or bear the risk that after adaptation, source examples which are originally already in the target domain may be outside the target domain. To address the limitations of these methods, we propose a four-level optimization based framework which simultaneously selects and adapts source data. Our method can automatically identify in-domain and out-of-domain source examples and apply example-specific processing methods: selection for in-domain examples and adaptation for out-of-domain examples. Experiments on various datasets demonstrate the effectiveness of our proposed method.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.25.pdf",
        "keywords": [
            "four level optimization",
            "simultaneous selection and adaptation",
            "use selected"
        ]
    },
    {
        "title": "ConvoSense: Overcoming Monotonous Commonsense Inferences for Conversational AI",
        "authors": [
            "Sarah E. Finch",
            "Jinho D. Choi"
        ],
        "published": "2024",
        "summary": "Mastering commonsense understanding and reasoning is a pivotal skill essential for conducting engaging conversations. While there have been several attempts to create datasets that facilitate commonsense inferences in dialogue contexts, existing datasets tend to lack in-depth details, restate information already present in the conversation, and often fail to capture the multifaceted nature of commonsense reasoning. In response to these limitations, we compile a new synthetic dataset for commonsense reasoning in dialogue contexts using GPT, \u2102onvoSense, that boasts greater contextual novelty, offers a higher volume of inferences per example, and substantially enriches the detail conveyed by the inferences. Our dataset contains over 500,000 inferences across 12,000 dialogues with 10 popular inference types, which empowers the training of generative commonsense models for dialogue that are superior in producing plausible inferences with high novelty when compared to models trained on the previous datasets. To the best of our knowledge, \u2102onvoSense is the first of its kind to provide such a multitude of novel inferences at such a large scale.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.26.pdf",
        "keywords": [
            "commonsense",
            "commonsense reasoning",
            "dialogues",
            "commonsense models",
            "inferences",
            "conversational"
        ]
    },
    {
        "title": "Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies",
        "authors": [
            "Liangming Pan",
            "Michael Saxon",
            "Wenda Xu",
            "Deepak Nathani",
            "Xinyi Wang",
            "William Yang Wang"
        ],
        "published": "2024",
        "summary": "While large language models (LLMs) have shown remarkable effectiveness in various NLP tasks, they are still prone to issues such as hallucination, unfaithful reasoning, and toxicity. A promising approach to rectify these flaws is correcting LLMs with feedback, where the LLM itself is prompted or guided with feedback to fix problems in its own output. Techniques leveraging automated feedback\u2014either produced by the LLM itself (self-correction) or some external system\u2014are of particular interest as they make LLM-based solutions more practical and deployable with minimal human intervention. This paper provides an exhaustive review of the recent advances in correcting LLMs with automated feedback, categorizing them into training-time, generation-time, and post-hoc approaches. We also identify potential challenges and future directions in this emerging field.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.27.pdf",
        "keywords": [
            "large language models"
        ]
    },
    {
        "title": "KoBBQ: Korean Bias Benchmark for Question Answering",
        "authors": [
            "Jiho Jin",
            "Jiseon Kim",
            "Nayeon Lee",
            "Haneul Yoo",
            "Alice Oh",
            "Hwaran Lee"
        ],
        "published": "2024",
        "summary": "Warning: This paper contains examples of stereotypes and biases. The Bias Benchmark for Question Answering (BBQ) is designed to evaluate social biases of language models (LMs), but it is not simple to adapt this benchmark to cultural contexts other than the US because social biases depend heavily on the cultural context. In this paper, we present KoBBQ, a Korean bias benchmark dataset, and we propose a general framework that addresses considerations for cultural adaptation of a dataset. Our framework includes partitioning the BBQ dataset into three classes\u2014Simply-Transferred (can be used directly after cultural translation), Target-Modified (requires localization in target groups), and Sample-Removed (does not fit Korean culture)\u2014and adding four new categories of bias specific to Korean culture. We conduct a large-scale survey to collect and validate the social biases and the targets of the biases that reflect the stereotypes in Korean culture. The resulting KoBBQ dataset comprises 268 templates and 76,048 samples across 12 categories of social bias. We use KoBBQ to measure the accuracy and bias scores of several state-of-the-art multilingual LMs. The results clearly show differences in the bias of LMs as measured by KoBBQ and a machine-translated version of BBQ, demonstrating the need for and utility of a well-constructed, culturally aware social bias benchmark.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.28.pdf",
        "keywords": [
            "bias",
            "question answering",
            "social bias",
            "social biases",
            "social bias benchmark",
            "bias benchmark",
            "kobbq",
            "language models"
        ]
    },
    {
        "title": "AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning",
        "authors": [
            "Han Zhou",
            "Xingchen Wan",
            "Ivan Vuli\u0107",
            "Anna Korhonen"
        ],
        "published": "2024",
        "summary": "Large pretrained language models are widely used in downstream NLP tasks via task- specific fine-tuning, but such procedures can be costly. Recently, Parameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task performance while updating much fewer parameters than full model fine-tuning (FFT). However, it is non-trivial to make informed design choices on the PEFT configurations, such as their architecture, the number of tunable parameters, and even the layers in which the PEFT modules are inserted. Consequently, it is highly likely that the current, manually designed configurations are suboptimal in terms of their performance-efficiency trade-off. Inspired by advances in neural architecture search, we propose AutoPEFT for automatic PEFT configuration selection: We first design an expressive configuration search space with multiple representative PEFT modules as building blocks. Using multi-objective Bayesian optimization in a low-cost setup, we then discover a Pareto-optimal set of configurations with strong performance-cost trade-offs across different numbers of parameters that are also highly transferable across different tasks. Empirically, on GLUE and SuperGLUE tasks, we show that AutoPEFT-discovered configurations significantly outperform existing PEFT methods and are on par or better than FFT without incurring substantial training efficiency costs.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.29.pdf",
        "keywords": [
            "autopeft",
            "fine tuning",
            "configuration search",
            "language models",
            "parameter efficient fine tuning"
        ]
    },
    {
        "title": "What Formal Languages Can Transformers Express? A Survey",
        "authors": [
            "Lena Strobl",
            "William Merrill",
            "Gail Weiss",
            "David Chiang",
            "Dana Angluin"
        ],
        "published": "2024",
        "summary": "As transformers have gained prominence in natural language processing, some researchers have investigated theoretically what problems they can and cannot solve, by treating problems as formal languages. Exploring such questions can help clarify the power of transformers relative to other models of computation, their fundamental capabilities and limits, and the impact of architectural choices. Work in this subarea has made considerable progress in recent years. Here, we undertake a comprehensive survey of this work, documenting the diverse assumptions that underlie different results and providing a unified framework for harmonizing seemingly contradictory findings.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.30.pdf",
        "keywords": [
            "formal languages",
            "transformers",
            "survey"
        ]
    },
    {
        "title": "Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap",
        "authors": [
            "Michael Staniek",
            "Raphael Schumann",
            "Maike Z\u00fcfle",
            "Stefan Riezler"
        ],
        "published": "2024",
        "summary": "We present Text-to-OverpassQL, a task designed to facilitate a natural language interface for querying geodata from OpenStreetMap (OSM). The Overpass Query Language (OverpassQL) allows users to formulate complex database queries and is widely adopted in the OSM ecosystem. Generating Overpass queries from natural language input serves multiple use-cases. It enables novice users to utilize OverpassQL without prior knowledge, assists experienced users with crafting advanced queries, and enables tool-augmented large language models to access information stored in the OSM database. In order to assess the performance of current sequence generation models on this task, we propose OverpassNL,1 a dataset of 8,352 queries with corresponding natural language inputs. We further introduce task specific evaluation metrics and ground the evaluation of the Text-to-OverpassQL task by executing the queries against the OSM database. We establish strong baselines by finetuning sequence-to-sequence models and adapting large language models with in-context examples. The detailed evaluation reveals strengths and weaknesses of the considered learning strategies, laying the foundations for further research into the Text-to-OverpassQL task.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.31.pdf",
        "keywords": [
            "openstreetmap",
            "natural language",
            "natural language interface",
            "overpass",
            "overpass query language",
            "geodata",
            "models"
        ]
    },
    {
        "title": "Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions",
        "authors": [
            "Jiahuan Li",
            "Hao Zhou",
            "Shujian Huang",
            "Shanbo Cheng",
            "Jiajun Chen"
        ],
        "published": "2024",
        "summary": "Large-scale pretrained language models (LLMs), such as ChatGPT and GPT4, have shown strong abilities in multilingual translation, without being explicitly trained on parallel corpora. It is intriguing how the LLMs obtain their ability to carry out translation instructions for different languages. In this paper, we present a detailed analysis by finetuning a multilingual pretrained language model, XGLM-7.5B, to perform multilingual translation following given instructions. Firstly, we show that multilingual LLMs have stronger translation abilities than previously demonstrated. For a certain language, the translation performance depends on its similarity to English and the amount of data used in the pretraining phase. Secondly, we find that LLMs\u2019 ability to carry out translation instructions relies on the understanding of translation instructions and the alignment among different languages. With multilingual finetuning with translation instructions, LLMs could learn to perform the translation task well even for those language pairs unseen during the instruction tuning phase.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.32.pdf",
        "keywords": [
            "finetuning",
            "translation",
            "translation ability",
            "multilingual finetuning",
            "multilingual translation",
            "pretrained language models",
            "large language models"
        ]
    },
    {
        "title": "Semantics of Multiword Expressions in Transformer-Based Models: A Survey",
        "authors": [
            "Filip Mileti\u0107",
            "Sabine Schulte im Walde"
        ],
        "published": "2024",
        "summary": "Multiword expressions (MWEs) are composed of multiple words and exhibit variable degrees of compositionality. As such, their meanings are notoriously difficult to model, and it is unclear to what extent this issue affects transformer architectures. Addressing this gap, we provide the first in-depth survey of MWE processing with transformer models. We overall find that they capture MWE semantics inconsistently, as shown by reliance on surface patterns and memorized information. MWE meaning is also strongly localized, predominantly in early layers of the architecture. Representations benefit from specific linguistic properties, such as lower semantic idiosyncrasy and ambiguity of target expressions. Our findings overall question the ability of transformer models to robustly capture fine-grained semantics. Furthermore, we highlight the need for more directly comparable evaluation setups.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.33.pdf",
        "keywords": [
            "multiword expressions",
            "transformer",
            "semantics"
        ]
    },
    {
        "title": "The Thai Discourse Treebank: Annotating and Classifying Thai Discourse Connectives",
        "authors": [
            "Ponrawee Prasertsom",
            "Apiwat Jaroonpol",
            "Attapol T. Rutherford"
        ],
        "published": "2024",
        "summary": "Discourse analysis is a highly applicable area of natural language processing. In English and other languages, resources for discourse-based tasks are widely available. Thai, however, has hitherto lacked such resources. We present the Thai Discourse Treebank, the first, large Thai corpus annotated in the style of the Penn Discourse Treebank. The resulting corpus has over 10,000 sentences and 18,000 instances of connectives in 33 different relations. We release the corpus alongside our list of 148 potentially polysemous discourse connectives with a total of 340 form-sense pairs and their classification criteria to facilitate future research. We also develop models for connective identification and classification tasks. Our best models achieve an F1 of 0.96 in the identification task and 0.46 on the sense classification task. Our results serve as benchmarks for future models for Thai discourse tasks.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.34.pdf",
        "keywords": [
            "discourse connectives",
            "treebank",
            "discourse treebank",
            "discourse",
            "corpus",
            "discourse analysis",
            "classification"
        ]
    },
    {
        "title": "Federated Learning for Exploiting Annotators\u2019 Disagreements in Natural Language Processing",
        "authors": [
            "Nuria Rodr\u00edguez-Barroso",
            "Eugenio Mart\u00ednez C\u00e1mara",
            "Jose Camacho Collados",
            "M. Victoria Luz\u00f3n",
            "Francisco Herrera"
        ],
        "published": "2024",
        "summary": "The annotation of ambiguous or subjective NLP tasks is usually addressed by various annotators. In most datasets, these annotations are aggregated into a single ground truth. However, this omits divergent opinions of annotators, hence missing individual perspectives. We propose FLEAD (Federated Learning for Exploiting Annotators\u2019 Disagreements), a methodology built upon federated learning to independently learn from the opinions of all the annotators, thereby leveraging all their underlying information without relying on a single ground truth. We conduct an extensive experimental study and analysis in diverse text classification tasks to show the contribution of our approach with respect to mainstream approaches based on majority voting and other recent methodologies that also learn from annotator disagreements.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.35.pdf",
        "keywords": [
            "disagreements",
            "natural language processing"
        ]
    },
    {
        "title": "Computational Complexity of Natural Morphology Revisited",
        "authors": [
            "Hajime Senuma",
            "Akiko Aizawa"
        ],
        "published": "2024",
        "summary": "This paper revisits a classical, yet fundamental, discussion of theoretical computational linguistics: the computational complexity of natural languages. Past studies have revealed that syntax, as observed in Swiss-German, is not weakly context-free. Concerning morphology, Culy (1985) employed a construction in Bambara to show that morphology is not weakly context-free; however, Manaster-Ramer (1988) pointed out that the Bambara case can be problematic because the wordhood of the construction is reliant on special tonal behaviors, and it is ambiguous whether the behaviors belong to the morphological domain. This raises doubts about whether the case can be considered a genuine morphological phenomenon. In this paper, we argue that Classical Ainu, a language we examine, also defies weak context-freeness at the morphological level. The construction we introduce is unambiguously morphological because this language\u2019s valency-sensitive structure and valency-changing operations, such as noun incorporation, preclude its grammatical interpretation as syntactic.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.36.pdf",
        "keywords": [
            "grammatical",
            "natural languages",
            "natural morphology",
            "morphology",
            "weakly context free",
            "computational linguistics",
            "computational complexity",
            "morphological domain"
        ]
    },
    {
        "title": "Improving Probability-based Prompt Selection Through Unified Evaluation and Analysis",
        "authors": [
            "Sohee Yang",
            "Jonghyeon Kim",
            "Joel Jang",
            "Seonghyeon Ye",
            "Hyunji Lee",
            "Minjoon Seo"
        ],
        "published": "2024",
        "summary": "Previous work in prompt engineering for large language models has introduced different gradient-free probability-based prompt selection methods that aim to choose the optimal prompt among the candidates for a given task but have failed to provide a comprehensive and fair comparison between each other. In this paper, we propose a unified framework to interpret and evaluate the existing probability-based prompt selection methods by performing extensive experiments on 13 common and diverse NLP tasks. We find that each of the existing methods can be interpreted as some variant of the method that maximizes mutual information between the input and the predicted output (MI). Utilizing this finding, we develop several other combinatorial variants of MI and increase the effectiveness of the oracle prompt selection method from 87.79% to 94.98%, measured as the ratio of the performance of the selected prompt to that of the optimal oracle prompt. Furthermore, considering that all the methods rely on the output probability distribution of the model that might be biased, we propose a novel calibration method called Calibration by Marginalization (CBM) that is orthogonal to the existing methods and helps increase the prompt selection effectiveness of the best method to 96.85%, achieving 99.44% of the oracle prompt F1 without calibration.1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.37.pdf",
        "keywords": [
            "probability",
            "prompt selection methods"
        ]
    },
    {
        "title": "Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering",
        "authors": [
            "Vaibhav Adlakha",
            "Parishad BehnamGhader",
            "Xing Han Lu",
            "Nicholas Meade",
            "Siva Reddy"
        ],
        "published": "2024",
        "summary": "Instruction-following models are attractive alternatives to fine-tuned approaches for question answering (QA). By simply prepending relevant documents and an instruction to their input, these models can be adapted to various information domains and tasks without additional training. However, these models tend to produce verbose responses with supplementary information, which makes traditional QA metrics like exact match (EM) and F1 unreliable for accurately quantifying model performance. In this work, we evaluate instruction-following models along two fronts: 1) how well they satisfy user\u2019s information need (correctness), and 2) whether they disseminate information supported by the provided knowledge (faithfulness). Guided by human evaluation and analysis, we highlight the shortcomings of traditional metrics for both correctness and faithfulness and propose simple token-overlap metrics that correlate highly with human judgments. Our analysis reveals that for correctness, instruction-following models perform comparably to models specifically fine-tuned for that task. However, they struggle to accurately judge the relevance of the provided knowledge and often hallucinate in their responses. We hope our work encourages more holistic evaluation of instruction-following models for QA. Our code and human annotation data is available at https://github.com/McGill-NLP/instruct-qa.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.38.pdf",
        "keywords": [
            "faithfulness",
            "question answering",
            "encourages",
            "correctness",
            "models",
            "perform",
            "instruction"
        ]
    },
    {
        "title": "The Ethics of Automating Legal Actors",
        "authors": [
            "Josef Valvoda",
            "Alec Thompson",
            "Ryan Cotterell",
            "Simone Teufel"
        ],
        "published": "2024",
        "summary": "The introduction of large public legal datasets has brought about a renaissance in legal NLP. Many of these datasets are composed of legal judgments\u2014the product of judges deciding cases. Since ML algorithms learn to model the data they are trained on, several legal NLP models are models of judges. While some have argued for the automation of judges, in this position piece, we argue that automating the role of the judge raises difficult ethical challenges, in particular for common law legal systems. Our argument follows from the social role of the judge in actively shaping the law, rather than merely applying it. Since current NLP models are too far away from having the facilities necessary for this task, they should not be used to automate judges. Furthermore, even in the case that the models could achieve human-level capabilities, there would still be remaining ethical concerns inherent in the automation of the legal process.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.39.pdf",
        "keywords": [
            "ethics",
            "automating legal actors"
        ]
    },
    {
        "title": "Source-Free Domain Adaptation for Question Answering with Masked Self-training",
        "authors": [
            "Maxwell J. Yin",
            "Boyu Wang",
            "Yue Dong",
            "Charles Ling"
        ],
        "published": "2024",
        "summary": "Previous unsupervised domain adaptation (UDA) methods for question answering (QA) require access to source domain data while fine-tuning the model for the target domain. Source domain data may, however, contain sensitive information and should be protected. In this study, we investigate a more challenging setting, source-free UDA, in which we have only the pretrained source model and target domain data, without access to source domain data. We propose a novel self-training approach to QA models that integrates a specially designed mask module for domain adaptation. The mask is auto-adjusted to extract key domain knowledge when trained on the source domain. To maintain previously learned domain knowledge, certain mask weights are frozen during adaptation, while other weights are adjusted to mitigate domain shifts with pseudo-labeled samples generated in the target domain. Our empirical results on four benchmark datasets suggest that our approach significantly enhances the performance of pretrained QA models on the target domain, and even outperforms models that have access to the source data during adaptation.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.40.pdf",
        "keywords": [
            "domain adaptation",
            "question answering",
            "source free domain adaptation",
            "self training",
            "masked"
        ]
    },
    {
        "title": "Scope Ambiguities in Large Language Models",
        "authors": [
            "Gaurav Kamath",
            "Sebastian Schuster",
            "Sowmya Vajjala",
            "Siva Reddy"
        ],
        "published": "2024",
        "summary": "Sentences containing multiple semantic operators with overlapping scope often create ambiguities in interpretation, known as scope ambiguities. These ambiguities offer rich insights into the interaction between semantic structure and world knowledge in language processing. Despite this, there has been little research into how modern large language models treat them. In this paper, we investigate how different versions of certain autoregressive language models\u2014GPT-2, GPT-3/3.5, Llama 2, and GPT-4\u2014treat scope ambiguous sentences, and compare this with human judgments. We introduce novel datasets that contain a joint total of almost 1,000 unique scope-ambiguous sentences, containing interactions between a range of semantic operators, and annotated for human judgments. Using these datasets, we find evidence that several models (i) are sensitive to the meaning ambiguity in these sentences, in a way that patterns well with human judgments, and (ii) can successfully identify human-preferred readings at a high level of accuracy (over 90% in some cases).1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.41.pdf",
        "keywords": [
            "scope ambiguities",
            "ambiguity"
        ]
    },
    {
        "title": "Visually Grounded Speech Models Have a Mutual Exclusivity Bias",
        "authors": [
            "Leanne Nortje",
            "Dan Onea\u0163\u0103",
            "Yevgen Matusevych",
            "Herman Kamper"
        ],
        "published": "2024",
        "summary": "When children learn new words, they employ constraints such as the mutual exclusivity (ME) bias: A novel word is mapped to a novel object rather than a familiar one. This bias has been studied computationally, but only in models that use discrete word representations as input, ignoring the high variability of spoken words. We investigate the ME bias in the context of visually grounded speech models that learn from natural images and continuous speech audio. Concretely, we train a model on familiar words and test its ME bias by asking it to select between a novel and a familiar object when queried with a novel word. To simulate prior acoustic and visual knowledge, we experiment with several initialization strategies using pretrained speech and vision networks. Our findings reveal the ME bias across the different initialization approaches, with a stronger bias in models with more prior (in particular, visual) knowledge. Additional tests confirm the robustness of our results, even when different loss functions are considered. Based on detailed analyses to piece out the model\u2019s representation space, we attribute the ME bias to how familiar and novel classes are distinctly separated in the resulting space.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.42.pdf",
        "keywords": [
            "me bias",
            "visually grounded speech models",
            "mutual exclusivity"
        ]
    },
    {
        "title": "Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias",
        "authors": [
            "Itay Itzhak",
            "Gabriel Stanovsky",
            "Nir Rosenfeld",
            "Yonatan Belinkov"
        ],
        "published": "2024",
        "summary": "Recent studies show that instruction tuning (IT) and reinforcement learning from human feedback (RLHF) improve the abilities of large language models (LMs) dramatically. While these tuning methods can help align models with human objectives and generate high-quality text, not much is known about their potential adverse effects. In this work, we investigate the effect of IT and RLHF on decision making and reasoning in LMs, focusing on three cognitive biases\u2014the decoy effect, the certainty effect, and the belief bias\u2014all of which are known to influence human decision-making and reasoning. Our findings highlight the presence of these biases in various models from the GPT-3, Mistral, and T5 families. Notably, we find a stronger presence of biases in models that have undergone instruction tuning, such as Flan-T5, Mistral-Instruct, GPT3.5, and GPT4. Our work constitutes a step toward comprehending cognitive biases in instruction-tuned LMs, which is crucial for the development of more reliable and unbiased language models.1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.43.pdf",
        "keywords": [
            "bias",
            "decision making",
            "cognitive biases",
            "instruction tuning",
            "instruction tuned language models",
            "language models",
            "emergent cognitive bias",
            "instruction tuned lms"
        ]
    },
    {
        "title": "Beyond Boundaries: A Human-like Approach for Question Answering over Structured and Unstructured Information Sources",
        "authors": [
            "Jens Lehmann",
            "Dhananjay Bhandiwad",
            "Preetam Gattogi",
            "Sahar Vahdati"
        ],
        "published": "2024",
        "summary": "Answering factual questions from heterogenous sources, such as graphs and text, is a key capacity of intelligent systems. Current approaches either (i) perform question answering over text and structured sources as separate pipelines followed by a merge step or (ii) provide an early integration, giving up the strengths of particular information sources. To solve this problem, we present \u201cHumanIQ\u201d, a method that teaches language models to dynamically combine retrieved information by imitating how humans use retrieval tools. Our approach couples a generic method for gathering human demonstrations of tool use with adaptive few-shot learning for tool augmented models. We show that HumanIQ confers significant benefits, including i) reducing the error rate of our strongest baseline (GPT-4) by over 50% across 3 benchmarks, (ii) improving human preference over responses from vanilla GPT-4 (45.3% wins, 46.7% ties, 8.0% loss), and (iii) outperforming numerous task-specific baselines.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.44.pdf",
        "keywords": [
            "question answering",
            "tool use"
        ]
    },
    {
        "title": "Comparing Humans and Large Language Models on an Experimental Protocol Inventory for Theory of Mind Evaluation (EPITOME)",
        "authors": [
            "Cameron R. Jones",
            "Sean Trott",
            "Benjamin Bergen"
        ],
        "published": "2024",
        "summary": "We address a growing debate about the extent to which large language models (LLMs) produce behavior consistent with Theory of Mind (ToM) in humans. We present EPITOME: a battery of six experiments that tap diverse ToM capacities, including belief attribution, emotional inference, and pragmatic reasoning. We elicit a performance baseline from human participants for each task. We use the dataset to ask whether distributional linguistic information learned by LLMs is sufficient to explain ToM in humans. We compare performance of five LLMs to a baseline of responses from human comprehenders. Results are mixed. LLMs display considerable sensitivity to mental states and match human performance in several tasks. Yet, they commit systematic errors in others, especially those requiring pragmatic reasoning on the basis of mental state information. Such uneven performance indicates that human-level ToM may require resources beyond distributional information.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.45.pdf",
        "keywords": [
            "theory of mind",
            "language models",
            "theory of mind evaluation",
            "large language models",
            "pragmatic reasoning",
            "emotional"
        ]
    },
    {
        "title": "A Closer Look at Classification Evaluation Metrics and a Critical Reflection of Common Evaluation Practice",
        "authors": [
            "Juri Opitz"
        ],
        "published": "2024",
        "summary": "Classification systems are evaluated in a countless number of papers. However, we find that evaluation practice is often nebulous. Frequently, metrics are selected without arguments, and blurry terminology invites misconceptions. For instance, many works use so-called \u2018macro\u2019 metrics to rank systems (e.g., \u2018macro F1\u2019) but do not clearly specify what they would expect from such a \u2018macro\u2019 metric. This is problematic, since picking a metric can affect research findings and thus any clarity in the process should be maximized. Starting from the intuitive concepts of bias and prevalence, we perform an analysis of common evaluation metrics. The analysis helps us understand the metrics\u2019 underlying properties, and how they align with expectations as found expressed in papers. Then we reflect on the practical situation in the field, and survey evaluation practice in recent shared tasks. We find that metric selection is often not supported with convincing arguments, an issue that can make a system ranking seem arbitrary. Our work aims at providing overview and guidance for more informed and transparent metric selection, fostering meaningful evaluation.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.46.pdf",
        "keywords": [
            "common evaluation metrics",
            "classification",
            "survey evaluation"
        ]
    },
    {
        "title": "Revisiting Meta-evaluation for Grammatical Error Correction",
        "authors": [
            "Masamune Kobayashi",
            "Masato Mita",
            "Mamoru Komachi"
        ],
        "published": "2024",
        "summary": "Metrics are the foundation for automatic evaluation in grammatical error correction (GEC), with their evaluation of the metrics (meta-evaluation) relying on their correlation with human judgments. However, conventional meta-evaluations in English GEC encounter several challenges, including biases caused by inconsistencies in evaluation granularity and an outdated setup using classical systems. These problems can lead to misinterpretation of metrics and potentially hinder the applicability of GEC techniques. To address these issues, this paper proposes SEEDA, a new dataset for GEC meta-evaluation. SEEDA consists of corrections with human ratings along two different granularities: edit-based and sentence-based, covering 12 state-of-the-art systems including large language models, and two human corrections with different focuses. The results of improved correlations by aligning the granularity in the sentence-level meta-evaluation suggest that edit-based metrics may have been underestimated in existing studies. Furthermore, correlations of most metrics decrease when changing from classical to neural systems, indicating that traditional metrics are relatively poor at evaluating fluently corrected sentences with many edits.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.47.pdf",
        "keywords": [
            "grammatical error correction",
            "meta evaluation"
        ]
    },
    {
        "title": "Context-Aware Machine Translation with Source Coreference Explanation",
        "authors": [
            "Huy Hien Vu",
            "Hidetaka Kamigaito",
            "Taro Watanabe"
        ],
        "published": "2024",
        "summary": "Despite significant improvements in enhancing the quality of translation, context-aware machine translation (MT) models underperform in many cases. One of the main reasons is that they fail to utilize the correct features from context when the context is too long or their models are overly complex. This can lead to the explain-away effect, wherein the models only consider features easier to explain predictions, resulting in inaccurate translations. To address this issue, we propose a model that explains the decisions made for translation by predicting coreference features in the input. We construct a model for input coreference by exploiting contextual features from both the input and translation output representations on top of an existing MT model. We evaluate and analyze our method in the WMT document-level translation task of English-German dataset, the English-Russian dataset, and the multilingual TED talk dataset, demonstrating an improvement of over 1.0 BLEU score when compared with other context-aware models.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.48.pdf",
        "keywords": [
            "context aware machine translation",
            "context aware",
            "source coreference explanation"
        ]
    },
    {
        "title": "Can Authorship Attribution Models Distinguish Speakers in Speech Transcripts?",
        "authors": [
            "Cristina Aggazzotti",
            "Nicholas Andrews",
            "Elizabeth Allyn Smith"
        ],
        "published": "2024",
        "summary": "Authorship verification is the task of determining if two distinct writing samples share the same author and is typically concerned with the attribution of written text. In this paper, we explore the attribution of transcribed speech, which poses novel challenges. The main challenge is that many stylistic features, such as punctuation and capitalization, are not informative in this setting. On the other hand, transcribed speech exhibits other patterns, such as filler words and backchannels (e.g., um, uh-huh), which may be characteristic of different speakers. We propose a new benchmark for speaker attribution focused on human-transcribed conversational speech transcripts. To limit spurious associations of speakers with topic, we employ both conversation prompts and speakers participating in the same conversation to construct verification trials of varying difficulties. We establish the state of the art on this new benchmark by comparing a suite of neural and non-neural baselines, finding that although written text attribution models achieve surprisingly good performance in certain settings, they perform markedly worse as conversational topic is increasingly controlled. We present analyses of the impact of transcription style on performance as well as the ability of fine-tuning on speech transcripts to improve performance.1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.49.pdf",
        "keywords": [
            "authorship attribution",
            "authorship verification",
            "written text attribution",
            "speaker attribution",
            "distinguish speakers",
            "speech transcripts",
            "speech"
        ]
    },
    {
        "title": "Decision-Oriented Dialogue for Human-AI Collaboration",
        "authors": [
            "Jessy Lin",
            "Nicholas Tomlin",
            "Jacob Andreas",
            "Jason Eisner"
        ],
        "published": "2024",
        "summary": "We describe a class of tasks called decision-oriented dialogues, in which AI assistants such as large language models (LMs) must collaborate with one or more humans via natural language to help them make complex decisions. We formalize three domains in which users face everyday decisions: (1) choosing an assignment of reviewers to conference papers, (2) planning a multi-step itinerary in a city, and (3) negotiating travel plans for a group of friends. In each of these settings, AI assistants and users have disparate abilities that they must combine to arrive at the best decision: Assistants can access and process large amounts of information, while users have preferences and constraints external to the system. For each task, we build a dialogue environment where agents receive a reward based on the quality of the final decision they reach. We evaluate LMs in self-play and in collaboration with humans and find that they fall short compared to human assistants, achieving much lower rewards despite engaging in longer dialogues. We highlight a number of challenges models face in decision-oriented dialogues, ranging from goal-directed behavior to reasoning and optimization, and release our environments as a testbed for future work.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.50.pdf",
        "keywords": [
            "dialogue",
            "collaborate",
            "plans",
            "language",
            "human ai collaboration",
            "agents",
            "language models",
            "decision",
            "complex"
        ]
    },
    {
        "title": "Exploring Continual Learning of Compositional Generalization in NLI",
        "authors": [
            "Xiyan Fu",
            "Anette Frank"
        ],
        "published": "2024",
        "summary": "Compositional Natural Language Inference (NLI) has been explored to assess the true abilities of neural models to perform NLI. Yet, current evaluations assume models to have full access to all primitive inferences in advance, in contrast to humans that continuously acquire inference knowledge. In this paper, we introduce the Continual Compositional Generalization in Inference (C2Gen NLI) challenge, where a model continuously acquires knowledge of constituting primitive inference tasks as a basis for compositional inferences. We explore how continual learning affects compositional generalization in NLI, by designing a continual learning setup for compositional NLI inference tasks. Our experiments demonstrate that models fail to compositionally generalize in a continual scenario. To address this problem, we first benchmark various continual learning algorithms and verify their efficacy. We then further analyze C2Gen, focusing on how to order primitives and compositional inference types, and examining correlations between subtasks. Our analyses show that by learning subtasks continuously while observing their dependencies and increasing degrees of difficulty, continual learning can enhance composition generalization ability.1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.51.pdf",
        "keywords": [
            "continual learning",
            "composition generalization",
            "compositional inference",
            "compositional nli inference",
            "natural language inference"
        ]
    },
    {
        "title": "State of What Art? A Call for Multi-Prompt LLM Evaluation",
        "authors": [
            "Moran Mizrahi",
            "Guy Kaplan",
            "Dan Malkin",
            "Rotem Dror",
            "Dafna Shahaf",
            "Gabriel Stanovsky"
        ],
        "published": "2024",
        "summary": "Recent advances in LLMs have led to an abundance of evaluation benchmarks, which typically rely on a single instruction template per task. We create a large-scale collection of instruction paraphrases and comprehensively analyze the brittleness introduced by single-prompt evaluations across 6.5M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks. We find that different instruction templates lead to very different performance, both absolute and relative. Instead, we propose a set of diverse metrics on multiple instruction paraphrases, specifically tailored for different use cases (e.g., LLM vs. downstream development), ensuring a more reliable and meaningful assessment of LLM capabilities. We show that our metrics provide new insights into the strengths and limitations of current LLMs.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.52.pdf",
        "keywords": [
            "evaluations",
            "instruction paraphrases",
            "multiple instruction paraphrases",
            "evaluation benchmarks",
            "benchmarks",
            "instruction templates",
            "assessment",
            "state",
            "what art"
        ]
    },
    {
        "title": "CreoleVal: Multilingual Multitask Benchmarks for Creoles",
        "authors": [
            "Heather Lent",
            "Kushal Tatariya",
            "Raj Dabre",
            "Yiyi Chen",
            "Marcell Fekete",
            "Esther Ploeger",
            "Li Zhou",
            "Ruth-Ann Armstrong",
            "Abee Eijansantos",
            "Catriona Malau",
            "Hans Erik Heje",
            "Ernests Lavrinovics",
            "Diptesh Kanojia",
            "Paul Belony",
            "Marcel Bollmann",
            "Lo\u00efc Grobol",
            "Miryam de Lhoneux",
            "Daniel Hershcovich",
            "Michel DeGraff",
            "Anders S\u00f8gaard",
            "Johannes Bjerva"
        ],
        "published": "2024",
        "summary": "Creoles represent an under-explored and marginalized group of languages, with few available resources for NLP research. While the genealogical ties between Creoles and a number of highly resourced languages imply a significant potential for transfer learning, this potential is hampered due to this lack of annotated data. In this work we present CreoleVal, a collection of benchmark datasets spanning 8 different NLP tasks, covering up to 28 Creole languages; it is an aggregate of novel development datasets for reading comprehension relation classification, and machine translation for Creoles, in addition to a practical gateway to a handful of preexisting benchmarks. For each benchmark, we conduct baseline experiments in a zero-shot setting in order to further ascertain the capabilities and limitations of transfer learning for Creoles. Ultimately, we see CreoleVal as an opportunity to empower research on Creoles in NLP and computational linguistics, and in general, a step towards more equitable language technology around the globe.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.53.pdf",
        "keywords": [
            "multitask benchmarks",
            "benchmark",
            "languages",
            "group of languages",
            "classification"
        ]
    },
    {
        "title": "xcomet: Transparent Machine Translation Evaluation through Fine-grained Error Detection",
        "authors": [
            "Nuno M. Guerreiro",
            "Ricardo Rei",
            "Daan van Stigt",
            "Luisa Coheur",
            "Pierre Colombo",
            "Andr\u00e9 F. T. Martins"
        ],
        "published": "2024",
        "summary": "Widely used learned metrics for machine translation evaluation, such as Comet and Bleurt, estimate the quality of a translation hypothesis by providing a single sentence-level score. As such, they offer little insight into translation errors (e.g., what are the errors and what is their severity). On the other hand, generative large language models (LLMs) are amplifying the adoption of more granular strategies to evaluation, attempting to detail and categorize translation errors. In this work, we introduce xcomet, an open-source learned metric designed to bridge the gap between these approaches. xcomet integrates both sentence-level evaluation and error span detection capabilities, exhibiting state-of-the-art performance across all types of evaluation (sentence-level, system-level, and error span detection). Moreover, it does so while highlighting and categorizing error spans, thus enriching the quality assessment. We also provide a robustness analysis with stress tests, and show that xcomet is largely capable of identifying localized critical errors and hallucinations.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.54.pdf",
        "keywords": [
            "translation errors",
            "machine translation",
            "large language models",
            "error span detection",
            "error detection",
            "xcomet",
            "learned metric",
            "robustness analysis"
        ]
    },
    {
        "title": "SEEP: Training Dynamics Grounds Latent Representation Search for Mitigating Backdoor Poisoning Attacks",
        "authors": [
            "Xuanli He",
            "Qiongkai Xu",
            "Jun Wang",
            "Benjamin I. P. Rubinstein",
            "Trevor Cohn"
        ],
        "published": "2024",
        "summary": "Modern NLP models are often trained on public datasets drawn from diverse sources, rendering them vulnerable to data poisoning attacks. These attacks can manipulate the model\u2019s behavior in ways engineered by the attacker. One such tactic involves the implantation of backdoors, achieved by poisoning specific training instances with a textual trigger and a target class label. Several strategies have been proposed to mitigate the risks associated with backdoor attacks by identifying and removing suspected poisoned examples. However, we observe that these strategies fail to offer effective protection against several advanced backdoor attacks. To remedy this deficiency, we propose a novel defensive mechanism that first exploits training dynamics to identify poisoned samples with high precision, followed by a label propagation step to improve recall and thus remove the majority of poisoned instances. Compared with recent advanced defense methods, our method considerably reduces the success rates of several backdoor attacks while maintaining high classification accuracy on clean test sets.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.55.pdf",
        "keywords": [
            "backdoor attacks",
            "backdoor poisoning attacks",
            "backdoors",
            "data poisoning attacks",
            "training dynamics"
        ]
    },
    {
        "title": "Do LLMs Exhibit Human-like Response Biases? A Case Study in Survey Design",
        "authors": [
            "Lindia Tjuatja",
            "Valerie Chen",
            "Tongshuang Wu",
            "Ameet Talwalkwar",
            "Graham Neubig"
        ],
        "published": "2024",
        "summary": "One widely cited barrier to the adoption of LLMs as proxies for humans in subjective tasks is their sensitivity to prompt wording\u2014but interestingly, humans also display sensitivities to instruction changes in the form of response biases. We investigate the extent to which LLMs reflect human response biases, if at all. We look to survey design, where human response biases caused by changes in the wordings of \u201cprompts\u201d have been extensively explored in social psychology literature. Drawing from these works, we design a dataset and framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior, particularly in models that have undergone RLHF. Furthermore, even if a model shows a significant change in the same direction as humans, we find that they are sensitive to perturbations that do not elicit significant changes in humans. These results highlight the pitfalls of using LLMs as human proxies, and underscore the need for finer-grained characterizations of model behavior.1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.56.pdf",
        "keywords": [
            "response",
            "response biases",
            "biases",
            "case study",
            "survey design"
        ]
    },
    {
        "title": "Multi-level Shared Knowledge Guided Learning for Knowledge Graph Completion",
        "authors": [
            "Yongxue Shan",
            "Jie Zhou",
            "Jie Peng",
            "Xin Zhou",
            "Jiaqian Yin",
            "Xiaodong Wang"
        ],
        "published": "2024",
        "summary": "In the task of Knowledge Graph Completion (KGC), the existing datasets and their inherent subtasks carry a wealth of shared knowledge that can be utilized to enhance the representation of knowledge triplets and overall performance. However, no current studies specifically address the shared knowledge within KGC. To bridge this gap, we introduce a multi-level Shared Knowledge Guided learning method (SKG) that operates at both the dataset and task levels. On the dataset level, SKG-KGC broadens the original dataset by identifying shared features within entity sets via text summarization. On the task level, for the three typical KGC subtasks\u2014head entity prediction, relation prediction, and tail entity prediction\u2014we present an innovative multi-task learning architecture with dynamically adjusted loss weights. This approach allows the model to focus on more challenging and underperforming tasks, effectively mitigating the imbalance of knowledge sharing among subtasks. Experimental results demonstrate that SKG-KGC outperforms existing text-based methods significantly on three well-known datasets, with the most notable improvement on WN18RR (MRR: 66.6%\u2192 72.2%, Hit@1: 58.7%\u219267.0%).",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.57.pdf",
        "keywords": [
            "knowledge graph completion",
            "relation prediction",
            "knowledge sharing",
            "loss weights",
            "multi task learning",
            "shared knowledge guided learning",
            "subtasks"
        ]
    },
    {
        "title": "Do Multi-Document Summarization Models Synthesize?",
        "authors": [
            "Jay DeYoung",
            "Stephanie C. Martinez",
            "Iain J. Marshall",
            "Byron C. Wallace"
        ],
        "published": "2024",
        "summary": "Multi-document summarization entails producing concise synopses of collections of inputs. For some applications, the synopsis should accurately synthesize inputs with respect to a key aspect, e.g., a synopsis of film reviews written about a particular movie should reflect the average critic consensus. As a more consequential example, narrative summaries that accompany biomedical systematic reviews of clinical trial results should accurately summarize the potentially conflicting results from individual trials. In this paper we ask: To what extent do modern multi-document summarization models implicitly perform this sort of synthesis? We run experiments over opinion and evidence synthesis datasets using a suite of summarization models, from fine-tuned transformers to GPT-4. We find that existing models partially perform synthesis, but imperfectly: Even the best performing models are over-sensitive to changes in input ordering and under-sensitive to changes in input compositions (e.g., ratio of positive to negative reviews). We propose a simple, general, effective method for improving model synthesis capabilities by generating an explicitly diverse set of candidate outputs, and then selecting from these the string best aligned with the expected aggregate measure for the inputs, or abstaining when the model produces no good candidate.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.58.pdf",
        "keywords": [
            "document summarization",
            "multi document summarization",
            "model synthesis",
            "narrative"
        ]
    },
    {
        "title": "ARN: Analogical Reasoning on Narratives",
        "authors": [
            "Zhivar Sourati",
            "Filip Ilievski",
            "Pia Sommerauer",
            "Yifan Jiang"
        ],
        "published": "2024",
        "summary": "As a core cognitive skill that enables the transferability of information across domains, analogical reasoning has been extensively studied for both humans and computational models. However, while cognitive theories of analogy often focus on narratives and study the distinction between surface, relational, and system similarities, existing work in natural language processing has a narrower focus as far as relational analogies between word pairs. This gap brings a natural question: can state-of-the-art large language models (LLMs) detect system analogies between narratives? To gain insight into this question and extend word-based relational analogies to relational system analogies, we devise a comprehensive computational framework that operationalizes dominant theories of analogy, using narrative elements to create surface and system mappings. Leveraging the interplay between these mappings, we create a binary task and benchmark for Analogical Reasoning on Narratives (ARN), covering four categories of far (cross-domain)/near (within-domain) analogies and disanalogies. We show that while all LLMs can largely recognize near analogies, even the largest ones struggle with far analogies in a zero-shot setting, with GPT4.0 scoring below random. Guiding the models through solved examples and Chain-of-Thought reasoning enhances their analogical reasoning ability. Yet, since even in the few-shot setting, the best model only performs halfway between random and humans, ARN opens exciting directions for computational analogical reasoners.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.59.pdf",
        "keywords": [
            "narratives",
            "analogical reasoning",
            "computational",
            "computational models",
            "language models",
            "thought reasoning",
            "cognitive theories"
        ]
    },
    {
        "title": "Are Language Models More Like Libraries or Like Librarians? Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs",
        "authors": [
            "Harvey Lederman",
            "Kyle Mahowald"
        ],
        "published": "2024",
        "summary": "Are LLMs cultural technologies like photocopiers or printing presses, which transmit information but cannot create new content? A challenge for this idea, which we call bibliotechnism, is that LLMs generate novel text. We begin with a defense of bibliotechnism, showing how even novel text may inherit its meaning from original human-generated text. We then argue that bibliotechnism faces an independent challenge from examples in which LLMs generate novel reference, using new names to refer to new entities. Such examples could be explained if LLMs were not cultural technologies but had beliefs, desires, and intentions. According to interpretationism in the philosophy of mind, a system has such attitudes if and only if its behavior is well explained by the hypothesis that it does. Interpretationists may hold that LLMs have attitudes, and thus have a simple solution to the novel reference problem. We emphasize, however, that interpretationism is compatible with very simple creatures having attitudes and differs sharply from views that presuppose these attitudes require consciousness, sentience, or intelligence (topics about which we make no claims).",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.60.pdf",
        "keywords": [
            "bibliotechnism",
            "photocopiers",
            "language models",
            "reference problem",
            "libraries"
        ]
    },
    {
        "title": "Segmentation-Free Streaming Machine Translation",
        "authors": [
            "Javier Iranzo-S\u00e1nchez",
            "Jorge Iranzo-S\u00e1nchez",
            "Adri\u00e0 Gim\u00e9nez",
            "Jorge Civera",
            "Alfons Juan"
        ],
        "published": "2024",
        "summary": "Streaming Machine Translation (MT) is the task of translating an unbounded input text stream in real-time. The traditional cascade approach, which combines an Automatic Speech Recognition (ASR) and an MT system, relies on an intermediate segmentation step which splits the transcription stream into sentence-like units. However, the incorporation of a hard segmentation constrains the MT system and is a source of errors. This paper proposes a Segmentation-Free framework that enables the model to translate an unsegmented source stream by delaying the segmentation decision until after the translation has been generated. Extensive experiments show how the proposed Segmentation-Free framework has better quality-latency trade-off than competing approaches that use an independent segmentation model.1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.61.pdf",
        "keywords": [
            "streaming machine translation",
            "segmentation"
        ]
    },
    {
        "title": "Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation",
        "authors": [
            "Cyril Chhun",
            "Fabian M. Suchanek",
            "Chlo\u00e9 Clavel"
        ],
        "published": "2024",
        "summary": "Storytelling is an integral part of human experience and plays a crucial role in social interactions. Thus, Automatic Story Evaluation (ASE) and Generation (ASG) could benefit society in multiple ways, but they are challenging tasks which require high-level human abilities such as creativity, reasoning, and deep understanding. Meanwhile, Large Language Models (LLMs) now achieve state-of-the-art performance on many NLP tasks. In this paper, we study whether LLMs can be used as substitutes for human annotators for ASE. We perform an extensive analysis of the correlations between LLM ratings, other automatic measures, and human annotations, and we explore the influence of prompting on the results and the explainability of LLM behaviour. Most notably, we find that LLMs outperform current automatic measures for system-level evaluation but still struggle at providing satisfactory explanations for their answers.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.62.pdf",
        "keywords": [
            "story evaluation",
            "storytelling",
            "automatic story evaluation",
            "language models"
        ]
    },
    {
        "title": "How Often Are Errors in Natural Language Reasoning Due to Paraphrastic Variability?",
        "authors": [
            "Neha Srikanth",
            "Marine Carpuat",
            "Rachel Rudinger"
        ],
        "published": "2024",
        "summary": "Large language models have been shown to behave inconsistently in response to meaning-preserving paraphrastic inputs. At the same time, researchers evaluate the knowledge and reasoning abilities of these models with test evaluations that do not disaggregate the effect of paraphrastic variability on performance. We propose a metric, PC, for evaluating the paraphrastic consistency of natural language reasoning models based on the probability of a model achieving the same correctness on two paraphrases of the same problem. We mathematically connect this metric to the proportion of a model\u2019s variance in correctness attributable to paraphrasing. To estimate PC, we collect ParaNlu, a dataset of 7,782 human-written and validated paraphrased reasoning problems constructed on top of existing benchmark datasets for defeasible and abductive natural language inference.1 Using ParaNlu, we measure the paraphrastic consistency of several model classes and show that consistency dramatically increases with pretraining but not fine-tuning. All models tested exhibited room for improvement in paraphrastic consistency.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.63.pdf",
        "keywords": [
            "paraphrastic consistency",
            "test evaluations",
            "paraphrastic variability",
            "language",
            "natural language reasoning",
            "paraphrased reasoning",
            "abductive natural language"
        ]
    },
    {
        "title": "Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization",
        "authors": [
            "George Chrysostomou",
            "Zhixue Zhao",
            "Miles Williams",
            "Nikolaos Aletras"
        ],
        "published": "2024",
        "summary": "Despite the remarkable performance of generative large language models (LLMs) on abstractive summarization, they face two significant challenges: their considerable size and tendency to hallucinate. Hallucinations are concerning because they erode reliability and raise safety issues. Pruning is a technique that reduces model size by removing redundant weights, enabling more efficient sparse inference. Pruned models yield downstream task performance comparable to the original, making them ideal alternatives when operating on a limited budget. However, the effect that pruning has upon hallucinations in abstractive summarization with LLMs has yet to be explored. In this paper, we provide an extensive empirical study across five summarization datasets, two state-of-the-art pruning methods, and five instruction-tuned LLMs. Surprisingly, we find that hallucinations are less prevalent from pruned LLMs than the original models. Our analysis suggests that pruned models tend to depend more on the source document for summary generation. This leads to a higher lexical overlap between the generated summary and the source document, which could be a reason for the reduction in hallucination risk.1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.64.pdf",
        "keywords": [
            "hallucination",
            "summarization",
            "abstractive summarization",
            "pruned"
        ]
    },
    {
        "title": "Hypernetworks for Personalizing ASR to Atypical Speech",
        "authors": [
            "Max M\u00fcller-Eberstein",
            "Dianna Yee",
            "Karren Yang",
            "Gautam Varma Mantena",
            "Colin Lea"
        ],
        "published": "2024",
        "summary": "Parameter-efficient fine-tuning (PEFT) for personalizing automatic speech recognition (ASR) has recently shown promise for adapting general population models to atypical speech. However, these approaches assume a priori knowledge of the atypical speech disorder being adapted for\u2014the diagnosis of which requires expert knowledge that is not always available. Even given this knowledge, data scarcity and high inter-/intra-speaker variability further limit the effectiveness of traditional fine-tuning. To circumvent these challenges, we first identify the minimal set of model parameters required for ASR adaptation. Our analysis of each individual parameter\u2019s effect on adaptation performance allows us to reduce Word Error Rate (WER) by half while adapting 0.03% of all weights. Alleviating the need for cohort-specific models, we next propose the novel use of a meta-learned hypernetwork to generate highly individualized, utterance-level adaptations on-the-fly for a diverse set of atypical speech characteristics. Evaluating adaptation at the global, cohort, and individual-level, we show that hypernetworks generalize better to out-of-distribution speakers, while maintaining an overall relative WER reduction of 75.2% using 0.1% of the full parameter budget.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.65.pdf",
        "keywords": [
            "asr",
            "personalizing asr",
            "personalizing",
            "hypernetworks",
            "wer",
            "relative wer",
            "fine tuning",
            "parameter"
        ]
    },
    {
        "title": "Retrieval-Pretrained Transformer: Long-range Language Modeling with Self-retrieval",
        "authors": [
            "Ohad Rubin",
            "Jonathan Berant"
        ],
        "published": "2024",
        "summary": "Retrieval-augmented language models (LMs) have received much attention recently. However, typically the retriever is not trained jointly as a native component of the LM, but added post-hoc to an already-pretrained LM, which limits the ability of the LM and the retriever to adapt to one another. In this work, we propose the Retrieval-Pretrained Transformer (RPT), an architecture and training procedure for jointly training a retrieval-augmented LM from scratch and applying it to the task of modeling long texts. Given a recently generated text chunk in a long document, the LM computes query representations, which are then used to retrieve earlier chunks in the document, located potentially tens of thousands of tokens before. Information from retrieved chunks is fused into the LM representations to predict the next target chunk. We train the retriever component with a semantic objective, where the goal is to retrieve chunks that increase the probability of the next chunk, according to a reference LM. We evaluate RPT on four long-range language modeling tasks, spanning books, code, and mathematical writing, and demonstrate that RPT improves retrieval quality and subsequently perplexity across the board compared to strong baselines.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.66.pdf",
        "keywords": [
            "long range language modeling",
            "long range language",
            "retrieval augmented language models",
            "self retrieval"
        ]
    },
    {
        "title": "Retrieval-style In-context Learning for Few-shot Hierarchical Text Classification",
        "authors": [
            "Huiyao Chen",
            "Yu Zhao",
            "Zulong Chen",
            "Mengjia Wang",
            "Liangyue Li",
            "Meishan Zhang",
            "Min Zhang"
        ],
        "published": "2024",
        "summary": "Hierarchical text classification (HTC) is an important task with broad applications, and few-shot HTC has gained increasing interest recently. While in-context learning (ICL) with large language models (LLMs) has achieved significant success in few-shot learning, it is not as effective for HTC because of the expansive hierarchical label sets and extremely ambiguous labels. In this work, we introduce the first ICL-based framework with LLM for few-shot HTC. We exploit a retrieval database to identify relevant demonstrations, and an iterative policy to manage multi-layer hierarchical labels. Particularly, we equip the retrieval database with HTC label-aware representations for the input texts, which is achieved by continual training on a pretrained language model with masked language modeling (MLM), layer-wise classification (CLS, specifically for HTC), and a novel divergent contrastive learning (DCL, mainly for adjacent semantically similar labels) objective. Experimental results on three benchmark datasets demonstrate superior performance of our method, and we can achieve state-of-the-art results in few-shot HTC.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.67.pdf",
        "keywords": [
            "htc",
            "hierarchical text classification",
            "context learning",
            "htc label aware representations",
            "few shot htc",
            "in context learning"
        ]
    },
    {
        "title": "Do Vision and Language Models Share Concepts? A Vector Space Alignment Study",
        "authors": [
            "Jiaang Li",
            "Yova Kementchedjhieva",
            "Constanza Fierro",
            "Anders S\u00f8gaard"
        ],
        "published": "2024",
        "summary": "Large-scale pretrained language models (LMs) are said to \u201clack the ability to connect utterances to the world\u201d (Bender and Koller, 2020), because they do not have \u201cmental models of the world\u201d (Mitchell and Krakauer, 2023). If so, one would expect LM representations to be unrelated to representations induced by vision models. We present an empirical evaluation across four families of LMs (BERT, GPT-2, OPT, and LLaMA-2) and three vision model architectures (ResNet, SegFormer, and MAE). Our experiments show that LMs partially converge towards representations isomorphic to those of vision models, subject to dispersion, polysemy, and frequency. This has important implications for both multi-modal processing and the LM understanding debate (Mitchell and Krakauer, 2023).1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.68.pdf",
        "keywords": [
            "mental models",
            "vector space alignment",
            "language models",
            "vision models",
            "polysemy",
            "resnet",
            "lm understanding",
            "evaluation"
        ]
    },
    {
        "title": "Assessing the Role of Context in Chat Translation Evaluation: Is Context Helpful and Under What Conditions?",
        "authors": [
            "Sweta Agrawal",
            "Amin Farajian",
            "Patrick Fernandes",
            "Ricardo Rei",
            "Andr\u00e9 F. T. Martins"
        ],
        "published": "2024",
        "summary": "Despite the recent success of automatic metrics for assessing translation quality, their application in evaluating the quality of machine-translated chats has been limited. Unlike more structured texts like news, chat conversations are often unstructured, short, and heavily reliant on contextual information. This poses questions about the reliability of existing sentence-level metrics in this domain as well as the role of context in assessing the translation quality. Motivated by this, we conduct a meta-evaluation of existing automatic metrics, primarily designed for structured domains such as news, to assess the quality of machine-translated chats. We find that reference-free metrics lag behind reference-based ones, especially when evaluating translation quality in out-of-English settings. We then investigate how incorporating conversational contextual information in these metrics for sentence-level evaluation affects their performance. Our findings show that augmenting neural learned metrics with contextual information helps improve correlation with human judgments in the reference-free scenario and when evaluating translations in out-of-English settings. Finally, we propose a new evaluation metric, Context-MQM, that utilizes bilingual context with a large language model (LLM) and further validate that adding context helps even for LLM-based evaluation metrics.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.69.pdf",
        "keywords": [
            "translations",
            "translation quality",
            "translated chats",
            "context",
            "chat translation evaluation",
            "language model"
        ]
    },
    {
        "title": "Not Eliminate but Aggregate: Post-Hoc Control over Mixture-of-Experts to Address Shortcut Shifts in Natural Language Understanding",
        "authors": [
            "Ukyo Honda",
            "Tatsushi Oka",
            "Peinan Zhang",
            "Masato Mita"
        ],
        "published": "2024",
        "summary": "Recent models for natural language understanding are inclined to exploit simple patterns in datasets, commonly known as shortcuts. These shortcuts hinge on spurious correlations between labels and latent features existing in the training data. At inference time, shortcut-dependent models are likely to generate erroneous predictions under distribution shifts, particularly when some latent features are no longer correlated with the labels. To avoid this, previous studies have trained models to eliminate the reliance on shortcuts. In this study, we explore a different direction: pessimistically aggregating the predictions of a mixture-of-experts, assuming each expert captures relatively different latent features. The experimental results demonstrate that our post-hoc control over the experts significantly enhances the model\u2019s robustness to the distribution shift in shortcuts. Additionally, we show that our approach has some practical advantages. We also analyze our model and provide results to support the assumption.1",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.70.pdf",
        "keywords": [
            "natural language",
            "post hoc control",
            "aggregate",
            "shortcut dependent models",
            "shortcut shifts"
        ]
    },
    {
        "title": "Reading Subtext: Evaluating Large Language Models on Short Story Summarization with Writers",
        "authors": [
            "Melanie Subbiah",
            "Sean Zhang",
            "Lydia B. Chilton",
            "Kathleen McKeown"
        ],
        "published": "2024",
        "summary": "We evaluate recent Large Language Models (LLMs) on the challenging task of summarizing short stories, which can be lengthy, and include nuanced subtext or scrambled timelines. Importantly, we work directly with authors to ensure that the stories have not been shared online (and therefore are unseen by the models), and to obtain informed evaluations of summary quality using judgments from the authors themselves. Through quantitative and qualitative analysis grounded in narrative theory, we compare GPT-4, Claude-2.1, and LLama-2-70B. We find that all three models make faithfulness mistakes in over 50% of summaries and struggle with specificity and interpretation of difficult subtext. We additionally demonstrate that LLM ratings and other automatic metrics for summary quality do not correlate well with the quality ratings from the writers.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.71.pdf",
        "keywords": [
            "language models",
            "reading subtext",
            "subtext",
            "summarizing",
            "narrative theory",
            "short story summarization",
            "writers"
        ]
    },
    {
        "title": "L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models",
        "authors": [
            "Ansong Ni",
            "Pengcheng Yin",
            "Yilun Zhao",
            "Martin Riddell",
            "Troy Feng",
            "Rui Shen",
            "Stephen Yin",
            "Ye Liu",
            "Semih Yavuz",
            "Caiming Xiong",
            "Shafiq Joty",
            "Yingbo Zhou",
            "Dragomir Radev",
            "Arman Cohan",
            "Arman Cohan"
        ],
        "published": "2024",
        "summary": "Recently, large language models (LLMs), especially those that are pretrained on code, have demonstrated strong capabilities in generating programs from natural language inputs. Despite promising results, there is a notable lack of a comprehensive evaluation of these models\u2019 language-to-code generation capabilities. Existing studies often focus on specific tasks, model architectures, or learning paradigms, leading to a fragmented understanding of the overall landscape. In this work, we present L2CEval, a systematic evaluation of the language-to-code generation capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing, math reasoning, and Python programming, analyzing the factors that potentially affect their performance, such as model size, pretraining data, instruction tuning, and different prompting methods. In addition, we assess confidence calibration, and conduct human evaluations to identify typical failures across different tasks and models. L2CEval offers a comprehensive understanding of the capabilities and limitations of LLMs in language-to-code generation. We release the evaluation framework1 and all model outputs, hoping to lay the groundwork for further future research. All future evaluations (e.g., LLaMA-3, StarCoder2, etc) will be updated on the project website: https://l2c-eval.github.io/.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.72.pdf",
        "keywords": [
            "language",
            "language models",
            "code generation",
            "confidence calibration",
            "python"
        ]
    },
    {
        "title": "NoviCode: Generating Programs from Natural Language Utterances by Novices",
        "authors": [
            "Asaf Achi Mordechai",
            "Yoav Goldberg",
            "Reut Tsarfaty"
        ],
        "published": "2024",
        "summary": "Current Text-to-Code models demonstrate impressive capabilities in generating executable code from natural language snippets. However, current studies focus on technical instructions and programmer-oriented language, and it is an open question whether these models can effectively translate natural language descriptions given by non-technical users and express complex goals, to an executable program that contains an intricate flow\u2014composed of API access and control structures as loops, conditions, and sequences. To unlock the challenge of generating a complete program from a plain non-technical description we present NoviCode, a novel NL Programming task, which takes as input an API and a natural language description by a novice non-programmer, and provides an executable program as output. To assess the efficacy of models on this task, we provide a novel benchmark accompanied by test suites wherein the generated program code is assessed not according to their form, but according to their functional execution. Our experiments show that, first, NoviCode is indeed a challenging task in the code synthesis domain, and that generating complex code from non-technical instructions goes beyond the current Text-to-Code paradigm. Second, we show that a novel approach wherein we align the NL utterances with the compositional hierarchical structure of the code, greatly enhances the performance of LLMs on this task, compared with the end-to-end Text-to-Code counterparts.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.73.pdf",
        "keywords": [
            "novicode",
            "natural language",
            "nl programming",
            "natural language description",
            "code",
            "text to code"
        ]
    },
    {
        "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",
        "authors": [
            "Tyler A. Chang",
            "Zhuowen Tu",
            "Benjamin K. Bergen"
        ],
        "published": "2024",
        "summary": "How do language models learn to make predictions during pre-training? To study this, we extract learning curves from five autoregressive English language model pre-training runs, for 1M unseen tokens in context. We observe that the language models generate short repetitive phrases before learning to generate longer and more coherent text. We also find that individual tokens often exhibit sudden increases or decreases in loss that are surprisingly consistent across pre-training runs. To better understand these fluctuations, we quantify the final surprisal, within-run variability, age of acquisition, forgettability, and cross-run variability of learning curves for individual tokens in context. More frequent tokens reach lower final surprisals, exhibit less variability within and across pre-training runs, are learned earlier, and are less likely to be \u201cforgotten\u201d during pre-training. Higher n-gram probabilities further accentuate these effects. Independent of the target token, shorter and more frequent contexts correlate with marginally more stable and quickly acquired predictions. Based on our results, we argue for the existence of sequential learning dependencies between different model capabilities, and we characterize language model learning as early n-gram learning before gradual refinement of tail n-gram predictions.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.74.pdf",
        "keywords": [
            "learning",
            "forgetting",
            "learning curves",
            "forgettability",
            "language model learning",
            "language model pre",
            "pre training",
            "gram learning"
        ]
    },
    {
        "title": "Addressing Topic Leakage in Cross-Topic Evaluation for Authorship Verification",
        "authors": [
            "Jitkapat Sawatphol",
            "Can Udomcharoenchaikit",
            "Sarana Nutanong"
        ],
        "published": "2024",
        "summary": "Authorship verification (AV) aims to identify whether a pair of texts has the same author. We address the challenge of evaluating AV models\u2019 robustness against topic shifts. The conventional evaluation assumes minimal topic overlap between training and test data. However, we argue that there can still be topic leakage in test data, causing misleading model performance and unstable rankings. To address this, we propose an evaluation method called Heterogeneity-Informed Topic Sampling (HITS), which creates a smaller dataset with a heterogeneously distributed topic set. Our experimental results demonstrate that HITS-sampled datasets yield a more stable ranking of models across random seeds and evaluation splits. Our contributions include: 1. An analysis of causes and effects of topic leakage; 2. A demonstration of the HITS in reducing the effects of topic leakage; and 3. The Robust Authorship Verification bENchmark (RAVEN) that allows topic shortcut test to uncover AV models\u2019 reliance on topic-specific features.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.75.pdf",
        "keywords": [
            "authorship verification",
            "topic sampling",
            "cross topic evaluation",
            "topic leakage"
        ]
    },
    {
        "title": "Beyond Prompt Brittleness: Evaluating the Reliability and Consistency of Political Worldviews in LLMs",
        "authors": [
            "Tanise Ceron",
            "Neele Falk",
            "Ana Bari\u0107",
            "Dmitry Nikolaev",
            "Sebastian Pad\u00f3"
        ],
        "published": "2024",
        "summary": "Due to the widespread use of large language models (LLMs), we need to understand whether they embed a specific \u201cworldview\u201d and what these views reflect. Recent studies report that, prompted with political questionnaires, LLMs show left-liberal leanings (Feng et al., 2023; Motoki et al., 2024). However, it is as yet unclear whether these leanings are reliable (robust to prompt variations) and whether the leaning is consistent across policies and political leaning. We propose a series of tests which assess the reliability and consistency of LLMs\u2019 stances on political statements based on a dataset of voting-advice questionnaires collected from seven EU countries and annotated for policy issues. We study LLMs ranging in size from 7B to 70B parameters and find that their reliability increases with parameter count. Larger models show overall stronger alignment with left-leaning parties but differ among policy programs: They show a (left-wing) positive stance towards environment protection, social welfare state, and liberal society but also (right-wing) law and order, with no consistent preferences in the areas of foreign policy and migration.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.76.pdf",
        "keywords": [
            "reliability",
            "political worldviews",
            "political",
            "llms",
            "political leaning"
        ]
    },
    {
        "title": "Self-supervised Topic Taxonomy Discovery in the Box Embedding Space",
        "authors": [
            "Yuyin Lu",
            "Hegang Chen",
            "Pengbo Mao",
            "Yanghui Rao",
            "Haoran Xie",
            "Fu Lee Wang",
            "Qing Li"
        ],
        "published": "2024",
        "summary": "Topic taxonomy discovery aims at uncovering topics of different abstraction levels and constructing hierarchical relations between them. Unfortunately, most prior work can hardly model semantic scopes of words and topics by holding the Euclidean embedding space assumption. What\u2019s worse, they infer asymmetric hierarchical relations by symmetric distances between topic embeddings. As a result, existing methods suffer from problems of low-quality topics at high abstraction levels and inaccurate hierarchical relations. To alleviate these problems, this paper develops a Box embedding-based Topic Model (BoxTM) that maps words and topics into the box embedding space, where the asymmetric metric is defined to properly infer hierarchical relations among topics. Additionally, our BoxTM explicitly infers upper-level topics based on correlation between specific topics through recursive clustering on topic boxes. Finally, extensive experiments validate high-quality of the topic taxonomy learned by BoxTM.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.77.pdf",
        "keywords": [
            "topic taxonomy",
            "topic taxonomy discovery",
            "topic model",
            "box"
        ]
    },
    {
        "title": "When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs",
        "authors": [
            "Ryo Kamoi",
            "Yusen Zhang",
            "Nan Zhang",
            "Jiawei Han",
            "Rui Zhang"
        ],
        "published": "2024",
        "summary": "Self-correction is an approach to improving responses from large language models (LLMs) by refining the responses using LLMs during inference. Prior work has proposed various self-correction frameworks using different sources of feedback, including self-evaluation and external feedback. However, there is still no consensus on the question of when LLMs can correct their own mistakes, as recent studies also report negative results. In this work, we critically survey broad papers and discuss the conditions required for successful self-correction. We first find that prior studies often do not define their research questions in detail and involve impractical frameworks or unfair evaluations that over-evaluate self-correction. To tackle these issues, we categorize research questions in self-correction research and provide a checklist for designing appropriate experiments. Our critical survey based on the newly categorized research questions shows that (1) no prior work demonstrates successful self-correction with feedback from prompted LLMs, except for studies in tasks that are exceptionally suited for self-correction, (2) self-correction works well in tasks that can use reliable external feedback, and (3) large-scale fine-tuning enables self-correction.",
        "pdf_link": "https://aclanthology.org/2024.tacl-1.78.pdf",
        "keywords": [
            "self correction",
            "categorize",
            "survey"
        ]
    }
]