[
    {
        "title": "Model Cascading: Towards Jointly Improving Efficiency and Accuracy of NLP Systems",
        "authors": [
            "Neeraj Varshney",
            "Chitta Baral"
        ],
        "published": "2022",
        "summary": "Do all instances need inference through the big models for a correct prediction? Perhaps not; some instances are easy and can be answered correctly by even small capacity models. This provides opportunities for improving the computational efficiency of systems. In this work, we present an explorative study on ‘model cascading’, a simple technique that utilizes a collection of models of varying capacities to accurately yet efficiently output predictions. Through comprehensive experiments in multiple task settings that differ in the number of models available for cascading (K value), we show that cascading improves both the computational efficiency and the prediction accuracy. For instance, in K=3 setting, cascading saves up to 88.93% computation cost and consistently achieves superior prediction accuracy with an improvement of up to 2.18%. We also study the impact of introducing additional models in the cascade and show that it further increases the efficiency improvements. Finally, we hope that our work will facilitate development of efficient NLP systems making their widespread adoption in real-world applications possible.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.756.pdf",
        "keywords": [],
        "venue": "EMNLP",
        "year": "2022",
        "source_file": "emnlp2022_tnt_kid.json"
    },
    {
        "title": "Answer Consolidation: Formulation and Benchmarking",
        "authors": [
            "Wenxuan Zhou",
            "Qiang Ning",
            "Heba Elfardy",
            "Kevin Small",
            "Muhao Chen"
        ],
        "published": "2022",
        "summary": "Current question answering (QA) systems primarily consider the single-answer scenario, where each question is assumed to be paired with one correct answer. However, in many real-world QA applications, multiple answer scenarios arise where consolidating answers into a comprehensive and non-redundant set of answers is a more efficient user interface. In this paper, we formulate the problem of answer consolidation, where answers are partitioned into multiple groups, each representing different aspects of the answer set. Then, given this partitioning, a comprehensive and non-redundant set of answers can be constructed by picking one answer from each group. To initiate research on answer consolidation, we construct a dataset consisting of 4,699 questions and 24,006 sentences and evaluate multiple models. Despite a promising performance achieved by the best-performing supervised models, we still believe this task has room for further improvements.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.320.pdf",
        "keywords": [
            "consolidating",
            "answer consolidation",
            "benchmarking",
            "question answering"
        ],
        "venue": "NAACL",
        "year": "2022",
        "source_file": "naacl2022_tnt_kid.json"
    },
    {
        "title": "T-Modules: Translation Modules for Zero-Shot Cross-Modal Machine Translation",
        "authors": [
            "Paul-Ambroise Duquenne",
            "Hongyu Gong",
            "Benoît Sagot",
            "Holger Schwenk"
        ],
        "published": "2022",
        "summary": "We present a new approach to perform zero-shot cross-modal transfer between speech and text for translation tasks. Multilingual speech and text are encoded in a joint fixed-size representation space. Then, we compare different approaches to decode these multimodal and multilingual fixed-size representations, enabling zero-shot translation between languages and modalities. All our models are trained without the need of cross-modal labeled translation data.Despite a fixed-size representation, we achieve very competitive results on several text and speech translation tasks. In particular, we significantly improve the state-of-the-art for zero-shot speech translation on Must-C. Incorporating a speech decoder in our framework, we introduce the first results for zero-shot direct speech-to-speech and text-to-speech translation.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.391.pdf",
        "keywords": [
            "translation",
            "translation modules",
            "speech translation",
            "cross modal machine translation",
            "shot translation",
            "zero shot speech translation"
        ],
        "venue": "EMNLP",
        "year": "2022",
        "source_file": "emnlp2022_tnt_kid.json"
    },
    {
        "title": "Adversarial Support Alignment",
        "authors": [
            "Shangyuan Tong",
            "Timur Garipov",
            "Yang Zhang",
            "Shiyu Chang",
            "Tommi S. Jaakkola"
        ],
        "published": "ICLR 2022 Spotlight",
        "summary": "We study the problem of aligning the supports of distributions. Compared to the existing work on distribution alignment, support alignment does not require the densities to be matched. We propose symmetric support difference as a divergence measure to quantify the mismatch between supports. We show that select discriminators (e.g. discriminator trained for Jensen-Shannon divergence) are able to map support differences as support differences in their one-dimensional output space. Following this result, our method aligns supports by minimizing a symmetrized relaxed optimal transport cost in the discriminator 1D space via an adversarial process. Furthermore, we show that our approach can be viewed as a limit of existing notions of alignment by increasing transportation assignment tolerance. We quantitatively evaluate the method across domain adaptation tasks with shifts in label distributions. Our experiments show that the proposed method is more robust against these shifts than other alignment-based baselines.",
        "pdf_link": "https://openreview.net/pdf/baf81164820438550e81b120efdf1f7f96cd349d.pdf",
        "forum_url": "https://openreview.net/forum?id=26gKg6x-ie",
        "keywords": [
            "aligns",
            "support alignment",
            "distribution alignment",
            "adversarial support alignment"
        ],
        "venue": "ICLR",
        "year": "2022",
        "source_file": "iclr2022_tnt_kid.json"
    },
    {
        "title": "MUSIED: A Benchmark for Event Detection from Multi-Source Heterogeneous Informal Texts",
        "authors": [
            "Xiangyu Xi",
            "Jianwei Lv",
            "Shuaipeng Liu",
            "Wei Ye",
            "Fan Yang",
            "Guanglu Wan"
        ],
        "published": "2022",
        "summary": "Event detection (ED) identifies and classifies event triggers from unstructured texts, serving as a fundamental task for information extraction. Despite the remarkable progress achieved in the past several years, most research efforts focus on detecting events from formal texts (e.g., news articles, Wikipedia documents, financial announcements). Moreover, the texts in each dataset are either from a single source or multiple yet relatively homogeneous sources. With massive amounts of user-generated text accumulating on the Web and inside enterprises, identifying meaningful events in these informal texts, usually from multiple heterogeneous sources, has become a problem of significant practical value. As a pioneering exploration that expands event detection to the scenarios involving informal and heterogeneous texts, we propose a new large-scale Chinese event detection dataset based on user reviews, text conversations, and phone conversations in a leading e-commerce platform for food service. We carefully investigate the proposed dataset’s textual informality and multi-domain heterogeneity characteristics by inspecting data samples quantitatively and qualitatively. Extensive experiments with state-of-the-art event detection methods verify the unique challenges posed by these characteristics, indicating that multi-domain informal event detection remains an open problem and requires further efforts. Our benchmark and code are released at https://github.com/myeclipse/MUSIED.",
        "pdf_link": "https://aclanthology.org/2022.emnlp-main.191.pdf",
        "keywords": [
            "event detection",
            "informal event detection",
            "heterogeneous informal texts",
            "announcements",
            "informality"
        ],
        "venue": "EMNLP",
        "year": "2022",
        "source_file": "emnlp2022_tnt_kid.json"
    },
    {
        "title": "GMN: Generative Multi-modal Network for Practical Document Information Extraction",
        "authors": [
            "Haoyu Cao",
            "Jiefeng Ma",
            "Antai Guo",
            "Yiqing Hu",
            "Hao Liu",
            "Deqiang Jiang",
            "Yinsong Liu",
            "Bo Ren"
        ],
        "published": "2022",
        "summary": "Document Information Extraction (DIE) has attracted increasing attention due to its various advanced applications in the real world. Although recent literature has already achieved competitive results, these approaches usually fail when dealing with complex documents with noisy OCR results or mutative layouts. This paper proposes Generative Multi-modal Network (GMN) for real-world scenarios to address these problems, which is a robust multi-modal generation method without predefined label categories. With the carefully designed spatial encoder and modal-aware mask module, GMN can deal with complex documents that are hard to serialized into sequential order. Moreover, GMN tolerates errors in OCR results and requires no character-level annotation, which is vital because fine-grained annotation of numerous documents is laborious and even requires annotators with specialized domain knowledge. Extensive experiments show that GMN achieves new state-of-the-art performance on several public DIE datasets and surpasses other methods by a large margin, especially in realistic scenes.",
        "pdf_link": "https://aclanthology.org/2022.naacl-main.276.pdf",
        "keywords": [
            "document information extraction",
            "multi modal network",
            "generative multi modal network"
        ],
        "venue": "NAACL",
        "year": "2022",
        "source_file": "naacl2022_tnt_kid.json"
    },
    {
        "title": "Dodging the Data Bottleneck: Automatic Subtitling with Automatically Segmented ST Corpora",
        "authors": [
            "Sara Papi",
            "Alina Karakanta",
            "Matteo Negri",
            "Marco Turchi"
        ],
        "published": "2022",
        "summary": "Speech translation for subtitling (SubST) is the task of automatically translating speech data into well-formed subtitles by inserting subtitle breaks compliant to specific displaying guidelines. Similar to speech translation (ST), model training requires parallel data comprising audio inputs paired with their textual translations. In SubST, however, the text has to be also annotated with subtitle breaks. So far, this requirement has represented a bottleneck for system development, as confirmed by the dearth of publicly available SubST corpora. To fill this gap, we propose a method to convert existing ST corpora into SubST resources without human intervention. We build a segmenter model that automatically segments texts into proper subtitles by exploiting audio and text in a multimodal fashion, achieving high segmentation quality in zero-shot conditions. Comparative experiments with SubST systems respectively trained on manual and automatic segmentations result in similar performance, showing the effectiveness of our approach.",
        "pdf_link": "https://aclanthology.org/2022.aacl-short.59.pdf",
        "keywords": [
            "subtitling",
            "speech translation",
            "st corpora"
        ],
        "venue": "AACL",
        "year": "2022",
        "source_file": "aacl2022_tnt_kid.json"
    },
    {
        "title": "Graph Enhanced Contrastive Learning for Radiology Findings Summarization",
        "authors": [
            "Jinpeng Hu",
            "Zhuo Li",
            "Zhihong Chen",
            "Zhen Li",
            "Xiang Wan",
            "Tsung-Hui Chang"
        ],
        "published": "2022",
        "summary": "The impression section of a radiology report summarizes the most prominent observation from the findings section and is the most important section for radiologists to communicate to physicians. Summarizing findings is time-consuming and can be prone to error for inexperienced radiologists, and thus automatic impression generation has attracted substantial attention. With the encoder-decoder framework, most previous studies explore incorporating extra knowledge (e.g., static pre-defined clinical ontologies or extra background information). Yet, they encode such knowledge by a separate encoder to treat it as an extra input to their models, which is limited in leveraging their relations with the original findings. To address the limitation, we propose a unified framework for exploiting both extra knowledge and the original findings in an integrated way so that the critical information (i.e., key words and their relations) can be extracted in an appropriate way to facilitate impression generation. In detail, for each input findings, it is encoded by a text encoder and a graph is constructed through its entities and dependency tree. Then, a graph encoder (e.g., graph neural networks (GNNs)) is adopted to model relation information in the constructed graph. Finally, to emphasize the key words in the findings, contrastive learning is introduced to map positive samples (constructed by masking non-key words) closer and push apart negative ones (constructed by masking key words). The experimental results on two datasets, OpenI and MIMIC-CXR, confirm the effectiveness of our proposed method, where the state-of-the-art results are achieved.",
        "pdf_link": "https://aclanthology.org/2022.acl-long.320.pdf",
        "keywords": [
            "contrastive learning",
            "graph neural networks",
            "radiology"
        ],
        "venue": "ACL",
        "year": "2022",
        "source_file": "acl2022_tnt_kid.json"
    },
    {
        "title": "ALCAP: Alignment-Augmented Music Captioner",
        "authors": [
            "Zihao He",
            "Weituo Hao",
            "Wei-Tsung Lu",
            "Changyou Chen",
            "Kristina Lerman",
            "Xuchen Song"
        ],
        "published": "2023",
        "summary": "Music captioning has gained significant attention in the wake of the rising prominence of streaming media platforms. Traditional approaches often prioritize either the audio or lyrics aspect of the music, inadvertently ignoring the intricate interplay between the two. However, a comprehensive understanding of music necessitates the integration of both these elements. In this study, we delve into this overlooked realm by introducing a method to systematically learn multimodal alignment between audio and lyrics through contrastive learning. This not only recognizes and emphasizes the synergy between audio and lyrics but also paves the way for models to achieve deeper cross-modal coherence, thereby producing high-quality captions. We provide both theoretical and empirical results demonstrating the advantage of the proposed method, which achieves new state-of-the-art on two music captioning datasets.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.1028.pdf",
        "keywords": [
            "lyrics",
            "captioning",
            "music captioning",
            "alignment augmented music captioner"
        ],
        "venue": "EMNLP",
        "year": "2023",
        "source_file": "emnlp2023_tnt_kid.json"
    },
    {
        "title": "Dialect-robust Evaluation of Generated Text",
        "authors": [
            "Jiao Sun",
            "Thibault Sellam",
            "Elizabeth Clark",
            "Tu Vu",
            "Timothy Dozat",
            "Dan Garrette",
            "Aditya Siddhant",
            "Jacob Eisenstein",
            "Sebastian Gehrmann"
        ],
        "published": "2023",
        "summary": "Text generation metrics that are not robust to dialect variation make it impossible to tell how well systems perform for many groups of users, and can even penalize systems for producing text in lower-resource dialects. In this paper, we introduce a suite of methods to assess whether metrics are dialect robust. These methods show that state-of-the-art metrics are not dialect robust: they often prioritize dialect similarity over semantics, preferring outputs that are semantically incorrect over outputs that match the semantics of the reference but contain dialect differences. As a step towards dialect-robust metrics for text generation, we propose NANO, which introduces regional and language information to the metric’s pretraining. NANO significantly improves dialect robustness while preserving the correlation between automated metrics and human ratings. It also enables a more ambitious approach to evaluation, dialect awareness, in which system outputs are scored by both semantic match to the reference and appropriateness in any specified dialect.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.331.pdf",
        "keywords": [
            "dialect robust",
            "generated text",
            "text generation",
            "text",
            "dialect robust metrics",
            "language information",
            "dialect similarity",
            "automated metrics",
            "dialect awareness",
            "art metrics",
            "methods",
            "dialect variation",
            "evaluation",
            "semantic"
        ],
        "venue": "ACL",
        "year": "2023",
        "source_file": "acl2023_tnt_kid.json"
    },
    {
        "title": "PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers",
        "authors": [
            "Michael Saxon",
            "Xinyi Wang",
            "Wenda Xu",
            "William Yang Wang"
        ],
        "published": "2023",
        "summary": "Building natural language inference (NLI) benchmarks that are both challenging for modern techniques, and free from shortcut biases is difficult. Chief among these biases is “single sentence label leakage,” where annotator-introduced spurious correlations yield datasets where the logical relation between (premise, hypothesis) pairs can be accurately predicted from only a single sentence, something that should in principle be impossible. We demonstrate that despite efforts to reduce this leakage, it persists in modern datasets that have been introduced since its 2018 discovery. To enable future amelioration efforts, introduce a novel model-driven technique, the progressive evaluation of cluster outliers (PECO) which enables both the objective measurement of leakage, and the automated detection of subpopulations in the data which maximally exhibit it.",
        "pdf_link": "https://aclanthology.org/2023.eacl-main.223.pdf",
        "keywords": [
            "natural language inference",
            "cluster outliers",
            "label leakage",
            "progressive evaluation"
        ],
        "venue": "EACL",
        "year": "2023",
        "source_file": "eacl2023_tnt_kid.json"
    },
    {
        "title": "DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models",
        "authors": [
            "Zijie J. Wang",
            "Evan Montoya",
            "David Munechika",
            "Haoyang Yang",
            "Benjamin Hoover",
            "Duen Horng Chau"
        ],
        "published": "2023",
        "summary": "With recent advancements in diffusion models, users can generate high-quality images by writing text prompts in natural language. However, generating images with desired details requires proper prompts, and it is often unclear how a model reacts to different prompts or what the best prompts are. To help researchers tackle these critical challenges, we introduce DiffusionDB, the first large-scale text-to-image prompt dataset totaling 6.5TB, containing 14 million images generated by Stable Diffusion, 1.8 million unique prompts, and hyperparameters specified by real users. We analyze the syntactic and semantic characteristics of prompts. We pinpoint specific hyperparameter values and prompt styles that can lead to model errors and present evidence of potentially harmful model usage, such as the generation of misinformation. The unprecedented scale and diversity of this human-actuated dataset provide exciting research opportunities in understanding the interplay between prompts and generative models, detecting deepfakes, and designing human-AI interaction tools to help users more easily use these models. DiffusionDB is publicly available at: https://poloclub.github.io/diffusiondb.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.51.pdf",
        "keywords": [
            "generative models"
        ],
        "venue": "ACL",
        "year": "2023",
        "source_file": "acl2023_tnt_kid.json"
    },
    {
        "title": "Continual Dialogue State Tracking via Example-Guided Question Answering",
        "authors": [
            "Hyundong Cho",
            "Andrea Madotto",
            "Zhaojiang Lin",
            "Khyathi Chandu",
            "Satwik Kottur",
            "Jing Xu",
            "Jonathan May",
            "Chinnadhurai Sankar"
        ],
        "published": "2023",
        "summary": "Dialogue systems are frequently updated to accommodate new services, but naively updating them by continually training with data for new services in diminishing performance on previously learnt services. Motivated by the insight that dialogue state tracking (DST), a crucial component of dialogue systems that estimates the user’s goal as a conversation proceeds, is a simple natural language understanding task, we propose reformulating it as a bundle of granular example-guided question answering tasks to minimize the task shift between services and thus benefit continual learning. Our approach alleviates service-specific memorization and teaches a model to contextualize the given question and example to extract the necessary information from the conversation. We find that a model with just 60M parameters can achieve a significant boost by learning to learn from in-context examples retrieved by a retriever trained to identify turns with similar dialogue state changes. Combining our method with dialogue-level memory replay, our approach attains state of the art performance on DST continual learning metrics without relying on any complex regularization or parameter expansion methods.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.235.pdf",
        "keywords": [
            "dialogue state",
            "dialogue state tracking",
            "question answering",
            "dialogue systems",
            "example",
            "estimates"
        ],
        "venue": "EMNLP",
        "year": "2023",
        "source_file": "emnlp2023_tnt_kid.json"
    },
    {
        "title": "A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding",
        "authors": [
            "Andrea Burns",
            "Krishna Srinivasan",
            "Joshua Ainslie",
            "Geoff Brown",
            "Bryan Plummer",
            "Kate Saenko",
            "Jianmo Ni",
            "Mandy Guo"
        ],
        "published": "2023",
        "summary": "Webpages have been a rich, scalable resource for vision-language and language only tasks. Yet only pieces of webpages are kept in existing datasets: image-caption pairs, long text articles, or raw HTML, never all in one place. Webpage tasks have resultingly received little attention and structured image-text data left underused. To study multimodal webpage understanding, we introduce the Wikipedia Webpage suite (WikiWeb2M) containing 2M pages with all of the associated image, text, and structure data. We verify its utility on three generative tasks: page description generation, section summarization, and contextual image captioning. We design a novel attention mechanism Prefix Global, which selects the most relevant image and text content as global tokens to attend to the rest of the webpage for context. By using page structure to separate such tokens, it performs better than full attention with lower computational complexity. Extensive experiments show that the new data in WikiWeb2M improves task performance compared to prior work.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.119.pdf",
        "keywords": [
            "generative tasks"
        ],
        "venue": "EMNLP",
        "year": "2023",
        "source_file": "emnlp2023_tnt_kid.json"
    },
    {
        "title": "MeaeQ: Mount Model Extraction Attacks with Efficient Queries",
        "authors": [
            "Chengwei Dai",
            "Minxuan Lv",
            "Kun Li",
            "Wei Zhou"
        ],
        "published": "2023",
        "summary": "We study model extraction attacks in natural language processing (NLP) where attackers aim to steal victim models by repeatedly querying the open Application Programming Interfaces (APIs). Recent works focus on limited-query budget settings and adopt random sampling or active learning-based sampling strategies on publicly available, unannotated data sources. However, these methods often result in selected queries that lack task relevance and data diversity, leading to limited success in achieving satisfactory results with low query costs. In this paper, we propose MeaeQ (Model extraction attack with efficient Queries), a straightforward yet effective method to address these issues. Specifically, we initially utilize a zero-shot sequence inference classifier, combined with API service information, to filter task-relevant data from a public text corpus instead of a problem domain-specific dataset. Furthermore, we employ a clustering-based data reduction technique to obtain representative data as queries for the attack. Extensive experiments conducted on four benchmark datasets demonstrate that MeaeQ achieves higher functional similarity to the victim model than baselines while requiring fewer queries.",
        "pdf_link": "https://aclanthology.org/2023.emnlp-main.781.pdf",
        "keywords": [
            "model extraction",
            "victim model",
            "mount model extraction",
            "natural language processing",
            "efficient queries",
            "data reduction"
        ],
        "venue": "EMNLP",
        "year": "2023",
        "source_file": "emnlp2023_tnt_kid.json"
    },
    {
        "title": "PMAES: Prompt-mapping Contrastive Learning for Cross-prompt Automated Essay Scoring",
        "authors": [
            "Yuan Chen",
            "Xia Li"
        ],
        "published": "2023",
        "summary": "Current cross-prompt automated essay scoring (AES) is a challenging task due to the large discrepancies between different prompts, such as different genres and expressions. The main goal of current cross-prompt AES systems is to learn enough shared features between the source and target prompts to grade well on the target prompt. However, because the features are captured based on the original prompt representation, they may be limited by being extracted directly between essays. In fact, when the representations of two prompts are more similar, we can gain more shared features between them. Based on this motivation, in this paper, we propose a learning strategy called “prompt-mapping” to learn about more consistent representations of source and target prompts. In this way, we can obtain more shared features between the two prompts and use them to better represent the essays for the target prompt. Experimental results on the ASAP++ dataset demonstrate the effectiveness of our method. We also design experiments in different settings to show that our method can be applied in different scenarios. Our code is available at https://github.com/gdufsnlp/PMAES.",
        "pdf_link": "https://aclanthology.org/2023.acl-long.83.pdf",
        "keywords": [
            "essay scoring",
            "automated essay scoring"
        ],
        "venue": "ACL",
        "year": "2023",
        "source_file": "acl2023_tnt_kid.json"
    },
    {
        "title": "Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers",
        "authors": [
            "Jiawen Xie",
            "Pengyu Cheng",
            "Xiao Liang",
            "Yong Dai",
            "Nan Du"
        ],
        "published": "2024",
        "summary": "Although dominant in natural language processing, transformer-based models still struggle with long-sequence processing, due to the computational costs of their self-attention operations, which increase exponentially as the length of the input sequence grows. To address this challenge, we propose a **Sim**ple framework to enhance the long-content processing of off-the-shelf pre-trained transformers via three steps: **C**hunk, **A**lign, and **S**elect (SimCAS). More specifically, we first divide each long-sequence input into a batch of chunks, then align the inter-chunk information during the encoding steps, and finally, select the most representative hidden states from the encoder for the decoding process. With our SimCAS, the computation and memory costs can be reduced to linear complexity. In experiments, we demonstrate the effectiveness of the proposed method on various real-world long-text summarization and reading comprehension tasks, in which SimCAS significantly outperforms prior long-sequence processing baselines. The code is at [https://github.com/xjw-nlp/SimCAS](https://github.com/xjw-nlp/SimCAS).",
        "pdf_link": "https://aclanthology.org/2024.acl-long.729.pdf",
        "keywords": [
            "transformers",
            "sequence processing",
            "batch",
            "summarization",
            "align"
        ],
        "venue": "ACL",
        "year": "2024",
        "source_file": "acl2024_tnt_kid.json"
    },
    {
        "title": "ValUES: A Framework for Systematic Validation of Uncertainty Estimation in Semantic Segmentation",
        "authors": "Kim-Celine Kahl;Carsten T. Lüth;Maximilian Zenk;Klaus Maier-Hein;Paul F Jaeger",
        "site": "https://iclr.cc/virtual/2024/poster/17416",
        "summary": "Uncertainty estimation is an essential and heavily-studied component for the reliable application of semantic segmentation methods. While various studies exist claiming methodological advances on the one hand, and successful application on the other hand, the field is currently hampered by a gap between theory and practice leaving fundamental questions unanswered: Can data-related and model-related uncertainty really be separated in practice? Which components of an uncertainty method are essential for real-world performance? Which uncertainty method works well for which application? In this work, we link this research gap to a lack of systematic and comprehensive evaluation of uncertainty methods. Specifically, we identify three key pitfalls in current literature and present an evaluation framework that bridges the research gap by providing 1) a controlled environment for studying data ambiguities as well as distribution shifts, 2) systematic ablations of relevant method components, and 3) test-beds for the five predominant uncertainty applications: OoD-detection, active learning, failure detection, calibration, and ambiguity modeling. Empirical results on simulated as well as real-world data demonstrate how the proposed framework is able to answer the predominant questions in the field revealing for instance that 1) separation of uncertainty types works on simulated data but does not necessarily translate to real-world data, 2) aggregation of scores is a crucial but currently neglected component of uncertainty methods, 3) While ensembles are performing most robustly across the different downstream tasks and settings, test-time augmentation often constitutes a light-weight alternative. Code is at: https://github.com/IML-DKFZ/values",
        "forum_url": "https://iclr.cc/virtual/2024/poster/17416",
        "keywords": [
            "semantic segmentation",
            "uncertainty",
            "uncertainty estimation",
            "systematic validation"
        ],
        "venue": "ICLR",
        "year": "2024",
        "source_file": "iclr2024_tnt_kid.json"
    },
    {
        "title": "Semi-Supervised Spoken Language Glossification",
        "authors": [
            "Huijie Yao",
            "Wengang Zhou",
            "Hao Zhou",
            "Houqiang Li"
        ],
        "published": "2024",
        "summary": "Spoken language glossification (SLG) aims to translate the spoken language text into the sign language gloss, i.e., a written record of sign language. In this work, we present a framework named Semi-Supervised Spoken Language Glossification (S3LG) for SLG. To tackle the bottleneck of limited parallel data in SLG, our S3LG incorporates large-scale monolingual spoken language text into SLG training. The proposed framework follows the self-training structure that iteratively annotates and learns from pseudo labels. Considering the lexical similarity and syntactic difference between sign language and spoken language, our S3LG adopts both the rule-based heuristic and model-based approach for auto-annotation. During training, we randomly mix these complementary synthetic datasets and mark their differences with a special token. As the synthetic data may be less quality, the S3LG further leverages consistency regularization to reduce the negative impact of noise in the synthetic data. Extensive experiments are conducted on public benchmarks to demonstrate the effectiveness of the S3LG. Our code is available at https://github.com/yaohj11/S3LG.",
        "pdf_link": "https://aclanthology.org/2024.acl-long.504.pdf",
        "keywords": [
            "spoken language glossification"
        ],
        "venue": "ACL",
        "year": "2024",
        "source_file": "acl2024_tnt_kid.json"
    },
    {
        "title": "Human-Centered Evaluation of Language Technologies",
        "authors": [
            "Su Lin Blodgett",
            "Jackie Chi Kit Cheung",
            "Vera Liao",
            "Ziang Xiao"
        ],
        "published": "2024",
        "summary": "Evaluation is a cornerstone topic in NLP. However, many criticisms have been raised about the community’s evaluation practices, including a lack of human-centered considerations about people’s needs for language technologies and their actual impact on people. This “evaluation crisis” is exacerbated by the recent development of large generative models with diverse and uncertain capabilities. This tutorial aims to inspire more human-centered evaluation in NLP by introducing perspectives and methodologies from human-computer interaction (HCI), a field concerned primarily with the design and evaluation of technologies. The tutorial will start with an overview of current NLP evaluation practices and their limitations, then introduce the “toolbox of evaluation methods” from HCI with varying considerations such as what to evaluate for, how generalizable the results are to the real-world contexts, and pragmatic costs to conduct the evaluation. The tutorial will also encourage reflection on how these HCI perspectives and methodologies can complement NLP evaluation through Q&A discussions and a hands-on exercise.",
        "pdf_link": "https://aclanthology.org/2024.emnlp-tutorials.6.pdf",
        "keywords": [
            "evaluation",
            "human centered evaluation",
            "human centered",
            "language technologies",
            "methodologies",
            "technologies",
            "nlp evaluation",
            "generative models",
            "human computer interaction",
            "evaluation practices",
            "hci",
            "uncertain",
            "needs"
        ],
        "venue": "EMNLP",
        "year": "2024",
        "source_file": "emnlp2024_tnt_kid.json"
    },
    {
        "title": "Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement",
        "authors": [
            "Hana Kim",
            "Kai Ong",
            "Seoyeon Kim",
            "Dongha Lee",
            "Jinyoung Yeo"
        ],
        "published": "2024",
        "summary": "Memorizing and utilizing speakers’ personas is a common practice for response generation in long-term conversations. Yet, human-authored datasets often provide uninformative persona sentences that hinder response quality. This paper presents a novel framework that leverages commonsense-based persona expansion to address such issues in long-term conversation.While prior work focuses on not producing personas that contradict others, we focus on transforming contradictory personas into sentences that contain rich speaker information, by refining them based on their contextual backgrounds with designed strategies. As the pioneer of persona expansion in multi-session settings, our framework facilitates better response generation via human-like persona refinement. The supplementary video of our work is available at https://caffeine-15bbf.web.app/.",
        "pdf_link": "https://aclanthology.org/2024.eacl-short.11.pdf",
        "keywords": [
            "commonsense",
            "conversations",
            "response generation",
            "speakers",
            "context aware",
            "commonsense augmented memory construction",
            "supplementary video"
        ],
        "venue": "EACL",
        "year": "2024",
        "source_file": "eacl2024_tnt_kid.json"
    },
    {
        "title": "Document Image Machine Translation with Dynamic Multi-pre-trained Models Assembling",
        "authors": [
            "Yupu Liang",
            "Yaping Zhang",
            "Cong Ma",
            "Zhiyang Zhang",
            "Yang Zhao",
            "Lu Xiang",
            "Chengqing Zong",
            "Yu Zhou"
        ],
        "published": "2024",
        "summary": "Text image machine translation (TIMT) is a task that translates source texts embedded in the image to target translations. The existing TIMT task mainly focuses on text-line-level images. In this paper, we extend the current TIMT task and propose a novel task, **D**ocument **I**mage **M**achine **T**ranslation to **Markdown** (**DIMT2Markdown**), which aims to translate a source document image with long context and complex layout structure to markdown-formatted target translation.We also introduce a novel framework, **D**ocument **I**mage **M**achine **T**ranslation with **D**ynamic multi-pre-trained models **A**ssembling (**DIMTDA**).A dynamic model assembler is used to integrate multiple pre-trained models to enhance the model’s understanding of layout and translation capabilities.Moreover, we build a novel large-scale **Do**cument image machine **T**ranslation dataset of **A**rXiv articles in markdown format (**DoTA**), containing 126K image-translation pairs.Extensive experiments demonstrate the feasibility of end-to-end translation of rich-text document images and the effectiveness of DIMTDA.",
        "pdf_link": "https://aclanthology.org/2024.naacl-long.392.pdf",
        "keywords": [
            "translation",
            "text image machine translation",
            "assembler",
            "document image machine translation",
            "ranslation"
        ],
        "venue": "NAACL",
        "year": "2024",
        "source_file": "naacl2024_tnt_kid.json"
    },
    {
        "title": "End-to-end Learning of Logical Rules for Enhancing Document-level Relation Extraction",
        "authors": [
            "Kunxun Qi",
            "Jianfeng Du",
            "Hai Wan"
        ],
        "published": "2024",
        "summary": "Document-level relation extraction (DocRE) aims to extract relations between entities in a whole document. One of the pivotal challenges of DocRE is to capture the intricate interdependencies between relations of entity pairs. Previous methods have shown that logical rules can explicitly help capture such interdependencies. These methods either learn logical rules to refine the output of a trained DocRE model, or first learn logical rules from annotated data and then inject the learnt rules into a DocRE model using an auxiliary training objective. However, these learning pipelines may suffer from the issue of error propagation. To mitigate this issue, we propose Joint Modeling Relation extraction and Logical rules or JMRL for short, a novel rule-based framework that jointly learns both a DocRE model and logical rules in an end-to-end fashion. Specifically, we parameterize a rule reasoning module in JMRL to simulate the inference of logical rules, thereby explicitly modeling the reasoning process. We also introduce an auxiliary loss and a residual connection mechanism in JMRL to better reconcile the DocRE model and the rule reasoning module. Experimental results on four benchmark datasets demonstrate that our proposed JMRL framework is consistently superior to existing rule-based frameworks, improving five baseline models for DocRE by a significant margin.",
        "pdf_link": "https://aclanthology.org/2024.acl-long.391.pdf",
        "keywords": [
            "docre",
            "document level relation extraction",
            "logical rules"
        ],
        "venue": "ACL",
        "year": "2024",
        "source_file": "acl2024_tnt_kid.json"
    },
    {
        "title": "DeMuX: Data-efficient Multilingual Learning",
        "authors": [
            "Simran Khanuja",
            "Srinivas Gowriraj",
            "Lucio Dery",
            "Graham Neubig"
        ],
        "published": "2024",
        "summary": "Pre-trained multilingual models have enabled deployment of NLP technologies for multiple languages. However, optimally fine-tuning these models under an annotation budget, such that performance on desired target languages is jointly maximized, still remains an open question. In this paper, we introduce DeMuX, a framework that prescribes the exact data-points to label from vast amounts of unlabelled multilingual data, having unknown degrees of overlap with the target set. Unlike most prior works, our end-to-end framework is language-agnostic, accounts for model representations, and supports multilingual target configurations. Our active learning strategies rely upon distance and uncertainty measures to select task-specific neighbors that are most informative to label, given a model. DeMuX outperforms strong baselines in 84% of the test cases, in the zero-shot setting of disjoint source and target language sets (including multilingual target pools), across three models and four tasks. Notably, in low-budget settings (5-100 examples), we observe gains of up to 8-11 F1 points. Our code is released here: https://github.com/simran-khanuja/demux.",
        "pdf_link": "https://aclanthology.org/2024.naacl-long.412.pdf",
        "keywords": [
            "active learning",
            "multilingual models",
            "unlabelled multilingual"
        ],
        "venue": "NAACL",
        "year": "2024",
        "source_file": "naacl2024_tnt_kid.json"
    },
    {
        "title": "Natural Language Processing for Multilingual Task-Oriented Dialogue",
        "authors": [
            "Evgeniia Razumovskaia",
            "Goran Glavaš",
            "Olga Majewska",
            "Edoardo Ponti",
            "Ivan Vulić"
        ],
        "published": "2022",
        "summary": "Recent advances in deep learning have also enabled fast progress in the research of task-oriented dialogue (ToD) systems. However, the majority of ToD systems are developed for English and merely a handful of other widely spoken languages, e.g., Chinese and German. This hugely limits the global reach and, consequently, transformative socioeconomic potential of such systems. In this tutorial, we will thus discuss and demonstrate the importance of (building) multilingual ToD systems, and then provide a systematic overview of current research gaps, challenges and initiatives related to multilingual ToD systems, with a particular focus on their connections to current research and challenges in multilingual and low-resource NLP. The tutorial will aim to provide answers or shed new light to the following questions: a) Why are multilingual dialogue systems so hard to build: what makes multilinguality for dialogue more challenging than for other NLP applications and tasks? b) What are the best existing methods and datasets for multilingual and cross-lingual (task-oriented) dialog systems? How are (multilingual) ToD systems usually evaluated? c) What are the promising future directions for multilingual ToD research: where can one draw inspiration from related NLP areas and tasks?",
        "pdf_link": "https://aclanthology.org/2022.acl-tutorials.8.pdf",
        "keywords": [
            "natural language processing",
            "task oriented dialogue",
            "dialogue"
        ],
        "venue": "ACL",
        "year": "2022",
        "source_file": "acl2022_tnt_kid.json"
    },
    {
        "title": "On the well-spread property and its relation to linear regression",
        "authors": [
            "Hongjie Chen",
            "Tommaso d'Orsi"
        ],
        "published": "2022-06-16T11:17:44Z",
        "summary": "We consider the robust linear regression model $\\boldsymbol{y} = X\\beta^* +\n\\boldsymbol{\\eta}$, where an adversary oblivious to the design $X \\in\n\\mathbb{R}^{n \\times d}$ may choose $\\boldsymbol{\\eta}$ to corrupt all but a\n(possibly vanishing) fraction of the observations $\\boldsymbol{y}$ in an\narbitrary way. Recent work [dLN+21, dNS21] has introduced efficient algorithms\nfor consistent recovery of the parameter vector. These algorithms crucially\nrely on the design matrix being well-spread (a matrix is well-spread if its\ncolumn span is far from any sparse vector).\n  In this paper, we show that there exists a family of design matrices lacking\nwell-spreadness such that consistent recovery of the parameter vector in the\nabove robust linear regression model is information-theoretically impossible.\n  We further investigate the average-case time complexity of certifying\nwell-spreadness of random matrices. We show that it is possible to efficiently\ncertify whether a given $n$-by-$d$ Gaussian matrix is well-spread if the number\nof observations is quadratic in the ambient dimension. We complement this\nresult by showing rigorous evidence -- in the form of a lower bound against\nlow-degree polynomials -- of the computational hardness of this same\ncertification problem when the number of observations is $o(d^2)$.",
        "pdf_link": "https://arxiv.org/pdf/2206.08092v1.pdf",
        "keywords": [
            "well spread",
            "well spread property",
            "linear regression",
            "certifying well spreadness",
            "robust linear regression model",
            "consistent recovery"
        ],
        "venue": "DATA",
        "year": "2022",
        "source_file": "data_collected_Jan2022-July2022_tnt_kid.json"
    },
    {
        "title": "An Embedded Feature Selection Framework for Control",
        "authors": [
            "Jiawen Wei",
            "Fangyuan Wang",
            "Wanxin Zeng",
            "Wenwei Lin",
            "Ning Gui"
        ],
        "published": "2022-06-19T07:03:40Z",
        "summary": "Reducing sensor requirements while keeping optimal control performance is\ncrucial to many industrial control applications to achieve robust, low-cost,\nand computation-efficient controllers. However, existing feature selection\nsolutions for the typical machine learning domain can hardly be applied in the\ndomain of control with changing dynamics. In this paper, a novel framework,\nnamely the Dual-world embedded Attentive Feature Selection (D-AFS), can\nefficiently select the most relevant sensors for the system under dynamic\ncontrol. Rather than the one world used in most Deep Reinforcement Learning\n(DRL) algorithms, D-AFS has both the real world and its virtual peer with\ntwisted features. By analyzing the DRL's response in two worlds, D-AFS can\nquantitatively identify respective features' importance towards control. A\nwell-known active flow control problem, cylinder drag reduction, is used for\nevaluation. Results show that D-AFS successfully finds an optimized five-probes\nlayout with 18.7\\% drag reduction than the state-of-the-art solution with 151\nprobes and 49.2\\% reduction than five-probes layout by human experts. We also\napply this solution to four OpenAI classical control cases. In all cases, D-AFS\nachieves the same or better sensor configurations than originally provided\nsolutions. Results highlight, we argued, a new way to achieve efficient and\noptimal sensor designs for experimental or industrial systems. Our source codes\nare made publicly available at https://github.com/G-AILab/DAFSFluid.",
        "pdf_link": "https://arxiv.org/pdf/2206.11064v1.pdf",
        "keywords": [
            "feature selection",
            "embedded feature selection",
            "embedded attentive feature selection",
            "afs",
            "d afs"
        ],
        "venue": "DATA",
        "year": "2022",
        "source_file": "data_collected_Jan2022-July2022_tnt_kid.json"
    },
    {
        "title": "Adversarial Permutation Invariant Training for Universal Sound Separation",
        "authors": [
            "Emilian Postolache",
            "Jordi Pons",
            "Santiago Pascual",
            "Joan Serrà"
        ],
        "published": "2022-10-21T17:04:17Z",
        "summary": "Universal sound separation consists of separating mixes with arbitrary sounds\nof different types, and permutation invariant training (PIT) is used to train\nsource agnostic models that do so. In this work, we complement PIT with\nadversarial losses but find it challenging with the standard formulation used\nin speech source separation. We overcome this challenge with a novel\nI-replacement context-based adversarial loss, and by training with multiple\ndiscriminators. Our experiments show that by simply improving the loss (keeping\nthe same model and dataset) we obtain a non-negligible improvement of 1.4 dB\nSI-SNRi in the reverberant FUSS dataset. We also find adversarial PIT to be\neffective at reducing spectral holes, ubiquitous in mask-based separation\nmodels, which highlights the potential relevance of adversarial losses for\nsource separation.",
        "pdf_link": "https://arxiv.org/pdf/2210.12108v2.pdf",
        "keywords": [
            "separation",
            "source separation",
            "universal sound separation",
            "permutation invariant training",
            "adversarial permutation invariant"
        ],
        "venue": "DATA",
        "year": "2022",
        "source_file": "data_collected_July2022-Jan2023_tnt_kid.json"
    },
    {
        "title": "A System for Interactive Examination of Learned Security Policies",
        "authors": [
            "Kim Hammar",
            "Rolf Stadler"
        ],
        "published": "2022-04-03T17:55:32Z",
        "summary": "We present a system for interactive examination of learned security policies.\nIt allows a user to traverse episodes of Markov decision processes in a\ncontrolled manner and to track the actions triggered by security policies.\nSimilar to a software debugger, a user can continue or or halt an episode at\nany time step and inspect parameters and probability distributions of interest.\nThe system enables insight into the structure of a given policy and in the\nbehavior of a policy in edge cases. We demonstrate the system with a network\nintrusion use case. We examine the evolution of an IT infrastructure's state\nand the actions prescribed by security policies while an attack occurs. The\npolicies for the demonstration have been obtained through a reinforcement\nlearning approach that includes a simulation system where policies are\nincrementally learned and an emulation system that produces statistics that\ndrive the simulation runs.",
        "pdf_link": "https://arxiv.org/pdf/2204.01126v2.pdf",
        "keywords": [
            "security",
            "learned security",
            "reinforcement learning",
            "simulation system"
        ],
        "venue": "DATA",
        "year": "2022",
        "source_file": "data_collected_Jan2022-July2022_tnt_kid.json"
    },
    {
        "title": "DP-PCA: Statistically Optimal and Differentially Private PCA",
        "authors": [
            "Xiyang Liu",
            "Weihao Kong",
            "Prateek Jain",
            "Sewoong Oh"
        ],
        "published": "2022-05-27T02:02:17Z",
        "summary": "We study the canonical statistical task of computing the principal component\nfrom $n$ i.i.d.~data in $d$ dimensions under\n$(\\varepsilon,\\delta)$-differential privacy. Although extensively studied in\nliterature, existing solutions fall short on two key aspects: ($i$) even for\nGaussian data, existing private algorithms require the number of samples $n$ to\nscale super-linearly with $d$, i.e., $n=\\Omega(d^{3/2})$, to obtain non-trivial\nresults while non-private PCA requires only $n=O(d)$, and ($ii$) existing\ntechniques suffer from a non-vanishing error even when the randomness in each\ndata point is arbitrarily small. We propose DP-PCA, which is a single-pass\nalgorithm that overcomes both limitations. It is based on a private minibatch\ngradient ascent method that relies on {\\em private mean estimation}, which adds\nminimal noise required to ensure privacy by adapting to the variance of a given\nminibatch of gradients. For sub-Gaussian data, we provide nearly optimal\nstatistical error rates even for $n=\\tilde O(d)$. Furthermore, we provide a\nlower bound showing that sub-Gaussian style assumption is necessary in\nobtaining the optimal error rate.",
        "pdf_link": "https://arxiv.org/pdf/2205.13709v1.pdf",
        "keywords": [
            "differentially private pca",
            "single pass"
        ],
        "venue": "DATA",
        "year": "2022",
        "source_file": "data_collected_Jan2022-July2022_tnt_kid.json"
    },
    {
        "title": "The Fairness Field Guide: Perspectives from Social and Formal Sciences",
        "authors": [
            "Alycia N. Carey",
            "Xintao Wu"
        ],
        "published": "2022-01-13T21:30:03Z",
        "summary": "Over the past several years, a slew of different methods to measure the\nfairness of a machine learning model have been proposed. However, despite the\ngrowing number of publications and implementations, there is still a critical\nlack of literature that explains the interplay of fair machine learning with\nthe social sciences of philosophy, sociology, and law. We hope to remedy this\nissue by accumulating and expounding upon the thoughts and discussions of fair\nmachine learning produced by both social and formal (specifically machine\nlearning and statistics) sciences in this field guide. Specifically, in\naddition to giving the mathematical and algorithmic backgrounds of several\npopular statistical and causal-based fair machine learning methods, we explain\nthe underlying philosophical and legal thoughts that support them. Further, we\nexplore several criticisms of the current approaches to fair machine learning\nfrom sociological and philosophical viewpoints. It is our hope that this field\nguide will help fair machine learning practitioners better understand how their\nalgorithms align with important humanistic values (such as fairness) and how we\ncan, as a field, design methods and metrics to better serve oppressed and\nmarginalized populaces.",
        "pdf_link": "https://arxiv.org/pdf/2201.05216v2.pdf",
        "keywords": [
            "fairness",
            "fair machine learning",
            "social sciences",
            "fairness field"
        ],
        "venue": "DATA",
        "year": "2022",
        "source_file": "data_collected_Jan2022-July2022_tnt_kid.json"
    },
    {
        "title": "A Deep Generative Model for Feasible and Diverse Population Synthesis",
        "authors": [
            "Eui-Jin Kim",
            "Prateek Bansal"
        ],
        "published": "2022-08-01T05:02:02Z",
        "summary": "An ideal synthetic population, a key input to activity-based models, mimics\nthe distribution of the individual- and household-level attributes in the\nactual population. Since the entire population's attributes are generally\nunavailable, household travel survey (HTS) samples are used for population\nsynthesis. Synthesizing population by directly sampling from HTS ignores the\nattribute combinations that are unobserved in the HTS samples but exist in the\npopulation, called 'sampling zeros'. A deep generative model (DGM) can\npotentially synthesize the sampling zeros but at the expense of generating\n'structural zeros' (i.e., the infeasible attribute combinations that do not\nexist in the population). This study proposes a novel method to minimize\nstructural zeros while preserving sampling zeros. Two regularizations are\ndevised to customize the training of the DGM and applied to a generative\nadversarial network (GAN) and a variational autoencoder (VAE). The adopted\nmetrics for feasibility and diversity of the synthetic population indicate the\ncapability of generating sampling and structural zeros -- lower structural\nzeros and lower sampling zeros indicate the higher feasibility and the lower\ndiversity, respectively. Results show that the proposed regularizations achieve\nconsiderable performance improvement in feasibility and diversity of the\nsynthesized population over traditional models. The proposed VAE additionally\ngenerated 23.5% of the population ignored by the sample with 79.2% precision\n(i.e., 20.8% structural zeros rates), while the proposed GAN generated 18.3% of\nthe ignored population with 89.0% precision. The proposed improvement in DGM\ngenerates a more feasible and diverse synthetic population, which is critical\nfor the accuracy of an activity-based model.",
        "pdf_link": "https://arxiv.org/pdf/2208.01403v1.pdf",
        "keywords": [
            "feasibility",
            "population synthesis",
            "synthetic population",
            "diversity"
        ],
        "venue": "DATA",
        "year": "2022",
        "source_file": "data_collected_July2022-Jan2023_tnt_kid.json"
    },
    {
        "title": "Learning Multitask Gaussian Bayesian Networks",
        "authors": [
            "Shuai Liu",
            "Yixuan Qiu",
            "Baojuan Li",
            "Huaning Wang",
            "Xiangyu Chang"
        ],
        "published": "2022-05-11T08:38:00Z",
        "summary": "Major depressive disorder (MDD) requires study of brain functional\nconnectivity alterations for patients, which can be uncovered by resting-state\nfunctional magnetic resonance imaging (rs-fMRI) data. We consider the problem\nof identifying alterations of brain functional connectivity for a single MDD\npatient. This is particularly difficult since the amount of data collected\nduring an fMRI scan is too limited to provide sufficient information for\nindividual analysis. Additionally, rs-fMRI data usually has the characteristics\nof incompleteness, sparsity, variability, high dimensionality and high noise.\nTo address these problems, we proposed a multitask Gaussian Bayesian network\n(MTGBN) framework capable for identifying individual disease-induced\nalterations for MDD patients. We assume that such disease-induced alterations\nshow some degrees of similarity with the tool to learn such network structures\nfrom observations to understanding of how system are structured jointly from\nrelated tasks. First, we treat each patient in a class of observation as a task\nand then learn the Gaussian Bayesian networks (GBNs) of this data class by\nlearning from all tasks that share a default covariance matrix that encodes\nprior knowledge. This setting can help us to learn more information from\nlimited data. Next, we derive a closed-form formula of the complete likelihood\nfunction and use the Monte-Carlo Expectation-Maximization(MCEM) algorithm to\nsearch for the approximately best Bayesian network structures efficiently.\nFinally, we assess the performance of our methods with simulated and real-world\nrs-fMRI data.",
        "pdf_link": "https://arxiv.org/pdf/2205.05343v2.pdf",
        "keywords": [
            "bayesian network",
            "multitask gaussian bayesian network",
            "gaussian bayesian networks",
            "fmri",
            "rs fmri",
            "depressive disorder"
        ],
        "venue": "DATA",
        "year": "2022",
        "source_file": "data_collected_Jan2022-July2022_tnt_kid.json"
    },
    {
        "title": "Category-Level Multi-Part Multi-Joint 3D Shape Assembly",
        "authors": [
            "Yichen Li",
            "Kaichun Mo",
            "Yueqi Duan",
            "He Wang",
            "Jiequan Zhang",
            "Lin Shao",
            "Wojciech Matusik",
            "Leonidas Guibas"
        ],
        "published": "2023-03-10T19:02:26Z",
        "summary": "Shape assembly composes complex shapes geometries by arranging simple part\ngeometries and has wide applications in autonomous robotic assembly and CAD\nmodeling. Existing works focus on geometry reasoning and neglect the actual\nphysical assembly process of matching and fitting joints, which are the contact\nsurfaces connecting different parts. In this paper, we consider contacting\njoints for the task of multi-part assembly. A successful joint-optimized\nassembly needs to satisfy the bilateral objectives of shape structure and joint\nalignment. We propose a hierarchical graph learning approach composed of two\nlevels of graph representation learning. The part graph takes part geometries\nas input to build the desired shape structure. The joint-level graph uses part\njoints information and focuses on matching and aligning joints. The two kinds\nof information are combined to achieve the bilateral objectives. Extensive\nexperiments demonstrate that our method outperforms previous methods, achieving\nbetter shape structure and higher joint alignment accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2303.06163v1.pdf",
        "keywords": [
            "shape assembly",
            "joint 3d shape assembly",
            "bilateral objectives",
            "joint level graph",
            "multi part assembly",
            "geometry reasoning"
        ],
        "venue": "DATA",
        "year": "2023",
        "source_file": "data_collected_Jan2023-July2023_tnt_kid.json"
    },
    {
        "title": "Best-of-three-worlds Analysis for Linear Bandits with Follow-the-regularized-leader Algorithm",
        "authors": [
            "Fang Kong",
            "Canzhe Zhao",
            "Shuai Li"
        ],
        "published": "2023-03-13T02:50:59Z",
        "summary": "The linear bandit problem has been studied for many years in both stochastic\nand adversarial settings. Designing an algorithm that can optimize the\nenvironment without knowing the loss type attracts lots of interest.\n\\citet{LeeLWZ021} propose an algorithm that actively detects the loss type and\nthen switches between different algorithms specially designed for specific\nsettings. However, such an approach requires meticulous designs to perform well\nin all environments. Follow-the-regularized-leader (FTRL) is another type of\npopular algorithm that can adapt to different environments. This algorithm is\nof simple design and the regret bounds are shown to be optimal in traditional\nmulti-armed bandit problems compared with the detect-switch type. Designing an\nFTRL-type algorithm for linear bandits is an important question that has been\nopen for a long time. In this paper, we prove that the FTRL algorithm with a\nnegative entropy regularizer can achieve the best-of-three-world results for\nthe linear bandit problem. Our regret bounds achieve the same or nearly the\nsame order as the previous detect-switch type algorithm but with a much simpler\nalgorithmic design.",
        "pdf_link": "https://arxiv.org/pdf/2303.06825v2.pdf",
        "keywords": [
            "regularized",
            "regret bounds",
            "bandit",
            "linear bandit",
            "analysis",
            "switch type"
        ],
        "venue": "DATA",
        "year": "2023",
        "source_file": "data_collected_Jan2023-July2023_tnt_kid.json"
    },
    {
        "title": "SPOTR: Spatio-temporal Pose Transformers for Human Motion Prediction",
        "authors": [
            "Avinash Ajit Nargund",
            "Misha Sra"
        ],
        "published": "2023-03-11T01:44:29Z",
        "summary": "3D human motion prediction is a research area of high significance and a\nchallenge in computer vision. It is useful for the design of many applications\nincluding robotics and autonomous driving. Traditionally, autogregressive\nmodels have been used to predict human motion. However, these models have high\ncomputation needs and error accumulation that make it difficult to use them for\nrealtime applications. In this paper, we present a non-autogressive model for\nhuman motion prediction. We focus on learning spatio-temporal representations\nnon-autoregressively for generation of plausible future motions. We propose a\nnovel architecture that leverages the recently proposed Transformers. Human\nmotion involves complex spatio-temporal dynamics with joints affecting the\nposition and rotation of each other even though they are not connected\ndirectly. The proposed model extracts these dynamics using both convolutions\nand the self-attention mechanism. Using specialized spatial and temporal\nself-attention to augment the features extracted through convolution allows our\nmodel to generate spatio-temporally coherent predictions in parallel\nindependent of the activity. Our contributions are threefold: (i) we frame\nhuman motion prediction as a sequence-to-sequence problem and propose a\nnon-autoregressive Transformer to forecast a sequence of poses in parallel;\n(ii) our method is activity agnostic; (iii) we show that despite its\nsimplicity, our approach is able to make accurate predictions, achieving better\nor comparable results compared to the state-of-the-art on two public datasets,\nwith far fewer parameters and much faster inference.",
        "pdf_link": "https://arxiv.org/pdf/2303.06277v1.pdf",
        "keywords": [
            "pose transformers",
            "motion prediction",
            "human motion prediction",
            "spotr"
        ],
        "venue": "DATA",
        "year": "2023",
        "source_file": "data_collected_Jan2023-July2023_tnt_kid.json"
    },
    {
        "title": "Generating Realistic Counterfactuals for Retinal Fundus and OCT Images using Diffusion Models",
        "authors": [
            "Indu Ilanchezian",
            "Valentyn Boreiko",
            "Laura Kühlewein",
            "Ziwei Huang",
            "Murat Seçkin Ayhan",
            "Matthias Hein",
            "Lisa Koch",
            "Philipp Berens"
        ],
        "published": "2023-11-20T09:28:04Z",
        "summary": "Counterfactual reasoning is often used in clinical settings to explain\ndecisions or weigh alternatives. Therefore, for imaging based specialties such\nas ophthalmology, it would be beneficial to be able to create counterfactual\nimages, illustrating answers to questions like \"If the subject had had diabetic\nretinopathy, how would the fundus image have looked?\". Here, we demonstrate\nthat using a diffusion model in combination with an adversarially robust\nclassifier trained on retinal disease classification tasks enables the\ngeneration of highly realistic counterfactuals of retinal fundus images and\noptical coherence tomography (OCT) B-scans. The key to the realism of\ncounterfactuals is that these classifiers encode salient features indicative\nfor each disease class and can steer the diffusion model to depict disease\nsigns or remove disease-related lesions in a realistic way. In a user study,\ndomain experts also found the counterfactuals generated using our method\nsignificantly more realistic than counterfactuals generated from a previous\nmethod, and even indistinguishable from real images.",
        "pdf_link": "https://arxiv.org/pdf/2311.11629v2.pdf",
        "keywords": [
            "counterfactuals",
            "counterfactual images",
            "counterfactual reasoning",
            "weigh",
            "diffusion",
            "diffusion models",
            "optical coherence tomography",
            "retinal fundus images"
        ],
        "venue": "DATA",
        "year": "2023",
        "source_file": "data_collected_July2023-Jan2024_tnt_kid.json"
    },
    {
        "title": "Enhancing Generalization of Universal Adversarial Perturbation through Gradient Aggregation",
        "authors": [
            "Xuannan Liu",
            "Yaoyao Zhong",
            "Yuhang Zhang",
            "Lixiong Qin",
            "Weihong Deng"
        ],
        "published": "2023-08-11T08:44:58Z",
        "summary": "Deep neural networks are vulnerable to universal adversarial perturbation\n(UAP), an instance-agnostic perturbation capable of fooling the target model\nfor most samples. Compared to instance-specific adversarial examples, UAP is\nmore challenging as it needs to generalize across various samples and models.\nIn this paper, we examine the serious dilemma of UAP generation methods from a\ngeneralization perspective -- the gradient vanishing problem using small-batch\nstochastic gradient optimization and the local optima problem using large-batch\noptimization. To address these problems, we propose a simple and effective\nmethod called Stochastic Gradient Aggregation (SGA), which alleviates the\ngradient vanishing and escapes from poor local optima at the same time.\nSpecifically, SGA employs the small-batch training to perform multiple\niterations of inner pre-search. Then, all the inner gradients are aggregated as\na one-step gradient estimation to enhance the gradient stability and reduce\nquantization errors. Extensive experiments on the standard ImageNet dataset\ndemonstrate that our method significantly enhances the generalization ability\nof UAP and outperforms other state-of-the-art methods. The code is available at\nhttps://github.com/liuxuannan/Stochastic-Gradient-Aggregation.",
        "pdf_link": "https://arxiv.org/pdf/2308.06015v1.pdf",
        "keywords": [
            "universal adversarial perturbation",
            "gradient aggregation",
            "generalization",
            "stochastic gradient aggregation"
        ],
        "venue": "DATA",
        "year": "2023",
        "source_file": "data_collected_July2023-Jan2024_tnt_kid.json"
    },
    {
        "title": "ALDi: Quantifying the Arabic Level of Dialectness of Text",
        "authors": [
            "Amr Keleg",
            "Sharon Goldwater",
            "Walid Magdy"
        ],
        "published": "2023-10-20T18:07:39Z",
        "summary": "Transcribed speech and user-generated text in Arabic typically contain a\nmixture of Modern Standard Arabic (MSA), the standardized language taught in\nschools, and Dialectal Arabic (DA), used in daily communications. To handle\nthis variation, previous work in Arabic NLP has focused on Dialect\nIdentification (DI) on the sentence or the token level. However, DI treats the\ntask as binary, whereas we argue that Arabic speakers perceive a spectrum of\ndialectness, which we operationalize at the sentence level as the Arabic Level\nof Dialectness (ALDi), a continuous linguistic variable. We introduce the\nAOC-ALDi dataset (derived from the AOC dataset), containing 127,835 sentences\n(17% from news articles and 83% from user comments on those articles) which are\nmanually labeled with their level of dialectness. We provide a detailed\nanalysis of AOC-ALDi and show that a model trained on it can effectively\nidentify levels of dialectness on a range of other corpora (including dialects\nand genres not included in AOC-ALDi), providing a more nuanced picture than\ntraditional DI systems. Through case studies, we illustrate how ALDi can reveal\nArabic speakers' stylistic choices in different situations, a useful property\nfor sociolinguistic analyses.",
        "pdf_link": "https://arxiv.org/pdf/2310.13747v1.pdf",
        "keywords": [
            "arabic",
            "dialectness",
            "genres",
            "arabic level of dialectness",
            "standard arabic",
            "speech"
        ],
        "venue": "DATA",
        "year": "2023",
        "source_file": "data_collected_July2023-Jan2024_tnt_kid.json"
    },
    {
        "title": "CLIPA-v2: Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy within a \\$10,000 Budget; An Extra \\$4,000 Unlocks 81.8% Accuracy",
        "authors": [
            "Xianhang Li",
            "Zeyu Wang",
            "Cihang Xie"
        ],
        "published": "2023-06-27T17:51:06Z",
        "summary": "The recent work CLIPA presents an inverse scaling law for CLIP training --\nwhereby the larger the image/text encoders used, the shorter the sequence\nlength of image/text tokens that can be applied in training. This finding\nenables us to train high-performance CLIP models with significantly reduced\ncomputations. Building upon this work, we hereby present CLIPA-v2 with two key\ncontributions. Technically, we find this inverse scaling law is also applicable\nin the finetuning stage, enabling further reduction in computational needs.\nEmpirically, we explore CLIPA at scale, extending the experiments up to the\nH/14 model with ~13B image-text pairs seen during training.\n  Our results are exciting -- by only allocating a budget of \\$10,000, our CLIP\nmodel achieves an impressive zero-shot ImageNet accuracy of 81.1%, surpassing\nthe prior best CLIP model (from OpenCLIP, 80.1%) by 1.0% and meanwhile reducing\nthe computational cost by ~39X. Moreover, with an additional investment of\n$4,000, we can further elevate the zero-shot ImageNet accuracy to 81.8%. Our\ncode and models are available at https://github.com/UCSC-VLAA/CLIPA.",
        "pdf_link": "https://arxiv.org/pdf/2306.15658v1.pdf",
        "keywords": [
            "clip training",
            "imagenet",
            "clip models",
            "clipa",
            "shot imagenet",
            "scaling clip",
            "inverse scaling",
            "shot imagenet accuracy",
            "image text tokens",
            "clipa v2"
        ],
        "venue": "DATA",
        "year": "2023",
        "source_file": "data_collected_Jan2023-July2023_tnt_kid.json"
    },
    {
        "title": "S2ST: Image-to-Image Translation in the Seed Space of Latent Diffusion",
        "authors": [
            "Or Greenberg",
            "Eran Kishon",
            "Dani Lischinski"
        ],
        "published": "2023-11-30T18:59:49Z",
        "summary": "Image-to-image translation (I2IT) refers to the process of transforming\nimages from a source domain to a target domain while maintaining a fundamental\nconnection in terms of image content. In the past few years, remarkable\nadvancements in I2IT were achieved by Generative Adversarial Networks (GANs),\nwhich nevertheless struggle with translations requiring high precision.\nRecently, Diffusion Models have established themselves as the engine of choice\nfor image generation. In this paper we introduce S2ST, a novel framework\ndesigned to accomplish global I2IT in complex photorealistic images, such as\nday-to-night or clear-to-rain translations of automotive scenes. S2ST operates\nwithin the seed space of a Latent Diffusion Model, thereby leveraging the\npowerful image priors learned by the latter. We show that S2ST surpasses\nstate-of-the-art GAN-based I2IT methods, as well as diffusion-based approaches,\nfor complex automotive scenes, improving fidelity while respecting the target\ndomain's appearance across a variety of domains. Notably, S2ST obviates the\nnecessity for training domain-specific translation networks.",
        "pdf_link": "https://arxiv.org/pdf/2312.00116v1.pdf",
        "keywords": [
            "diffusion",
            "latent diffusion",
            "image translation",
            "photorealistic images",
            "seed space"
        ],
        "venue": "DATA",
        "year": "2023",
        "source_file": "data_collected_July2023-Jan2024_tnt_kid.json"
    },
    {
        "title": "Data Quality in Crowdsourcing and Spamming Behavior Detection",
        "authors": [
            "Yang Ba",
            "Michelle V. Mancenido",
            "Erin K. Chiou",
            "Rong Pan"
        ],
        "published": "2024-04-04T02:21:38Z",
        "summary": "As crowdsourcing emerges as an efficient and cost-effective method for\nobtaining labels for machine learning datasets, it is important to assess the\nquality of crowd-provided data, so as to improve analysis performance and\nreduce biases in subsequent machine learning tasks. Given the lack of ground\ntruth in most cases of crowdsourcing, we refer to data quality as annotators'\nconsistency and credibility. Unlike the simple scenarios where Kappa\ncoefficient and intraclass correlation coefficient usually can apply, online\ncrowdsourcing requires dealing with more complex situations. We introduce a\nsystematic method for evaluating data quality and detecting spamming threats\nvia variance decomposition, and we classify spammers into three categories\nbased on their different behavioral patterns. A spammer index is proposed to\nassess entire data consistency and two metrics are developed to measure crowd\nworker's credibility by utilizing the Markov chain and generalized random\neffects models. Furthermore, we showcase the practicality of our techniques and\ntheir advantages by applying them on a face verification task with both\nsimulation and real-world data collected from two crowdsourcing platforms.",
        "pdf_link": "https://arxiv.org/pdf/2404.17582v1.pdf",
        "keywords": [
            "crowdsourcing",
            "spamming",
            "credibility",
            "face verification",
            "data quality",
            "markov chain",
            "spammer",
            "machine learning",
            "crowd"
        ],
        "venue": "DATA",
        "year": "2024",
        "source_file": "data_collected_Apr2024_Nov2024_tnt_kid.json"
    },
    {
        "title": "Out-of-distribution detection based on subspace projection of high-dimensional features output by the last convolutional layer",
        "authors": [
            "Qiuyu Zhu",
            "Yiwei He"
        ],
        "published": "2024-05-02T18:33:02Z",
        "summary": "Out-of-distribution (OOD) detection, crucial for reliable pattern\nclassification, discerns whether a sample originates outside the training\ndistribution. This paper concentrates on the high-dimensional features output\nby the final convolutional layer, which contain rich image features. Our key\nidea is to project these high-dimensional features into two specific feature\nsubspaces, leveraging the dimensionality reduction capacity of the network's\nlinear layers, trained with Predefined Evenly-Distribution Class Centroids\n(PEDCC)-Loss. This involves calculating the cosines of three projection angles\nand the norm values of features, thereby identifying distinctive information\nfor in-distribution (ID) and OOD data, which assists in OOD detection. Building\nupon this, we have modified the batch normalization (BN) and ReLU layer\npreceding the fully connected layer, diminishing their impact on the output\nfeature distributions and thereby widening the distribution gap between ID and\nOOD data features. Our method requires only the training of the classification\nnetwork model, eschewing any need for input pre-processing or specific OOD data\npre-tuning. Extensive experiments on several benchmark datasets demonstrates\nthat our approach delivers state-of-the-art performance. Our code is available\nat https://github.com/Hewell0/ProjOOD.",
        "pdf_link": "https://arxiv.org/pdf/2405.01662v1.pdf",
        "keywords": [
            "subspace projection",
            "distribution",
            "distribution detection",
            "convolutional layer"
        ],
        "venue": "DATA",
        "year": "2024",
        "source_file": "data_collected_Apr2024_Nov2024_tnt_kid.json"
    },
    {
        "title": "Weakly-Supervised Residual Evidential Learning for Multi-Instance Uncertainty Estimation",
        "authors": [
            "Pei Liu",
            "Luping Ji"
        ],
        "published": "2024-05-07T15:31:58Z",
        "summary": "Uncertainty estimation (UE), as an effective means of quantifying predictive\nuncertainty, is crucial for safe and reliable decision-making, especially in\nhigh-risk scenarios. Existing UE schemes usually assume that there are\ncompletely-labeled samples to support fully-supervised learning. In practice,\nhowever, many UE tasks often have no sufficiently-labeled data to use, such as\nthe Multiple Instance Learning (MIL) with only weak instance annotations. To\nbridge this gap, this paper, for the first time, addresses the\nweakly-supervised issue of Multi-Instance UE (MIUE) and proposes a new baseline\nscheme, Multi-Instance Residual Evidential Learning (MIREL). Particularly, at\nthe fine-grained instance UE with only weak supervision, we derive a\nmulti-instance residual operator through the Fundamental Theorem of Symmetric\nFunctions. On this operator derivation, we further propose MIREL to jointly\nmodel the high-order predictive distribution at bag and instance levels for\nMIUE. Extensive experiments empirically demonstrate that our MIREL not only\ncould often make existing MIL networks perform better in MIUE, but also could\nsurpass representative UE methods by large margins, especially in\ninstance-level UE tasks. Our source code is available at\nhttps://github.com/liupei101/MIREL.",
        "pdf_link": "https://arxiv.org/pdf/2405.04405v2.pdf",
        "keywords": [
            "uncertainty estimation",
            "residual evidential learning",
            "weakly supervised",
            "multi instance uncertainty estimation",
            "multi instance"
        ],
        "venue": "DATA",
        "year": "2024",
        "source_file": "data_collected_Apr2024_Nov2024_tnt_kid.json"
    },
    {
        "title": "Learning Visual Prompts for Guiding the Attention of Vision Transformers",
        "authors": [
            "Razieh Rezaei",
            "Masoud Jalili Sabet",
            "Jindong Gu",
            "Daniel Rueckert",
            "Philip Torr",
            "Ashkan Khakzar"
        ],
        "published": "2024-06-05T14:13:38Z",
        "summary": "Visual prompting infuses visual information into the input image to adapt\nmodels toward specific predictions and tasks. Recently, manually crafted\nmarkers such as red circles are shown to guide the model to attend to a target\nregion on the image. However, these markers only work on models trained with\ndata containing those markers. Moreover, finding these prompts requires\nguesswork or prior knowledge of the domain on which the model is trained. This\nwork circumvents manual design constraints by proposing to learn the visual\nprompts for guiding the attention of vision transformers. The learned visual\nprompt, added to any input image would redirect the attention of the\npre-trained vision transformer to its spatial location on the image.\nSpecifically, the prompt is learned in a self-supervised manner without\nrequiring annotations and without fine-tuning the vision transformer. Our\nexperiments demonstrate the effectiveness of the proposed optimization-based\nvisual prompting strategy across various pre-trained vision encoders.",
        "pdf_link": "https://arxiv.org/pdf/2406.03303v1.pdf",
        "keywords": [
            "visual prompting",
            "vision transformers",
            "learning visual prompts",
            "attention"
        ],
        "venue": "DATA",
        "year": "2024",
        "source_file": "data_collected_Apr2024_Nov2024_tnt_kid.json"
    },
    {
        "title": "Dynamically enhanced static handwriting representation for Parkinson's disease detection",
        "authors": [
            "Moises Diaz",
            "Miguel Angel Ferrer",
            "Donato Impedovo",
            "Giuseppe Pirlo",
            "Gennaro Vessio"
        ],
        "published": "2024-05-22T08:28:42Z",
        "summary": "Computer aided diagnosis systems can provide non-invasive, low-cost tools to\nsupport clinicians. These systems have the potential to assist the diagnosis\nand monitoring of neurodegenerative disorders, in particular Parkinson's\ndisease (PD). Handwriting plays a special role in the context of PD assessment.\nIn this paper, the discriminating power of \"dynamically enhanced\" static images\nof handwriting is investigated. The enhanced images are synthetically generated\nby exploiting simultaneously the static and dynamic properties of handwriting.\nSpecifically, we propose a static representation that embeds dynamic\ninformation based on: (i) drawing the points of the samples, instead of linking\nthem, so as to retain temporal/velocity information; and (ii) adding pen-ups\nfor the same purpose. To evaluate the effectiveness of the new handwriting\nrepresentation, a fair comparison between this approach and state-of-the-art\nmethods based on static and dynamic handwriting is conducted on the same\ndataset, i.e. PaHaW. The classification workflow employs transfer learning to\nextract meaningful features from multiple representations of the input data. An\nensemble of different classifiers is used to achieve the final predictions.\nDynamically enhanced static handwriting is able to outperform the results\nobtained by using static and dynamic handwriting separately.",
        "pdf_link": "https://arxiv.org/pdf/2405.13438v1.pdf",
        "keywords": [
            "handwriting",
            "static handwriting",
            "computer aided diagnosis",
            "synthetically generated"
        ],
        "venue": "DATA",
        "year": "2024",
        "source_file": "data_collected_Apr2024_Nov2024_tnt_kid.json"
    },
    {
        "title": "Lightweight Zero-shot Text-to-Speech with Mixture of Adapters",
        "authors": [
            "Kenichi Fujita",
            "Takanori Ashihara",
            "Marc Delcroix",
            "Yusuke Ijima"
        ],
        "published": "2024-07-01T13:45:31Z",
        "summary": "The advancements in zero-shot text-to-speech (TTS) methods, based on\nlarge-scale models, have demonstrated high fidelity in reproducing speaker\ncharacteristics. However, these models are too large for practical daily use.\nWe propose a lightweight zero-shot TTS method using a mixture of adapters\n(MoA). Our proposed method incorporates MoA modules into the decoder and the\nvariance adapter of a non-autoregressive TTS model. These modules enhance the\nability to adapt a wide variety of speakers in a zero-shot manner by selecting\nappropriate adapters associated with speaker characteristics on the basis of\nspeaker embeddings. Our method achieves high-quality speech synthesis with\nminimal additional parameters. Through objective and subjective evaluations, we\nconfirmed that our method achieves better performance than the baseline with\nless than 40\\% of parameters at 1.9 times faster inference speed. Audio samples\nare available on our demo page\n(https://ntt-hilab-gensp.github.io/is2024lightweightTTS/).",
        "pdf_link": "https://arxiv.org/pdf/2407.01291v1.pdf",
        "keywords": [
            "adapters",
            "mixture of adapters",
            "text to speech",
            "tts",
            "speech synthesis",
            "shot text to speech",
            "variance adapter"
        ],
        "venue": "DATA",
        "year": "2024",
        "source_file": "data_collected_Apr2024_Nov2024_tnt_kid.json"
    },
    {
        "title": "Deep Learning-based Prediction of Breast Cancer Tumor and Immune Phenotypes from Histopathology",
        "authors": [
            "Tiago Gonçalves",
            "Dagoberto Pulido-Arias",
            "Julian Willett",
            "Katharina V. Hoebel",
            "Mason Cleveland",
            "Syed Rakin Ahmed",
            "Elizabeth Gerstner",
            "Jayashree Kalpathy-Cramer",
            "Jaime S. Cardoso",
            "Christopher P. Bridge",
            "Albert E. Kim"
        ],
        "published": "2024-04-25T08:15:37Z",
        "summary": "The interactions between tumor cells and the tumor microenvironment (TME)\ndictate therapeutic efficacy of radiation and many systemic therapies in breast\ncancer. However, to date, there is not a widely available method to\nreproducibly measure tumor and immune phenotypes for each patient's tumor.\nGiven this unmet clinical need, we applied multiple instance learning (MIL)\nalgorithms to assess activity of ten biologically relevant pathways from the\nhematoxylin and eosin (H&E) slide of primary breast tumors. We employed\ndifferent feature extraction approaches and state-of-the-art model\narchitectures. Using binary classification, our models attained area under the\nreceiver operating characteristic (AUROC) scores above 0.70 for nearly all gene\nexpression pathways and on some cases, exceeded 0.80. Attention maps suggest\nthat our trained models recognize biologically relevant spatial patterns of\ncell sub-populations from H&E. These efforts represent a first step towards\ndeveloping computational H&E biomarkers that reflect facets of the TME and hold\npromise for augmenting precision oncology.",
        "pdf_link": "https://arxiv.org/pdf/2404.16397v1.pdf",
        "keywords": [
            "immune phenotypes",
            "breast cancer",
            "breast cancer tumor",
            "tumor microenvironment",
            "deep learning based prediction",
            "gene expression",
            "multiple instance learning"
        ],
        "venue": "DATA",
        "year": "2024",
        "source_file": "data_collected_Apr2024_Nov2024_tnt_kid.json"
    },
    {
        "title": "Artificial Intelligence for Multi-Unit Auction design",
        "authors": [
            "Peyman Khezr",
            "Kendall Taylor"
        ],
        "published": "2024-04-24T03:51:26Z",
        "summary": "Understanding bidding behavior in multi-unit auctions remains an ongoing\nchallenge for researchers. Despite their widespread use, theoretical insights\ninto the bidding behavior, revenue ranking, and efficiency of commonly used\nmulti-unit auctions are limited. This paper utilizes artificial intelligence,\nspecifically reinforcement learning, as a model free learning approach to\nsimulate bidding in three prominent multi-unit auctions employed in practice.\nWe introduce six algorithms that are suitable for learning and bidding in\nmulti-unit auctions and compare them using an illustrative example. This paper\nunderscores the significance of using artificial intelligence in auction\ndesign, particularly in enhancing the design of multi-unit auctions.",
        "pdf_link": "https://arxiv.org/pdf/2404.15633v3.pdf",
        "keywords": [
            "artificial intelligence",
            "reinforcement learning",
            "model free learning",
            "auction",
            "auction design",
            "multi unit auctions"
        ],
        "venue": "DATA",
        "year": "2024",
        "source_file": "data_collected_Apr2024_Nov2024_tnt_kid.json"
    },
    {
        "title": "Automated Quantum Circuit Design with Nested Monte Carlo Tree Search",
        "authors": [
            "Pei-Yong Wang",
            "Muhammad Usman",
            "Udaya Parampalli",
            "Lloyd C. L. Hollenberg",
            "Casey R. Myers"
        ],
        "published": "2022-07-01T00:30:01Z",
        "summary": "Quantum algorithms based on variational approaches are one of the most\npromising methods to construct quantum solutions and have found a myriad of\napplications in the last few years. Despite the adaptability and simplicity,\ntheir scalability and the selection of suitable ans\\\"atzs remain key\nchallenges. In this work, we report an algorithmic framework based on nested\nMonte-Carlo Tree Search (MCTS) coupled with the combinatorial multi-armed\nbandit (CMAB) model for the automated design of quantum circuits. Through\nnumerical experiments, we demonstrated our algorithm applied to various kinds\nof problems, including the ground energy problem in quantum chemistry, quantum\noptimisation on a graph, solving systems of linear equations, and finding\nencoding circuit for quantum error detection codes. Compared to the existing\napproaches, the results indicate that our circuit design algorithm can explore\nlarger search spaces and optimise quantum circuits for larger systems, showing\nboth versatility and scalability.",
        "pdf_link": "https://arxiv.org/pdf/2207.00132v1.pdf",
        "keywords": [
            "quantum circuit design",
            "quantum algorithms",
            "quantum optimisation"
        ],
        "venue": "DATA",
        "year": "2022",
        "source_file": "data_collected_July2022-Jan2023_tnt_kid.json"
    }
]