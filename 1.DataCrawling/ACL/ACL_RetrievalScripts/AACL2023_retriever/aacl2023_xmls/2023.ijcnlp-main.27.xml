<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">24-bit Languages</title>
				<funder ref="#_wBj28ry #_ujNRbkk">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yiran</forename><surname>Wang</surname></persName>
							<email>yiran.wang@nict.go.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Information and Communications Technology (NICT)</orgName>
								<address>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nara Institute of Science and Technology (NAIST)</orgName>
								<address>
									<settlement>Nara</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
							<email>mutiyama@nict.go.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Information and Communications Technology (NICT)</orgName>
								<address>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
							<email>yuji.matsumoto@riken.jp</email>
							<affiliation key="aff2">
								<orgName type="department">RIKEN Center for Advanced Intelligence Project (AIP)</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">24-bit Languages</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1326A0BDCDDF7399DBC84E06DF011B09</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a contrastive hashing method to compress and interpret the contextual representation of pre-trained language models into binary codes. Unlike previous work that generates token-level tags, our method narrows the representation bottleneck to codes with only 24 bits, retaining task-relevant information in a more interpretable and fine-grained format without sacrificing performance (in most cases).</p><p>We provide experiments and discussions on various structured prediction tasks, such as part-ofspeech tagging, named entity recognition, and constituency parsing, to demonstrate the effectiveness and interpretability of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-trained language models <ref type="bibr" target="#b6">(Devlin et al., 2019;</ref><ref type="bibr" target="#b20">Liu et al., 2019;</ref><ref type="bibr" target="#b18">Lewis et al., 2020;</ref><ref type="bibr" target="#b26">Radford et al., 2019;</ref><ref type="bibr" target="#b10">He et al., 2021)</ref> have already become the de-facto infrastructure of modern natural language processing. They have significantly improved performance on various tasks and, at the same time have profoundly and permanently changed the research paradigm. However, lacking interpretability still keeps them a black box to humans, the inability to explain their decision-making mechanisms hinders researchers from further improving them. Fortunately, two recently published papers, which focus on compressing and interpreting continuous representation as discrete tags from pre-trained language models, have shed some light on this issue.</p><p>On the one hand, Li and Eisner (2019) propose to compress the contextual representation from pre-trained language models into discrete tags. They utilize the variational information bottleneck <ref type="bibr" target="#b30">(Tishby and Zaslavsky, 2015;</ref><ref type="bibr" target="#b1">Alemi et al., 2017)</ref> to nonlinearly interpret high-dimensional continuous vectors into discrete tags, retaining only the information that aids the downstream parsing task. These obtained tags form an alternative tag set and contain necessary syntactic properties. Moreover, Figure <ref type="figure">1</ref>: The architecture of the hashing stage model for named entity recognition. The transformer hash layer ( §3.1) produces both contextual representation h and ego-attention scores s ( §3.1) for the task-specific fine-tuning and contrastive hashing ( §2.1), respectively. Solid lines indicate the positive instance, while dotted lines show negatives. Note that the token Frodo appears twice in different sentences, thus, to avoid including false positives and false negatives ( §2.2), there is no arrow pointing from the first Frodo to the second one.</p><p>the mechanism of the variational information bottleneck, on which their method relies, is to maximize the mutual information between latent discrete tags and targets, while simultaneously minimizing the mutual information between inputs and latent discrete tags. In this way, only the task-relevant information remains in these tags.</p><p>On the other hand, <ref type="bibr" target="#b16">Kitaev et al. (2022)</ref> similarly collapse vectors into discrete tags by employing a narrow bottleneck that limits the size of the discrete token vocabulary. Their approach consists of two stages. In the first stage, the contextual vectors of tokens are mapped to discrete tags via the vector quantization method <ref type="bibr">(van den Oord et al., 2017)</ref>. In the second stage, tags are fed into a down-stream model, referred to as the read-out network in the original paper, for downstream constituency parsing. Importantly, this read-out network has no access to the continuous vectors but only to these discrete tags, therefore, these tags are forced to encode all the needed syntactic information. Their model achieves comparable performance with only a few bits required for each word.</p><p>Different from the two methods above, we provide a novel contrastive hashing method to obtain binary codes from high-dimensional hidden states of pre-trained language models. We push the compression limit by further narrowing the information bottleneck to 24 bits. Following <ref type="bibr" target="#b16">Kitaev et al. (2022)</ref>, we also introduce a stage to verify whether the information is properly preserved in these binary codes. Additionally, we train an extremely lightweight model using these binary codes as the sole inputs. Experiments show that it successfully reproduces comparable or even slightly better performance than the original full-size model.</p><p>Moreover, our method hashes vectors into bitlevel binary codes, rather than using token-level tags as in the two previous works. Therefore, the compressed codes are much more interpretable and compact. More specifically, our hashing results not only indicate whether the syntactic properties of two given tokens are different, but also distinguish exactly which bits they differ in.</p><p>Our method builds upon contrastive hashing. We introduce a recently proposed Hamming similarity approximation <ref type="bibr" target="#b11">(Hoe et al., 2021)</ref> to combine contrastive learning with deep hashing methods. In addition, we introduce an instance selection strategy aimed at mitigating issues related to contextual false positives and false negatives. Moreover, we design a novel transformer-based hash layer, in which each attention head corresponds to a single bit. The entire model is trained to learn to hash by using both the downstream task objective and the contrastive hashing objective simultaneously. These two objectives share a portion of the attention matrix from the hash layer, ensuring that the learned binary codes are likely to properly preserve task-relevant information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Method</head><p>For many tasks, the standard approach of modern language processing is first feeding the input sentence, i.e, w 1 , . . . , w n , into a pre-trained language model to assign each token a continuous vector, i.e., x i ∈ R d , and leveraging them in the downstream task. In this work, we aim to interpret these continuous vectors as discrete binary codes, i.e., c i ∈ {-1, +1} K , which contains task-relevant information as well. In this way, our method converts continuous vectors to an interpretable format, thereby making the internal mechanism more transparent and comprehensible.</p><p>Our framework consists of two stages. In the first stage, i.e., hashing stage, we learn to hash the continuous vectors as discrete tokens. We append a transformer-based hash layer ( §3.1) to the end of a pre-trained language model and train the entire model to learn to hash by fine-tuning it on the downstream task. Novelly, we employ the contrastive hashing method ( §2.1) and carefully exclude potentially false positive and negative instances with a selection strategy ( §2.2). After training, we utilize the hash layer to re-annotate the entire dataset by assigning each token a binary code.</p><p>In the second stage, i.e., the validation stage, we evaluate whether these binary codes preserve task-relevant information or simply contain meaningless bits. Using these binary codes as the sole inputs, we train a much more lightweight model from scratch. Experiments show that even with such limited capability, our model still achieves comparable or even slightly better performance than the original full-size model. Therefore, we claim that our method properly preserves task-relevant information in these binary codes. The pseudocode can be found in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Contrastive Hashing</head><p>Contrastive learning <ref type="bibr" target="#b5">(Chopra et al., 2005;</ref><ref type="bibr" target="#b23">Oord et al., 2018;</ref><ref type="bibr" target="#b4">Chen et al., 2020;</ref><ref type="bibr" target="#b34">Zbontar et al., 2021;</ref><ref type="bibr" target="#b9">Grill et al., 2020)</ref> has already been shown to be an effective representation learning method. Its fundamental concept involves employing an encoder network to map instances into a continuous representation, i.e., x ∈ R d . It then pulls together the positive pairs and pushes apart the negative pairs by applying the following objective function 1 .</p><formula xml:id="formula_0">L self = -log exp s(x, x + ) x ′ ∈X exp s(x, x ′ ) = log x ′ ∈X exp s(x, x ′ ) -s(x, x + )</formula><p>where X is the instance batch, and s(x, y) returns the similarity between the two given instances.</p><p>Contrastive learning commonly expects instances uniformly distributed on a unit hypersphere. Therefore, the most commonly used similarity function is the cosine function,</p><formula xml:id="formula_1">s(x, y) = x ⊤ y ∥x∥ • ∥y∥ (1)</formula><p>Deep hashing methods <ref type="bibr" target="#b3">(Cao et al., 2017;</ref><ref type="bibr" target="#b29">Su et al., 2018;</ref><ref type="bibr" target="#b11">Hoe et al., 2021</ref>) also aim at mapping instances into informative representation but in discrete space, i.e., c ∈ {-1, +1} K . They first utilize an encoder network to map instances to continuous score vectors, i.e., s ∈ R K , and then obtain binary codes by taking signs, i.e., c = sign (s). Besides, deep hashing methods also pull together the positive pairs by encouraging all their bits to become the same and at the same time making negatives pairs have as many as possible different bits. Commonly, this is implemented as Hamming similarity. To be more specific, for two given score vectors, x, y ∈ R K , the similarity is defined as,</p><formula xml:id="formula_2">s(x, y) = K i=1 sign (x i ) • sign (y i )<label>(2)</label></formula><p>We notice that deep hashing shares the common fundamental concept with contrastive learning, except it represents instances in a K-dimensional Hamming space, i.e., {-1, +1} K , instead of a unit hypersphere, i.e., R d-1 . Therefore, we propose introducing Hamming similarity to extend the contrastive learning to learn to hash.</p><p>However, the Hamming similarity above is not differentiable, introducing it directly is intractable. <ref type="bibr">Recently, Hoe et al. (2021)</ref> proposed a novel similarity function that takes the sign of one of its inputs before computing their cosine similarity. They 1 We omit the temperature τ for clarity.</p><p>Algorithm 1 PyTorch-like style pseudocode.</p><p>def flatten(tokens):</p><p>""" removes &lt;pad&gt; and concatenates the remaining tokens. e.g., say the &lt;pad&gt; token is 0, and the given tokens are, &gt;&gt;&gt; <ref type="bibr">[[1, 2, 3, 4, 5]</ref>, [6, 7, 0, 0, 0], [8, 9, 10, 0, 0]] then this function returns &gt;&gt;&gt; <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">3,</ref><ref type="bibr">4,</ref><ref type="bibr">5,</ref><ref type="bibr">6,</ref><ref type="bibr">7,</ref><ref type="bibr">8,</ref><ref type="bibr">9,</ref><ref type="bibr">10]</ref> """ demonstrate that maximizing this similarity preserves semantic information as well. Therefore, we instead introduce this approach to our contrastive learning framework to learn to hash.</p><p>s(x, y) = cos (x, sign (y))</p><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Instance Selection</head><p>One of the most appealing properties of contrastive learning is that it successfully converts tasks from wh-questions to yes-no questions. Conventional classification requires specifying target labels for all instances, but contrastive learning only demands knowing whether two instances are identical or not. Due to this benefit, effective representation learning becomes possible even in unsupervised settings. <ref type="bibr" target="#b8">Gao et al. (2021)</ref> pass instances into a neural network twice to obtain two semantically identical but slightly augmented representations, i.e., x and x + , relying on the independently sampled dropout masks <ref type="bibr" target="#b28">(Srivastava et al., 2014)</ref>. They employ the objective L self to perform representation learning, treat these two views as positive to each other, and consider all existing instances in the batch as negatives. This simple method surprisingly works well and results in expressive representation.</p><p>Furthermore, in supervised settings, <ref type="bibr" target="#b13">Khosla et al. (2020)</ref> proposed leveraging label information by introducing an objective function capable of handling cases with multiple positive instances.</p><formula xml:id="formula_3">L sup = -1 |X + | x + ∈X + log exp s(x, x + ) x ′ ∈X exp s(x, x ′ ) = log x ′ ∈X exp s(x, x ′ ) - 1 |X + | x + ∈X + s(x, x + )</formula><p>where the X + is the set of positive instances. Obviously, the first term of L sup and L self are identical. The difference between their the second terms is that L self pulls together only one positive while L sup pulls together all positive instances. However, we observe that tokens are likely assigned different information in varying contexts, making it challenging to determine whether two identical tokens truly form a positive pair. For example, in Figure <ref type="figure">1</ref>, the token Frodo appears in both sentences. It serves as the subject in the first sentence and as the object in the second, resulting in dissimilar parses. Therefore, identical tokens may contain distinct task-relevant information and, in such cases, deserve different binary codes.</p><p>Since it is difficult to determine whether two identical tokens contain identical task-relevant information in practice, we opt not to include them in either the positive or the negative set. For the numerator part of the objective function, we remove all identical token pairs and retain only the augmented version of themselves as the sole positive instance, thereby reverting to the single positive instance scenario. For the denominator part, we also remove all identical tokens from X to exclude potential false negatives.</p><formula xml:id="formula_4">L hash = -log exp s(x, x + ) x ′ ∈{x + }∪X -exp s(x, x ′ ) (4)</formula><p>Where X -only contains tokens that are different from x. More specifically, as shown in Figure <ref type="figure">1</ref>, we consider the second Frodo as neither a positive nor a negative instance to the first Frodo, so we remove it from both the numerator and the denominator. The pseudocode of this objective function can be found in the compute_hash_loss of Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Architecture</head><p>Before introducing our transformer-based hashing layer, we briefly review the mechanism of multihead attention <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref>. The attention layer first projects the input vectors into queries, keys, and values. It then constructs output vectors by aggregating desired information from these keyvalue pairs.</p><formula xml:id="formula_5">s h i,j = (W h q x i ) ⊤ (W h k x j ) √ d h<label>(5)</label></formula><formula xml:id="formula_6">a h i,j = softmax j (s h i,j )<label>(6)</label></formula><formula xml:id="formula_7">z h i = j a h i,j (W h v x j )<label>(7)</label></formula><formula xml:id="formula_8">o i = W o z 1 i , . . . , z H i (8)</formula><p>where</p><formula xml:id="formula_9">W h q , W h k , W h v ∈ R d h ×d</formula><p>are the projection weights of query, key, and value of the h-th head, respectively. The W o ∈ R d×(H×d h ) is the output weight, d, d h , H are the input dimension, head dimension, and the number of heads, respectively. [•, . . . , •] indicate concatenation and bias terms are omitted for clarity. These hidden states o i are then fed into a feed-forward network to obtain the output vectors h i = FFN (o i ) ∈ R d for downstream tasks. Conventionally, the head size d h is simply bounded to d and H, but we let the d h become an independent hyper-parameter, therefore, d does not have to equal to d h × H in our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transformer Hash Layer</head><p>Intuitively speaking, the mechanism of attention is to selectively aggregate information from tokens. The attention score s i,j ∈ R estimates the amount of desired information that token i may obtain from token j. Specifically, s i,i estimates how much desired information is retained in token i itself. Furthermore, by increasing the number of heads to K, the vector s i,i ∈ R K reflects the desired information scores of token i from K different aspects, and can produce K bits by taking their signs.</p><p>Therefore, we add an additional transformer layer with its number of heads increased to K, and use the diagonal entries s i,i of its attention matrix as the hashing scores to learning to hash, and take their signs to generate binary codes as the hashing results after training, i.e., c i = sign (s i,i ). Since s i,i represents a form of attention directed at oneself, to distinguish it from the commonly known term self-attention, we use the term ego-attention to describe it in the remainder of this paper.</p><p>In summary, the full attention matrix s i,j is utilized in a dual manner: it not only serves the conventional purpose in the Transformer architecture for computing the output vector for target prediction, but also lends its diagonal entries s i,i to learn to hash. Given that a portion of the attention matrix is shared between these two objectives, the learned binary codes are inclined to preserve task-relevant information. This hypothesis is demonstrated by our experimental results in the validation stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hashing Stage Architecture</head><p>The architecture of the hashing stage model, as shown in Figure <ref type="figure">1</ref>, consists of one pre-trained language model, one transformer-based hash layer, and the task-specific layers. We initialize RoBERTa <ref type="bibr" target="#b20">(Liu et al., 2019)</ref> with the checkpoint roberta-base as the pre-trained language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part-of-speech Tagging</head><p>We employ an onelayered classifier and a conditional random field (CRF) <ref type="bibr" target="#b17">(Lafferty et al., 2001)</ref> to compute the loglikelihood and utilize the Viterbi algorithm <ref type="bibr" target="#b7">(Forney, 1973)</ref> for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Named Entity Recognition</head><p>We transform the sequence of vectors from the sub-token level back to the token level by taking the average of the subtoken vectors of each individual token. We use the same task-specific layers as part-of-speech tagging.</p><p>Constituency Parsing Similarly, we generate the token-level representation by averaging the vectors of sub-tokens. In addition, following <ref type="bibr" target="#b35">Zhang et al. (2020)</ref>, we use a biaffine span classifier along with a tree-structured CRF. We identify the most probable tree from all valid trees using the Cocke-Kasami-Younger (CKY) algorithm <ref type="bibr" target="#b12">(Kasami, 1965)</ref>. Following <ref type="bibr" target="#b16">Kitaev et al. (2022)</ref>, we also incorporate GPT-2 <ref type="bibr" target="#b26">(Radford et al., 2019)</ref> using the gpt2-medium checkpoint for incremental parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Validation Stage Architecture</head><p>As mentioned above, this stage is only to validate if the task-relevant information has been properly preserved in these binary codes, and is not to distill knowledge into a lightweight model. In this stage, we introduce an extremely lightweight model to ensure that the model lacks the capacity to learn the tasks from scratch. As such, any performance gains can only be owed to the information already preserved within the binary codes. The architecture for this validation stage consists of a binary code embedding layer, a conventional one-layered transformer as encoder, and the same task-specific layers used during the hashing stage.</p><p>The binary code embedding layer produces code embeddings through constructing instead of looking up. For a given binary code, c ∈ {-1, +1} K , the binary code embedding layer simply flips the direction of each bit embedding b i , and returns the concatenation of these flipped vectors, where b i ∈ R d/K is the embedding of the i-th bit.</p><formula xml:id="formula_10">w = [c 1 b 1 , . . . , c K b K ] ∈ R d<label>(9)</label></formula><p>Compared with the learned discrete tags of <ref type="bibr" target="#b16">Kitaev et al. (2022)</ref>, our binary codes literally encode information at the bit level, while their tags remain at the token level. Thus, although <ref type="bibr" target="#b16">Kitaev et al. (2022)</ref> emphasize that their model requires only K bits per word, in practice, their model demands an embedding matrix with shape 2 K × d, while our real bit-level embedding needs only K × d K .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training and Inference</head><p>In the hashing stage, we balance the task-specific loss L task and the hashing loss L hash , as the fine_tuning_step function in Algorithm 1. Besides, our training procedure is also simpler than </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Settings</head><p>We implement our models with the deep learning framework PyTorch <ref type="bibr" target="#b24">(Paszke et al., 2019)</ref> and fetch weights of pre-trained language model from huggingface/tramsformers <ref type="bibr" target="#b33">(Wolf et al., 2020)</ref>.</p><p>For each batch, we keep collating sentences until the total number of tokens reaches 1024. The reason that we don't use the number of sentences as batch size is to stabilize contrastive learning, since it is performed at token-level, not at sentencelevel. We employ AdamW <ref type="bibr" target="#b14">(Kingma and Ba, 2014;</ref><ref type="bibr" target="#b21">Loshchilov and Hutter, 2019)</ref> with 50,000 training steps and 6% warm-up steps. In the hashing stage, we evaluate the performance with different number of bits, specifically K ∈ {16, 24, 32}.</p><p>We run experiments on a single NVIDIA Tesla V100 graphics card. The hashing stage training takes about 2 hours, while the validation stage requires only around 30 minutes. We run the experiments four times with different random seeds. The reported numbers in the following tables are their averages. For comparison, we additionally conduct a baseline experiment for each task without using the contrastive hashing loss, i.e., β = 0.</p><p>Part-of-speech Tagging We conduct experiments on the English Penn Treebank <ref type="bibr" target="#b22">(Marcus et al., 1993)</ref> datasets. The task involves assigning a syntactic label to each token in a given sentence. We report the accuracy scores on the test split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Named Entity Recognition</head><p>The OntoNotes English dataset <ref type="bibr" target="#b25">(Pradhan et al., 2013)</ref> is used for evaluation. We transform span annotations into the BIOES encoding scheme <ref type="bibr" target="#b27">(Ramshaw and Marcus, 1995)</ref>, and report the F1 scores on the test split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constituency Parsing</head><p>We evaluate on the English Penn Treebank <ref type="bibr" target="#b22">(Marcus et al., 1993)</ref>. Following <ref type="bibr" target="#b35">Zhang et al. (2020)</ref> and <ref type="bibr" target="#b16">Kitaev et al. (2022)</ref>, we transform the original tree into those of Chomsky normal form and adopt left binarization with NLTK <ref type="bibr" target="#b2">(Bird et al., 2009)</ref>. We report the F1 scores on the WSJ test split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>As presented in Table <ref type="table" target="#tab_1">1</ref>, experiments on the part-ofspeech tagging show that 32 bits achieve slightly better results than 16 bits and 24 bits on both stages. Besides, we notice that results in the validation stage are constantly superior to hashing stage results, no matter how many bits are used.</p><p>For named entity recognition, we achieve 90.39 in F 1 score with 24 bits, which is even slightly higher than its hashing stage performance, i.e., 90.27. For 16 bits and 32 bits, the validation stage performance also consistently surpasses their hashing stage performance. We hypothesize that this is because hashing the ego-attention scores may implicitly exclude some unconfident attention scores that might lead to wrong predictions. For example, consider a token that barely contains the desired information of a query, it should be ignored by getting a small attention score. However, if the network unconfidently assigns it an attention score that is only slightly less than 0, then its information still occupies a certain proportion in the final output. On the contrary, our method truncates the attention scores to be -1 or +1, and eases the issue in some degree.</p><p>For constituency parsing, our method outperforms <ref type="bibr" target="#b16">Kitaev et al. (2022)</ref> with 32 bits in the bidirectional parsing task, even they introduce much more tags, i.e., 256 in total. Besides, our 16 bits and 24 bits settings also achieve remarkable performance and are only slightly inferior to theirs. In this task, all experiments in the validation stage show worse results than the corresponding hashing stage results. We hypothesize that this is because constituency parsing is a span-level classification task, token-level hashing is unable to capture the span information completely. This may also be the reason that our method works well on part-ofspeech and named entity recognition tasks since they are just at the token level.</p><p>For all these tasks, with such a lightweight model in validation stages, our codes still reproduce comparable or even slightly better performance than the original full-size model. We claim that these results demonstrate that our learned binary codes have properly preserved task-relevant information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>Table <ref type="table" target="#tab_2">2</ref> shows that the similarity and objective functions are essential to our method. Using the cosine similarity, the model shows relatively high performance in the hashing stage, however, the naive cosine similarity can not preserve information properly, as its performance dramatically drops in validation stage. Furthermore, the fact that L hash consistently outperforms both L sup and L self demon- strates our hypothesis that false positives and false negatives are harmful. Additionally, as indicated in Equation <ref type="formula">10</ref>, the coefficient β serves to balance the two terms. According to Table <ref type="table" target="#tab_3">3</ref>, even though the contrastive hashing loss requires only a minor proportion of the overall loss, demonstrated by the optimal performance of a small β = 0.005, it is critical for preserving information. Experiments reveal that removing the contrastive hashing loss, i.e., β = 0, results in a dramatic performance drop.</p><formula xml:id="formula_11">β 0 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Studies</head><p>We present the hashing and constituency parsing results in Figure <ref type="figure" target="#fig_1">3</ref> to demonstrate the interpretability of our learned binary codes. For comparison with <ref type="bibr" target="#b16">Kitaev et al. (2022)</ref>, we use the exact same examples as in their paper. Additional parsing results can be found in Appendix D.</p><p>We begin by discussing bidirectional parsing. In our transformer-based hash layer, each head corresponds to a single bit, and these heads operate independently of one another. This design allows each bit to capture distinct and orthogonal syntactic and semantic properties. Notably, we observe that the generated binary codes cluster based on the part-of-speech properties. For example, the past tense verbs brought and approved receive similar codes even when they appear in different sentences, differing by only four bits. Similarly, the common nouns groceries and proposal share 28 bits, highlighting their shared noun properties.</p><p>Moreover, since both groceries and proposal finalize a similar noun phrase, the article the before them is assigned the same code. However, the article the before the council retains quite different bits. We hypothesize these bits indicate the varied attachment locations. Besides, for the two sentences on the left side, the final attachments him and himself determine the attachment location of the for phrases. We observe that there are only 2 bits differ between them, and hypothesize these two bits reflect the differences in the attachment locations. Apart from that, the subject Lucas and the predicate verb brought also flip one bit, respectively, to indicate the different phrase structures. Similarly, for the right side sentences, Monday and taxes differ in 5 bits, and the attachment locations of all the phrases that depend on this phrase are influenced, thus, approved, the, and proposal alters their bits as well.</p><p>Besides, incremental parsing disallows the information from future tokens, and the future tokens potentially contain syntactic properties that is needed for committing parsing decisions. Therefore, compressed codes should not only retain the already revealed information but also be open to all possible upcoming tokens, as called speculation free in <ref type="bibr" target="#b16">Kitaev et al. (2022)</ref>. Therefore, needed information is mostly distributed in the last tokens, and thus they are likely to obtain varied codes reflecting varied phrases. For example, on the left side, the last noun tokens him and himself obtain quite different codes, 5 bits different in total, more than the 2 bits in the bidirectional parsing case above. Besides, incremental parsing model also commits similar bits for the article the before groceries and proposal, i.e., only 1 different bit, but assigns a much different code to the article the before council, which has 15 nonidentical bits. By comparison, even <ref type="bibr" target="#b16">Kitaev et al. (2022)</ref> also assign them distinct tags, e.g., 11, 92, and 122, but it is hard for them to tell how different they are and where the differences lie exactly. Thus, we claim that our binary codes are much more informative and interpretable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Bit Distribution</head><p>To further analyze what specific information is preserved by each bit, we display the bit distribution for named entity recognition in Figure <ref type="figure">4</ref>.</p><p>The sub-figure above illustrates the distribution of bits related to different syntactic information, which serves to indicate the boundary of each entity. It is noteworthy that the bit distributions for the non-entity label O are uniform, such that in all these positions the probability of being assigned a 1 is roughly around 50%. In contrast, the distribution of bits for other labels exhibits a clear bias. For instance, on the 9-th bit position, we observed that the label S and B have 80% and 73% probabilities of being assigned a 1, while the numbers drop to only 47% and 17% for the E and I labels. We hypothesize the reason is that both S and B can in-  <ref type="table" target="#tab_5">53 52 49 44 50 48 58 50 50 49 53 48 45 49 43 51 49 51 56 58 47 49 53   76 0 49 100 81 59 66 65 51 17 64 41 71 100 79 73 91 84 42 13 0 41 38 24   70 28 38 100 51 63 94 64 58 0 47 29 50 30 0 72 57 58 60 1 58 61 44 30   0 57 24 35 50 59 73 42 45 91 55 16 45 100 29 91 41 100 29 27 74 19</ref>  dicate the beginning of an entity, but such syntactic function is not shared by the other two labels.</p><p>The sub-figure below shows the bit distribution related to semantic information and reveals more distinct distributional features. Although the nonentity label O continues to display uniform distribution characteristics, labels MONEY, NORP, and PERCENT show that the probabilities at the 4-th and 17-th bits are skewed to 100% and 0%, respectively. Such a clear tendency, low entropy in other words, suggests that task-relevant information is clearly and deterministically preserved within these bits, such that each bit carries a distinct meaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we have proposed a contrastive hashing method to generate interpretable binary codes from pre-trained language models. We designed a transformer-based hash layer, incorporated it into the contrastive hashing framework, and introduced a novel instance selection strategy to exclude false positives and negatives. Experimental results indicate that our lightweight model achieves superior performance and preserve task-relevant information properly with even fewer bits. Further analyses show that the generated binary codes retain syntactic and semantic information in a highly interpretable and fine-grained format. Although we only focus on structured prediction tasks in this paper, as a novel interpretable representation, our method can be easily adapted to other tasks and may inspire future research on designing efficient architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>Although our methods surpass previous work, there is still room for improvement in tasks not at the token level, e.g., constituency parsing. Besides, even the limit has been pushed to 24 bits, which is much better than previous work. However, this is still not the theoretical limit. For example, the total number of labels of named entity recognition is 73, thus, the limit is ⌈log 2 73⌉ = 7 bits, which is still fewer than ours. We remain solving this limitation and further narrowing the information bottleneck as future work. Table <ref type="table">5</ref>: OntoNotes ablation study results with the temperature τ , which controls the strength of penalties on hard negative samples <ref type="bibr" target="#b32">(Wang and Liu, 2021)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hyper-parameter Settings</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>def compute_hash_loss(x, y, tokens): # Equation 3 score = cos(x[:, None], y[None, :].sign(), dim=-1) score = score / tau # [tok, tok] # excludes potentially false positives and negatives mask = tokens[:, None] == tokens[None, :] # [tok, tok] score[mask ^eye] = -float('inf') # Equation 4 return (score.logsumexp(dim=-1) -score.diag()).mean() def fine_tuning_step(plm, task_model, inputs, targets): h1, s1 = plm(inputs) # [bsz, snt, dim], [bsz, snt, K] h2, s2 = plm(inputs) # [bsz, snt, dim], [bsz, snt, K] task_loss1 = compute_task_loss(task_model(h1), targets) task_loss2 = compute_task_loss(task_model(h2), targets) task_loss = task_loss1 + task_loss2 s1 = flatten(s1) # [tok, K] s2 = flatten(s2) # [tok, K] tokens = flatten(inputs) # [tok] hash_loss1 = compute_hash_loss(s1, s2, tokens) hash_loss2 = compute_hash_loss(s2, s1, tokens) hash_loss = hash_loss1 + hash_loss2 # Equation 10 return task_loss + beta * hash_loss def reannotate(plm, dataset): new_dataset = [] for inputs in dataset: _, s = plm(inputs) # [bsz, snt, k] codes = s.sign() # [bsz, snt, k] new_dataset.extend(codes) return new_dataset def validation_step(lite_task_model, codes, targets): logits = lite_task_model(codes) task_loss = compute_task_loss(logits, targets) return task_loss plm: the pre-trained language model with an additional transformer layer; task_model: the task-specific model; lite_task_model: the lightweight task-specific model with binary code embedding; bsz: the batch size; snt: the sentence length; tok: the total number of tokens in this batch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Examples of the hashing and constituency parsing There are three numbers below each token, the first two are represented in hexadecimal (32 bits), and indicate the hashing results of the bidirectional (RoBERTa) and unidirectional (GPT2) pre-trained language models, respectively. The third number is taken from<ref type="bibr" target="#b16">Kitaev et al. (2022)</ref> for comparison and is represented in decimal. The red and blue parts indicate the exact different bits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure4: The heatmap of bits distribution. The sub-figure above shows the distribution of bits concerning different syntactic information, while the one below corresponds to semantic information. The number inside cell represents the probability of this label being assigned a 1 at the n-th bit position. For example, the 72 at the bottom left corner indicates that among all of the WORK_OF_ART labels, 72% of them are assgiend a 1 at the first bit position.</figDesc><graphic coords="9,119.61,155.54,354.99,148.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Derivation of the sentence The quick brown fox jumps over the lazy dog, and the sentence The lazy dog jumps over the quick brown fox.S</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Examples of our method on the named entity recognition task. We assign each word a binary code, i.e., these hexadecimal numbers, and use them as the sole input to recognize entities. PER and PROD are the entity labels for person and product, respectively.</figDesc><table><row><cell></cell><cell>PER</cell><cell>PROD</cell></row><row><cell cols="3">Frodo held the ring</cell></row><row><cell cols="3">fb63a5 a5bc2d 4cf759 a10628</cell></row><row><cell>PER</cell><cell></cell><cell>PER</cell></row><row><cell cols="3">Angmar stabbed Frodo with a blade</cell></row><row><cell>476e71</cell><cell>81e529</cell><cell>fb63e6 e8853d 51cd5c 210628</cell></row><row><cell>Figure 2:</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The main results on three datasets. The results of our methods are displayed in two rows, which indicate the performance in hashing and validation stages, respectively. |θ| columns show the number of parameters, and the bold numbers indicats the best validation performance of each setting.</figDesc><table><row><cell cols="2">Kitaev et al. (2022), since we don't need to employ</cell></row><row><cell cols="2">the k-mean algorithm (Ackermann et al., 2012) to</cell></row><row><cell>initialize the centroids in the first two epochs.</cell><cell></cell></row><row><cell>L = L task + β • L hash</cell><cell>(10)</cell></row><row><cell cols="2">In the validation stage, we re-annotate the entire</cell></row><row><cell cols="2">dataset first and then use the task-specific loss</cell></row><row><cell cols="2">L task only to train the lightweight model with only</cell></row><row><cell cols="2">these binary codes as inputs. The procedures for</cell></row><row><cell cols="2">reannotate and validation_step are described</cell></row><row><cell>in Algorithm 1, respectively.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different similarity functions and objective functions on the OntoNotes dataset. The numbers on the left and right sides of → represent the hashing and validation performance, respectively.</figDesc><table><row><cell>s(x, y)</cell><cell>L contrastive</cell><cell>NER</cell></row><row><cell></cell><cell>L self</cell><cell>90.12 → 88.74</cell></row><row><cell>cos (x, y)</cell><cell>L sup</cell><cell>90.07 → 86.91</cell></row><row><cell></cell><cell>L hash</cell><cell>90.19 → 88.94</cell></row><row><cell></cell><cell>L self</cell><cell>90.15 → 90.21</cell></row><row><cell>cos (x, sign (y))</cell><cell>L sup</cell><cell>90.19 → 90.04</cell></row><row><cell></cell><cell>L hash</cell><cell>90.27 → 90.39</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Named Entity Recognition experiments with β. The two rows display hashing and validation performance, respectively.</figDesc><table><row><cell></cell><cell>001 0.005 0.01</cell><cell>0.05</cell></row><row><cell>NER</cell><cell cols="2">90.24 90.25 90.27 90.10 90.02 79.60 90.29 90.39 90.24 90.23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>49 44 50 48 58 50 50 49 53 48 45 49 43 51 49 51 56 58 47 49 53 77 42 34 67 66 44 73 48 80 30 94 86 99 52 42 51 58 59 41 24 26 48 85 23 64 22 49 64 68 48 87 49 73 35 73 4 54 57 58 90 40 82 44 19 47 59 54 84 26 26 36 89 74 59 78 58 47 22 59 49 42 89 39 62 31 78 27 39 62 60 62 43 56 12 56 85 34 69 57 45 17 25 43 16 38 83 32 55 83 77 37 37 10 16 27 38</figDesc><table><row><cell>O S B E I ORG PERSON DATE GPE CARDINAL MONEY NORP PERCENT WORK_OF_ART O</cell><cell>48 53 52 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Statistics of these three datasets.</figDesc><table><row><cell cols="6">DATASET TRAIN DEV TEST LABEL</cell></row><row><cell>POS</cell><cell cols="4">39,832 1,700 2,416</cell><cell>45</cell></row><row><cell>NER</cell><cell cols="4">59,924 8,528 8,262</cell><cell>73</cell></row><row><cell cols="5">PARSING 39,832 1,700 2,416</cell><cell>143</cell></row><row><cell>τ</cell><cell>0.01</cell><cell>0.02</cell><cell>0.05</cell><cell>0.1</cell><cell>0.2</cell></row><row><cell>NER</cell><cell cols="5">90.19 90.08 90.02 90.27 90.13 89.13 89.12 89.81 90.39 90.16</cell></row></table><note><p>B Ablation Study on Temperature τ</p></note></figure>
		</body>
		<back>

			<div type="funding">
<div><head>Pre-trained Language Model</head><p><rs type="person">Transformer Hash Layer Classifier + CRF Angmar stabbed Frodo</rs> with a blade Frodo held the ring</p></div>
<div><head>S-PER O S-PER O O O S-PER O B-PROD E-PROD</head><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L w 1 f 0 g R M 7 m q y + R Y T F t s <rs type="programName">K V T r E U K Q = " &gt; A A A C 0 X</rs> i c j V H L S g M x F D 0 d X 7 W + q i 7 d D B b B V Z k R X w s X B T c u K 9 o H 1 C o z 0 7 Q d O i + S j F B K Q d z 6 A 2 7 1 p 8 Q / 0 L / w J k 5 B L a I Z J j k 5 9 5 6 T 3 F w 3 C X w h L e s 1 Z 8 z M z s 0 v 5 B c L S 8 s r q 2 v F 9 Y 2 6 i F P u s Z o X B z F v u o 5 g g R + x m v R l w J o J Z 0 7 o B q z h D k 5 V v H H L u P D j 6</p><p>O p 5 q Z 8 X + 5 j 3 S n u p u Q 1 r d z C s k V q J P 7 F + 6 S e Z / d a o W i S 6</p><p>h &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z q g j Q s B v E y / C g W y E 4 H 7 t 4 f u N V F Y = " &gt; A A A C 0 X i c j V H L S g M x F D 0 d X 7 W + q i 7 d D B b B V Z m K r 4 W L g h u X F e 0 D 2 i o z 0 7 Q O n R d J R h h K Q d z 6 A 2 7 1 p 8 Q / 0 L / w J k 5 B L a I Z J j k 5 9 5 6 T 3 F w n 9 j 0 h L e s 1 Z 8 z M z s 0 v 5 B c L S 8 s r q 2 v F 9 Y 2 G i B L u s r o b + R F v O b Z g v h e y u v S k z 1 o x Z 3 b g + K z p D E 9 V v H n L u P C i 8 F K m M e s G 9 i D 0 + p 5 r S 6 K u O k 7 k 9 0 Q a 0 D I S 4 + t i y S p b e p j T o J K B E r J R i 4 o v <rs type="grantNumber">6 K C H C C 4</rs> <rs type="projectName">S B G A I I Q n 7 s C H</rs> o a 6 M C C z F x X Y y I 4 4 Q 8 H W c Y o 0 D a h L I Y Z d j E D m k e 0 K 6 d s S H t l a f Q a p d O 8 e n n p D S</p><p>x Q 5 q I 8 j h h d Z q p 4 4 l 2 V u x v 3 i P t q e 6 W 0 u p k X g G x E j f E / q W b Z P 5 X p 2 q R 6 O N Y 1 + B R T b F m V H V u 5 p L o V 1 E 3 N 7 9 U J c k h J k 7 h H s U 5 Y V c r J + 9 s a o 3 Q t a u 3 t X X 8 T W c q V u 3 d L D f B u 7 o l N b j y s 5 3 T o L F X r h y W D 8 7 3 S 9 W T r N V 5 b G E b u 9 T P I 1 R x h h r q 5 M 3 x i C c 8 G x d G a t w Z 9 5</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wBj28ry">
					<orgName type="program" subtype="full">K V T r E U K Q = &quot; &gt; A A A C 0 X</orgName>
				</org>
				<org type="funded-project" xml:id="_ujNRbkk">
					<idno type="grant-number">6 K C H C C 4</idno>
					<orgName type="project" subtype="full">S B G A I I Q n 7 s C H</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Streamkm++ a clustering algorithm for data streams</title>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Marcel R Ackermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Märtens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamil</forename><surname>Raupach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christiane</forename><surname>Swierkot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Lammersen</surname></persName>
		</author>
		<author>
			<persName><surname>Sohler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Algorithmics (JEA)</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2" to="3" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep variational information bottleneck</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Natural language processing with Python: analyzing text with the natural language toolkit</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hashnet: Deep learning to hash by continuation</title>
		<author>
			<persName><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The viterbi algorithm. ings of the IEEE</title>
		<author>
			<persName><forename type="first">David</forename><surname>Forney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="268" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SimCSE: Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.552</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6894" to="6910" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2006.07733</idno>
		<editor>Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, and Michal Valko</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DeBERTa: Decoding-enhanced BERT with Disentangled Attention</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">One loss for all: Deep hashing with a single cosine similarity based learning objective</title>
		<author>
			<persName><forename type="first">Kam</forename><forename type="middle">Woh</forename><surname>Jiun Tian Hoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chee</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Zhe</forename><surname>Seng Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An efficient recognition and syntax-analysis algorithm for context-free languages</title>
		<author>
			<persName><forename type="first">Tadao</forename><surname>Kasami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18661" to="18673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multilingual constituency parsing with self-attention and pre-training</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1340</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3499" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learned incremental representations for parsing</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.220</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3086" to="3095" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
		<title level="m">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Specializing word embeddings (for parsing) by information bottleneck</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Eisner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1276</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2744" to="2754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Roberta: A robustly optimized bert pretraining approach</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards robust linguistic analysis using OntoNotes</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tou</forename><surname>Hwee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Text chunking using transformation-based learning</title>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitch</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Workshop on Very Large Corpora</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Greedy hash: Towards fast optimization for accurate hash coding in cnn</title>
		<author>
			<persName><forename type="first">Shupeng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle. Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu</title>
		<author>
			<persName><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noga</forename><surname>Zaslavsky</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1503.02406</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015. 2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>Neural discrete representation learning</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Understanding the behaviour of contrastive loss</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2495" to="2504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<title level="m">Transformers: State-of-the-Art Natural Language Processing</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephane</forename><surname>Deny</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="12310" to="12320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast and accurate neural crf constituency parsing</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/560</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4046" to="4053" />
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization. Main track</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
