<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Assessment of Pre-Trained Models Across Languages and Grammars</title>
				<funder ref="#_pfyRsFd #_JxjzrMy">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_hWF8qPS">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
				<funder>
					<orgName type="full">Formación Profesional e Universidades</orgName>
				</funder>
				<funder ref="#_NeahWpX">
					<orgName type="full">ERDF/MICINN-AEI (SCANNER-UDC</orgName>
				</funder>
				<funder ref="#_uUtjyTg">
					<orgName type="full">Xunta de Galicia</orgName>
				</funder>
				<funder ref="#_FCrNzNC">
					<orgName type="full">Centro de Investigación de Galicia &quot;CITIC&quot;</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alberto</forename><surname>Muñoz-Ortiz</surname></persName>
							<email>alberto.munoz.ortiz@udc.es</email>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Vilares</surname></persName>
							<email>david.vilares@udc.es</email>
						</author>
						<author>
							<persName><forename type="first">Carlos</forename><surname>Gómez-Rodríguez</surname></persName>
							<email>carlos.gomez@udc.es</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Universidade da Coruña</orgName>
								<address>
									<country>CITIC</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Departamento de Ciencias de la Computación y Tecnologías de la Información</orgName>
								<address>
									<addrLine>Campus de Elviña s/n</addrLine>
									<postCode>15071 A</postCode>
									<settlement>Coruña</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Assessment of Pre-Trained Models Across Languages and Grammars</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F3FE90FA2D5212F7B97684DE3824AFB3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an approach for assessing how multilingual large language models (LLMs) learn syntax in terms of multi-formalism syntactic structures. We aim to recover constituent and dependency structures by casting parsing as sequence labeling. To do so, we select a few LLMs and study them on 13 diverse UD treebanks for dependency parsing and 10 treebanks for constituent parsing. Our results show that: (i) the framework is consistent across encodings, (ii) pre-trained word vectors do not favor constituency representations of syntax over dependencies, (iii) sub-word tokenization is needed to represent syntax, in contrast to character-based models, and (iv) occurrence of a language in the pretraining data is more important than the amount of task data when recovering syntax from the word vectors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large Language Models (LLMs) are the backbone for most NLP architectures. Their performance has not yet reached a plateau, and factors such as scale, language objective, token segmentation or amount of pre-training time -among many others -play a role in their capabilities.</p><p>To shed light on what is being learned, work on interpretability explains what these models encode in their representational space. Authors have explored whether these models exhibit stereotypical biases <ref type="bibr" target="#b33">(Nadeem et al., 2021)</ref>, encode facts <ref type="bibr" target="#b37">(Poerner et al., 2020)</ref> or capture structural knowledge in multi-modal environments <ref type="bibr">(Milewski et al., 2022)</ref>. Whether LLMs encode syntaxin their latent space has also been studied. In this respect, different probing frameworks <ref type="bibr" target="#b20">(Kulmizev and Nivre, 2022;</ref><ref type="bibr" target="#b3">Belinkov, 2022)</ref> have been introduced to measure the syntactic capability of models, although authors such as <ref type="bibr" target="#b28">Maudslay and Cotterell (2021)</ref> point out that we need to take this concept with caution, since they might not be completely isolating syntax.</p><p>Still, interpretability work on parsing focuses on either multilingual and mono-paradigm setups, or English and multi-paradigm setups. But we are not aware of multi-dimensional work. This relates to the problem of square one bias in NLP research <ref type="bibr" target="#b38">(Ruder et al., 2022)</ref>, that states that most work expands the current knowledge along just one dimension (e.g., a single language, or a single task). Related to our work, <ref type="bibr" target="#b21">Kulmizev et al. (2020)</ref> study if LLMs showed preferences across two annotation styles: deep syntactic and surface-syntactic universal dependencies, but both schemes were dependency-based. <ref type="bibr" target="#b48">Vilares et al. (2020)</ref> did study two different syntactic formalisms, dependencies and constituents, and used a sequence-labeling-like recovery framework, relying on the pretraining architectures to associate output vectors with syntactic labels. We will build on top of this framework. Yet, they only studied English, and their analysis focused on static vectors and early LLMs; apart from other limitations that we discuss later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution</head><p>We move from square one bias in syntax assessment, and propose the first multiparadigm, multilingual, recovery framework for dependency and constituent structures learned by LLMs. We select representative LLMs that vary in scale, language pretraining objectives, and token representation formats. We then study their capability to retrieve syntax information from the pretrained representations on a diverse set of constituent and dependency treebanks, that vary in factors such as language family or size, as well as the presence or absence of their languages among the pretraining data of the LLMs. The code is available at https://github.com/amunozo/ multilingual-assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>There is a long-standing effort in the NLP community to model syntax, either as a final goal or as a way to model compositionality. Yet, the ways in which this has been pursued have evolved with time.</p><p>Modeling syntax in the pre-neural times. Learning grammars through corpus-based approaches <ref type="bibr" target="#b26">(Marcus et al., 1993;</ref><ref type="bibr" target="#b7">Collins, 1996;</ref><ref type="bibr" target="#b4">Charniak, 1997;</ref><ref type="bibr" target="#b36">Petrov and Klein, 2007)</ref> has been the dominating approach in the last decades. However, early models required extensive feature engineering to obtain competitive parsers. This suggested that support vector machines (SVMs) had severe limitations understanding language structure, and needed the help of parsing algorithms <ref type="bibr" target="#b34">(Nivre, 2008;</ref><ref type="bibr" target="#b27">Martins et al., 2010)</ref>, language-dependent features <ref type="bibr" target="#b2">(Ballesteros and Nivre, 2012)</ref>, or tree-kernels <ref type="bibr" target="#b23">(Lin et al., 2014;</ref><ref type="bibr" target="#b53">Zhang and Li, 2009)</ref> to model syntax properly.</p><p>Modeling syntax in neural times. With the rise of word vectors <ref type="bibr" target="#b29">(Mikolov et al., 2013)</ref>, LSTMs <ref type="bibr" target="#b15">(Hochreiter and Schmidhuber, 1997)</ref> and Transformers <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref>, modeling structure has become less relevant to obtain a good performance, both for parsing and downstream tasks. For instance, while the classic parser by <ref type="bibr" target="#b54">Zhang and Nivre (2011)</ref> used a rich set of features (including third-order, distance, and valency features, among others) to be competitive, the parser by <ref type="bibr" target="#b5">Chen and Manning (2014)</ref> only needed 18 word and PoS tag features (and 6 dependency features) to obtain strong results, which was possible thanks to their reliance on pre-trained word vectors and neural networks. The need for feature engineering was reduced further with bidirectional LSTMs, e.g., <ref type="bibr" target="#b16">Kiperwasser and Goldberg (2016)</ref> showed that four vectors corresponding to elements in the buffer and the stack sufficed to obtain state-of-the-art performance, while <ref type="bibr" target="#b40">Shi et al. (2017)</ref> showed that competitive accuracies were possible with only two features.</p><p>Modeling syntax in the era of language models. In the context of these (almost) end-to-end parsers performing very competitively without the need of explicitly modeling syntactic linguistic features, recent efforts have been dedicated to interpret to what extent syntax is encoded in the representational space of neural networks, and in particular of LLMs. <ref type="bibr" target="#b45">Tenney et al. (2019)</ref> and <ref type="bibr">Liu et al. (2019a)</ref> proposed probing frameworks for partial parsing, in the sense that they tried to demonstrate that certain syntactic information, such as dependency types, was encoded in pre-trained models. <ref type="bibr" target="#b48">Vilares et al. (2020)</ref> defined a probing framework for full dependency and constituent parsing. They cast dependency and constituent parsing as sequence labeling and associated output vectors with syntactic labels by freezing their models. <ref type="bibr" target="#b14">Hewitt and Manning (2019)</ref> proposed a structural probing framework and identified that pre-trained models encoded a linear transformation that indicates the distance between words in a dependency tree. The framework was later upgraded to extract directed and labeled trees, while using fewer parameters <ref type="bibr">(Müller-Eberstein et al., 2022a)</ref>. <ref type="bibr" target="#b13">Hewitt and Liang (2019)</ref> pointed out that we need to be careful with probing frameworks, since the probe might be learning the linguistic task itself, instead of demonstrating the presence of the target linguistic property. For that, they recommend to use control experiments, and relied on control tasks, i.e., learning a random task with the same dimensional output space. <ref type="bibr" target="#b28">Maudslay and Cotterell (2021)</ref> showed that semantic cues in the data might guide the probe and therefore they might not isolate syntax, although their experiments still outperformed the baselines. <ref type="bibr">Müller-Eberstein et al. (2022b)</ref> found the most suitable pre-trained LLMs to plug into a dependency parser for a given treebank. Particularly, they proposed to rank frozen encoder representations by determining the percentage of trees that are recoverable from them, and based on that ranking choose which LLM to plug. Focused on morphology, <ref type="bibr" target="#b42">Stanczak et al. (2022)</ref> showed that subsets of neurons model morphosyntax across a variety of languages in multilingual LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multilingual probing frameworks</head><p>Let w = [w 1 , w 2 , ..., w n ] be an input sentence. We are interested in linear probing frameworks that can associate a sequence of word vectors ⃗ w = [ ⃗ w 1 , ⃗ w 2 , ..., ⃗ w n ] to a given linguistic property [p 1 , p 2 , ..., p n ]. For some properties, the mapping can be quite direct, such as for instance the case of part-of-speech (PoS) tagging (by putting a linear layer on top of w and outputting the PoS tag category), or lexical semantics (e.g. computing word vector similarity). We want an analogous mapping, but for multiple syntactic formalisms. In this case, the association is not trivial since syntactic parsing is a tree-based structured prediction problem. Also, we are interested in multilingual pre-trained models, which have gained interest in recent years.</p><p>Then, the goal is to associate their word vectors to an estimate of to what extent characteristics of a given formalism are encoded in their representational space, and whether this can differ across dimensions such as tested models, formalisms, and treebanks.</p><p>Linear probing framework for parsing We take the study by <ref type="bibr" target="#b48">Vilares et al. (2020)</ref> as our starting point. However, we first identify some weaknesses in their work: (i) it is limited to English, (ii) they do not give specific estimates of the amount of trees recoverable with respect to control experiments, and (iii) they only test one type of tree linearization. For the latter, the main motivation, in particular for the case of dependency parsing, was that the chosen linearization had performed the best in previous work <ref type="bibr" target="#b43">(Strzyz et al., 2019)</ref> when training from scratch a transducer without pre-training. However, later work suggests that that is debatable: for instance, <ref type="bibr" target="#b32">Muñoz-Ortiz et al. (2021)</ref> show that different tree linearizations might be better suited to different languages, and <ref type="bibr" target="#b46">Vacareanu et al. (2020)</ref>'s results indicate that other encodings worked better when pre-trained language models are used.</p><p>To recover dependency and constituent structures, we will represent the trees using existing encodings for parsing as sequence labeling <ref type="bibr" target="#b12">(Gómez-Rodríguez and Vilares, 2018;</ref><ref type="bibr" target="#b43">Strzyz et al., 2019)</ref>. Under this configuration, the interaction between learning a model and searching for linguistic properties is now direct. We can use probing architectures that rely entirely on the pretrained representations, and simply add a linear layer on top to map continuous vectors to discrete labels. We can expect that the capabilities of the output layer are not enough to learn the syntactic tasks at hand by themselves, so it must rely on the quality of the pretrained representations. Yet, we also will include control baselines that we will discuss later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Research questions</head><p>We want to answer two questions: (i) how much syntax is recoverable from different LLMs? and (ii) how is it affected by aspects such as the models, the type of formalism, and the pretraining and assessment data?</p><p>In what follows, we describe the sequence labeling encodings, both for dependency and constituent paradigms ( §3.1), and the specifics of the probing setup used for our experiments ( §3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sequence labeling encodings of syntax</head><p>Parsing as sequence labeling can be defined as learning a function f n : V n → L n to map a sequence of words into a sequence of linearized labels that can be decoded to fully recover a constituent or dependency tree. Here we are not interested in the parsers per se, but in whether the sequence-labeling encodings defined for them provide a simple, lossless representation of dependency and constituent trees that is useful for probing. In what follows, we briefly describe these representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Dependency parsing</head><p>Dependencies between tokens can be encoded using labels of the form (x i , l i ), where x i is a subset of the arcs related to the token w i , and l i denotes the dependency relation <ref type="bibr" target="#b43">(Strzyz et al., 2019)</ref>. There are different ways of encoding x i<ref type="foot" target="#foot_0">1</ref> . We compare three families of linearizations (due to brevity, we refer to the references below for the details): Head-selection <ref type="bibr" target="#b41">(Spoustová and Spousta, 2010;</ref><ref type="bibr" target="#b22">Li et al., 2018;</ref><ref type="bibr" target="#b43">Strzyz et al., 2019)</ref>. x i encodes the dependency arc pointing directly to w i . This can be done using an absolute index or a relative offset computing the difference between w i 's index and its head. We use (r h ) encoding where the head of w i is the x i th word to the right, if x i &gt; 0, and the</p><formula xml:id="formula_0">ROOT This1 painting2 is3 great4 .5 r h +1 +1 +2 -4 -2 2p b . . &lt;\ . &lt; . &lt;\\ / * . &gt; * ah tb SH_LA SH SH_LA_LA SH_RA SH</formula><formula xml:id="formula_1">x i th word to the left if x i &lt; 0. 2</formula><p>Bracketing-based (Yli-Jyrä and <ref type="bibr" target="#b52">Gómez-Rodríguez, 2017;</ref><ref type="bibr" target="#b44">Strzyz et al., 2020)</ref>. x i encodes the arcs using strings of brackets to represent a subset of the incoming and outgoing arcs of w i and its direct neighbors. We use a 2-planar bracketing encoding (2p b ) that uses two independent planes of brackets to encode non-projective trees.</p><p>Transition-based <ref type="bibr" target="#b11">(Gómez-Rodríguez et al., 2020)</ref>. x i encodes a sub-sequence of the transitions that are generated by a left-to-right transition-based parser. Given a transition list t = t 1 , ..., t m with n read transitions, t is split into n sub-sequences such that the ith sub-sequence is assigned to w i . We use a mapping from the arc-hybrid algorithm (ah tb ) <ref type="bibr" target="#b19">(Kuhlmann et al., 2011)</ref>. These mappings are implicit and often perform worse than more direct encodings, but they are learnable.</p><p>These encodings produce labels with different information. Following Figure <ref type="figure" target="#fig_0">1</ref>, for w 2 (painting), the 2p b encoding states that the previous word w 1 has one incoming arc from the right ("&lt;" symbol, but it does not say from where, as that information is encoded in other labels) and that w 2 has one outgoing arc to the left ("\" symbol, but it does not specify where). For the transition-based encoding, the mapping is less straightforward across words, but still connected to them. For instance, for w 1 ('This') the label indicates that the w 1 has no connection to w 0 , that it is a dependent of w 1 , and that it has no children. The motivation to compare encodings is to test: (i) the consistency of the framework, i.e., if trends across LLMs remain, and (ii) to see what information is easier to recover when the LLM weights are frozen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Constituent parsing</head><p>We here use the encoding approach by <ref type="bibr" target="#b12">Gómez-Rodríguez and Vilares (2018)</ref>, which encodes common levels in the tree between pairs of tokens. <ref type="foot" target="#foot_2">3</ref> The labels are of the form (n i , c i , u i ). The element n i encodes the number of tree levels that are common between w i and w i+1 , computed as the difference with respect to n i-1 . The element c i encodes the lowest non-terminal symbol that is shared between those two words. u i encodes the leaf unary branch located at w i , if it exists. An example is shown in Figure <ref type="figure" target="#fig_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Probing architecture</head><p>We use a 1-layered feed-forward network on top of the LLMs to predict the labels. We propose three setups (training hyperparameters are detailed in Appendix A):</p><p>Frozen weights (frz) The LLM weights are frozen and only the weights of the linear output layer are updated during fine-tuning.</p><p>Random weights (rnd) Only the weights of the linear classifier layer are updated, but the weights of the encoders are randomized. We aim to prevent misleading conclusions in the hypothetical case that the linear layer can learn the mapping itself, i.e., we use this setup as a lower bound baseline. It is also a control experiment, as the difference between the results of this setup and the frz setup would be the measure we are looking for to estimate the amount of syntax information encoded in the representational space of pre-trained LLMs.</p><p>Fine-tuned weights (ftd) A fine-tuned LLM where all weights are updated, i.e., this setup is used as an upper bound baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multilingual Language Models</head><p>The method here proposed is model-agnostic. Our aim is not to obtain the highest results or to use the largest LLM. We select a few LLMs that are representative and runnable with our resources: mBERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> It uses WordPiece tokenization. While subword tokenizers are effective with representative splits, they yield suboptimal subtokens for low-resource languages <ref type="bibr" target="#b0">(Agerri et al., 2020;</ref><ref type="bibr" target="#b49">Virtanen et al., 2019)</ref>, as wrong subtokens will not encode meaningful information. mBERT is pretrained on 104 languages from the dump of the largest Wikipedias.</p><p>xlm-roberta <ref type="bibr" target="#b8">(Conneau et al., 2020)</ref> A multilingual LLM trained as RoBERTa <ref type="bibr">(Liu et al., 2019b)</ref>. It has the same architecture as BERT, but only pretrained on the masked word prediction task and uses a byte-level BPE for tokenization. It has been pretrained on 2.5TB of filtered Com-monCrawl data that contains text in 100 languages (XLM-100), and for longer time than mBERT. canine (-c and -s) <ref type="bibr" target="#b6">(Clark et al., 2022)</ref> It uses char-based tokenization, which is believed to perform better in languages that are challenging for subword tokenization, such as those with vowel harmony. It eliminates the issue of unknown tokens. It is pre-trained on masked language modeling and next sentence prediction on the same data as mBERT: canine-c is pretrained using a char-level loss, while canine-s includes a previous subword tokenization to predict masked subword tokens.</p><p>In all models, labels are first broken down into subtokens before being processed by the LLMs to assign them to the n input tokens. The classifier layer then assigns a label to each subtoken (i.e. subword for mBERT and xlm-roberta and character for canine). Then, we select the label assigned to the first sub-element, which is a common approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology and Experiments</head><p>Data for dependency parsing For the assessment of dependency structures, we selected 13 Universal Dependencies (UD 2.9; <ref type="bibr" target="#b35">Nivre et al., 2020)</ref> treebanks from different language families and with different amounts of annotated data. Although mBERT, xlm-roberta, and canine have been pretrained on different (multilingual) crawled datasets, we select treebanks whose languages are either present in all our LLMs' pretraining data or in none of them (although presence proportions might vary in the case of xlm-roberta). For more details, see Table <ref type="table" target="#tab_0">1</ref>. Data sizes have been obtained from <ref type="bibr" target="#b50">Wu and Dredze (2020)</ref> for Wiki-100 and <ref type="bibr" target="#b8">Conneau et al. (2020)</ref> for XLM-100.</p><p>Data for constituent parsing We assess constituent structures on the PTB <ref type="bibr" target="#b26">(Marcus et al., 1993)</ref>, the CTB <ref type="bibr" target="#b51">(Xue et al., 2005)</ref>, and 8 constituent treebanks from the SPMRL shared task <ref type="bibr" target="#b39">(Seddah et al., 2014)</ref>  Metrics For dependency parsing, we use Labeled Attachment Score (LAS). For constituent parsing, we use the labeled bracketing F1-score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We present the assessment for dependency structures in §5.1, and for constituent structures in §5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dependency parsing results</head><p>We break down the results comparing frozen vs: (i) random, and (ii) fine-tuned weights.</p><p>Frozen (frz) vs random weights (rnd) setups ah tb r h rnd frz rnd frz rnd frz rnd frz rnd frz rnd frz rnd frz rnd frz rnd frz rnd frz rnd frz rnd frz Skolt Sami 11.5 9.2 8.0 8.4 10.4 13.5 14.2 6.9 6.5 3.0 10.5 8.5 7.6 7.2 9.0 5.1 9.2 6.2 10.5 10. clearly surpasses the rnd baseline, i.e., the control experiment. The results suggest that under the frozen setups, mbert is better than xlm-roberta at recovering dependencies, although pre-trained xlm-roberta models are usually better at downstream tasks <ref type="bibr">(Liu et al., 2019b)</ref>. The ranking of the LLMs is stable across treebanks. The LAS scores across encodings are in a similar range, and the average LAS across different encodings is very similar too (bottom row in Table <ref type="table" target="#tab_3">3</ref>). On the other hand, the results for canine do not surpass the lower bound baseline in most cases. This is unlikely to be because of a bad fitting, since the random weights baselines perform almost the same across pre-trained models, encodings and treebanks. Also, while canine-s outperforms the random baseline for the highest-resourced languages, canine-c underperforms it for all languages except for Chinese. For a clearer picture, Figure <ref type="figure" target="#fig_2">3</ref> shows the relative LAS error reductions ϵ LAS (rnd,frz) for the 2planar encoding and sorted by the size of the training set used for the probe. Next, we focus on 2p b as previous work has demonstrated its robustness across various configurations <ref type="bibr" target="#b32">(Muñoz-Ortiz et al., 2021;</ref><ref type="bibr" target="#b43">Strzyz et al., 2019</ref><ref type="bibr" target="#b44">Strzyz et al., , 2020))</ref>. 5 For larger treebanks, whose languages are supported by LLMs, the error reductions between the frz and rnd setups are large, showing that the LLMs encode to some extent dependency structures in their representational space. For languages that are not supported by the LLMs, the error reductions are clearly smaller. This happens for low-resource treebanks, in which only mBERT is able to obtain improvements over the rnd baseline, but also for high-resource 5 The trends for the other encodings are similar and they can be seen in Appendix B. ones, such as Ancient Greek (the largest tested treebank), suggesting that the treebank size is not a key factor for the probes (we discuss this in detail §5.3). Frozen (frz) vs fine-tuned (ftd) setup Table <ref type="table" target="#tab_4">4</ref> shows the scores for the fine-tuned models. In this case, xlm-roberta sequence labeling parsers obtain a larger average error reduction, while mbert obtains slightly better results for the ftd setup. The results show that even if under the frz setup dependency structures can be recovered, fine-tuning the whole architecture gives significant improvements. Also, the performance across the board for the fine-tuned models is very competitive for all treebanks supported by the LLMs. Note that even if such results lag below the state of the art (not the target of our work), we rely exclusively on multilingual pretraining vectors, without any powerful parser decoder, such as <ref type="bibr" target="#b17">Kitaev and Klein (2018)</ref> for constituent parsing, or <ref type="bibr" target="#b10">Dozat et al. (2017)</ref> for dependencies.</p><p>Encoding comparison Results from Table <ref type="table" target="#tab_3">3</ref> show that the three encodings are able to recover a similar amount of syntax. It is worth noting that, although r h performs better for the rnd setup, this does not translate into a better recovering from frz representations. It seems also that 2p b recovers more syntactic information in higher-resourced setups (i.e. Bulgarian), while r h and ah tb perform better in lower-resourced configurations (i.e Skolt Sami, Ligurian).</p><p>Dependency displacements Figure <ref type="figure" target="#fig_4">4</ref> shows the performance across arcs of different length and direction for the frz models with the 2p b encoding over 4 languages: the one with most left arcs (Turkish), with most right arcs 6 (Vietnamese), and two balanced ones (Basque and Welsh). The multilingual LLMs capture the particularities of languages (for the case of the Welsh CCG treebank, even if it is balanced in terms of the number of left/right arcs, left arcs are on average of a distance of 1.6 ±1.8 units while right arcs are of 3.9 ±4.9 units). Also, the LLMs keep the trends across displacements, i.e., no LLM notably changes their expected performance with respect to the others for a specific subset of dependencies.  We removed displacements occurring less than 10 times.</p><p>6 Guajajara is excluded due to dataset size limitations.</p><p>Treebank mBERT xlm-roberta canine-c canine-s frz ftd err frz ftd err frz ftd err frz ftd err Skolt Sami 9.2 14.6 5.9 6.9 11.2 4.6 7.2 2.6 -5.0 10.3 6.3 -4.5 Guajajara 30.9 46.6 22.6 19.0 39.0 24.7 22.2 16.6 -7.2 29.8 29.9 0.1 Ligurian 7.2 28.4 22.8 1.6 24.7 23.5 3.6 0.5 -3.2 5.7 0.8 -5.2 Bhojpuri 17.0 24.9 9.5 11.8 15.0 3.6 9.1 7.4 -1.9 11.3 14.2 3.6 Kiche 51.0 69.2 37.1 33.4 61.8 42.6 25.7 22.8 -3.9 43.1 51.1 13.9 Welsh 44.9 68.9 43.6 27.9 69.0 57.0 12.5 8.0 -5.1 20.5 31.5 13.8 Armenian 38.8 71.7 53.8 31.0 74.5 63.1 13.8 16.7 3.2 19.9 31.9 15.0 Vietnamese 37.4 58.5 33.7 24.6 60.8 48.0 10.0 7. <ref type="bibr">7 -2.6 15.3 24.4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Constituent parsing results</head><p>We break down the results comparing frozen vs: (i) random, and (ii) fine-tuned weights.</p><p>Frozen (frz) vs random weights (rnd) setups Table <ref type="table">5</ref> shows the bracketing F1 score across treebanks and the encodings for the two setups. The trend from dependency parsing remains: mBERT outperforms xlm-roberta for all languages, while canine-s outperforms canine-c. In this case, canine-s improves over the random baseline for all treebanks, while canine-c only outperforms the random baseline for 3 out of 10 models, which suggests the difficulties that these character-level language models have to model syntax, even if they perform well on other downstream tasks. The exceptions are Korean, German and Chinese. Chinese was also an exception in the case of dependency parsing, so an explanation might be that its writing systems encode more information per character than other languages. Chinese characters represent a whole morpheme, being more similar to a subword token, while Korean Hangul encodes a syllable per character, instead of a single sound as alphabets of the other languages tested.</p><p>Figure <ref type="figure" target="#fig_6">5</ref> shows the error reductions across the board, sorted by the size of the training data used for the probing. In this case, all tested languages are supported by the LLMs, but there are large differences in the size of the training data (e.g., Swedish with 5 000 sentences vs German with 40 472 sentences). However, we do not see an increase in error reduction when the size of training data grows.</p><p>Frozen (frz) vs fine-tuned (ftd) setup Table <ref type="table" target="#tab_5">6</ref> compares the bracketing F1-scores for the frozen   Span lengths Plotting the F1-score for each span length is the rough alternative to dependency displacements in the context of constituent parsing. In Figure <ref type="figure" target="#fig_7">6</ref> we again show specific examples for some of the studied languages: the most left-branching language (Korean), two balanced ones (Basque and Hungarian), and the most right-branching one (English). Similarly to the case of dependency parsers, the trends across models persist across different span lengths. They show that, regarding LLMs, mbert obtains the highest F-score for longer spans, while xlm-roberta shows great differences between shorter and longer spans. The canine models perform worse for all lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion</head><p>We now discuss the main insights and potential limitations of the proposed assessment framework.</p><p>Pretraining data versus Assessment data An interesting question that arises from multilingual recovery is whether the probe is able to recover the trees due to the size of the training data used for the assessment, although in theory it should be hard to learn by itself by an initially clueless classifier (the random baseline). The experiments show evidence that the size of the training data is not a primary factor to do multilingual, multi-formalism linear probing as sequence labeling. For constituent parsing, we observed that larger treebanks did not come with an increment in the error reductions between the frozen and the random setups, and that the control experiment can thus be used to give an estimate of the amount of structure recoverable from pretrained representations. Similarly, in the context of dependency parsing, we encountered an analogous situation. Despite the existence of treebanks for languages unsupported by the LLMs, spanning both large (Ancient Greek) and small treebanks (Skolt Sami or Bhojpuri), we observe that treebank size does not significantly impact the reduction in errors between the frozen and random setups.</p><p>Either with big or small data, the error reduction between the random and the frozen models is clearly lower than for the treebanks where the language is supported by the LLMs. Among richresource treebanks, the size of the data does not have a great influence on the error reductions between the random and frozen weights setups, suggesting that dataset size does not influence the estimates of the dependency structure that are recoverable from the representations.</p><p>Language model differences The results on the tested LLMs suggest that subword tokenization is necessary to represent syntax, in contrast with token-free models, even if these can later perform well on downstream tasks that require compositionality. Particularly, not only do subwordbased models outperform char-based ones, but also canine-s, which is trained using subword loss even though it is a char-level model, performs significantly better than canine-c. It is noteworthy that xlm-roberta generally outperforms mBERT in most downstream tasks, including parsing, as previous studies showed <ref type="bibr" target="#b8">(Conneau et al., 2020)</ref> and in our fine-tuned results, it performs on par on dependency parsing (Table <ref type="table" target="#tab_4">4</ref>) and outperforms mBERT in constituency parsing (Table <ref type="table" target="#tab_5">6</ref>). Yet, for the frozen weights setup, mBERT's representations recovered slightly but consistently better syntactic representations. This suggests that the improvements in how xlm-roberta was trained with respect to mBERT, e.g., training for longer time, or more data, are not key factors to better encode syntax. Additionally, based on our experiments, it appears that mBERT demonstrates a certain level of proficiency in recovering syntax information for the smallest treebanks, particularly for languages not included in the pretraining data (such as Ligurian, Bhojpuri, and Kiche). This suggests a capacity to extend its syntactic knowledge to previously unseen languages, albeit to a limited extent, unlike the other models.</p><p>Syntactic formalism Previous studies (e.g., <ref type="bibr" target="#b48">Vilares et al. (2020)</ref>), hypothesized that pre-trained word vectors might fit better constituent-than dependency-based tasks, since the masked language objective links better with the former formalism, i.e., when a model is learning to unblur a masked token, the constituent structure is to some extent implicit (e.g., an adjective is missing be-tween the determiner and the noun, forming a noun phrase), while dependencies are less obvious. We could not find a clear evidence of this. Although some of the frz models are unable to surpass the rnd baseline in the case of dependencies (while this is not the case for constituents), these instances are languages that are not present in the pretraining data, except for the canine models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a sequence-labeling framework to recover multi-formalism syntactic structures from multilingual LLMs. By mapping syntactic trees to labels we associated output word vectors to labels that encode a portion of the tree, while using a single assessment framework for both constituent and dependency structures. We compared three popular multilingual language models. The results show that subword LLMs can recover a percentage of these structures. We evaluated the outcomes by calculating the reduction in errors compared to control models, aiming to gauge the extent to which an LLM can recover specific syntactic structures. The assessment appears reliable and unaffected by variables like the training set's size employed for probing, highlighting that pretraining data is an important factor for recoverability. Last, we found no clear evidence that contextualized vectors encode constituent structures better than dependencies (nor the opposite).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Physical resources We did not consider larger language models as we do not have access to the necessary computational resources to run then, hence limiting the scope of our study. We only had access to 2 GeForce RTX 3090, having a total GPU memory of 48 GB, insufficient for fine-tuning many LLMs over different treebanks and formalisms, as in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language diversity</head><p>The constituent treebanks used are all from languages that are relatively richresource and are present on the pretraining data of the LLMs. To the best of our knowledge there are no available constituent treebanks from lowerresource languages that are also absent in multilingual LLMs. In consequence, we could not test the effect of absence of pretraining data in order to see if the trends obtained in dependency treebanks prevail here. In addition, for dependency parsing, even a large multilingual resource like Universal Dependencies only has data for about 100 languages, a tiny fraction of the 7 000 existing human languages.</p><p>Interpretation As mentioned in the introduction, we have to be careful when dealing with probing frameworks. Although we developed solid experiments, and also included control experiments, syntax knowledge is hard to isolate, measure and interpret, so we have tried to be careful with our conclusions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of a dependency tree linearization. Dependency types are omitted. For 2p b , the dot indicates no bracket in the first and/or second plane.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of a constituent tree linearization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: ϵ LAS (rnd,frz) on the dependency treebanks test sets for the 2p b encoding.</figDesc><graphic coords="6,313.23,344.41,204.10,171.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Average F1 score using 2p b for different dependency displacements (signed lengths) and LLMs. We removed displacements occurring less than 10 times.</figDesc><graphic coords="7,71.60,577.41,104.31,79.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>F-score for the test sets of the constituent treebanks. LLMs analyzed for the frz and rnd setups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: ϵ F 1 (rnd,frz) on the constituent test sets.</figDesc><graphic coords="8,77.95,239.12,204.10,125.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Average F1 score for different span lengths and LLMs.</figDesc><graphic coords="8,306.87,169.26,104.31,81.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>4 , whose languages are shown in Table2. Dependency treebanks used in this work.Language disparityWe use (mostly) different languages for each paradigm. For constituent treebanks, we only have access to rich-resource languages, so we prioritize diversity for dependencies. Comparing languages across syntax paradigms is not particularly useful, due to varying metrics, annotation complexity, and treebank comparisons. Instead, we compare error reductions against control models to estimate the recoverability of specific syntactic formalisms by an LLM (see §5).</figDesc><table><row><cell>Treebank</cell><cell>Family</cell><cell cols="2"># Trees # Tokens</cell><cell cols="2">Wiki-100 XLM-100 size (GB) size (GB)</cell></row><row><cell cols="2">Skolt SamiGiellagas Sami</cell><cell>200</cell><cell>2 461</cell><cell>-</cell><cell>-</cell></row><row><cell>GuajajaraTuDeT</cell><cell>Tupi-Guarani</cell><cell>284</cell><cell>2 052</cell><cell>-</cell><cell>-</cell></row><row><cell>LigurianGLT</cell><cell>Romance</cell><cell>316</cell><cell>6 928</cell><cell>-</cell><cell>-</cell></row><row><cell>BhojpuriBHTB</cell><cell>Indic</cell><cell>357</cell><cell>6 665</cell><cell>-</cell><cell>-</cell></row><row><cell>KicheIU</cell><cell>Mayan</cell><cell cols="2">1 435 10 013</cell><cell>-</cell><cell>-</cell></row><row><cell>WelshCCG</cell><cell>Celtic</cell><cell cols="2">2 111 41 208</cell><cell>&lt;0.1</cell><cell>0.8</cell></row><row><cell>ArmenianArmTDP</cell><cell>Armenian</cell><cell cols="3">2 502 52 630 0.2-0.4</cell><cell>5.5</cell></row><row><cell>VietnameseVTB</cell><cell>Viet-Muon)</cell><cell cols="3">3 000 43 754 0.4-0.7</cell><cell>137.3</cell></row><row><cell>ChineseGSDSimp</cell><cell>Sinitic</cell><cell cols="3">4 997 128 291 1.4-2.8</cell><cell>46.9</cell></row><row><cell>BasqueBDT</cell><cell>Basque</cell><cell cols="3">8 993 121 443 0.1-0.2</cell><cell>2.0</cell></row><row><cell>TurkishBOUN</cell><cell>Turkic</cell><cell cols="3">9 761 122 383 0.4-0.7</cell><cell>20.9</cell></row><row><cell>BulgarianBTB</cell><cell>Slavic</cell><cell cols="3">11 138 146 159 0.2-0.4</cell><cell>57.5</cell></row><row><cell cols="2">Ancient GreekPerseus Greek</cell><cell cols="2">13 919 202 989</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Constituent treebanks used in this work</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>LAS for the test sets of the dependency treebanks. LLMs and dependency encodings analyzed for the frz and rnd setups. Languages in italics are absent among the crawled data used to pre-train the LLMs.</figDesc><table><row><cell>3 9.3 8.0 9.2 8.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>LAS for the frz and ftd setups on the test sets, together with ϵ LAS (frz,ftd) for the 2pb b encoding for all treebanks and LLMs tested. Languages in italics are absent in the pretraining data of the LLMs.</figDesc><table><row><cell>10.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>F-score for the test sets of the constituent treebanks, LLMs analyzed for the ftd vs the frz setup.</figDesc><table><row><cell>Treebank</cell><cell>mBERT frz ftd err frz ftd err frz ftd err frz ftd err xlm-roberta canine-c canine-s</cell></row><row><cell cols="2">Swedish 56.0 79.4 53.2 42.3 79.9 65.2 22.7 29.6 8.9 28.7 47.8 65.2</cell></row><row><cell>Hebrew</cell><cell>74.5 75.4 3.5 59.9 76.2 40.6 29.8 36.4 9.4 40.1 53.8 22.9</cell></row><row><cell>Polish</cell><cell>77.0 93.4 70.9 68.0 94.0 81.2 33.9 56.6 34.3 42.9 73.9 54.3</cell></row><row><cell>Basque</cell><cell>56.6 85.0 65.4 47.1 85.1 71.6 33.0 49.1 24.0 41.4 62.7 36.2</cell></row><row><cell cols="2">Hungarian 69.8 91.5 71.9 66.0 92.1 76.8 31.5 52.7 30.9 41.0 65.8 42.0</cell></row><row><cell>French</cell><cell>50.1 82.2 64.3 32.4 82.6 74.3 12.1 70.2 66.1 20.1 76.0 70.0</cell></row><row><cell>Korean</cell><cell>57.4 86.4 68.1 53.2 88.0 74.4 37.5 66.0 45.6 41.9 71.4 50.8</cell></row><row><cell>English</cell><cell>57.2 91.9 81.1 40.5 92.8 87.9 9.6 82.4 80.5 17.5 86.6 83.8</cell></row><row><cell cols="2">German 45.4 87.3 76.7 41.1 88.4 80.3 18.5 71.7 65.3 24.0 77.3 70.1</cell></row><row><cell cols="2">Chinese 56.6 85.5 66.6 45.6 88.9 79.6 25.6 67.0 55.6 39.3 74.4 57.8</cell></row><row><cell cols="2">Average 60.1 85.8 62.2 49.6 86.8 73.2 25.4 58.2 42.1 33.7 69.0 55.3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>To ensure that the labels produce a valid tree, we apply the postprocessing described in the paper of each encoding.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>There are other head-selection encodings where the offset depends on some word property, e.g., PoS tags like in<ref type="bibr" target="#b48">(Vilares et al., 2020)</ref>, but using these encodings can blur the probing, since we need to access such external information.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>To our knowledge, when we did the experiments, this encoding (together with variants) was the only available family of sequence-labeling encodings for constituency parsing. Contemporaneously to the end of this work, another family of encodings -based on the tetra-tagging<ref type="bibr" target="#b18">(Kitaev and Klein, 2020)</ref> -has been proposed and implemented as a pure tagging approach<ref type="bibr" target="#b1">(Amini and Cotterell, 2022)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>We do not have the license for the Arabic treebank.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We acknowledge the <rs type="funder">European Research Council (ERC)</rs>, which has funded this research under the <rs type="programName">Horizon Europe research and innovation programme (SALSA</rs>, grant agreement No <rs type="grantNumber">101100615</rs>), <rs type="funder">ERDF/MICINN-AEI (SCANNER-UDC</rs>, <rs type="grantNumber">PID2020-113230RB-C21</rs>), <rs type="funder">Xunta de Galicia</rs> (<rs type="grantNumber">ED431C 2020/11</rs>), grant <rs type="grantNumber">FPI 2021</rs> (<rs type="grantNumber">PID2020-113230RB-C21</rs>) funded by <rs type="grantNumber">MCIN/AEI/10.13039/501100011033</rs>, and <rs type="funder">Centro de Investigación de Galicia "CITIC"</rs>, funded by the <rs type="funder">Xunta de Galicia</rs> through the collaboration agreement between the <rs type="institution">Consellería de Cultura, Educación</rs>, <rs type="funder">Formación Profesional e Universidades</rs> and the <rs type="institution">Galician universities</rs> for the reinforcement of the research centres of the Galician University System (CIGUS).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_hWF8qPS">
					<idno type="grant-number">101100615</idno>
					<orgName type="program" subtype="full">Horizon Europe research and innovation programme (SALSA</orgName>
				</org>
				<org type="funding" xml:id="_NeahWpX">
					<idno type="grant-number">PID2020-113230RB-C21</idno>
				</org>
				<org type="funding" xml:id="_uUtjyTg">
					<idno type="grant-number">ED431C 2020/11</idno>
				</org>
				<org type="funding" xml:id="_pfyRsFd">
					<idno type="grant-number">FPI 2021</idno>
				</org>
				<org type="funding" xml:id="_JxjzrMy">
					<idno type="grant-number">PID2020-113230RB-C21</idno>
				</org>
				<org type="funding" xml:id="_FCrNzNC">
					<idno type="grant-number">MCIN/AEI/10.13039/501100011033</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyperparameters</head><p>We selected a learning rate of 5 • 10 -5 for the ftd models and 2 • 10 -3 for the rnd and frz models based on the results of our preliminary experiments, as the ftd models showed faster convergence. For the three setups, we trained the models during 20 epochs (models had converged at this point). We trained our models on two GeForce RTX 3090 using a batch of 32 on each and a gradient accumulation of 2 for a total batch of 128. Training time of the final models accounts for approximately 60 GPU hours (24 for constituent, 6 per LLM, and 36 for dependency, 8 per LLM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 mBERT hyperparameters</head><p>Hyperparameter Value "attention_probs_dropout_prob" 0.1 "classifier_dropout" null "directionality" "bidi" "hidden_act" "gelu" "hidden_dropout_prob" 0.1 "hidden_size" 768 "layer_norm_eps" 1e-12 "max_position_embeddings" 512 "model_type" "bert" "num_attention_heads" 12 "num_hidden_layers" 12 "pad_token_id" 0 "pooler_fc_size" 768 "pooler_num_attention_heads" 12 "pooler_num_fc_layers" 3 "pooler_size_per_head" 128 "pooler_type" "first_token_transform" "position_embedding_type" "absolute" "torch_dtype" "float32" "transformers_version" "4.25.1" "type_vocab_size" 2 "use_cache" true "vocab_size" 119547</p><p>Table <ref type="table">7</ref>: Hyperparameters for mBERT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 xlm-roberta-base hyperparameters</head><p>Hyperparameter Value "attention_probs_dropout_prob" 0.1 "classifier_dropout" null "eos_token_id" 2 "hidden_act" "gelu" "hidden_dropout_prob" 0.1 "hidden_size" 768 "layer_norm_eps" 1e-05 "max_position_embeddings" 514 "model_type" "xlm-roberta" "num_attention_heads" 12 "num_hidden_layers" 12 "pad_token_id" 1 "position_embedding_type" "absolute" "torch_dtype" "float32" "transformers_version" "4.25.1" "type_vocab_size" 1 "use_cache" true "vocab_size" 250002</p><p>Table <ref type="table">8</ref>: Hyperparameters for xml-roberta models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 canine hyperparameters</head><p>Hyperparameter Value "attention_probs_dropout_prob" 0.1 "bos_token_id" 57344 "downsampling_rate" 4 "eos_token_id" 57345 "hidden_act" "gelu" "hidden_dropout_prob" 0.1 "hidden_size" 768 "layer_norm_eps" 1e-12 "local_transformer_stride" 128 "max_position_embeddings" 16384 "model_type" "canine" "num_attention_heads" 12 "num_hash_buckets" 16384 "num_hash_functions" 8 "num_hidden_layers" 12 "pad_token_id" 0 "torch_dtype" "float32" "transformers_version" "4.25.1" "type_vocab_size" 16 "upsampling_kernel_size" 4 "use_cache" true Table <ref type="table">9</ref>: Hyperparameters for canine-c and -s models.</p><p>B Error reduction for r h and ah tb  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Evaluation scripts</head><p>We used the evaluation scripts conll18_eval. for dependencies and EVALB for constituencies.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Give your text representation models some love: the case for Basque</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Agerri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">San</forename><surname>Iñaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">Ander</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ander</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xabier</forename><surname>Barrena</surname></persName>
		</author>
		<author>
			<persName><surname>Saralegi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Language Resources and Evaluation Conference</title>
		<meeting>the Twelfth Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4781" to="4788" />
		</imprint>
	</monogr>
	<note>Aitor Soroa, and Eneko Agirre</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On parsing as tagging</title>
		<author>
			<persName><forename type="first">Afra</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.emnlp-main.607</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="8884" to="8900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MaltOptimizer: A system for MaltParser optimization</title>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA)</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2757" to="2763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Probing classifiers: Promises, shortcomings, and advances</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli_a_00422</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="207" to="219" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Statistical parsing with a context-free grammar and word statistics</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI/IAAI</title>
		<imprint>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="1997">1997. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1082</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Canine: Pre-training an efficient tokenization-free encoder for language representation</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iulia</forename><surname>Turc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00448</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="73" to="91" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A new statistical parser based on bigram lexical dependencies</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collins</forename></persName>
		</author>
		<idno type="DOI">10.3115/981863.981888</idno>
	</analytic>
	<monogr>
		<title level="m">34th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Santa Cruz, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="184" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stanford&apos;s graph-based neural dependency parser at the CoNLL 2017 shared task</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K17-3002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="20" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A unifying theory of transition-based and sequence labeling parsing</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Gómez-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michalina</forename><surname>Strzyz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Vilares</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.336</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3776" to="3793" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Constituent parsing as sequence labeling</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Rodríguez</forename></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Vilares</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Designing and interpreting probes with control tasks</title>
		<author>
			<persName><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1275</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2733" to="2743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A structural probe for finding syntax in word representations</title>
		<author>
			<persName><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1419</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4129" to="4138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simple and accurate dependency parsing using bidirectional LSTM feature representations</title>
		<author>
			<persName><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00101</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="313" to="327" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Constituency parsing with a self-attentive encoder</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1249</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2676" to="2686" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tetra-tagging: Word-synchronous parsing with linear-time inference</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.557</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6255" to="6261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic programming algorithms for transition-based dependency parsers</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Kuhlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Gómez-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="673" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Schrödinger&apos;s tree -on syntax and neural language models</title>
		<author>
			<persName><forename type="first">Artur</forename><surname>Kulmizev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">796788</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Do neural language models show preferences for syntactic formalisms?</title>
		<author>
			<persName><forename type="first">Artur</forename><surname>Kulmizev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinit</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Abdou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.375</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4077" to="4091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Seq2seq dependency parsing</title>
		<author>
			<persName><forename type="first">Zuchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shexia</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3203" to="3214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Descending-path convolution kernel for syntactic structures</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Kho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Dligach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guergana</forename><surname>Savova</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-2014</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="81" to="86" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Linguistic knowledge and transferability of contextual representations</title>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1112</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Turbo parsers: Dependency parsing by approximate variational inference</title>
		<author>
			<persName><forename type="first">André</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mário</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="34" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Do syntactic probes probe syntax? experiments with jabberwocky probing</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maudslay</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.11</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="124" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Advances in neural information processing systems, 26. Victor Milewski, Miryam de Lhoneux, and Marie-Francine Moens. 2022. Finding structural knowledge in multimodal-BERT</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.388</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5658" to="5671" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rob van der Goot, and Barbara Plank. 2022a. Probing for labeled dependency trees</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Müller-Eberstein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.532</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7711" to="7726" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rob van der Goot, and Barbara Plank. 2022b. Sort by structure: Language model ranking as dependency probing</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Müller-Eberstein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.93</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Seattle, United States. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="1296" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Not all linearizations are equally datahungry in sequence labeling parsing</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Muñoz-Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michalina</forename><surname>Strzyz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Vilares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)</title>
		<meeting>the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)</meeting>
		<imprint>
			<publisher>Held Online. INCOMA Ltd</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="978" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">StereoSet: Measuring stereotypical bias in pretrained language models</title>
		<author>
			<persName><forename type="first">Moin</forename><surname>Nadeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Bethke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.416</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5356" to="5371" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Algorithms for deterministic incremental dependency parsing</title>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli.07-056-R1-07-027</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="513" to="553" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Universal Dependencies v2: An evergrowing multilingual treebank collection</title>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Language Resources and Evaluation Conference</title>
		<meeting>the Twelfth Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4034" to="4043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improved inference for unlexicalized parsing</title>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies 2007: The Conference of the North American Chapter</title>
		<meeting><address><addrLine>Rochester, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
	<note>Proceedings of the Main Conference</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">E-BERT: Efficient-yet-effective entity embeddings for BERT</title>
		<author>
			<persName><forename type="first">Nina</forename><surname>Poerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulli</forename><surname>Waltinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.71</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Square one bias in NLP: Towards a multidimensional exploration of the research manifold</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-acl.184</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2340" to="2354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Introducing the SPMRL 2014 shared task on parsing morphologically-rich languages</title>
		<author>
			<persName><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages</title>
		<meeting>the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Dublin City University</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="103" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fast(er) exact decoding and global training for transition-based dependency parsing via a minimal feature set</title>
		<author>
			<persName><forename type="first">Tianze</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="12" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dependency parsing as a sequence labeling task</title>
		<author>
			<persName><forename type="first">Drahomíra</forename><surname>Spoustová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Spousta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="7" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Same neurons, different languages: Probing morphosyntax in multilingual pre-trained models</title>
		<author>
			<persName><forename type="first">Karolina</forename><surname>Stanczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edoardo</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Torroba Hennigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.114</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1589" to="1598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Viable dependency parsing as sequence labeling</title>
		<author>
			<persName><forename type="first">Michalina</forename><surname>Strzyz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Vilares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Gómez-Rodríguez</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1077</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="717" to="723" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bracketing encodings for 2-planar dependency parsing</title>
		<author>
			<persName><forename type="first">Michalina</forename><surname>Strzyz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Vilares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Gómez-Rodríguez</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.223</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2472" to="2484" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Najoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName><surname>Das</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06316</idno>
		<title level="m">What do you learn from context? probing for sentence structure in contextualized word representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Parsing as tagging</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Vacareanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Caique</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gouveia</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><forename type="middle">A</forename><surname>Valenzuela-Escárcega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Language Resources and Evaluation Conference</title>
		<meeting>the Twelfth Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5225" to="5231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Parsing as pretraining</title>
		<author>
			<persName><forename type="first">David</forename><surname>Vilares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michalina</forename><surname>Strzyz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Gómez-Rodríguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9114" to="9121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Antti</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenna</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Ilo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jouni</forename><surname>Luoma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhani</forename><surname>Luotolahti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tapio</forename><surname>Salakoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.07076</idno>
		<title level="m">Multilingual is not enough: Bert for finnish</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Are all languages created equal in multilingual BERT?</title>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.repl4nlp-1.16</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Representation Learning for NLP</title>
		<meeting>the 5th Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The penn chinese treebank: Phrase structure annotation of a large corpus</title>
		<author>
			<persName><forename type="first">Naiwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural language engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Generic axiomatization of families of noncrossing graphs in dependency parsing</title>
		<author>
			<persName><forename type="first">Anssi</forename><surname>Yli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Jyrä</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Gómez-Rodríguez</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1160</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1745" to="1755" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Tree kernel-based SVM with structured syntactic knowledge for BTGbased phrase reordering</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="698" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="188" to="193" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
