<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Human-Like Distractor Response in Vision-Language Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaonan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cologne</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haoshuo</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nokia Bell Labs</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Human-Like Distractor Response in Vision-Language Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C41E3D03BC93C32C3F979619AB0D76FE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T14:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous studies exploring the human-like capabilities of machine-learning models have primarily focused on pure language models. Limited attention has been given to investigating whether models exhibit human-like behavior when performing tasks that require the integration of visual and language information. In this study, we investigate the impact of tags of semantic, phonological, and bilingual features on the visual question-answering task performance of an unsupervised model. Our findings reveal its similarities with the influence of distractors in the picture-naming task (known as the picture-word-interference paradigm) observed in human experiments: 1) Semanticallyrelated tags have a more negative effect on task performance compared to unrelated tags, indicating a more robust competition between visual and tag information which are semantically closer to each other when generating an answer. 2) Even presenting a partial section (wordpiece) of the originally detected tag significantly improves task performance, with the portion that plays a lesser role in determining the overall meaning of the original tag leading to a more pronounced improvement. 3) Tags in two languages that refer to the same meaning exhibit a symmetrical-like effect on performance in balanced bilingual models. Datasets and code of this project are released at https: //github.com/NLPbelllabs/PWI</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning models possess a broad range of reasoning abilities like humans. Comparing these models to human capabilities can aid in understanding the decision-making process underlying their predictions and ultimately enhance their accuracy. Numerous studies exploring human-like behavior in models primarily focus on language processing alone, revealing that, even though these models may encounter difficulties in certain specialized areas of language, they can attain significant human-like capabilities across diverse linguistic domains <ref type="bibr" target="#b12">(Ettinger, 2020;</ref><ref type="bibr" target="#b33">Rogers et al., 2021)</ref>. In this study, we expand the scope of the investigation to the field of vision and language, an area that, to the best of our knowledge, has been relatively unexplored from the perspective of the language community <ref type="bibr" target="#b11">(Dobreva and Keller, 2021;</ref><ref type="bibr">Cao et al., 2020a)</ref>.</p><p>In cognitive psychological research on human language production, the paradigm of picture-word interference (PWI) plays a crucial role in understanding how humans access the appropriate words in their mental processes <ref type="bibr" target="#b4">(BÃ¼rki et al., 2020;</ref><ref type="bibr" target="#b39">van Maanen et al., 2009;</ref><ref type="bibr" target="#b30">Lupker, 1979)</ref>. In a typical PWI task, participants are presented with a picture and asked to name the object (target word, e.g., dog) depicted in the picture accurately and quickly. Concurrently, a linguistically related distractor word (e.g. cat is semantically related) is presented superimposed on the picture. Compared to an unrelated distractor (e.g. cap), a related distractor can either interfere with or facilitate the process by which humans produce the correct response. The PWI is employed to simulate the cognitive process of selecting the most appropriate representation of a word (target) from multiple possibilities stored in long-term memory.</p><p>This selection process involved in PWI can be likened to a visual question answering (VQA) task, where a single answer is assigned the highest probability of being the correct response among a set of choices. To achieve successful performance in the task, a vision-language pre-training (VLP) model relies on the inclusion of additional tags, which are accurately detected by an object detector. These tags enhance the visual information extracted from the image, which is particularly important for the unsupervised model which is pre-trained with unaligned text and image corpora <ref type="bibr" target="#b27">(Li et al., 2020)</ref>. Compared to the baseline cases where no tags are included, the inclusion of tags that possess certain features may enhance task performance, similar to the facilitation effect in the PWI paradigm. Conversely, the presence of tags with some other features may negatively impact task performance, akin to the interference effect in the PWI paradigm.</p><p>From this perspective, we examine how tags with different features affect the VQA task performance of unsupervised VisualBERT model <ref type="bibr" target="#b26">(Li et al., 2019</ref><ref type="bibr" target="#b27">(Li et al., , 2020))</ref>. Our findings reveal similarities between the effects of tags and the influence of distractors observed in PWI experiments with human participants: A) Semantically-related tags have a more pronounced negative impact on task performance compared to unrelated tags. B) A partial section (wordpiece) of the detected tags improves task performance. C) Tags in two languages that refer to the same meaning exhibit a symmetrical-like effect on a balanced bilingual model which is fine-tuned in both languages. Additionally, our results indicate that: D) When visual and tag information are semantically closer, there is a heightened competition between them to be chosen as the final answer. E) The portion of the tag that has a lesser role in determining its overall meaning contributes to a more substantial improvement in task performance.</p><p>We outline three main contributions of our study: Firstly, we extend the scope of investigation on human-like intelligence beyond pure language models to include models that integrate visual and language information. Secondly, our findings indicate that the model's performance demonstrates some degree of similarity to human cognitive abilities under various distractor conditions. Thirdly, our study highlights the impact of tag quality on the effectiveness of the model and underscores the need for careful attention to this aspect during the model design phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Object Tag Previous studies have demonstrated that incorporating object tags can improve performance in various vision-language tasks, including VQA <ref type="bibr" target="#b13">(Fang et al., 2021;</ref><ref type="bibr" target="#b8">Cho et al., 2021;</ref><ref type="bibr">Zhang et al., 2021b;</ref><ref type="bibr" target="#b42">Wang et al., 2020)</ref>, image captioning <ref type="bibr" target="#b18">(Hu et al., 2021;</ref><ref type="bibr">Zhang et al., 2021a;</ref><ref type="bibr" target="#b19">Hu et al., 2020)</ref> and visual commonsense reasoning <ref type="bibr" target="#b28">(Lin et al., 2019)</ref>. However, there is still a lack of comprehensive understanding regarding the influence of tags on the task. In this study, we explore the impact of tags with different features on the performance of unsupervised VisualBERT.</p><p>Human-level Intelligence Numerous studies have been conducted to examine the linguistic capabilities of pre-trained transformer-based language models, exploring their resemblance to human abilities in various aspects such as syntactic knowledge <ref type="bibr" target="#b29">(Linzen et al., 2016;</ref><ref type="bibr" target="#b15">Gulordava et al., 2018)</ref>, semantic knowledge <ref type="bibr" target="#b12">(Ettinger, 2020;</ref><ref type="bibr" target="#b23">Kementchedjhieva et al., 2021;</ref><ref type="bibr" target="#b32">Misra et al., 2020)</ref>, and the integration of semantic and syntactic information <ref type="bibr" target="#b44">(Xu and Chen, 2022)</ref>. However, there is still limited knowledge about the linguistic capabilities of VLP models in relation to human behavior <ref type="bibr" target="#b11">(Dobreva and Keller, 2021;</ref><ref type="bibr">Cao et al., 2020a)</ref>. Our study seeks to expand the current understanding by investigating whether the impact of tags with various features on the VQA task is comparable to the effect of distractors in the PWI task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PWI and Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Semantic Relatedness</head><p>Inspiration PWI research has suggested that a distractor that shares a closer semantic relationship with the target word tends to have a more substantial interference effect on lexical selection. For instance, many studies <ref type="bibr" target="#b41">(Vigliocco et al., 2004;</ref><ref type="bibr" target="#b40">Vieth et al., 2014;</ref><ref type="bibr" target="#b2">Aristei and Rahman, 2013;</ref><ref type="bibr" target="#b34">Rose et al., 2019;</ref><ref type="bibr">cf. Hutson and Damian, 2014)</ref> demonstrate that the time taken to name a target word (e.g., dog) is longer when presented with a distractor from the same category (e.g. cat), compared to an unrelated distractor (e.g. cap).</p><p>The interference effect caused by a semanticallyrelated distractor can also be observed in pretrained language models. <ref type="bibr" target="#b32">Misra et al. (2020)</ref> investigated BERT's <ref type="bibr" target="#b10">(Devlin et al., 2018)</ref> sensitivity to lexical cues and observed that when a target word is masked within a sentence, e.g., bacon in pork/meteorite. she cooked up some eggs, [MASK], and toast, the probability of [MASK] being predicted as bacon was lower when a semantically related prime word pork was present compared to an unrelated word meteorite. This finding indicates that a semantically related prime word acts as a negative distractor, causing interference effect in certain situations (see also <ref type="bibr" target="#b22">Kassner and SchÃ¼tze, 2019)</ref>.</p><p>Prediction Building upon these findings, we hypothesize that the presence of semantically-related tags, i.e., from the same semantic category, leads to a more pronounced negative effect on VQA task performance compared to unrelated ones. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Phonological Relatedness</head><p>Inspiration In contrast to the interference effect from semantically-related distractors, phonologically-related ones contribute positively in the PWI task performance (Meyer and <ref type="bibr" target="#b31">Schriefers, 1991;</ref><ref type="bibr" target="#b35">Schriefers, 1999;</ref><ref type="bibr" target="#b3">Ayora et al., 2011)</ref>. For example, distractors sharing either the first syllable (e.g., ha-vik) or the second syllable (e.g., zo-mer) to disyllabic target words (e.g., hamer in Dutch) show a facilitation effect compared to unrelated distractors (Meyer and <ref type="bibr" target="#b31">Schriefers, 1991)</ref>. Moreover, the facilitation effect is stronger when the first syllable is shared compared to the second one<ref type="foot" target="#foot_0">1</ref> (Meyer and <ref type="bibr" target="#b31">Schriefers, 1991;</ref><ref type="bibr" target="#b35">Schriefers, 1999)</ref>.</p><p>Prediction We use the originally detected tags consisting of two wordpieces and substitute these tags with a single wordpiece. The replaced tag (a single wordpiece) shares one wordpiece with the original tag, which is in line with the methodology employed in PWI research. We predict that a) both the first and second wordpieces will yield a positive effect on the VQA task in comparison to the cases in which no tags are present, and b) the positive effect will be more prominent with the first wordpiece compared to the second one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Bilingual Relatedness</head><p>Inspiration PWI research has suggested that the effect size of distractors varies to different degrees between balanced bilinguals, who are highly proficient in two languages, and dominant bilinguals, who exhibit higher proficiency in one language compared to the other. For example, for balanced bilinguals, no significant difference in effect size is observed concerning the languages in which the distractors are presented <ref type="bibr" target="#b9">(Costa et al., 1999;</ref><ref type="bibr" target="#b16">Guo and Peng, 2006)</ref>. This indicates a symmetrical pattern for balanced bilinguals, i.e., the effect size is consistent regardless of the language used for the distractors. Similarly, research on multilingual language models also suggests that they are capable of aligning word meaning across languages <ref type="bibr">(Cao et al., 2020b;</ref><ref type="bibr" target="#b21">K et al., 2019;</ref><ref type="bibr" target="#b43">Wang et al., 2019;</ref><ref type="bibr" target="#b36">Schuster et al., 2019)</ref>. However, such a symmetrical-like pattern is not found for dominant bilinguals. For example, different degrees of effect size were observed between semantically-related distractors in two languages (e.g., valley in English vs. dal meaning 'valley' in Dutch) when dominant bilinguals naming pictures in their second language with lower proficiency (e.g., mountain in English) <ref type="bibr" target="#b17">(Hermans et al., 1998;</ref><ref type="bibr" target="#b0">Altarriba and Mathis, 1997)</ref>.</p><p>Prediction We predict a symmetrical-like effect, i.e., the replaced tags in two languages referring to the same meaning will result in highly similar task performance for a balanced model that is fine-tuned in two languages. This pattern, however, is not expected for a dominant model that is exclusively fine-tuned in a single language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">VQA Model</head><p>Semantic and Phonological Relatedness Following Li et al. ( <ref type="formula">2020</ref>), the monolingual English VQA model VQA P W I is pre-trained using "mask-andpredict" objective with unaligned data: shuffled text segments (Conceptual Captions <ref type="bibr" target="#b37">(Sharma et al., 2018)</ref> and BookCorpus<ref type="foot" target="#foot_1">2</ref> ) and images (Microsoft COCO <ref type="bibr" target="#b7">(Chen et al., 2015)</ref>). We use the Adam optimizer (Kingma and Ba, 2014) with a lineardecayed learning-rate schedule and pre-train the model for 10 epochs with a batch size of 144. In each batch, part of the text or part of the image regions is masked and the model is trained to predict the masked words or the image regions. We use the image region features and associated tags from LXMERT <ref type="bibr" target="#b38">(Tan and Bansal, 2019)</ref>, which are extracted and detected using Faster R-CNN <ref type="bibr" target="#b1">(Anderson et al., 2018)</ref>. These tags are appended as words to the visual input and the mask-and-predict objective is also applied to the tags.</p><p>Fine-tuning for the VQA downstream task <ref type="bibr" target="#b14">(Goyal et al., 2017)</ref> is conducted using training questions that are related to the images taken from Microsoft COCO <ref type="bibr" target="#b7">(Chen et al., 2015)</ref>. To avoid intervention from factors such as unrelated image features, we only retained the region features that correspond to the correct answers as the visual input. The remaining region features were masked out by setting the unrelated feature vectors to zero. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the input is composed of a question, an image feature that is detected as the pizza object, and the correctly detected tag pizza. To enhance the model's understanding, the replaced tags are the correct answers from other instances.</p><p>Bilingual Relatedness We include the German Wikipedia text corpus<ref type="foot" target="#foot_2">3</ref> in the pre-training of all three bilingual VQA models: VQA EN &amp;DE , VQA EN , and VQA DE . In order to align the English and German text embeddings with the visual representations, we use the googletrans<ref type="foot" target="#foot_3">4</ref> tool to translate the object tags of 50% of the images into German and a multilingual tokenizer<ref type="foot" target="#foot_4">5</ref> to support both languages during pre-training. During the fine-tuning phase for the VQA task, VQA EN only uses the original tags in English detected in the images, while VQA DE only utilizes the translated tags in German. For VQA EN &amp;DE , tags applied during fine-tuning are either in English or German for each image, which is expected to provide a better alignment between both languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Instance Selection</head><p>To maximize the effect of replaced tags and simplify the result analysis, we collect specific instances from the training dataset that meet the following two criteria: 1) the model outputs the detected tag with the highest probability as the final answer, 2) at least one region feature from the input image is detected as the answer object. Figure <ref type="figure" target="#fig_0">1</ref> provides an example instance, where the question What is this? is asked about an image COCO_train2014_000000074253<ref type="foot" target="#foot_5">6</ref> that contains a pizza object and the word pizza is detected as a tag. The VQA Annotations 6 correctly label pizza as the answer, and it is also identified as the output with the highest probability. The probability of pizza may decrease and an incorrect response might be generated as the final answer if no tag is present or a different tag such as dog is given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Measures</head><p>Accuracy There exists a number of valid instances that identify one tag</p><formula xml:id="formula_0">t i o in N original tags t 0 o , ..., t N o</formula><p>as their correct answer. We replace each original tag t i o with N different new tags t 0 r , ..., t N r . A set V i of instances that have t i o as their correct answer is collected with a number of m i in total. The function C V QA () is used to count the number of instances where t i o remains selected as the correct answer after it is replaced by t i r . Note that t i o is the correct answer for all instances in V i and the accuracy value with the original tags is 100%. This value will decrease with replaced tags or without any tags. A lower accuracy value indicates a greater impact of tag replacement on image-text alignment.</p><formula xml:id="formula_1">Accuracy = N i=1 C V QA (t i r , V i )/ N i=1 m i (1)</formula><p>F-score and Similarity We use F-score to further examine the degree of change in the probability of a correct answer caused by a replaced tag for each instance in the experiment on semanticand bilingual-relatedness. For each valid instance v, where the original tag t o is the labeled correct answer, we define surprisal S as the negative logarithm of the probability of the model outputting the correct answer (V QA ans = t o ). The surprisal So(v) (Eq. 2) is computed when the original tag t o is used as the answer. When a replaced tag t r is present, the corresponding surprisal Sr(v) is calculated using Eq. 3. The F-score (Eq. 4) measures the difference between Sr(v) and So(v), indicating the extent of the impact of t r compared to t o on the probability of a correct answer. If the replacement of t o with t r leads to a greater decrease in the probability, then the F-score increases 7 .</p><formula xml:id="formula_2">S o (v) = -log e (prob(V QA ans = t o |(v, t o ))) (2) S r (v) = -log e (prob(V QA ans = t o |(v, t r )))<label>(3)</label></formula><formula xml:id="formula_3">F(v) = S r (v) -S o (v)<label>(4)</label></formula><p>The F-scores are analyzed in relation to the semantic distance between the replaced tags and the original ones. The vector similarity Sim between t o and t r is calculated using cosine similarity cos(). We define two types of similarities, word (noncontextual) similarity Sim word and contextual similarity Sim contextual , as:</p><formula xml:id="formula_4">Sim word (v) = cos(e o (t o ), e o (t r ))</formula><p>(5)</p><formula xml:id="formula_5">Sim contextual (v) = cos(e 1 (t o , v), e 1 (t r , v)) (6)</formula><p>The function e o () in Eq. 5 returns the token embeddings of t o and t r at the 0 th layer of the model, while Eq. 6 uses e 1 () to generate contextualized embeddings of t o and t r from the last (12 th ) hidden layer for instance v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>A comprehensive list of tags used in the three experiments can be found in Table <ref type="table">4</ref> in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Semantic Relatedness</head><p>Setup To investigate whether the effect of semantically-related features on task performance can apply to various categories, we conduct the 7 The concept of surprisal and F-score have similarities with that defined in <ref type="bibr" target="#b32">Misra et al. (2020)</ref>, where surprisal is based on the probability of a masked token instead. experiment using two word lists representing animals and food, respectively. The VQA performance is tested using different types of tags, including originally detected tags (T orig ), semanticallyrelated tags from the same category (T same ), unrelated tags from a different category (T diff ) (using food words for animal images and vice versa), and without any tags (T none ). Examples of the types are shown in Table <ref type="table" target="#tab_0">1</ref>. Two additional types are introduced to expand the spectrum of semantic similarity alongside the existing tags: tags of hypernyms (T hype ) <ref type="bibr" target="#b25">(Kuipers et al., 2006)</ref> (using food for the list food and animal for the list animal) and pseudowords (T pseu ) which are made up of random letters and have no meaning in English. To avoid any influence from wordpiece segmentation, the tags cannot be divided into subwords within the word embedding.</p><p>Result The task accuracy presented in Table <ref type="table" target="#tab_0">1</ref>(left) shows the same trend for both lists: T orig &gt; T none &gt; T diff &gt; T same . This finding strongly supports the prediction that the tags from the same semantic category negatively impact task performance more significantly compared to unrelated ones.</p><p>To determine if the interference effect is linked to the semantic similarity between the original and replaced ones, we plot the F-score against Sim word /Sim contextual for all instances with the tags T hype , T pseu , T diff , and T same . Figure <ref type="figure" target="#fig_1">2a</ref> illustrates that the tags with lower Sim word /Sim contextual tend to have a more scattered distribution towards higher F-scores for the list animal. A consistent tendency can be found for the list of food in Figure <ref type="figure" target="#fig_4">4</ref> in the Appendix. We apply a Gaussian distribution to fit the Sim for each F-score and plot the mean of these fits against the F-scores for the list food and the list animal in Figure <ref type="figure" target="#fig_1">2b</ref>. Importantly, we evaluate the statistical significance of the relationship between F-scores and Sim word /Sim contextual , and found significant coefficients in a linear mixed model with Sim as an explanatory variable and the F-score as a dependent variable, as shown in Table <ref type="table" target="#tab_0">1</ref>(right). The statistical findings and the evident negative correlation between the F-scores and Sim contextual in Figure <ref type="figure" target="#fig_1">2b</ref>, collectively indicate that semantically closer tags result in a less pronounced decrease in the probability correct answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Phonological Relatedness</head><p>Setup To investigate whether the impact of phonologically-related features on VQA task performance can be applied to various word categories, we find instances with tags of three different types of word compounds: open compound words (C open ), which are composed of two words written separately with a space like tennis player, closed compound words (C closed ), which are composed of two words written together as a single word like bathtub, and non-compound words (C non ), which can not be divided into two words, such as buoy. We expect the predicted positive effect to be consistent for all three types of words. All the words can be split into two wordpieces using the tokenizer from the BERT base model (uncased) 8 . The first and second parts are labeled as T 1st and T 2nd , respectively. Table <ref type="table" target="#tab_1">2</ref> shows examples of these tags and T none refers to the cases where no tags are used.</p><p>Result Table <ref type="table" target="#tab_1">2</ref> demonstrates that the task accuracy for the three types of words consistently follows this order: T 1st &gt; T 2nd &gt; T none . This result supports the prediction that both wordpieces as tags contribute to improved task performance compared to the cases without tags, and that the first wordpiece has a stronger effect than the second wordpiece.</p><p>Interestingly, the result reveals a consistent trend in the accuracy values of T 1st and T 2nd : both exhibit the order of C non &gt; C closed &gt; C open . In contrast to the groups C closed and C open , T 1st /T 2nd in the group C non are wordpieces that do not have independent meaning. Their higher accuracy (96.3 for T 1st and 95.9 for T 2nd ) compared to the T 1st and T 2nd for both C closed <ref type="bibr">(91.2 and 88.4, respectively)</ref> and C open (89.9 and 82.7, respectively) suggests the need to consider linguistic factors in the observed trend, which will be discussed in the Discussion section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Bilingual Relatedness</head><p>Setup The dominant bilingual models VQA EN and VQA DE are fine-tuned with tags in English and German, respectively. Their performance is tested with original tags in the corresponding language (T EN for VQA EN and T DE for VQA DE ), and with replaced tags translated into the other language (T EN for VQA DE and T DE for VQA EN ). For the balanced bilingual model VQA EN &amp;DE which is fine-tuned with an equal distribution of 50% En-   glish tags and 50% German tags, all the original tags are translated into the other language. We additionally add the types T diff-DE and T diff-EN , which present the tags in a different category in German and in English, respectively. The type T none refers to the cases without tags. Table <ref type="table" target="#tab_3">3</ref> showcases one example for each type.</p><p>Result The results in Table <ref type="table" target="#tab_3">3</ref> show that the tags referring to the same meaning as the original tags (T EN /T DE ) achieve superior task performance compared to unrelated tags (T diff-EN /T diff-DE ) and cases without tags (T none ). Importantly, VQA EN &amp;DE shows smaller accuracy differences between a) T EN and T DE (3.4) and b) T diff-EN and T diff-DE (0.6) than that for VQA EN (22.7 and 5, respectively) and VQA DE (9.7 and 1.1, respectively). This supports the prediction that tags in two languages referring to the same meaning result in similar task performance for the balanced model compared to the dominant model. This symmetricallike effect is also supported by the 2-D histogram plot of the corresponding F-scores of T diff-DE and T diff-EN , see Figure <ref type="figure" target="#fig_3">3</ref>(a, top) where balanced model VQA EN &amp;DE shows a clear diagonal distribution. Similarly to the result in the experiment on semantic relatedness, the significant negative coefficients of Sim word /Sim contextual against F-scores in Table 3(right) and the clear negative correlation between F-score and Sim contextual in Figure <ref type="figure" target="#fig_3">3(b)</ref> further reinforce the suggestion that the semantically closer tags result in a smaller decrease in the probability of correct answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Our study reveals that the influence of the tested tags on VQA task performance parallels the effects of distractors observed in PWI experiments on human participants. We will discuss the experimental results from the following perspectives.</p><p>Competition between semantically closer visual and tag information hinders performance. On the one hand, similar to the human performance in the PWI task, the experimental results on se- mantic relatedness (Table <ref type="table" target="#tab_0">1</ref>(left)) demonstrate that replacing tags within the same category leads to a lower accuracy compared to the unrelated tags of a different category. On the other hand, both the experiment on semantic (Table <ref type="table" target="#tab_0">1</ref>(right)) and bilingual relatedness (Table <ref type="table" target="#tab_3">3</ref>(right)) reveal a negative correlation between semantic similarity (Sim) and F-scores, suggesting that semantically closer tags cause a smaller change in the probability of correct answers. The two metrics, despite their differences, are not contradictory. In the PWI paradigm, the processing of an image activates the representation of the target and related concepts <ref type="bibr" target="#b4">(BÃ¼rki et al., 2020)</ref>. Likewise, the results indicate that the image feature in the VQA task activates information about the correct answer and closely related objects. These related objects are more likely to be chosen as the final answer compared to unrelated ones. When a semantically-related tag is present, it tends to increase the likelihood of the model choosing itself as the final, but incorrect answer, thus leading to reduced accuracy. In this case, the probability of the correct answer remains high due to the closer similarity between visual and tag information, resulting in a low F-score. In contrast, an unrelated tag does not receive significant activation from the image feature, making it less likely to be chosen as the final answer, even when provided as a tag.</p><p>As the visual and tag information diverges from each other, the probability of the correct answer is expected to be lower, resulting in a higher F-score, compared to the cases in which the visual and tag information align closely.</p><p>Wordpiece partially representing visual objects enhances performance. In PWI experiments, sharing syllables between distractors and target words facilitates the picture-naming process. Consistent with this finding, the experiment on phonological relatedness (Table <ref type="table" target="#tab_1">2</ref>) shows that task performance is significantly improved when either T 1st or T 2nd are included as input. These results suggest that presenting a single wordpiece, regardless of its semantic meaning, aids the model in selecting the final correct answer, subsequently enhancing task performance.</p><p>Wordpiece determining overall word meaning may also compete with visual information. We observed that the first wordpieces tend to achieve better performance compared to the second wordpieces (Table <ref type="table" target="#tab_1">2</ref>). We attribute this to the greater weight placed on the second wordpieces in determining the overall word meaning, particularly evident in the cases of C open like tennis player and C closed like bathtub. Typically, the first wordpiece serves as a modifier or specifier, while the second wordpiece carries the central meaning of the words, representing the main object being referred to. For example, the first wordpiece tennis acts as a modifier in the compound tennis player, while the second wordpiece player represents the main object. The second wordpieces are more closely associated with the original tags, which can potentially lead to competition between visual and tag information (as discussed in the previous part) and result in worse task performance.</p><p>Compounding ability of wordpiece affects task performance. The result of the experiment on phonological relatedness shows a specific order of accuracy values: C non &gt; C closed &gt; C open for both T 1st and T 2nd . This pattern can be attributed to the varying degrees of compounding ability between three word groups. In the case of C non , the wordpieces have limited possibilities to form meaningful words, as exemplified by buo in buoy. The association between the image feature representing a buoy and the wordpiece buo significantly narrows down the available options for the model, strongly indicating the correct answer as buoy. In contrast, a single wordpiece or lexeme in C open offers more opportunities to create related words, such as tennis in tennis player can be a standalone word, or form words like tennis racket, tennis coach, tennis ball, etc. This offers the model a broader range of options to choose from, considering the image feature representing a tennis player and the tag tennis. Thus, the model is likely to make more accurate predictions when the answer options are limited (C non ) compared to when there is a wider range of options (C closed and C open ), leading to the observed performance order of C non &gt; C closed &gt; C open for both T 1st and T 2nd .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Cognitive psychological research on PWI demonstrates that different distractors have varying effects on picture-naming tasks performed by human participants. We replace the tags detected in images with new words possessing semantic, phonological, and bilingual characteristics relative to the original tags, and examine their impact on the VQA task performance. Our findings indicate that the influence of these tags on task performance parallels the effects of distractors in PWI experiments on human participants.</p><p>Taking the task performance in cases where no tags are present as the baseline, we found that A) semantically-related tags have a greater negative impact on task performance compared to unrelated ones, suggesting that when visual and tag information is semantically closer to each other, they compete more strongly to be selected as the final answer. B) Presenting even a portion (wordpiece) of the original tag improves task performance significantly. The portion that plays a lesser role in determining its original tag's overall meaning leads to a more significant improvement. C) Tags in two languages referring to the same meaning exhibit a symmetrical-like effect on performance in balanced bilingual models which is fine-tuned in both languages. However, Similar behavior is not observed in dominant bilingual models that are fine-tuned in only one language.</p><p>For future work, we will explore additional prob-ing measures, such as attention probing, to gain a deeper understanding of the internal behavior of tags in the VQA task. We will also investigate whether the observed effects of tags in this study can be generalized to other VLP models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Limitations</head><p>We acknowledge several limitations in our study. First, the replaced tags used in our experiments are originally detected tags from other instances, which ensures the models' understanding of the replaced tags but restricts the inclusion of additional tag types. One type that warrants further investigation is the use of synonyms as tags, such as using puppy as a replacement for dog. It would also be valuable to examine the symmetry-like effect in the experiment on bilingual relatedness by using semantically related tags, e.g., from the same category.</p><p>Second, even though a majority of the questions used in our study focus on object identification such as what is this?, there is a small number of questions that involve additional object entities. For instance, What is floating near the bird? and What is in the water?. Bird and water in the questions may act as distractors and potentially affect the model's performance. Despite this potential impact, we chose to retain these questions in the study due to their limited quantity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of unsupervised VisualBERT architecture in the fine-tuning and for inference. (FC: fully connected layer)</figDesc><graphic coords="3,135.21,70.87,324.86,157.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) 2D histogram displaying the F-scores by Sim contextual (upper) and by Sim word (lower) for the list animal; (b) F-score versus Sim word /Sim contextual from Gaussian fitting the 2D histogram for the list animal in (a) and for the list food in Figure 4 in Appendix.</figDesc><graphic coords="6,70.87,70.87,455.40,163.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(left) Accuracy (%) of VQA task and examples for the bilingually-related conditions. (right) Results of the linear mixed model, where cosine similarity (Sim word /Sim contextual ) is treated as a fixed effect together with the intercepts of items in each list as random effects, using the formula: F-score â¼ cosine similarity + (1|item), ***: p &lt; .001.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) 2D histogram displaying the F-scores by Tdiff-DE and Tdiff-EN for model VAQ EN &amp;DE (top), VQAEN (middle) and VQADE (bottom). (b) F-scores versus Sim word /Sim contextual from Gaussian fitting the 2D histogram in (a).</figDesc><graphic coords="7,313.80,187.23,202.94,220.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: 2D histogram displaying the F-scores by Sim contextual (upper) and by Sim word (lower) for the list food.</figDesc><graphic coords="11,306.99,336.11,216.58,123.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>(left) Accuracy (%) of the VQA task and examples for semantically-related conditions for the list food and the list animal. (right) Results of the linear mixed model, where cosine similarity (Sim word /Sim contextual ) is treated as a fixed effect together with the intercepts of items in each list as random effects, using the formula: F-score â¼ cosine similarity + (1|item), ***: p &lt; .001.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Accuracy</cell><cell></cell><cell cols="2">Estimate (intercept)</cell></row><row><cell></cell><cell></cell><cell>T orig</cell><cell>Tnone</cell><cell>T diff</cell><cell>Tsame</cell><cell>Sim word</cell><cell>Sim contextual</cell></row><row><cell>food</cell><cell cols="2">Acc example banana 100</cell><cell>79.1 /</cell><cell>66.0 cow</cell><cell>58.8 mango</cell><cell cols="2">-1.083***(2.103) -1.8813***(2.982)</cell></row><row><cell>animal</cell><cell>Acc example</cell><cell>100 dog</cell><cell>88.9 /</cell><cell>71.3 squash</cell><cell>56.6 cat</cell><cell cols="2">-0.6924***(1.177) -3.011***(3.014)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Examples</figDesc><table><row><cell>8 https://huggingface.co/</cell></row><row><cell>bert-base-uncased</cell></row></table><note><p>and Accuracy (%) of VQA task for each phonologically-related condition within the three groups Copen, C closed and Cnon.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>A stronger effect means that the facilitation effect is consistently present across a wider range of time intervals between the onset of the picture and the presentation of the distractor.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/jackroos/VL-BERT/ blob/master/data/PREPARE_DATA.md</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/ t-systems-on-site-services-gmbh/ german-wikipedia-text-corpus</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://pypi.org/project/googletrans/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://huggingface.co/ bert-base-multilingual-uncased</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://visualqa.org/download.html</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>In adherence to their license agreements, we use publicly available resources in our experiments. The datasets have undergone complete anonymization, ensuring that they do not include any personal information regarding the caption annotators or any data that could expose the identities of the subjects photographed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>list</head><p>T orig /Tsame T diff Tpseu T hype food pepper(65), 'cabbage'(52), 'tomato'(2028), 'banana'(6903), 'apple'(2925), 'pasta'(338), 'bread'(1092), 'cheese'(378), 'egg'(468), 'chocolate'(715), 'pancakes'(52), 'sandwich'(3796), 'pizza'(18109), 'fries'(754) <ref type="bibr">'cat', 'dog', 'sheep', 'deer', 'cow', 'horse', 'zebra', 'elephant', 'goose', 'goat', 'bear', 'panda', 'pigeon', 'butterfly' 'san', 'lan', 'ren', 'fen', 'jia', 'cho', 'jon', 'nan', 'gan', 'kam', 'yan', 'abe', 'dia', 'pia' 'food' animal 'dog'(11778)</ref>  <ref type="formula">57</ref>), 'tarmac'(52), 'visor'(480), 'donut'(4620), 'donuts'(7339), 'bib'(294), 'kayak'(186), 'tarp'(65), 'buoy'(320)</p><p>'train'(649), 'airplane'(48), 'boat'(152), 'television'(57), 'clock'(357), 'phone'(780), 'camera'(105), 'dog'(535), 'cow'(484), 'tree'(567), 'mountain'(35) <ref type="bibr">'wagen', 'zug', 'flugzeug', 'boot', 'fernsehen', 'uhr', 'telefon', 'kamera', 'hund', 'kuh', 'baum', 'berg' 'cat', 'dog', 'sheep', 'deer', 'cow', 'horse', 'zebra', 'elephant', 'goose', 'goat', 'bear', 'panda', 'pigeon', 'butterfly' 'hund', 'kuh', 'ente', 'adler', 'mais', 'stein', 'blatt', 'zeitschrift', 'wagen', 'zug', 'wein', '</ref>  <ref type="bibr">'train', 'wine', 'hair' 'hund', 'kuh', 'ente', 'mais', 'stein', 'blatt', 'zeitschrift', 'wagen', 'zug', 'wein', 'haar'</ref> Table <ref type="table">4</ref>: Lists of tags used in the semantically-related (top), phonologically-related (middle), and bilingually-related (bottom) experiments. The frequencies of the original tags are shown in parentheses. In the phonologically-related experiment, the wordpieces (examples can be found in Table <ref type="table">2</ref> in the main text) from the original tags are omitted.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Conceptual and lexical development in second language acquisition</title>
		<author>
			<persName><forename type="first">Jeanette</forename><surname>Altarriba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><forename type="middle">M</forename><surname>Mathis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of memory and language</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="550" to="568" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic interference in language production is due to graded similarity, not response relevance</title>
		<author>
			<persName><forename type="first">Sabrina</forename><surname>Aristei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasha</forename><surname>Abdel Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="571" to="582" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What phonological facilitation tells about semantic interference: A dual-task study</title>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Ayora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesca</forename><surname>Peressotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F-Xavier</forename><surname>Alario</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Mulatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pluchino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remo</forename><surname>Job</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Dell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename><surname>Acqua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in psychology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">What did we learn from forty years of research on semantic interference? a bayesian meta-analysis</title>
		<author>
			<persName><forename type="first">Audrey</forename><surname>BÃ¼rki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shereen</forename><surname>Elbuy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Madec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shravan</forename><surname>Vasishth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page">104125</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Behind the scene: Revealing the secrets of pre-trained vision-and-language models</title>
		<author>
			<persName><forename type="first">Jize</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-08-23">2020. August 23-28, 2020</date>
			<biblScope unit="page" from="565" to="580" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI 16</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Steven</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03518</idno>
		<title level="m">Multilingual alignment of contextual word representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Microsoft COCO captions: Data collection and evaluation server</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unifying vision-and-language tasks via text generation</title>
		<author>
			<persName><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1931" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lexical selection in bilinguals: Do words in the bilingual&apos;s two lexicons compete for selection?</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Miozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfonso</forename><surname>Caramazza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and language</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="365" to="397" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Investigating negation in pre-trained vision-and-language models</title>
		<author>
			<persName><forename type="first">Radina</forename><surname>Dobreva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="350" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models</title>
		<author>
			<persName><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="34" to="48" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Compressing visual-linguistic model via knowledge distillation</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1428" to="1438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6904" to="6913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Gulordava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11138</idno>
		<title level="m">Colorless green recurrent networks dream hierarchically</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Event-related potential evidence for parallel activation of two languages in bilingual speech production</title>
		<author>
			<persName><forename type="first">Taomei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danling</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroReport</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="1757" to="1760" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Producing words in a foreign language: Can speakers prevent interference from their first language?</title>
		<author>
			<persName><forename type="first">Daan</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Bongaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kees</forename><forename type="middle">De</forename><surname>Bot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Schreuder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bilingualism: language and cognition</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="213" to="229" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Scaling up vision-language pre-training for image captioning</title>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yumao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12233</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">VIVO: Surpassing human performance in novel object captioning with visual vocabulary pre-training</title>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic gradients in picture-word interference tasks: Is the size of interference effects affected by the degree of semantic overlap?</title>
		<author>
			<persName><forename type="first">James</forename><surname>Hutson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><forename type="middle">F</forename><surname>Damian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">872</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.07840</idno>
		<title level="m">Cross-lingual ability of multilingual BERT: An empirical study</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Negated and misprimed probes for pretrained language models: Birds can talk</title>
		<author>
			<persName><forename type="first">Nora</forename><surname>Kassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>SchÃ¼tze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03343</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">but cannot fly. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Yova</forename><surname>Kementchedjhieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>SÃ¸gaard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01060</idno>
		<title level="m">John praised mary because he? implicit causality bias and its interaction with explicit cues in LMs</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A further look at semantic context effects in language production: The role of response congruency</title>
		<author>
			<persName><forename type="first">Jan-Rouke</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wido</forename></persName>
		</author>
		<author>
			<persName><forename type="first">La</forename><surname>Heij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Costa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Cognitive Processes</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7-8</biblScope>
			<biblScope unit="page" from="892" to="919" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Liunian</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Da Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">VisualBERT: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Unsupervised vision-and-language pre-training without parallel images and captions</title>
		<author>
			<persName><forename type="first">Liunian</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhecan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12831</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">TAB-VCR: Tags and attributes based visual commonsense reasoning baselines</title>
		<author>
			<persName><forename type="first">Jingxiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Unnat</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Assessing the ability of LSTMs to learn syntaxsensitive dependencies</title>
		<author>
			<persName><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="521" to="535" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The semantic nature of response competition in the picture-word interference task</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName><surname>Lupker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="485" to="495" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Phonological facilitation in picture-word interference experiments: Effects of stimulus onset asynchrony and types of interfering stimuli</title>
		<author>
			<persName><forename type="first">S</forename><surname>Antje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herbert</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><surname>Schriefers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1146" to="1160" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Exploring BERT&apos;s sensitivity to lexical cues using tests from semantic priming</title>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><forename type="middle">Taylor</forename><surname>Rayz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03010</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A primer in bertology: What we know about how bert works</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="842" to="866" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The closer they are, the more they interfere: Semantic similarity of word distractors increases competition in language production</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Sabrina</forename><surname>Aristei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alissa</forename><surname>Melinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasha</forename><surname>Abdel Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="753" to="763" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Phonological facilitation in the production of two-word utterances</title>
		<author>
			<persName><forename type="first">Herbert</forename><surname>Schriefers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="50" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Cross-lingual alignment of contextual word embeddings, with applications to zero-shot dependency parsing</title>
		<author>
			<persName><forename type="first">Tal</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ori</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09492</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">LXMERT: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07490</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stroop and picture-word interference are two sides of the same coin</title>
		<author>
			<persName><forename type="first">Hedderik</forename><surname>Leendert Van Maanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jelmer</forename><forename type="middle">P</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName><surname>Borst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="987" to="999" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Feature overlap slows lexical selection: Evidence from the picture-word interference paradigm</title>
		<author>
			<persName><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Vieth</surname></persName>
		</author>
		<author>
			<persName><surname>Mcmahon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zubicaray</forename><surname>De</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2325" to="2339" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Representing the meanings of object and action words: The featural and unitary semantic space hypothesis</title>
		<author>
			<persName><forename type="first">Gabriella</forename><surname>Vigliocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Vinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Merrill</forename><forename type="middle">F</forename><surname>Garrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="488" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.06946</idno>
		<title level="m">MiniVLM: A smaller and faster vision-language model</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06775</idno>
		<title level="m">Cross-lingual BERT transformation for zero-shot dependency parsing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Who did what to whom? Language models and humans respond diversely to features affecting argument hierarchy construction</title>
		<author>
			<persName><forename type="first">Xiaonan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoshuo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="254" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">2021a. Multiscale vision longformer: A new vision transformer for high-resolution image encoding</title>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="page" from="2998" to="3008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">VinVL: Revisiting visual representations in vision-language models</title>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5579" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
