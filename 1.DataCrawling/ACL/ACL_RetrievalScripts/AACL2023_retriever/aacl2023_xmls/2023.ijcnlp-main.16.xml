<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reimagining Complaint Analysis: Adopting Seq2Path for a Generative Text-to-Text Framework</title>
				<funder>
					<orgName type="full">Science and Engineering Research Board</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Apoorva</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Patna</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raghav</forename><surname>Jain</surname></persName>
							<email>raghavjain106@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Patna</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sriparna</forename><surname>Saha</surname></persName>
							<email>sriparna@iitp.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Patna</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reimagining Complaint Analysis: Adopting Seq2Path for a Generative Text-to-Text Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A7BBBFC83079592E88E0A09FB8BFBFFC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T14:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The escalating volume and frequency of social media complaints necessitate robust automated complaint analysis techniques. Much of the existing body of research in this area has been devoted to two primary aspects: identifying complaint-specific content amidst other noncomplaint communications, and predicting the severity of a complaint, which involves classifying complaints into different severity levels based on the anticipated resolution from the complainant's perspective. These automated analysis tools equip companies with the means to effectively manage complaints and generate suitable responses. In our study, we present a unified generative approach for complaint detection, transforming the multitask learning problem into a text-to-text generation task. As part of our training strategy, we adopt the Seq2Path training paradigm that conceptualizes the outcome as a tree structure as opposed to a traditional sequence. This innovative approach tackles the drawbacks of conventional sequences, such as the lack of order among the outputs, yielding a more coherent and structured output. Our model's effectiveness is assessed against the benchmark Complaints dataset, highlighting its superior performance across diverse evaluation metrics when compared with state-of-the-art models and other baselines 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated complaint analysis benefits companies by efficiently handling large volumes of data, ensuring objectivity, reducing human biases and errors, and streamlining the process of identifying and classifying complaints. It helps to bridge the gap between customer expectations and reality, serving as a crucial feedback tool to pinpoint areas of dissatisfaction and highlight avenues for improvement. Notably, Trosborg et al. <ref type="bibr" target="#b30">(Trosborg, 2011)</ref> proposed a four-tiered framework for understanding the severity of complaints, categorizing them from subtle disapproval to outright blame. With the surge in digital platforms, customer complaints have significantly increased, presenting a daunting task for manual processing. This necessitates a more efficient complaint detection approach, as exhibited in recent studies <ref type="bibr" target="#b16">(Preotiuc-Pietro et al., 2019;</ref><ref type="bibr">Singh et al., 2022a)</ref> that have successfully automated the classification of binary complaints and their associated severity levels. Emotion recognition (ER) and sentiment recognition (SR) play vital roles in understanding the affective aspects of customer complaints <ref type="bibr">(Singh and Saha, 2021;</ref><ref type="bibr">Singh et al., 2023a)</ref>. In this transition towards automation, inspiration can be drawn from multitask learning <ref type="bibr" target="#b2">(Caruana, 1997)</ref>-an approach that mirrors our inherent human ability to learn multiple tasks simultaneously and transfer knowledge across them. This technique, while advantageous, poses its own set of challenges, such as negative transfer (where multiple tasks, rather than benefiting the learning pro-cess, begin to hinder the training process) (Crawshaw, 2020) and optimization scheme (assigning weights to different tasks during training) <ref type="bibr" target="#b32">(Wu, 2020)</ref>. However, a paradigm shift is observed in the natural language processing (NLP) community, where an increasing number of tasks are being formulated as text-to-text generation problems <ref type="bibr" target="#b6">(Du et al., 2021)</ref>. This approach, harnessing the power of large pre-trained language models and offering unified solutions via a single model, addresses the sheer scale of digital customer complaints. However, it isn't devoid of challenges, such as a) Sequencing, the sequence or order between the outputs does not exist in reality, b) Conditioning, the generation of output should not rely or depend on previously generated outputs. In this light, the recent success of models transforming text-to-text generation into text-to-tree structures offers an intriguing solution <ref type="bibr" target="#b14">(Mao et al., 2022;</ref><ref type="bibr" target="#b36">Yu et al., 2022;</ref><ref type="bibr" target="#b0">Bao et al., 2022)</ref>. These models, addressing the challenges of sequencing and conditioning, present outputs as paths of a tree, following a '1-to-n' relationship. In stark contrast to the '1-to-1' relationships depicted by sequence-to-sequence techniques, these models eliminate the need for order or dependence on previously generated outputs. Moreover, our innate capacity to infer implied meanings from explicit expressions-a function of our commonsense-could provide vital insights for automated complaint detection systems. The integration of such external or commonsense knowledge <ref type="bibr" target="#b22">(Sabour et al., 2022)</ref> could potentially enrich our understanding of user contexts and situations, contributing to the development of more effective models. The relevance of commonsense reasoning, largely explored in conversational agents and summarization tasks, extends promisingly to complaint detection in this work. With a view to mitigate the challenges presented by multitask learning and Seq2Seq models, and inspired by the promising implications of the textto-tree training paradigm, we introduce a novel approach in this paper. We present a commonsenseaware unified generative framework that employs a sequence-to-path (Seq2Path) training paradigm. This framework aims to streamline the primary tasks of complaint detection and severity level classification. Our proposed approach brings a paradigm shift in this regard, offering a more flexible and context-aware mechanism for output generation, thus enhancing the efficacy of automated complaint analysis. Figure <ref type="figure" target="#fig_0">1</ref> shows a user sharing case details with customer service. The user is awaiting a callback (xNeed) and desires to be contacted regarding the case (xWant). Our Seq2Path training generates multiple outputs in a tree-like structure, solving emotion and sentiment in one path, and severity classification and complaint identification in another, as depicted in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Contributions: Our work's significant contributions are as follows: 1) We propose a unified generative approach for complaint detection to concurrently address four tasks: complaint identification (CI), severity classification (SC), with emotion recognition (ER), and sentiment recognition (SR) as auxiliary tasks.</p><p>2) We introduce a Seq2Path training method that views the output as a tree rather than a sequence, enabling each associated task to be treated as an individual tree path 3) Our proposed model, evaluated on a benchmark Complaints dataset, demonstrates superior performance across various metrics, surpassing other baselines and state-of-the-art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Automatic complaint detection has received a lot of attention in recent years. Earlier studies concentrated on single-task complaint detection, using feature-based machine learning <ref type="bibr" target="#b16">(Preotiuc-Pietro et al., 2019;</ref><ref type="bibr">Coussement and Van den Poel, 2008)</ref> and leveraging transformer networks <ref type="bibr">(Jin and</ref><ref type="bibr">Aletras, 2020, 2021a;</ref><ref type="bibr">Singh et al., 2023b</ref><ref type="bibr">Singh et al., , 2021))</ref>. Apart from complaint mining, research has concentrated on detecting product hazards and risks <ref type="bibr" target="#b1">(Bhat and Culotta, 2017)</ref>, as well as the likelihood for escalation of complaints <ref type="bibr" target="#b34">(Yang et al., 2019)</ref>. Recently, multitask complaint detection models <ref type="bibr">(Singh and Saha, 2021;</ref><ref type="bibr">Singh et al., 2022a)</ref> that incorporated sentiment and emotion information to improve the complaint mining process were developed. Developing effective systems for downstream tasks such as chatbots and customer support systems requires a cognitive understanding of the user's conditions and emotions <ref type="bibr" target="#b21">(Sabour et al., 2021)</ref>. Consequently, we believe that permitting complaint detection models utilize commonsense information and draw conclusions based on what the customer has openly shared is especially beneficial for explaining the user's circumstances, leading to more efficient and socially conscious customer support systems. Recent advances in deep learning and pre-trained language models have had a significant impact in the area of neural text generation <ref type="bibr">(Raffel et al., 2020a;</ref><ref type="bibr" target="#b12">Lewis et al., 2019)</ref>. Encoder-decoder Transformers consisting of BART <ref type="bibr" target="#b13">(Lewis et al., 2020)</ref> and T5 <ref type="bibr">(Raffel et al., 2020b)</ref>, have shown massive improvements and success in many NLP tasks such as summarization and translation. Recently, Yan et al. <ref type="bibr" target="#b33">(Yan et al., 2021)</ref> solved the task of aspect-based sentiment analysis in a generative manner using the BART model. The BART model is implemented to generate the target sequence in an end-to-end process based on unified task generation. Past studies on complaint detection leveraged pretrained language models that were fine-tuned for certain tasks by layering task-specific layers on top of the model. Our current work contrasts in that we redefine the multitask problem as a language generation task, enabling the model to learn to perform the tasks without the requirement for task-specific layers to be trained. A comprehensive literature study has led to the conclusion that Seq2Path, a tree-based generative method, is preferable for complaint identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Methodology</head><p>We define our problem before getting into the specifics of the proposed model. Figure <ref type="figure" target="#fig_1">2</ref> depicts the overall architecture of our proposed model CGenPath.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>In our study, we aim to explore product reviews through four interconnected tasks: two primary tasks -complaint identification and severity classification; and two auxiliary tasks -sentiment polarity and emotion recognition. Each review, represented as X i = {x 0 , x 1 , .., x i , .., x n }, with n denoting the instance length, is classified as follows:</p><p>(1) Complaint Identification (c): The primary task where a review is assigned a class c from the complaint class set C.</p><p>(2) Severity Classification (s): Another primary task where the identified complaint is assigned a severity level s from the set S.</p><p>(3) Polarity Classification (p): An auxiliary task that assigns a sentiment polarity p from the set P to the review. (4) Emotion Recognition (e): An auxiliary task that assigns an emotion e from the emotion class set E to the review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Complaint Detection as Seq2Path Task</head><p>Here, we conceptualized Complaint detection in multitask setting as a sequence-to-path (Seq2Path) problem <ref type="bibr" target="#b14">(Mao et al., 2022)</ref> in which each path can be interpreted as a branch of a tree and can be created separately. At first, we train our commonsense aware seq2seq model (defined in Sec 3.3), where each path is seen as a distinct target, and estimate the average loss. Second, the token generation process is modeled as a tree, and a constrained beam search is used to create paths independently. Finally, given a text as input, the output is a set of all legitimate individual paths with a discriminating token (true or false) attached at the end to pick the correct paths automatically. Formally, given an input review X i , our task of obtaining compliant label (c), severity class (s), emotion class (e), and polarity class (p) can be modeled as generating a tree with two paths: (1) One path will generate c and s labels, and (2) second path will generate e and p labels. Finally, the target sequence Y i is represented as following:</p><formula xml:id="formula_0">Y i = {&lt; c &gt;&lt; s &gt;, &lt; e &gt;&lt; p &gt;}<label>(1)</label></formula><p>To distinguish valid paths from the others, we added a distinguishing token "true" to the valid paths and "false" to the invalid ones, thereby allowing us to clearly separate them. In order to achieve better performance on the model, data augmentation is necessary due to the absence of negative samples for discriminative token. To facilitate a good selection of viable paths, each negative sample is augmented with a discriminative token of "false". We generate negative samples by randomly substituting the components of a path. This substitution process helps to boost the model's capacity to recognize valid paths during inference, thus aiding the overall performance of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Commonsense aware Generative</head><p>Seq2Path Model for Complaint Detection (CGenPath)</p><p>We introduce the Commonsense aware Generative Seq2Path Model (CGenPath), a unified generative framework designed to tackle the challenge of complaint detection within a multitask context. Our approach is divided into three main steps for simplicity and clarity: (1) Commonsense Extractor, (2) Commonsense-Fused Encoder, and (3) Seq2Path Training and Inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Commonsense Extractor</head><p>Our approach begins with the implementation of the Commonsense Extractor, a key component in providing an additional layer of context and commonsense reasoning to typically brief and concise customer reviews. The ATOMIC dataset <ref type="bibr" target="#b23">(Sap et al., 2019)</ref> serves as our knowledge base, offering insights into six commonsense relations associated with the entity participating in an event, such as the event's effects (xEffect), the entity's needs (xNeed), and desires (xWant). In the context of complaint detection task, we interpret the review instance as the event and aim to comprehend the customer's needs and desires from their review. Thus, we exclusively focus on two commonsense relations: xNeed and xWant 2 . To generate commonsense reasoning from the customer reviews, we utilize the pre-trained BART <ref type="bibr" target="#b12">(Lewis et al., 2019)</ref> based language model, COMET <ref type="bibr" target="#b8">(Hwang et al., 2021)</ref>, which has been fine-tuned on the ATOMIC dataset. This model is especially effective in providing commonsense reasoning for unseen events <ref type="bibr" target="#b22">(Sabour et al., 2022)</ref>. The functioning of the Commonsense Extractor involves appending the commonsense relation tokens (xNeed and xWant) to each review X i . These concatenated inputs are processed through the pretrained COMET model to generate two common-2 We performed a thorough comparative analysis of all the commonsense relations available in the ATOMIC dataset.</p><p>sense reasonings cs r need and cs rwant for the xNeed and xWant relation tokens, respectively. We generate the final commonsense reasoning CS for each review X i by concatenating these two reasonings, represented by the following equation: CS = cs r need ⊕ cs rwant .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Commonsense-Fused Encoder</head><p>To effectively utilize the commonsense reasoning CS derived from the Commonsense Extractor, we propose a commonsense-aware encoderdecoder framework. This architecture is designed to seamlessly incorporate CS within its sequenceto-sequence learning process, as explained below: In our architecture, the initial stage involves feeding both the review input X i and the commonsense reasoning CS to a pre-trained BART encoder. This results in two encoded representations, namely U x and U cs . To integrate the information carried by these two representations, we propose a novel commonsense-fused encoder, an enhancement of the conventional transformer encoder <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref>. This involves the generation of two triplets of query, key, and value matrices corresponding to U x and U cs : (Q x ,K x ,V x ) and (Q cs ,K cs ,V cs ). Diverging from the standard transformer encoder which projects identical inputs as query, key, and value, our model introduces a cross-attention layer. This layer, consisting of two sublayers of multi-head-cross attention and a normalization layer, exchanges keys and values. It treats (Q x ,K cs ,V cs ) and (Q cs ,K x ,V x ) as inputs to the cross-attention layer which is computed as defined below, resulting in a cross-infused vector representation.</p><formula xml:id="formula_1">Attention(Q, K, V ) = sof tmax( QK T √ d k )V (2)</formula><p>where (Q,K,V) represents the set of the query, key, and value and d k represents the dimension of the query and key.</p><p>The cross-attention layer enables a two-way flow of information between U x and U cs . As a result, the outputs of the multihead cross-attention layer, namely U x-&gt;cs and U cs-&gt;x , are enriched with information from each other. In the next step, we merge U x-&gt;cs and U cs-&gt;x into a single output, U z . This merged output is then processed via a self-attention layer, normalization layers, and fully connected layers with residual connections, thereby producing the final output of our commonsense-fused encoder. At last, we bring together the original text representation U x , the commonsense reasoning U cs , and the output from the commonsense-fused encoder, culminating in the final commonsense-fused input representation vector, Z. Once we obtain the commonsense-aware input representation vector, denoted as Z, we proceed to feed Z along with all the output tokens until time step t -1, represented as Y &lt;t , into the decoder module. This process allows us to obtain the hidden state at time step t, which can be defined as follows:</p><formula xml:id="formula_2">H t DE = G Decoder (Z, Y &lt;t )</formula><p>where G Decoder denotes the decoder computations.</p><p>The conditional probability of predicting the output token at the t-th time step, given the input and the previous t -1 tokens, is determined by applying the softmax function to the hidden state H t DEC as follows:</p><formula xml:id="formula_3">P θ (Y ′ t |R, Y &lt;t ) = F sof tmax (θ T H t DE ) (3)</formula><p>where F sof tmax represents softmax computation and θ denotes weights of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Seq2Path Training and Inference</head><p>In this section, we discuss training and inference of our commonsense aware model specific to the Seq2Path paradigm.</p><p>Training: For a given review text X i , we want to generate a set of different paths. Our dataset consists of pairs of (X i , Y i ) where Y i = {y 1 , y 2 , ..., y k }. As depicted in Figure <ref type="figure" target="#fig_0">1</ref>, Y i can be expressed as a tree, and each y has a distinct path in the tree. The total number of paths is represented by k. When predicting Y ′ i from X i , the loss can be defined as the average loss of the k paths motivated by <ref type="bibr" target="#b14">Mao et al. (Mao et al., 2022)</ref>.</p><formula xml:id="formula_4">L(Y ′ , Y i |X i ) = y∈Y i L M LE (y ′ , y|X i ) k (4)</formula><p>where L M LE is the standard loss function based on the maximum likelihood estimation (MLE) objective function. In our case, as defined in equation 1, we have 2 paths setting the total number of paths parameter k as 2.</p><p>Inference: At the inference stage, we use beam search with constrained decoding motivated by different text-to-tree generation works <ref type="bibr" target="#b14">(Mao et al., 2022;</ref><ref type="bibr" target="#b36">Yu et al., 2022;</ref><ref type="bibr" target="#b0">Bao et al., 2022)</ref>. Beam Search considers multiple alternative options based on the hyperparameter beamwidth (B) and conditional probability which is more optimal than a simple greedy search technique that only selects a single best token at each time step. Utilizing beam search, we output the top-k paths with diminishing probabilities which signify the possibility of the paths being right. During the decoding process, constrained decoding is employed in order to restrict beam search to look only within specific candidate tokens. As suggested by Cao et al. <ref type="bibr" target="#b5">(De Cao et al., 2020)</ref>, these tokens can be both derived from the given input text and/or some special tokens intended for the specific task at hand<ref type="foot" target="#foot_0">3</ref> . We also applied a filtering process to filter out the invalid paths. We output the valid paths with a discriminative token "true" and remove the other paths with a discriminative token "false". This step ensured that only the valid paths were retained and any invalid ones were excluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>This section describes the dataset used, experiments, results, and analysis of our proposed model.</p><p>The experiments are intended to address the following research questions: RQ1: How does the generative paradigm perform in comparison to traditional multi-task models? RQ2: How does Seq2Path paradigm overcome the drawbacks of Seq2Seq and multi-task models? RQ3: What is the impact of external knowledge and the Seq2Path training paradigm on the performance of our framework? RQ4: Is the proposed model able to outperform state-of-the-art models for complaint detection and severity classification tasks?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset Description</head><p>In (4) The sentiment classes for the tweets (SA task) <ref type="bibr">(Singh et al., 2022a)</ref> were broken down as follows: 1,041 negative, 1,198 neutral, and 1,210 positive.</p><p>(5) In <ref type="bibr" target="#b16">(Preotiuc-Pietro et al., 2019)</ref>, the inter-rater agreement score for the main task CD is reported as 0.73 (Cohen's Kappa). For the SD task, the inter-rater agreement is 0.64 (Fleiss' Kappa score) according to <ref type="bibr">(Jin and Aletras, 2021a)</ref>. The auxiliary tasks ED and SA have inter-rater agreement scores of 0.68 and 0.82 (Cohen's Kappa score) respectively, as reported in <ref type="bibr">(Singh et al., 2022a</ref>). Table <ref type="table" target="#tab_0">1</ref> presents example instances from the Complaints dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Models and Experimental Setup</head><p>The primary model in our study, CGenPath, uses BART <ref type="bibr" target="#b12">(Lewis et al., 2019)</ref> as its foundation, and its configuration and training methodology are outlined in Appendix A.1. For comparative baselines, we consider several multitask systems and text-totext generation models. These include Baseline 1 , which was inspired by previous work from Singh et al. <ref type="bibr">(Singh et al., 2022b)</ref>, and M T GloV e , which uses a GloVe pre-trained word embedding and a subsequent BiGRU layer. We also studied BERT-MT, which is a multitask model that's based on BERT, as well as BART and T5 models. Besides these, we developed a variant of CGenPath called CGenP ath con that simply combines the input text and commonsense reasonings. We also made two versions of our main model that leave out certain features: CGenPath-Seq2Path, which doesn't use the Seq2Path training mechanism, and CGen-Path-CS, which leaves out commonsense reasoning. Full details about these models' development and training are available in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Discussions</head><p>It is crucial to emphasize that the primary focus of this study is to improve the performance of the CI (Complaint Identification) and SC (Severity Classification) tasks. Consequently, the results and analysis presented in this research solely consider CI and SC as the primary tasks.</p><p>(RQ1) The results presented in Table <ref type="table" target="#tab_1">2</ref> demonstrate the superior performance of CGenPath over all multitask baselines in both the Complaint Identification (CI) and Severity Classification (SC) tasks.</p><p>CGenPath surpasses Baseline1, BERT-MT, and MTGlove by margins of 9.4%, 6.1%, and 7.9%, respectively, in terms of macro-F1 scores for the CI task. A similar trend is observed for the SC task. Notably, other generative methods such as BART, T5, and CGenP ath con also exhibit significant performance advantages over the multitask baselines in both tasks. These findings highlight the superiority of pre-trained sequence-to-sequence language models in the context of our experiments.</p><p>(RQ2) As can be seen from the table 2 CGenPath clearly outperforms other generative baselines and its variant CGenPath con across both tasks. CGen-Path outperforms BART, T5 and CGenPath con by a margin of: (1) 1.9%, 4.1%, and 1.1% in CI task on macro-F1 score and (2) 11.3%, 5%, and 3.6% in SC task on macro-F1 score. This performance gain over BART and T5 can be attributed to the following: (1) Trees can be viewed as a better semantic representation to more effectively capture the structure of different tasks, and (2) Seq2Path training is able to make the model understand the order of output thus making results more consistent. These findings validate the effectiveness of reframing the multitasking problem as a Seq2Path generation task. Furthermore, the performance improvement over CGenPath con highlights the necessity for a well-designed and appropriately formulated fusion technique to effectively integrate diverse sources of information.</p><p>(RQ3) Ablation Study: To investigate the impact of commonsense reasoning and Seq2Path training on our model's performance, we conducted an ablation study ( The reasons for these improvements can also be attributed to the facts: 1) Our model, CGenPath is leveraging pretrained BART model's knowledge which already has been trained on a huge corpus of data, 2) This model has extra context in the form of commonsense reasoning due to which they are making better predictions, and 3) Seq2Path training can handle complex relations between different tasks and produce more consistent results.</p><p>To assess the statistical significance of the obtained results, we conducted a paired T-test. The analysis revealed that the performance improvement achieved by our proposed model compared to the state-of-the-art is statistically significant with a 95% confidence level (i.e., p-value &lt; 0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Analysis</head><p>During our qualitative analysis, we found tweets displaying evident complaint markers, such as blame-associated language or accusations, are less prone to misclassification. A comparison of our system's results with those of the state-of-the-art (SOTA) system is presented in label as Non-complaint in instance 2, while its predicted severity level is Disapproval, leading to a discrepancy between the two labels. On the contrary, our model successfully predicts both labels accurately, thereby demonstrating that CGenPath yields more consistent outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Error Analysis</head><p>We've conducted a detailed examination of a selection of test set samples, comparing the complaint and severity labels produced by CGenPath to those annotated by a human. During this evaluation, we've come across a few instances where the model exhibited errors: 1. Misinterpretation of Implicit Complaints: The model struggles with predictions when the true intention is subtly embedded in the text. In cases where complaints are implied rather than explicitly stated, the model inaccurately identifies them as non-complaints due to a literal interpretation of the text. An example of this is the sentence, &lt;USER&gt; You guys are doing an amazing job ensuring that every week there's a new bug in the software. This is a complaint, but the model mistakenly labels it as a non-complaint. This issue is likely due to the indirect expression of dissatisfaction or blame by the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Severity Misclassification Due to Linguistic</head><p>Overlaps: Misclassification can also take place when instances sharing similar linguistic and structural characteristics exist within adjacent severity levels. Consider the sentence "I am surprised that a reputable company like yours has such a complex return policy. It's quite disappointing.". In this case, the model incorrectly identifies the severity level as disapproval instead of the correct label -accusation. This misclassification may be attributed to the presence of words such as "disappointed", which are typically associated with the disapproval class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this study, we aimed to achieve three objectives: firstly, developing a cohesive generative strategy for complaint identification by redefining the multitask learning approach as a text-to-text genera- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>We attempted to develop a novel unified generative framework for complaint analysis. But the proposed approach is having some limitations as enumerated below:</p><p>(1) The proposed methodology has been validated on an English language complaint dataset; further training would be required to scale up to codemixed language datasets which are prevalent in multilingual countries.</p><p>(2) Users often post some images along with text while writing complaints. The current system is unable to handle such multi-modal forms of inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Experimental Setup</p><p>In this section, we present the hyperparameters and experimental configurations employed in our study. All experiments were conducted on the Tyrone machine, equipped with Intel's Xeon W-2155 Processor, 196 GB DDR4 RAM, and an Nvidia 1080Ti GPU with 11 GB memory. The dataset was randomly divided, with 80% used for training, 5% for validation, and the remaining 15% for testing. Each model was run ten times, and the average results were reported. For our CGenPath model, we utilized BART <ref type="bibr" target="#b12">(Lewis et al., 2019)</ref> as the base model. Training was conducted for a maximum of 60 epochs with a batch size of 16. We employed the Adam optimizer with an epsilon value of translation. BART is pre-trained with various denoising pretraining objectives such as token masking, sentence permutation, sentence rotation etc. b) T5: T5 is also an encoder-decoder-based transformer model which aims to solve all the text-totext generation problems. The main difference between BART and T5 is the pre-training objective. In T5, the transformer is pre-trained with a denoising objective where 15% of the input tokens are randomly masked and the decoder tries to predict all these masked tokens whereas, during pre-training of BART, the decoder generates the complete input sequence.</p><p>We fine-tune both these models on the proposed dataset with complaint text as the input sequence and concatenated outputs as the target sequence with Maximum likelihood Estimation as the objective function. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of unified generative approach with commonsense knowledge and Seq2Path training used to identify complaints and associated tasks. The two paths represent the related tasks being solved together as a tree-like structure. &lt;bos&gt;: beginning of string, &lt;eos&gt;: end of token.</figDesc><graphic coords="1,335.90,212.60,158.74,118.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of unified generative approach with commonsense knowledge and Seq2Path training used to identify complaints and associated tasks. The two paths represent the related tasks being solved together as a tree-like structure.</figDesc><graphic coords="3,70.87,70.87,226.77,214.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Different example instances of Complaints dataset.</figDesc><table><row><cell>the current study, we use the Complaints dataset</cell></row><row><cell>provided in (Preotiuc-Pietro et al., 2019), which</cell></row><row><cell>comprises of 3,449 tweet instances in English. We</cell></row><row><cell>chose this dataset since it is open source and in-</cell></row><row><cell>cludes annotated complaints from Twitter, a promi-</cell></row><row><cell>nent data-analysis platform. Recently, Jin et al. (Jin</cell></row><row><cell>and Aletras, 2021a) added five severity levels to</cell></row><row><cell>the Complaints dataset (no explicit reproach, disap-</cell></row><row><cell>proval, accusation, blame, and non-complaints). In</cell></row><row><cell>the work, Singh et al. (Singh et al., 2022a) added</cell></row><row><cell>sentiment (negative, neutral, positive) and emotion</cell></row><row><cell>(anger, disgust, fear, happiness, sadness, surprise,</cell></row></table><note><p><p><p><p><p>and other) classes to this dataset; the 'other' emotion class represents tweets that are not covered by Ekman's six basic emotions</p><ref type="bibr" target="#b7">(Ekman et al., 1987)</ref></p>. For our current study, we use this extended dataset (3) Singh et al.'s</p>(Singh et al., 2022a</p>) study found that 844 tweets were categorized as 'Anger', 7 as 'Disgust', 8 as 'Fear', 473 as 'Joy', 1,479 as 'Other', 626 as 'Sadness', and 12 as 'Surprise' in the emotion classification task.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of different baselines, SOTA and the proposed framework,CGenPath. For the CI and SC tasks, the results are in terms of macro-F1 score (F1) and Accuracy (A) values. F1, A metrics are given in %. The maximum scores attained are represented by bold-faced values. The † denotes statistically significant findings.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Complaint Identification (CI) Severity Classification (SC)</cell></row><row><cell></cell><cell></cell><cell>Model</cell><cell></cell><cell>F1</cell><cell>A</cell><cell>F1</cell><cell>A</cell></row><row><cell cols="5">SOTA(Jin and Aletras, 2021b) 86.6</cell><cell>87.6</cell><cell>59.4</cell><cell>55.5</cell></row><row><cell></cell><cell cols="2">CGenPath</cell><cell></cell><cell>90.8  †</cell><cell>91.3  †</cell><cell>73.7  †</cell><cell>70.4  †</cell></row><row><cell cols="5">Baseline 1 (Singh et al., 2022b) 81.4</cell><cell>82.8</cell><cell>60.3</cell><cell>62.8</cell></row><row><cell></cell><cell></cell><cell>BART</cell><cell></cell><cell>88.9</cell><cell>88.9</cell><cell>62.4</cell><cell>62.6</cell></row><row><cell></cell><cell></cell><cell>T5</cell><cell></cell><cell>86.7</cell><cell>86.6</cell><cell>68.7</cell><cell>69.3</cell></row><row><cell></cell><cell cols="2">CGenP ath con</cell><cell></cell><cell>89.7</cell><cell>89.9</cell><cell>70.1</cell><cell>68.4</cell></row><row><cell></cell><cell cols="2">M T GloV e</cell><cell></cell><cell>82.9</cell><cell>84.3</cell><cell>52.4</cell><cell>50.1</cell></row><row><cell></cell><cell cols="2">BERT-MT</cell><cell></cell><cell>84.7</cell><cell>86.1</cell><cell>57.7</cell><cell>53.3</cell></row><row><cell></cell><cell cols="4">Complaint (CI) Severity (SC)</cell></row><row><cell>Model</cell><cell>F1</cell><cell>A</cell><cell>F1</cell><cell>A</cell></row><row><cell cols="2">CGenPath 90.8  †</cell><cell>91.3  †</cell><cell cols="2">73.7 70.4  †</cell></row><row><cell cols="2">-Seq2Path 89.1</cell><cell>88.9</cell><cell>64.5</cell><cell>65.1</cell></row><row><cell>-CS</cell><cell>89.8</cell><cell>90.3</cell><cell>71.4</cell><cell>69.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results of the ablation studies performed on the proposed framework's key components in terms of macro-F1 score (F1) and Accuracy (A) values. The maximum scores attained are represented by bold-faced values. The † denotes statistically significant findings.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>). The results clearly indi-</cell></row><row><cell>cate that removing the Seq2Path training mecha-</cell></row><row><cell>nism leads to a noticeable decline in performance</cell></row><row><cell>for both the CI (Complaint Identification) and SC</cell></row><row><cell>(Severity Classification) tasks, as reflected in the</cell></row><row><cell>evaluation metrics. Furthermore, when the com-</cell></row><row><cell>monsense reasoning (CS) component is removed</cell></row><row><cell>from our model, we observed a reduction in perfor-</cell></row><row><cell>mance of 1% and 2.3% for the CI and SC tasks, re-</cell></row><row><cell>spectively. However, when the CS component and</cell></row><row><cell>Seq2Path are combined in the CGenPath model,</cell></row><row><cell>it surpasses all ablated models and baseline mod-</cell></row><row><cell>els in terms of performance across all evaluation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>. This</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Qualitative analysis of the SOTA(Jin and Aletras, 2021b) and the proposed model for CI and SC task predictions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Concatenation based CGenPath: We also proposed a variation of our framework named CGenP ath con , where we directly concatenate the input review and commonsense reasoning instead of a fusing mechanism. Ablation Models: The CGenPath model comprises two key components: (1) Commonsense Reasoning (CS) and (2) Seq2Path Training. In order to establish the necessity of both of these components individually, we conduct an ablation study of the proposed framework. In this case, we propose two ablated models; (1) CGenPath-Seq2Path where we replaced the seq2path training with standard seq2seq training, and (2) CGenPath-CS where we didn't include the commonsense reasoning into our encoder.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>We used the special tokens method.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>https://www.iitp.ac.in/~ai-nlp-ml/resources. html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>https://scikit-learn.org/stable/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>https://huggingface.co/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>https://pytorch.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>GloVe: http://nlp.stanford.edu/data/wordvecs/ glove.840B.300d.zip</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p><rs type="person">Dr. Sriparna Saha</rs> would like to acknowledge the support of the <rs type="funder">Science and Engineering Research Board</rs> (SERB)-POWER scheme, India</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>[SPG/2021/003801-G], a statutory body of the Department of Science &amp; Technology, Government of India) to carry out this research. 0.00000001. A seed value of 32 was chosen for fair comparisons. The implementation of all models utilized the Scikit-Learn 5 , Huggingface library 6 , and the PyTorch 7 backend. The predictive performance of our proposed model on all tasks was evaluated using two metrics: accuracy and macro-F1 score. The specifications of the transformer model used are as follows: (1) Number of encoder layers: 6, (2) Number of decoder layers: 6, (3) Dimensionality of layers: 1024, and (4) Embedding size: 1024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Baseline Models</head><p>Multitask systems: Drawing inspiration from the advancements in the multitask CI framework, we developed Baseline 1 <ref type="bibr">(Singh et al., 2022b)</ref> as one of the multitask baselines. This model allows for simultaneous learning of CI (Complaint Identification), SC (Severity Classification), PR (Polarity Classification), and ER (Emotion Recognition), employing the same experimental setup as our current study. We also developed another baseline model called M T GloV e <ref type="bibr" target="#b17">(Qureshi et al., 2020)</ref>. This approach leverages pre-trained GloVe word embeddings <ref type="bibr" target="#b15">(Pennington et al., 2014)</ref> as its initial step, retrieving embeddings from the GloVe pre-trained word embedding file 8 . The embedding layer's output is then fed into a word sequence encoder, which captures contextual information from the sentence. The M T GloV e model incorporates a fully shared BiGRU layer with 256 units, followed by a shared attention layer. The output of the attention layer is further processed by four task-specific dense layers before being directed to the output layers. In addition, we developed a Basic Multi-task System (BERT-MT) based on BERT <ref type="bibr" target="#b35">(Yi and Hu, 2019)</ref>. The architecture of BERT-MT comprises a shared BiGRU layer with 256 units, followed by a shared attention layer. The output of the attention layer is then fed into four task-specific dense layers, each accompanied by its respective output layer. Text to Text Generation Model: We use BART <ref type="bibr" target="#b12">(Lewis et al., 2019)</ref> and T5 <ref type="bibr" target="#b18">(Raffel et al., 2019)</ref> as the baseline text-to-text generation models. a) BART: BART is an encoder-decoder-based transformer model which is mainly pre-trained for text generation tasks such as summarization and</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Aspect-based sentiment analysis with opinion tree generation</title>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Zhongqing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoushan</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2022/561</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22</title>
		<meeting>the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4044" to="4050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Identifying leading indicators of product recalls from online reviews using positive unlabeled learning and domain adaptation</title>
		<author>
			<persName><forename type="first">Shreesh</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aron</forename><surname>Culotta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International AAAI Conference on Web and Social Media</title>
		<meeting>the International AAAI Conference on Web and Social Media</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="480" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1007379606734</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving customer complaint management by automatic email classification using linguistic style features as predictors</title>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="870" to="882" />
			<date type="published" when="2008">2008</date>
			<publisher>Kristof Coussement and Dirk Van den Poel</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multi-task learning with deep neural networks: A survey</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Crawshaw</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09796</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2010.00904</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Autoregressive entity retrieval</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">All nlp tasks are generation tasks: A general pretraining framework</title>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Universals and cultural differences in the judgments of facial expressions of emotion</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maureen</forename><forename type="middle">O</forename><surname>Wallace V Friesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>'sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Diacoyanni-Tarlatzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Heider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">Ayhan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Lecompte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pio</forename><forename type="middle">E</forename><surname>Pitcairn</surname></persName>
		</author>
		<author>
			<persName><surname>Ricci-Bitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">712</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs</title>
		<author>
			<persName><forename type="first">Jena</forename><forename type="middle">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Complaint identification in social media with transformer networks</title>
		<author>
			<persName><forename type="first">Mali</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Aletras</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.157</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020</title>
		<meeting>the 28th International Conference on Computational Linguistics, COLING 2020<address><addrLine>Barcelona, Spain (Online)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-08">2020. December 8-13, 2020</date>
			<biblScope unit="page" from="1765" to="1771" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">2021a. Modeling the severity of complaints in social media</title>
		<author>
			<persName><forename type="first">Mali</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Aletras</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.180</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">June 6-11, 2021</date>
			<biblScope unit="page" from="2264" to="2274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">2021b. Modeling the severity of complaints in social media</title>
		<author>
			<persName><forename type="first">Mali</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Aletras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<biblScope unit="page" from="2264" to="2274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
			<pubPlace>Bart</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Seq2Path: Generating sentiment tuples as paths of a tree</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoying</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longjun</forename><surname>Cai</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-acl.174</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2215" to="2225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatically identifying complaints in social media</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Preotiuc-Pietro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Gaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Aletras</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1495</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5008" to="5019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving depression level estimation by concurrently learning emotion intensity</title>
		<author>
			<persName><forename type="first">Arbaaz</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gael</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriparna</forename><surname>Hasanuzzaman</surname></persName>
		</author>
		<author>
			<persName><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="47" to="59" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>text transformer</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">CEM: commonsense-aware empathetic response generation</title>
		<author>
			<persName><forename type="first">Sahand</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chujie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<idno>CoRR, abs/2109.05739</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cem: Commonsense-aware empathetic response generation</title>
		<author>
			<persName><forename type="first">Sahand</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chujie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="11229" to="11237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Atomic: An atlas of machine commonsense for ifthen reasoning</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Allaway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Roof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3027" to="3035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">2023a. Complaint and severity identification from online financial content</title>
		<author>
			<persName><forename type="first">Apoorva</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriparna</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Social Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tanmay Sen, and Sriparna Saha. 2023b. Federated multi-task learning for complaint identification using graph attention network</title>
		<author>
			<persName><forename type="first">Apoorva</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddarth</forename><surname>Chandrasekar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Artificial Intelligence</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">2022a. Adversarial multi-task model for emotion, sentiment, and sarcasm aided complaint detection</title>
		<author>
			<persName><forename type="first">Apoorva</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arousha</forename><surname>Nazir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriparna</forename><surname>Saha</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-99736-6_29</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval -44th European Conference on IR Research, ECIR 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Stavanger, Norway</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">April 10-14, 2022</date>
			<biblScope unit="page" from="428" to="442" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">2022b. Adversarial multi-task model for emotion, sentiment, and sarcasm aided complaint detection</title>
		<author>
			<persName><forename type="first">Apoorva</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arousha</forename><surname>Nazir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriparna</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="428" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Are you really complaining? a multi-task framework for complaint identification, emotion, and sentiment classification</title>
		<author>
			<persName><forename type="first">Apoorva</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriparna</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="715" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Federated multi-task learning for complaint identification from social media data</title>
		<author>
			<persName><forename type="first">Apoorva</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanmay</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriparna</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Hasanuzzaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd ACM Conference on Hypertext and Social Media</title>
		<meeting>the 32nd ACM Conference on Hypertext and Social Media</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Interlanguage pragmatics: Requests, complaints, and apologies</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Trosborg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Walter de Gruyter</publisher>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Automating Knowledge Distillation and Representation from Richly Formatted Data</title>
		<author>
			<persName><forename type="first">Sen</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Stanford University</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A unified generative framework for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR, abs/2106.04300</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Detecting customer complaint escalation with recurrent neural networks and manually-engineered features</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luchen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunwei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anqi</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muzi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-2008</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="56" to="63" />
		</imprint>
	</monogr>
	<note>Industry Papers</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pre-trained bert-gru model for relation extraction</title>
		<author>
			<persName><forename type="first">Rongli</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 8th International Conference on Computing and Pattern Recognition</title>
		<meeting>the 2019 8th International Conference on Computing and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="453" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Constrained sequence-to-tree generation for hierarchical text classification</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Mao</surname></persName>
		</author>
		<idno type="DOI">10.1145/3477495.3531765</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;22</title>
		<meeting>the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;22<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1865" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
