<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Multi-document Summarization with Holistic Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
							<email>haopeng@ifmlab.org</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IFM lab</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sangwoo</forename><surname>Cho</surname></persName>
							<email>swcho@global.tencent.com</email>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaiqiang</forename><surname>Song</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
							<email>hongweiw@global.tencent.com</email>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
							<email>jiawei@ifmlab.org</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IFM lab</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Work done during Haopeng Zhang&apos;s internship at Tencent AI Lab Seattle</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Multi-document Summarization with Holistic Inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7B4B3F02AA02E237A2BFDA15DA332C2C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-document summarization aims to obtain core information from a collection of documents written on the same topic. This paper proposes a new holistic framework for unsupervised multi-document extractive summarization. Our method incorporates the holistic beam search inference method associated with the holistic measurements, named Subset Representative Index (SRI). SRI balances the importance and diversity of a subset of sentences from the source documents and can be calculated in unsupervised and adaptive manners. To demonstrate the effectiveness of our method, we conduct extensive experiments on both small and large-scale multi-document summarization datasets under both unsupervised and adaptive settings. The proposed method outperforms strong baselines by a significant margin, as indicated by the resulting ROUGE scores and diversity measures. Our findings also suggest that diversity is essential for improving multi-document summary performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The multi-document summarization (MDS) is one of the essential tools to obtain core information from a collection of documents written for the same topic. It seeks to find the main ideas from multiple sources with diversified messages. In spite of recent advances in MDS system designs <ref type="bibr" target="#b26">(Mihalcea and Tarau, 2004;</ref><ref type="bibr">Liu and Lapata, 2019a;</ref><ref type="bibr" target="#b39">Xiao et al., 2022)</ref>, three major challenges hinder its development:</p><p>First, existing extractive multi-document summarization systems rely on optimization with individual scoring. It becomes sub-optimal when we need to extract multiple summary sentences <ref type="bibr" target="#b48">(Zhong et al., 2020)</ref>. A typical individual system scores each candidate summary with only measurements of the newly added sentences during inference. Exxon-Mobil merger talks continues, and the stocks of both companies surge.</p><p>Oil prices are lowest in 12 years and future exploration will be costly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exxon Corp and Mobil Corp are reported</head><p>to be discussing a bussiness merger.</p><p>Other oil companies have merged to compensate for low oil prices and increasing costs of oil exploration. In contrast, the holistic system simultaneously measures all summary sentences and the relations among them. Despite recent efforts in holistic methods on a single document <ref type="bibr" target="#b0">(An et al., 2022;</ref><ref type="bibr" target="#b48">Zhong et al., 2020)</ref>, how to extract sentences holistically for multi-document summarization remains open.</p><p>In this work, we propose an inference method that holistically optimizes the extractive summary under multi-document setting. Second, multi-document summarization naturally contains excessively redundant information <ref type="bibr" target="#b17">(Lebanoff et al., 2018</ref>). An ideal summary should provide important information with diversified perspectives <ref type="bibr" target="#b29">(Nenkova and McKeown, 2011)</ref>.</p><p>In Figure <ref type="figure" target="#fig_1">1</ref>, we show a salient and diversified summary versus a salient but redundant summary. A salient and diversified summary often covers the information thoroughly, while the salient but redundant summary is usually incomplete. Different from existing approaches <ref type="bibr" target="#b36">(Suzuki and Nagata, 2017;</ref><ref type="bibr">Cho et al., 2019b;</ref><ref type="bibr" target="#b40">Xiao and Carenini, 2020)</ref> for limiting the repetitions, we introduce Subset Representative Index (SRI), a holistically balanced measurement between importance and diversity for extractive multi-document summarization.</p><p>Finally, recent deep learning-based supervised summarization methods are data-driven and require a massive number of high-quality summaries in the training data. Nevertheless, hiring humans to write summaries is always expensive, time-consuming, and thus hard to scale up. This problem becomes more severe for multi-document summarization, since it requires more effort to read more documents. Therefore, existing multi-document summarization datasets are either small-scale <ref type="bibr" target="#b30">(Over and Yen, 2004;</ref><ref type="bibr" target="#b7">Dang and Owczarzak, 2008)</ref> or created by acquiring data from the Internet with automatic alignments <ref type="bibr" target="#b10">(Fabbri et al., 2019;</ref><ref type="bibr">Antognini and Faltings, 2020)</ref> that could be erroneous. Here we propose an unsupervised multi-document summarization method to tackle the low-resource issue. It can further benefit the unsupervised multidocument summarization, with the adaptive setting using large-scale high-quality single-document summarization data (e.g., <ref type="bibr">CNN/DailyMail (Hermann et al., 2015)</ref>).</p><p>In this work, we present a novel framework for unsupervised extractive multi-document summarization, aiming to holistically select the extractive summary sentences. The framework contains the holistic beam search inference method associated with the holistic measurements named SRI ( Subset Representative Index). The SRI is designed as a holistic measurement for balancing the importance of individual sentences and the diversity among sentences within a set. To address data sparsity, we propose to calculate SRI in both unsupervised and adaptive manners. Unsupervised SRI relies on the centrality from graph-based methods <ref type="bibr" target="#b9">(Erkan and Radev, 2004;</ref><ref type="bibr" target="#b26">Mihalcea and Tarau, 2004)</ref> for subset importance measurement, while adaptive SRI uses BERT <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref> fine-tuned on single document summarization (SDS) corpus for sentence importance measurement. Our method shows performance improvements in both the summary informativeness and diversity scores, indicating our approach can achieve better coverage of documents while maintaining the gist information of multi-documents. We highlight the contributions of our work as follows:</p><p>• We propose a novel holistic framework for multi-document extractive summarization.</p><p>Our framework incorporates a holistic inference method for summary sentence extraction and holistic measurement called Subset Representative Index (SRI) for balancing the importance and diversity of a subset of sentences.</p><p>• We propose two unsupervised ways to measure SRI by using graph-based centrality or</p><p>Step 1</p><p>Step 2</p><p>Step 3 ...</p><p>Step 1</p><p>Step 2</p><p>Step 3 ... adapting from a single document corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised</head><p>• We conduct extensive experiments on several benchmark datasets, and the results demonstrate the effectiveness of our paradigm under both unsupervised and adaptive settings. Our findings suggest that effectively modeling sentence importance and pairwise sentence similarity is crucial for extracting diverse summaries and improving summarization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Multi-document Summarization Traditional non-neural approaches to multi-document summarization have been both extractive <ref type="bibr" target="#b2">(Carbonell and Goldstein, 1998;</ref><ref type="bibr" target="#b9">Erkan and Radev, 2004;</ref><ref type="bibr" target="#b26">Mihalcea and Tarau, 2004)</ref> and abstractive <ref type="bibr" target="#b11">(Ganesan et al., 2010)</ref>. Recent neural MDS systems rely on Transformer-based encoder-decoder model to process the integrated long documents with hierarchical inter-paragraph attention <ref type="bibr">(Liu and Lapata, 2019a;</ref><ref type="bibr" target="#b10">Fabbri et al., 2019)</ref>, or attention across representations of different granularity <ref type="bibr" target="#b15">(Jin et al., 2020)</ref>. This work focuses on unsupervised MDS scenarios where gold reference summaries are unavailable. Prior unsupervised MDS systems are mostly graph-based <ref type="bibr" target="#b9">(Erkan and Radev, 2004;</ref><ref type="bibr" target="#b21">Liu et al., 2021)</ref>. Similar to our adaptive setting, <ref type="bibr" target="#b17">Lebanoff et al. (2018)</ref> proposed to adapt the encoder-decoder framework from a single document corpus, but our work focuses on extractive summarization setting with holistic inference.</p><p>Sentence Importance Measurements Most works formulate extractive summarization as a sequence classification problem and use sequen-tial neural models with different encoders like recurrent neural networks <ref type="bibr" target="#b3">(Cheng and Lapata, 2016;</ref><ref type="bibr" target="#b27">Nallapati et al., 2016)</ref> and pre-trained language models <ref type="bibr">(Liu and Lapata, 2019b;</ref><ref type="bibr">Zhang et al., 2023b)</ref>. The prediction probabilities are treated as the importance measurement of sentences. On the other hand, unsupervised graphbased methods calculate the importance of sentences with node centrality and rank them for the summaries, including TextRank <ref type="bibr" target="#b26">(Mihalcea and Tarau, 2004)</ref>, LexRank <ref type="bibr" target="#b9">(Erkan and Radev, 2004)</ref>, PACSUM <ref type="bibr" target="#b47">(Zheng and Lapata, 2019)</ref>, and its variants <ref type="bibr" target="#b19">(Liang et al., 2021;</ref><ref type="bibr" target="#b21">Liu et al., 2021)</ref>. Recent researches <ref type="bibr" target="#b41">(Xu et al., 2019;</ref><ref type="bibr" target="#b38">Wang et al., 2020;</ref><ref type="bibr" target="#b42">Zhang et al., 2022</ref><ref type="bibr">Zhang et al., , 2023a) )</ref>   <ref type="bibr" target="#b2">(Carbonell and Goldstein, 1998)</ref>, Determinantal Point Process (DPP) <ref type="bibr" target="#b16">(Kulesza and Taskar, 2012)</ref>, and submodular selection <ref type="bibr" target="#b21">(Lin et al., 2009)</ref>. Trigram blocking is introduced to explicitly reduce redundancy by avoiding sentences that share a 3-gram with the previously added one <ref type="bibr">(Liu and Lapata, 2019b)</ref>. <ref type="bibr" target="#b32">Paulus et al. (2017)</ref> first adopt trigram blocking in decoding for abstractive summarization. <ref type="bibr" target="#b25">Ma et al. (2016)</ref> proposed the sentence filtering and beam search methods to for extractive summarization sentence selection. <ref type="bibr" target="#b40">Xiao and Carenini (2020)</ref> conducted a systematic study of redundancy in long documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>This section provides a detailed description of our proposed holistic MDS summarization framework.</p><p>We first explain how we formulate the MDS problem holistically in Section 3.1. The overall architecture of our holistic framework is shown in Figure <ref type="figure" target="#fig_2">2</ref>, which includes holistic inference methods for summary sentence extraction in Section 3.2, and a new holistic measurement, the Subset Representative Index (SRI) in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Multi-document summarization typically takes a collection of n documents D = {D (1) , . . . , D (n) } as inputs. Each document contains a varying number of sentences</p><formula xml:id="formula_0">D (i) = {s (i) 0 , . . . , s<label>(i)</label></formula><p>l i }, where l i is the number of sentences in the i-th document. Let S be the collection of all sentences, i.e. S = D (1) ∪ • • • ∪ D (n) . Additionally, let e i,j denote the similarity score between sentence s i and sentence s j . Our goal is to select a representative subset of sentences S ′ ⊂ S that maximizes the total importance of the subset while minimizing the redundancy within sentences in the subset at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Holistic Inference</head><p>Most existing approaches for unsupervised extractive summarization formulate it as an individual sentence ranking problem. They first calculate a measurement M(s i ) (e.g. sentence importance) for each sentence s i ∈ S and rank all sentences in S accordingly. For summary inference, they directly use an individual greedy method that adds one sentence with the highest ranking at a time until the desired total number of summary sentences is reached. In contrast, a holistic summarization method should evaluate a subset of sentences M(S ′ ) as a whole, then select the best subset S ′ . The setting formulates the holistic summary inference into a best subset selection problem, which has exponential time complexity.</p><p>To address the exponential time complexity issue, we propose several holistic inference methods for summary sentence extraction. These methods optimize subsets of sentences using subset measurements, as opposed to the individual greedy inference method. We describe the different variants of the proposed method as below.</p><p>Holistic Greedy Method. The most straightforward way to address the exponential time complexity issue is to adopt a greedy approach. Similar to the individual greedy method, the holistic greedy method also adds one sentence at a time. However, it picks the sentence using a subset measurement that takes into account the previously selected sentences. Formally, at each step, the method selects the sentence that maximizes the following objective:</p><formula xml:id="formula_1">argmax s i ∈S\S ′ M(S ′ ∪ {s i }),<label>(1)</label></formula><p>where S ′ represents the previously selected sentences.</p><p>Holistic Exhaustive Search. It is a brute-force method that considers every possible subset with the desired number of sentences. However, due to the exponential computation time, it is necessary to first filter out low-importance candidates using M({s i }) to reduce the search space.</p><p>Holistic Beam Inference . We also propose Holistic Beam Inference which balances the tradeoff between search space size and efficiency. It is a more advanced holistic inference method that adapts the beam-search decoding algorithm. We illustrate the algorithm in Algorithm 1. At each step, it considers the top-k candidate subsets, which enlarges the search space and therefore has a higher chance of finding a better subset solution compared to the holistic greedy method. Meanwhile, the algorithm has linear time complexity, making it more efficient than the holistic exhaustive search method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Subset Representative Index</head><p>To complement the holistic inference methods, we propose a new subset measurement, Subset Representative Index (SRI), denoted as M(S ′ ). It balances the importance measurement I(S ′ ) and redundancy measurement R(S ′ ).</p><p>An ideal extractive summary should select the most representative subset from a collection of the input sentences, maximizing the total nonredundant salient information passed to the user. SRI is a holistic subset measurement that balances the importance and redundancy of a subset of sentences from the source documents. Formally, we define SRI as below:</p><formula xml:id="formula_2">M(S ′ ) =I(S ′ ) -λ • R(S ′ ),<label>(2)</label></formula><p>where I(S ′ ) measures the informativeness of a set of sentences, and R(S ′ ) measures the redundancy within the set. The parameter λ is used to control the weight of the redundancy in the overall SRI score. We detail the methods for measuring the set importance and redundancy in an unsupervised manner as follows.</p><p>Graph-Based Importance Measurement. To measure the importance of sentences, we use a graph-based approach. We construct a graph G = (V, E), where node v i ∈ V represents sentence for X ∈ C do 5:</p><formula xml:id="formula_3">X ′ ← arg-top-k-max s∈S\X M(X ∪ {s}) 6:</formula><p>for x ∈ X ′ do 7:</p><p>Add X ∪ {x} to C ′ 8:</p><p>end for 9:</p><p>end for 10:</p><p>C ← arg-top-k-max X ∈C ′ M(X ) 11: end for 12: return argmax X ∈C M(X ) s i ∈ S, and edge e i,j ∈ E represents the similarity between sentence s i and s j . Our proposed approach for sentence similarity score employs a combination of two methods: TF-IDF and Sentence-BERT <ref type="bibr" target="#b34">(Reimers and Gurevych, 2019)</ref>. TF-IDF is used to encode sentences with surface-form similarity, while Sentence-BERT is used to encode sentences with semantic similarity:</p><formula xml:id="formula_4">e i,j = α • c ⊤ i c j + (1 -α) • r ⊤ i r j ,<label>(3)</label></formula><p>where c i , c j , r i and r j are the corresponding TF-IDF features and sentence embeddings for the i-th and j-th sentences, respectively. The weight term α ∈ [0, 1] is a configurable hyperparameter to balance between statistical similarity and contextualized similarity.</p><p>Inspired from <ref type="bibr" target="#b26">(Mihalcea and Tarau, 2004;</ref><ref type="bibr" target="#b9">Erkan and Radev, 2004)</ref>, we define the importance of a sentence as its node centrality in the graph, which is calculated as the sum of the weights of edges connected to the node representing this sentence:</p><formula xml:id="formula_5">I(s i ) = s j ∈S\s i e i,j .<label>(4)</label></formula><p>Similarly, the importance of a subset of sentences is defined as the total weights between the subgraph and the remaining graph:</p><formula xml:id="formula_6">I(S ′ ) = 1 |S| -|S ′ | s i ∈S ′ ,s j ∈S\S ′ e i,j ≈ 1 |S| s i ∈S ′ ,s j ∈S\S ′ e i,j .<label>(5)</label></formula><p>Since |S ′ | is usually far smaller than |S| in summarization tasks, we can approximate the denominator by using |S| directly. This way, the subset importance only takes into account the relationship of the subset with the remaining sentences, rather than considering dependencies within the subset.</p><p>Adaptive Importance Measurement. In spite of the data sparsity issue in MDS, the Single Document Summarization (SDS) task has abundant high-quality labeled data <ref type="bibr" target="#b13">(Hermann et al., 2015;</ref><ref type="bibr" target="#b28">Narayan et al., 2018;</ref><ref type="bibr" target="#b6">Cohan et al., 2018)</ref>. We propose a method called adaptive importance measurement, which adapts SDS data for MDS importance measurement. This method utilizes the labeled data from SDS to train a model for predicting the importance of sentences in MDS.</p><p>In the adaptive setting, we fine-tune the BERT <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref> to a sentence importance scorer on SDS datasets, and then adapt the fine-tuned model to the target MDS datasets. Specifically, we first calculate the normalized salience of a sentence as:</p><p>f</p><formula xml:id="formula_7">(s i ) = v ⊤ tanh (W 1 r i ) , salience (s i ) = f (s i ) s j ∈D f (s j ) , (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>where W is a trainable weight, and r i is the contextualized representation of sentence s i . Then, we fine-tune BERT to minimize the following loss:</p><formula xml:id="formula_9">R (s i ) = softmax (ROUGE (s i )) , L = - D s i ∈D R (s i ) log salience(s i ). (7)</formula><p>The fine-tuned BERT can be directly adapted to the MDS datasets and calculate the adaptive importance measurement for sentences.</p><p>Redundancy Measurement. The redundancy measurement for a subset of sentences S ′ is defined as the total similarity score of each sentence with its most similar counterpart. This measurement captures the degree of overlap between the sentences in the subset, indicating the level of redundancy present in the selected sentences:</p><formula xml:id="formula_10">R(S ′ ) = s i ∈S ′ max s j ∈S ′ \{s i } e i,j .<label>(8)</label></formula><p>Overall, we can calculate SRI in both unsupervised and adaptive manners. Our holistic framework extracts summaries as a whole with the holistic inference method, which is guided by SRI to measure the importance and redundancy of a subset of sentences. This approach allows us to balance the importance and redundancy of a summary, making it more informative and coherent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we provide details on our experimental setup, including the datasets, evaluation metrics, baselines, and implementation details (Section 4.1). We then present the results of our model on benchmark MDS datasets in both unsupervised (Section 4.2) and adaptive (Section 4.3) settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head><p>Dataset. We evaluate our unsupervised method on benchmark multi-document summarization datasets. Particularly, we use MultiNews <ref type="bibr" target="#b10">(Fabbri et al., 2019)</ref>, WikiSum <ref type="bibr">(Liu et al., 2018)</ref>, DUC-04 <ref type="bibr" target="#b30">(Over and Yen, 2004)</ref>, and TAC-11 (Dang and Owczarzak, 2008) datasets. MultiNews is collected from a diverse set of news articles on newser.com. It is a large-scale dataset containing reference summaries written by professional editors. WikiSum is another large-scale dataset that provides documents and summaries from Wikipedia webpages where the documents come from the reference webpages of Wikipedia articles and top-10 Google searches, and the summaries are the lead section of the Wikipedia articles. We use the top-40 highranked paragraphs for the document inputs following <ref type="bibr">(Liu and Lapata, 2019a)</ref>.</p><p>For summary extraction, we use the average number of reference sentences: 10 and 5, respectively on MultiNews and WikiSum. For the DUC and TAC datasets, the task is to generate a succinct summary of up to 100 words from a set of 10 news articles. We report results on DUC-04 and TAC-11, which are standard test sets used in previous studies <ref type="bibr" target="#b14">(Hong et al., 2014;</ref><ref type="bibr">Cho et al., 2019a)</ref>. DUC-03 and TAC-08/09/10 are used for the validation set to tune hyper-parameters. For adaptive setting, we fine-tune BERT on single document summarization dataset CNN/DailyMail <ref type="bibr" target="#b13">(Hermann et al., 2015)</ref> and directly adapt to MDS test sets. Table <ref type="table" target="#tab_1">1</ref> shows the statistics of the datasets in detail. Evaluation Metrics. The extracted summaries are evaluated against human reference summaries using ROUGE <ref type="bibr" target="#b20">(Lin, 2004)</ref> <ref type="foot" target="#foot_0">1</ref> for the summarization quality. We report ROUGE-1, ROUGE-2, ROUGE-SU4, and ROUGE-L<ref type="foot" target="#foot_1">2</ref> that respectively measure the overlap of unigrams, bigrams, skip bigrams with a maximum distance of 4 words, and the longest common sequence between extracted summary and reference summary. To align with previous works, we report R-1, R-2, R-L for Multinews and Wikisum datasets, and R-1, R-2, R-SU4 for DUC and TAC datasets. For all baseline methods, we report ROUGE results from their original papers if available or use results reported in <ref type="bibr">(Cho et al., 2019a;</ref><ref type="bibr" target="#b21">Liu et al., 2021)</ref>. We also report the measure of diversity for the generated summaries by calculating a unique n-gram ratio <ref type="bibr" target="#b40">(Xiao and Carenini, 2020;</ref><ref type="bibr" target="#b33">Peyrard et al., 2017)</ref> defined as:</p><formula xml:id="formula_11">uniq n-gram ratio = # uniq-n-gram #n-gram<label>(9)</label></formula><p>Baselines. We compare our methods with strong unsupervised summarization baselines. In particular, MMR <ref type="bibr" target="#b2">(Carbonell and Goldstein, 1998)</ref> combines query relevance with information novelty in the context of summarization. LexRank <ref type="bibr" target="#b9">(Erkan and Radev, 2004)</ref> computes sentence importance based on eigenvector centrality in a graph representation of sentences. TextRank <ref type="bibr" target="#b26">(Mihalcea and Tarau, 2004)</ref> adopts PageRank <ref type="bibr" target="#b31">(Page et al., 1999)</ref> to compute node centrality recursively based on a Markov chain model. SumBasic <ref type="bibr" target="#b37">(Vanderwende et al., 2007)</ref> is an extractive approach assuming words frequently occurring in a document cluster are more likely to be included in the summary. KL-Sum <ref type="bibr" target="#b12">(Haghighi and Vanderwende, 2009)</ref> uses a greedy approach to add a sentence to the summary to minimize the KL divergence. PRIMERA <ref type="bibr" target="#b39">(Xiao et al., 2022</ref>) is a pyramid-based pre-trained model for MDS that achieves state-of-the-art performance.</p><p>We compare it under its zero-shot setting. Implementation Details. We run all experiments with 88 Intel(R) Xeon(R) CPUs. We combine the surface indicator based on TF-IDF and contextualized embeddings. We treat each document clus-ter as a corpus and each sentence as a document when calculating the TF-IDF scores. We employ the pre-trained sentence-transformer <ref type="bibr" target="#b34">(Reimers and Gurevych, 2019)</ref> and extract sentence representations using a checkpoint of 'all-mpnet-base-v2'.</p><p>The graph edges with low similarity are treated as disconnected to emphasize the connectivity of the graph and avoid noisy edge connections. We keep a threshold ẽ for edge weights such that edges with similarity scores smaller than ẽ will be set to 0. Here ẽ is controlled by a hyper-parameter to be tuned according to datasets. The final representation of edge weight between two sentences</p><formula xml:id="formula_12">(s i , s j ) is e i,j = max(sim(s i , s j ) -ẽ, 0),<label>(10)</label></formula><p>where ẽ = min(e) + θ (max(e) -min(e)) is the threshold controlled by hyper-parameter θ. For exhaustive search, we filter out the sentences with low centrality and only keep the top 15 sentences at inference. All hyper-parameters are tuned on validation sets on MultiNews and WikiSum and training sets on DUC and TAC. The best parameters are selected based on the highest R-1 score. More specific, for the balancing factor λ in SRI, we use {2 -13 , 2 -7 , 2 -4 , 2 -6 } on DUC, TAC, MultiNews and Wik-iSum dataset. For α that weighted the contributions of TF-IDF and contextualized sentence similarity, we use 0.9 on News domain datasets and 0.8 on the WikiSum dataset. The edge weight threshold θ is {0, 0, 0.1, 0.1} for DUC, TAC, MultiNews and WikiSum. As for beam search, we use beam size {4, 4, 4, 3} on the corresponding datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Unsupervised Summarization Results</head><p>The unsupervised summarization results on four benchmark MDS datasets are shown in Table <ref type="table" target="#tab_2">2</ref>.</p><p>The summarization performance of our method outperforms strong unsupervised baselines. Note that MultiNews and WikiSum datasets provide abundant training samples and contain shorter input than the DUC or TAC datasets. Our method performs better than the pre-trained model, PRIMERA with a zero-shot setting. Compared to the baseline (Sent. greedy) that extracts sentences solely based on importance, balancing diversity with SRI boosts performance by a large margin.</p><p>For the DUC-04 and TAC-11 datasets, our proposed methods outperform unsupervised baselines by a large margin. It demonstrates that balancing the summary informativeness and diversity during <ref type="bibr">LEAD 30.77 8.27 7.35 32.88 7.84 11.46 39.41 11.77 14.51 37.63 14.75 33.76 MMR (1998</ref><ref type="bibr">) 30.14 4.55 8.16 31.43 6.14 11.16 38.77 11.98 12.91 31.22 10.24 22.48 LexRank (2004</ref><ref type="bibr">) 34.44 7.11 11.19 33.10 7.50 11.13 38.27 12.70 13.20 36.12 11.67 22.52 TextRank (2004) 33.16 6.13 10.16 33.24 7.62 11.27 38.44 13.10 13.50</ref>  the sentence extraction process is crucial for better summary quality. Note that the input length of DUC/TAC datasets is extremely long spanning an average of 180 sentences. These long input easily exceeds the input capacity of transformer-based models possibly resulting in information loss from documents. The proposed methods on the other hand process documents regardless of the input length or formats (SDS or MDS). Also, our unsupervised methods have the advantage of processing datasets with small training data. The supreme performances on datasets with different input lengths and low-resource data illustrate the effectiveness of our methods. To further verify the model performance, we also conduct a human evaluation by experts on a scale of 5. The results shown in Table <ref type="table" target="#tab_3">3</ref> also prove our method outputs better summaries in unsupervised setting.</p><formula xml:id="formula_13">DUC-04 TAC-11 MultiNews WikiSum System R-1 R-2 R-SU R-1 R-2 R-SU R-1 R-2 R-L R-1 R-2 R-L* Unsupervised Systems</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Adptive Summarization Results</head><p>The experimental results under the adaptive setting are shown in Table <ref type="table" target="#tab_4">4</ref>. Compared to large pretrained generation model (BART) and other taskspecific pre-trained summarization models (PEGA-SUS, PRIMERA), our framework shows strong performance when adapting from a single document summarization dataset. We also notice fine- tuning on single document summarization corpus improves the performance of all pre-trained models, but still, our framework achieves the best results under the adaptive setting.</p><formula xml:id="formula_14">DUC-04 MultiNews System R-1 R-2 R-L R-1 R-2 R</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Summary Diversity</head><p>Other than summary quality, we also test the effectiveness of our SRI in terms of the diversity of the output summaries. We present the unique n-gram ratios of output summaries under unsupervised and adaptive settings and the reference summary on the TAC-11 dataset in Figure <ref type="figure" target="#fig_4">3</ref>. According to the results, our framework is extremely effective in reducing summary redundancy and increasing summary diversity under both unsupervised and adaptive settings. Compared to the ROUGE-F1 results, holistic inference with importance-diversity balancing measurement SRI increases both summary quality and diversity at the same time. The results suggest that considering summary diversity is beneficial in extractive summarization, especially in redundant cases like MDS and long document summarization. Our finding also verifies the crucial rule of effective modeling of sentence importance and similarity. To test the robustness of our proposed approaches, we study the hyperparameter sensitivity of our proposed methods. The results are shown in Figure <ref type="figure" target="#fig_5">4</ref>. The first plot shows the impact of balancing factor λ in SRI. The second plot shows the impact of α, which balances the contextualized and TF-IDF sentence embedding and the edge weight threshold. The results show that our methods are relatively stable towards the hyperparameter values and could be easily adapted to unseen datasets. Table <ref type="table">5</ref>: ROUGE-F1 (w/o word limit) results of SRIbeam with different beam sizes on TAC with λ = 0.125.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hyperparameter Sensitivity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Inference Approaches Analysis</head><p>We also compare the efficiency and effectiveness of different inference methods. As in Figure . 5, we compare sentence-level greedy search, set-level greedy search, set-level beam search (beam size = 4), and set-level exhaustive search with pre-filtering as inference methods for both unsupervised and adaptive settings. We pick the filter size of 20 here since the search space without filtering C(N, K) is extremely large. According to the results, all setlevel inference methods outperform the sentencelevel methods. This suggests that extracting summaries at a set level (holistic) is optimal over the common sentence-level setting that extracts sentences individually. The finding is also consistent with the inherent performance gap between sentence-level and holistic extractors in <ref type="bibr" target="#b48">(Zhong et al., 2020)</ref>. Moreover, we realize the set-level beam search and set-level exhaustive search achieve the comparable best performance. However, set-level beam search speed-wise is much more efficient than setlevel exhaustive search. We also show the effect of different beam sizes in Table <ref type="table">5</ref>. The results indicate that a reasonably small beam size achieves the best ROUGE results, which are both effective and efficient. To conclude, set-level beam search with SRI shows the best overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper proposes a holistic framework for unsupervised multi-document extractive summarization. Our framework incorporates the holistic beam search inference methods and SRI, a holistically balanced measurement between importance and diversity. We conduct extensive experiments on both small and large-scale MDS datasets under both unsupervised and adaptive settings and the proposed method outperforms strong baselines by a large margin. We also find that balancing summary set importance and diversity benefits both the quality and diversity of output summaries for MDS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>The proposed framework in this paper is mainly designed for low-resource scenarios without gold summaries for multi-document summarization. Adapting the framework for a supervised setting requires further investigation. Recently, large language models (LLM) like ChatGPT have shown strong zero-shot summarization ability, which may raise doubt about the necessity of unsupervised summarization methods.</p><p>However, LLM suffers from the hallucination problem and MDS may exceed its input limit (e.g. 4,696 words for TAC) than the input limit of Chat-GPT (500-word/4,000-character). In contrast, unsupervised summarization methods can tackle input of arbitrary length and have a faster inference speed than ChatGPT when processing long input documents. In addition, a recent study <ref type="bibr">(Zhang et al., 2023c)</ref> shows that ChatGPT's extractive summarization performance is still inferior to existing supervised systems in terms of ROUGE scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Consideration</head><p>Our proposed framework forms summary by directly extracting sentences from source documents. Therefore, the extracted summary may be incoherent or contain unfactual co-references. In addition, the extracted summary will keep biased contents from the source sentences, if any.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of a diverse summary vs. a redundant summary. Sentences in the redundant summary have higher semantic similarity than a diverse summary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the proposed holistic framework for multi-document summarization. The individual inference only resorts to each candidate while the holistic inference is based on all candidates. Orange and Green indicate newly added sentences and already added ones to the summary respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1</head><label>1</label><figDesc>Holistic Beam InferenceInput: set of sentences S, Measurement M(•) Parameter: # summary sentences N &lt; |S|, beam size k Output: the selected subset S ′ 1: The candidate set C ← {∅} 2: for N times do 3:The beam set C ′ ← {∅} 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Unique n-gram ratios (n = 1, 2, 3, 4) of the output summary by different methods on TAC-11.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Average ROUGE-F1 (w/o word limit) results with different hyperparameter values on TAC-11.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Detailed statistics of four multi-document datasets. #test denotes the number of document clusters in the test set, #ref denotes the number of reference summaries, avg.word(doc) denotes the average number of words in the source document cluster, avg.word(sum) denotes the average number of words in the ground truth summary.</figDesc><table><row><cell>Dataset</cell><cell cols="4"># test # ref. avg.w/doc avg.w/sum</cell></row><row><cell>DUC-04</cell><cell>50</cell><cell>4</cell><cell>4, 636</cell><cell>109.6</cell></row><row><cell>TAC-11</cell><cell>44</cell><cell>4</cell><cell>4, 696</cell><cell>99.7</cell></row><row><cell cols="2">Multi-News 5, 622</cell><cell>1</cell><cell>2, 104</cell><cell>264.7</cell></row><row><cell>Wikisum</cell><cell cols="2">38, 205 1</cell><cell>2, 800</cell><cell>139.4</cell></row><row><cell cols="3">CNNDM(SDS) 11,489 1</cell><cell>766.1</cell><cell>58.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>ROUGE-F1 scores on four datasets under the unsupervised setting. Best unsupervised results are bold. For a fair comparison, we report R-L on Multinews and R-Lsum<ref type="bibr" target="#b35">(See et al., 2017)</ref> for WikiSum and limit summaries to 100 words on DUC-04 and TAC-11. R-L are marked with * if reporting ROUGE-Lsum numbers.</figDesc><table><row><cell>23.66 7.79 21.23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Human evaluation results on a scale of 1-5.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>ROUGE-F1 results on DUC-04 and Multinews datasets under the adaptive setting. Models adapted from CNN/DailyMail dataset are marked in the bracket.</figDesc><table><row><cell>-L</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Figure 5: Efficiency vs. average ROUGE (w/o word limit) scores of different inference methods on TAC-11.</figDesc><table><row><cell></cell><cell>19.25 19.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Sen. Greedy beam_graph(4) beam_adaptive(4) exh_graph(20) exh_adaptive(20)</cell></row><row><cell>avg. ROUGE-F1</cell><cell>18.50 18.75 19.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>18.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>18.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00</cell><cell>0.25</cell><cell>0.50</cell><cell cols="2">0.75 Time s/intance 1.00</cell><cell>1.25</cell><cell>1.50</cell><cell>1.75</cell></row><row><cell cols="2">Beam Size</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell cols="9">ROUGE-1 33.43 33.65 33.62 33.76 34.72 33.64 33.67</cell></row><row><cell cols="9">ROUGE-2 7.71 8.00 7.87 7.93 7.84 7.86 7.85</cell></row><row><cell cols="9">ROUGE-L* 28.74 29.03 28.99 29.10 29.01 28.94 29.01</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>w/ options -n</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>-m -w 1.2 -c 95 -r 1000 -l 100 for DUC/TAC 2 Due to some legacy issues, some baselines report the original ROUGE-L, others report ROUGE-Lsum.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Colo: A contrastive learning based re-ranking framework for one-stage summarization</title>
		<author>
			<persName><forename type="first">Chenxin</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14569</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th Language Resources and Evaluation Conference</title>
		<meeting>The 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2022. 2020</date>
			<biblScope unit="page" from="6645" to="6650" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Diego Antognini and Boi Faltings. European Language Resources Association</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">Longformer: The long-document transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The use of mmr, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 21st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07252</idno>
		<title level="m">Neural summarization by extracting sentences and words</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving the similarity measure of determinantal point processes for extractive multidocument summarization</title>
		<author>
			<persName><forename type="first">Sangwoo</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Lebanoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1098</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1027" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-document summarization with determinantal point processes and contextualized representations</title>
		<author>
			<persName><forename type="first">Sangwoo</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5412</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on New Frontiers in Summarization</title>
		<meeting>the 2nd Workshop on New Frontiers in Summarization<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="98" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soon</forename><surname>Doo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seokhwan</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazli</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Goharian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05685</idno>
		<title level="m">A discourse-aware attention model for abstractive summarization of long documents</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Overview of the TAC 2008 update summarization task</title>
		<author>
			<persName><forename type="first">Trang</forename><surname>Hoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karolina</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><surname>Owczarzak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Text Analysis Conference</title>
		<meeting>Text Analysis Conference</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">LexRank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName><forename type="first">Günes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1102</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1074" to="1084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Opinosis: A graph-based approach to abstractive summarization of highly redundant opinions</title>
		<author>
			<persName><forename type="first">Kavita</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING)</title>
		<meeting>the International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring content models for multi-document summarization</title>
		<author>
			<persName><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of human language technologies: The 2009 annual conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>human language technologies: The 2009 annual conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="362" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A repository of state of the art and competitive baseline summaries for generic news summarization</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Favre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA)</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1608" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-granularity interaction network for extractive and abstractive multi-document summarization</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Hanqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th annual meeting of the association for computational linguistics</title>
		<meeting>the 58th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6244" to="6254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Determinantal Point Processes for Machine Learning</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adapting the neural encoder-decoder framework from single to multi-document summarization</title>
		<author>
			<persName><forename type="first">Logan</forename><surname>Lebanoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiqiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06218</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving unsupervised extractive summarization with facet-aware modeling</title>
		<author>
			<persName><forename type="first">Xinnian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1685" to="1697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised extractive text summarization with distance-augmented sentence graphs</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shasha</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">Jingzhou</forename><surname>Ieee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominic</forename><forename type="middle">Jd</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2009">2009. 2021</date>
			<biblScope unit="page" from="2313" to="2317" />
		</imprint>
	</monogr>
	<note>2009 IEEE Workshop on Automatic Speech Recognition &amp; Understanding</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Generating wikipedia by summarizing long sequences</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><surname>Shazeer</surname></persName>
		</author>
		<idno>ArXiv, abs/1801.10198</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical transformers for multi-document summarization</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1500</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5070" to="5081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08345</idno>
		<title level="m">Text summarization with pretrained encoders</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An unsupervised multi-document summarization framework based on neural document model</title>
		<author>
			<persName><forename type="first">Shulei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunlun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1514" to="1523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Textrank: Bringing order into text</title>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 conference on empirical methods in natural language processing</title>
		<meeting>the 2004 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06023</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.08745</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic summarization</title>
		<author>
			<persName><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">An introduction to DUC-2004</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Yen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Stanford InfoLab</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04304</idno>
		<title level="m">A deep reinforced model for abstractive summarization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to score system summaries for better content selection evaluation</title>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Peyrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teresa</forename><surname>Botschen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on New Frontiers in Summarization</title>
		<meeting>the Workshop on New Frontiers in Summarization</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04368</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cutting-off redundant repeating generations for neural abstractive summarization</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Short Papers; Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="291" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beyond sumbasic: Task-focused summarization with sentence simplification and lexical expansion</title>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hisami</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1606" to="1618" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Heterogeneous graph neural networks for extractive document summarization</title>
		<author>
			<persName><forename type="first">Danqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yining</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.553</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6209" to="6219" />
		</imprint>
	</monogr>
	<note>Xipeng Qiu, and Xuanjing Huang</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">PRIMERA: Pyramid-based masked sentence pre-training for multi-document summarization</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.360</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5245" to="5263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Systematically exploring redundancy reduction in summarizing long documents</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00052</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14142</idno>
		<title level="m">Discourse-aware neural extractive text summarization</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">HEGEL: Hypergraph transformer for long document summarization</title>
		<author>
			<persName><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.emnlp-main.692</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10167" to="10176" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">2023a. Contrastive hierarchical discourse graph for scientific document summarization</title>
		<author>
			<persName><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.codi-1.4</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Computational Approaches to Discourse (CODI 2023)</title>
		<meeting>the 4th Workshop on Computational Approaches to Discourse (CODI 2023)<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="37" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">2023b. Diffusum: Generation enhanced extractive summarization with diffusion</title>
		<author>
			<persName><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.01735</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">2023c. Extractive summarization via chatgpt for faithful summary generation</title>
		<author>
			<persName><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.04193</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11328" to="11339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Sentence centrality revisited for unsupervised summarization</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03508</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Xipeng Qiu, and Xuanjing Huang</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqing</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08795</idno>
	</analytic>
	<monogr>
		<title level="m">Extractive summarization as text matching</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
