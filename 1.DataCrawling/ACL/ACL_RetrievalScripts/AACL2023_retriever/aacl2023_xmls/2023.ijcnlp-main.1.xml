<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Toward Unified Controllable Text Generation via Regular Expression Instruction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xin</forename><surname>Zheng</surname></persName>
							<email>zhengxin2020@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
							<email>hongyu@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
							<email>xianpei@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">State Key Laboratory of Computer Science Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Le Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">State Key Laboratory of Computer Science Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Toward Unified Controllable Text Generation via Regular Expression Instruction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E35F746B30822BDCD1E1E016E68907EE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T14:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Controllable text generation is a fundamental aspect of natural language generation, with numerous methods proposed for different constraint types. However, these approaches often require significant architectural or decoding modifications, making them challenging to apply to additional constraints or resolve different constraint combinations. To address this, our paper introduces Regular Expression Instruction (REI), which utilizes an instructionbased mechanism to fully exploit regular expressions' advantages to uniformly model diverse constraints. Specifically, our REI supports all popular fine-grained controllable generation constraints, i.e., lexical, positional, and length, as well as their complex combinations, via regular expression-style instructions. Our method only requires fine-tuning on mediumscale language models or few-shot, in-context learning on large language models, and requires no further adjustment when applied to various constraint combinations. Experiments demonstrate that our straightforward approach yields high success rates and adaptability to various constraints while maintaining competitiveness in automatic metrics and outperforming most previous baselines. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generating texts according to human requirements has long been a critical challenge in natural language generation <ref type="bibr">(Ziegler et al., 2019;</ref><ref type="bibr" target="#b40">Ouyang et al., 2022)</ref>. With the emergence of large language models, many tasks in natural language processing can be unified and converted into the formation of controllable generation <ref type="bibr" target="#b42">(Prabhumoye et al., 2020)</ref>. For example, text classification <ref type="bibr" target="#b3">(Apté et al., 1994)</ref>, cloze test <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref>, and multiple-choice question answering <ref type="bibr" target="#b23">(Lai et al., 2017)</ref> tasks constraint the output text to be exactly one of the given Lexicon &amp; length constraint Input &lt;expression&gt; &lt;mask_0&gt; stood(0) &lt;mask_1&gt; field(1) &lt;mask_2&gt; looking(2) &lt;mask_3&gt; &lt;length=10&gt; &lt;/expres-sion&gt; Output &lt;expression&gt; The_1 player_2 stood(0)_3 in_4 the_5 field(1)_6 looking(2)_7 at_8 the_9 batter_10 &lt;/expres-sion&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Position &amp; lexicon constraint</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Stephen was at a party. &lt;expression&gt; &lt;mask_0&gt; knocked(0) &lt;mask_1&gt; &lt;/expression&gt; He checked it but it was completely broken. Output &lt;expression&gt; Stephen knocked(0) over a vase while drunk. &lt;/expression&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Position constraint &amp; alternative ending</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>My friends all love to go to the club to dance. They think it's a lot of fun and always invite. I finally decided to tag along last Saturday. &lt;expression&gt; &lt;options&gt; &lt;choice_0&gt; &lt;mask_0&gt; My friends decided to keep inviting me out as I am so much fun. &lt;/choice_0&gt; &lt;choice_1&gt; &lt;mask_1&gt; The next weekend, I was asked to please stay home. &lt;/choice_1&gt; &lt;/options&gt; &lt;/expression&gt; Output &lt;expression&gt; I danced terribly and broke a friend's toe. The next weekend, I was asked to please stay home. &lt;/expression&gt; Table 1: Input and output of instruction prompt based Regular Expression Instruction (REI). REI can describe various types of complex fine-grain constraints, and here we present three examples. Meta-data instruction label is colored, lexicon constraints or correct choice is boldfaced, and auxiliary marks for length or lexicon uses gray color. options; abductive reasoning <ref type="bibr" target="#b6">(Bhagavatula et al., 2020)</ref> specifies that the position of the output text is between the previous and future contexts; summarization task <ref type="bibr" target="#b34">(Luhn, 1957)</ref> limits the length of output; machine translation <ref type="bibr" target="#b4">(Bar-Hillel, 1960)</ref> demands to use the vocabulary of the target language for text generation.</p><p>For controllable text generation, typical fine-grained control tasks include lexicon <ref type="bibr" target="#b28">(Lin et al., 2020)</ref>, generating position <ref type="bibr" target="#b47">(Shen et al., 2020)</ref> and length <ref type="bibr" target="#b8">(Carlsson et al., 2022)</ref>. Recently, various approaches have been proposed to satisfy these constraints, which can be categorized into three different paradigms: retraining or refactoring the model <ref type="bibr" target="#b18">(Keskar et al., 2019;</ref><ref type="bibr">Zhang et al., 2020;</ref><ref type="bibr" target="#b17">He, 2021;</ref><ref type="bibr">Chan et al., 2021a)</ref>; tuning on given data <ref type="bibr" target="#b24">(Lester et al., 2021;</ref><ref type="bibr">Stiennon et al., 2020a)</ref>; manually designed post-processing <ref type="bibr" target="#b43">(Qin et al., 2020</ref><ref type="bibr" target="#b44">(Qin et al., , 2022;;</ref><ref type="bibr" target="#b36">Meng et al., 2022;</ref><ref type="bibr" target="#b33">Lu et al., 2021</ref><ref type="bibr" target="#b32">Lu et al., , 2022;;</ref><ref type="bibr" target="#b56">Wang et al., 2021)</ref>.</p><p>Despite the reasonable performance, current methods on transformer-based language models mainly focus on certain constraints but may not be easily transferred to others, let alone the combination of constraints. For example, Non-Residual Prompting <ref type="bibr" target="#b8">(Carlsson et al., 2022)</ref> and A*esque Decoding <ref type="bibr" target="#b32">(Lu et al., 2022)</ref> only considered lexical and length constraints, but it cannot arbitrarily specify which position the generated text shall occur; on the other hand, COLD <ref type="bibr" target="#b44">(Qin et al., 2022)</ref> can generate text given past and future context, but may not add word inclusion constraint nor restrict the output length. Moreover, these controlling methods assume that we have access to the probability distribution or even gradient of the model, but in the case of large language models where we can only obtain the output token via API, these methods may not be available, and thus black-box controlling techniques need further exploration.</p><p>To address the above challenges, we proposed instruction-based Regular Expression Instruction (REI), for universal fine-grained controllable generation. Table <ref type="table">1</ref> present a few examples. Our instruction design is inspired by regular expression, which can easily describe mainstream constraints and their combinations. Following <ref type="bibr" target="#b46">Rosenbaum et al. (2022)</ref>, we use markup language to construct the expression, hoping that model can better distinguish between meta-data (instructions) and data (actual words). We use two popular paradigms, language model fine-tuning, and large language model few-shot, to teach the model to understand the input constraint expression.</p><p>Our method has several advantages. First, our constraint expression supports all typical finegrained controlling task and is powerful enough to describe composite control specifications. Second, our method can be adapted to various scenarios, such as summarization with length con-straint, terminology-constrained machine translation, and alternative-ending story infilling. Third, our method is easy to implement and highly transferrable to other models since it requires only finetuning on medium-size models and no further modification on large language models, and it does not need access to probability distribution or gradient.</p><p>Experiments demonstrate that current state-ofthe-art language models can understand our controlling language, achieving high success rate while maintaining high automatic evaluation metric score and surpassing most of the strong previous baselines under various constraints. We hope our work can shed light on future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Instruction Design</head><p>The controlling language REI follows the style of regular expression due to its expressiveness. Also, it's easy to evaluate whether the input expression instruction matches the generated text or not. Following <ref type="bibr" target="#b46">Rosenbaum et al. (2022)</ref>, HTML-like markup language is used, which helps the model learn that they are meaningful meta-data instructions rather than plain symbols, especially when using large language models in-context learning with limited examples and no parameter update. This markup label can also avoid the usage of the escape character.</p><p>REI contains several special labels, as shown in Table <ref type="table">1</ref>. &lt;expression&gt; and &lt;/expression&gt; mark the beginning and the end of the expression and can be put anywhere in the input text, assuming we only generate according to one expression at a time. &lt;mask_i&gt; is equivalent to the regular expression ".*" and similar to the mask token in BART <ref type="bibr" target="#b25">(Lewis et al., 2020)</ref> and T5 <ref type="bibr" target="#b45">(Raffel et al., 2022)</ref>, where at its position the model shall generate zero or more tokens. &lt;options&gt; and &lt;/options&gt; is equivalent to the parentheses "(" and ")" in regular expression, the model shall choose one expression among the group. To make the recognition easier, we use &lt;choice_i&gt; and &lt;/choice_i&gt; to wrap each choice. The regular expression notation of length counts at the character level, but in practice, we want to control the output word length. Therefore, we use the &lt;length=n&gt; label to denote the constraint of output word count.</p><p>We avoid the shortcoming of T5 <ref type="bibr" target="#b45">(Raffel et al., 2022)</ref>  Table <ref type="table">2</ref>: Constraint expression of each task. We fine-tune on tasks and variations listed in Table <ref type="table">2a</ref>, and additionally evaluate the unseen tasks listed in Table <ref type="table">2b</ref>. Notice that for few-shot learning, all the tasks are not trained before.</p><p>natural sentences <ref type="bibr" target="#b24">(Lester et al., 2021)</ref>. On the other hand, we also overcome the redundancy of BART denoising schema <ref type="bibr" target="#b17">(He, 2021)</ref>, where the whole input is generated again, since we only generate the realized expression. Moreover, beyond fill-in-theblank, we introduce choice-making, which further enriches the expressiveness of our controlling language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training</head><p>Fine-tuning We could automatically construct the training data from the corpus and conduct selfsupervised learning. Alternatively, we could also directly convert the input of existing supervised datasets into the form of our controlling language, and use them to fine-tune state-of-the-art models such as <ref type="bibr">FLAN-T5 (Chung et al., 2022)</ref>. The input format is shown in Table <ref type="table">2a</ref>. We include αNLG <ref type="bibr" target="#b6">(Bhagavatula et al., 2020</ref>) and CommonGen <ref type="bibr" target="#b28">(Lin et al., 2020)</ref>, two English controllable generation datasets of position and lexicon constraint. In αNLG, given the past observation O 1 and the future observation O 2 , the goal is to generate a hypothesis h that could follow O 1 and trigger O 2 . The regular expression of the constraint is ".*" since no lexicon constraint is required. In CommonGen, given a set of k concepts C = {c 0 , c 1 , ..., c k-1 }, the output text shall include those concepts and at the same time be consistent with common sense. While in the original setting, the appearance order of concepts and their word sense change is not provided, and the model shall make these decisions, here in our controlling language, the exact word and order must be given. Otherwise, we cannot construct the corresponding expression. So, we preprocess the original instances and recover the order and word sense of the concepts by the reference text. To help the model generate the concepts sequentially and track how many concepts it has already used, we append the serial number label (i) to every concept c i on both the input and output sides and remove the labels from the output generation once completed. The regular expression of the constraint is ".*c 0 .*c 1 ... .*c k-1 .*".</p><p>We also leverage these two datasets to teach the model to control the output length by simply adding the length label with the ground truth length. To better track how many words the model itself has already generated, we append the length number label _i to every word w i ; for example, the sentence "Stephen knocked over a vase while drunk." becomes "Stephen_0 knocked_1 over_2 a_3 vase_4 while_5 drunk._6". Similarly, we remove the length number labels after completion.</p><p>Finally, we need to teach the model about choosing grammar. We use αNLI <ref type="bibr" target="#b6">(Bhagavatula et al., 2020)</ref> dataset, the task of which is to determine whether H 1 or H 2 is the more plausible hypothesis given the past and future observations O 1 and O 2 , and the constraint of the regular expression is "(H 1 |H 2 )".</p><p>In-context Learning For large language models like GPT-3.5 <ref type="bibr" target="#b7">(Brown et al., 2020)</ref>, where typically access is typically provided via API, we may not apply many traditional controllable generation technics. However, we can leverage its ability of incontext learning to conduct fine-grain constraint generation. More specifically, we leverage the ability to discover and imitate the repeated pattern <ref type="bibr" target="#b35">(Madaan and Yazdanbakhsh, 2022;</ref><ref type="bibr" target="#b37">Min et al., 2022)</ref>, which is desirable in our case, since unlike other natural language understanding tasks, the specific fine-grain constraint is a well-defined simple pattern that could be easily discoverable and imitable.</p><p>Given the input with control expression, we can select k instances with the same expression structure as the instruction prompt and send it to the large language model together with input. Naturally, when evaluating the test set, we can select examples from the training set or validation set, or other instances of the test set when they are not available. Consistantly, we use the same input and output format described before, which saves extra efforts on prompt engineering. In addition, we simply use the popular json format " {"input": <ref type="bibr">[INPUT]</ref>, "output": [OUTPUT]} " for each demonstrating instances, and naturally seperate them with "\n". By using json, we can further avoid the need for escape character if the input text happens to contain metadata like "Input" or "\n".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Inference</head><p>We use rejection sampling to generate output text that is matched by the control expression. Verifying the output is simple, since we could convert the control expression into regular expression and check the validity. Additionally, if the expression contains length constraint label, we count and compare the number of words in the output text. We try at most k times to avoid infinite loop and save costs if we use large language model API. When using medium or small size langauge model, to increase the generation quality, we can perform beam search first and see if it can generate a valid result at the first try.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Recursive Decoding</head><p>Different choice might affect the generated text.</p><p>For example, consider the case "S 1 S 2 S 3 .*(E 1 |E 2 )", which gives the first three sentence and two alternative endings and the goal is to choose the correct ending while infill the fourth sentence at the same, which is not included in our fine-tuning data. Instead of directly jumping to the answer with possibly insufficient computation, we could also let the model "think step by step <ref type="bibr" target="#b21">(Kojima et al., 2022)</ref>". We can solve each choice expression first, then compare the complete choices "(S 4 E 1 |S ′ 4 E 2 )"". The generalized decoding procedure is presented at Algorithm 1, which assumes that each options is independ with each other and greedily solve them from left to right. We leave the evaluation of expression with multipe consecutive options <ref type="bibr" target="#b32">(Lu et al., 2022)</ref> for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>We conduct experiments on 2 Nvidia A100 GPUs, with about 10 total GPU hours locally. For mediumsize language model, we use FLAN-T5-xl <ref type="bibr" target="#b11">(Chung et al., 2022)</ref> with Apache 2.0 license, which has 3B parameters and is fine-tuned on many natural language understanding and generation tasks. We use Huggingface Transformers library <ref type="bibr" target="#b57">(Wolf et al., 2020)</ref> with Apache-2.0 license for fine-tuning and evaluation. We trained the model for 3 epochs, with a batch size of 16 and learning rate of 3e-5. We set beam size to 4 for beam search and p to 0.95 for top-p sampling. We generate at most k = 512 samples if we do not obtain any valid outcome.</p><p>For large language model, we use GPT-3 <ref type="bibr" target="#b7">(Brown et al., 2020)</ref> text-davinci-003 version via Ope-nAI API, and the 175B model is calibrated with Reinforcement Learning from Human Feedback <ref type="bibr">(Stiennon et al., 2020b)</ref>. We feed 8 in-domain examples as the prompt, set the temperature to 0.7, and retry at most k = 8 times if the result is not valid. All results are from "single" run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Lexicon Constraint</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Lexicon Constraint Only</head><p>Setting We evaluate our method on the devset of CommonGen <ref type="bibr" target="#b28">(Lin et al., 2020)</ref>, as the reference text of the test set is not publicly available. As mentioned in 2.2, we feed the model with oracle concept order and word sense. For automatic metrics we use BLEU-4 <ref type="bibr" target="#b41">(Papineni et al., 2002)</ref>, CIDEr <ref type="bibr" target="#b55">(Vedantam et al., 2015)</ref>, SPICE <ref type="bibr" target="#b1">(Anderson et al., 2016)</ref> and Coverage (Cov.), which is the average ratio of input concepts that are present in lemmatizatized outputs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We compare the performance of our method with other baselines, including fine-tuning methods BART <ref type="bibr" target="#b28">(Lin et al., 2020)</ref> and T5-Large <ref type="bibr" target="#b28">(Lin et al., 2020)</ref>, auxilary guiding model method NADO <ref type="bibr" target="#b36">(Meng et al., 2022)</ref>, prompting method NRP <ref type="bibr" target="#b8">(Carlsson et al., 2022)</ref>, and 8-shot pure natural language instruction (NLI) on GPT-3.5, which is shown at Table <ref type="table" target="#tab_2">3a</ref>.</p><p>Given only 8 examples with a clear connection between input and output, GPT-3.5 still shows competitive performance in terms of text automatic metrics, and achieves high concept coverage, surpassing all the previous baselines. Compared with natural language instruction, the success rate is very close. And with more supervised data to modify the model's parameter, FLAN-T5-xl performs significantly better than GPT-3.5 and other previous baselines in all metrics and successfully satisfies all lexicon constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Lexicon &amp; Length Constraint</head><p>As described in Section 2.2, we slightly modify the devset of CommonGen to introduce the additional length constraint and evaluate GPT-3.5 and FLAN-T5. For metric, we replace Coverage (Cov.) with Success Rate (SuR.), which is the average percentage of output that matches the input expression. In a composite task, the performance of GPT-3.5 downgrades dramatically and struggles to generate valid output, indicating that multi-concept inclusion and length control at the same time is challenging, especially for few-shot in-context learning. Yet, REI still outperforms NLI in terms of success rate, and the "high" n-gram metrics might also indicate the poor instruction following ability in terms of challenging fine-grain constraints, which is consistent with the finding of Zhou et al. ( <ref type="formula">2023</ref>). FLAN-T5 only has a minor drop in performance and still maintains a high success rate since it has trained on this composite constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Position Constraint</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Position constraint only</head><p>Setting We evaluate our method on the testset of αNLG <ref type="bibr" target="#b6">(Bhagavatula et al., 2020)</ref>. The automatic metrics include BLEU-4 <ref type="bibr" target="#b41">(Papineni et al., 2002)</ref>, ROUGE-L <ref type="bibr" target="#b29">(Lin, 2004)</ref> and BERTScore <ref type="bibr">(Zhang* et al., 2020)</ref>. We do not report Success Rate since it's always 100%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>As presented in Table <ref type="table" target="#tab_3">4a</ref>, we compare our method with two unsupervised baselines De-Lorean <ref type="bibr" target="#b43">(Qin et al., 2020)</ref> and COLD <ref type="bibr" target="#b44">(Qin et al., 2022)</ref>, non-autoregressive Diffusion-LM <ref type="bibr">(Li et al., 2022)</ref> and two fine-tuned methods on 11B T5 <ref type="bibr" target="#b19">(Khashabi et al., 2021)</ref>, 20B UL2 <ref type="bibr" target="#b54">(Tay et al., 2022)</ref> and 8-shot NLI on GPT-3.5.</p><p>With few-shot learning, GPT-3.5 outperforms two unsupervised baselines and Diffusion-LM, demonstrating its strong in-context learning ability given only a few infilling examples. Since it's a relatively simple constraint, the performance between REI and NLI is very close. With our careful instruction prompt design and adequate fine-tuning, 3B FLAN-T5 shows stronger performance than 11B T5, and remains competitive compared to 20B UL2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Position &amp; Length Constraint</head><p>As mentioned in Section 2.2, we slightly modify the αNLG test set to add the length constraint.</p><p>We change the BERTScore metric to SuccessRate (SuR.). Table <ref type="table" target="#tab_3">4b</ref> shows the results. GPT-3.5 manages to imitate both position and length constraints, showing relatively high success rate, while under NLI, it performs badly. But with full-scale supervised learning, FLAN-T5 can robustly generate valid output on the test set 100% of the time. Also, in terms of automatic metrics, the output of both models does not downgrade dramatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Position &amp; Lexicon Constraint</head><p>We can also modify the αNLG test set to add lexicon constraint, setting the keyword to be the first verb on the reference text. The input format is shown in Table <ref type="table">2b</ref>, and Table <ref type="table" target="#tab_3">4c</ref> shows the results.</p><p>For GPT-3.5, it still is very likely to generate valid output nearly all of the time, and the automatic metrics enjoy improvement compared with the results of no lexicon constraint, since the additional gold words are provided, and the verb constraint limits the vast scope of possible hypothesis space. Also, REI is slightly better than NLI. For FLAN-T5, although it has been trained on position constraint or lexicon constraint separately, it has not seen the combination, and yet still demonstrates strong performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Position &amp; Lexicon &amp; Length Constraint</head><p>We can further combine all conditions together, adding both length and lexicon constraints on the test set of αNLG. The input format is presented in Table <ref type="table">2b</ref>, and Table <ref type="table" target="#tab_3">4d</ref> shows the results. Compositional constraints challenge few-shot GPT-3.5, as it's more difficult to generate output that matches all three requirements, and the success rate drops slightly. Interestingly, NLI got a very low success rate. But fully-trained FLAN-T5 exhibits robust transfer ability, as the simultaneous three constraints are not included in training data, but FLAN-T5 still manages to achieve close to 100% success rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.5">Position Constraint &amp; Alternative Endings</head><p>On the test set of Story Cloze Test <ref type="bibr" target="#b38">(Mostafazadeh et al., 2016)</ref>, which is to choose between the right ending and the wrong one given the four-sentence context, we additionally mask the fourth sentence and require the model to infill the missing sentence while determining the correct ending. The input format is shown in Table <ref type="table">2b</ref>, and the result is shown in Table <ref type="table" target="#tab_5">6</ref>. We change the Success Rate (SuR.) metric to Accuracy (Acc.), since choosing either ending is valid. For GPT-3.5, we directly construct promoting examples with the initial input and final output, and surprisingly find that GPT-3.5 handles the composite constraint quite well, and chooses the right ending with not bad accuracy. Also, REI comes close to NLI in performance. For FLAN-T5-xl, we use the recursive decoding (Section 2.4, and it shows moderate performance, with lower accuracy but higher BLEU / ROUGE compared with GPT-3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Summarization with length constraint</head><p>REI can also easily support abstractive summarization with desired length <ref type="bibr" target="#b20">(Kikuchi et al., 2016;</ref><ref type="bibr" target="#b14">Fan et al., 2018)</ref>, as long as the base model has been trained on the summarization task, which is the case in our choosing models <ref type="bibr">FLAN-T5 (Chung et al., 2022)</ref> and GPT-3.5 <ref type="bibr" target="#b40">(Ouyang et al., 2022)</ref>. We choose to evaluate on the test set of English headline generation dataset Gigaword <ref type="bibr">(Graff et al.</ref>,</p><formula xml:id="formula_0">Wiktionary IATE Method</formula><p>Term% BLEU Term% BLEU Constraint decoding <ref type="bibr" target="#b13">(Dinu et al., 2019)</ref> 99.50 25.80 82.00 25.30 Train-by-replace <ref type="bibr" target="#b13">(Dinu et al., 2019)</ref> 93.40 26.30 94.50 26.00 RePP <ref type="bibr" target="#b51">(Sun et al., 2022)</ref> 93.67 30.52 95.41 29.38 TADA <ref type="bibr" target="#b0">(Ailem et al., 2021)</ref> 96.84 26.73 98.02 27.11 EDITOR <ref type="bibr" target="#b58">(Xu and Carpuat, 2021)</ref> 99.8 29.30 100.0 28.90 Levenshtein Transformer <ref type="bibr" target="#b52">(Susanto et al., 2020)</ref>  2003), due to its short input and output length. Also, Gigaword is not included in the training set of FLAN-T5 or GPT-3.5. The input format is written in Table <ref type="table">2b</ref>. We use ROUGE-L <ref type="bibr" target="#b29">(Lin, 2004)</ref> and Success Rate (SuR.) for metrics.</p><p>We compare our methods with two unsupervised unconstrainted baselines SEQ <ref type="bibr" target="#b5">(Baziotis et al., 2019)</ref> and TED <ref type="bibr" target="#b60">(Yang et al., 2020)</ref>, and the results are shown in Table <ref type="table" target="#tab_6">7</ref>. Both GPT-3.5 and FLAN-T5 exceed the two baselines in ROUGE-L score, showing relatively good text quality. Since the summarization task constrains more on the semantic of output compared with pure lexicon constraint (CommonGen) or position constraint (αNLG), satisfying length constraint might be more difficult, and GPT-3.5 shows a relatively lower success rate, but NLI has the worst success rate. But nevertheless, FLAN-T5 still achieves 100% success rate. Notice that with limited REI training tasks, the model can still generalize to new tasks with the specific format, demonstrating the robust transfer ability under supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Terminology-constrainted machine transaltion</head><p>We can also apply REI to machine translation with terminology constraint <ref type="bibr" target="#b13">(Dinu et al., 2019)</ref>, which is to ensure the given terminologies T = (t 0 , t 1 , ...) are used in translation. We only test GPT-3.5 here, due to its superiority in multi-language understanding, while the majority of output language during pre-training, multi-task learning, and fine-tuning is English. We evaluate on the test set of Wiktionary and IATE <ref type="bibr" target="#b13">(Dinu et al., 2019)</ref>, two English-German translation dataset, using BLEU-4 <ref type="bibr" target="#b41">(Papineni et al., 2002)</ref> and Terminology Coverage (Term) for metrics.</p><p>We compare our method with several strong baselines, including Constraint decoding <ref type="bibr" target="#b13">(Dinu et al., 2019)</ref>, Train-by-replace <ref type="bibr" target="#b13">(Dinu et al., 2019)</ref>, RePP <ref type="bibr" target="#b51">(Sun et al., 2022)</ref>, TADA <ref type="bibr" target="#b0">(Ailem et al., 2021)</ref>, EDITOR <ref type="bibr" target="#b58">(Xu and Carpuat, 2021)</ref>, Levenshtein Transformer <ref type="bibr" target="#b52">(Susanto et al., 2020)</ref>, and 8-shot NLI on GPT-3.5. Due to its vast parameters, GPT-3.5 outperforms all other baselines in terms of BLEU score. Also, GPT-3.5 achieves near 100% terminology coverage rate, which is close to the existing upper limit. Finally, REI has a slightly higher term coverage than NLI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Qualitative Results</head><p>Table <ref type="table">8</ref> shows the samples of lexicon &amp; length constraints (Section 3.2.2), position &amp; lexicon &amp; length constraints (Section 3.3.4), position constraint with alternative ending (Section 3.3.5), summarization with length constraint (Section 3.4) and translation with terminology constraint (Section 3.5). Both FLAN-T5 and GPT-3.5 generate valid and fluent sentences. GPT-3.5 also uses more vivid or humanlike words like "antihistamines" or the abbreviation "FIA", probably due to its large-scale model size and training corpus.</p><p>CommonGen+length &lt;expression&gt; &lt;mask_0&gt; dance(0) &lt;mask_1&gt; performed(1) &lt;mask_2&gt; stage(2) &lt;mask_3&gt; wearing(3) &lt;mask_4&gt; costumes(4) &lt;mask_5&gt; &lt;length=11&gt; &lt;/expression&gt; FLAN-T5-xl A_1 dance(0)_2 is_3 performed(1)_4 on_5 a_6 stage(2)_7 by_8 people_9 wearing(3)_10 costumes(4)_11 GPT-3.5, 8 shot A_1 traditional_2 dance(0)_3 is_4 performed(1)_5 on_6 the_7 stage(2),_8 wearing(3)_9 colorful_10 costumes(4)_11 αNLG+length+lexicon Jim was not confident in his home repair skills. &lt;expression&gt; &lt;mask_0&gt; attended(0) &lt;mask_1&gt; &lt;length=9&gt; &lt;/expression&gt; Jim was so excited to learn a new skill. FLAN-T5-xl Jim_1 bought_2 new_3 gloves_4 and_5 attended(0)_6 a_7 home_8 repair._9 GPT-3.5, 8 shot Jim_1 attended(0)_2 a_3 home_4 repair_5 workshop_6 to_7 gain_8 confidence._9 StoryCompletion+infill I tried going to the park the other day. The weather seemed nice enough for a walk. Within minutes of getting there I started sneezing. &lt;expression&gt; &lt;options&gt; &lt;choice_0&gt; &lt;mask_0&gt; My allergies were too bad and I had to go back home. &lt;/choice_0&gt; &lt;choice_1&gt; &lt;mask_1&gt; It reminded me of how much I loved spring flowers. &lt;/choice_1&gt; &lt;/options&gt; &lt;/expression&gt; FLAN-T5-xl There were a lot of people at the park. My allergies were too bad and I had to go back home. GPT-3.5, 8 shot I realized I had forgotten the antihistamines at home. My allergies were too bad and I had to go back home. Gigaword+length japan 's toyota team europe were banned from the world rally championship for one year here on friday in a crushing ruling by the world council of the international automobile federation.\n Summarize the aforementioned text in a single phrase.\n &lt;expression&gt; &lt;mask_0&gt; &lt;length=6&gt; &lt;/expression&gt; FLAN-T5-xl toyota_1 team_2 europe_3 banned_4 from_5 rallying_6 GPT-3.5, 8 shot toyota_1 team_2 europe_3 banned_4 by_5 fia_6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wiktionary</head><p>Translate from English to German:\n\n English: Jennifer Aniston need not always be perfect or successful. \n German: &lt;expression&gt; &lt;mask_0&gt; erfolgreich(0) &lt;mask_1&gt; &lt;/expression&gt; GPT-3.5, 8 shot Jennifer Aniston muss nicht immer perfekt oder erfolgreich(0) sein.</p><p>Table <ref type="table">8</ref>: Qualitative examples of various constraints by fine-tuned FLAN-T5-xl and few-shot GPT-3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Tasks of Controllable Text Generation Controllable text generation refers to the tasks that generate text according to the controlling signals <ref type="bibr" target="#b42">(Prabhumoye et al., 2020)</ref>. Typically, the output can be constrained at three levels from coarse to fine: <ref type="bibr" target="#b61">(Zhang et al., 2022)</ref> semantic, structural and lexical. At semantic level, the signals include topic <ref type="bibr" target="#b53">(Tang et al., 2019)</ref>, sentiment <ref type="bibr" target="#b31">(Logeswaran et al., 2018)</ref>, format <ref type="bibr" target="#b26">(Li et al., 2020)</ref>, toxity <ref type="bibr" target="#b22">(Krause et al., 2021)</ref> and other abstract attribute. At the structural level, the constraints include key-value data table <ref type="bibr" target="#b39">(Novikova et al., 2017)</ref>, syntax tree, and partsof-speech <ref type="bibr">(Li et al., 2022)</ref>. At lexical level, then controlling elements include keyword <ref type="bibr" target="#b28">(Lin et al., 2020)</ref>, generating position <ref type="bibr" target="#b47">(Shen et al., 2020)</ref> and length <ref type="bibr" target="#b8">(Carlsson et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods of Controllable Text Generation</head><p>Current approach for controllable text generation can be summarized as three main categories <ref type="bibr" target="#b61">(Zhang et al., 2022)</ref>: retraining or refactoring the model, e.g. CTRL <ref type="bibr" target="#b18">(Keskar et al., 2019)</ref>, POINTER <ref type="bibr">(Zhang et al., 2020)</ref>, CMDP <ref type="bibr">(Chan et al., 2021b)</ref>, Constrained BART <ref type="bibr" target="#b17">(He, 2021)</ref>, CoCon <ref type="bibr">(Chan et al., 2021a)</ref>, PlanGen <ref type="bibr" target="#b50">(Su et al., 2021) and</ref><ref type="bibr">InstructCTG (Zhou et al., 2023)</ref>; tuning on given data, including model fine-tuning, Prompt Tuning <ref type="bibr" target="#b24">(Lester et al., 2021)</ref> and RL-Fine Tuning <ref type="bibr">(Stiennon et al., 2020a)</ref>; and post-processing, which can either design specific decoding strategy, e.g. Constrainted Beam Search <ref type="bibr" target="#b2">(Anderson et al., 2017)</ref>, DeLorean <ref type="bibr" target="#b43">(Qin et al., 2020)</ref>, COLD <ref type="bibr" target="#b44">(Qin et al., 2022)</ref>, Neuro-Logic <ref type="bibr" target="#b33">(Lu et al., 2021)</ref>; or using auxilary guiding model, e.g. PPLM <ref type="bibr" target="#b2">(Anderson et al., 2017)</ref>, GeDI <ref type="bibr" target="#b22">(Krause et al., 2021)</ref>, FUDGE <ref type="bibr" target="#b59">(Yang and Klein, 2021)</ref>, CTRLsum <ref type="bibr" target="#b16">(He et al., 2022)</ref>, Plug-and-Play Content Planning <ref type="bibr" target="#b30">(Liu et al., 2022)</ref>, NADO <ref type="bibr" target="#b36">(Meng et al., 2022)</ref>, and MACSum <ref type="bibr">(Zhang et al., 2023)</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed Regular Expression Instruction (REI), a novel instruction-based method that unifies finegrain lexical-level constrained text generation. Our method is highly adaptable, fitting either language model fine-tuning or large language model incontext learning. Our controlling language can also easily be applied to other related tasks, including story completion while infilling, summarization with length constraint, and machine translation with terminology constraint. Experiments show that our method has a high success rate and outperforms most of the previous strong baselines, demonstrating its effectiveness despite the simplicity. We leave the evaluation and improvement of more complex constraints for future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Our proposed Regular Expression Instruction is serialized and cannot describe a set of keyword constraints where the appearing order is arbitrary, but only a list of keywords with determined order. Future work is needed to exceed the limit, either by approximating the word order or by repeated random sampling. Also, to obtain valid results we use reject sampling, which might need many repeated trials, thus reducing the efficiency and downgrading the speed. More efficient mechanisms with less retry are worth investigating. Additionally, under the current trends of the instruction following, more sophisticated prompts under 0-shot is worth investigating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>This work involves no sensitive data and uses several public-available datasets. This work discusses controllable text generation, which aims for better usage of the black-box language model and may better reduce the problematic biases. We notice that the method proposed in this work can be used to generate disinformation or harmful content directly via controlling language, but the malicious usage can be further avoided by filtering out improper control input and stopping harmful content generation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Generation Statistics</head><p>The data of average try and the first-time success rate during generation is presented in Table <ref type="table">10</ref>.</p><p>REI models tend to succeed on the first attempt for simple constraints, and only for challenging constraints, REI models would retry. Also, finetuned FLAN needs the least retry, while natural language instruction requires the most retry and may not be likely to succeed on the first try.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Natural Language Instruction Examples</head><p>For the method of Natural Language Instruction on GPT-3.5, the instructions used on each task are shown in Table <ref type="table">11</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>span-corruption schema, where the model only generates discontinued spans rather than full</figDesc><table><row><cell>Task</cell><cell>Input with Control Expression</cell></row><row><cell>αNLG</cell><cell>O1 &lt;expression&gt; &lt;mask_0&gt; &lt;/expression&gt; O2</cell></row><row><cell>αNLG+length</cell><cell>O1 &lt;expression&gt; &lt;mask_0&gt; &lt;length=l&gt; &lt;/expression&gt; O2</cell></row><row><cell>αNLI</cell><cell>O1 &lt;expression&gt; &lt;options&gt; &lt;choice_0&gt; H1 &lt;/choice_0&gt; &lt;choice_1&gt; H2 &lt;/choice_1&gt;</cell></row><row><cell></cell><cell>&lt;/options&gt; &lt;/expression&gt; O2</cell></row><row><cell>CommonGen</cell><cell>&lt;expression&gt; &lt;mask_0&gt; c0(0) &lt;mask_1&gt; c1(1) &lt;mask_2&gt; c2(2) &lt;mask_3&gt; &lt;/expression&gt;</cell></row><row><cell>CommonGen+length</cell><cell>&lt;expression&gt; &lt;mask_0&gt; c0(0) &lt;mask_1&gt; c1(1) &lt;mask_2&gt; c2(2) &lt;mask_3&gt; &lt;length=l&gt;</cell></row><row><cell></cell><cell>&lt;/expression&gt;</cell></row><row><cell></cell><cell>(a) Fine-tune Task</cell></row><row><cell>Task</cell><cell>Input with Control Expression</cell></row><row><cell>αNLG+lexicon</cell><cell>O1 &lt;expression&gt; &lt;mask_0&gt; w(0) &lt;mask_1&gt; &lt;/expression&gt; O2</cell></row><row><cell>αNLG+length+lexicon</cell><cell>O1 &lt;expression&gt; &lt;mask_0&gt; w(0) &lt;mask_1&gt; &lt;length=l&gt; &lt;/expression&gt; O2</cell></row><row><cell>StoryCompletion+infill</cell><cell>S1S2S3 &lt;expression&gt; &lt;mask_0&gt; &lt;options&gt; &lt;choice_0&gt; E1 &lt;/choice_0&gt; &lt;choice_1&gt; E2</cell></row><row><cell></cell><cell>&lt;/choice_1&gt; &lt;/options&gt; &lt;/expression&gt;</cell></row><row><cell>Gigaword+length</cell><cell>[Text]\n Summarize the aforementioned text in a single phrase.\n &lt;expression&gt; &lt;mask_0&gt;</cell></row><row><cell></cell><cell>&lt;length=l&gt; &lt;/expression&gt;</cell></row><row><cell>Wiktionary/ITAE</cell><cell>Translate from English to German:\n\n English: [Text] \n German: &lt;expression&gt; &lt;mask_0&gt;</cell></row><row><cell></cell><cell>t0(0) &lt;mask_1&gt; t1(1) &lt;mask_2&gt; &lt;/expression&gt;</cell></row><row><cell></cell><cell>(b) Transfer Task</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on devset of CommonGen. The best models are bold within each metric.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Result on test of αNLG.</figDesc><table><row><cell></cell><cell cols="3">BLEU ROUGE BERT</cell></row><row><cell>Qin et al. (2020)</cell><cell>1.38</cell><cell>18.94</cell><cell>42.86</cell></row><row><cell>Qin et al. (2022)</cell><cell>1.79</cell><cell>19.50</cell><cell>42.67</cell></row><row><cell>Li et al. (2022)</cell><cell>7.10</cell><cell>28.30</cell><cell>89.00</cell></row><row><cell>Khashabi et al. (2021)</cell><cell>19.47</cell><cell>44.60</cell><cell>92.87</cell></row><row><cell>Tay et al. (2022)</cell><cell>24.34</cell><cell>49.30</cell><cell>93.51</cell></row><row><cell>NLI+GPT-3.5, 8 shot</cell><cell>13.62</cell><cell>36.38</cell><cell>91.05</cell></row><row><cell>REI+GPT-3.5, 8 shot</cell><cell>13.01</cell><cell>37.29</cell><cell>91.27</cell></row><row><cell>REI+FLAN-T5-xl</cell><cell>25.44</cell><cell>48.45</cell><cell>93.28</cell></row><row><cell cols="3">(a) Position constraint</cell><cell></cell></row><row><cell>Model</cell><cell cols="3">BLEU ROUGE SuR.</cell></row><row><cell>NLI+GPT-3.5, 8 shot</cell><cell>9.9</cell><cell>32.93</cell><cell>42.09</cell></row><row><cell>REI+GPT-3.5, 8 shot</cell><cell>10.63</cell><cell>34.87</cell><cell>96.80</cell></row><row><cell>REI+FLAN-T5-xl</cell><cell>19.92</cell><cell>46.17</cell><cell>100.0</cell></row><row><cell cols="3">(b) Position &amp; length constraint</cell><cell></cell></row><row><cell>Model</cell><cell cols="3">BLEU ROUGE SuR.</cell></row><row><cell>NLI+GPT-3.5, 8 shot</cell><cell>14.76</cell><cell>42.04</cell><cell>99.01</cell></row><row><cell>REI+GPT-3.5, 8 shot</cell><cell>18.59</cell><cell>44.67</cell><cell>99.44</cell></row><row><cell>REI+FLAN-T5-xl</cell><cell>23.56</cell><cell>48.81</cell><cell>99.78</cell></row><row><cell cols="3">(c) Position &amp; lexicon constraint</cell><cell></cell></row><row><cell>Model</cell><cell cols="3">BLEU ROUGE SuR.</cell></row><row><cell>NLI+GPT-3.5, 8 shot</cell><cell>19.14</cell><cell>43.67</cell><cell>28.00</cell></row><row><cell>REI+GPT-3.5, 8 shot</cell><cell>17.45</cell><cell>43.90</cell><cell>94.02</cell></row><row><cell>REI+FLAN-T5-xl</cell><cell>21.99</cell><cell>49.17</cell><cell>99.69</cell></row><row><cell cols="4">(d) Position &amp; length &amp; lexicon constraint</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results on Wiktionary and IATE.</figDesc><table><row><cell>100.0</cell><cell>31.20</cell><cell>100.0</cell><cell>30.13</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Results on Story Cloze Test with positional constraint.</figDesc><table><row><cell>Method</cell><cell cols="2">ROUGE SuR.</cell></row><row><cell cols="2">SEQ (Baziotis et al., 2019) 22.68</cell><cell>-</cell></row><row><cell>TED (Yang et al., 2020)</cell><cell>22.83</cell><cell>-</cell></row><row><cell>NLI+GPT-3.5, 8 shot</cell><cell>24.62</cell><cell>28.87</cell></row><row><cell>REI+GPT-3.5, 8 shot</cell><cell>25.46</cell><cell>79.51</cell></row><row><cell>REI+FLAN-T5-xl</cell><cell>28.49</cell><cell>100.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Results on the test set of Gigaword.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Zeng, and RuiZhang. 2023. MACSum: Controllable Summarization with Mixed Attributes. Transactions of the Association for Computational Linguistics, 11:787-803. Wangchunshu Zhou, Yuchen Eleanor Jiang, Ethan Wilcox, Ryan Cotterell, and Mrinmaya Sachan. 2023. Controlled text generation with natural language instructions. CoRR, abs/2304.14293. Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul F. Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. CoRR, abs/1909.08593. Staststics of used dataset</figDesc><table><row><cell cols="3">A Recursive Algorithm</cell><cell></cell></row><row><cell cols="4">Algorithm 1 Recursive decoding</cell></row><row><cell cols="4">1: function RECURSIVEDECODING(exp)</cell></row><row><cell>2:</cell><cell cols="3">if not ContainNonterminal(exp) then</cell></row><row><cell>3:</cell><cell cols="2">return input</cell><cell></cell></row><row><cell>4:</cell><cell>end if</cell><cell></cell><cell></cell></row><row><cell>5:</cell><cell cols="3">if not ContainOptions(exp) then</cell></row><row><cell>6:</cell><cell cols="3">return Generate(exp)</cell></row><row><cell>7:</cell><cell>end if</cell><cell></cell><cell></cell></row><row><cell>8:</cell><cell cols="4">exp_before_opts, opts, exp_after_opts ←</cell></row><row><cell cols="3">SplitByFirstOption(exp)</cell><cell></cell></row><row><cell>9:</cell><cell cols="3">for i, ch in enumerate(opts) do</cell></row><row><cell>10:</cell><cell cols="3">exp_ch ← exp_before_opts+ch</cell></row><row><cell>11:</cell><cell cols="3">opts[i] ← Genereate(exp_ch)</cell></row><row><cell>12:</cell><cell>end for</cell><cell></cell><cell></cell></row><row><cell>13:</cell><cell cols="3">best_ch ← Generate(opts)</cell></row><row><cell>14:</cell><cell cols="4">remain_res ← Generate(exp_after_opts)</cell></row><row><cell cols="4">return best_ch + remain_res</cell></row><row><cell cols="2">15: end function</cell><cell></cell><cell></cell></row><row><cell cols="3">B Dataset Statistics</cell><cell></cell></row><row><cell cols="2">Dataset</cell><cell>Train</cell><cell cols="2">Validation Test</cell></row><row><cell cols="2">αNLG</cell><cell>50481</cell><cell>1780</cell><cell>3561</cell></row><row><cell>αNLI</cell><cell></cell><cell cols="2">169654 1532</cell><cell>3059</cell></row><row><cell cols="2">CommonGen</cell><cell>67216</cell><cell>993</cell><cell>-</cell></row><row><cell cols="2">Gigaword</cell><cell>-</cell><cell>189644</cell><cell>1933</cell></row><row><cell>IATE</cell><cell></cell><cell>-</cell><cell>-</cell><cell>414</cell></row><row><cell cols="2">Wiktionary</cell><cell>-</cell><cell>-</cell><cell>727</cell></row><row><cell cols="3">StoryCompletion -</cell><cell>1871</cell><cell>1871</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Instruction Example aNLG</head><p>The first sentence is " The Smiths were having holidays done of the children. " and the last sentence is " Ty's face lit up as he ran to the new toy, happily posing for photos. " . Insert a middle sentence with similar style, and the length shall not exceed 10 words. aNLG, length</p><p>The first sentence is " The Smiths were having holidays done of the children. " and the last sentence is " Ty's face lit up as he ran to the new toy, happily posing for photos. " . Insert a middle sentence with similar style, and the length shall be exactly 7 words without counting punctuation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>aNLG, lexicon</head><p>The first sentence is " The Smiths were having holidays done of the children. " and the last sentence is " Ty's face lit up as he ran to the new toy, happily posing for photos. " . Insert a middle sentence with similar style, while also containing the keyword "bought". aNLG, length &amp; lexicon The first sentence is " The Smiths were having holidays done of the children. " and the last sentence is " Ty's face lit up as he ran to the new toy, happily posing for photos. " . Insert a middle sentence with similar style, while also containing the keyword "bought", and the length shall be exactly 7 words without counting punctuation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CommonGen</head><p>Generate a sentence that mentions all of these concepts in sequential order: "stood", "field", "looking". CommonGen, length Generate a sentence that mentions all of these concepts in sequential order with the word count of 10, punctuation ignored: "stood", "field", "looking".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gigaword, length</head><p>Given the text "japan 's nec corp. and UNK computer corp. of the united states said wednesday they had agreed to join forces in supercomputer sales .", summarize the aforementioned text in a single phrase with the word count of 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IATE / Wiktionary</head><p>Translate from English to German using terminology "Interview":\n\nEnglish: That is what the Hollywood star has made abundantly clear in an interview.\nGerman: StoryCloze, position Given the first three sentences of the story "My friends all love to go to the club to dance. They think it's a lot of fun and always invite. I finally decided to tag along last Saturday." and two endings "My friends decided to keep inviting me out as I am so much fun." and "The next weekend, I was asked to please stay home.", infill the missing fourth sentence and choose the correct ending from the two. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Encouraging neural machine translation to satisfy terminology constraints</title>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Ailem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingshu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raheel</forename><surname>Qader</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.125</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1450" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Guided open vocabulary image captioning with constrained beam search</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1098</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="936" to="945" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automated learning of decision rules for text categorization</title>
		<author>
			<persName><forename type="first">Chidanand</forename><surname>Apté</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Damerau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sholom</forename><forename type="middle">M</forename><surname>Weiss</surname></persName>
		</author>
		<idno type="DOI">10.1145/183422.183423</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="251" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The present status of automatic translation of languages**this article was prepared with the sponsorship of the informations systems branch, office of naval research, under contract nr 049130</title>
		<author>
			<persName><forename type="first">Yehoshua</forename><surname>Bar-Hillel</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0065-2458(08)60607-5</idno>
	</analytic>
	<monogr>
		<title level="j">Advances in Computers</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="91" to="163" />
			<date type="published" when="1960">1960</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
	<note>reproduction as a whole or in part for the purposes of the u. s. government is permitted</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SEQˆ3: Differentiable sequence-to-sequence-to-sequence autoencoder for unsupervised abstractive sentence compression</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Baziotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Potamianos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1071</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="673" to="681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Abductive commonsense reasoning</title>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Tau Yih</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Alec Radford, Ilya Sutskever, and Dario Amodei</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fine-grained controllable text generation using nonresidual prompting</title>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Carlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joey</forename><surname>Öhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Severine</forename><surname>Verlinden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.471</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6837" to="6857" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">2021a. Cocon: A self-supervised approach for controlled text generation</title>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yew-Soon</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Pung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">2021b. Controllable summarization with constrained Markov decision process</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Hou Pong Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>King</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00423</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1213" to="1232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Scaling instruction-finetuned language models</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2210.11416</idno>
		<idno>CoRR, abs/2210.11416</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Training neural machine translation to apply terminology constraints</title>
		<author>
			<persName><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1294</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3063" to="3068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Controllable abstractive summarization</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-2706</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</title>
		<meeting>the 2nd Workshop on Neural Machine Translation and Generation<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<title level="m">English gigaword. Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">CTRLsum: Towards generic controllable text summarization</title>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Kryscinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Rajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.emnlp-main.396</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5879" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Parallel refinements for lexically constrained text generation with BART</title>
		<author>
			<persName><forename type="first">Xingwei</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.681</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8653" to="8666" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">CTRL: A conditional transformer language model for controllable generation</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lav</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>CoRR, abs/1909.05858</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Genie: A leaderboard for human-in-the-loop evaluation of text generation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06561</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Controlling output length in neural encoder-decoders</title>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1140</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large language models are zero-shot reasoners</title>
		<author>
			<persName><forename type="first">Takeshi</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Machel</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><surname>Iwasawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">GeDi: Generative discriminator guided sequence generation</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhilesh</forename><surname>Deepak Gotmare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Fatema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajani</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.424</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4929" to="4952" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">RACE: Large-scale ReAding comprehension dataset from examinations</title>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1082</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.243</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3045" to="3059" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rigid formats controlled text generation</title>
		<author>
			<persName><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haisong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.68</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="742" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Diffusion-LM improves controllable text generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Thickstun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><surname>Hashimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CommonGen: A constrained text generation challenge for generative commonsense reasoning</title>
		<author>
			<persName><forename type="first">Wangchunshu</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.165</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1823" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Plug-and-play recipe generation with content planning</title>
		<author>
			<persName><forename type="first">Yinhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Shareghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)</title>
		<meeting>the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates (Hybrid). Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="223" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Content preserving text generation with attribute controls</title>
		<author>
			<persName><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS&apos;18</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems, NIPS&apos;18<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5108" to="5118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">NeuroLogic a*esque decoding: Constrained text generation with lookahead heuristics</title>
		<author>
			<persName><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianhui</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjae</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.57</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="780" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neuro-Logic decoding: (un)supervised neural text generation with predicate logic constraints</title>
		<author>
			<persName><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.339</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4288" to="4299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A statistical approach to mechanized encoding and searching of literary information</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Luhn</surname></persName>
		</author>
		<idno type="DOI">10.1147/rd.14.0309</idno>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="309" to="317" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Text and patterns: For effective chain of thought, it takes two to tango</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Yazdanbakhsh</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2209.07686</idno>
		<idno>CoRR, abs/2209.07686</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Controllable text generation with neurallydecomposed oracle</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sidi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Rethinking the role of demonstrations: What makes in-context learning work?</title>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxi</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>CoRR, abs/2202.12837</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A corpus and cloze evaluation for deeper understanding of commonsense stories</title>
		<author>
			<persName><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1098</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The E2E dataset: New challenges for endto-end generation</title>
		<author>
			<persName><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-5525</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Saarbrücken, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="201" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fraser</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maddie</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">F</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2203.02155</idno>
		<idno>CoRR, abs/2203.02155</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Exploring controllable text generation techniques</title>
		<author>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Back to the future: Unsupervised backprop-based decoding for counterfactual and abductive commonsense reasoning</title>
		<author>
			<persName><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jena</forename><forename type="middle">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.58</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="794" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">COLD decoding: Energy-based constrained text generation with langevin dynamics</title>
		<author>
			<persName><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">LINGUIST: Language model instruction tuning to generate annotated utterances for intent classification and slot tagging</title>
		<author>
			<persName><forename type="first">Andy</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saleh</forename><surname>Soltan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Versley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Boese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics</title>
		<meeting>the 29th International Conference on Computational Linguistics<address><addrLine>Gyeongju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="218" to="241" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Blank language models</title>
		<author>
			<persName><forename type="first">Tianxiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Quach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.420</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5186" to="5198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning to summarize from human feedback</title>
		<author>
			<persName><forename type="first">Nisan</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS&apos;20</title>
		<meeting>the 34th International Conference on Neural Information Processing Systems, NIPS&apos;20<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to summarize with human feedback</title>
		<author>
			<persName><forename type="first">Nisan</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">F</forename><surname>Christiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3008" to="3021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Plan-then-generate: Controlled data-to-text generation via planning</title>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sihui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yimai</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.76</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana</addrLine></address></meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="895" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Zeroshot domain adaptation for neural machine translation with retrieved phrase-level prompts</title>
		<author>
			<persName><forename type="first">Zewei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingnan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanbo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2209.11409</idno>
		<idno>CoRR, abs/2209.11409</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Lexically constrained neural machine translation with Levenshtein transformer</title>
		<author>
			<persName><forename type="first">Raymond Hendy</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shamil</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liling</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.325</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3536" to="3543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A topic augmented text generation model: Joint learning of semantics and structural features</title>
		<author>
			<persName><forename type="first">Hongyin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beihong</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1513</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5090" to="5099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Unifying language learning paradigms</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaixiu</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Steven Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><surname>Metzler</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2205.05131</idno>
		<idno>CoRR, abs/2205.05131</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Ramakrishna Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Mention flags (MF): Constraining transformer-based text generators</title>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.9</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="103" to="113" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">EDITOR: An Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints</title>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00368</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="311" to="328" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">FUDGE: Controlled text generation with future discriminators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.276</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3511" to="3535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">TED: A pretrained unsupervised summarization model with theme modeling and denoising</title>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Gmyr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Darve</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.168</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1865" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">A survey of controllable text generation using transformer-based pre-trained language models</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haolin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Song</surname></persName>
		</author>
		<idno>CoRR, abs/2201.05337</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">POINTER: Constrained progressive text generation via insertionbased generative pre-training</title>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.698</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8649" to="8670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yusen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<pubPlace>Michael</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
