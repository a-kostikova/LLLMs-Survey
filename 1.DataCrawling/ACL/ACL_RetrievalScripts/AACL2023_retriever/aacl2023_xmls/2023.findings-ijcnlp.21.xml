<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Comprehensive Neural and Behavioral Task Taxonomy Method for Transfer Learning in NLP</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yunhao</forename><surname>Zhang</surname></persName>
							<email>zhangyunhao2021@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">State Key Laboratory of Multimodal Artificial Intelligence Systems</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">State Key Laboratory of Multimodal Artificial Intelligence Systems</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaohan</forename><surname>Zhang</surname></persName>
							<email>xiaohan.zhang@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Xinyi</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">State Key Laboratory of Multimodal Artificial Intelligence Systems</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Laboratory of Cognitive Neuroscience and Learning</orgName>
								<orgName type="institution">Beijing Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shaonan</forename><surname>Wang</surname></persName>
							<email>shaonan.wang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">State Key Laboratory of Multimodal Artificial Intelligence Systems</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Comprehensive Neural and Behavioral Task Taxonomy Method for Transfer Learning in NLP</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">191BC3938D21F9FAC70058B6DA12E2C4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transfer learning is frequently utilized in scenarios with limited labeled examples, where a crucial step is to identify a related task to the target task. CogTaskonomy (Luo et al., 2022)  was proposed to acquire a taxonomy of NLP tasks, specifically focusing on assessing the similarities between tasks. This method, inspired by cognitive processes, exhibits notable time efficiency. Nevertheless, it does not fully exploit the task-related information present in cognitive data and lacks a comprehensive evaluation of various types of cognitive data. To address these limitations, this paper proposes a comprehensive neural and behavioral method to investigate the relationship among NLP tasks. Our approach utilizes cognitive data, encompassing both neural data such as fMRI and EEG, as well as behavioral data including eye-tracking and semantic feature ratings. Each data modality is employed to establish a common representation space with Representation Similarity Analysis for projecting task-related representations. To fully leverage the cognitive information, we effectively extract the task-relevant information extracted from neural data through feature ranking. Experimental results on 12 NLP tasks demonstrate that our proposed method outperforms state-of-the-art methods on evaluating task similarity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-trained Language Models (PLMs) achieve remarkable performance on downstream tasks through fine-tuning on abundant labeled data <ref type="bibr" target="#b5">(Devlin et al., 2019;</ref><ref type="bibr" target="#b20">Radford et al., 2019;</ref><ref type="bibr" target="#b19">Peters et al., 2018)</ref>. However, their performance tends to degrade when facing with limited labeled data <ref type="bibr" target="#b3">(Chen et al., 2023;</ref><ref type="bibr" target="#b7">Hedderich et al., 2021)</ref>. To overcome this challenge, researchers employ transfer learning by initially fine-tuning a PLM on a related task with ample labeled data, followed by fine-tuning † Equal Contribution.</p><p>on the target task <ref type="bibr" target="#b6">(Dwivedi and Roig, 2019;</ref><ref type="bibr" target="#b23">Song et al., 2019)</ref>. Nevertheless, devising an effective method to identify a suitable similar task remains a challenging endeavor <ref type="bibr" target="#b22">(Ramirez et al., 2023)</ref>.</p><p>To evaluate the relatedness between tasks, different methods have been employed. The first category of method is task embedding, which learns a dedicated high-dimensional representation for each task using a task encoder <ref type="bibr" target="#b10">(James et al., 2018;</ref><ref type="bibr" target="#b14">Lan et al., 2019;</ref><ref type="bibr" target="#b0">Achille et al., 2019;</ref><ref type="bibr" target="#b25">Vu et al., 2020)</ref>. Despite the low time complexity, modulating the model to adapt for a new task using this method is challenging. Another approach is Taskonomy method <ref type="bibr" target="#b30">(Zamir et al., 2018)</ref>, which fine-tunes a model on each task, and transfers each fine-tuned model to other tasks in a fully supervised manner. This process can effectively capture task similarity, while it is demanding and timeconsuming. The third category encompasses the cognitively inspired CogTaskonomy proposed by <ref type="bibr" target="#b15">Luo et al. (2022)</ref>. This method projects task-related representations into a shared space based on fMRI and subsequently evaluates the similarity between tasks. It only needs to fine-tune a model for each task separately, which exhibits notable time efficiency. However, shared spaces in CogTaskonomy are not strict shared spaces, and this method does not effectively leverage task-related information from neural data. Our study falls under the third category and improves upon the existing approach by fully utilizing cognitive information to generate an enhanced shared space.</p><p>This paper proposes a method, called NBT (A Comprehensive Neural and Behavioral Task Taxonomy Method), integrating both neural data (i.e., fMRI, EEG) and behavioral data (i.e., Eyetracking, Semantic feature ratings) to investigate the relationship among NLP tasks. We employ each data modality to establish a common representation space with Representation Similarity Analysis (RSA) <ref type="bibr" target="#b12">(Kriegeskorte et al., 2008)</ref> for projecting task-related representations. Moreover, to fully exploit cognitive information, we employ feature ranking to effectively extract the task-relevant information extracted from neural data. Results on 12 NLP tasks show that NBT outperforms the previous cognitive-inspired method, and achieves comparable performance to state-of-the-art methods with lower computational time complexity on evaluating task similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>To evaluate task similarity, we propose NBT as demonstrated in Figure <ref type="figure" target="#fig_0">1</ref>, which involves three steps: 1) calculating task-specific representations, 2) generating a shared space from cognitive data, and 3) mapping task-specific representations to cognitive data in a shared space to calculate the task similarity. Each step in Figure <ref type="figure" target="#fig_0">1</ref> is described as follows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ridge Regression</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EEG</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Extracting Task-Specific Representations</head><p>We take task-specific representations through the last layer of PLMs fine-tuned on various NLP tasks.</p><p>For word-level stimuli, following the method in previous studies <ref type="bibr" target="#b28">(Wang et al., 2022</ref><ref type="bibr" target="#b27">(Wang et al., , 2023))</ref>, we randomly sample a maximum of 1,000 sentences for each target word from Xinhua News corpus 1 , and feed sentences into fine-tuned PLMs, with vectors extracted from the last layer. Subsequently, we average vectors of each target word to obtain a task-related representation. For sentence-level stimuli, the sentence representation is obtained by extracting the hidden state of [CLS] token from each sequence in the last layer.</p><p>1 http://www.xinhuanet.com/whxw.htm</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generating a Shared Space from Cognitive Data</head><p>To fully extract task-relevant information from high-dimensional fMRI, we select a certain number of voxels by scoring them with their relevance to PLMs. Specifically, regression models are trained for each voxel to predict each dimension of representations of PLMs with this voxel and its adjacent three-dimensional neighbors. The correlation between the true and the predicted representations is regarded as the informative score of each voxel. Subsequently, we utilize RSA, a widely employed technique for discerning correlations between neuronal responses derived from brain data and models. This facilitates the mapping of cognitive data originating from distinct subject-specific spaces into a unified representational space. It can also help to mitigate the inherent noise in cognitive data. To be more precise, we extract a group of word or sentence cognitive representations E = {e 1 , e 2 , ..., e n } from cognitive data. For each pair of representations (e i , e j ), its similarity is measured by the Pearson correlation (ρ). Thus, we obtain cognitive data in the representational similarity space M ∈ R n×n , where M ij = ρ(e i , e j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Mapping Task-specific Representations to the Shared Space</head><p>Finally, we use ridge regression<ref type="foot" target="#foot_0">2</ref> to learn a mapping function between cognitive data M and taskspecific representations P u ∈ R n×w obtained by the PLM f (θ u ) fine-tuned on the u-th task, where n is the number of words or sentences as well as the dimensionality of M , and w is the dimensionality of representations of f (θ u ). The regression coefficients l, which is a n-dimension vector, and l 0 are learned by minimizing loss(l,</p><formula xml:id="formula_0">l 0 ) = ∥P u l + l 0 -m∥ 2 2 + λ∥l∥ 2 2 (1)</formula><p>for each column m ∈ R n×1 which is a single dimension of the M matrix. The regularization parameter λ for each dimension is set by the nested cross-validation. Each dimension of M and P u is standardized across training stimuli.</p><p>After mapping, we obtain the final predict matrix Mu by averaging predict matrices over all participants for the u-th task, and calculate the correlation coefficients between Mu and M across each dimension to obtain task-specific representation CogP u in the shared space, defined as follows:</p><formula xml:id="formula_1">CogP u =[c ( mu0 , m 0 ) , . . . , c ( muh , m h ) . . . , c ( muv , m v )] , 0 ≤ h ≤ v (2)</formula><p>where mhu and m h are respectively the predicted and ground-truth vectors of the h-th dimension, n is the number of dimensions, and c(•) is the correlation function for vector pairs, e.g., ρ and the coefficient of determination (R 2 ). We then utilize s(•) involving three different similarity functions (Cosine similarity (cos), R 2 and ρ) to calculate pairwise task similarity as follows:</p><formula xml:id="formula_2">Sim uu ′ = s (Cog P u , Cog P u ′ ) (3)</formula><p>3 Experimental settings </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Transfer Learning Tasks</head><p>Twelve NLP tasks are involved in our experiments, covering sentence/token-level classification, information extraction, and passage ranking tasks <ref type="bibr" target="#b24">(Tjong Kim Sang and De Meulder, 2003;</ref><ref type="bibr" target="#b8">Hendrickx et al., 2010;</ref><ref type="bibr" target="#b16">Nguyen et al., 2016;</ref><ref type="bibr" target="#b21">Rajpurkar et al., 2018;</ref><ref type="bibr" target="#b26">Wang et al., 2019)</ref>. Task details are shown in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Baseline Methods</head><p>Direct Similarity Estimation (DSE) DSE approximates the similarity of task pairs using the average similarity of sentence representations from PLMs fine-tuned on the corresponding task.</p><p>Analytic Hierarchy Process (AHP) On the other hand, the similarity of task pairs can be estimated from the pair-wise transfer learning results <ref type="bibr" target="#b30">(Zamir et al., 2018)</ref>. Given a target task, PLMs transferred from different source tasks are compared on a hold-out dataset to determine the transferability of the target task, which is further used to approximate the similarity between tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cognitive Representation Analytics (CRA)</head><p>CRA first calculates the Representation Dissimilarity Matrix (RDM) by the dissimilarity of sentence representations, then approximates the similarity between tasks by the similarity between the corresponding RDMs <ref type="bibr" target="#b15">(Luo et al., 2022)</ref>.</p><p>Cognitive-Neural Mapping (CNM) CNM calculates the task similarity by mapping sentence representations from multiple fine-tuned PLMs to the same fMRI data <ref type="bibr" target="#b15">(Luo et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Hyperparameters</head><p>Most of the hyperparameters used in our transferlearning and baseline experiments are in line with the ones in <ref type="bibr" target="#b15">Luo et al. (2022)</ref>. The only exception is that the source model of TinyBERT is distilled from our fine-tuned BERT, rather than initialized from the public models of <ref type="bibr" target="#b11">Jiao et al. (2020)</ref> whose download links are missing now. The hyperparameter λ in ridge regression for each dimension is set utilizing nested crossvalidation within the training set, respectively. Each voxel is normalized across training stimuli, as is the dimension of representations of PLMs. More formally, the nested cross-validation framework is applied to make sure that the data utilized for the regularization parameter tuning and the data employed to test the model is firmly independent. The interior 10-fold cross-validation is utilized to choose the optimal regularization parameter, and the extrinsic 10-fold nested cross-validation is applied to predict the values using the model with the optimal regularization parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Evaluation Metric</head><p>To empirically evaluate the similarity between NLP tasks, we conducted pair-wise transfer learning experiments for each task in Table <ref type="table" target="#tab_1">1</ref>, and quantify it with the actual transfer learning performance (Results are reported in Appendix B). In other words, the more similar the source task is, the better performance on the target task the model gets.</p><p>Task Ranking Score (TRS) is used to assess the distance between the task similarity estimated from different methods mentioned before, e.g., DSE, and the empirical task similarity in the transfer learning experiments. Specifically, the task ranking score is obtained from the average ranking of the best source task estimated by the method in the real transfer learning experiment for all target tasks. Then, the random guess leads to N 2 in the task ranking score, while the best one always gets 1 under the N tasks setting.  Evaluating NBT with different similarity measure combinations There are multiple options to calculate correlation coefficients of mapping performance (e.g., ρ, R 2 ) and task similarity (e.g., cos, R 2 , ρ). To demonstrate the robustness of the proposed method, we evaluate the performance of NBT across various measure combinations. As the CNM utilizes fMRI to measure task relations, we compare it with NBT using identical cognitive data. It can be seen from Table <ref type="table" target="#tab_3">3</ref> that NBT f mri has better performance than CNM in all cases. Specifically, NBT f mri outperforms CNM by 35.39% on average. In addition, TinyBERT and BERT show minimal performance difference using NBT f mri , while TinyBERT is more resilient to different cases compared to BERT with CNM, which indicates that the proposed method has greater generalization for different models. Evaluating NBT with numbers of voxels The number of voxels is a critical parameter in the proposed method. In this part, we further evaluate the stability of our proposed method by comparing it with CNM across voxel numbers. Figure <ref type="figure" target="#fig_1">2</ref> shows that NBT f mri exhibits superior performance to CNM across various numbers of voxels. Moreover, it's hard to predict the number of voxels when achieving the best ranking score using CNM, while NBT f mri can achieve relatively optimal results in the 2000-10000 voxel range. Analysis on the spatial distribution of informative voxels In this section, we explore the distribution of voxels selected by NBT f mri and CNM across brain networks associated with semantic processing. As can be obtained from Figure <ref type="figure" target="#fig_2">3</ref>, NBT f mri selects a obviously larger proportion of voxels within areas relating to semantic processing compared to CNM, including languagePC, lan-guageP, semantic and languageLH, which suggests the proposed method can effectively extract the task-relevant information from neural signal. Moreover, voxels selected using NBT f mri also distribute in areas relating to visual semantics, indicating that the method can effectively utilize semantic information from each brain functional area to accurately separate task presentations from neural data. Analysis on the voxel prediction evaluation In this part, we compare results of the proposed method and CNM on voxel prediction. It can be noticed from Figure <ref type="figure" target="#fig_3">4</ref> that NBT f mri obtains higher correlation than CNM, suggesting that the proposed method can better establish the connection between neural signals and task-representations of fine-tuned PLMs, and can better isolate task presentations from neural signals. Analysis on taxonomy tree of 12 NLP tasks In this section, we explore the task similarity matrix and the taxonomy tree of 12 NLP tasks from NBT f mri . Compared with the taxonomy tree from CogTaskonomy, "QA, STS-B" and "QNLI, SST-2" are both found to be put in one cluster in two taxonomies. Furthermore, the proposed method clusters three tasks that need to infer semantic relationships (MNLI, STS-B and RTE) in one cluster, while CogTaskonomy divides STS-B into other clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main result</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a comprehensive neural and behavioral method to investigate the similarity between various NLP tasks. This method can fully extract taskrelevant information from neural data thus capture task taxonomy and effectively guide transfer learning across diverse NLP tasks, which also can be beneficial for other cross-task learning paradigms, including multi-task learning <ref type="bibr" target="#b31">(Zhang et al., 2021;</ref><ref type="bibr" target="#b4">Chen et al., 2021)</ref>, meta learning <ref type="bibr" target="#b29">(Yin, 2020)</ref> and lifelong learning <ref type="bibr" target="#b1">(Biesialska et al., 2020)</ref>. Results on 12 tasks show that the proposed method outperforms the previous cognitive-inspired method, and reaches comparable performance to the stateof-the-art method with O(n) computational time complexity on evaluating task similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>The proposed method utilizes cognitive data, including neural data such as fMRI and EEG, along with behavioral data such as eye-tracking and semantic feature ratings, to efficiently capture the inter-task relationships in NLP with reduced computational time complexity. Although existing work has proven that human brain contains information about NLP tasks <ref type="bibr" target="#b17">(Oota et al., 2022)</ref>, it is unclear what kind of task-relevant information human brain contains (i.e., sentence/token-level classification, information extraction, and passage ranking tasks). Moreover, pre-trained models are fine-tuned on downstream tasks in a fully supervised manner, which is different from how human learn and understand new knowledge <ref type="bibr" target="#b13">(Kühl et al., 2022)</ref>. Therefore, the task similarity based on cognitive data may show different pattern from the real task similarity. Finally, since the proposed method currently exhibits effective performance on NLP, it will be extended to vision and multi-modal domains in our future work.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of NBT.</figDesc><graphic coords="2,83.43,413.31,84.53,64.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: TRS of NBT f mri and CNM with TinyBERT (left) and BERT (right) predicting different numbers of voxels.</figDesc><graphic coords="4,300.75,610.04,133.87,89.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distribution of informative voxels of NBT f mri and CNM with TinyBERT (left) and BERT (right) across the brain (averaged over 5 participants).</figDesc><graphic coords="5,69.37,231.27,117.92,89.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Voxel prediction results of NBT f mri and CNM with TinyBERT (left) and BERT (right) (averaged over 5 participants and 30K voxels).</figDesc><graphic coords="5,181.17,593.78,117.61,80.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Task similarity matrix from the results of NBT f mri (left) and taxonomy tree (right).</figDesc><graphic coords="5,180.77,231.27,117.92,89.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Task ranking scores with different correlation coefficients in NBT f mri based on TinyBERT (left) and BERT (right).</figDesc><graphic coords="8,60.38,114.23,267.50,178.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistic of tasks used.</figDesc><table><row><cell>Task</cell><cell>Dataset</cell><cell>#Train</cell></row><row><cell>Acceptability</cell><cell>CoLA</cell><cell>8,551</cell></row><row><cell>Natural Language Inference</cell><cell>MNLI</cell><cell>392,702</cell></row><row><cell>Paraphrase</cell><cell>QQP</cell><cell>363,846</cell></row><row><cell>Paraphrase</cell><cell>MRPC</cell><cell>3,668</cell></row><row><cell>Question Answering</cell><cell>QNLI</cell><cell>104,743</cell></row><row><cell>Sentiment Analysis</cell><cell>SST-2</cell><cell>67,349</cell></row><row><cell>Entailment</cell><cell>RTE</cell><cell>2,490</cell></row><row><cell>Textual Similarity</cell><cell>STS-B</cell><cell>5,749</cell></row><row><cell cols="2">Extractive Question Answering SQuAD-2.0</cell><cell>129,941</cell></row><row><cell>Relation Extraction</cell><cell>Semeval-2010</cell><cell>8,000</cell></row><row><cell>Named Entity Recognition</cell><cell>CoNLL-2003</cell><cell>14,042</cell></row><row><cell>Passage Reranking</cell><cell cols="2">MS MARCO 3,213,835</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="2">Method Complexity</cell><cell>TRS ↓</cell><cell></cell></row><row><cell></cell><cell cols="3">TinyBERT BERT</cell></row><row><cell></cell><cell>Baselines</cell><cell></cell><cell></cell></row><row><cell>Random</cell><cell>O(1)</cell><cell>6.0</cell><cell>6.0</cell></row><row><cell>DSE</cell><cell>O(n)</cell><cell>4.9</cell><cell>4.8</cell></row><row><cell>CRA</cell><cell>O(n)</cell><cell>5.0</cell><cell>4.4</cell></row><row><cell>CNM</cell><cell>O(n)</cell><cell>4.1</cell><cell>4.6</cell></row><row><cell>AHP  *</cell><cell>O(n 2 )</cell><cell>1.4</cell><cell>2.5</cell></row><row><cell></cell><cell>Ours</cell><cell></cell><cell></cell></row><row><cell>NBTeeg</cell><cell>O(n)</cell><cell>4.0</cell><cell>4.2</cell></row><row><cell>NBTeye</cell><cell>O(n)</cell><cell>3.6</cell><cell>3.8</cell></row><row><cell>NBTsem</cell><cell>O(n)</cell><cell>3.3</cell><cell>3.9</cell></row><row><cell>NBT f mri</cell><cell>O(n)</cell><cell>2.4</cell><cell>2.9</cell></row></table><note><p>Task ranking scores (TRS) for different task similarity estimation methods. Results denoted by * come from Luo et al. (2022).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>TRS of NBT f mri and CNM for BERT and TinyBERT with different measures of task similarity and correlation coefficients.</figDesc><table><row><cell cols="5">tice 3 . Moreover, NBT has comparable performance</cell></row><row><cell cols="5">to AHP with lower computational time and less</cell></row><row><cell>memory.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Corr. Coef. Task Sim. Method</cell><cell>TRS ↓</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TinyBERT BERT</cell></row><row><cell></cell><cell>R 2</cell><cell>CNM NBT f mri</cell><cell>4.3 2.7</cell><cell>4.8 3.3</cell></row><row><cell>cos</cell><cell>ρ</cell><cell>CNM NBT f mri</cell><cell>4.1 2.9</cell><cell>4.6 3.0</cell></row><row><cell></cell><cell>cos</cell><cell>CNM NBT f mri</cell><cell>4.2 2.9</cell><cell>4.4 3.2</cell></row><row><cell></cell><cell>R 2</cell><cell>CNM NBT f mri</cell><cell>4.4 2.6</cell><cell>4.4 2.7</cell></row><row><cell>ρ</cell><cell>ρ</cell><cell>CNM NBT f mri</cell><cell>4.2 2.4</cell><cell>4.6 2.9</cell></row><row><cell></cell><cell>cos</cell><cell>CNM NBT f mri</cell><cell>4.2 3.0</cell><cell>4.8 3.0</cell></row></table><note><p>Table 2 displays the task ranking scores obtained from various methods. It is evident that NBT outperforms CogTaskonmoy (CNM and CRA), with NBT f mri exhibiting an average improvement of 41.13% over them. Additionally, NBT eeg , NBT eye and NBT sem separately exceed them by 8.93%, 24.40%, and 20.02%. These results suggest that NBT is superior to CogTaskonomy in capturing the relation among NLP tasks by utilizing distinct neural and behavioral data. Although NBT has the same time complexity with CNM, NBT is more efficient than CNM in prac-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Transfer learning results of BERT. Mcc denotes the Matthews correlation coefficient, r s is the Spearman's rank correlation coefficient, and MRR@10 denotes the Mean Reciprocal Rank for the top 10. The ranking for the source task to the target task is denoted in the right parenthesis.</figDesc><table><row><cell>Target Task</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Transfer learning results of TinyBERT. Mcc denotes the Matthews correlation coefficient, r s is the Spearman's rank correlation coefficient, and MRR@10 denotes the Mean Reciprocal Rank for the top 10. The ranking for the source task to the target task is denoted in the right parenthesis.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Compared to Multi-layer Perception, ridge regression has fewer parameters and lower time complexity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>To map a fine-tuned model to fMRI data, NBT only costs 23.3s (for BERT), which is 2.7% of time spent with CNM (879.3s) when the voxel number is 30K</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Correlation selection in NBT f mri</head><p>There are three options for us to calculate the correlation coefficient (R 2 , ρ, cos) on all voxels between predicted values and ground-truth values. We calculate task ranking scores using these options for NBT f mri with TinyBERT and BERT, as shown in Figure <ref type="figure">6</ref>. Our findings suggest that ρ outperforms R 2 in almost all cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Oracle Task Ranking</head><p>After pair-wise transfer learning, we evaluate the performance of models on the validation set of target tasks and report them in Table <ref type="table">4</ref> and<ref type="table">Table 5</ref> for <ref type="bibr">BERT and TinyBERT, respectively.</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Task2vec: Task embedding for meta-learning</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Charless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6430" to="6439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Magdalena</forename><surname>Biesialska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarzyna</forename><surname>Biesialska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta R Costa-Jussa</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09823</idno>
		<title level="m">Continual lifelong learning in natural language processing: A survey</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Toward a brainbased componential semantic representation</title>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">L</forename><surname>Jeffrey R Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">J</forename><surname>Conant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Humphries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">B</forename><surname>Fernandino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rutvik H</forename><surname>Aguilar</surname></persName>
		</author>
		<author>
			<persName><surname>Desai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive neuropsychology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="130" to="174" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An empirical survey of data augmentation for limited data learning in nlp</title>
		<author>
			<persName><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="191" to="211" />
		</imprint>
	</monogr>
	<note>Transactions of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multitask learning in natural language processing: An overview</title>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.09138</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Representation similarity analysis for efficient task taxonomy &amp; transfer learning</title>
		<author>
			<persName><forename type="first">Kshitij</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Roig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12387" to="12396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey on recent approaches for natural language processing in low-resource scenarios</title>
		<author>
			<persName><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Hedderich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heike</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jannik</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietrich</forename><surname>Strötgen</surname></persName>
		</author>
		<author>
			<persName><surname>Klakow</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.201</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2545" to="2568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SemEval-2010 task 8: Multiway classification of semantic relations between pairs of nominals</title>
		<author>
			<persName><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ó</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenza</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Zuco, a simultaneous eeg and eye-tracking resource for natural sentence reading</title>
		<author>
			<persName><forename type="first">Nora</forename><surname>Hollenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Rotsztejn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Troendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Pedroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Langer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Task-embedded control networks for fewshot imitation learning</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bloesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on robot learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="783" to="795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">TinyBERT: Distilling BERT for natural language understanding</title>
		<author>
			<persName><forename type="first">Xiaoqi</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.372</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4163" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Representational similarity analysisconnecting the branches of systems neuroscience</title>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marieke</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">A</forename><surname>Bandettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in systems neuroscience</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Human vs. supervised machine learning: Who learns patterns faster? Cognitive Systems Research</title>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Kühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Goutier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Baier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wolff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="78" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Lin</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohong</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pinghui</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06527</idno>
		<title level="m">Meta reinforcement learning with task embedding and shared policy</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cog-Taskonomy: Cognitively inspired task taxonomy is beneficial to transfer learning in NLP</title>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.64</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="904" to="920" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m">the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016)</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-09">2016. December 9. 2016</date>
			<biblScope unit="volume">1773</biblScope>
		</imprint>
	</monogr>
	<note>CEUR Workshop Proceedings</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural language taskonomy: Which NLP tasks are the most predictive of fMRI brain activity?</title>
		<author>
			<persName><forename type="first">Subba</forename><surname>Reddy Oota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jashn</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veeral</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mounika</forename><surname>Marreddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bapi</forename><surname>Surampudi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.235</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3220" to="3237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Toward a universal decoder of linguistic meaning from brain activation</title>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brianna</forename><surname>Pritchett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Samuel J Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kanwisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evelina</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><surname>Fedorenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">963</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for SQuAD</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2124</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning good features to transfer across tasks and domains</title>
		<author>
			<persName><forename type="first">Zama</forename><surname>Pierluigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriano</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Cardace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessio</forename><surname>De Luigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuele</forename><surname>Tonioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Di</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep model transferability from attribution maps</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengchao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meulder</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Tu</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mattarella-Micke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00770</idno>
		<title level="m">Exploring and predicting transferability across nlp tasks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-05-06">2019. 2019. May 6-9, 2019</date>
			<biblScope unit="volume">7</biblScope>
			<pubPlace>New Orleans, LA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>th International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A large dataset of semantic ratings and its computational extension</title>
		<author>
			<persName><forename type="first">Shaonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiting</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">106</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An fmri dataset for concept representation with semantic feature annotations</title>
		<author>
			<persName><forename type="first">Shaonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">721</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Meta-learning for few-shot natural language processing: A survey</title>
		<author>
			<persName><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09604</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3712" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multi-task support vector machine with pinball loss. Engineering Applications of Artificial Intelligence</title>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Zhong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page">104458</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
