<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PICK: Polished &amp; Informed Candidate Scoring for Knowledge-Grounded Dialogue Systems</title>
				<funder ref="#_mQQ8Ed4">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_Rm5PDWu">
					<orgName type="full">Hong Kong Research Grants Council</orgName>
					<orgName type="abbreviated">RGC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Bryan</forename><surname>Wilie</surname></persName>
							<email>bwilie@connect.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Artificial Intelligence Research (CAiRE)</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Artificial Intelligence Research (CAiRE)</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Willy</forename><surname>Chung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Artificial Intelligence Research (CAiRE)</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Samuel</forename><surname>Cahyawijaya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Artificial Intelligence Research (CAiRE)</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Holy</forename><surname>Lovenia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Artificial Intelligence Research (CAiRE)</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Artificial Intelligence Research (CAiRE)</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PICK: Polished &amp; Informed Candidate Scoring for Knowledge-Grounded Dialogue Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8FBE150D72F53E7D1FE95E8A309A09D2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T14:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Grounding dialogue response generation on external knowledge is proposed to produce informative and engaging responses. However, current knowledge-grounded dialogue (KGD) systems often fail to align the generated responses with human-preferred qualities due to several issues like hallucination and the lack of coherence. Upon analyzing multiple language model generations, we observe the presence of alternative generated responses within a single decoding process. These alternative responses are more faithful and exhibit a comparable or higher level of relevance to prior conversational turns compared to the optimal responses prioritized by the decoding processes. To address these challenges and driven by these observations, we propose Polished &amp; Informed Candidate Scoring (PICK), a generation re-scoring framework that empowers models to generate faithful and relevant responses without requiring additional labeled data or model tuning. Through comprehensive automatic and human evaluations, we demonstrate the effectiveness of PICK in generating responses that are more faithful while keeping them relevant to the dialogue history. Furthermore, PICK consistently improves the system's performance with both oracle and retrieved knowledge in all decoding strategies. We provide the detailed implementation in 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge-grounded dialogue (KGD) has been introduced as a means to ground conversation towards the provided knowledge, thereby enabling the generation of informative and engaging responses <ref type="bibr" target="#b1">(Dinan et al., 2019;</ref><ref type="bibr" target="#b37">Zhou et al., 2018)</ref>. Despite the advancements in training KGD systems to convincingly simulate human language on a linguistic plane, these systems still struggle with the challenge of producing responses that align with 1 https://github.com/bryanwilie/pick Knowledge snippet Due to his powerful and very large vocal range and energetic live performances, Rose has been named one of the greatest singers of all time by various media outlets , including "Rolling Stone" and "NME".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dialogue History</head><p>Speaker 1: Didn't their guitarist slash leave the band? Speaker 2: When did he leave did he release the six albums with them Speaker 1: I heard Axl Rose was known for throwing tantrums</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response</head><p>Vanilla He was known for throwing tantrums PICK He has been named one of the greatest singers of all time by various media outlets Table <ref type="table">1</ref>: PICK empowers models to generate more faithful to the knowledge snippet and serve as a more appropriate reply to the conversational context. In this sample, the response prioritized by PICK is more grounded in external knowledge (highlighted in blue) than the optimal response prioritized by the decoding process (i.e., beam search) and is also more relevant to the dialogue context. Here, the vanilla response repeats the dialogue history (highlighted in red).</p><p>those human-preferred qualities. Such deficits can be attributed to various issues, e.g., hallucination as well as the lack of coherence and engagingness in the generated responses <ref type="bibr" target="#b3">(Fu et al., 2022;</ref><ref type="bibr" target="#b26">Shuster et al., 2022;</ref><ref type="bibr">Rashkin et al., 2021b;</ref><ref type="bibr">Zhao et al., 2020a)</ref>.</p><p>Numerous methodologies have been investigated to leverage the potential of various training and decoding methods to address these identified issues. For instance, the recent human quality alignment methods, such as <ref type="bibr" target="#b18">Ouyang et al. (2022)</ref>, hinge on collecting extensive human annotations, followed by the reward model fine-tuning to approximate human preference. This process then guides the optimization of the language model (LM) through reinforcement learning. While this approach has demonstrated promising results, it is noteworthy that accumulating such a significant volume of manual human data is highly resource-intensive in terms of both time and human labor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>It is very important, it is a very important way of measuring the financials of a business.</p><p>. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response quality scorer</head><p>Yes, accounting is a great way to keep track of the results of an organization's economic activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generate</head><p>It is also called the language of business. It is the communication of financial information to a variety of users.</p><p>Yes, accounting is a great way to keep track of the results of an organization's economic activities. I do, accounting is all about keeping track of finances especially for businesses.</p><p>Yes, it is very important in order for businesses to see where their money is going.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Snippet</head><p>Accounting, which has been called the "language of business", measures the results of an organization's economic activities ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dialogue History</head><p>Response candidates Through analyzing various LM generations, we observe that within one decoding process, there exist alternative generated responses that are more faithful and relevant to prior conversational turns. These candidates, however, are overlooked by the decoding processes as they are not prioritized as the optimal responses. Driven by these observations, we propose a straightforward yet effective humanaligned re-ranking framework to direct model responses closer to KGD qualities.</p><p>We introduce Polished &amp; Informed Candidate Scoring (PICK), a generation re-scoring framework for KGD tasks, which empowers models to generate optimal dialogue responses that are more faithful to the knowledge provided and relevant to the dialogue history without requiring additional model tuning. The proposed framework is also modelagnostic; thus, it can be applied to various LMs with different architectures and sizes. Furthermore, it circumvents the need for supplementary labeled data by exploiting off-the-shelves metrics that correlate well with human judgment. While considering its contextual relevance to the dialogue history, utilizing these metrics allows the model to produce better responses. However, to enable the generation of responses that are more faithful and relevant, it is essential to condition the response on the dialogue history and accurate knowledge grounding. To do so, we explore various metrics that ensure the response is aligned with the knowledge and utilize the existing automatic metrics that correlate well with human judgment. Our experiments and human evaluation show that PICK enables models to produce responses more faithful to the provided knowledge and relevant to the dialogue history.</p><p>Our contributions to this work are three-fold. (1)</p><p>We propose PICK, a generation re-scoring framework for KGD that empowers models to generate dialogue responses that are more faithful to the provided knowledge and relevant to the dialogue history. The proposed framework is simple yet effective; it does not require further model tuning and additional labelled data for language modelling alignment. ( <ref type="formula">2</ref>) We analyze the improvement from PICK-reranked responses in the systems with both oracle and retrieved knowledge and show that PICK consistently improves the performance in all decoding strategies. (3) We investigate the impact of diverse scoring metrics and decoding settings on generation quality. Then, we present the best scoring and decoding configurations for PICK on KGD tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Knowledge-Grounded Dialogue <ref type="bibr" target="#b1">Dinan et al. (2019)</ref> develop a large dataset with conversations directly grounded on knowledge retrieved from Wikipedia. Alongside the work, recent works aim to build dialogue models that could conduct faithful and relevant knowledgeable discussions on opendomain topics <ref type="bibr" target="#b11">(Li et al., 2022;</ref><ref type="bibr" target="#b13">Liu et al., 2021;</ref><ref type="bibr" target="#b29">Xu et al., 2022</ref><ref type="bibr" target="#b30">Xu et al., , 2023))</ref>. Aiming to improve informativeness, a knowledge selection process is introduced to determine which specific elements of knowledge are informative to the dialogue <ref type="bibr" target="#b8">(Kim et al., 2020;</ref><ref type="bibr">Zhao et al., 2020b)</ref>. Further, <ref type="bibr" target="#b10">Li et al. (2020)</ref> propose learning how knowledge is expressed to improve coherence and knowledge relevance. <ref type="bibr" target="#b27">Shuster et al. (2021)</ref> utilize neural-retrieval-in-the-loop architectures to develop models that maximize knowledgeability while retaining conversational ability. <ref type="bibr">Rashkin et al. (2021b)</ref>   <ref type="formula">2019</ref>) evaluate responses for coherence and engagement using a supervised conversational evaluator with human-annotated labels. <ref type="bibr" target="#b31">Yi et al. (2019)</ref> collect human interaction data as implicit human feedback. <ref type="bibr" target="#b4">Hancock et al. (2019)</ref> develop an agent that would ask for feedback to improve its dialogue abilities further. Those works accumulate manual human data and are highly resource-intensive in terms of time and human labor. On the other hand, there are also works that re-rank response candidates to improve the dialogue response quality. <ref type="bibr" target="#b17">Mei et al. (2017)</ref> utilize the Latent Dirichlet Allocation (LDA) method to learn document-level latent topics to select the best continuation based on document-level topic-level matching. <ref type="bibr" target="#b28">Welleck et al. (2019)</ref> improve the consistency by re-rank utterances using an NLI model trained on a Dialogue NLI dataset that they created for the purpose. <ref type="bibr" target="#b9">Ko et al. (2019)</ref> train four classifiers on synthetically generated data to re-rank plausible sentences. Unlike those works, we leverage off-the-shelf automatic metrics that correlate well with human judgment on conversation-level qualities; hence it does not require additional labeled data for the language modeling alignment. Furthermore, we devise a framework that doesn't require further model tuning to promote faithful and relevant candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Knowledge-grounded dialogue (KGD) systems are built to be informative teachers. Such systems must be faithful to one or more source documents we implicitly trust and serve as an appropriate reply to the conversational context <ref type="bibr">(Rashkin et al., 2021a;</ref><ref type="bibr" target="#b32">Zhan et al., 2021;</ref><ref type="bibr" target="#b6">Honovich et al., 2021)</ref>. In KGD systems, a model is trained to generate a response based on the dialogue utterances with the user and ground to the knowledge snippet. We denote a KGD dataset as {D n } N n=1 . At every turn t we have dialogue history at turn t denoted as</p><formula xml:id="formula_0">D t = {(U i , S i )} t i=1</formula><p>, where U t is the user utterance and S t the system response. Each of these S t responses is grounded to knowledge snippets K t that are retrieved from a knowledge base As illustrated in Figure <ref type="figure" target="#fig_1">1</ref>, our proposed framework takes in input X t = (T t , K t , D t-1 , U t ) with T t resembling the conversation topic at turn t, to a fine-tuned model f θ to generate a relevant and faithful response sequence Ŝt . The concatenation of D t-1 and U t is a dialogue history.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Re-ranking Framework</head><p>Beam search and nucleus sampling decoding methods allow the model to generate multiple responses (i.e., hypotheses) to the same inputs. Instead of selecting the response with the highest probability from the model, we propose a re-ranking method that ensures better relevance and faithfulness of the generated responses without further tuning. Our approach treats all of the r hypotheses as a pool of r response candidates C = {C 1 , ..., C r } to be further ranked based on their qualities. We evaluate each response candidate with ready-to-use scorers to get its quality score of µ. Our goal is to select the best scoring candidate according to their associated scores C µ = {µ(C 1 ), ..., µ(C r )}, that is, to identify the best dialogue response candidate Ŝt according to the metrics, which is given by:</p><formula xml:id="formula_1">Ŝt = arg max C j ∈C {µ(C 1 ), ..., µ(C r )}</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decoding Strategy</head><p>To produce the top-r response candidates C = {C 1 , ..., C r }, we take the same input X t to the finetuned model f θ and perform generations with the number of return sequences to be r, with r being larger than 1. Each response candidate C j is an independently computed returned sequence from the search hypotheses or random sampling. Although by both paradigms, the last (r -1) hypotheses are seen as inferior response candidates, we will later show that it is not the case and that by evaluating their qualities using the ready-to-use automatic metric, we can let the same fine-tuned model f θ reach a more optimal response quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Response Quality Scorer</head><p>KGD aims to ground the conversation by generating responses that are faithful to the provided knowledge and relevant to the dialogue history. To achieve this, we leverage off-the-shelf automatic  metrics to evaluate the quality of response candidates. These metrics allow us to assess the faithfulness and the relevance of the responses without the need for additional labeled data. We consider the qualities of the response candidate w.r.t the dialogue history µ D and the input knowledge snippet µ K to construct the final quality score of µ. We elaborate on the metric corresponding to each aspect score in Section 4.2 and 4.3. The relevance score µ D is calculated given the response candidates and the dialogue history, µ D (C j , (D t-1 , U t )), which the faithfulness score µ K is calculated regarding the input knowledge snippet, µ K (C j , K t ). In this work, we consider both qualities of the response candidate equally important; thus, we derive the final quality score µ based on the sum of µ D and µ K . Our proposed method allows more randomness in the decoding process, which may cause high meaningless repetition in some r hypotheses. To filter this, we remove the hypotheses that contain repetitive words. We also filter hypotheses that contain a word more than 30 characters long since words that long are not likely to occur in an English general text.<ref type="foot" target="#foot_0">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Models</head><p>We use Wizard of Wikipedia (WoW) <ref type="bibr" target="#b1">(Dinan et al., 2019)</ref>, a large-scale corpus of multi-turn knowledge-grounded dialogues between an "apprentice" and a "wizard", to conduct our experiments in developing the KGD systems. We use the same split in <ref type="bibr" target="#b1">(Dinan et al., 2019)</ref> as stated in <ref type="bibr" target="#b25">(Shuster et al., 2020)</ref>. We aim to produce better responses, thus we focus on only modeling "wizard" response utterances in the dialogue where they respond to the "apprentice" utterances. The data statistics of WoW are shown in Table <ref type="table" target="#tab_1">2</ref>. We adopt a pre-trained GPT-2 <ref type="bibr" target="#b21">(Radford et al., 2019)</ref>, and T5small <ref type="bibr" target="#b22">(Raffel et al., 2020)</ref> as the backbones. We fine-tune both models and limit the maximum sequence length to 512. Maximizing our GPU (RTX 2080Ti) capacity, we train the GPT-2 model in a batch size of 4 and the T5 model in a batch size of 8 for 10 epochs with early stopping patience of 3. We train using all of the training data and use the dev (seen topics) split and monitor the model's loss in this split to do the early stopping and to choose the best model to use in the experiment. More training details are provided in §4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Faithfulness Score</head><p>Faithfulness problem can also be considered an intrinsic hallucination problem for KGD tasks. Following <ref type="bibr" target="#b27">Shuster et al. (2021)</ref>, we leverage Knowledge F1 (KF1)<ref type="foot" target="#foot_1">3</ref> , calculated based on the unigram overlap between the generated response and the input knowledge snippet, to assess the faithfulness of responses. There are also other alternative n-grambased automatic metrics such as BLEU <ref type="bibr" target="#b19">(Papineni et al., 2002)</ref>, ROUGE <ref type="bibr" target="#b12">(Lin, 2004)</ref>, entailment measurement from a state-of-the-art natural language interference (NLI) model <ref type="bibr" target="#b14">(Liu et al., 2019)</ref>, and the similarity measurement from BLEURT <ref type="bibr" target="#b24">(Sellam et al., 2020)</ref>. We further investigate the impact of different faithfulness scorers in Section 6.1 and find out that KF1 shows its distinct effectiveness in ensuring both faithfulness and overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Relevance Score</head><p>Overlap-based automatic evaluation metrics are known to be ineffective in distinguishing the relevance between the generated response with the dialogue history due to the one-to-many nature of dialogue <ref type="bibr" target="#b34">(Zhao et al., 2017;</ref><ref type="bibr" target="#b30">Yeh et al., 2021)</ref>. Therefore, we explore reference-free model-based metrics on top of them. Specifically, we utilize the FED metric <ref type="bibr" target="#b16">(Mehri and Eskenazi, 2020)</ref> as the relevance scorer. FED is an unsupervised evaluation metric that uses DialoGPT <ref type="bibr" target="#b33">(Zhang et al., 2020)</ref> to measure 18 fine-grained turn-and dialogue-level qualities of dialogue. It calculates the likelihood of manually designed follow-up utterances to measure multiple qualities of dialogue. Moreover, it is proven to correlate well with human judgment.</p><p>We follow the hierarchical groupings from <ref type="bibr" target="#b20">(Phy et al., 2020)</ref>   we group semantically appropriate, understandable, and fluent as turn-level metrics that measure the basic qualities of responses. We see the additional qualities as the ones that make the response more likeable and group the interesting, engaging, specific, relevant, and correct measurements into one. Similarly, at the dialogue level (DL), we group coherent, error recovery, consistent, and diverse as the dialogue-level basic qualities of responses. At the same level, we also group depth, likeable, understandable, flexible, informative, and inquisitive as dialogue-level metrics that measure the further qualities of responses. On top of that, we also experiment with combining each level of metrics and all of the measurements to find the best combination to produce responses relevant to the dialogue history.</p><p>We also explore another reference-free modelbased metric, USL-H <ref type="bibr" target="#b20">(Phy et al., 2020)</ref>, for comparison. USL-H combines three models trained to determine whether a response is valid and grammatically correct and to evaluate the sensibleness and the likelihood of a given response. The analysis is included in Section 6.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Baselines</head><p>We select baseline models that utilize gold knowledge snippets in their generation process. We take the performances of MemNet <ref type="bibr" target="#b1">(Dinan et al., 2019)</ref>, dodecaDialogue <ref type="bibr" target="#b25">(Shuster et al., 2020)</ref>, GPT-2 and T5 with control code and resampling <ref type="bibr">(Rashkin et al., 2021b)</ref>, and PLUG-Golden Knowledge <ref type="bibr" target="#b11">(Li et al., 2022)</ref> as our baselines. We also experiment with PICK in the settings where the provided knowledge is retrieved instead of using the oracle knowledge. We leverage <ref type="bibr">KnowledGPT Zhao et al. (2020b)</ref> and perform a similar procedure elaborated in Section 3, to select responses that are more relevant to the dialogue history and the ones that are more faithful to the retrieved knowledge. We di-  rectly utilize the codes and models provided 4 and adjust the generation parameters as required.</p><p>We note the vanilla responses performance as our lower bound: the responses performance on each decoding method without PICK (i.e. taking the top-1 hypotheses from beam search).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Training details</head><p>During training, the concatenated utterances are delimited using speaker ID of either &lt;speaker1&gt; or &lt;speaker2&gt;, and the concatenations of the topic, knowledge snippet, and the utterances are separated by a separator token \n. We experiment using learning rates (lr) of 1e -5, 5e -4, 1e -4, 5e -4 to fine-tune the models, and then we pick the models with the lowest loss on the dev (seen topics) split to be the models that we will be using throughout the experiments. Ultimately, the best GPT-2 model is fine-tuned with lr of 1e-5 and the T5 with 5e-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Evaluation</head><p>Automatic Metrics We evaluate the final response qualities by comparing them to the gold responses. We perform the automatic evaluation using BLEU-4 <ref type="bibr" target="#b19">(Papineni et al., 2002)</ref>, ROUGE-L <ref type="bibr" target="#b12">(Lin, 2004)</ref>, and unigram-F1 3 . We implement the BLEU measurements following Rashkin et al. (2021b). To make a fair comparison with the previous work, we utilize BLEU-4 scoring as it is implemented in Rashkin et al. (2021b) and ROUGE-L scoring (the mean F1 measures) as it is implemented in 5 . Further, we also use KF1 as stated in Section 4.2 for faithfulness measurement.</p><p>4 https://github.com/zhaoxlpku/KnowledGPT.git 5 https://huggingface.co/spaces/evaluate-metri c/rouge Human Evaluation We conduct manual evaluations to measure the qualities of the generated responses from two aspects: Faithfulness and Relevance. We take 100 random generation samples from the test (unseen topics) split and ask crowdsourced annotators<ref type="foot" target="#foot_2">6</ref> to evaluate on a 4-point Likert scale from 1 (low quality) to 4 (high quality). We ask for three level-1 (all-kinds) contributors and three level-3 (experienced only) contributors and report their average scores. The complete annotation guideline is attached in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results with Oracle Knowledge</head><p>Overall, as observed in Table <ref type="table" target="#tab_3">3</ref>, the proposed method achieved significantly better performances than the baselines from the previous works, especially in the comparison of BLEU-4 scores. In this table, the PICK responses are all re-ranked based on the sum of FED turn-level basic metrics and KF1. The proposed method significantly improves the performances of all models and decoding methods in all of the BLEU-4, ROUGE-L, F1, and KF1 metrics. Interestingly, for the top-k and top-p sampling decoding that previously gained low scores BLEU-4, ROUGE-L, F1, and even KF1 on their vanilla responses, there exist alternative responses that have a more similar quality to the gold response and our proposed re-ranking and scoring framework promotes that.</p><p>All the re-ranked responses also obtained a huge increase of KF1 compared to their vanilla baseline, especially in the top-k and the top-p sampling, where the vanilla KF1 is far much lower than the re-ranked response's KF1. Although the scores of 986 BLEU-4, ROUGE-L, and F1 are comparable in the inference of the T5 model using beam search, we can still see the improvement in KF1, signifying that the proposed method better addresses the use of knowledge in its response. Lastly, PICK also closes the performance gap between unseen and seen evaluation, as it is especially observed in the GPT-2 model performances. We provide samples of the responses in Table <ref type="table">A1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results with Retrieved Knowledge</head><p>We reproduce the greedy decoding performance similar to what was reported in <ref type="bibr">(Zhao et al., 2020b)</ref>.</p><p>Here, we use the same scoring metrics on PICK as in §5.1. Although other decoding methods' underperform in comparison to greedy in the knowledge retrieval setting (see Table <ref type="table" target="#tab_5">4</ref>), PICK still shows improvement over each decoding method taken individually as the BLEU-4, ROUGE-L, F1, and KF1 scores all increase. Interestingly, although our method improves the generated response's performance compared to its vanilla counterpart, the top performances are still comparable with the greedy decoding performance, except for the significantly better KF1 on the responses re-ranked by PICK. It is important to note that the KF1 here is calculated w.r.t the retrieved knowledge instead of the gold knowledge. We also investigate the underperformance of KnowledGPT being inferenced through decoding methods other than greedy, and we found that the issue persists through repeated trials in different settings. We leave this issue out of the scope of this paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Human Evaluation</head><p>We conduct the manual evaluation on responses from GPT-2 and T5 models decoded with beam search 10. We compare both the vanilla (r=1) and the PICK responses (r=10) and evaluate the quality of the responses in the aspect of Relevance and Faithfulness. Table <ref type="table">5</ref> shows that PICK responses from GPT-2 and T5 models are more faithful and relevant than the vanilla generations. These findings correlate well with the automatic results shown in Table <ref type="table" target="#tab_3">3</ref>, where for both of the responses, PICK responses achieved higher BLEU-4, ROUGE-L, F1, and KF1 scores in comparison to the vanilla responses. We attach a detailed visualization of the Likert score distribution in Figure <ref type="figure">B</ref>.</p><p>It is also known from previous works that attempts to make the system more faithful usually lead to trade-offs between the response's relevance and faithfulness scores <ref type="bibr">(Rashkin et al., 2021b)</ref>, either due to the response being not quite as pertinent to the previous conversation turns or it gives overly extractive responses. This result also showcases the merit of the proposed re-ranking method to improve the faithfulness of the responses without a trade-off on the response's relevance towards the previous conversation turn.  6 Analysis and Discussion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Rescoring Metrics</head><p>We study the effectiveness of automatic metrics explored in Section 4.2 and 4.3. We employ GPT-2+PICK with each automatic metric as the scoring method by normalizing the performances on each of their mean and standard deviations for a fairer comparison. We report the normalized comparison as a heatmap in Figure <ref type="figure" target="#fig_4">2</ref>. The best performance is achieved by using FED turn-level that measures the basic qualities of responses w.r.t the dialogue history (FED turn-level basic) alongside KF1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Decoding Strategy</head><p>We perform ablation of GPT-2+PICK and vanilla on the decoding methods to find the optimum configurations with our proposed method. For beam search, we increase the number of beams starting from 10. For the top-k and nucleus sampling, we increase the threshold of k and p to perform the sampling, starting from k = 3 and p = 0.3. For each experiment, we keep r = 10, and we note the comparison of performances w.r.t the gold as a probe towards the degree of the responses being in a certain desired quality range that the gold responses reflect (Figure <ref type="figure" target="#fig_5">3</ref>).</p><p>From Figure <ref type="figure" target="#fig_5">3</ref>, We observe that increasing the number of beams does not improve the performance of the generated responses, while as the p and k increase, the performances degrade. We conjecture that loosening the respective p and k sampling thresholds weakens the mitigation of bad continuations and, in turn, produces worse generations (i.e., generations with considerably low perplexity <ref type="bibr" target="#b5">(Holtzman et al., 2019)</ref>). The two sampling strategies show different optimum thresholds, as top-k sampling responses start to deteriorate with k larger than 3, while for the nucleus sampling, we see benefits in relaxing the p threshold to 0.5. We conjecture that due to the probability selection of the nucleus sampling, top-p tokens within p ≤ 0.5 could still be reliable, as it produces generations with considerably low perplexity as mentioned in <ref type="bibr" target="#b5">(Holtzman et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Number of Return Sequences (r)</head><p>We also perform further ablation of GPT-2+PICK and vanilla on varying r. We extend r on the nucleus sampling with p = 0.5, and on the beam search, we set the r to follow the number of beams n to retain the top n choices when a new token in the sequence is generated. The performance comparisons are noted in Figure <ref type="figure" target="#fig_6">4</ref>.</p><p>Our observation shows that increasing r in both the search and sampling decoding experiments promotes the existence of responses that are more similar to the gold response. With the gold response holding the desired qualities we aim to achieve, these findings also indicate that increasing the r could help increase the response qualities generated by the same model to some extent. Figure <ref type="figure" target="#fig_6">4</ref>. shows that PICK response performance begins to saturate around n = 10 and r = 10 in the beam search and nucleus sampling experiment, most likely because the best candidate response is consistently found within that range of return sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Error Analysis</head><p>To better understand our method's limitation, we provide manually sampled study cases with GPT-2+PICK using beam search (n = 50, r = 50), in which better responses are not selected by the scorer. We observe three kinds of errors. First, the current metric fails to promote the selection of better responses. We conjecture that this happens due to the low correlation of the automatic metrics towards the human judgments <ref type="bibr" target="#b30">(Yeh et al., 2021)</ref>; hence, implementing better human preference metrics will aid in better response promotion.</p><p>Second, in some cases, substandard responses are selected due to their high overlap with the knowledge snippet. This could be because the metrics used rely on the spurious correlation between attribution and word overlap and thus do not reliably distinguish attributable abstractive responses <ref type="bibr" target="#b15">(McCoy et al., 2019;</ref><ref type="bibr" target="#b2">Dziri et al., 2022)</ref>. We perform a further ablation study on this error in Appendix D. Third, the knowledge snippets provided are irrelevant to the dialogue history. We provide these entries of case study samples in Table <ref type="table" target="#tab_8">A2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This work investigates the alignment of KGD responses to faithfulness and relevance. We propose PICK, a straightforward yet effective generation re-ranking framework for KGD. PICK is modelagnostic, does not require further model tuning, nor requires additional labelled data for the language modelling alignment. Experimental results show that the proposed method enables models to produce better responses that are more faithful to the provided knowledge and relevant to the dialogue history.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dialogue samples</head><p>We provide the dialogue samples in Table <ref type="table">A1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge snippet</head><p>Due to his powerful and very large vocal range and energetic live performances, Rose has been named one of the greatest singers of all time by various media outlets, including "Rolling Stone" and "NME". Dialogue History Speaker 1: Didn't their guitarist slash leave the band? Speaker 2: When did he leave did he release the six albums with them Speaker 1: I heard Axl Rose was known for throwing tantrums </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge snippet</head><p>, the books have sold more than 500 million copies worldwide, making them the best-selling book series in history, and have been translated into seventy-three languages. Dialogue History Speaker 1: I liked it because of all the different animals, I am in love with animals, and the ones they made were so unique! Speaker 2: Harry Potter has attracted a wide adult audience, as well as younger readers. If you go to Universal you will see whole families dressed up! Speaker 1: I know everyone was young when everyone was into it, it was huge when I was young, like all the rage Response Vanilla It's hard to imagine how much the books have sold worldwide!It must be so popular now! PICK I agree, the books have sold more than 500 million copies worldwide, and they have been translated into seventy-three languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold</head><p>The books have sold 500 million copies worldwide, making it the best-selling book series in history. Wow that is impressive! Table <ref type="table">A1</ref>: Samples of the responses from the beam search (n=10), top-k sampling, and nucleus sampling generated by GPT-2 using the vanilla, re-ranked, and oracle methods. We also attach the gold responses for each dialogue. The vanilla response is not as faithful w.r.t. the knowledge snippet, especially compared to the other 3 responses.</p><p>The re-ranked responses, however, are more faithful and yet still relevant to the dialogue history as they are not a verbatim copy of the knowledge snippet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details of Human Evaluation Results</head><p>We further show the distribution of Likert scores in Figure <ref type="figure" target="#fig_7">A1</ref> in addition to the average reports. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Error analysis</head><p>We provide the dialogue samples of our error analysis in Table <ref type="table" target="#tab_8">A2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge snippet</head><p>The first match of American football was played on November 6, 1869, between two college teams, Rutgers and Princeton, under rules based on the association football rules of the time.</p><p>Dialogue History Speaker 1: Same here. My favorite team is the Saints! Speaker 2: The first football game was played on November 6, 1869. Speaker 1: Wow! I did not know it was that old. Do you know who the teams were? Response PICK The first match of American football was played on November 6, 1869 between two college teams, Rutgers and Princeton, under rules based on the association football rules of the time. Other candidate The teams were Rutgers and Princeton.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold</head><p>The teams were Rutgers and Princeton. Two college teams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge snippet</head><p>The maximum score is 300, which is achieved by getting 12 strikes in a row. Dialogue History Speaker 1: That's cool! So I can just get started again. :) Do you know much about bowling? Speaker 2: I am knowledgeable. In bowling your target is to knock all pins in the lane. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>accounting in school. Do you have any interest in the topic?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of PICK. Instead of taking the response with the highest joint probability over the generated tokens, we select the response with the highest overall response quality score on faithfulness and relevance from the top-r responses. We propose to assess the response candidates' quality based on the dialogue history and the corresponding knowledge without further tuning. Simple yet effective, PICK ensures better relevance and coherence of the generated response.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>PICK enables models to produce responses more faithful to the provided knowledge and relevant to the dialogue history, as shown from the human evaluation on responses from GPT-2 and T5 models. In each section, * indicates that this result is significantly better (p-value &lt; 0.05) from their respective baseline comparison. See Figure B for a detailed visualization of the Likert score distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparing combinations of automatic evaluation of response qualities w.r.t dialogue history and knowledge snippet, PICK with FED turn-level basic metrics and KF1 produced responses with the best qualities. The comparisons are shown here as a heatmap of the sum of mean-normalized BLEU-4, ROUGE-L, F1, and KF1 w.r.t gold responses. The x-axis labels the knowledge-oriented metrics, and the y-axis labels the dialogue-history-oriented metrics used in the comparisons. TL denotes turn level, and DL denotes dialogue level.</figDesc><graphic coords="7,306.14,70.87,218.27,155.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance comparison with unigram-F1 while varying the number of beams, k and p in respectively beam search, top-k and top-p sampling with r kept at 10. Extending the number of beams does not help, but k and p threshold help to mitigate a bad response formation.</figDesc><graphic coords="8,77.21,73.21,145.14,79.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance comparison with unigram-F1 while varying r to follow the number of beams (left figure) and varying r on nucleus sampling with p = 0.5 (right figure). Increasing r in both the search and sampling decoding experiments promotes the existence of responses that are more similar to the gold response.</figDesc><graphic coords="8,142.30,210.09,145.14,81.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure A1 :</head><label>A1</label><figDesc>Figure A1: Distribution of human evaluation's Likert score on Faithfulness and Relevance</figDesc><graphic coords="12,93.55,567.87,408.18,116.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="15,70.87,177.20,453.54,224.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="15,70.87,418.71,453.55,208.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="16,70.87,193.56,453.55,158.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="16,70.87,368.93,453.54,241.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of Wizard of Wikipedia.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>and separate the fine-grained metrics in FED between basic (w.r.t understandability) and further (w.r.t likeability) response qualities, both at the turn-and dialogue-level. At the turn level (TL),</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Test (seen topics)</cell><cell></cell><cell></cell><cell cols="2">Test (unseen topics)</cell><cell></cell></row><row><cell>Models</cell><cell cols="2">BLEU-4 ROUGE-L</cell><cell>F1</cell><cell>KF1</cell><cell cols="2">BLEU-4 ROUGE-L</cell><cell>F1</cell><cell>KF1</cell></row><row><cell>Baselines</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MemNet (w/ aux loss) (Dinan et al., 2019)</cell><cell>1.5</cell><cell>-</cell><cell>35.5%</cell><cell>-</cell><cell>0.3</cell><cell>-</cell><cell>32.2%</cell><cell>-</cell></row><row><cell>dodecaDialogue (Shuster et al., 2020)</cell><cell>10</cell><cell>-</cell><cell>38.4%</cell><cell>-</cell><cell>9.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Controlled GPT-2 (Rashkin et al., 2021b)</cell><cell>8.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>8.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Controlled T5 (Rashkin et al., 2021b)</cell><cell>8.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>8.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PLUG-Golden Knowledge (Li et al., 2022)</cell><cell>11.5</cell><cell>31.1</cell><cell cols="2">36.0% 47.8%</cell><cell>8.8</cell><cell>29.0</cell><cell cols="2">33.4% 46.0%</cell></row><row><cell>GPT-2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Greedy</cell><cell>12.4</cell><cell>29.9</cell><cell cols="2">32.6% 48.6%</cell><cell>12.1</cell><cell>29.9</cell><cell cols="2">32.3% 46.8%</cell></row><row><cell>Beam search (n = 5, r = 1)</cell><cell>15.0</cell><cell>33.1</cell><cell cols="2">35.6% 64.5%</cell><cell>13.9</cell><cell>32.2</cell><cell cols="2">34.5% 60.3%</cell></row><row><cell>+ PICK (n = 5, r = 5)</cell><cell>16.6</cell><cell>34.1</cell><cell cols="2">37.0% 73.7%</cell><cell>15.6</cell><cell>33.7</cell><cell cols="2">36.4% 71.0%</cell></row><row><cell>Beam search (n = 10, r = 1)</cell><cell>15.4</cell><cell>33.4</cell><cell cols="2">35.8% 68.9%</cell><cell>14.3</cell><cell>32.5</cell><cell cols="2">34.7% 64.5%</cell></row><row><cell>+ PICK (n = 10, r = 10)</cell><cell>16.7</cell><cell>34.5</cell><cell cols="2">37.4% 80.4%</cell><cell>16.0</cell><cell>34.5</cell><cell cols="2">37.1% 78.2%</cell></row><row><cell>Top-k sampling (k = 3, r = 1)</cell><cell>8.7</cell><cell>26.3</cell><cell cols="2">29.0% 39.7%</cell><cell>8.1</cell><cell>25.5</cell><cell cols="2">28.2% 37.8%</cell></row><row><cell>+ PICK (k = 3, r = 10)</cell><cell>14.9</cell><cell>33.0</cell><cell cols="2">36.2% 67.6%</cell><cell>14.2</cell><cell>32.7</cell><cell cols="2">35.6% 64.6%</cell></row><row><cell>Top-p sampling (p = 0.5, r = 1)</cell><cell>11.5</cell><cell>28.3</cell><cell cols="2">31.2% 46.2%</cell><cell>10.4</cell><cell>27.6</cell><cell cols="2">30.1% 43.5%</cell></row><row><cell>+ PICK (p = 0.5, r = 10)</cell><cell>16.0</cell><cell>34.1</cell><cell cols="2">37.2% 72.7%</cell><cell>15.2</cell><cell>34.0</cell><cell cols="2">36.9% 70.2%</cell></row><row><cell>T5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Greedy</cell><cell>14.7</cell><cell>33.0</cell><cell cols="2">35.6% 56.0%</cell><cell>14.4</cell><cell>32.4</cell><cell cols="2">35.0% 56.2%</cell></row><row><cell>Beam search (n = 5, r = 1)</cell><cell>16.3</cell><cell>34.8</cell><cell cols="2">37.7% 77.8%</cell><cell>15.6</cell><cell>34.7</cell><cell cols="2">37.4% 78.8%</cell></row><row><cell>+ PICK (n = 5, r = 5)</cell><cell>16.3</cell><cell>34.9</cell><cell cols="2">37.8% 79.6%</cell><cell>15.6</cell><cell>34.8</cell><cell cols="2">37.6% 80.2%</cell></row><row><cell>Beam search (n = 10, r = 1)</cell><cell>16.2</cell><cell>34.8</cell><cell cols="2">37.7% 81.8%</cell><cell>15.5</cell><cell>34.7</cell><cell cols="2">37.5% 82.7%</cell></row><row><cell>+ PICK (n = 10, r = 10)</cell><cell>16.1</cell><cell>34.8</cell><cell cols="2">37.7% 84.3%</cell><cell>15.4</cell><cell>34.8</cell><cell cols="2">37.6% 84.8%</cell></row><row><cell>Top-k sampling (k = 3, r = 1)</cell><cell>11.7</cell><cell>30.3</cell><cell cols="2">33.3% 49.3%</cell><cell>11.4</cell><cell>29.9</cell><cell cols="2">32.8% 49.8%</cell></row><row><cell>+ PICK (k = 3, r = 10)</cell><cell>15.9</cell><cell>34.2</cell><cell cols="2">37.3% 71.8%</cell><cell>15.2</cell><cell>34.0</cell><cell cols="2">37.0% 72.4%</cell></row><row><cell>Top-p sampling (p = 0.5, r = 1)</cell><cell>13.9</cell><cell>32.3</cell><cell cols="2">35.1% 55.3%</cell><cell>14.0</cell><cell>31.9</cell><cell cols="2">34.6% 55.6%</cell></row><row><cell>+ PICK (p = 0.5, r = 10)</cell><cell>16.9</cell><cell>34.9</cell><cell cols="2">38.0% 74.6%</cell><cell>16.4</cell><cell>34.7</cell><cell cols="2">37.5% 74.7%</cell></row><row><cell></cell><cell></cell><cell>984</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Overall performance comparisons. PICK significantly improves the performances of all models and decoding methods, even on the top-k and top-p sampling that gained low automatic metrics scores on their vanilla responses. The best performances in each section are in bold, while the overall best is underlined.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results of KnowledGPT(Zhao et al., 2020b)  with PICK and different inference methods. Although other decoding methods underperform the greedy baseline, our method still improves over each of them. Here, KF1 is calculated w.r.t the retrieved knowledge instead of the gold knowledge. The best performances in each section are in bold, while the overall best is underlined.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Response Vanilla He was known for tantrums PICK He has been named one of the greatest singers of all time by various media outlets Gold Maybe that was due to his powerful and large vocal range Knowledge snippet His music career began there in 1954, recording at Sun Records with producer Sam Phillips, who wanted to bring the sound of African American music to a wider audience. Dialogue History Speaker 1: I like to listen to Elvis! Have you heard him? Speaker 2: Yeah my favorite song of his is "Hound Dog" written by Jerry Leiber. Speaker 1: Yes, I love that song! I really like his version of "My Way" Response Vanilla He started his career in 1954, and recorded his first album in the early 60s. PICK His music career began in 1954, he was a producer and he wanted to bring the sound of African American music to a wider audience. Gold Elvis started his music career in 1954 recording at Sun Records with producer Sam Phillips.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Knowledge snippetThe album has sold approximately 30 million copies worldwide, including 18 million units in the United States, making it the best-selling debut album of all time in the US, as well as the eleventh best-selling album in the United States. Yes, he actually played for Monaco, Barcelona, New York, and Arsenal, where he was the club's all-time record goalscorer! Speaker 1: Wow! I bet he is really good at what he is doing. How old is he? Response PICK Thierry Henry is a retired French professional footballer who played as a forward and is the second assistant manager of the Belgium national team Other candidate He was born on August 17, 1977. Gold He was born on 17 August 1977 which makes him 40 years old. Do you like watching soccer? Knowledge snippet Dylan Lauren (born May 9, 1974) is an American entrepreneur. Dialogue History Speaker 1: I love chocolate bars. Have you ever heard of Dylan's Candy Bar? Speaker 2: I have heard of it, the daughter of Dylan Lauren owns it, but she is a fashion designer Speaker 1: Who is Dylan Lauren? Response PICK Dylan Lauren (born May 9, 1974) is an American entrepreneur. Other candidate She is an american entrepreneur! Gold She is an american entrepreneur!</figDesc><table><row><cell>Dialogue History</cell><cell></cell></row><row><cell cols="2">Speaker 1: I totally forgot about that song! Didn't listen to it for a while!</cell></row><row><cell cols="2">Speaker 2: You know that album sold 30 million copies!</cell></row><row><cell cols="2">Speaker 1: Really? That's insane! I didn't know that</cell></row><row><cell>Response</cell><cell></cell></row><row><cell>PICK</cell><cell>The album has sold approximately 30 million copies worldwide, including 18 million units in the United States, making it the best-selling debut album</cell></row><row><cell cols="2">Other candidate It is the best selling debut album of all time!</cell></row><row><cell>Gold</cell><cell>Maybe that was due to his powerful and large vocal range</cell></row><row><cell cols="2">Knowledge snippet</cell></row><row><cell cols="2">Thierry Daniel Henry (; born 17 August 1977) is a retired French professional footballer who played as a forward and is the second assistant manager of the Belgium</cell></row><row><cell>national team.</cell><cell></cell></row><row><cell>Dialogue History</cell><cell></cell></row><row><cell cols="2">Speaker 1: No i didn't know that! that's pretty cool!</cell></row><row><cell>Speaker 2:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table A2 :</head><label>A2</label><figDesc>Speaker 1: Oh yeah? What happens if you don't hit any pins? Response PICK The maximum score is 300 which is achieved by getting 12 strikes in a row Other candidate If you don't hit any pins, you get 12 strikes in a row. Gold If you will make a strike when you don't hit any pins. snippet To Kill a Mockingbird is a novel by Harper Lee published in 1960. Dialogue History Speaker 1: Oh wow, a Jack of all Trades it seems. Do you have a favorite novel? I'd love a recommendation! Speaker 2: His first, "A Time to Kill" is older, it was published in 1989 but he spent four years writing it and in my opinion, it's his best. Speaker 1: 4 years is an incredible amount of time to spend on a book, I bet it is good. I will definitely check it out! I imagine it's a thriller about murder? Response PICK It was published in 1960 by Harper Lee. Other candidate I'm not sure, but I do know that to Kill a Mockingbird is a book by Harper Lee. Gold Yes, another book I like is To Kill a Mockingbird but that was written by Harper Lee.That is also about a crime Six study case samples from the GPT-2+PICK using beam search (n = 50, r = 50). The first two samples show errors caused by the current metric failing to promote the selection of better responses seen in the other candidate. The next two samples show errors caused by the selection of worse responses due to high overlap towards the knowledge snippet. The last two sample errors were because of the utilization of irrelevant provided knowledge snippets. The other candidate is one of the best response alternatives that are not selected either by vanilla or even by PICK.</figDesc><table><row><cell>Knowledge</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://en.wikipedia.org/wiki/Longest_word_i n_English</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://github.com/facebookresearch/ParlAI</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>https://appen.com/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank the anonymous reviewers for their valuable and constructive comments. We thank <rs type="person">Tiezheng Yu</rs> for the insightful discussions. This work has been partially funded by the <rs type="grantName">PF20-43679 Hong Kong PhD Fellowship Scheme, Research Grant Council</rs>, <rs type="person">Hong Kong</rs>, and the <rs type="grantName">Hong Kong Fellowship Scheme</rs> by the <rs type="funder">Hong Kong Research Grants Council (RGC)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_mQQ8Ed4">
					<orgName type="grant-name">PF20-43679 Hong Kong PhD Fellowship Scheme, Research Grant Council</orgName>
				</org>
				<org type="funding" xml:id="_Rm5PDWu">
					<orgName type="grant-name">Hong Kong Fellowship Scheme</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>While our proposed method, PICK, is modelagnostic and can be adopted by various model architectures, our exploration in this work is limited to GPT-2 and T5. Generating multiple alternative responses within a single decoding process also can increase the computational overhead of the KGD system, thus ways to increase PICK's efficiency would be beneficial in the future. Additionally, while PICK improves the faithfulness and relevance of responses, it may not address other challenges in knowledge-grounded dialogue systems such as self-consistency, engagingness, long-term coherence, and more. Further research is needed to explore these limitations and develop more comprehensive approaches for generating better responses. We leave these explorations open for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>In this paper, we propose a re-ranking framework, targeting better correlating the final generation with some concrete attributes of the response. However, our work has a broader impact given the current popularity of ChatGPT. ChatGPT replies on a reward model to model human feedback for reinforcement learning. However, the training of the reward model requires a huge amount of human annotations, which is time-and resource-consuming. Then it comes to a question -what is the expression of human preference and whether it is possible to model human preference without heavy human annotations? Though far from perfect, we take an initial step in this direction by exploring the usage of automatic metrics to re-rank the responses. We believe it is a promising and valuable research topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Re-ranking responses w.r.t the dialogue history, the knowledge, or both</head><p>We also perform an ablation study of doing the re-ranking with different considerations on the scoring method: only considering the response quality w.r.t the dialogue history (using FED turn-level basic metrics), the knowledge (using unigram-F1 to the knowledge), or both (see Table <ref type="table">A3</ref>). Re-ranking the response candidates considering only the qualities w.r.t the dialogue history results in generations that are less knowledgeable and perform worse across the board. On the contrary, considering only how much of the response refers to the knowledge snippets promotes more responses that are direct copies of the knowledge. While considering both the response candidate's qualities w.r.t the dialogue history and the knowledge snippet produces similar response quality per their automatic metrics performances, the responses contain far less direct knowledge copies within them. This is a desirable result since the direct knowledge copies responses could be considered responses less relevant to the dialogue as they would be perceived as very rigid non-human-like responses. This is also an interesting finding, that the combination of both qualities in consideration alleviate the failure of overlap-based metrics to rank abstractive but faithful responses as previously observed in <ref type="bibr" target="#b2">(Dziri et al., 2022)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Annotation guideline</head><p>We gave the following annotation instructions, guidelines, and examples to the human evaluator for them to follow. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning from human preferences. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Paul F Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miljan</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Martic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wizard of wikipedia: Knowledge-powered conversational agents</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluating attribution in dialogue systems: The begin benchmark</title>
		<author>
			<persName><forename type="first">Nouha</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Reitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1066" to="1083" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">There are a thousand hamlets in a thousand people&apos;s eyes: Enhancing knowledge-grounded dialogue with personal memory</title>
		<author>
			<persName><forename type="first">Tingchen</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3901" to="3913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning from dialogue after deployment: Feed yourself, chatbot</title>
		<author>
			<persName><forename type="first">Braden</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Emmanuel</forename><surname>Mazare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3667" to="3684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evaluating factual consistency in knowledgegrounded dialogues via question generation and question answering</title>
		<author>
			<persName><forename type="first">Or</forename><surname>Honovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leshem</forename><surname>Choshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ella</forename><surname>Neeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="7856" to="7870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Way off-policy batch deep reinforcement learning of implicit human preferences in dialog</title>
		<author>
			<persName><forename type="first">Natasha</forename><surname>Jaques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asma</forename><surname>Ghandeharioun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><forename type="middle">Hanwen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosalind</forename><surname>Picard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00456</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sequential latent knowledge selection for knowledge-grounded dialogue</title>
		<author>
			<persName><forename type="first">Byeongchang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Linguistically-informed specificity and semantic plausibility for dialogue generation</title>
		<author>
			<persName><forename type="first">Wei-Jen</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi Jessy</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3456" to="3466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Zero-resource knowledge-grounded dialogue generation</title>
		<author>
			<persName><forename type="first">Linxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8475" to="8485" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledgegrounded dialogue generation with a unified knowledge representation</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Liden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter</title>
		<meeting>the 2022 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="206" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A three-stage learning framework for low-resource knowledge-grounded dialogue generation</title>
		<author>
			<persName><forename type="first">Shilei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bochao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiliang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujuan</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2262" to="2272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3428" to="3448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised evaluation of interactive dialog with dialogpt</title>
		<author>
			<persName><forename type="first">Shikib</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="225" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Coherent dialogue with attention-based language models</title>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deconstruct to reconstruct a configurable evaluation metric for open-domain dialogue systems</title>
		<author>
			<persName><forename type="first">Vitou</forename><surname>Phy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4164" to="4178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2021a. Measuring attribution in natural language generation models</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.12870</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
			<pubPlace>Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Exploring the limits of transfer learning with a unified text-to-text transformer</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">2021b. Increasing faithfulness in knowledge-grounded dialogue with controllable features</title>
		<author>
			<persName><forename type="first">David</forename><surname>Hannah Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Reitter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Singh Tomar</surname></persName>
		</author>
		<author>
			<persName><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="704" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bleurt: Learning robust metrics for text generation</title>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The dialogue dodecathlon: Open-domain knowledge and image grounded conversational agents</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2453" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Language models that seek for knowledge: Modular search &amp; generation for dialogue and prompt completion</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojtaba</forename><surname>Komeili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Adolphs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Retrieval augmentation reduces hallucination in conversation</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Poff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3784" to="3803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dialogue natural language inference</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3731" to="3741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Retrieval-free knowledgegrounded dialogue response generation with adapters</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etsuko</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Cahyawijaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Genta</forename><surname>Indra Winata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering</title>
		<meeting>the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="93" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kilm: Knowledge injection into encoder-decoder language models</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahdi</forename><surname>Namazifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Padmakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Ting</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikib</forename><surname>Mehri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.09170</idno>
	</analytic>
	<monogr>
		<title level="m">The First Workshop on Evaluations and Assessments of Neural Conversation Systems</title>
		<imprint>
			<date type="published" when="2021">2023. 2021</date>
			<biblScope unit="page" from="15" to="33" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A comprehensive assessment of dialog evaluation metrics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards coherent and engaging spoken dialog response generation using automatic conversation evaluators</title>
		<author>
			<persName><forename type="first">Sanghyun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Khatri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandra</forename><surname>Cervone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tagyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Hedayatnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anu</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raefer</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Natural Language Generation</title>
		<meeting>the 12th International Conference on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="65" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Augmenting knowledge-grounded conversations with sequential knowledge transition</title>
		<author>
			<persName><forename type="first">Haolan</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hainan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongshen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoye</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5621" to="5630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dialogpt: Largescale generative pre-training for conversational response generation</title>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="270" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning discourse-level diversity for neural dialog models using conditional variational autoencoders</title>
		<author>
			<persName><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="654" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">2020a. Low-resource knowledge-grounded dialogue generation</title>
		<author>
			<persName><forename type="first">Xueliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Knowledgegrounded dialogue generation with pre-trained language models</title>
		<author>
			<persName><forename type="first">Xueliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3377" to="3390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Commonsense knowledge aware conversation generation with graph attention</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haizhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4623" to="4629" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
