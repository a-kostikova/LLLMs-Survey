<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Target Semantic Parsing with Collaborative Deliberation Network</title>
				<funder ref="#_FPFFBuH">
					<orgName type="full">Youth Innovation Promotion Association CAS, Yunnan Provincial Major Science and Technology Special Plan Projects</orgName>
				</funder>
				<funder>
					<orgName type="full">OPPO Research Fund</orgName>
				</funder>
				<funder ref="#_5kdADCU">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_83rM5NF #_rJbyG7W #_eS6Cu8S">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">The Laboratory of Cognition and Decision Intelligence for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fangyu</forename><surname>Lei</surname></persName>
							<email>leifangyu2022@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">The Laboratory of Cognition and Decision Intelligence for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
							<email>shizhu.he@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">The Laboratory of Cognition and Decision Intelligence for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
							<email>kliu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">The Laboratory of Cognition and Decision Intelligence for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
							<email>jzhao@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">The Laboratory of Cognition and Decision Intelligence for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Target Semantic Parsing with Collaborative Deliberation Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FB982D3AB39F72E029D9BAA194284B20</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic parsing aims at mapping natural language utterances into machine-interpretable meaning representations, facilitating user accesses to knowledge bases. However, knowledge in real-world scenarios is often duplicated in multiple storages and different representations. Although researchers have made great success by improving neural semantic parsers, existing works can only handle a specific kind of meaning representation, i.e., the single-target semantic parsing. In this paper, we introduce a multi-target semantic parsing model based on a collaborative deliberation network, which can not only decode multiple meaning representations simultaneously but also allow meaning representations to make use of information from each other while decoding. Experiments show that the proposed model improves the EM accuracy of four MRs averagely by 2.48% to 5.05% on three public datasets 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge bases (KBs) <ref type="bibr" target="#b1">(Bollacker et al., 2008;</ref><ref type="bibr" target="#b13">Lehmann et al., 2015;</ref><ref type="bibr" target="#b23">Weikum et al., 2021)</ref> are fundamental facilities for many AI applications. The large amount of structural information they contain is mainly accessed by corresponding formal query language. A variety of query languages are adopted due to the diversity of implementations and domains, such as Prolog for expert systems, and the standardized SQL and SPARQL for relational databases and RDF stores respectively. Semantic parsing <ref type="bibr" target="#b28">(Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b2">Dahl et al., 1994)</ref> provides a friendly interface to people unfamiliar with these formal languages by transducing the natural language questions into the underlying query language, which has been widely used in abundant tasks, such as question answering system <ref type="bibr" target="#b12">(Lan et al., 2021;</ref><ref type="bibr" target="#b9">Hu et al., 2018)</ref> and robot navigation <ref type="bibr" target="#b22">(Walker et al., 2019)</ref>. Recently, neural semantic parsing with the notable sequenceto-sequence (Seq2Seq) framework has made impressive progress <ref type="bibr" target="#b10">(Jia and Liang, 2016;</ref><ref type="bibr" target="#b27">Yin and Neubig, 2017;</ref><ref type="bibr" target="#b4">Dong and Lapata, 2018;</ref><ref type="bibr" target="#b19">Peter Shaw and Altun, 2019)</ref>. However, recent parsers only focus on singletarget semantic parsing designed for only one certain KB and its corresponding query language, whereas in real-world scenarios, the same fact is commonly duplicated and stored in many systems. For example, personnel and financial data are stored in both tabular and graph structures in some businesses, which calls for a multi-target semantic parsing (MulTSP) model with the capability of generating multiple meaning representations <ref type="bibr">(MRs)</ref>.</p><p>There are two approaches to adopt existing technologies for MulTSP. Specifically, the first is to train N seq2seq models <ref type="bibr" target="#b3">(Dong and Lapata, 2016;</ref><ref type="bibr" target="#b10">Jia and Liang, 2016)</ref> for N target formal languages independently, which is far from an efficient way for MulTSP. And the second approach is to resort to the multi-task learning for all the target formal languages with different decoders, which only exchange information at the encoder. Figure <ref type="figure" target="#fig_0">1(a)</ref> and Figure <ref type="figure" target="#fig_0">1</ref>(b) illustrate their differences. As recent work has shown <ref type="bibr" target="#b8">(Guo et al., 2020)</ref>, there's a significant performance gap between different MRs, for example, the FunQL is more competitive than Lambda Calculus and Prolog. This experimental finding suggests the performance of decoding one MR might get improved with the help of other MRs by introducing collaborations among their decoders.</p><p>To implement collaborations among decoders of different MRs, a straightforward way is to augment attention mechanism within tokens from all MRs at each decoding timestep. However, this is less likely to succeed due to two concerns: 1) The erroneous alignment problem, in which the tokens of different MRs could be hardly aligned by position and the attention mechanism could be even harmful to the current decoding step, for example, considering parsing "How large is Texas ?" to Lambda Calculus and FunQL in Table <ref type="table" target="#tab_1">1</ref>. At 2nd timestep, current token '(' in FunQL is most likely to be useless for predicting 'size&lt;lo,i&gt;' in Lambda Calculus. 2) Holistic information ignorance, in which future tokens from one MR are invisible for other MRs. For example, at the 2nd decoding step, 'size' in FunQL is valuable yet invisible for 'size&lt;lo,i&gt;' in Lambda Calculus.</p><p>To address the aforementioned problems, we present a multi-target semantic parsing model based on a collaborative deliberation network(CDNParser). As shown in Figure <ref type="figure" target="#fig_0">1</ref>(c), each decoder in CDNParser performs two-stage decoding through its two child decoders. During the first decoding stage, each first-pass decoder generates a coarse target formal language sequence for its MR without teach forcing. After that, each second-pass decoder will predict final target sequences as well as collaborate with each other by sharing sequences of coarse target logical forms, which are almost complete target sequences for each MR. Through this design, our method meets the requirements of MulTSP and avoids the problem of erroneous alignments and holistic information ignorance.</p><p>To evaluate the proposed model in this paper, we have conducted extensive experiments on three public benchmarks. Compared with strong baselines, the experimental results demonstrate that the proposed CDNParser can 1) successfully meet the need for MulTSP, 2) for the EM (exact-match) accuracy of four MRs, CDNParser provides 2.52% average improvement on Geo dataset, 5.05% on Job dataset, and 2.48% on ATIS dataset.</p><p>To sum up, our contributions are as follows:</p><p>1. We propose to perform multi-target semantic parsing, which supports simultaneous decoding of multiple meaning representations. 2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic Parsing Methods</head><p>Various semantic parsing models have been proposed over the past decades, which devotes to converting natural language utterances into logical forms that can be easily executed on a knowledge graph <ref type="bibr" target="#b11">(Kwiatkowski et al., 2011;</ref><ref type="bibr" target="#b0">Andreas et al., 2013;</ref><ref type="bibr" target="#b31">Zhao and Huang, 2015)</ref>. These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input. With the development of neural networks, sequence-to-sequence models have been employed to semantic parsing with remarkable results. The neural-based parsing models avoid the requirement for feature engineering and explicit semantic composition rules <ref type="bibr" target="#b3">(Dong and Lapata, 2016;</ref><ref type="bibr" target="#b10">Jia and Liang, 2016;</ref><ref type="bibr" target="#b17">Ling et al., 2016)</ref> in previous methods. Lots of ideas have been explored to improve the performance of these models, including data augmentation <ref type="bibr" target="#b24">(Wu et al., 2021)</ref> and designing structural decoders <ref type="bibr" target="#b4">(Dong and Lapata, 2018;</ref><ref type="bibr" target="#b14">Li et al., 2021)</ref>. Although there have been some efforts to use share parameters for multiple languages or meaning representations, there hasn't been any attempts to study multi-target semantic parsing, which is the main focus of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Meaning Representations</head><p>We adopt four formal languages (that is Meaning Representations, MRs) in this paper: Prolog, Lambda Calculus, FunQL, and SQL. Since these MRs are explored extensively in semantic parsing task, making it easier to obtain labeled data in these MRs. Table <ref type="table" target="#tab_1">1</ref> shows a logical form for each of the four MRs in the Geo domain.</p><p>Prolog is a logic programming language for deductive reasoning. Since its birth, it has been extended to many application areas, such as relational databases, mathematical logic, natural language understanding, etc. Prolog is based on first-order predicates logic, incorporating higher-order predicates to handle problems such as quantification and aggregation.</p><p>Lambda calculus is a formal system for describing operations, which can represent all first-order predicates, and can support some higher-level predicates. Lambda calculus expresses natural language through constants, quantifiers, logical connectives, Lambda operators, etc. These advantages promote the widespread use of Lambda calculus in semantic parsing tasks.</p><p>FunQL is a variable-free language that abstracts variables away and uses nested structures to represent compositionality. Unlike languages such as Prolog and Lambda calculus, predicates in FunQL take a set of entities as input and return another set of entities as output.</p><p>SQL is short for Structured Query Language and is a popular relational database query language. Given that SQL is domain-agnostic and has wellestablished execution engines, the subtask of se-mantic parsing, Text-to-SQL has also sparked a lot of attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deliberation Network</head><p>Deliberation network <ref type="bibr" target="#b25">(Xia et al., 2017)</ref> is inspired by human behavior. For example, to write a good document, we usually first create a complete draft and then polish it based on a global understanding of the whole draft. Deliberation network leverages the global information with both looking back and forward in sequence decoding through a deliberation process. <ref type="bibr">(Li et al., 2019a)</ref> and <ref type="bibr" target="#b26">(Xiong et al., 2019)</ref> apply deliberation network in dialogue system and machine translation and and adopt a new simplified training algorithm instead of the original Monte Carlo based algorithm. In our work, deliberation network acts as a key technology to implement collaborative multi-target decoding among different MRs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>Given a dataset D = {(x, y 1 , ..., y N )} where x = {x 1 , x 2 , ...x Tx } is the source natural language sequence, for example "How large is Texas ?", and each y i = {y i 1 , y i 2 , ...y i T y i } is a target logical form sequence for one MR, such as FunQL sequence "answer(size(stateid('texas')))" or Prolog sequence "answer(A,(size(B,A),const(B, stateid(texas))))". T x . and T y i represent sequence length. Our model is aimed at learning a parser which transforms x into (y 1 , ..., y N ), simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Overview</head><p>Figure <ref type="figure">2</ref> visualizes the overall architecture of our multi-target semantic parsing model. Our model is based on a collaborative deliberation network. A collaborative deliberation network consists of an encoder E and N (N =2 in this example) deliberation decoders D i (i = 1, 2, ..., N ). In this example, two deliberation decoders are responsible for generating Lambda Calculus and FunQL, respectively. Each deliberation decoder is composed of a first-pass decoder D i 1 and a second-pass decoder D i 2 , where collaboration happens. Briefly speaking, E is used to encode the source sequence into a sequence of vector representations. Each D i 1 reads the encoder representations and generates a first-pass target sequence as a draft, which further participates in the collaboration mechanism and is provided as input to the second-pass decoders to Figure <ref type="figure">2</ref>: The overview of the collaborative deliberation network for MulTSP. For simplicity, we omit the encoderdecoder attention. The encoder on the left encodes natural language utterances before feeding them to two deliberation decoders. Each decoder performs two-phase decoding to generate Lambda Calculus and FunQL respectively. Moreover, during the second decoding phase, the two decoders are supposed to collaborate.</p><p>perform second pass decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoder and First-Pass Decoder</head><p>When an source sequence x is fed into the encoder E, it is encoded into a hidden vector sequence {h 1 , h 2 , ..., h Tx } with the length of T x . To be specific, h i = LSTM(x i , h i-1 ), where x i is the ith word embedding and h 0 is intialized to zero vector. For the sake of convenience, in the following part of this section, we will omit the superscript of y i , D i 1 , and D i 2 and denote them as y, D 1 , and D 2 . Each first-pass decoder D 1 will generate a sequence of hidden states ŝj (∀ j ∈ [T ŷ]), and the firstpass target logical form sequence ŷj (∀ j ∈ [T ŷ]).</p><p>Next we explain the process in detail.</p><p>Identificial to the conventional encoder-decoder model, an additive attention is included in each D 1 . At timestep t, D 1 first calculate contextual vector ctx e as follows:</p><formula xml:id="formula_0">α i = exp(v T e tanh(W e 0 h i + W e 1 ŝt-1 )) Tx k=1 exp(v T e tanh(W e 0 h k + W e 1 ŝt-1 )) ctx e = Tx i=1 α i h i</formula><p>where v T e , W e 0 , W e 1 are parameters.</p><p>After obtaining ctx e , the hidden state ŝt i of firstpass decoder is got by:</p><formula xml:id="formula_1">ŝt = LSTM([ŷ t-1 ; ctx e ], ŝt-1 )</formula><p>After that, the concatenated vector [ŝ t , ctx e , ŷt-1 ] will be fed into a feedforward network to predict the probability of ŷt as:</p><formula xml:id="formula_2">p(ŷ t |x, ŷ&lt;t ) = softmax(FFN([ŝ t , ctx e , ŷt-1 ]))</formula><p>Finally, ŷt is predicted by: ŷt = argmax p(ŷ t |x, ŷ&lt;t )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Second-Pass Decoder</head><p>The second-pass decoder is the key component of our approach since it contains the vital collaboration mechanism. Figure <ref type="figure">2</ref> exhibits the behavior of second-pass decoder.</p><p>Once first-pass decoder has generated the firstpass target logical form sequences ŷ as a draft. It is fed into the second-pass decoder for further collaboration. Besides the sequence ŷ and hidden state ŝ produced by its corresponding first-pass decoder, the second-pass decoder also collaborates with other second-pass decoders, taking outputs from other first-pass decoders into consideration.</p><p>First, the information from source sequence is integrated by calculating contextual vector ctx ′ e , the computation detail is similar to ctx e in Section 3.2.</p><p>To synthesize information from the first-pass target logical form sequence, we compute the contextual vector ctx c by:</p><formula xml:id="formula_3">α i = exp(v T c tanh(W c 0 [ŝ i ; ŷi ] + W c 1 s t-1 )) Ty j=1 exp(v T c tanh(W c 0 [ŝ j ; ŷj ] + W c 1 s t-1 )) ctx c = Ty i=1 α i [ŝ i ; ŷi ]</formula><p>where v T c , W c 0 , W c 1 are parameters. Second-pass decoder calculates contextual vector ctx o to collaborate with other deliberation decoders based on ŷo and ŝo , where ŷo = [y j ∀j = 1, 2, ..., N ∧ j ̸ = i] represents the concatenation of first-pass target logical form sequences predicted by other deliberation decoders. Simmilar for ŝo and ctx o is defined as follow:</p><formula xml:id="formula_4">α i = exp(v T o tanh(W o 0 [ŝ o i ; ŷo i ] + W o 1 s t-1 )) Ty j=1 exp(v T o tanh(W o 0 [ŝ o j ; ŷo j ] + W o 1 s t-1 )) ctx o = Ty i=1 α i [ŝ o i ; ŷo i ]</formula><p>where v T o , W o 0 , W o 1 are parameters. After obtaining all of the three contextual vectors, D 2 takes the previous hidden state s t-1 generated by itself, previously decoded token y t-1 to calculate s t as:</p><formula xml:id="formula_5">s t = LSTM([y t-1 ; ctx ′ e ; ctx c ], s t-1 )</formula><p>Finally, the concantenated vector:</p><formula xml:id="formula_6">a t = [s t ; ctx ′ e ; ctx c ; ctx o ; y t-1 ]</formula><p>is fed to a feedforward network followed by a softmax layer to predict the probability of y t . As can be seen from the above computation, the contextual vector ctx o aggregates the global information extracted from the first-pass logical form sequence of other MRs, making the most use of assistance from other MRs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>Since our model output multiple MRs, we minimize summed training for all MRs:</p><formula xml:id="formula_7">L = N i=1 L i</formula><p>each L i represents a loss for a certain MR. Next, we explain how L i is calculated in detail.</p><p>In contrast to the original deliberation network <ref type="bibr" target="#b7">(Gu et al., 2019)</ref>, where they propose a complex training algorithm based on Monte Carlo Method, we follow the works of <ref type="bibr" target="#b26">Xiong et al. (2019)</ref> and <ref type="bibr">Li et al. (2019b)</ref>, maximizing the summed likelihood of the two generated logical form sequences. And L i is computed as:</p><formula xml:id="formula_8">L i = L 1 i + L 2 i L 1 i = -D T y i j=1 logP (ŷ i j ) L 2 i = -D T y i j=1 logP (y i j )</formula><p>where P (ŷ i j ) is the probability predicted for jth token in ith MR by first-pass decoder and P (y i j ) is the probability predicted for jth token in ith MR by second-pass decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We trained our model on the three datasets covering different domains: Geo, ATIS, and Job, each of which has been widely used in previous works and has been labeled with logical forms for four MRs including Prolog, Lambda Calculus, FunQL and SQL.</p><p>Geo focuses on querying a US geography database, originally containing 880 (utterance, logical form) pairs annotated by <ref type="bibr" target="#b28">(Zelle and Mooney, 1996)</ref> through a Prolog-style MR. Latter, <ref type="bibr" target="#b20">(Popescu et al., 2003)</ref> and (Rohit J. <ref type="bibr" target="#b21">Kate and Mooney, 2005)</ref> proposed to use SQL and FunQL to represent the meanings, respectively. Furthermore, <ref type="bibr" target="#b30">(Zettlemoyer and Collins, 2005)</ref> proposed to use Lambda Calculus and manually converted the Prolog logical forms to equivalent expressions in Lambda Calculus. By convention, we adopt the standard 600/280 training/test split.</p><p>ATIS is a standard semantic parsing dataset about flight booking, consisting of 5410 questions and their corresponding SQL queries. <ref type="bibr" target="#b29">(Zettlemoyer and Collins, 2007)</ref> use Lambda Calculus to represent the meanings and automatically map SQL queries to equivalent Lambda Calculus. The standard split has 4480 training instances, 480 development instances and 450 test instances.</p><p>Job dataset contains 640 utterances and their corresponding Prolog queries to a dataset of job listings. Similar to Geo, <ref type="bibr" target="#b30">(Zettlemoyer and Collins, 2005)</ref> proposed to represent the meanings with Lambda Calculus and manually converted Prolog logical forms to equivalent Lambda Calculus logical forms.</p><p>Since not all the four MRs introduced above are used in these three domains, following previous works, <ref type="bibr" target="#b8">(Guo et al., 2020</ref>) has annotated the above three datasets with all the four MRs by semiautomatically translating logical forms in one MR into others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>Our models are compared with two baselines:</p><p>1. Seq2Seq model: We formulate semantic parsing task as a neural machine translation task and employ the encoder-decoder architecture to solve it. To predict N MRs, we need N independent Seq2Seq models to generate logical forms for each MR. Each Seq2Seq model contains a one-layer BI-LSTM <ref type="bibr" target="#b6">(Gers et al., 2000)</ref> encoder and a one-layer LSTM decoder.</p><p>2. Multi-task learning model: Applying multitask learning to parse multiple MRs. It consists of a one-layer BI-LSTM encoder and multiple one-layer LSTM decoders. Each decoder is responsible for generating logical forms for one MR. The decoders only share the same encoder and have no communication with each other. The crucial distinction between this model and our model is whether to introduce the help of other MRs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementations</head><p>We adopt AllenNLP <ref type="bibr" target="#b5">(Gardner et al., 2018)</ref> and Pytorch <ref type="bibr" target="#b18">(Paszke et al., 2019)</ref> framework to implement each approach. Due to the limited number of test data in each dataset, we run each approach five times and report the mean performance and standard deviation. Many neural semantic parsing approaches adopt data anonymization to replace entities in utterances with placeholders. However, in this paper, we do not apply data anonymization to avoid bias.</p><p>For parameters, we tune the hyper-parameters of each approach on the development set or through cross-validation on the training set. We put the search space of hyper-parameters in Appendix A.</p><p>We initialize all parameters uniformly within the interval [-0.1, 0.1]. We use Adam algorithm to update parameters. Gradients are clipped to 5.0 to alleviate the gradient explosion problem. Early stop is used to determine the number of training epochs. We use greedy search to generate logical forms during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation Metrics</head><p>Following previous work <ref type="bibr" target="#b8">(Guo et al., 2020)</ref>, we use EM (exact-match) accuracy as the evaluation metric, which is defined as the percentage of the samples that are correctly parsed to their gold standard meaning representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experimental Results</head><p>At first, we evaluate our model's performance on three standard benchmarks along with baseline models, in performing the MulTSP (multi-target semantic parsing) task. All approaches are required to predict four MRs, namely Prolog, Lambda Calculus, FunQL, and SQL.</p><p>Table <ref type="table">2</ref> shows our results. By comparing various methods, we observe that 1) our model consistently outperforms all other candidates. On the one hand, our model achieves the best parsing accuracy among all methods, on the other hand, the performance of our model is more steady, making our method more reliable. For example, on Job dataset, CDNParser achieves 5.05% absolute improvement on average compared to the Seq2Seq model with a lower standard deviation. This justifies the effectiveness of our approach on MulTSP task. 2) Both our model and multi-task learning model can produce all four MRs. However, our model beats the multi-task learning model substantially, indicating that incorporating help from other MRs plays a crucial role in enhancing accuracy.</p><p>By comparing the performance of various MRs, we observe that 1) FunQL tends to outperform other MRs and our model's advancement on FunQL is relatively lesser as compared to the other three MRs. One possible reason is that FunQL is more compact than the other MRs due to its elimination of variables and quantifiers, making it easier to predict even without the aid of other MRs. 2) SQL appears to have the poorest performance yet tends to achieve the highest improvement. For example, in Geo dataset, the exact-match accuracy of FunQL Table <ref type="table">2</ref>: Experimental results of our multi-target semantic parsing model and two baseline models. Seq2Seq* reports the results from <ref type="bibr" target="#b8">(Guo et al., 2020)</ref>. The results are averaged over 5 random seeds (mean±std). We highlight the best performance for every MR and our model's absolute improvement over Seq2Seq*. The poor SQL exact-match accuracy in ATIS is caused by the different distribution of SQL queries in test set.  is substantially higher than that of SQL in all approaches. However, SQL gets a remarkable 6.9% improvement from our model. This is understandable, SQL is a domain-general language, unlike domain-specific MRs (Prolog, Lambda Calculus, and FunQL), the domain knowledge can not be injected into SQL easily, making it a challenge for neural networks to generate SQL. Therefore, by obtaining assistance from the other three domainspecific MRs, SQL achieves the maximum promotion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation Study</head><p>We conduct ablation study to explore the effects of collaboration mechanism in our model. We assess the performance of several collaboration weakened models. We denote M as our original multi-target semantic parsing model, and M 2 as collaborative weakened model, in which each decoder is limited to randomly collaborate with other two out of three decoders. Similarly, each decoder in M 1 can only collaborate with one aonther random decoder and decoders in M 0 are not allowed to collaborate with each other, Figure <ref type="figure" target="#fig_1">3</ref> summarizes the experimental results.</p><p>From the results, we observe that: 1) As the collaboration weakens, the EM accuracy of each MR generally shows a downward trend. 2) When collaboration is completely deactivated, there is often a sudden drop in performance. This implies that the collaboration mechanism is a vital component in our method and the outstanding performance achieved by our model benefits a lot from the mutual assistance among MRs brought by collaboration mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Analysis</head><p>Our main experiment has shown that incorporating information from other MRs can improve semantic parsing performance. We call them auxiliary MRs. This subsection will dive deeper into this subject. We hypothesize that auxiliary MRs contribute differently and fewer MRs could even give better performance according to the results from Figure <ref type="figure" target="#fig_1">3</ref>. This brings up an interesting question: In the case of generating logical forms for a specific MR, how do we choose other auxiliary MRs? We hypothesize that an MR will be enhanced more by a homogeneous MR than a heterogeneous one. For example, the Prolog is intuitively closer to lambda calculus than SQL. A comprehensive measurement of the similarity is not trivial, and thus we resort to the approximated similarity based on empirical results. Specifically, we employ a Seq2Seq neural network to transform an MR into another one. The 4-by-4 transformation results form a matrix (Table <ref type="table" target="#tab_3">3</ref>). We propose a Heuristic Selection Strategy (HSS) based on this matrix: For any MR in demand as the target, the source MRs with higher transformation accuracy are more preferred. For example, taking Prolog as the target, the most similar MR is the FunQL (71.1%), followed by Lambda Calculus (59.4%) and SQL (54.0%). Note the similarity is not symmetric in this setting. When the amount of auxiliary MRs is restricted to 1 (or 2), HSS will choose only FunQL (or Lambda Calculus in addition).</p><p>To evaluate HSS, we respectively pick up 2 and 1 auxiliary MRs for each MR, there are 3 choices in both cases as marked by letters in parentheses in Table <ref type="table" target="#tab_4">4</ref> (or Table <ref type="table" target="#tab_5">5</ref>). P, L, F, and S represent Pro-   log, Lambda Calculus, FunQL, and SQL is picked respectively. The "↑" indicates the higher EM accuracy than taking all the other 3 MRs as auxiliary MRs, while the "↓" indicates the lower EM accuracy. Table <ref type="table" target="#tab_4">4</ref> and Table <ref type="table" target="#tab_5">5</ref> shows that: 1) After choosing one or two auxiliary MRs, the EM accuracy for each MR may either rise or fall, confirming our assumption that auxiliary MRs do not contribute equally. 2) After picking one or two auxiliary MRs following our strategy, the EM accuracy for most MRs increases, proving that our strategy based on heuristic features is empirically reasonable and valuable in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a multi-target semantic parsing model based on a collaborative deliberation network. On the one hand, our model is capable of predicting multiple MRs simultaneously, successfully completing the multi-target semantic parsing task, on the other hand, each MR in our model can leverage assistance information from other MRs thanks to the design of collaboration mechanism in our model. We believe that our method has good application prospects in real-world scenarios due to the diversity of knowledge forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Our multi-target semantic parsing model based on collaborative deliberation network focues on specific domain, and is unable to handle multidomain settings. In the future, we plan to extend our method to adapt multi-domain scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Different implementations for multi-target semantic parsing (MulTSP). (a) Training one parser for each target. (b) A multi-task parser that independently decodes different targets with a shared encoder. (c) The collaborative deliberation network which exploits other coarse decoding outputs to enhance the decoding of any target.</figDesc><graphic coords="1,327.97,294.67,174.60,92.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(Figure 3 :</head><label>3</label><figDesc>Figure 3: Experimental results for ablation studies on three datasets. For each MR, four bars in different colors stand for four models with varying degrees of collaboration. As the collaboration weakens from left to right, the EM accuracy of each MR typically displays a declining trend.</figDesc><graphic coords="7,85.36,378.91,136.06,102.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>L)↑ 72.0(F)↑ 66.3(S)↓ Lambda 71.6(P)↑ 70.0(F)↑ 69.0(S)↑ FunQL 78.0(P)↑ 77.3(L)↑ 78.8(S)↑ SQL 64.9(P)↓ 65.2(L)↑ 65.0(F)↑ JOB Prolog 74.3(L)↑ 74.1(F)↑ 72.7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,93.55,70.87,408.19,226.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Examples of MRs for utterance "How large is Texas?" in the Geo domain.</figDesc><table><row><cell>MR</cell><cell>Logical Form</cell></row><row><cell>Prolog</cell><cell>answer(A, (size(B, A), const(B, stateid(texas))))</cell></row><row><cell>Lambda Calculus</cell><cell>(size:&lt;lo,i&gt; texas:s)</cell></row><row><cell cols="2">FunQL answer(size(stateid('texas')))</cell></row><row><cell>SQL</cell><cell>select STATEalias0.AREA from ... where ... STATE_NAME = "texas"</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Empirical Transformation Accuracy, row and column respectively represents the source and target MRs.</figDesc><table><row><cell></cell><cell>Prolog</cell><cell>Lambda Calculus</cell><cell>FunQL</cell><cell>SQL</cell></row><row><cell>Prolog</cell><cell>-</cell><cell>70.4%</cell><cell cols="2">82.3% 71.2%</cell></row><row><cell>Lambda Calculus</cell><cell>59.4%</cell><cell>-</cell><cell cols="2">73.3% 69.3%</cell></row><row><cell>FunQL</cell><cell>71.1%</cell><cell>72.2%</cell><cell>-</cell><cell>75.8%</cell></row><row><cell>SQL</cell><cell>54.0%</cell><cell>57.0%</cell><cell>60.6%</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>EM accuracy for each MR when picking up 2 auxiliary MRs.We highlight HSS selection for each MR.</figDesc><table><row><cell>Target MR</cell><cell>Composition</cell></row><row><cell>GEO</cell><cell></cell></row><row><cell>Prolog</cell><cell>69.6(LF)↑ 68.8(LS)↓ 68.0(FS)↓</cell></row><row><cell>Lambda</cell><cell>69.9(PF)↑ 67.9(PS)↓ 67.4(FS)↓</cell></row><row><cell>FunQL</cell><cell>77.0(PL)↓ 75.7(PS)↓ 75.5(LS)↓</cell></row><row><cell>SQL</cell><cell>65.0(PL)↑ 65.1(PF)↑ 64.0(LF)↑</cell></row><row><cell>JOB</cell><cell></cell></row><row><cell>Prolog</cell><cell>74.1(LF)↑ 73.3(LS)↑ 73.6(FS)↑</cell></row><row><cell>Lambda</cell><cell>74.8(PF)↑ 74.4(PS)↑ 73.8(FS)↑</cell></row><row><cell>FunQL</cell><cell>72.9(PL)↓ 73.8(PS)↑ 74.4(LS)↑</cell></row><row><cell>SQL</cell><cell>73.2(PL)↓ 74.3(PF)↑ 73.5(LF)↓</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>EM accuracy for each MR when picking up 1 auxiliary MR. We highlight HSS selection for each MR.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement This work was supported by <rs type="funder">National Key R&amp;D Program of China</rs> (No.<rs type="grantNumber">2022ZD0118501</rs>) and the <rs type="funder">National Natural Science Foundation of China</rs> (No.<rs type="grantNumber">62376270</rs>, No.<rs type="grantNumber">U1936207</rs>, No.<rs type="grantNumber">61976211</rs>). This work was supported by the <rs type="funder">Youth Innovation Promotion Association CAS, Yunnan Provincial Major Science and Technology Special Plan Projects</rs> (No.<rs type="grantNumber">202202AD080004</rs>) and <rs type="funder">OPPO Research Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5kdADCU">
					<idno type="grant-number">2022ZD0118501</idno>
				</org>
				<org type="funding" xml:id="_83rM5NF">
					<idno type="grant-number">62376270</idno>
				</org>
				<org type="funding" xml:id="_rJbyG7W">
					<idno type="grant-number">U1936207</idno>
				</org>
				<org type="funding" xml:id="_eS6Cu8S">
					<idno type="grant-number">61976211</idno>
				</org>
				<org type="funding" xml:id="_FPFFBuH">
					<idno type="grant-number">202202AD080004</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appedix</head><p>A.1 Preprocess According to <ref type="bibr" target="#b10">(Jia and Liang, 2016)</ref>, we preprocess the Prolog logical forms to the De Brujin index notation. We standardize the variable naming of Lambda Calculus based on their occurrence order in logical forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Hyper-Parameters</head><p>For our model, a one-layer bi-directional LSTM is chosen for encoder and a one-layer LSTM is selected as decoder. The embedding dimension of both source and target languages ranges over {100, 200}. The decoder keeps the same hidden dimension as encoder, ranging over {32, 64}. As for the attention model, we apply additive attention. We employ dropout at training time to alleviate</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic parsing as machine translation</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="47" to="52" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2008 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Expanding the scope of the ATIS task: The ATIS-3 corpus</title>
		<author>
			<persName><forename type="first">Deborah</forename><forename type="middle">A</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Hunicke-Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Pallett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Pao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rudnicky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology: Proceedings of a Workshop held at Plainsboro</title>
		<meeting><address><addrLine>New Jersey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-03-08">1994. March 8-11, 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language to logical form with neural attention</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coarse-to-fine decoding for neural semantic parsing</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1068</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="731" to="742" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">AllenNLP: A deep semantic natural language processing platform</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Grus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-2501</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</title>
		<meeting>Workshop for NLP Open Source Software (NLP-OSS)<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with lstm</title>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Felix A Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Levenshtein transformer</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Benchmarking meaning representations in neural semantic parsing</title>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.118</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1520" to="1540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A statetransition framework to answer complex questions over knowledge base</title>
		<author>
			<persName><forename type="first">Sen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinbo</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1234</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2098" to="2108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lexical generalization in ccg grammar induction for semantic parsing</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1512" to="1523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A survey on complex knowledge base question answering: Methods, challenges and solutions</title>
		<author>
			<persName><forename type="first">Yunshi</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaole</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 30th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dbpedia-a large-scale, multilingual knowledge base extracted from wikipedia</title>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anja</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Morsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic web</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="195" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Keep the structure: A latent shift-reduce parser for semantic parsing</title>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2021/532</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3864" to="3870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Incremental transformer with deliberation decoder for document grounded conversations</title>
		<author>
			<persName><forename type="first">Zekang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Incremental transformer with deliberation decoder for document grounded conversations</title>
		<author>
			<persName><forename type="first">Zekang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Latent predictor networks for code generation</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Kočiskỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fumin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="599" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generating logical forms from graph representations of text and entities</title>
		<author>
			<persName><forename type="first">Angelica</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Massey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="95" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards a theory of natural language interfaces to databases</title>
		<author>
			<persName><forename type="first">Ana-Maria</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th international conference on Intelligent user interfaces</title>
		<meeting>the 8th international conference on Intelligent user interfaces</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="149" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning to transform natural to formal languages</title>
		<author>
			<persName><forename type="first">Yuk</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wong</forename><surname>Rohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1062" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural semantic parsing with anonymization for command understanding in general-purpose service robots</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Tang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maya</forename><surname>Cakmak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robot World Cup</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="337" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Machine knowledge: Creation and curation of comprehensive knowledge bases</title>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luna</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Razniewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Databases</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="108" to="490" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Data augmentation with hierarchical sql-to-question generation for cross-domain text-to-sql parsing</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8974" to="8983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deliberation networks: Sequence generation beyond one-pass decoding</title>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Modeling coherence for discourse neural machine translation</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7338" to="7345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A syntactic neural model for general-purpose code generation</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="440" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th National Conference on Artificial Intelligence</title>
		<meeting>the 13th National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Online learning of relaxed ccg grammars for parsing to logical form</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="678" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-First Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Type-driven incremental semantic parsing with polymorphism</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter</title>
		<meeting>the 2015 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1416" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">over-fitting with rate ranging over {0.1, 0.2, 0.3}. We select batch size from {16, 32, 64}, and select learning rate from {0</title>
		<imprint/>
	</monogr>
	<note>.005, 0.01}</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
