<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Query Rewriting for Effective Misinformation Discovery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ashkan</forename><surname>Kazemi</surname></persName>
							<email>ashkank@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<addrLine>Meta AI 2, Meedan 3</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Oxford Internet Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Artem</forename><surname>Abzaliev</surname></persName>
							<email>abzaliev@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<addrLine>Meta AI 2, Meedan 3</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Oxford Internet Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Naihao</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<addrLine>Meta AI 2, Meedan 3</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Oxford Internet Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Hou</surname></persName>
							<email>rayhou@meta.com</email>
						</author>
						<author>
							<persName><forename type="first">Scott</forename><forename type="middle">A</forename><surname>Hale</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Verónica</forename><surname>Pérez-Rosas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<addrLine>Meta AI 2, Meedan 3</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Oxford Internet Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
							<email>mihalcea@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<addrLine>Meta AI 2, Meedan 3</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Oxford Internet Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Query Rewriting for Effective Misinformation Discovery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8DF3E10E479A493F194C18CEAD008037</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel system to help factcheckers formulate search queries for known misinformation claims and effectively search across multiple social media platforms. We introduce an adaptable rewriting strategy, where editing actions for queries containing claims (e.g., swap a word with its synonym; change verb tense into present simple) are automatically learned through offline reinforcement learning. Our model uses a decision transformer to learn a sequence of editing actions that maximizes query retrieval metrics such as mean average precision. We conduct a series of experiments showing that our query rewriting system achieves a relative increase in the effectiveness of the queries of up to 42%, while producing editing action sequences that are human interpretable.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the wide spread of both human and automatically generated misinformation, there is an increasing need for tools that assist fact-checkers while retrieving relevant evidence to fact-check a claim. This process often involves searching for similar claims across social media using initial clues or keywords based on users' intuition. However, the available mechanisms for search on social media sites are often platform-specific, with restrictions on the allowed number of search queries and access to retrieved documents. This can be attributed, among others, to the dynamic nature of social media feeds, the differences among users' interactions, and the architectural differences in how platforms perform search on their data. As a result, optimizing for arbitrary black-box search end-points containing ever-changing and different document sets means that a generic claim rewriter operating across all search end-points has a high chance of being sub-optimal.</p><p>To address these challenges, we draw upon a direct collaboration among fact-checkers and NLP </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>… … Deployment Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action</head><p>Figure <ref type="figure">1</ref>: Overview of our proposed approach: we train a decision transformer with "state", "action" and "reward" sequences discovered by searching the space of potential query edits. During the deployment stage, the decision transformer predicts action(s) to rewrite the claim into a more effective query.</p><p>researchers, and introduce an adaptive claim rewriting system that can be used for effective misinformation discovery. We develop an interface in which users can edit individual tokens in the input claim using a predefined set of actions, and obtain updated queries leading to different levels of retrieval performance. Using this environment, we build a system that learns to rewrite input claims as effective queries by leveraging reinforcement learning (RL) to maximize desired retrieval metrics (e.g., average precision at K (AP@K)). An offline RL agent is then trained to learn the best editing sequences using a decision transformer model <ref type="bibr" target="#b2">(Chen et al., 2021)</ref> as shown in Figure <ref type="figure">1</ref>.</p><p>Given the limited access to social media search APIs, we use off-the-shelf retrievers such as BM25 <ref type="bibr" target="#b24">(Robertson and Zaragoza, 2009)</ref> and approximate K-nearest neighbours (kNN) <ref type="bibr" target="#b11">(Malkov and Yashunin, 2018)</ref> to simulate platform search endpoints. Our system is trained using a modified version of FEVER <ref type="bibr" target="#b28">(Thorne et al., 2018)</ref>, a well known misinformation dataset containing a mix of true and false claims linked to Wikipedia evidence sentences. We transform FEVER claims into sequences of (claim, edit action, reward) triplets by using Breadth First Search (BFS) and heuristics such as constraining search space depth. These triplets are used to train a decision transformer model to autoregressively predict a sequence of editing actions leading to retrieval improvements.</p><p>Through several experiments, we show that our query rewriting approach leads to relative performance improvements of up to 42% when compared to using the original claim. We also find that a simplified version of this approach-i.e., fine-tuning a classifier to predict a single edit, leads to comparable performance while being more resource efficient during training and inference. We conduct ablation experiments to further evaluate the model performance across several settings, including variations on the retriever type, the reward metric, and the presence of negative training examples.</p><p>To the best of our knowledge, our system is the first to leverage RL to learn to edit text from a set of human-readable actions only. From a practical perspective, it provides initial experimental evidence on the potential of interpretable systems in helping users, including fact-checkers, media writers, and platform trust and safety teams, to more effectively discover misinformation on the Internet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Prior Work</head><p>Our work is closely related to three previous research directions.</p><p>Finding Similar Claims. The problem of finding similar claims has been explored from the perspective of system building, and supports a key step in human-led fact-checking <ref type="bibr" target="#b16">(Nakov et al., 2021)</ref>. <ref type="bibr" target="#b26">Shaar et al. (2020)</ref> conducted retrieval and ranking of previously fact-checked claims given an input claim to detect debunked misinformation in English. <ref type="bibr" target="#b7">Kazemi et al. (2021)</ref> tackled a similar problem in non-English languages. <ref type="bibr" target="#b8">Kazemi et al. (2022)</ref> investigated systems and models for finding applicable fact-checks for tweets.</p><p>While most prior work on this area has focused on building retrieval systems to identify similar claims, our work focuses on query rewriting to assist fact-checkers in the discovery of misinformation. During this process we assume that the retrieval system is a black-box to which we only have search access.</p><p>Query Rewriting. Query reformulation methods such as relevance feedback and local or global query expansion have been well-studied within the information retrieval literature. <ref type="bibr" target="#b9">Lavrenko and Croft (2001)</ref> proposed the relevance model, an unsuper-vised local expansion method in which the probability of adding a term to the query is proportional to the probability of the term being generated from language models of the original query and the document the term appears in. <ref type="bibr" target="#b1">Cao et al. (2008)</ref> proposed a supervised pseudo relevance feedback in which expansion terms are selected by a classifier that determines their usefulness to the query performance. <ref type="bibr" target="#b10">Li et al. (2014)</ref> introduced REC-REQ, an iterative double-loop relevance feedback process in which a user provides relevance feedback to a classifier that is trained to identify relevant documents.</p><p>RL approaches have been previously applied to query rewriting. <ref type="bibr" target="#b18">Nogueira and Cho (2017)</ref> and <ref type="bibr" target="#b17">Narasimhan et al. (2016)</ref> used RL to learn to pick terms from pseudo-relevant documents that upon addition to the query improve retrieval performance metrics such as recall. In more recent work, <ref type="bibr" target="#b29">Wu et al. (2021)</ref> proposed CONQRR, a system that rewrites conversational queries into standalone questions. The authors first trained a T5 model to generate human rewritten queries for the QReCC dataset <ref type="bibr" target="#b0">(Anantha et al., 2021)</ref> and then used them to generate candidate queries, which are selected based on maximizing search utility by an RL agent.</p><p>A key difference between our method and prior work is that we do not use information from the retrieved documents to reformulate queries as the queries themselves are the only input to the model.</p><p>Text Editing Models. Also related to our work is research done on "text-editing" models <ref type="bibr" target="#b13">(Malmi et al., 2022)</ref>. This line of research has gained traction in recent years as models such as EdiT5 and LEWIS <ref type="bibr" target="#b12">(Mallinson et al., 2022;</ref><ref type="bibr" target="#b21">Reid and Zhong, 2021)</ref> promise hallucination-free and controlled text generation for tasks where the input and output texts are similar enough so that a model can learn to transform the input into the output by applying a limited number of editing actions. <ref type="bibr" target="#b27">Stahlberg and Kumar (2020)</ref> proposed Seq2Edits, a fast textediting model for text generation tasks such as grammatical error correction and text simplification. Seq2Edits uses an edited transformer encoder and decoder to generate sequences of edits for the positions in the input text that need to be altered with suggested new tokens. <ref type="bibr" target="#b21">Reid and Zhong (2021)</ref> introduced a multi-span text editing algorithm that uses Levenstein edit operations for the tasks of sentiment and politeness transfer in text, based on the intuition that text style transfer usually can be done with a few edits on the input text. Overall, text-editing models are usually faster than other sequence generation models such as seq2seq, since they only predict actions on a few input tokens rather than regenerating the whole sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>In this paper, we focus on the task of query rewriting for discovering similar claims from an opaque search end-point. We have a collection of input claims (C 1 , C 2 , ..., C n ) that contain at least one fact-checkable claim. For any given claim C i in the collection, there exists one or more collections of similar claims (SC i1 , SC i2 , ..., SC im ), either supporting or refuting the claim in-part or as a whole. The RL agent operates on a fixed set of actions A = {A 1 , A 2 , ..., A k } that can be applied to any of C i 's tokens (T i1 , T i2 , ..., T iq ), where k is the number of possible actions, q is the number of tokens in C i . We rewrite the query by applying the sequence of actions (A ij , 1 ≤ i ≤ k, 1 ≤ j ≤ q) generated by the RL model to the original query. We can then use this improved query to retrieve related evidence statements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Overview</head><p>Our system rewrites a query using concepts from RL and query expansion. We pass the query into a pre-trained language model and then use the pooled representation from the final layer as the state representation. We use a decision transformer architecture, where states, actions, and rewards are provided to the model as a flattened sequence. The decision transformer uses a decoder-only GPT architecture <ref type="bibr" target="#b19">(Radford et al., 2018)</ref> to learn the optimal policy during training time. During inference time, it autoregressively predicts actions for a given state. An overview of our model architecture is shown in Figure <ref type="figure">2</ref>. Below, we describe important elements of the model architecture related to the query rewriting process. Rewriting Actions. Queries are rewritten using the following set of actions.</p><p>(1) Add synonym: adds the synonym of a selected word to the query. Previous work by work by Riezler and Liu (2010); <ref type="bibr" target="#b14">Mandal et al. (2019)</ref>, showed that rewriting queries with synonyms can improve query performance by potentially resolving ambiguous query terms.</p><p>(2) Swap with synonym: replaces a specific word </p><formula xml:id="formula_0">R 0 S 0 A 0 S 1 R 1 A 1 A ' 0 A ' 1 Figure 2: Model architecture. R, S, A represent reward,</formula><p>state and action, respectively. For instance, the state S 0 corresponds to a query, and the reward R 0 is the retrieval score such as AP@K. After we apply the action A 0 to the query S 0 , the query becomes S 1 . In inference time, the decision transformer predicts a series of actions</p><formula xml:id="formula_1">{A ′ 0 , A ′ 1 , • • • } to apply to the original query.</formula><p>from the query with its synonym. This action has the same goal as add synonym. Note that it includes the removal of the original token remove(original_token) .</p><p>(3) Change tense to present simple: changes verb tense into present simple for selected verbs in the input. Changing verbs to their morphological variants has been previously found useful for query rewriting <ref type="bibr" target="#b20">(Rafiei and Li, 2009;</ref><ref type="bibr" target="#b5">Haviv et al., 2021)</ref>.</p><p>(4) Remove: deletes selected words from the query. Previous work has found that deleting words in queries can lead to higher coverage of the search content <ref type="bibr" target="#b6">(Jones and Fain, 2003)</ref>.</p><p>We implement these actions using WordNet <ref type="bibr" target="#b15">(Miller, 1994)</ref> and the spaCy's part-of-speech tagger. Note that only certain actions are permitted for each part of speech tag: verbs support all four actions, nouns, adjectives and adverbs support all actions except changing verb tense, and stop words and other parts of speech support only the remove action. State Representation. We use sentence embeddings of the input claim as its state representation. An input claim C i is passed through a Sentence-BERT <ref type="bibr" target="#b22">(Reimers and Gurevych, 2019)</ref> network . The weights of the underlying pretrained language model (LM) are fine-tuned together with the decision transformer. Action Representation. Our action space is twodimensional: the first dimension represents the four action types (add synonym, swap with synonym, change tense to present simple and remove) and the second dimension represents the position of the token under edit, up to a maximum of 32 tokens. We pack these dimensions into a single dimension by taking their product, as shown in Table <ref type="table">1</ref>. Similar to the original implementation of the decision transformer, we pass the actions through a learned embedding layer to obtain an action vector representation.</p><p>Rewards. We use the retrieval score for the edited query as the system reward at time step t. Since the decision transformer uses returns-to-go to inform the model about future rewards, we use the sum of future rewards as a returns-to-go R t = T t ′ =t r t ′ . We also experimented with a delayed reward strategy, where we set the returns-to-go for the last time step to be the maximum score for given claim seen during the data generation process, and zero for intermediate steps. During inference, we initialize returns-to-go to the maximum reward and decrease it by the achieved score after we apply an action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Retriever</head><p>Since access to social media API search endpoints is limited, it is difficult to train an RL agent on top of them. Furthermore, the changing nature of misinformation on social media is another important factor to take into account, given that misinformative posts are periodically removed from social media platforms and are thus no longer available once fact-checked. These issues made us opt for a simulated search environment, with the added benefit of making our methods adaptable to arbitrary search endpoints. We experiment with two main systems: BM25. A retriever frequently used in the literature as a retrieval baseline <ref type="bibr" target="#b24">(Robertson and Zaragoza, 2009)</ref>. We use the Elasticsearch implementation of BM25 with the default parameters. Approximate kNN. A kNN retriever implemented using Elasticsearch's dense vector retrieval. We encode our data using pre-trained Sentence-BERT <ref type="bibr" target="#b22">(Reimers and Gurevych, 2019)</ref> and use the embeddings to conduct an approximate kNN search using the Hierarchical Navigable Small Worlds (HNSW) algorithm <ref type="bibr" target="#b11">(Malkov and Yashunin, 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">FEVER Dataset</head><p>The FEVER dataset <ref type="bibr" target="#b28">(Thorne et al., 2018</ref>) is a collection of manually written claims from Wikipedia that are connected with evidence sentences that either "support" or "refute" them. Since we are interested in claims linked to related evidence, we discard the claims in the dataset labeled as "NotE-noughInfo." This leaves us with 102,292 claims in the training and 13,089 claims in the development sets. <ref type="bibr" target="#b25">(Schuster et al., 2019)</ref> identified issues caused by the construction processes of the original FEVER dataset such as uses of negation in claims being heavily correlated with the "refute" outcome, therefore causing a "claim only" fact verification system to performs as well as an evidence-aware fact verification system. However, since our work is not concerned with the fact verification application of FEVER, we do not find this to be an issue.</p><p>FEVER is a well-known dataset among the misinformation and fact-checking communities. Even if FEVER is not a social media dataset, it is nonetheless based on user-contributed data, and thus we believe that the findings obtained using this dataset can be generalized to claims on social media platforms with minor domain-specific revisions, especially since the linguistic structure of claims and discussions around them is similar to the claims in the FEVER dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Generating RL-Friendly Training Data</head><p>To generate training data, we transform FEVER pairs (claim, evidence set) into sequences of editing actions that improve upon the original query. These transformations are obtained by exploring the state space of possible outcomes after applying different permutations of edits on the initial claims. We use a Breadth-First Search (BFS) strategy that applies editing actions to an input claim C i0 and finds the collection of the action sequences of (C ij-1 , A j , C ij , R) that can improve the initial claim, where C ij is the generated claim after applying the edit A j to the claim C ij-1 , and R is the reward of Q(C ij ) (querying retriever with C ij ).</p><p>Although understanding the effects of different search algorithms on our model remains an interesting problem for future work, our experiments show that using simple heuristics on BFS search Figure <ref type="figure">3</ref>: Sample sequence of claims generated by different actions: remove, change tense to present simple, swap with synonym, add synonym highlight the token to remove, the corresponding tokens to change tense as well as to swap to its synonym, or the corresponding places to add synonym in red, green, yellow and blue, respectively. We report the corresponding AP@50 scores below each claim. Section 3.2 Rewriting Actions provides intuitions of why these actions lead to better scores.</p><p>is effective while generating training data from the FEVER dataset. For instance, we find that limiting the depth of the breadth-first exploration to K levels is effective for improving the query results. Also, when conducting parallel runs on different sections of the dataset, even for K = 4, the vanilla depth-limited BFS takes a half to 2 days to generate the training data. Additionally, we find that restricting the state-space search to include only improvement edits at every step reduces the size of the search space. We also prune search paths leading to minor improvements (i.e. less than 3%) or at random in 5% of instances. Since most edits do not lead to significant improvements, it is unlikely we skip meaningful paths during the search. Finally, we only include sequences with the highest gains through serial edits, e.g. picking the top 50 or 100 most beneficial editing sequences for each claim, in our training set. Overall, these heuristics improve the generation speed and quality of the training instances.</p><p>Moreover the perfect score, and also edited claims leading to no improvement. Figure <ref type="figure">3</ref> shows examples of the sequences of claims generated by different actions.</p><p>Table <ref type="table">2</ref> reports the distribution of actions as well as the average improvement of AP@50 scores for each action when tested against the BM25 retriever.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We perform several experiments to determine the effectiveness of our adaptable query rewriting strategy. As a search environment, we use the BM25 and approximate kNN information retrieval methods described in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Settings</head><p>During our experiments, we use the original decision transformer implementation. <ref type="foot" target="#foot_0">1</ref> We use a 6layer decoder-only transformer with 8 heads, embedding dimension of 768. We set K (also called a block size) to be the maximum number of edits to the original query. We pad all sequences shorter than K. After flattening all the returns-togo, states and actions, our sequence becomes of length K * 3. We use the all-mpnet-base-v2 embedding model from the Huggingface's sentence transformers library. <ref type="foot" target="#foot_1">2</ref> We also experimented with the all-MiniLM-L12 model from the sentence transformers, but the results were worse, possibly because of all-MiniLM-L12 being a smaller model. Our intermediate state representations for an input claim is a vector of size 768. Our model is trained with cross entropy loss for 5 epochs performed on one Nvidia 2080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Results in Table <ref type="table" target="#tab_4">3</ref> show that the decision transformer model with fine-tuned state embeddings and dense rewards outperforms all systems with BM25 as retriever and AP@50 as reward. than the dense reward setting, suggesting that providing more granular information about each action's reward during training brings performance advantages. Both models turn the input into a significantly more effective query with performance improvements of up to 23% (relatively) as compared to just searching for the original claim. According to Table <ref type="table" target="#tab_5">4</ref> these gains are the highest for kNN as retriever and recall as reward. Table <ref type="table" target="#tab_4">3</ref> also shows that performing a random sequence of edit actions negatively affects performance. This suggests that there is a "query improvement process" that needs to be learned and applying a random sequence of edits by itself does not bring any inherent advantages, i.e. our systems do well not because there is an inherent gain in how we transform the problem, since if that was true, applying random action sequences should have yielded improvements over the claim baseline, which it did not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis</head><p>Figure <ref type="figure">4</ref> shows the mean AP@50 (mAP@50) score changes for all the generated sequences for the experiment of decision transformer with sparse reward. We plot the mAP@50 scores for queries generated at each step, where the x-axis shows the number of edits, with 1 representing the original claim and 5 the final rewritten query. The size of the circle indicates the number of queries at each turn.</p><p>If the claim achieves the perfect score, no further rewrites will be generated in the next turn and we stop early. We observe sequences with improved mAP@50 scores shrink along the turns. This indicates that some claims reach a perfect mAP@50 score after only one or two modifications. In contrast, for sequences with a decreased mAP@50 scores, the circle sizes remain the same while performance drops. This suggests that for such claims, the more the model modifies it, the worse its performance is. For the sequence of claims with the same mAP@50 scores at the beginning and the end, there is a slight up and down for the slopes for the lines in between. This suggests that there are mAP@50 e &gt; mAP@50 b mAP@50 e = mAP@50 b mAP@50 e &lt; mAP@50 b</p><p>Figure <ref type="figure">4</ref>: mAP@50 scores for all rewritten queries in the development set run against BM25. The x-axis indicates the claim rewriting sequence. The size of each circle represents the number of queries at each turn.The subscripts "e" and "b" correspond to "end" and "beginning" of the claim rewriting sequence, respectively. some sequences where the modified query achieves a better score while later modifications hurt the performance or vice versa. However, such scenarios are rare. Of the 13,089 claims in the development set, 1541 claims have the same AP@50 scores at the beginning and the end. Among these, 1243 are constant along the entire sequence and 271 have minor score changes, as reflected in Figure <ref type="figure">4</ref> as the blue line (mAP@50 e = mAP@50 b ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rmv</head><p>Figure <ref type="figure" target="#fig_1">5</ref> shows the distribution of actions in the model output corresponding to increased performance, no changes in performance, and decreased performance. We can see that most of the actions lead to no performance change. The remove and swap with synonym actions result more often in increases in performance than decreases. In con- trast, add synonym and change tense to present simple more often result in performance reduction. Figure <ref type="figure" target="#fig_2">6</ref> shows the average change per action. In this plot we observe that the net performance changes for remove and swap with synonym are positive, with an average of 1.83 and 0.88, respectively. The net performance changes for add synonym and change tense to present simple are negative, with an average of -0.34 and -0.98, respectively. We hypothesize that the model does not learn add synonym and change tense to present simple actions well due to the sparsity of such examples in the data as shown in Table <ref type="table">2</ref>. We further discuss the importance of these actions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rmv</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablations</head><p>We conduct ablation experiments to evaluate the ability of our system in adapting to arbitrary endpoints and different performance metrics. Although the space of possible ablations is far larger than what we present here, we pick three dimensions of ablations that could be useful for practitioners and future researchers: (i) retriever type (BM25 or kNN), (ii) reward metric (average precision, recall, reciprocal rank) and (iii) presence of negative training examples.</p><p>Table <ref type="table" target="#tab_5">4</ref> shows the results on each ablation when compared against a baseline of just using the initial claim. Across different metrics and retrievers we observe improvements in query performance: our system improves the original claim of up to 11% absolute recall points (42% relative improve-ment) and works on both BM25 and kNN retrievers. We also observe that the inclusion of training sequences with query performance decrease (negative training examples), consistently leads to performance decreases on all metrics and retrievers as compared to just training on positive edit sequences -with the only exception of querying kNN with RR as reward. We posit that this performance gap is due to the difference in data quality, i.e, providing our models with noiseless training signals leads to more effective queries. However, even in cases where we include negative training examples our models still meaningfully improve over the original claim.</p><p>6 Discussion Do we need to use (offline) RL for claim rewriting? It can be argued that a computationally expensive RL agent for query rewriting could be replaced by more economic design choices such as a sequence labeling model by fine-tuning a pretrained language model. In fact, as we discussed in the prior work section (2), researchers have indeed taken several different approaches for training neural text-editing models. In order to dig deeper into this question, we chose AP@50 as reward and trained a classifier on only the first edit in the training instances as 128-way classification (4 actions * 32 tokens), and the resulting classifier performed slightly worse than the RL agent trained on the whole edit sequence. However, we also observe from Figure <ref type="figure">4</ref> that when using the BM25 retriever and AP@50 as reward, the first action in training data is four times more effective than the following three actions on average, which means that the comparison between the classifier and the RL agent might not be a fair one. However, we also interpret the strong performance of the classifier as a more efficient alternative to training expensive reinforcement learning models. We leave a deeper comparison of the capabilities of sequence classification modeling and offline reinforcement learning for future work.</p><p>Are pretrained sentence embeddings good candidates for state representation? In our initial set of experiments we used frozen pretrained Sentence-BERT embeddings as state representation, and we did not see significant improvements over the initial claim. We observed a significant performance jump ( 5 mAP@50 points) once the sentence embeddings were also trained alongside the RL agent. This improvement highlights the importance of state representation and shows that task-specific embeddings perform better than general-purpose embeddings. This finding also indicates that the presence of Wikipedia data in the training data of LLMs does not simplify our task. Furthermore, there is significant prior work emphasizing the role and difficulty of the combinatorial and compositional nature of the state space in language tasks for reinforcement learning. <ref type="bibr" target="#b3">(Côté et al., 2018)</ref>, which also makes text-based RL agents a good choice for advancing our understanding of natural language.</p><p>What is the relation between query rewriting with sequence action learning and keyword extraction? We find that some of our models predict the remove action the vast majority of times, upwards of 80% in the case of using BM25 as retriever and AP@50 as reward. This brings up a natural question around how our method compares with keyword extraction methods, since the prevalence of remove edits during inference suggests that our approach works similar to keyword extraction. Our initial experiments with KeyBERT <ref type="bibr" target="#b4">(Grootendorst, 2020)</ref> show that this is not the case as keyword extraction does not perform comparably with the claim baseline on BM25 and AP@50 as reward. Although further analysis is required to make firm conclusions, it could be implied that including actions other than remove for rewriting queries can bring in significant gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we presented our findings on using an offline RL agent that learns editing strategies for query rewriting, so that fact-checkers can discover misinformation across social media platforms more effectively. Using a decision transformer, we showed that we can learn to rewrite misinformation claims by applying a series of interpretable actions such as adding synonyms or removing specific words. These actions can transform the claims into more effective queries, leading to a relative performance increase of up to 42% over a simpler kNN retriever baseline. Additionally, we conducted further analyses and ablation studies to develop a better understanding of our system, which showed that its adaptable to a variety of metrics and search engines. Our findings are an initial step towards building AI-assisted technologies to help fact-checkers discover online misinformation more effectively.</p><p>Future Work. While our work lays the grounds on using RL for building effective misinformation discovery tools, the practical application of our model requires further work to account for the limited access to social network APIs. This means additional constraints such as: (1) learning to rewrite claims under a fixed budget of training queries, and</p><p>(2) learning without supervision. While there are already several solutions available for (2) <ref type="bibr" target="#b26">(Shaar et al., 2020;</ref><ref type="bibr" target="#b7">Kazemi et al., 2021)</ref>, we believe ( <ref type="formula">1</ref>) is an exciting area for further exploration. Additionally, we posit our approach to be applicable on languages other than English since the RL agent we train is mainly language-agnostic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Limitations</head><p>Although we conduct ablations across several experimental settings, there are still important design decisions that require further research such as the design of action space and the utility of humanreadable edits for explainability. Our action space is one choice among the set of many possible text editing actions, thus there could be more expressive or efficient action spaces that lead to more efficient queries. Although there is no need for the rewrites to be explainable, our method has the potential to be explainable since the rewriting process is entirely human-readable. To understand the explainability potential, a study augmented with human evaluation of the rewritten claims is necessary, which we leave for future work.</p><p>work was partially supported by a grant from Meta and an award from the Robert Wood Johnson Foundation (#80345). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of Meta or the Robert Wood Johnson Foundation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Distribution of predicted actions (remove, swap with synonym, add synonym and change to present tense) with AP@50 reward and BM25 retriever.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Average change in AP@50 scores of the predicted actions (remove (Rmv), swap with synonym (S_ Syn), add synonym (A_ Syn) and change tense to present simple (Pre)) against BM25. Statistics for actions with no changes in AP@50 are excluded as this results in 0 scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Experiment results with BM25 as retriever.</figDesc><table><row><cell>The same model</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation experiments, RR refers to reciprocal rank.</figDesc><table><row><cell>Retriever(Query)</cell><cell cols="2">↑ rewards only mAP@50 Recall</cell><cell>RR</cell><cell>↑ + ↓ rewards mAP Recall</cell><cell>RR</cell></row><row><cell>BM25(RL[Claim])</cell><cell>32.43</cell><cell>35.8</cell><cell cols="3">30.23 31.50 32.82 29.80</cell></row><row><cell>BM25(Claim)</cell><cell>26.82</cell><cell cols="4">29.68 22.30 26.82 29.68 22.30</cell></row><row><cell>kNN(RL[Claim])</cell><cell>36.69</cell><cell cols="4">36.95 29.17 34.49 35.06 29.79</cell></row><row><cell>kNN(Claim)</cell><cell>28.40</cell><cell cols="4">25.93 21.27 28.40 25.93 21.27</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/kzl/decision-transformer</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://huggingface.co/sentence-transformers</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">Lan Zhang</rs>, <rs type="person">Qinyue Tan</rs>, and <rs type="person">Davis Liang</rs> for their help and feedback to this project. This</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The self-empowerment of the LGBT community is discussed in Born This Way </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Open-Domain Question Answering Goes Conversational via Question Rewriting</title>
		<author>
			<persName><forename type="first">Raviteja</forename><surname>Anantha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svitlana</forename><surname>Vakulenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhucheng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivas</forename><surname>Chappidi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.44</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.naacl-main.44" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="520" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Selecting good expansion terms for pseudo-relevance feedback</title>
		<author>
			<persName><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 31st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="243" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Decision Transformer: Reinforcement Learning via Sequence Modeling</title>
		<author>
			<persName><forename type="first">Lili</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/file/7" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Wortman</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="15084" to="15097" />
		</imprint>
	</monogr>
	<note>f489f642a0ddb10272b5c31057f0663-Paper</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Textworld: A learning environment for text-based games</title>
		<author>
			<persName><forename type="first">Marc-Alexandre</forename><surname>Pdf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akos</forename><surname>Côté</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingdi</forename><surname>Kádár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tavian</forename><surname>Kybartas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emery</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName><surname>Adada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Computer Games</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">KeyBERT: Minimal keyword extraction with BERT</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Grootendorst</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4461265</idno>
		<ptr target="https://doi.org/10.5281/zenodo.4461265" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERTese: Learning to Speak to BERT</title>
		<author>
			<persName><forename type="first">Adi</forename><surname>Haviv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.316</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.eacl-main.316" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. Association for Computational Linguistics</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3618" to="3623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Query word deletion prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">C</forename><surname>Fain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</title>
		<meeting>the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Claim Matching Beyond English to Scale Global Fact-Checking</title>
		<author>
			<persName><forename type="first">Ashkan</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Garimella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devin</forename><surname>Gaffney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Hale</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.347</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.acl-long.347" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4504" to="4517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ashkan</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verónica</forename><surname>Pérez-Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">A</forename><surname>Hale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07094</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">Matching Tweets With Applicable Fact-Checks Across Languages. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Relevance based language models</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Req-rec: High recall retrieval with query pooling and interactive classification</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</title>
		<meeting>the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="163" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><forename type="middle">A</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName><surname>Yashunin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="824" to="836" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Adamek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Malmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.12209</idno>
		<title level="m">Semi-Autoregressive Text-Editing with T5 Warm-Start</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Text Generation with Text-Editing Models</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Malmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandr</forename><surname>Chuklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Adamek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniil</forename><surname>Mirylenka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-tutorials.1</idno>
		<ptr target="https://doi.org/10.18653/v1/2022.naacl-tutorials.1" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorial Abstracts</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorial Abstracts<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Query Rewriting using Automatic Synonym Extraction for E-commerce Search</title>
		<author>
			<persName><forename type="first">Aritra</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ishita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prathyusha Senthil</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">eCOM@SIGIR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">WordNet: A Lexical Database for English</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/H94-1111" />
	</analytic>
	<monogr>
		<title level="m">Human Language Technology: Proceedings of a Workshop</title>
		<meeting><address><addrLine>Plainsboro, New Jersey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-03-08">1994. March 8-11. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automated Fact-Checking for Assisting Human Fact-Checkers</title>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Corney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maram</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Firoj</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Papotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaden</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">San</forename><surname>Martino</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2021/619</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2021/619SurveyTrack" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</title>
		<editor>
			<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</editor>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4551" to="4558" />
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Yala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1261</idno>
		<ptr target="https://doi.org/10.18653/v1/D16-1261" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2355" to="2365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Task-Oriented Query Reformulation with Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1061</idno>
		<ptr target="https://doi.org/10.18653/v1/D17-1061" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="574" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Davood</forename><surname>Rafiei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haobin</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0908.2588</idno>
		<title level="m">Wild Card Queries for Searching Resources on the Web</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">LEWIS: Levenshtein Editing for Unsupervised Text Style Transfer</title>
		<author>
			<persName><forename type="first">Machel</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.344</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.findings-acl.344" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3932" to="3944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1410" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Query Rewriting Using Monolingual Statistical Machine Translation</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli_a_00010</idno>
		<ptr target="https://doi.org/10.1162/coli_a_00010" />
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2010-09">2010. Sept. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<title level="m">The probabilistic relevance framework: BM25 and beyond</title>
		<imprint>
			<publisher>Now Publishers Inc</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards Debiasing Fact Verification Models</title>
		<author>
			<persName><forename type="first">Tal</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darsh</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serene</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Roberto Filizzola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName><surname>Barzilay</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1341</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1341" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3419" to="3425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">That is a Known Lie: Detecting Previously Fact-Checked Claims</title>
		<author>
			<persName><forename type="first">Shaden</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><surname>Nakov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.332</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.acl-main.332" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3607" to="3618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Seq2Edits: Sequence Transduction Using Span-level Edit Operations</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.418</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.emnlp-main.418" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5147" to="5159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">FEVER: a Large-scale Dataset for Fact Extraction and VERification</title>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arpit</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1074</idno>
		<ptr target="https://doi.org/10.18653/v1/N18-1074" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="809" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Zeqiu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Reitter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><forename type="middle">Singh</forename><surname>Tomar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.08558</idno>
		<title level="m">CONQRR: Conversational Query Rewriting for Retrieval with Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
