<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring the Impact of Training Data Distribution and Subword Tokenization on Gender Bias in Machine Translation</title>
				<funder>
					<orgName type="full">LINDAT/CLARIAH-CZ</orgName>
				</funder>
				<funder ref="#_y9VuFhS">
					<orgName type="full">Ministry of Education, Youth and Sports of the Czech Republic</orgName>
				</funder>
				<funder ref="#_FhVCBEK">
					<orgName type="full">Charles University Grant Agency</orgName>
				</funder>
				<funder ref="#_Hgn6cCe">
					<orgName type="full">Israeli Ministry of Science and Technology</orgName>
				</funder>
				<funder ref="#_uAFx5wn">
					<orgName type="full">Czech Science Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Mobility Fund of Charles University</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bar</forename><surname>Eluz</surname></persName>
							<email>bar.iluz@mail.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Mathematics and Physics</orgName>
								<orgName type="institution" key="instit1">The Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit2">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tomasz</forename><surname>Limisiewicz</surname></persName>
							<email>limisiewicz@ufal.mff.cuni.cz</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Mathematics and Physics</orgName>
								<orgName type="institution" key="instit1">The Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit2">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
							<email>gabriel.stanovsky@mail.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Mathematics and Physics</orgName>
								<orgName type="institution" key="instit1">The Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit2">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Mareček</surname></persName>
							<email>marecek@ufal.mff.cuni.cz</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Mathematics and Physics</orgName>
								<orgName type="institution" key="instit1">The Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit2">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring the Impact of Training Data Distribution and Subword Tokenization on Gender Bias in Machine Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9E117110CBF05CE48BB0C43246D4C198</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the effect of tokenization on gender bias in machine translation, an aspect that has been largely overlooked in previous works. Specifically, we focus on the interactions between the frequency of gendered profession names in training data, their representation in the subword tokenizer's vocabulary, and gender bias. We observe that female and nonstereotypical gender inflections of profession names (e.g., Spanish "doctora" for "female doctor") tend to be split into multiple subword tokens. Our results indicate that the imbalance of gender forms in the model's training corpus is a major factor contributing to gender bias and has a greater impact than subword splitting. We show that analyzing subword splits provides good estimates of gender-form imbalance in the training data and can be used even when the corpus is not publicly available. We also demonstrate that fine-tuning just the token embedding layer can decrease the gap in gender prediction accuracy between female and male forms without impairing the translation quality.</p><p>1 * Equal contribution. † Work partially done while visiting the Hebrew University.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine translation has been one of the fastestgrowing research directions in NLP. However, with the intensive growth of the technology, multiple potential harms were identified <ref type="bibr" target="#b7">(Hovy and Spruit, 2016)</ref>, including gender bias, where models rely on spurious correlations (doctors tend to be male) to make their predictions rather than more meaningful signals in their input <ref type="bibr" target="#b18">(Stanovsky et al., 2019)</ref>.</p><p>There are many reasons for the bias, such as imbalances in the training set or architecture choice. Previous works proposed various approaches to combat gender bias in translation models <ref type="bibr">(Saunders</ref> Figure <ref type="figure">1</ref>: The schema depicts two factors affecting the accuracy of translating profession words into correct gender-inflected forms in morphologically rich languages. The first factor is the frequency of gender inflections of profession names in the training corpus, and the second is the number of subword tokens that these forms are split into. Our analysis reveals that the frequency significantly correlates with the translation accuracy and number of tokens per word. However, when we control for frequency, the correlation between the number of tokens and translation accuracy is insignificant, indicating that frequency is a confounding variable. and Byrne, 2020; Escudé <ref type="bibr" target="#b4">Font and Costa-jussà, 2019)</ref>.</p><p>In this paper, we focus on the role of tokenization on gender bias which has been largely overlooked in previous approaches to the problem.</p><p>We want to study the causal relationship between tokenization and gender bias. Specifically, we want to know: 1. How do subword tokenizers handle different gender forms, i.e., are female and non-stereotypical gender forms split into more tokens than male and stereotypical gender forms? 2. Whether subword splitting has an impact on the accuracy of translation? 3. Would subword tokenization's effect be significant when accounting for the frequency of gender forms in the training corpus?</p><p>To answer those questions, we analyze pretrained machine translation models from English to a diverse set of three languages that denote morphological gender in nouns <ref type="bibr">( German, Spanish, and Hebrew)</ref>.</p><p>First, we compare the number of tokens of different gender forms in the target language and find out that, indeed, female and anti-stereotypical forms are split into more tokens. Second, the causality analysis shows that the number of subword tokens may initially appear to explain the translation accuracy of gender forms. However, we find out that these factors are conditionally independent when we also consider the word frequency in the training set, as depicted in Figure <ref type="figure">1</ref>. To support this finding, we fine-tune the model on gender balanced dataset and update its tokenizer, showing that the dataset's role is more impactful on gender bias than tokenization.</p><p>To the best of our knowledge, this work is the first in-depth analysis of interactions between training data, tokenization method, and gender bias. Our findings confirm the previous observations <ref type="bibr" target="#b15">(Saunders and Byrne, 2020;</ref><ref type="bibr" target="#b23">Zmigrod et al., 2019)</ref> indicating that the distribution of gender forms in the training data significantly influences the bias of a model. Subword tokenizers, typically trained on the same data, can also perpetuate biases present in the data. We show that it is feasible to analyze the representation of gender forms in the learned vocabulary to obtain information on gender distribution in the model's training corpus even without having access to it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Models, languages and tokenizers</head><p>The translation models we used for the analysis are OpusMT <ref type="bibr" target="#b20">(Tiedemann and Thottingal, 2020)</ref>, based on Marian NMT framework <ref type="bibr" target="#b9">(Junczys-Dowmunt et al., 2018)</ref>, trained on the Opus dataset<ref type="foot" target="#foot_0">2</ref> , and MBART50 <ref type="bibr" target="#b19">(Tang et al., 2020)</ref>, a multilingual encoder-decoder model trained on 50 languages. Both OpusMT and MBART50 use SentencePiece based tokenizer <ref type="bibr" target="#b12">(Kudo and Richardson, 2018)</ref> with unigram language model <ref type="bibr" target="#b11">(Kudo, 2018)</ref>. We test models in translating from English to German, Spanish, and Hebrew.</p><p>Choice of target languages. We chose German, Spanish, and Hebrew as target languages since they are diverse, and all assign grammatical gender to profession names, adjectives, and nouns. Moreover, the authors of this study possess a proficiency ranging from intermediate to native levels in the aforementioned languages. To highlight typological differences between the languages: Hebrew is a Semitic language from the Afroasiatic language family with Abjad script. German is a Germanic language with the Latin alphabet. Spanish is a Romance language with Latin script that uses a change of suffix (instead of addition) for male-to-female infections. Both German and Spanish belong to the Indo-European language family. Choice of models. We chose OPUS and mBART models because they are accessible through Huggingface, they support all the languages we selected for analysis, and they manifest strong performance in translation tasks. Both models follow state-ofthe-art design choices, specifically Transformer architecture and SentencePiece tokenizer, which was shown to be preferred over BPE in multilingual models <ref type="bibr">(Bostrom and Durrett, 2020a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data</head><p>For the evaluation of gender bias, we use the WinoMT <ref type="bibr" target="#b18">(Stanovsky et al., 2019)</ref>. It is a synthetic English dataset of sentences containing profession names coreferred by gendered pronouns. The sentences are balanced in terms of the number of male and female pronouns used. The construction of this dataset allows checking if translation models show preference toward particular gender forms. The methods for measuring these preferences are described in the following subsection 2.3.</p><p>Gender forms in target languages. To validate the gender translation correctness of professions, we collected human translations from native speakers of Hebrew, German, and Spanish (three annotators for each language) for a list of 40 professions from WinoBias dataset <ref type="bibr" target="#b22">(Zhao et al., 2018)</ref>. The list contains an equal share of professions that are majorly performed by men and women (based on labor statistics). Each profession is translated to a pair of masculine and feminine forms with the same stem (e.g., "Mediziner" -"Medizinerin" and "Arzt" -"Ärztin" are two pairs in German). <ref type="foot" target="#foot_1">3</ref> The annotators could propose up to 3 pairs of translations for each profession. Subsequently, the authors selected a list of pairs that were proposed by at least two annotators<ref type="foot" target="#foot_2">4</ref> . As a result, we accepted 67 pairs for German (77% of the annotators' propositions), 54 pairs for Hebrew (76% of the propositions), and 45 pairs for Spanish (73% of the propositions). The lists with the translations will be released upon the publication of this work.</p><p>Gender forms frequency analysis. To estimate the frequency of gender forms in the training corpus, we analyze the OPUS-100 dataset <ref type="bibr" target="#b21">(Zhang et al., 2020)</ref>. It is a sample from OPUS collection on which OpusMT was trained. It comprises multiple corpora subjects like movie subtitles, code documentation, and the Bible. We used all three training, development, and test corpora of OPUS-100, which contains 1,004,000 sentences for each of the analyzed languages.</p><p>Gender-balanced dataset for fine-tuning. We compile a gender-balanced bilingual dataset containing a simple English template: "He/She is the [profession]" paired with translations to Hebrew and German. The number of examples with male and female pronouns is equal. The templates are filled with profession names from WinoMT from English and their translations proposed by the annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Metrics</head><p>Gender Translation Accuracy (F1). To evaluate the model's performance, we check whether the model translates professions in English from WinoMT dataset to correct gender forms in the target language proposed by annotators. For instance, the profession "Physician", when the pronouns indicate male gender in the source sentence, should be translated to corresponding male forms in German ("Arzt" or "Mediziner"). Respectively, when a pronoun indicates female gender, we should obtain female forms in the translator's output sentence ("Ärztin" or "Medizinerin" for German). We compute the number of correct occurrences, i.e., translated sentences for which profession and gender match the English source.</p><p>We define recall for each profession in a specific gender as the share of correct occurrences out of a total number of source (English) sentences where the profession appeared.</p><p>The precision for each attested (i.e., proposed by annotators) profession translation is the share of correct occurrences out of the total number of outputted sentences where the profession translation appeared.</p><p>In our experiments, we report F1, which is the harmonic mean of precision and recall for each attested profession translation.</p><p>Measures of Gender Bias (∆G, ∆S, ∆T ). We use metrics proposed by <ref type="bibr" target="#b18">Stanovsky et al. (2019)</ref> to measure gender bias in MT: ∆G measures the difference in gender translation correctness (F1) between masculine and feminine entities; similarly, ∆S measures the difference in F1 between prostereotypical and anti-stereotypical instances of gender role assignments. <ref type="foot" target="#foot_3">5</ref>We compute ∆G for each pair of male and female translations. It is a more fine-grained approach than in previous works:</p><formula xml:id="formula_0">∆G = F1 m.trans. -F1 f.trans.</formula><p>(1)</p><p>Analogically we compute ∆S as the difference in F1 between pro-and anti-stereotypical translation. ∆S = F1 pro.trans. -F1 anti.trans.</p><p>(2)</p><p>Additionally, we define new metrics that measure the differences in the number of tokens that distinct gender forms split into for each pair of profession name translations. ∆T G , analogically to ∆G, quantifies the difference in the number of tokens between the male form and the female form:</p><formula xml:id="formula_1">∆T G = n. Tokens m.trans. -n. Tokens f.trans. (3)</formula><p>While, ∆T S corresponds to ∆S and measures the difference in number of tokens between the proand anti-stereotypical forms:</p><p>∆T S = n. Tokens pro.trans.n. Tokens anti.trans.</p><p>(4) With ∆T we metricize bias already on the tokenization level and inspect its effect on the machine translation performance. We expect the words split into more tokens to be harder to predict and thus observe correlations between pairs ∆T and translation bias metrics: ∆G and ∆S.</p><p>Examples: In case of translation from English to German, recall for English profession "Physician" in female form is the number of times "Physician" appeared in the source dataset as female and was translated to "Ärztin" or "Medizinerin", divided by the number of times "Physician" appeared as a female in the source dataset. Precision for translation "Ärztin" is the number of times "Physician' appeared in the source dataset as female and was translated to "Ärztin" divided by the number of times the word "Ärztin" appeared in the translator's output. F1 for translation "Ärztin" is a harmonic mean of recall for the female "Physician" and precision for "Ärztin".</p><p>∆G for a pair "Arzt" -"Ärztin" is difference in F1 for translation "Arzt" and translation "Ärztin". Because "Physician" is a stereotypically male profession ∆S equals to ∆G.</p><p>Both ∆T G and ∆T S are the difference between the number of tokens the words "Arzt" and "Ärztin" are divided into by the German tokenizer.<ref type="foot" target="#foot_4">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>To test how tokenization affects the translation accuracy of professionals' gender, and whether a model prefers to generate translations with fewer tokens, we design four experiments explained in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Are female and anti-stereotypical forms</head><p>split into more tokens?</p><p>We take human translations (obtained from the procedure described in Section 2.2) and check how many tokens they are split into by the analyzed system's tokenizer. We expect that female forms will be split into more tokens than male forms, partially due to derivational suffixes appearing only in female forms.</p><p>Results Figures <ref type="figure">2</ref> and<ref type="figure">3</ref> show how many translated WinoMT professions are split into a specific number of tokens. We observe that female forms tend to be divided into more tokens than male ones. Only a small portion of female forms are not split. Similarly, pro-stereotypical translations are split into fewer tokens than anti-stereotypical ones. However, the difference is smaller than in the case of gender. We observe that the difference in the male and female number of tokens in Spanish is smaller because female forms in Spanish are sometimes expressed by changing the suffix rather than addition (e.g. Consejero vs. Consejera).</p><p>The reason why female and anti-stereotypical forms are split into more tokens is probably that they appear in the training corpus less often.<ref type="foot" target="#foot_5">7</ref> </p><p>3.2 Does subword splitting affect the accuracy of translation?</p><p>We compute bias metrics: ∆G, ∆S for pairs of translations to compare them with the difference in the number of tokens between gender forms (∆T ).</p><p>We expect that when profession names differ in the number of tokens, the model will more likely generate the shorter form (typically the male or pro-stereotypical). Our inspection is that the preference for shorter forms is connected to gender bias. Therefore we expect to observe a negative correlation between the difference in translation accuracy and the number of subword tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>In Figures <ref type="figure" target="#fig_2">4</ref> and<ref type="figure">5</ref>, we observe negative correlations between ∆T and both ∆G and ∆S. The relationship is stronger in the latter case. This finding supports our hypothesis that the difference in the number of tokens leads to the model's preference for a form with fewer tokens. Additionally, for translation pairs with ∆T = 0, the median of the bias measure distribution is close to zero (with the notable exception of ∆G for Hebrew). This suggests that the model is less biased for professions when both translation forms are divided into the same number of tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">What is the causal relationship between tokenization, training data, and gender prediction accuracy?</head><p>An alternative explanation of the negative trend observed in the previous experiment is the presence of an underlying factor, in our case, the frequency of specific gender forms in the training corpus. Previous research has shown that the terms' frequency affects both tokenization <ref type="bibr" target="#b11">(Kudo, 2018)</ref> and gender bias <ref type="bibr" target="#b4">(Escudé Font and Costa-jussà, 2019)</ref>.</p><p>In this experiment, we measure the significance of the correlation between those three factors. Also, we check the conditional independence between the number of tokens per target profession and gender prediction accuracy (measured by F1 score for each Figure <ref type="figure">2</ref>: OpusMT: Human translated profession names were grouped by the number of tokens they were split into.</p><p>On the x-axis: number of tokens per word. On the y-axis: the count of male and female forms professions in each of the groups. Male forms tend to be split into fewer tokens than female forms. Figure <ref type="figure">3</ref>: OpusMT: Human translated profession names were grouped by the number of tokens they were split into. On the x-axis: number of tokens per word. On the y-axis: the count of pro-and anti-stereotypical forms of professions in each of the groups. Pro-stereotypical forms tend to be split into fewer tokens than anti-stereotypical forms.</p><p>- target profession described in Section 2) given the profession form's frequency in the training corpus:</p><formula xml:id="formula_2">F 1 ⊥ n. Tokens | Frequency (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>In Figure <ref type="figure" target="#fig_4">6</ref>, we observe that as the number of tokens decreases, F1 increases. Taking into account the correlation coefficient, the F1 measure is more sensitive to the frequency of a word in the training corpus. Moreover, less frequent words tend to be split into more tokens. From the density plots (on the diagonal of the figure), we see that male profession words (especially pro-stereotypical ones) appeared much more often in the training corpora. Thus, they tended to be split into fewer tokens.</p><p>All the correlations between frequency and the two remaining factors are statistically significant (p &lt; 0.05), while the correlation between the number of tokens and F1 score is significant only for German.</p><p>We performed Jonckheere-Terpstra test <ref type="bibr" target="#b8">(Jonckheere, 1954)</ref> to check conditional independence as described in Equation <ref type="formula">3</ref>.3. The test showed that Figure <ref type="figure">5</ref>: OpusMT: ∆S as the difference between F1-score for pro-and anti-stereotypical test instances for each of paired translations. ∆T G is the difference between the number of tokens in a pro-and an anti-stereotypical translation. An orange line marks the median. We observe a significant negative correlation between the two measures. Pearson's correlation coefficients and corresponding p-values: ρ = -0.37, p = 0.010 for German, ρ = -0.37, p = 0.024 for Spanish, and ρ = -0.41, p = 0.008 for Hebrew. conditional independence cannot be rejected for all the target languages (p = 0.78 for German, p = 0.62 for Spanish, and p = 0.39 for Hebrew).</p><p>Those results show that the frequency of a word is a confounding factor affecting both the number of tokens per profession word and F1 scores. Hence subword splitting of profession words is not a significant contributor to the correctness of gender prediction given the frequency of gender forms in the training corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Will intervening in the model's training data and the tokenizer's vocabulary reduce gender bias?</head><p>To verify our findings about causes of gender bias, we propose two interventions in the translation models to German, Spanish, and Hebrew: 1. finetuning on a dataset with an equal number of male and female forms;<ref type="foot" target="#foot_6">8</ref> 2. Adapting the tokenizer's vocabulary by adding all translations proposed by annotators to assert that they will not be split into subword tokens. We monitor standard gender bias measures from <ref type="bibr" target="#b18">Stanovsky et al. (2019)</ref>: gender accuracy, ∆G, and ∆S,<ref type="foot" target="#foot_7">9</ref> and also BLEU on OPUS-100 test split to check if fine-tuning leads to deterioration of translation quality. To determine if the potential improvement results from embeddings' fine-tuning or adding profession words to the vocabulary, we evaluate the baseline, where the embedding layer is fine-tuned without updating the tokenizer's vocabulary.</p><p>Results Table <ref type="table">1</ref> shows that fine-tuning embedding layers improve the accuracy of translating to the correct gender and decrease preference of male forms (∆G), while the quality of translation stays on a similar level as in the original model. Protecting gender forms from splitting (by adding them to the tokenizer's vocabulary) only slightly further converges ∆G towards zero for German while bringing over 2 points drop in BLEU. Performing the vocabulary update before fine-tuning models deteriorates bias measures and BLEU for Hebrew. For Spanish, ∆S improves while ∆G and BLEU worsen.</p><p>Interestingly, fine-tuning the embedding layer increases the stereotypical bias (∆S) for German and Spanish and decreases it only slightly for Hebrew, suggesting that this method could not be sufficient for mitigating this type of bias.</p><p>The results confirm our previous observation that the training (or fine-tuning) dataset has more impact on gender bias encoded in the model than subword splitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Evaluation and Mitigation of Gender Bias in NMT Escudé Font and Costa-jussà (2019) created a set of sentences of the format "I've known [him/her/Mary/John] for a long time, my friend works as a <ref type="bibr">[profession]</ref>." They translated all sentences to Spanish and checked whether "friend" was translated as "amigo" (male) or "amiga" (female). They found out that "Him" is predicted at almost 100% accuracy for all models, but the accuracy drops when predicting the word "her" on all models.</p><p>Stanovsky et al. ( <ref type="formula">2019</ref>) created a mechanism to evaluate gender bias in machine translation. They showed that almost all translation systems perform significantly better on male roles.</p><p>The past works acknowledged the effect of gender form imbalance in training corpus on gender bias manifested in the models. Specifically, <ref type="bibr" target="#b23">Zmigrod et al. (2019)</ref> reduce the bias by training the NMT systems on data augmented by adding female forms. <ref type="bibr" target="#b15">Saunders and Byrne (2020)</ref>; Costa-jussà and de Jorge (2020) propose a debiasing algorithm based on fine-tuning the model on a dataset with a comparable number of male and female forms. These approaches are more sustainable because they do not require training the model from scratch. These approaches are in line with our observation that the imbalance of gender forms in training data is the key source of gender bias in the model. <ref type="bibr" target="#b16">Savoldi et al. (2021)</ref> survey the methods for evaluating and mitigating bias in machine translation, identify risks connected to them and propose direction for their improvement. <ref type="bibr" target="#b6">Guo et al. (2022)</ref> propose an automatic promptbased method to mitigate the biases in pretrained language models. They identify biased prompts and propose a distribution alignment loss to mitigate it.</p><p>The Role of Tokenization on System's Performance <ref type="bibr" target="#b3">Domingo et al. (2018)</ref>   <ref type="table">1</ref>: Embedding fine-tuning results in German and Hebrew. Original OpusMT model results compared with the model after embedding layer fine-tuning (row 2) and the model after embedding layer fine-tuning and updating vocabulary with all profession gender forms in target language (row 3). ers play a significant role in the neural machine translation pipeline. The tokenization of the target language affects evaluation measures (BLEU) by up to 20 points. <ref type="bibr">Bostrom and Durrett (2020b)</ref> compare the popular subword tokenizers: Unigram <ref type="bibr" target="#b11">(Kudo, 2018)</ref> and BPE <ref type="bibr" target="#b17">(Sennrich et al., 2016)</ref>. They show that the former more often splits words on morphological boundaries and thus can improve the model's performance on downstream tasks.</p><p>While the recent survey <ref type="bibr">(Mielke et al., 2021)</ref> shows that the choice of effective tokenization method depends on the task, and there is no specific tokenization algorithm suiting all applications.</p><p>The role of tokenization on gender bias was not widely evaluated in past works. <ref type="bibr" target="#b13">Libovický et al. (2022)</ref> showed that character-based translators to morphologically rich languages (Czech and German) obtain similar bias results as subword-based systems, even though female forms in these languages contain relatively more characters. It aligns with our observation that tokenization is not a significant source of bias. <ref type="bibr" target="#b16">Gaido et al. (2021)</ref> explored how segmenting methods influence gender bias in speech translation models. Such models have different features, such as vocal characteristics used to measure gender bias over all words, while our work focuses on gender bias in profession words. In contrast to their work, we focus on formal gender bias definitions and analyzing the relation between frequency, accuracy, and number of tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Future Work</head><p>Our confirms the validity of the causal schema depicted in Figure <ref type="figure">1</ref> explaining the lower translation accuracy for female and anti-stereotypical profession names. We consciously analyzed the dataset of only non-ambiguous sentences where the gender of each profession is known. This selection was made to enable us to evaluate the accuracy of the ground-truth gender.</p><p>Furthermore, the stereotypical occupations are based on US Department of Labor statistics, but it cannot be guaranteed that these same stereotypes are present in other cultures. However, it can be considered a reasonable estimate for other languages, as evidenced by the observation that nonstereotypical professions appear less frequently in their training corpora. Future research may analyze gender roles in target languages to corroborate these observations.</p><p>Another future point of interest is mapping more factors for bias by isolating more features. The dependencies of tokens, frequency, and gender bias will be examined on a larger scale, with more words and different types of tokenization. We also intend to broaden the scope of the analysis to other languages and include neutral gender forms for the already analyzed languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Our study found that profession words in German, Hebrew, and Spanish tend to be split into more tokens in the female form than in the male form. We then investigated whether this phenomenon amplifies the tendency of NMT models to translate male professions more accurately than female ones. Our results showed that the frequency of gender forms confounds the relationship between the number of tokens and gender bias in the training set. However, the number of subword tokens per word can be used to estimate its frequency in the unavailable training corpus.</p><p>The findings of our analysis of translation models were supported by the trends observed in the results of the proposed debiasing method. Specifically, we found that fine-tuning token embeddings on a gender-balanced dataset had a more significant impact on reducing bias than updating the tokenizer's vocabulary with underrepresented gender forms.</p><p>These findings suggest that future research should also focus on other aspects of NMT models in order to mitigate gender bias effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>As pointed out in the last section, tokenization can reinforce gender bias in the models but is not the sole factor responsible for gender bias. We expect that even if male and female forms were tokenized to the same number of tokens, bias could still persist in the models.</p><p>The important aspect is the frequency of words in the training corpus, which also affects tokenization. We did not have exact information on the model's training corpus. However, the analysis of the OPUS-100 dataset, which was sampled from the full dataset, showed that male forms are much more frequent than female ones.</p><p>In addition, we analyzed only 40 stereotypical professions from WinoMT, which is only a subset of the whole dataset. Extending the analysis to other professions without typical gender-role assigned can reveal slightly different trends.</p><p>Finally, non-zero bias measures indicate the presence of bias in the model, but the opposite implication is not true. Even in the case where both ∆G and ∆S are close to zero, we cannot claim that the model is unbiased, as there may be other bias manifestations of bias involved.</p><p>A significant limitation of this work is that we compare only male and female forms of the professions while neglecting neutral forms. We expect neutral forms to be even harder for the model to predict than female forms because of the added suffixes that are rarely present in the training data. For instance, German uses forms with "*" and female prefixes (e.g., "Mechaniker*in") while "\" is used in Hebrew for that purpose. Our initial analysis shows that neutral forms are sporadic in the training corpora. For instance, in German, only 7 neutral forms are using the inclusive suffix "*in", compared to 1351 female forms of the analyzed professions. The proportion of neutral:female:male forms is approximately 1:193:1845.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Human Translation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Post-processing of Collected Data</head><p>The post-processing of the human translations was done according to the following rules: First, we kept the translations suggested by at least two translators. If there's no translation option suggested at least two times, we decided which translations to keep according to our knowledge of the language. This happened only for the following translations in Hebrew: "attendant", "construction worker", and "analyst". The translation selection for those words was made by an author who is a native Hebrew speaker. For professions where translators propose the same word as both male and female translations, we keep those as valid pairs.</p><p>In case spelling is inconsistent among annotators, we kept the more common spelling while fixing pronunciation mistakes according to our knowledge. Lastly, we keep at most five pairs of translations per profession.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Instructions for Annotators</head><p>The goal of the task is to get the correct translation of profession words into your language for both Male and Female forms.</p><p>1. Select the language you want to translate to (your native language: German, Spanish, or Hebrew).</p><p>2. For each word in English, write up to 3 translations both in Male and Female form.</p><p>3. Organize the translation into pairs that differ only in endings (e.g., words: Berater and Beraterin are a pair but Beraterin and Ratgaber are not a pair in German). Put these words in the neighboring cells ("Translation N Male" and "Translation N Female").</p><p>4. in case there is no translation in the other gender that differ only in an ending, leave the other cell of the pair blank.</p><p>5. If you propose less than 3 translations in each gender, leave the last cells in a row empty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details of Fine-tuning</head><p>In fine-tuning, we train only the translation model's input/output embedding layer while keeping the rest of the parameters frozen. We further trained the model for three epochs on the gender-balanced dataset (Section 2.2) with batches consisting of 16 examples. For optimization, we used Adam (Kingma and Ba, 2015) with default parameters with a constant learning rate of 5e -5. The finetuning took about 10 minutes on Nvidia A40 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C MBART results</head><p>We repeat the analysis of the number of tokens per word described in Section 3.1 for MBART.</p><p>Figures <ref type="figure">7</ref> and<ref type="figure">8</ref> show that similarly to Opus models, MBART split female and anti-stereotypical words in the target language into more tokens. Noticeably, almost no female words, in all the languages are represented as one token. Figures 9 and 10 show similar trends for ∆G and ∆S against ∆T as observed in Section 3.2 for OPUS models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D WinoMT details</head><p>The WinoMT dataset that we used for evaluation contains 3,888 sentences. Each sentence contains a profession and a pronoun that describes the gender of the profession. The dataset is equally balanced between male and female examples and also between stereotypical and non-stereotypical genderrole assignments. Specifically, there are 1826 sentences with a male pronoun assigned to the profession and 1822 sentences with a female pronoun. The remaining 240 sentences are gender-neutral, i.e., they contain the pronoun "they".</p><p>Note that this dataset contains only nonambiguous sentences where the gender of each profession is known. This selection was made to enable us to evaluate the accuracy of the groundtruth gender. (c) Hebrew</p><p>Figure <ref type="figure">7</ref>: MBART: Human translated profession names grouped by the number of tokens they were split into. On the x-axis: number of tokens per word. On the y-axis: the count of male and female forms professions in each of the groups. Male forms tend to be split into fewer tokens than female forms. Figure <ref type="figure">8</ref>: MBART: Human translated profession names grouped by the number of tokens they were split into. On the x-axis: number of tokens per word. On the y-axis: the count of pro-and anti-stereotypical forms of professions in each of the groups. Pro-stereotypical forms tend to be split into fewer tokens than anti-stereotypical forms. Figure <ref type="figure">9</ref>: MBART: ∆G as the difference between F1-score for male and female test instances for each of paired translations. ∆T is the difference between the number of tokens in a male and a female form. The median is marked by an orange line. Median is marked by an orange line.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: OpusMT: ∆G as the difference between F1-score for male and female test instances for each of paired translations. ∆T G is the difference between the number of tokens in a male and a female form. An orange line marks the median. Pearson's correlation coefficients and corresponding p-values: ρ = -0.25, p = 0.09 for German, ρ = -0.21, p = 0.20 for Spanish, and ρ = -0.11, p = 0.50 for Hebrew.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Pair analysis of gender prediction performance (F1), number of tokens, and the frequency of each profession in the OPUS-100 dataset. Each style of dots represents professions in the male/female pro-stereotypical/antistereotypical form. The diagonal plots show the density of the feature for the specific gender and stereotype sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure10: MBART: ∆S as the difference between F1-score for pro-and anti-stereotypical test instances for each of paired translations. ∆T is the difference between number of tokens in a pro-and an anti-stereotypical translation. Median is marked by an orange line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table</head><label></label><figDesc>show that tokeniz-</figDesc><table><row><cell></cell><cell></cell><cell>Accuracy</cell><cell></cell><cell>∆ G</cell><cell></cell><cell></cell><cell>∆ S</cell><cell></cell><cell></cell><cell>BLEU</cell></row><row><cell></cell><cell>DE</cell><cell>ES</cell><cell>HE DE</cell><cell>ES</cell><cell>HE</cell><cell>DE</cell><cell>ES</cell><cell>HE</cell><cell>DE</cell><cell>ES</cell><cell>HE</cell></row><row><cell>original</cell><cell cols="11">62.2 56.3 52.1 9.6 16.4 12.3 12.5 16.7 40.4 31.4 42.8 36.0</cell></row><row><cell>FT</cell><cell cols="11">67.1 58.9 54.0 -2.9 10.9 7.8 21.5 20.0 35.1 31.3 42.0 35.8</cell></row><row><cell cols="12">FT + VU 67.8 58.0 52.4 1.1 14.0 10.4 21.7 18.2 40.7 29.1 38.3 35.1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://opus.nlpl.eu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Both pairs are German translations of the English profession "Doctor".</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>For detailed rules for combining the final lists and discussion of special cases, see Appendix A</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>We use previous works' attribution of stereotype to particular English professions<ref type="bibr" target="#b22">(Zhao et al., 2018)</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>Please note that for stereotypically female professions, we have ∆G = -∆S and ∆TG = -∆TS.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>The results of MBART50 are similar to Opus-MT and are represented in Appendix C.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>The dataset contains the sentences in the form "He/She is the [profession]" translated to the target language and filled with professions from WinoMT.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>In this experiment, we use the original formulation of ∆G and ∆S. It means F 1 is computed for the whole dataset, instead of particular professions as in the previous experiments.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank anonymous reviewers for their valuable comments on the previous versions of this article. This work was partially supported by the <rs type="funder">Israeli Ministry of Science and Technology</rs> (grant no. <rs type="grantNumber">2088</rs>) and by grant <rs type="grantNumber">23-06912S</rs> of the <rs type="funder">Czech Science Foundation</rs>. <rs type="person">Tomasz Limisiewicz</rs>'s visit to the <rs type="institution">Hebrew University</rs> has been supported by grant <rs type="grantNumber">338521</rs> of the <rs type="funder">Charles University Grant Agency</rs> and the <rs type="funder">Mobility Fund of Charles University</rs>. We have been using language resources and tools developed, stored, and distributed by the <rs type="funder">LINDAT/CLARIAH-CZ</rs> project of the <rs type="funder">Ministry of Education, Youth and Sports of the Czech Republic</rs> (project <rs type="grantNumber">LM2018101</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Hgn6cCe">
					<idno type="grant-number">2088</idno>
				</org>
				<org type="funding" xml:id="_uAFx5wn">
					<idno type="grant-number">23-06912S</idno>
				</org>
				<org type="funding" xml:id="_FhVCBEK">
					<idno type="grant-number">338521</idno>
				</org>
				<org type="funding" xml:id="_y9VuFhS">
					<idno type="grant-number">LM2018101</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>We introduce new methods for evaluating and mitigating gender bias in machine translation. We believe that this solution will incentivize the development of fairer models in the future.</p><p>We do not identify any ethical risks connected to this work.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Byte pair encoding is suboptimal for language model pretraining</title>
		<author>
			<persName><forename type="first">Kaj</forename><surname>Bostrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03720</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Byte pair encoding is suboptimal for language model pretraining</title>
		<author>
			<persName><forename type="first">Kaj</forename><surname>Bostrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.414</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4617" to="4624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Finetuning neural machine translation on gender-balanced datasets</title>
		<author>
			<persName><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Jussà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrià</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Gender Bias in Natural Language Processing</title>
		<meeting>the Second Workshop on Gender Bias in Natural Language Processing<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="26" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">How much does tokenization affect neural machine translation?</title>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Domingo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mercedes</forename><surname>García-Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Helle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Casacuberta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Herranz</surname></persName>
		</author>
		<idno>ArXiv, abs/1812.08621</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Equalizing gender bias in neural machine translation with word embeddings techniques</title>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">Escudé</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Jussà</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-3821</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Gender Bias in Natural Language Processing</title>
		<meeting>the First Workshop on Gender Bias in Natural Language Processing<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="147" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Matteo Negri, and Marco Turchi. 2021. How to split: the effect of word segmentation on gender bias in speech translation</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gaido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Savoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.13782</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Autodebias: Debiasing masked language models with automated biased prompts</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Abbasi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.72</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1012" to="1023" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The social impact of natural language processing</title>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shannon</forename><forename type="middle">L</forename><surname>Spruit</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-2096</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="591" to="598" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A distribution-free k-sample test against ordered alternatives</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Jonckheere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="133" to="145" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Marian: Fast neural machine translation in C++</title>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Neckermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alham</forename><surname>Fikri Aji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bogoychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2018, System Demonstrations</title>
		<meeting>ACL 2018, System Demonstrations<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="116" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1007</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-2012</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Why don&apos;t people use character-level machine translation?</title>
		<author>
			<persName><forename type="first">Jindřich</forename><surname>Libovický</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-acl.194</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2470" to="2485" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Sabrina</forename><forename type="middle">J</forename><surname>Mielke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Salesky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Gallé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenglei</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2112.10508</idno>
		<title level="m">Benoît Sagot, and Samson Tan. 2021. Between words and characters: A brief history of open-vocabulary modeling and tokenization in nlp</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reducing gender bias in neural machine translation as a domain adaptation problem</title>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.690</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7724" to="7736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gender Bias in Machine Translation</title>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Savoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gaido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00401</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="845" to="874" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Evaluating gender bias in machine translation</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1164</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1679" to="1684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chau</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng-Jen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00401</idno>
		<title level="m">Multilingual translation with extensible multilingual pretraining and finetuning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">OPUS-MT -Building open translation services for the World</title>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santhosh</forename><surname>Thottingal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT)</title>
		<meeting>the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving massively multilingual neural machine translation and zero-shot translation</title>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.148</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1628" to="1639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution: Evaluation and debiasing methods</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2003</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="15" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology</title>
		<author>
			<persName><forename type="first">Ran</forename><surname>Zmigrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabrina</forename><forename type="middle">J</forename><surname>Mielke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1161</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1651" to="1661" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
