<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhancing Volatility Forecasting in Financial Markets: A General Numeral Attachment Dataset for Understanding Earnings Calls</title>
				<funder ref="#_Uz77H6Z">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_uAvMf6J">
					<orgName type="full">MOST</orgName>
				</funder>
				<funder ref="#_eqWyJ6U">
					<orgName type="full">JSPS KAKENHI</orgName>
				</funder>
				<funder ref="#_ghmcP5S">
					<orgName type="full">New Energy and Industrial Technology Development Organization</orgName>
					<orgName type="abbreviated">NEDO</orgName>
				</funder>
				<funder>
					<orgName type="full">National Science and Technology Council, Taiwan</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ming-Xuan</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chung-Chi</forename><surname>Chen</surname></persName>
							<email>c.c.chen@acm.org</email>
							<affiliation key="aff1">
								<orgName type="institution">AIST</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hen-Hsen</forename><surname>Huang</surname></persName>
							<email>hhhuang@iis.sinica.edu.tw</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
							<email>hhchen@ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Enhancing Volatility Forecasting in Financial Markets: A General Numeral Attachment Dataset for Understanding Earnings Calls</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F0E31ACD293FF9064B9B26665F390AA9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Volatility, a crucial statistical measure in the financial market, serves as an indicator of financial instrument risk. Accurate volatility capture aids in predicting stock movements and is valuable in derivative trading, such as options trading. While recent research focuses on volatility forecasting using earnings call transcriptions, most approaches rely on end-to-end models that directly process textual or vocal data. However, limited efforts have been made to simulate the reading and comprehension processes of financial professionals, thereby enhancing the capabilities of language models. To address this gap, we propose a general numeral attachment dataset designed to train language models to understand earnings calls with the expertise of professionals. Additionally, we introduce a pre-training process that improves the semantic understanding of earnings calls. Experimental results demonstrate that our pretrained language model enhances the accuracy of 3-day volatility forecasting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A key element in understanding financial narratives is pinpointing the specific target linked with every numeral, especially considering the prevalent use of numerals in these texts <ref type="bibr">(Chen et al., 2021a)</ref>. The notion of "numeral attachment" was first introduced by <ref type="bibr" target="#b3">Chen et al. (2019)</ref>. This concept seeks to clarify the connection between numerals and particular stocks. Yet, this methodology was specifically devised for a financial social media setting, making it less adaptable to more formal, general documentation. Addressing this gap, we put forth a broader numeral attachment framework, free from such specific constraints. Instead of developing an entirely new dataset, we amplify the existing EC-Num dataset <ref type="bibr">(Chen et al., 2021b)</ref> with augmented annotations. Consider the nuanced difference between "revenue decreased 20%" and "revenue decreased 2%" -two statements with varying impli-cations for investors. Furthermore, the distinction between "revenue decreased 20%" and "cost decreased 20%", where the numeral remains the same but the narrative diverges, exemplifies the essence of our endeavor. Our central goal is to refine the model's precision in recognizing and distinguishing situations inherent to general numeral attachment.</p><p>When making investment decisions, professional investors carefully consider financial risk. Estimating the risk associated with financial instruments is crucial due to the inherent trade-off between potential returns and risks involved. Volatility, which quantifies financial risk as the second-moment measure of price return, serves as a widely used indicator in this regard. Although previous studies have explored various models and data sources to improve volatility forecasting, they have largely neglected enhancing the semantic capabilities of language models for this task. In this paper, we introduce a novel approach that simulates the reading process of financial professionals, aiming to enhance the accuracy of volatility forecasting.</p><p>Earnings calls, which involve teleconferences among managers and investors to discuss company operations, have garnered significant attention in recent research. While prior studies have primarily focused on constructing multimodal models for earnings calls, they have often overlooked the finer semantic aspects. In this work, we demonstrate the value of understanding general numeral attachment in the context of earnings calls, particularly for improving volatility forecasting-an important downstream task in financial analysis.</p><p>Our contributions are threefold:</p><p>3. We propose an approach to enhance the language model's understanding of numerals, leading to improved performance in 3-day volatility forecasting.</p><p>2 Related Work  <ref type="bibr">et al., 2019)</ref> is fine-tuned with scientific publications. FinBERT <ref type="bibr" target="#b1">(Araci, 2019)</ref> is specifically pretrained with financial corpus, while BioBERT <ref type="bibr" target="#b9">(Lee et al., 2020)</ref>, mBERT <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref>, and patentBERT <ref type="bibr" target="#b8">(Lee and Hsiang, 2019)</ref> are fine-tuned with biomedical, multilingual, and patent corpora, respectively. Despite the availability of domain-specific BERT models, none of them have fine-tuned the BERT model with numeral-related tasks to enhance models' numeracy. In this paper, we propose a novel pretraining task focused on understanding the numerals in earnings calls at a more granular level. We release the associated dataset and pretrained language model, referred to as NumBERT, for future research and applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Annotation Process</head><p>We utilize the ECNum dataset <ref type="bibr">(Chen et al., 2021b)</ref>, which consists of 9,034 annotated instances from earnings calls. Each instance is associated with three annotations related to numeral information, including the target numeral category label and two domain-specific annotations. In this study, we extend the dataset by providing additional general numeral attachment labels for the ECNum instances.</p><p>Each instance was assigned to three annotators. We have regular meetings with all annotators to resolve the inconsistencies. Over a span of three weeks, they were diligently coached to adhere to the same guidelines. Regular discussions were held to clarify doubts and resolve ambiguities, culminating in a strong alignment in their annotations. During the annotation process, annotators with financial backgrounds were presented with a target numeral and a paragraph from an earnings call. Their task was to select the attached entity, such as a named entity or accounting account, that is relevant to the given numeral within the paragraph. The annotators exhibited high consistency in their annotations, with approximately 78.60% of instances receiving fully consistent annotations, 7.13% of instances having two different annotations, and only 14.26% of instances showing completely different annotations. After resolving inconsistencies and removing instances without attached entities, the dataset for the general numeral attachment task consists of 6,735 instances from ECNum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Annotations Analysis</head><p>Table <ref type="table" target="#tab_1">1</ref> presents the frequently attached entities observed in the annotations. To facilitate a comprehensive comparison of entity usage, particularly accounting accounts, in both managers' and investors' narratives, we include additional annotations from the NumClaim dataset <ref type="bibr" target="#b4">(Chen et al., 2020)</ref>, which comprises professional analysts' reports.</p><p>Based on the statistics depicted in Table <ref type="table" target="#tab_1">1</ref>, several key findings emerge. Firstly, managers predominantly report operational data, encompassing revenue, sales, EPS, and earnings. Secondly, investors exhibit interest not only in quantitative operational results such as revenue and EPS, but also in accounting ratios like gross margins and operating margins. Thirdly, while managers seldom mention the stock price, investors frequently engage in discussions regarding it, including price targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Research Questions</head><p>In this paper, we investigate the following two research questions:</p><p>(RQ1): How does pre-training with the proposed general numeral attachment task impact the perfor-mance of volatility forecasting? (RQ2): Which strategy, discrete or continuous, yields better results with the proposed model?</p><p>We start by fine-tuning the RoBERTa language model <ref type="bibr" target="#b10">(Liu et al., 2019)</ref> with the proposed general numeral attachment task, resulting in the Num-BERT model.</p><p>To address (RQ1), we employ NumBERT for the volatility forecasting task. Specifically, we train separate models for the 3-day, 7-day, 15-day, and 30-day volatility forecasting tasks.</p><p>To explore (RQ2), we develop both a discrete forecasting model and a continuous forecasting model based on the proposed GNA-Vol architecture. The discrete strategy involves simultaneous predictions for the 3-day, 7-day, 15-day, and 30-day volatility forecasting tasks, utilizing a multi-task model. On the other hand, the continuous strategy treats volatility forecasting as a sequential prediction task, making predictions for volatility from t + 1 to t + days. Here, we set T to 30. We consider the continuous strategy due to the well-known phenomenon of volatility clustering in the financial market, where large (small) volatility tends to be followed by large (small) volatility. We hypothesize that the continuous strategy can capture this pattern and further improve the performance of volatility forecasting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Architecture</head><p>In the general numeral attachment task, an input paragraph P ∈ {P 1 , P 2 , . . . , P M } is tokenized into words using the RoBERTa tokenizer: P = [x 1 , x 2 , . . . , x N ], where x i represents the i th word and N is the maximum number of words in any paragraph. To ensure the target numeral is not split by the tokenizer, we replace it with the [MASK] token. The hidden state of the i th word, denoted as h i , is obtained from the last layer of RoBERTa, and h mask represents the hidden state of the mask token. Start and end vectors S and E are defined.</p><p>To incorporate the target numeral information and generate the probability distribution of the start of the answer span, we concatenate h i and h mask to form a new vector c i . The probability p i of word i being the start of the answer span is computed as the softmax of the dot product between c i and S: p i = e S•c i j e S•c j . The fine-tuning loss function is cross-entropy. We perform fine-tuning for ten epochs using a learning rate of 1e-5 and a batch size of 4.  We employ the trained language model to encode earnings call transcriptions for volatility forecasting. We introduce a matrix T = [h 1 , h 2 , . . . , h N ], where h i represents the hidden state of the i th [MASK] token in the input transcription, and N is the maximum number of numerals in any transcription. To leverage multiple numeral information for volatility prediction, we construct a transformer-based model followed by a fully connected layer. The transformer model requires three inputs: Q, K, V , which are obtained by linear transformations of T : Q = T ×A and K = V = T ×B. In the case of continuous volatility forecasting, we replace the fully connected layer with an LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Language Model Selection</head><p>In this section, we discuss our choice of using RoBERTa as the base language model instead of the domain-specific language model, FinBERT <ref type="bibr" target="#b1">(Araci, 2019)</ref>. Our selection is based on the performance of the general numeral attachment task. We divided the instances into training, validation, and test sets, with proportions of 70%, 10%, and 20% respectively, for the general numeral attachment task. The evaluation metric for the span identification task is F-score. Table <ref type="table" target="#tab_3">2</ref> presents the performance of different language models on the general numeral attachment task. We observe that RoBERTa performs well in our proposed pre-training task. Additionally, we experiment with LinkBERT <ref type="bibr" target="#b13">(Yasunaga et al., 2022)</ref>, the latest well-performing pretrained language model that surpasses BERT in several benchmark datasets. However, RoBERTa outperforms LinkBERT in the general numeral attachment task. Therefore, we choose RoBERTa as the base language model for constructing NumBERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Results</head><p>For the volatility forecasting task, we utilize the same dataset and follow the separation criterion employed in previous works <ref type="bibr" target="#b11">(Qin and Yang, 2019;</ref><ref type="bibr" target="#b12">Yang et al., 2020;</ref><ref type="bibr">Chen et al., 2021b)</ref>. The mean square error (MSE) is employed as the evaluation metric. We compare the performance of our proposed approach with the following baseline models:</p><p>(1) MDRM <ref type="bibr" target="#b11">(Qin and Yang, 2019)</ref>: This model utilizes GloVe embeddings to represent textual data and feeds both textual and vocal features into a Bi-LSTM-based model for volatility forecasting. ( <ref type="formula">2</ref>) HTML <ref type="bibr" target="#b12">(Yang et al., 2020)</ref>: This model uses BERT to extract textual features and aligns sentence-level audio features with the textual data. Volatility forecasting is then performed based on the concatenated features. (3) FinBERT <ref type="bibr" target="#b1">(Araci, 2019)</ref>: This BERT-based model is pre-trained on a financial corpus and has demonstrated effectiveness in various financial tasks. ( <ref type="formula">4</ref>) NAM <ref type="bibr">(Chen et al., 2021b)</ref>: This model highlights the importance of extracting numeral features from earnings calls for volatility forecasting. It leverages the extracted numeral features with a Transformer architecture for making predictions.</p><p>Table <ref type="table" target="#tab_5">3</ref> presents the experimental results. Firstly, our proposed GNA-Vol achieves the top rank in the 3-day and 7-day volatility forecasting tasks, highlighting the effectiveness of our pretraining task and model. Given the dynamic nature of the financial market and the rapid incorporation of new information into asset prices, short-term forecasting is of utmost importance. These results demonstrate that our general numeral attachment task enables models to capture critical information in earnings conference calls for short-term risk forecasting. Secondly, despite the intuitive appeal of the continuous strategy for time series data prediction, we observe that the discrete strategy outperforms the continuous strategy in most cases. This suggests that the vanilla continuous strategy may not adequately capture the latent information underlying the volatility clustering phenomenon discussed in Section 4.1.</p><p>To assess the impact of our proposed pretraining process, we conduct an ablation analysis, the results of which are presented in Table <ref type="table" target="#tab_6">4</ref>. We observe that without pretraining using our proposed task, the performance significantly deteriorates in most cases, irrespective of the strategy employed. These results underscore the effectiveness of our pretraining task in risk forecasting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We address the challenge of identifying the target associated with numerals in financial narratives  through the novel task of general numeral attachment. By enhancing the publicly-available ECNum dataset with additional annotations, we provide a valuable resource for researchers in this field. Our approach, which simulates the reading process of financial professionals, enhances the semantic understanding of language models and improves the accuracy of 3-day volatility forecasting.</p><p>In the realms of tabular QA and math word problems, it's essential to note that earnings conference calls act as a nexus for corporate leaders and adept investors to converse about operational intricacies. Unlike traditional settings, these dialogues typically don't showcase tabular data or pose mathematical queries. This distinct context accentuates the importance of our general numeral attachment task. By deciphering numerical data embedded in such discourses, we augment the semantic grasp of these interactions. Future research can pivot on multiple avenues. Firstly, extrapolating the general numeral attachment task to diverse financial manuscripts and sectors could amplify its scope. Such extensions might delve into associating numerals with an array of financial elements, encompassing market indices, commodities, or macroeconomic markers. Secondly, the exploration of transfer learning or domain adaptation strategies to make our proposed model universally applicable to other financial prediction tasks stands as a promising endeavor. Finally, orchestrating user assessments or real-world appraisals of the improved volatility forecasting mechanisms can shed light on their tangible efficacy. Pursuing these trajectories will further crystallize the role of numeral</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Numeral attachment in different narratives.</figDesc><table><row><cell></cell><cell cols="2">Earnings call</cell><cell cols="2">Investor's report</cell></row><row><cell>Rank</cell><cell>Entity</cell><cell>Freq.</cell><cell>Entity</cell><cell>Freq.</cell></row><row><cell>1</cell><cell>revenue</cell><cell cols="2">767 revenue</cell><cell>855</cell></row><row><cell>2</cell><cell>Q</cell><cell>371 EPS</cell><cell></cell><cell>481</cell></row><row><cell>3</cell><cell>sales</cell><cell cols="2">255 gross margin</cell><cell>326</cell></row><row><cell>4</cell><cell>EPS</cell><cell cols="2">221 profit</cell><cell>275</cell></row><row><cell>5</cell><cell>earnings</cell><cell cols="2">165 operating margin</cell><cell>115</cell></row><row><cell>6</cell><cell>years</cell><cell cols="2">154 price target</cell><cell>108</cell></row><row><cell>7</cell><cell>free cash flow</cell><cell cols="2">110 operating profit</cell><cell>59</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Experimental results on general numeral attachment task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Experimental results of volatility forecasting task, reported in mean square error.</figDesc><table><row><cell></cell><cell cols="3">3-day 7-day 15-day 30-day</cell></row><row><cell>GNA-Vol + Discrete Strategy</cell><cell>0.700 0.322</cell><cell>0.252</cell><cell>0.207</cell></row><row><cell>w/o Pretrain</cell><cell>0.730 0.353</cell><cell>0.253</cell><cell>0.186</cell></row><row><cell cols="2">GNA-Vol + Continuous Strategy 0.705 0.362</cell><cell>0.250</cell><cell>0.237</cell></row><row><cell>w/o Pretrain</cell><cell>0.725 0.421</cell><cell>0.275</cell><cell>0.231</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation analysis. Note the lower the metric MSE, the better the performance.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We introduce the novel task of general numeral attachment, addressing a previously overlooked issue.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>2. We provide additional annotations on the publicly-available ECNum dataset. 11 http://gen-numattach.nlpfin.com/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research is supported by <rs type="funder">National Science and Technology Council, Taiwan</rs>, under grants <rs type="funder">MOST</rs> <rs type="grantNumber">110-2221-E-002-128-MY3</rs> and <rs type="grantNumber">NSTC 111-2634-F-002-023</rs>-. The work of <rs type="person">Chung-Chi Chen</rs> was supported in part by <rs type="funder">JSPS KAKENHI</rs> Grant Number <rs type="grantNumber">23K16956</rs> and a project <rs type="grantNumber">JPNP20006</rs>, commissioned by the <rs type="funder">New Energy and Industrial Technology Development Organization (NEDO)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_uAvMf6J">
					<idno type="grant-number">110-2221-E-002-128-MY3</idno>
				</org>
				<org type="funding" xml:id="_eqWyJ6U">
					<idno type="grant-number">NSTC 111-2634-F-002-023</idno>
				</org>
				<org type="funding" xml:id="_Uz77H6Z">
					<idno type="grant-number">23K16956</idno>
				</org>
				<org type="funding" xml:id="_ghmcP5S">
					<idno type="grant-number">JPNP20006</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>attachment in fine-tuning financial examinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>While this paper introduces a novel task of general numeral attachment and presents valuable contributions, there are some limitations to consider. First, the proposed general numeral attachment task is evaluated on the publicly-available ECNum dataset, which may have its own limitations and biases. The generalizability of the results to other financial documents and datasets needs to be further investigated. Second, the enhanced performance in 3-day volatility forecasting is demonstrated based on the specific approach proposed in this paper. It would be valuable to compare the performance with other existing volatility forecasting models and explore the robustness of the proposed approach across different datasets and market conditions. Third, the proposed approach focuses on enhancing the language model's understanding of numerals. While this is an important aspect, there may be other factors and features that contribute to accurate volatility forecasting. Future research could explore additional contextual information and features to further improve the forecasting accuracy.</p><p>Overall, while this paper provides valuable insights and advancements in the field of volatility forecasting through the general numeral attachment task, further research is needed to validate the findings on different datasets, compare with existing models, and explore additional features for enhanced performance.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Publicly available clinical BERT embeddings</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jindi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Mcdermott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Clinical Natural Language Processing Workshop</title>
		<meeting>the 2nd Clinical Natural Language Processing Workshop<address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Finbert: Financial sentiment analysis with pre-trained language models</title>
		<author>
			<persName><forename type="first">Dogu</forename><surname>Araci</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10063</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SciB-ERT: A pretrained language model for scientific text</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1371</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Numeral attachment with auxiliary tasks</title>
		<author>
			<persName><forename type="first">Chung-Chi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hen-Hsen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1161" to="1164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Numclaim: Investor&apos;s fine-grained claim detection</title>
		<author>
			<persName><forename type="first">Chung-Chi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hen-Hsen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1973" to="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">From Opinion Mining to Financial Argument Mining</title>
		<author>
			<persName><forename type="first">Chung-Chi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hen-Hsen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Springer Nature</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distilling numeral information for volatility forecasting</title>
		<author>
			<persName><forename type="first">Chung-Chi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hen-Hsen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Lieh</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3459637.3482089</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information amp; Knowledge Management<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2920" to="2924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Patentbert: Patent classification with fine-tuning a pre-trained bert model</title>
		<author>
			<persName><forename type="first">Jieh-Sheng</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieh</forename><surname>Hsiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02124</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">So</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">What you say and how you say it matters: Predicting stock volatility using verbal and vocal cues</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1038</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="390" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Html: Hierarchical transformerbased multi-task learning for volatility prediction</title>
		<author>
			<persName><forename type="first">Linyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tin</forename><surname>Lok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riuhai</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020, WWW &apos;20</title>
		<meeting>The Web Conference 2020, WWW &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="441" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">LinkBERT: Pretraining language models with document links</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8003" to="8016" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
