<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Emerging Challenges in Personalized Medicine: Assessing Demographic Effects on Biomedical Question Answering Systems</title>
				<funder ref="#_J3h8Ypw #_Vq4xyvK">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sagi</forename><surname>Shaier</surname></persName>
							<email>sagi.shaier@colorado.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Colorado Boulder</orgName>
								<orgName type="institution" key="instit2">University of Colorado Denver</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Bennett</surname></persName>
							<email>kevbennett@mhs.net</email>
						</author>
						<author>
							<persName><forename type="first">Lawrence</forename><surname>Hunter</surname></persName>
							<email>larry.hunter@cuanschutz.edu</email>
						</author>
						<author>
							<persName><forename type="first">Katharina</forename><surname>Von Der Wense</surname></persName>
							<email>katharina.kann@colorado.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Colorado Boulder</orgName>
								<orgName type="institution" key="instit2">University of Colorado Denver</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Johannes Gutenberg University Mainz</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Emerging Challenges in Personalized Medicine: Assessing Demographic Effects on Biomedical Question Answering Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5745985A05377E93D84812D3026E91A5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art question answering (QA) models exhibit a variety of social biases (e.g., with respect to sex or race), generally explained by similar issues in their training data. However, what has been overlooked so far is that in the critical domain of biomedicine, any unjustified change in model output due to patient demographics is problematic: it results in the unfair treatment of patients. Selecting only questions on biomedical topics whose answers do not depend on ethnicity, sex, or sexual orientation, we ask the following research questions: (RQ1) Do the answers of QA models change when being provided with irrelevant demographic information? (RQ2) Does the answer of RQ1 differ between knowledge graph (KG)-grounded and text-based QA systems? We find that irrelevant demographic information change up to 15% of the answers of a KG-grounded system and up to 23% of the answers of a text-based system, including changes that affect accuracy. We conclude that unjustified answer changes caused by patient demographics are a frequent phenomenon, which raises fairness concerns and should be paid more attention to. Code and data can be found here: https://github.com/ Shaier/personalized_medicine_ challenges.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language processing (NLP) has long been used in health care and life sciences. However, NLP systems exhibit surprising behaviors that can be difficult to predict or control: problems with general-purpose NLP systems reflecting stereotyping and stigmatizing biases have been apparent since the Microsoft Taybot debacle in 2016 and remain a major issue to this day <ref type="bibr">(Nadeem et al</ref> You should take the Paxlovid medication.</p><p>Figure <ref type="figure">1</ref>: An undesired behavior from a biomedical QA system: the model changes its answers when provided with different biomedically irrelevant information (e.g., names).</p><p>2021; <ref type="bibr" target="#b18">Rudinger et al., 2017;</ref><ref type="bibr" target="#b1">Blodgett et al., 2020;</ref><ref type="bibr" target="#b19">Savoldi et al., 2021;</ref><ref type="bibr" target="#b28">Zarrieß et al., 2022)</ref>.</p><p>The World Health Organization states that social determinants of health, including the experience of racism, sexism, and other forms of discrimination, "can be more important than health care or lifestyle choices in influencing health." ♠ Thus, for biomedical NLP systems it is of particular importance to not be affected by factors irrelevant to biology and medicine, and for researchers to ensure they serve their users fairly irrespective of irrelevant attributes, such as names, as shown in Figure <ref type="figure">1</ref>. Here, we test the effect irrelevant demographic information has on biomedical question answering (QA) systems.</p><p>As a test-bed, we choose a subset of questions from the US Medical Licensing Exam level 1 (USMLE1; <ref type="bibr" target="#b7">Jin et al., 2021)</ref> whose answers, according to two medical professionals, are independent of the patient's demographics. Although the questions are multiple-choice, correct answers require broad medical knowledge, including diagnosis and treatment of all common diseases, as well ♠ https://www.who.int/health-topics/social-determinantsof-health#tab=tab_1 as an understanding of the underlying molecular and physiological mechanisms, potential drug side effects, probabilistic reasoning, and more.</p><p>We add irrelevant demographic information in a controlled way to the USMLE questions in order to answer the following research questions: (RQ1) Do the models' answers change when being provided with irrelevant demographic information? (RQ2) Is the answer to RQ1 different for knowledge graph (KG)-grounded and text-based QA systems? We experiment with two biomedical QA systems: BioLinkBERT <ref type="bibr" target="#b26">(Yasunaga et al., 2022)</ref>, a text-based model, and QAGNN <ref type="bibr" target="#b27">(Yasunaga et al., 2021)</ref>, which is the highest performing KG-based model on USMLE.</p><p>There are good reasons to believe that neither system should be affected by irrelevant patient information: both are trained solely on biomedical text, which is most often independent of irrelevant demographic information, and QAGNN is additionally grounded by a KG that does not contain any demographic representations. Unfortunately, we find that both systems change many of their answers when provided with irrelevant patient demographic information. We also observe that the two systems differ in which demographic information affects them. Finally, we compare biomedical to generic systems (i.e., trained on generic English text) and find that, as expected, the generic system changes even more of its answers in most cases (up to 17% for gender). However, for some demographics, such as sexual orientation, the biomedical system changes up to 23% of its answers. We hope that shedding light on this problematic behavior will motivate future work to further investigate its impact as well as possible solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Medical QA Many medical QA datasets are drawn from a variety of medical settings. MLEC-QA <ref type="bibr" target="#b10">(Li et al., 2021)</ref> for example, is based on the National Medical Licensing Examination in China, while emrQA <ref type="bibr" target="#b17">(Pampari et al., 2018)</ref> is based on clinical notes. HEAD-QA <ref type="bibr" target="#b25">(Vilares and Gómez-Rodríguez, 2019)</ref> uses exams from the Spanish healthcare system, and MedQuAD (Abacha and Demner-Fushman, 2019) is based on 12 NIH websites and has questions on drugs, diseases, and other medical entities. DiSCQ <ref type="bibr" target="#b9">(Lehman et al., 2022)</ref> has questions from MIMIC-III discharge summaries that were generated by medical experts, and the Q-Pain dataset <ref type="bibr" target="#b12">(Logé et al., 2021)</ref> focuses on pain management. MedQA <ref type="bibr" target="#b7">(Jin et al., 2021)</ref> has questions from the professional medical board exams and covers three languages. Recent datasets focus on specific challenges identified from previous efforts <ref type="bibr" target="#b15">(Niu et al., 2003;</ref><ref type="bibr" target="#b8">Kell et al., 2021)</ref>. We selected English language questions from MedQA for this study, based on the breadth and depth of medical knowledge required and the fact that students must pass an exam with similar questions to become a physician in the US.</p><p>Biases in NLP Models Social biases have been reported in widely divergent NLP training sets and models, ranging from gender bias in machine translation <ref type="bibr" target="#b3">(Cho et al., 2019)</ref> to racial bias in opioid misuse prediction <ref type="bibr" target="#b23">(Thompson et al., 2021)</ref>. Social biases in dialog systems were examined through the use of demographically indicative names <ref type="bibr" target="#b21">(Smith and Williams, 2021)</ref>. Several studies of natural language generation systems, transformers, and related models have shown outputs influenced by a variety of demographic characteristics in prompts, e.g., <ref type="bibr" target="#b20">(Sheng et al., 2019)</ref>. Methods to measure stereotype bias in language models (LMs) have been proposed, such as StereoSet <ref type="bibr" target="#b13">(Nadeem et al., 2021)</ref> and the CrowS-Pairs dataset <ref type="bibr" target="#b14">(Nangia et al., 2020)</ref> which contains information on nine types of demographics, such as age and race.</p><p>Bias in Medical NLP Some also focus particularly on evaluating biases in the medical domain. One work <ref type="bibr" target="#b2">(Borgese et al., 2021)</ref> analyzes unhealthy alcohol use risk bias between classifiers on electronic health records in trauma patients, while another <ref type="bibr" target="#b12">(Logé et al., 2021)</ref> examines gender and ethnicity biases in a pain management setting between GPT-2 and GPT-3. However, getting unbiased data to investigate model bias in a controlled way is difficult for pain management, where there is extreme societal bias. Hence, we use other data in our study. Racial biases in clinical settings were also examined <ref type="bibr" target="#b6">(Huang et al., 2022)</ref>. Some also focus on using NLP to evaluate whether medical licensure exams contain language patterns that exhibit biased or stereotypical language <ref type="bibr" target="#b16">(Padhee et al., 2021)</ref>.</p><p>Lastly, there is also work on evaluating pretrained transformer models and examining whether they contain biased information towards different demographics <ref type="bibr" target="#b29">(Zhang et al., 2020)</ref>. In contrast to prior work, we 1) examine the effect irrelevant demographic information has on QA systems in a clinical setting, using questions which require broad medical knowledge and are used by US medical students; 2) focus on models that are trained on biomedical text; 3) compare the effect of KG grounding on biases in a transformer-based model; and 4) compare biomedically-trained systems to a generic one, trained on English text.</p><p>3 Experimental Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>Biomedical QA systems can be beneficial for both healthcare providers and patients for many reasons: 1) With traditional search engines, finding reliable medical information can take time and effort due to the vast amount of unfiltered content available online, while QA systems allow users to quickly find answers; 2) such systems can serve as powerful learning tools for students and residents seeking to deepen their understanding of complex medical topics; 3) in low-resource settings there may be limited access to qualified healthcare professionals, which leads to delayed or incorrect diagnoses that may worsen health outcomes over time. Fortunately, biomedical QA systems can bridge this gap and extend the reach of health services to vulnerable populations worldwide. However, in order for such systems to be safely deployed, ensuring that they provide fair behavior towards patients is critical. For example, imagine that a White and an African-American patient present themselves with similar symptoms at a hospital and that none of their symptoms indicate a problem related to their ethnicity. If one was treated with the correct medication while the other received an incorrect one, this would be highly problem-atic. Thus, it is important to understand if current biomedical QA systems could result in such an outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MedQA-USMLE</head><p>The MedQA-USMLE dataset <ref type="bibr" target="#b7">(Jin et al., 2021</ref>) is an open-domain QA dataset, which covers three languages: English, traditional Chinese, and simplified Chinese. MedQA has medical questions which represent real-world scenarios and evaluate physicians on their clinical decision making skills. The questions are varied and require a significant understanding of medical concepts. Here, we choose to only use the English version, which is composed of 12,723 multiple-choice prompts taken from the professional medical board exams. Each prompt consists of context and question, e.g., "An 18-year-old male presents to the emergency room smelling quite heavily of alcohol and is unconscious. A blood test reveals severe hypoglycemic and ketoacidemia. A previous medical history states that he does not have diabetes. The metabolism of ethanol in this patient's hepatocytes resulted in an increase of the [NADH]/[NAD+] ratio. Which of the following reaction is favored under this condition?". Each question comes with four answer choices. The options for the above example are: Pyruvate to acetyl-CoA, Citrate to isocitrate, Oxaloacetate to malate, and Oxaloacetate to phosphoenolpyruvate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Question Selection</head><p>Some phenomena are more prevalent in certain populations, such as pregnancy <ref type="bibr" target="#b22">(Sogancioglu et al., 2022)</ref>   <ref type="table" target="#tab_6">6 6 8 7 7 11 6 6 14 6 7 5 7 6 8 8 9 8 8 9 9 23 8 13 23</ref> Table <ref type="table">2</ref>: Percentage of questions with changed answers as compared to a question with no demographic information about the patient. M=male; F=female; W=White; B=Black; A-A=African-American; H=Hispanic; As=Asian; SOr=sexual orientation; Random=Random change as described in Section 3.5.</p><formula xml:id="formula_0">Gender Ethnicity SOr M F W A-A B H As Hetero Bi Homo Correct → Incorrect QAGNN 1 3 4 4 4 3 3 4 3 6 BioLinkBert 1 2 2 3 3 3 6 2 1 5 Incorrect→Incorrect QAGNN 2 1 2 2 3 3 3 3 3 6 BioLinkBert 4 2 2 4 2 2 3 2 3 6 Incorrect→Correct QAGNN 3 3 0 3 0 0 0 2 1 3 BioLinkBert 1 2 2 1 2 2 2 2 2 3</formula><p>Table <ref type="table">3</ref>: Percentage of answers that changed from from correct to incorrect, incorrect to incorrect, and incorrect to correct for each model. M=male; F=female; W=White; B=Black; A-A=African-American; H=Hispanic; As=Asian; SOr=sexual orientation.</p><p>should accordingly not be taken into account. For our experiments we build a dataset consisting of only questions whose answers do not depend on sex, ethnicity, or sexual orientation. We do so by following <ref type="bibr" target="#b12">Logé et al. (2021)</ref>'s approach and extract 100 vignettes, which are designed to allow for the inclusion of diverse ethnics and gender "profiles" in order to assess potential biases. These vignettes are verified by two medical experts to be demographics-independent, and after the demographics-enhancing process, which will be discussed in the next section, result in 16,700 questions overall, which are used to evaluate the effect irrelevant demographic information has on QA systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Demographics-enhanced Dataset Creation</head><p>We experiment with the following types of modified questions: dimensionless (i.e., no demographic information), ethnicity, gender, names, sexual orientation, gender+ethnicity, gender+sexual orientation, and gender+ethnicity+names. The reasoning for each chosen dimension are as follows: dimensionless shows no demographic information, and hence will be used as a baseline to compare how many of the answers change when we add irrelevant demographic information. Ethnicity, sexual orientation, and gender, while not always shown in medical text, are sometimes mentioned when the demographic information is relevant. Hence, we want to see if the models associate any medical conditions with them. We use two genders, but expect that our results will generalize to additional genders. As for names, these are clearly not medically relevant ever and are rarely shown in medical text. Hence, we choose them to see if there are unexpected differences in answers change.</p><p>Ethnicities include White, Black, African-American, Hispanic, and Asian. Genders include male and female. Names include the 10 names for each ethnicity from the Q-Pain dataset, which originated from the Harvard Dataverse's Demographic aspects of first names dataset <ref type="bibr" target="#b24">(Tzioumis, 2018)</ref>. And while "Black" and "African American" are largely synonymous, we want to see if they are different from the models' perspective. Notably, to medically-untrained users, all of these may seem relevant and hence potentially be added to queries when such users request medical assistance.</p><p>We follow a similar process as the creators of the Q-Pain dataset and make each context, question, and answer (CQA) as neutral as possible. Given a CQA, such as "A 23-year-old female presents to a psychiatrist...", we first automatically mask any word that indicates gender (e.g., male, female, he, she, wife, boyfriend): "A 23-year-old [GENDER_MASK] presents to a psychiatrist...". Then, given a dimension (e.g., gender), we automatically replace each unique masking with their corresponding token replacement (e.g., replacing "[GENDER_MASK]" with "male").</p><p>Overall, each of these dimensions and their variations augment each of the 100 vignettes and result in overall 16,700 questions. See Table <ref type="table" target="#tab_1">1</ref> for examples. And while we only use the English version of the dataset, this process can be easily applied to other languages. The data will be publicly available and have an MIT License.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Random Change</head><p>We use a version of the questions with no demographic information, and, in each prompt's first sentence, replace the word "patient" with "person". With this we examine the effect of a small but insignificant textual variation on each model. We choose this change over others (e.g., adding random words, irrelevant demographics, or fictitious cities) as this reduces the possibility of models changing their answers due to the context such random words had in the training data (e.g., Africa is more prevalent to the sleeping sickness disease than the US). Moreover, neither "person" nor "patient" reveal information about the human.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Models</head><p>We compare two existing algorithms: QAGNN <ref type="bibr" target="#b27">(Yasunaga et al., 2021)</ref> and BioLinkBert <ref type="bibr" target="#b26">(Yasunaga et al., 2022)</ref>. While better models exist for the USMLE dataset, many of them have billions of parameters and we are unable to test them for computational reasons. That being said, BioLinkBert is currently among the state of the art on the USMLE dataset, and QAGNN is the top (and, to the best of our knowledge, only) KG-grounded model. We use existing implementations and models and describe both systems in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">QAGNN</head><p>The main component of QAGNN is its KG, which is based on the Disease Database portion of the Unified Medical Language System (UMLS) and DrugBank. The graph contains about 10k nodes and 44k edges, where the embeddings for each node are initialized using the biomedically trained language model SapBERT <ref type="bibr" target="#b11">(Liu et al., 2020)</ref>. Sap-BERT was trained using the UMLS vocabulary set 2020AA version, which contains biomedical synonyms from more than 150 controlled vocabularies, such as Gene Ontology and MeSH. QAGNN has 360M parameters.</p><p>For each answer choice of a given question, QAGNN first retrieves a subgraph from its KG using entity linking. That is, it finds entity mentions in the question and retrieves any entity in the main KG that appears in any 2-hop paths between pairs of found entities. Then, it concatenates the answer choice and question, followed by encoding using a LM. Next, it connects the encoded representation to the graph as a node. It then performs relevance scoring on each node in the created subgraph by concatenating it to the encoded representation node and calculating the likelihood using a LM. Lastly, using an attention-based graph neural network (GNN) module, it reasons over the graph to get a score for the answer choice. During the training procedure, it optimizes both the LM and its GNN end-to-end using cross-entropy loss. On the MedQA-USMLE dataset, SapBERT-based QAGNN achieves 38% accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">BioLinkBert</head><p>The defining features of BioLinkBert are its pretraining method that incorporates document links and its LM which has similar hyperparameters to PubmedBERT <ref type="bibr" target="#b5">(Gu et al., 2020)</ref> and is trained from</p><formula xml:id="formula_1">Names Gender+Ethnicity+Names W A-A/B H As M+W M+A-A M+B M+H M+As F+W F+A-A F+B F+H F+As</formula><p>QAGNN 10.5 10.5 12.6 10.5 9.3 14.2 11.5 9.8 8.5 9.3 15.0 11.5 12.5 7.9 BioLinkBERT 7.4 6.0 8.5 6.0 8.8 11.9 9.8 8.1 9.6 8.5 11.5 10.3 10.0 9.0  <ref type="bibr">39.4 38.5 38.6 37.0 35.1 38.7 36.8 37.2 37.3 35.4</ref> Table <ref type="table">6</ref>: Accuracy when including names (rows 1 and 2) or names together with gender and ethnicity information (rows 3 and 4) for each model. W=White; B=Black; A-A=African-American; H=Hispanic; As=Asian; scratch on the PubMed abstracts PubmedBERT is trained on. BioLinkBert has 340M parameters.</p><p>Given a corpus of text, BioLinkBert views it as a graph: it uses Pubmed Parser to extract citation links between documents and views the hyperlinks as edges. Then, to use the links in its LM pretraining procedure it places two documents which share a link in the same context, in addition to placing two random documents in the same context or a single document (contiguous). Next, it uses two selfsupervised objectives. The first, masked language modeling, is common in many of the large LMs such as BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>. In the second, document relation prediction, it classifies the link between the two documents as random, linked, or contiguous. On the MedQA-USMLE dataset, the base version of BioLinkBert achieves 40% accuracy while the large version achieves 44.6%. Here, we work with the base version because of its lower compute requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We look at two different effects of providing the model with irrelevant demographic information: 1) the percentage of questions for each model that change and 2) the accuracy change for each model. Note that these are not necessarily correlated: for example, accuracy does not change when initially incorrect answers change to other incorrect answers, or if the same numbers of answers change from incorrect to correct. It is also worth mentioning that any change in model's answers is problematic, as these questions were verified to be independent of demographics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Changed Answers</head><p>Table <ref type="table">2</ref> shows the percentage of questions for each model that change between each dimension's attribute and the dimensionless variation (e.g., between male and genderless).</p><p>The first column of Table <ref type="table">2</ref>, "Random", shows the result of our random change (Sec. 3.5). While the other values in the table are larger, and while the words "patient" and "person" may have different connotations for each model based on its training data, this suggests that, to some extent, random noise plays a role in the amount of change each model exhibits. Notably for gender, ethnicity, and sexual orientation, both models change around the same number of answers, except that BioLinkBert has a much higher number for Asian. Additionally, both models have almost double the amount of changed answers for homosexual than bisexual or heterosexual. For gender+ethnicity, QAGNN has an equivalent amount or more than BioLinkBert, though for gender+sexual orientation, BioLinkBert has more than double the amount for homosexuals, with a massive percentage of 23. We also examine the amount of answers for each model for gender, ethnicity, and sexual orientation, that change from  being correct to incorrect, from incorrect to correct, and from incorrect to incorrect (Table <ref type="table">3</ref>). We can see that a model can have an increase in performance (see QAGNN males column which results in a 2% increase) while having the same number of answers change as a demographics which result in a decrease in performance (see QAGNN White column which result in a 4% decrease). This implies that accuracy alone is not sufficient to understand the effect irrelevant demographic information has on models' answer, and that further examination of the answers can contribute. For example, we see that adding most ethnicities results in 0 answers changing from incorrect to correct for QAGNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Changed Accuracy</head><p>While the reported accuracy on the original test dataset is 38% for QAGNN and 40% for Bi-oLinkBert, the accuracy on our 100 randomly selected demographic-independent questions use to construct the vignettes is 40% for QAGNN and 39% for BioLinkBert. Table <ref type="table" target="#tab_3">4</ref> shows our accuracy results for each dimension for each algorithm.</p><p>As noted, accuracy change does not always correlate with answer change. For example from Table <ref type="table">2</ref>, while both models have about the same number of changed answers for gender, only QAGNN's accuracy for males is affected (increased by 2%). For ethnicity, both models' accuracy drops, with BioLinkBert's accuracy by 3% for Asian and QAGNN's accuracy by 4% for Black. Sexual orientation improves BioLinkBert performance on bisexual and decreases QAGNN's on every variation. Gender+ethnicity decreases QAGNN performance the most (up to 6%), while gender+sexual orientation improves BioLinkBert's performance on any variation except for homosexual.</p><p>6 Analysis: Names Similarly to the above experiments, we also evaluate the effect names have on the two types of models. For names by themselves, for each ethnicity (Black, White, Hispanic, Asian) we use the corresponding 20 names (10 for males and 10 for females). For names+ethnicity+gender, we split the names into their ethnicity and gender.</p><p>Table <ref type="table" target="#tab_4">5</ref> and 6 show our results: Tables 5 displays the number of changed answers, while Table <ref type="table">6</ref> shows accuracy changes. We can see that names alone have a moderate effect on the performance of both models, decreasing the performance in any variation by up to 1.65%. From our baseline experiment this may be due to random noise. However, by looking at the number of changed answers, we can see that both models have the most change for Hispanics, with QAGNN change of up to 12.6% and BioLinkBert by up to 8.5%. Interestingly, QAGNN has the same number of changed answers for White, Black, and Asian, but a different number for Hispanic. More results can be seen in the combination of gender, ethnicity, and names, in which the performance can decrease by up to 3.9% for BioLinkBert in African American males, and by up to 2.1% for QAGNN in Asian females. However, the amount of changed answers is up to 15% in QAGNN for African American females and up to 11.9% for BioLinkBert in African American males. This implies that even though both models were trained on PubMed data, irrelevant information like names affect them, which is highly problematic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Medical vs. Generic LMs</head><p>In addition to our main results, we also compare how the performance of a biomedically-trained transformer differs from that of a generic one. In particular, we use the same code for the Bi-oLinkBert QA system, but instead of using the medically-trained base (trained from scratch on PubMed abstracts), we use a transformer which is trained on generic English text.</p><p>Similar to our analysis between QAGNN and Bi-oLinkBert above, our analysis between the biomedical and generic models can be split into the amount of answers and accuracy that changes when the dimensions change. From Table <ref type="table" target="#tab_5">7</ref> it is visible that the generic transformer has more than double the amount of answers change for each gender. It also has an equivalent amount or more for almost any ethnicity, except for Asians. Notably, for sexual orientation, the generic transformer has almost double the amount of answers change for bisexuals, while the biomedical transformer has more for homosexuals. The generic transformer has significantly larger values than the biomedical transformer in any gender+ethnicity combination, while for gender+sexual orientation, the biomedical system has significantly larger values for homosexuals. From Table <ref type="table" target="#tab_6">8</ref> it is clear that BioLinkBert significantly outperforms its generic LM variation. From the change in accuracy we can see that, while the biomedical transformer's accuracy increases when gender is removed ("no info"), the generic transformer's accuracy decreases. We can also see that the biomedical transformer's accuracy changes more for ethnicity and sexual orientation, while the generic model changes more for gender.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Future Work</head><p>Finally, we discuss three potential approaches to alleviate the aforementioned effects, including model architectures, data, and regularization.</p><p>Model Architecture While both the KGgrounded LM and the text-based one are susceptible to irrelevant demographic information, our initial assumption that the KG-based LM would be less susceptible still holds. In particular, KGs are a condensed representation of knowledge, which rarely holds such irrelevant information. Hence, models that use such representations have a significant potential to be less affected. That being said, a potential reason that the tested KG-based LM is still susceptible may be due to the fact that it grounds the text using the KG, and does not only uses the KG. Hence, the demographically irrelevant information may still leak into the final representation, which the model uses to answer the question.</p><p>Data Generally, large LMs are trained using a massive corpus. This is problematic as it is almost impossible to ensure that every piece of data is demographically independent. To try to alleviate this issue, we select biomedical models that are trained only on biomedical data, which often does not contain demographically irrelevant information. However, we still find that these models are susceptible to such information. Hence, future work should examine methods to reduce such issues in the training data, especially for models intended to critical settings.</p><p>Regularization While developing models with different architectures or ensuring that every piece of data is demographically independent is time consuming, a potentially simple method to alleviate such problem is to regularize the input itself. For example, by masking demographically-significant words. And while relatively simple to implement (e.g., using keywords search), in medicine it is sometimes the case where such demographicallysignificant words are in fact significant. Hence, simple masking might reduce bias, but will also reduce performance. Future work should examine potential masking approaches that consider times where such words are actually needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We examine the effect of irrelevant demographic information on purely text-based and KG-grounded biomedical QA systems as well as a generic QA system, using a subset of the USMLE questions whose answers do not depend on the patient's demographics. Our results show that irrelevant demographic information results in changed answers for all systems. We also find that, while all systems are affected by irrelevant demographic information, they differ with regards to how different types of demographic information influence them. These results provide evidence that more work is needed in order to ensure fair treatment of all patients by biomedical QA systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>As expert annotation is expensive, we only use 100 unique vignettes to create the 16, 700 questions. However, this is almost twice as many as other published datasets, such as <ref type="bibr" target="#b12">Logé et al. (2021)</ref>. Additionally, we only analyze one KG-grounded and one purely text-based system. While our main point, that there are problems one should be aware of, can be made based on experiments with two models, evaluating more systems can potentially lead to more fine-grained insights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>The main reason for this paper is to point out potential problems regarding fair treatment of all patients by biomedical QA systems. Future work should improve existing biomedical QA systems to ensure equal and just patient care. Moreover, such systems can be problematic for both patients and health experts. For example, a patient could follow the recommendations of such a QA model at home without expert supervision and a system could recommend an incorrect treatment because of their name, or physicians could use such systems to improve their quality of care, but the system could cloud their judgment and direct them to an incorrect answer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Black patient presents to a psychiatrist for evaluation of situational anxiety. The patient reports that they recently started a new job and is very stressed. She reports that she recently started a new job and is very stressed. Dimensions example. Given a question, for each dimension, we demographically-enhance the question by adding relevant words (e.g., Black, bisexual, named X) and changing its gender tokens in order to create multiple datasets for the specific dimension. SOr=sexual orientation.</figDesc><table><row><cell>Dimensionless</cell><cell>A 23-year-old patient presents to a psychiatrist for evaluation of situational anxiety. The patient reports that they recently started a new job and is very stressed.</cell></row><row><cell cols="2">Ethnicity A 23-year-old Gender A 23-year-old female presents to a psychiatrist for evaluation of situational anxiety. She reports that she recently started a new job and is very stressed.</cell></row><row><cell>Names</cell><cell>A 23-year-old patient named Tom presents to a psychiatrist for evaluation of situational anxiety. The patient reports that they recently started a new job and is very stressed.</cell></row><row><cell>SOr</cell><cell>A 23-year-old bisexual patient presents to a psychiatrist for evaluation of situational anxiety. The patient reports that they recently started a new job and is very stressed.</cell></row><row><cell>SOr+Gender</cell><cell>A 23-year-old bisexual female presents to a psychiatrist for evaluation of situational anxiety. She reports that she recently started a new job and is very stressed.</cell></row><row><cell>Ethnicity+Gender</cell><cell>A 23-year-old Asian male presents to a psychiatrist for evaluation of situational anxiety. He reports that he recently started a new job and is very stressed.</cell></row><row><cell>Ethnicity+Gender</cell><cell>A 23-year-old Hispanic female named Guadalupe presents to a psychiatrist for evaluation of</cell></row><row><cell>+Names</cell><cell>situational anxiety.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>or prostate cancer. For other diagnoses, patient demographic information is irrelevant and</figDesc><table><row><cell></cell><cell cols="6">Random Gender Ethnicity</cell><cell></cell><cell cols="2">SOr</cell><cell></cell><cell></cell><cell cols="7">Gender+Ethnicity</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Gender+SOr</cell></row><row><cell></cell><cell>M</cell><cell>F</cell><cell>W</cell><cell>A-A</cell><cell>B</cell><cell>H</cell><cell>As</cell><cell>Hetero</cell><cell>Bi</cell><cell>Homo</cell><cell>M+W</cell><cell>M+A-A</cell><cell>M+B</cell><cell>M+H</cell><cell>M+As</cell><cell>F+W</cell><cell>F+A-A</cell><cell>F+B</cell><cell>F+H</cell><cell>F+As</cell><cell>M+Hetero</cell><cell>M+Bi</cell><cell>M+Homo</cell><cell>F+Hetero</cell><cell>F+Bi</cell><cell>F+Homo</cell></row><row><cell>QAGNN</cell><cell>2 6</cell><cell cols="25">7 6 9 7 6 6 9 7 15 8 8 9 10 8 9 10 9 9 9 8 6 11 10 8 9</cell></row><row><cell>BioLinkBert</cell><cell>2 6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>38 40 40 42 40 36 39 36 37 37 38 38 37 38 38 36 35 37 35 34 35 35 35 36 39 38 36 36 37 2 40 39 40 40 40 40 38 39 39 36 40 41 38 39 37 41 40 40 40 36 40 41 40 41 41 36 41 40 36 Accuracy (in percentages) of the two models on our demographically enhanced datasets.</figDesc><table><row><cell cols="2">O* O D Gen.</cell><cell></cell><cell cols="3">Ethnicity</cell><cell></cell><cell cols="2">SOr</cell><cell></cell><cell></cell><cell></cell><cell cols="6">Gender+Ethnicity</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Gender+SOr</cell><cell></cell></row><row><cell>M</cell><cell>F</cell><cell>W</cell><cell>A-A</cell><cell>B</cell><cell>H</cell><cell>As</cell><cell>Hetero</cell><cell>Bi</cell><cell>Homo</cell><cell>M+W</cell><cell>M+A-A</cell><cell>M+B</cell><cell>M+H</cell><cell>M+As</cell><cell>F+W</cell><cell>F+A-A</cell><cell>F+B</cell><cell>F+H</cell><cell>F+As</cell><cell>M+Hetero</cell><cell>M+Bi</cell><cell>M+Homo</cell><cell>F+Hetero</cell><cell>F+Bi</cell><cell>F+Homo</cell></row><row><cell cols="26">1 M=male;</cell></row><row><cell cols="26">F=female; W=White; B=Black; A-A=African-American; H=Hispanic; As=Asian; SOr=sexual orientation;</cell></row><row><cell cols="26">O*=original test dataset; O=the original, unmodified 100 vignettes; D=No demographic information; Gen=Gender;</cell></row><row><cell cols="3">1=QAGNN; 2=BioLinkBERT.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Percentage of questions with changed answers as compared to a question with no demographic information about the patient. M=male; F=female; W=White; B=Black; A-A=African-American; H=Hispanic; As=Asian; SOr=sexual orientation.</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Names</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>W</cell><cell></cell><cell>B</cell><cell></cell><cell>A-A</cell><cell></cell><cell>H</cell><cell></cell><cell>AS</cell></row><row><cell>QAGNN</cell><cell>38.6</cell><cell></cell><cell>39.5</cell><cell></cell><cell>39.5</cell><cell></cell><cell>39.3</cell><cell></cell><cell>38.5</cell></row><row><cell>BioLinkBert</cell><cell>38.2</cell><cell></cell><cell>37.3</cell><cell></cell><cell>37.3</cell><cell></cell><cell>37.6</cell><cell></cell><cell>37.6</cell></row><row><cell></cell><cell>M</cell><cell>F</cell><cell>M</cell><cell>F</cell><cell>M</cell><cell>F</cell><cell>M</cell><cell>F</cell><cell>M</cell><cell>F</cell></row><row><cell>QAGNN</cell><cell cols="10">38.6 38.1 39.5 39.4 39.7 39.1 39.0 39.2 39.2 37.9</cell></row><row><cell>BioLinkBert</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Percentage of questions with changed answers between the biomedical and generic model as compared to a question with no demographic information about the patient. M=male; F=female; W=White; B=Black; A-A=African-American; H=Hispanic; As=Asian; SOr=sexual orientation.</figDesc><table><row><cell cols="10">Random Gender Ethnicity</cell><cell cols="2">SOr</cell><cell></cell><cell></cell><cell></cell><cell cols="7">Gender+Ethnicity</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Gender+SOr</cell></row><row><cell></cell><cell>M</cell><cell></cell><cell>F</cell><cell>W</cell><cell>A-A</cell><cell cols="2">B</cell><cell>H</cell><cell>As</cell><cell>Hetero</cell><cell>Bi</cell><cell>Homo</cell><cell>M+W</cell><cell>M+A-A</cell><cell>M+B</cell><cell>M+H</cell><cell>M+As</cell><cell>F+W</cell><cell>F+A-A</cell><cell cols="2">F+B</cell><cell cols="2">F+H</cell><cell cols="2">F+As</cell><cell>M+Hetero</cell><cell>M+Bi</cell><cell>M+Homo</cell><cell>F+Hetero</cell><cell>F+Bi</cell><cell>F+Homo</cell></row><row><cell>Generic</cell><cell cols="26">2 17 16 6 14 7 9 7 9 11 11 11 11 12 13 8 13 13 13 12 12 8 10 6 12 15 11</cell></row><row><cell>Biomedical</cell><cell>2 6</cell><cell></cell><cell cols="24">6 6 8 7 7 11 6 6 14 6 7 5 7 6 8 8 9 8 8 9 9 23 8 13 23</cell></row><row><cell cols="2">O* O D Gender</cell><cell></cell><cell cols="4">Ethnicity</cell><cell></cell><cell></cell><cell cols="2">SOr</cell><cell></cell><cell></cell><cell></cell><cell cols="6">Gender+Ethnicity</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Gender+SOr</cell></row><row><cell>M</cell><cell>F</cell><cell>W</cell><cell>A-A</cell><cell>B</cell><cell cols="2">H</cell><cell cols="2">As</cell><cell>Hetero</cell><cell>Bi</cell><cell>Homo</cell><cell>M+W</cell><cell>M+A-A</cell><cell>M+B</cell><cell>M+H</cell><cell>M+As</cell><cell>F+W</cell><cell>F+A-A</cell><cell cols="2">F+B</cell><cell cols="2">F+H</cell><cell cols="2">F+As</cell><cell cols="2">M+Hetero</cell><cell>M+Bi</cell><cell>M+Homo</cell><cell>F+Hetero</cell><cell>F+Bi</cell><cell>F+Homo</cell></row><row><cell cols="27">1 28.9 26 25 29 27 27 26 26 27 27 28 27 26 27 27 27 26 25 27 27 27 26 24 27 27 28 25 26 25</cell></row><row><cell cols="27">2 40 39 40 40 40 40 38 39 39 36 38 38 37 39 37 41 40 40 40 36 40 41 40 41 41 36 41 40 36</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Accuracy (in percentages) of the biomedical and generic models on our demographically enhanced datasets.</figDesc><table /><note><p>M=male; F=female; W=White; B=Black; A-A=African-American; H=Hispanic; As=Asian; SOr=sexual orientation; O*=original test dataset; O=the original, unmodified 100 questions; D=No demographic information; 1=Generic; 2=Biomedical.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Dr. Peter Pressman</rs> for his help with reviewing the data and the reviewers for their feedback. The authors acknowledge financial support from <rs type="funder">NIH</rs> grants <rs type="grantNumber">OT2TR003422</rs> and <rs type="grantNumber">R01LM013400</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_J3h8Ypw">
					<idno type="grant-number">OT2TR003422</idno>
				</org>
				<org type="funding" xml:id="_Vq4xyvK">
					<idno type="grant-number">R01LM013400</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A question-entailment approach to question answering</title>
		<author>
			<persName><forename type="first">Asma</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abacha</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12859-019-3119-4</idno>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language (technology) is power: A critical survey of &quot;bias&quot; in NLP</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Solon</forename><surname>Blodgett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.485</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5454" to="5476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bias assessment and correction in machine learning algorithms: A use-case in a natural language processing algorithm to identify hospitalized patients with unhealthy alcohol use</title>
		<author>
			<persName><forename type="first">Marissa</forename><surname>Borgese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cara</forename><surname>Joyce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">E</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">M</forename><surname>Churpek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Majid</forename><surname>Afshar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AMIA Annu. Symp. Proc</title>
		<imprint>
			<biblScope unit="page" from="247" to="254" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On measuring gender bias in translation of gender-neutral pronouns</title>
		<author>
			<persName><forename type="first">Ik</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><forename type="middle">Won</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><surname>Soo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-3824</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Gender Bias in Natural Language Processing</title>
		<meeting>the First Workshop on Gender Bias in Natural Language Processing<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="173" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evaluation and mitigation of racial bias in clinical machine learning models: Scoping review</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galal</forename><surname>Galal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mozziyar</forename><surname>Etemadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahesh</forename><surname>Vaidyanathan</surname></persName>
		</author>
		<idno type="DOI">10.2196/36388</idno>
	</analytic>
	<monogr>
		<title level="j">JMIR Med Inform</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">36388</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What disease does this patient have? a large-scale open domain question answering dataset from medical exams</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eileen</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nassim</forename><surname>Oufattole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
		<idno type="DOI">10.3390/app11146421</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What would it take to get biomedical QA systems into practice?</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Kell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Jaun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.mrqa-1.3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Machine Reading for Question Answering</title>
		<meeting>the 3rd Workshop on Machine Reading for Question Answering<address><addrLine>Punta Cana</addrLine></address></meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="28" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to ask like a physician</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladislav</forename><surname>Lialin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katelyn</forename><forename type="middle">Edelwina</forename><surname>Legaspi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><forename type="middle">Janelle</forename><surname>Sy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><forename type="middle">Therese</forename><surname>Pile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><forename type="middle">Rose</forename><surname>Alberto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Raymund</forename><surname>Ragasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Victoria Puyat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianne</forename><forename type="middle">Katharina</forename><surname>Taliño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><forename type="middle">Rose</forename><surname>Alberto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pia</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Gabrielle</forename><surname>Alfonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Moukheiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preethi</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Anthony Celi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Clinical Natural Language Processing Workshop</title>
		<meeting>the 4th Clinical Natural Language Processing Workshop</meeting>
		<imprint>
			<publisher>WA. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="74" to="86" />
		</imprint>
	</monogr>
	<note>Seattle</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MLEC-QA: A Chinese Multi-Choice Biomedical Question Answering Dataset</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangping</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaizhi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.698</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8862" to="8874" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Self-alignment pretraining for biomedical entity representations</title>
		<author>
			<persName><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Shareghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaiqiao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Basaldella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2010.11784</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Q-pain: A question answering dataset to measure social bias in pain management</title>
		<author>
			<persName><forename type="first">Cécile</forename><surname>Logé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amoah</forename><surname>Dadey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saahil</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriel</forename><surname>Saporta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2108.01764</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">StereoSet: Measuring stereotypical bias in pretrained language models</title>
		<author>
			<persName><forename type="first">Moin</forename><surname>Nadeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Bethke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.416</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5356" to="5371" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CrowS-pairs: A challenge dataset for measuring social biases in masked language models</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasika</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.154</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1953" to="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Answering clinical questions with role identification</title>
		<author>
			<persName><forename type="first">Yun</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Mcarthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Rodriguez-Gianolli</surname></persName>
		</author>
		<idno type="DOI">10.3115/1118958.1118968</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2003 Workshop on Natural Language Processing in Biomedicine</title>
		<meeting>the ACL 2003 Workshop on Natural Language Processing in Biomedicine<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Exploring language patterns in a medical licensure exam item bank</title>
		<author>
			<persName><forename type="first">Swati</forename><surname>Padhee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimberly</forename><surname>Swygert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Micir</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2111.10501</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">emrQA: A large corpus for question answering on electronic medical records</title>
		<author>
			<persName><forename type="first">Anusri</forename><surname>Pampari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preethi</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1258</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2357" to="2368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Social bias in elicited natural language inferences</title>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandler</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-1609</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</title>
		<meeting>the First ACL Workshop on Ethics in Natural Language Processing<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="74" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gender bias in machine translation</title>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Savoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gaido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00401</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="845" to="874" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The woman worked as a babysitter: On biases in language generation</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Premkumar</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1339</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3407" to="3412" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smith</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2109.03300</idno>
		<title level="m">Hi, my name is martha: Using names to measure and mitigate bias in generative dialogue models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Gizem</forename><surname>Sogancioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Mijsters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jelle</forename><surname>Amar Van Uden</surname></persName>
		</author>
		<author>
			<persName><surname>Peperzak</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2208.01341</idno>
		<title level="m">Bias in (non)-contextual clinical word embeddings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bias and fairness assessment of a natural language processing opioid misuse classifier: detection and mitigation of electronic health record data disadvantages across racial subgroups</title>
		<author>
			<persName><forename type="first">Brihat</forename><surname>Hale M Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randy</forename><surname>Bhalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Boley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Mccluskey</surname></persName>
		</author>
		<author>
			<persName><surname>Dligach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niranjan</forename><forename type="middle">S</forename><surname>Matthew M Churpek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Majid</forename><surname>Karnik</surname></persName>
		</author>
		<author>
			<persName><surname>Afshar</surname></persName>
		</author>
		<idno type="DOI">10.1093/jamia/ocab148</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2393" to="2403" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Data for: Demographic aspects of first names</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Tzioumis</surname></persName>
		</author>
		<idno type="DOI">10.7910/DVN/TYJKEZ</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">HEAD-QA: A healthcare dataset for complex reasoning</title>
		<author>
			<persName><forename type="first">David</forename><surname>Vilares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Gómez-Rodríguez</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1092</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="960" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">LinkBERT: Pretraining language models with document links</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.551</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8003" to="8016" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">QA-GNN: Reasoning with language models and knowledge graphs for question answering</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.45</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="535" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">This isn&apos;t the bias you&apos;re looking for: Implicit causality, names and gender in German language models</title>
		<author>
			<persName><forename type="first">Sina</forename><surname>Zarrieß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannes</forename><surname>Groener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torgrim</forename><surname>Solstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Bott</surname></persName>
		</author>
		<idno>KON- VENS 2022 Organizers</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Conference on Natural Language Processing (KONVENS 2022)</title>
		<meeting>the 18th Conference on Natural Language Processing (KONVENS 2022)<address><addrLine>Potsdam, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="129" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Hurtful words: Quantifying biases in clinical contextual word embeddings</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><forename type="middle">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Abdalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzyeh</forename><surname>Ghassemi</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2003.11515</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
