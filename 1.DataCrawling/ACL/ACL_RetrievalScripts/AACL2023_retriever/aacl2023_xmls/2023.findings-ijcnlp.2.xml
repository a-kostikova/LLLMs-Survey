<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Named Entity Recognition via Machine Reading Comprehension: A Multi-Task Learning Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yibo</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenting</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yao</forename><surname>Wan</surname></persName>
							<email>wanyao@hust.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhongfen</forename><surname>Deng</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
							<email>psyu@uic.edu</email>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Named Entity Recognition via Machine Reading Comprehension: A Multi-Task Learning Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D27573D034224A231371663CFE57C808</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Named Entity Recognition (NER) aims to extract and classify entity mentions in the text into pre-defined types (e.g., organization or person name). Recently, many works have been proposed to shape the NER as a machine reading comprehension problem (also termed MRC-based NER), in which entity recognition is achieved by answering the formulated questions related to pre-defined entity types through MRC, based on the contexts. However, these works ignore the label dependencies among entity types, which are critical for precisely recognizing named entities. In this paper, we propose to incorporate the label dependencies among entity types into a multi-task learning framework for better MRC-based NER. We decompose MRC-based NER into multiple tasks and use a self-attention module to capture label dependencies. Comprehensive experiments on both nested NER and flat NER datasets are conducted to validate the effectiveness of the proposed Multi-NER. Experimental results show that Multi-NER can achieve better performance on all datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named Entity Recognition (NER), which aims to locate and classify entity mentions in text into predefined types, is a fundamental task in information extraction <ref type="bibr" target="#b0">(Chinchor and Robinson, 1997;</ref><ref type="bibr" target="#b12">Nadeau and Sekine, 2007)</ref>. Typically, NER is formulated as a sequence labeling task, where each token is classified as one of the pre-defined types. However, the sequence labeling models can only assign one label to a token, resulting in the incapability of handling overlapping entities in nested NER (Finkel The Secretary of Homeland Security was in attendance. and <ref type="bibr" target="#b5">Manning, 2009)</ref>. Figure <ref type="figure" target="#fig_0">1</ref> shows an example of nested NER: Homeland Security can be recognized as ORGANIZATION , as well as PERSON.</p><p>To mitigate this issue, many works resort to formulating NER as a Machine Reading Comprehension (MRC) question answering task (termed MRC-based NER) <ref type="bibr" target="#b20">(Wang et al., 2020;</ref><ref type="bibr" target="#b8">Li et al., 2020;</ref><ref type="bibr" target="#b21">Wang et al., 2022)</ref>. For example, to recognize the ORGANIZATION, a natural-language question "Which ORGANIZATION is mentioned in the text?" is formulated. Then the goal of NER is transformed to answer the formulated questions through machine reading comprehension, given the contexts. MRC-based NER provides a unified solution for both flat and nested NER tasks since each entity type has its corresponding entity span positions as the answer, and these output answers are independent of each other.</p><p>Despite much progress having been made in MRC-based NER, existing approaches tend to ignore the label dependencies among entity types, which are critical for precise NER. Label dependencies indicate that different entities in the text have some relations with each other. For example, in a sentence "Ousted WeWork founder Adam Neumann lists his Manhattan penthouse for $37.5 million", once Adam Neumann is recognized as PERSON, it is expected to help with the recognition </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>… … …</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output Layer 1</head><p>Output Layer N of WeWork as ORGANIZATION because the founder preceding a person's name implies an organization.</p><p>To leverage the label dependencies among entity types, we propose a novel multi-task learning framework (termed Multi-NER) for MRC-based NER. In Multi-NER, MRC-based NER is decomposed into multiple tasks, each task focusing on one entity type. For each task, the corresponding input is the concatenation of an entity-class-related question and the context, and the output is expected to be the corresponding entity spans (i.e., start and end positions). The input is first encoded via a pre-trained BERT <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref>. The concatenation of embeddings of all tasks are fed into a self-attention module, which can preserve the label dependencies between different entity types. Finally, task-specific output layers are applied to different tasks.</p><p>To validate the effectiveness of our proposed Multi-NER, we conduct experiments on the datasets of both flat NER and nested NER. Experimental results show that Multi-NER can benefit the MRC-based NER in both flat NER and nested NER, with the important label dependencies among entities preserved. Additionally, we also visualize the self-attention maps to examine whether the label dependencies have been successfully captured.</p><p>Overall, the contributions of this paper are twofold: 1) We are the first to propose a multi-task learning framework for MRC-based NER tasks to capture label dependencies between entity types; 2) The introduced self-attention maps are visualized to verify that the self-attention modules can capture label dependencies.</p><p>All the source code and datasets are available at https://github.com/YiboWANG214/MultiNER 2 Methodology</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation</head><p>Given a sequence X = {x 1 , x 2 , . . . , x n }, where n denotes length of X, NER aims to find every entity mention in X and assign an entity type y ∈ Y to it. BERT-MRC <ref type="bibr" target="#b8">(Li et al., 2020)</ref> transforms tagging-style NER to MRC format with a triplet of (QUESTION, ANSWER, CONTEXT). The natural language question q (y) = {q (y) 1 , . . . , q (y) ly }, where l y denotes length of q (y) , is related to the entity type y and considered as QUESTION; The positions P y start,end of entity mentions of y is considered as ANSWER; The input sequence X is considered as CONTEXT. Given X and q (y) , the goal of BERT-MRC is to predict P y start,end . Our Multi-NER applies the same MRC format but further decomposes BERT-MRC into multiple tasks, where each task i ∈ {1, 2, . . . , |Y|} only focuses on one entity type. Thus, the task set {1, 2, . . . , |Y|} and the entity type set Y are bijection. Instead of processing one QUESTION at a time, Multi-NER processes all QUESTIONs in a multi-task framework. Therefore, in Multi-NER settings, given a CONTEXT X and multiple questions {q (y) } y∈Y , the goal is to predict {P y start,end } y∈Y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-NER</head><p>Figure <ref type="figure">2</ref> gives an overview of our proposed Multi-NER, which consists of |Y| tasks, where each task denotes the recognition of one specific entity type.</p><p>To share information between tasks, one shared encoder is used across tasks, and a self-attention module is employed to capture label dependencies.</p><p>Single Task Learning For every single task i ∈ {1, 2, . . . , |Y|}, the input sequence is the concatenation of the natural language question q (i) and the context X, as follows:</p><formula xml:id="formula_0">I (i) = [CLS], q (i) , [SEP], X .<label>(1)</label></formula><p>The output of task i has three components: start index prediction, end index prediction, and span matrix prediction. The start index prediction is the probability of each token being a start position; the end index prediction is the probability of each token being an end position; the span matrix prediction is the probability of each start-end pair being an entity mention position.</p><p>Task Interactions Task interactions are twofold. First, one shared large language model like BERT <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref> is used as the encoder for all tasks to make the embedding space consistent. Thus, the embedding of task i is</p><formula xml:id="formula_1">E (i) = Encoder(I (i) )</formula><p>. Second, a self-attention module <ref type="bibr" target="#b17">(Vaswani et al., 2017)</ref> is used across all tasks, accepting the concatenation of the embeddings of every task as input. The self-attention module incorporates information from every task and outputs the concatenation of hidden states |Y|) .</p><formula xml:id="formula_2">E = AT T E (1) , . . . , E<label>(</label></formula><p>(2)</p><p>The self-attention module enables E with the property of capturing the label dependencies between entity types, making E a better representation.</p><p>Model Learning At training time, all tasks are trained jointly. The loss function for multi-task learning is defined as follows:</p><formula xml:id="formula_3">L = |Y| i=1 (αL i start + βL i end + γL i span ) ,<label>(3)</label></formula><p>where α, β, and γ are tunable weights for start index prediction, end index prediction, and span matrix prediction. L i start , L i end and L i span are cross entropy loss of task i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>To evaluate the performance of the proposed Multiwe compare it with a state-of-the-art baseline BERT-MRC <ref type="bibr" target="#b8">(Li et al., 2020)</ref>, on datasets of flat NER and nested NER. We also perform a case study with attention maps visualized to further analyze the ability of Multi-NER to capture label dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We adopt three nested NER datasets (i.e., English <ref type="bibr">ACE-2004</ref><ref type="bibr" target="#b11">(Mitchell et al., 2005)</ref>, English ACE-2005 <ref type="bibr" target="#b18">(Walker et al., 2006)</ref> and GENIA <ref type="bibr" target="#b13">(Ohta et al., 2002)</ref>) and one flat NER dataset (i.e., English <ref type="bibr">CoNLL-2003 (Tjong Kim Sang and</ref><ref type="bibr" target="#b16">De Meulder, 2003)</ref>) to evaluate the performance of Multi-NER. <ref type="bibr">ACE-2004 and</ref><ref type="bibr">ACE-2005</ref> are two textual datasets from broadcast, newswire, telephone conversations and weblogs. GENIA is a collection of biomedical literature, containing Medline abstracts. CoNLL-2003 is extracted from Reuters news stories between August 1996 and August 1997. We conducted experiments on both nested NER datasets and flat NER dataset since our model is based on BERT-MRC, which can be applied to both flat and nested NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Settings</head><p>For a fair comparison, we select BERT base as the backbone encoder for all models. We adopt a onelayer linear transformation to predict the start index and end index and adopt a two-layer MLP with activation function GELU to predict the span matrix. We set the hidden size as 1,536 and the dropout rate as 0.1. More details of the hyperparameters setting are referred to the Appendix A.1. We follow the same process of question generation in <ref type="bibr" target="#b8">(Li et al., 2020)</ref> to use annotation guideline notes, which are the guidelines provided to the annotators when building datasets, as references to construct questions. Furthermore, the idea behind our proposed multitask framework is to use different output layers to disambiguate between entity types and use a self-attention module to obtain label dependencies between entity types. To evaluate the contribution of the different output layers and the selfattention module, we also conduct ablation studies on all datasets. The experimental results in Table <ref type="table" target="#tab_4">2</ref> show that both different output layers and the selfattention module contribute to Multi-NER.   ACE 2004, we show the ground-truth entities and predicted results of BERT-MRC and Multi-NER in Figure <ref type="figure" target="#fig_1">3</ref>. In BERT-MRC, etc is categorized as both ORG and PER, while in Multi-NER and ground truth etc is categorized as PER. Ambiguous tokens like etc are hard to categorize even with contextual information. However, when applying Multi-NER, different entity types and label dependencies are also considered, which is beneficial to ambiguous tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results and Analysis</head><p>We also show the attention map of the mean scores according to entity types of this example in Figure <ref type="figure">4</ref>. We can see that PER has a relatively large impact on other entity types, helping the model improve performance on other entity types with PER information. We attribute it to label dependencies obtained by information sharing between entity types using the self-attention module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>As language models have advanced <ref type="bibr" target="#b1">(Devlin et al., 2019;</ref><ref type="bibr" target="#b14">Raffel et al., 2020;</ref><ref type="bibr" target="#b25">Zhao et al., 2023;</ref><ref type="bibr" target="#b2">Dong et al., 2023;</ref><ref type="bibr" target="#b26">Zhao et al., 2021;</ref><ref type="bibr" target="#b10">Liu et al., 2021)</ref>, numerous efforts have emerged to enhance the performance of MRC-based NER. <ref type="bibr">Zhang et al. (2022)</ref> incorporated different domain knowledge into MRCbased NER task to improve model generalization ability. <ref type="bibr" target="#b9">Liu et al. (2022)</ref> proposed to use graph attention networks to capture label dependencies between entity types when applying MRC-based NER to electronic medical records. However, they only use entity type embeddings to build graph attention networks, ignoring the rich information in context. MRC-based NER is applied to different domains. <ref type="bibr" target="#b3">Du et al. (2022)</ref> designed an MRC-based method for medical NER through both sequence labeling and span boundary detection. <ref type="bibr">Zhang and Zhang (2022)</ref> applied MRC-based NER for financial named entity recognition from literature. <ref type="bibr" target="#b20">Wang et al. (2020)</ref> proposed MRC-based NER with the help of a distilled masked language model in ecommerce. <ref type="bibr" target="#b7">Jia et al. (2022)</ref> applied MRC-based methods for multimodal named entity recognition.</p><p>The span-based methods (Eberts and Ulges, 2020) that formulate nested NER as a span classification task are also mentionable. <ref type="bibr" target="#b19">Wan et al. (2022)</ref> improved span representation using retrieval-based span-level graphs based on n-gram similarity. <ref type="bibr" target="#b22">Yuan et al. (2022)</ref> integrated heterogeneous factors like inside tokens, boundaries, labels, and related spans to improve the performance of span representation and classification. <ref type="bibr" target="#b15">Shen et al. (2021)</ref> improved span-based NER using a two-stage entity identifier to filter out low-quality spans to reduce computational costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose to incorporate the label dependencies among entity types into a novel multitask learning framework (termed Multi-NER) for MRC-based NER. A self-attention mechanism is introduced to obtain label dependencies between entity types. Experimental results validate that Multi-NER outperforms BERT-MRC on both nested NER and flat NER. Case study and attention map visualization show that our introduced self-attention module is able to capture label dependencies among entities, contributing to a performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>One limitation of our proposed Multi-NER lies in that the number of tasks depends on the number of entity types because each entity type is considered as a task. Depending on the model structure we use in Multi-NER, the number of parameters will increase by 4M for each additional entity type with M ax_Length = 128. One possible solution to solve this problem is using parameter-efficient fine-tuning methods like Hypernetworks <ref type="bibr" target="#b6">(Ha et al., 2016)</ref> to effectively generate task-specific output layers. We leave this problem to our future work.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>PERORGFigure 1 :</head><label>1</label><figDesc>Figure 1: An example of nested NER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: The ground truth, predictions of BERT-MRC and Multi-NER for a randomly selected example. The middle of the sentence is omitted for better presentation. The complete example is referred to Appendix A.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The ground truth, results of BERT-MRC and Multi-NER of a randomly selected example. Delete lines and underlines represent false positive and bolded tokens represent false negative. Punctuation errors are not indicated in the figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>shows the experimental results on both</cell></row><row><cell>nested NER datasets and flat NER dataset. From</cell></row><row><cell>this table, we can observe that our Multi-NER</cell></row><row><cell>achieves 85.34% on ACE 2004, 84.25% on ACE</cell></row><row><cell>2005, 81.13% on GENIA, and 92.33% on CoNLL-</cell></row><row><cell>2003, achieving +1.3%, +0.4%, +1.24% and 1.25%</cell></row><row><cell>improvement, respectively, when comparing with</cell></row><row><cell>BERT-MRC. The performance improvements of</cell></row><row><cell>Multi-NER on all the datasets indicate that formu-</cell></row><row><cell>lating MRC-based NER into a multi-task learning</cell></row><row><cell>framework to obtain label dependencies between</cell></row><row><cell>different entity types can indeed bring model per-</cell></row><row><cell>formance improvement.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Experimental results on nested NER datasets and flat NER dataset.</figDesc><table><row><cell>Model</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell></cell><cell cols="2">ACE 2004</cell><cell></cell></row><row><cell>BERT-MRC</cell><cell>84.63</cell><cell>83.46</cell><cell>84.04</cell></row><row><cell>Multi-NER</cell><cell>86.01</cell><cell cols="2">84.68 85.34 (+1.3)</cell></row><row><cell></cell><cell cols="2">ACE 2005</cell><cell></cell></row><row><cell>BERT-MRC</cell><cell>83.22</cell><cell>84.48</cell><cell>83.85</cell></row><row><cell>Multi-NER</cell><cell>84.47</cell><cell cols="2">84.03 84.25 (+0.4)</cell></row><row><cell></cell><cell cols="2">GENIA</cell><cell></cell></row><row><cell>BERT-MRC</cell><cell>79.47</cell><cell>80.32</cell><cell>79.89</cell></row><row><cell>Multi-NER</cell><cell>82.63</cell><cell cols="2">79.68 81.13 (+1.24)</cell></row><row><cell></cell><cell cols="2">CoNLL-2003</cell><cell></cell></row><row><cell>BERT-MRC</cell><cell>90.61</cell><cell>91.55</cell><cell>91.08</cell></row><row><cell>Multi-NER</cell><cell>91.96</cell><cell cols="2">92.70 92.33 (+1.25)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies evaluate the contribution of components of Multi-NER.</figDesc><table><row><cell>Model</cell><cell cols="2">ACE04 ACE05 GENIA CoNLL03</cell></row><row><cell cols="2">Multi-NER 85.34 84.25 81.13</cell><cell>92.33</cell></row><row><cell>(w/o att)</cell><cell>84.84 84.01 80.87</cell><cell>91.32</cell></row><row><cell>(w/o diff)</cell><cell>84.72 84.05 80.58</cell><cell>91.55</cell></row><row><cell cols="2">3.4 Case Study</cell><cell></cell></row><row><cell cols="3">To further study the effect of Multi-NER, we ex-</cell></row><row><cell cols="3">amine some randomly selected examples. As an</cell></row><row><cell cols="3">example, for "Ahbulaity Ahbudurecy , chairman of</cell></row><row><cell cols="3">the Xinjiang Uigur Autonomous Region , presided</cell></row><row><cell cols="3">at the opening ceremony , and leaders such as</cell></row><row><cell cols="3">Tiemuer Dawamaiti , vice -chairman of the Stand-</cell></row><row><cell cols="3">ing Committee of the National People ' s Congress ,</cell></row><row><cell cols="3">Shimayi Aimaiti , member of the State Affairs Com-</cell></row><row><cell cols="3">mittee , etc . cut the ribbon for the meeting ." from</cell></row><row><cell cols="3">BERT-MRC: [ Ahbulaity Ahbudurecy ]PER , [ chairman of [ the</cell></row><row><cell cols="3">Xinjiang Uigur Autonomous Region ]GPE ]PER , presided at the</cell></row><row><cell cols="3">opening ceremony , and [ leaders such as …… , [ [etc]ORG ]PER</cell></row><row><cell cols="2">]PER. cut the ribbon for the meeting .</cell><cell></cell></row><row><cell cols="3">Multi-NER: [ Ahbulaity Ahbudurecy ]PER , [ chairman of [ the</cell></row><row><cell cols="3">Xinjiang Uigur Autonomous Region ]GPE ]PER , presided at the</cell></row><row><cell cols="3">opening ceremony , and [ leaders such as …… , etc ]PER. cut</cell></row><row><cell cols="2">the ribbon for the meeting .</cell><cell></cell></row><row><cell cols="3">Ground Truth: [ Ahbulaity Ahbudurecy ]PER , [ chairman of [</cell></row><row><cell cols="3">the Xinjiang Uigur Autonomous Region ]GPE ]PER , presided at</cell></row><row><cell cols="3">the opening ceremony , and [ leaders such as …… , etc . ]PER</cell></row><row><cell cols="2">cut the ribbon for the meeting .</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Hyperparameters for all models.</figDesc><table><row><cell></cell><cell></cell><cell>ACE 2004</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="5">Batch Size Max Length Learning Rate Epoch # Parameters</cell></row><row><cell>BERT-MRC</cell><cell>2</cell><cell>128</cell><cell>3e-5</cell><cell>14</cell><cell>112M</cell></row><row><cell>Multi-NER</cell><cell>2</cell><cell>128</cell><cell>2e-5</cell><cell>19</cell><cell>133M</cell></row><row><cell></cell><cell></cell><cell>ACE 2005</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="5">Batch Size Max Length Learning Rate Epoch # Parameters</cell></row><row><cell>BERT-MRC</cell><cell>2</cell><cell>128</cell><cell>2e-5</cell><cell>11</cell><cell>112M</cell></row><row><cell>Multi-NER</cell><cell>2</cell><cell>128</cell><cell>2e-5</cell><cell>16</cell><cell>133M</cell></row><row><cell></cell><cell></cell><cell>GENIA</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="5">Batch Size Max Length Learning Rate Epoch # Parameters</cell></row><row><cell>BERT-MRC</cell><cell>2</cell><cell>180</cell><cell>2e-5</cell><cell>9</cell><cell>112M</cell></row><row><cell>Multi-NER</cell><cell>2</cell><cell>128</cell><cell>2e-5</cell><cell>15</cell><cell>125M</cell></row><row><cell></cell><cell></cell><cell>CoNLL-2003</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="5">Batch Size Max Length Learning Rate Epoch # Parameters</cell></row><row><cell>BERT-MRC</cell><cell>2</cell><cell>200</cell><cell>3e-5</cell><cell>8</cell><cell>112M</cell></row><row><cell>Multi-NER</cell><cell>1</cell><cell>200</cell><cell>2e-5</cell><cell>18</cell><cell>123M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Results of an example "Ahbulaity Ahbudurecy , chairman of the Xinjiang Uigur Autonomous Region , presided at the opening ceremony , and leaders such as Tiemuer Dawamaiti , vice -chairman of the Standing Committee of the National People ' s Congress , Shimayi Aimaiti , member of the State Affairs Committee , etc . cut the ribbon for the meeting ." from ACE 2004. [ Ahbulaity Ahbudurecy ]PER , [ chairman of [ the Xinjiang Uigur Autonomous Region ]GPE ]PER , presided at the opening ceremony , and [ leaders such as [ Tiemuer Dawamaiti ]PER , [ vice -chairman of [ the Standing Committee of [ the National People ' s Congress ]ORG ]ORG ]PER , [ Shimayi Aimaiti ]PER , member of [ the State Affairs Committee ]ORG , [ [etc]ORG ]PER ]PER. cut the ribbon for the meeting . Multi-NER: [ Ahbulaity Ahbudurecy ]PER , [ chairman of [ the Xinjiang Uigur Autonomous Region ]GPE ]PER , presided at the opening ceremony , and [ leaders such as [ Tiemuer Dawamaiti ]PER , [ vice -chairman of [ the Standing Committee of [ the National People ' s Congress ]ORG ]ORG ]PER , [ [ Shimayi Aimaiti ]PER , [ member of [ the State Affairs Committee ]ORG ]PER ]PER , etc ]PER. cut the ribbon for the meeting . Ground Truth: [ Ahbulaity Ahbudurecy ]PER , [ chairman of [ the Xinjiang Uigur Autonomous Region ]GPE ]PER , presided at the opening ceremony , and [ leaders such as [ Tiemuer Dawamaiti ]PER , [ vice -chairman of [ the Standing Committee of the National People ' s Congress ]ORG ]PER , [ Shimayi Aimaiti ]PER , [ [ member of [ the State Affairs Committee ]PER ]ORG ]PER , etc . ]PER cut the ribbon for the meeting .</figDesc><table><row><cell>entity type</cell><cell>BERT-MRC</cell><cell>Multi-NER</cell><cell>Ground Truth</cell></row><row><cell>GPE</cell><cell>(5,9)</cell><cell>(5,9)</cell><cell>(5,9)</cell></row><row><cell>ORG</cell><cell>(28,37) (32,37) (44,47) (49,49)</cell><cell>(28,37) (32,37) (44,47)</cell><cell>(28,37) (44,47)</cell></row><row><cell></cell><cell>(0,1) (3,9) (18,49)</cell><cell>(0,1) (3,9) (18,49)</cell><cell>(0,1) (3,9) (18,50)</cell></row><row><cell>PER</cell><cell>(21,22) (24,37)</cell><cell>(21,22) (24,37)</cell><cell>(21,22) (24,37)</cell></row><row><cell></cell><cell>(39,40) (49,49)</cell><cell>(39,40) (39,47) (42,47)</cell><cell>(39,40) (42,47) (49,50)</cell></row><row><cell>FAC</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VEH</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LOC</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>WEA</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BERT-MRC:</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Hyperparameters</head><p>The hyperparameter details are shown in Table <ref type="table">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Examples</head><p>The ground truth, predicted results of Multi-NER and BERT-MRC of a randomly selected example are shown in Table <ref type="table">4</ref> and<ref type="table">Figure 5</ref>. From the table, we can observe that the true positive, false positive, and false negative positions of BERT-MRC and Multi-NER are 8, 4, 3, and 9, 3, 2. Besides, most errors of Multi-NER are due to the ambiguity of the entity mention boundaries like punctuation.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Muc-7 named entity task definition</title>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Chinchor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Conference on Message Understanding</title>
		<meeting>the 7th Conference on Message Understanding</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Closed-book question generation via contrastive learning</title>
		<author>
			<persName><forename type="first">Xiangjue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaying</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Caverlee</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.eacl-main.230</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 17th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Dubrovnik, Croatia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3150" to="3162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">MRC-based medical NER with multi-task learning and multi-strategies</title>
		<author>
			<persName><forename type="first">Xiaojing</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Yuxiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zan</forename><surname>Hongying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Chinese National Conference on Computational Linguistics</title>
		<meeting>the 21st Chinese National Conference on Computational Linguistics<address><addrLine>Nanchang, China</addrLine></address></meeting>
		<imprint>
			<publisher>Chinese Information Processing Society of China</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="836" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Span-based joint entity and relation extraction with transformer pre-training</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Eberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Ulges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI 2020</title>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2006" to="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nested named entity recognition</title>
		<author>
			<persName><forename type="first">Jenny</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 conference on empirical methods in natural language processing</title>
		<meeting>the 2009 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/1609.09106</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Hypernetworks</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Query prior matters: A mrc framework for multimodal named entity recognition</title>
		<author>
			<persName><forename type="first">Meihuizi</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhui</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lejian</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Multimedia</title>
		<meeting>the 30th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3549" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A unified MRC framework for named entity recognition</title>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5849" to="5859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fusing label relations for chinese emr named entity recognition with machine reading comprehension</title>
		<author>
			<persName><forename type="first">Shuyue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailin</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Bioinformatics Research and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="41" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Kg-bart: Knowledge graph-augmented bart for generative commonsense reasoning</title>
		<author>
			<persName><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="6418" to="6425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shudong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramez</forename><surname>Zakhary</surname></persName>
		</author>
		<title level="m">Ace 2004 multilingual training corpus. Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A survey of named entity recognition and classification</title>
		<author>
			<persName><forename type="first">David</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lingvisticae Investigationes</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="26" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The genia corpus: An annotated research abstract corpus in molecular biology domain</title>
		<author>
			<persName><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuka</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideki</forename><surname>Mima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the human language technology conference</title>
		<meeting>the human language technology conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="73" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Yongliang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.06804</idno>
		<title level="m">Locate and label: A two-stage identifier for nested named entity recognition</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meulder</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Medero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<title level="m">Ace 2005 multilingual training corpus. Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page">45</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nested named entity recognition with spanlevel graphs</title>
		<author>
			<persName><forename type="first">Juncheng</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyu</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.63</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="892" to="903" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to extract attribute value from product via question answering: A multi-task approach</title>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhargav</forename><surname>Kanagal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Elsas</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403047</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;20</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="47" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Continuous prompt tuning based textual entailment model for e-commerce entity typing</title>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/BigData55660.2022.10020766</idno>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1383" to="1388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fusing heterogeneous factors with triaffine mechanism for nested named entity recognition</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-acl.250</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3174" to="3186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A multi-task approach for machine reading comprehension form named entity recognition tasks</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 3rd International Conference on Artificial Intelligence and Education (IC-ICAIE 2022)</title>
		<imprint>
			<publisher>Atlantis Press</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="480" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Finbert-mrc: financial named entity recognition using bert under the machine reading comprehension paradigm</title>
		<author>
			<persName><forename type="first">Yuzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.15485</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Localize, retrieve and fuse: A generalized framework for free-form question answering over tables</title>
		<author>
			<persName><forename type="first">Wenting</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongfen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attend, memorize and generate: Towards faithful table-to-text generation in few shots</title>
		<author>
			<persName><forename type="first">Wenting</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.347</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana</addrLine></address></meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4106" to="4117" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
