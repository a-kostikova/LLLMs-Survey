<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Impact of Debiasing on the Performance of Language Models in Downstream Tasks is Underestimated</title>
				<funder ref="#_JfEW2A3">
					<orgName type="full">National Institute of Information and Communications Technology</orgName>
					<orgName type="abbreviated">NICT</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Masahiro</forename><surname>Kaneko</surname></persName>
							<email>masahiro.kaneko@mbzuai.ac.ae</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
							<email>danushka@liverpool.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Liverpool</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
							<email>okazaki@c.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Mbzuai</surname></persName>
						</author>
						<title level="a" type="main">The Impact of Debiasing on the Performance of Language Models in Downstream Tasks is Underestimated</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BE56B4149F258B195E6554D4EFEA2A68</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pre-trained language models trained on largescale data have learned serious levels of social biases. Consequently, various methods have been proposed to debias pre-trained models. Debiasing methods need to mitigate only discriminatory bias information from the pretrained models, while retaining information that is useful for the downstream tasks. In previous research, whether useful information is retained has been confirmed by the performance of downstream tasks in debiased pretrained models. On the other hand, it is not clear whether these benchmarks consist of data pertaining to social biases and are appropriate for investigating the impact of debiasing. For example in gender-related social biases, data containing female words (e.g. "she, female, woman"), male words (e.g. "he, male, man"), and stereotypical words (e.g. "nurse, doctor, professor") are considered to be the most affected by debiasing. If there is not much data containing these words in a benchmark dataset for a target task, there is the possibility of erroneously evaluating the effects of debiasing. In this study, we compare the impact of debiasing on performance across multiple downstream tasks using a wide-range of benchmark datasets that containing female, male, and stereotypical words. Experiments show that the effects of debiasing are consistently underestimated across all tasks. Moreover, the effects of debiasing could be reliably evaluated by separately considering instances containing female, male, and stereotypical words than all of the instances in a benchmark dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unfortunately, Pre-trained Language Models (PLMs) such as BERT <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref> and * Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool and is not associated with Amazon. Table <ref type="table">1</ref>: The total number of instances containing female, male, and occupational (Occ.) words in the GLUE development data.</p><p>RoBERTa <ref type="bibr" target="#b18">(Liu et al., 2019)</ref> easily learn discriminatory social biases expressed in human-written texts in massive datasets <ref type="bibr" target="#b16">(Kurita et al., 2019;</ref><ref type="bibr" target="#b31">Zhou et al., 2022;</ref><ref type="bibr">Kaneko et al., 2022)</ref>. For example, if a model is given "[MASK] is a nurse." as the input, a gender biased PLM would predict "She" with a higer likelihood score than for "He" when filling the <ref type="bibr">[MASK]</ref>.</p><p>Various debiasing methods have been proposed to mitigate social biases in PLMs. <ref type="bibr" target="#b30">Zhao et al. (2019)</ref>; <ref type="bibr" target="#b28">Webster et al. (2020)</ref> proposed a debiasing method by swapping the gender of female and male words in the training data. <ref type="bibr" target="#b12">Kaneko and Bollegala (2021)</ref> proposed a method for debiasing by orthogonalising the vectors representing gender information with the hidden layer of a language model given a sentence containing a stereotypical word. <ref type="bibr" target="#b28">Webster et al. (2020)</ref> showed that dropout regularization can reduce overfitting to gender information, thereby can be used for debiasing PLMs.</p><p>The debiasing method should mitigate only discriminatory information, while pre-trained useful information should be retained in the model. Evaluations in downstream tasks often employ the GLEU benchmark <ref type="bibr" target="#b25">(Wang et al., 2018)</ref>, which measures the ability to understand language <ref type="bibr" target="#b12">(Kaneko and Bollegala, 2021;</ref><ref type="bibr" target="#b10">Guo et al., 2022;</ref><ref type="bibr" target="#b19">Meade et al., 2022)</ref>. The data for downstream tasks are not selected in terms of whether they reflect the impact of debiasing. To mitigate gender bias, data containing female words such as "she" and "woman", male words such as "he" and "man", and stereotypical words such as "doctor" and "nurse" would be most affected by debiasing.</p><p>Table <ref type="table">1</ref> shows the total number of instances containing female, male, and occupational (Occ.) words in the development data in the GLUE benchmark suite <ref type="bibr" target="#b26">(Wang et al., 2019)</ref>, which is widely recognised as a standard evaluation benchmark for LLMs. Occupational words have been used for probing LLMs for stereotypical social biases <ref type="bibr" target="#b1">(Bolukbasi et al., 2016)</ref>. From Table <ref type="table">1</ref>, we see that the GLEU benchmark has little data related to females and occupations. Therefore, the impact of debiasing on data related to females and occupations may be potentially underestimated when LLMs are evaluated on GLUE.</p><p>We first extract instances containing female words, data containing male words, and data containing stereotypical words from the benchmarks. We then calculated the performance difference between the original model and the debiased model for each category and compared it to the performance difference using the entire benchmark. The results showed that the debiased model performed worse than the original model on data related to females and occupations compared to the original model when evaluated on the entire dataset. Therefore, existing evaluations underestimate the impact of debiasing on the performance of the downstream task.</p><p>It is important to be able to compare how well the effects of debiasing are captured in the datasets related to females, males, and occupations. We propose a method to control the degree of debiasing of PLMs and investigate whether the performance difference between original and debiased models widens as the degree of debiasing increases. Experimental results showed that the proportion of female, male and occupational words in the dataset is related to the susceptibility of the dataset to debiasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Debiasing Methods</head><p>We use the following three commonly used debiasing methods in our experiments. We apply these debiasing methods during fine-tuning in downstream tasks.</p><p>Counterfactual Data Augmentation (CDA) debiasing: CDA debiasing <ref type="bibr" target="#b28">(Webster et al., 2020)</ref> swaps the gender of gender words in the training data. For example, "She is a nurse" is swapped to "He is a nurse", and the swapped version is appended to the training dataset. This enables to learn a less biased model because the frequency of female and male words will be the same in the augmented dataset.</p><p>Dropout debiasing: <ref type="bibr" target="#b28">Webster et al. (2020)</ref> introduced dropout regularisation as a method to mitigate biases. They enhanced the dropout parameters for the attention weights and hidden activations of PLMs. Their research demonstrated that intensified dropout regularisation diminishes gender bias in these PLMs. They showed that dropout interfers with the attention mechanism in PLMs, and prevents undesirable associations between words. However, it is also possible that the model may no longer be able to learn desirable associations.</p><p>Context debiasing: <ref type="bibr" target="#b12">Kaneko and Bollegala (2021)</ref> proposed a method to debias MLMs through fine-tuning. It preserves semantic information while removing gender-related biases using orthogonal projections at token-or sentence-level. This method targets male and female words and occupational words in the text for debiasing. This method can be applied various MLMs, independent of the model architectures and pre-training methods. Token-level debiasing across all layers produces the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Settings</head><p>Although we use BERT (bert-base-cased<ref type="foot" target="#foot_0">1</ref> ) <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref> as our PML here as it has been the focus of much prior work on bias evaluations <ref type="bibr" target="#b12">(Kaneko and Bollegala, 2021;</ref><ref type="bibr" target="#b10">Guo et al., 2022;</ref><ref type="bibr" target="#b19">Meade et al., 2022)</ref>, the evaluation protocol we use can be applied to any PLM. We used the word lists<ref type="foot" target="#foot_1">2</ref> proposed by <ref type="bibr" target="#b12">Kaneko and Bollegala (2021)</ref> as female words, male words, and occupational words for extracting data instances and debiasing.</p><p>We use the following nine downstream tasks from the GLEU benchmark: CoLA <ref type="bibr" target="#b27">(Warstadt et al., 2019)</ref>, MNLI <ref type="bibr" target="#b29">(Williams et al., 2018)</ref>, MRPC <ref type="bibr" target="#b8">(Dolan and Brockett, 2005)</ref>   <ref type="bibr" target="#b4">(Dagan et al., 2006;</ref><ref type="bibr" target="#b11">Haim et al., 2006;</ref><ref type="bibr" target="#b9">Giampiccolo et al., 2007;</ref><ref type="bibr" target="#b0">Bentivogli et al., 2009)</ref>, SST-2 <ref type="bibr" target="#b24">(Socher et al., 2013)</ref>, STS-B <ref type="bibr" target="#b3">(Cer et al., 2017)</ref>, and WNLI <ref type="bibr" target="#b17">(Levesque et al., 2012)</ref>. Hyperparameters for debiasing follow previous studies <ref type="bibr" target="#b12">(Kaneko and Bollegala, 2021;</ref><ref type="bibr" target="#b28">Webster et al., 2020)</ref>, and we used the default values of huggingface for downstream task hyperparameters. 4 For fine-tuning we use the entire training dataset for each corresponding task, without splitting into male, female and occupational instances. We evaluate the performance on all tasks using the official development data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Performance of Original vs. Debiased Models</head><p>We extract instances containing female words, male words, and stereotypical words from each of the datasets. We then calculate the performance difference between the original model and the debiased model for each dataset, and compare against the performance differences obtained when using all instances. If the performance difference for all instances is smaller than that when evaluated for the female, male, and occupational instances, it would indicate that the effect of debiasing is underestimated when evaluated on the entire dataset.</p><p>Table <ref type="table" target="#tab_2">2</ref> shows the performance differences between the original model and the debiased model for each dataset/task in the GLEU benchmark. All, Female, Male, and Occ. are the performance differences when evaluated on the entire task dataset, instances containing female words, instances containing male words, and instances containing occupational words, respectively. From the results in Table <ref type="table" target="#tab_2">2</ref>, it can be seen that the performance difference between the original model and the debiased model is larger for the Female, Male, and Occ. instances compared to that when using all instances. In particular, instances related to females exhibit a significant decrease in performance after debiasing.</p><p>It can be seen that different word lists used for debiasing have different effects on the performance degradation in downstream tasks due to debiasing. Context debiasing uses occupational words for debiasing, while CDA debiasing does not. Therefore, in CDA debiasing, Occ. does not have a large performance difference compared to female-and male-related instances. Therefore, in CDA debiasing, the performance difference for occupationrelated instances is smaller than that for the female and male-related instances. On the other hand, in Context debiasing, occupation-related instances has the largest performance difference as well as female-and male-related instances. Dropout debiasing does not use word lists for debiasing. Therefore, unlike CDA and context debiasing, we see large drops in performance for female, male and Occ. across tasks with Dropout debiasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Debias Controlled Method</head><p>To understand how debiasing of an PLM affects the performance of individual downstream benchmark datasets, following the probing technique proposed by <ref type="bibr" target="#b14">Kaneko et al. (2023)</ref>, we apply different levels of debiasing to bert-base-cased PLM and measure the difference in performance with respect to its original (non-debiased) version. For this purpose we use CDA as the debiasing method, where we swap the gender-related pronouns in r ∈ [0, 1] fraction of the total N instances of a dataset (i.e.the total number of gender swapped instances in a dataset will be r × N ). r = 0 corresponds to not swapping gender in any training instances of the dataset, whereas r = 1 swaps the gender in all instances. We increment r in step size 0.1 to obtain increasingly debiased version of the PLM, which is then finetuned for the downstream task<ref type="foot" target="#foot_2">5</ref> . Figure <ref type="figure" target="#fig_1">1</ref> shows the difference in performance between the original vs. debiased versions of the PLM for QQP, MNLI, and QNLI, which have the largest numbers of instances in the GLEU benchmark.</p><p>Note that CDA debiasing reverses gender without considering the context, as in "He gets pregnant" for "She gets pregnant". This is problemantic because it eliminates even useful gender-related information learnt by the PLM via co-occurring contexts. Therefore, CDA debiasing has a negative impact on the performance of downstream tasks <ref type="bibr" target="#b32">(Zmigrod et al., 2019)</ref> as shown by all three subplots in Figure <ref type="figure" target="#fig_1">1</ref>. In fact, Table <ref type="table" target="#tab_2">2</ref> shows that the performance difference of CDA debiasing is larger than that of dropout debiasing and context debiasing. Therefore, the larger r is for CDA, the more gender instances is balanced and debiased, but the performance is unfortunately degraded.</p><p>If the dataset of the downstream task is sensitive to the effect of debiasing, the performance difference between the original model and the debiased model widens as r increases. On the other hand, if the data set is insensitive to the effect of debiasing, the performance difference between the original model and the debiased model is unlikely to increase with the value of r.</p><p>We find that the performance differences for the female, male, and occupational instances in the QQP, MNLI, and QNLI datasets increase with the value of r. On the other hand, for QQP and MNLI, there is a rise and fall in the performance difference when all data are used. These results indicate that All, which includes instances related to nongender, are less sensitive to the effect of debiasing compared to Female, Male, and Occupational instances.</p><p>On the other hand, for QNLI, All has a small rise and fall in the performance difference. As seen from Table <ref type="table">1</ref>, QNLI contains more gender-related instances than QQP and MNLI. Therefore, it is likely that the performance decreases with r even for All. All and Male instances have a similar trend in performance difference with r. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conclusion</head><p>This study focused on gender-related social biases and the presence of female, male, and stereotypical words in benchmark datasets. Prior work had used the performance on downstream tasks to prove the usefulness of debiasing methods, overlooking the fact that only a small fraction of those downstream benchmark datasets contain gender-related instances. On the contrary, we found that the effects of debiasing an PLM were consistently underestimated across all tasks. We recommend that the evaluation of debiasing effects must be more reliably conducted by considering instances containing specific gender-related words separately rather than evaluating all instances in a benchmark dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Ethical Considerations</head><p>This study uses existing methods and datasets for experiments and does not propose a debiasing method or create a new dataset for social bias. This study evaluates the impact of debiasing on the performance of the downstream task, and it is not possible to evaluate how much bias is mitigated in the PLMs. Therefore, when evaluating the bias of PLMs, it is necessary to use evaluation methods such as StereoSet <ref type="bibr" target="#b20">(Nadeem et al., 2021)</ref>, Crowds-Pairs <ref type="bibr" target="#b21">(Nangia et al., 2020)</ref>, and All Unmasked Likelihood <ref type="bibr">(Kaneko and Bollegala, 2022)</ref>.</p><p>In this study, we only included binary gender as a gender bias. However, gender bias regarding non-binary gender has also been reported (Cao and <ref type="bibr" target="#b2">Daumé III, 2020;</ref><ref type="bibr" target="#b6">Dev et al., 2021)</ref>. It is necessary to verify whether there is a similar trend in debiasing for non-binary genders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations</head><p>Many previous studies have shown that various social biases other than gender bias are learned in PLMs. This study targets only gender bias. While existing studies <ref type="bibr" target="#b28">(Webster et al., 2020;</ref><ref type="bibr" target="#b30">Zhao et al., 2019)</ref> have debiasing various PLMs, we have experimented only with bert-base-cased. Furthermore, although this study targets only English, which is a morphologically limited language. On the other hand, various types of social biases are also learned in the PLMs across many languages <ref type="bibr">(Kaneko et al., 2022;</ref><ref type="bibr" target="#b22">Névéol et al., 2022)</ref>. Therefore, if the proposed method is to be used with other social biases and PLMs, it is necessary to properly verify its effectiveness in languages other than English. Moreover, we have not verified the use of debias controlled methods in languages such as Spanish and Russian, where gender swapping is not easy from a grammatical point of view <ref type="bibr" target="#b32">(Zmigrod et al., 2019)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Bias Evaluation in NLI task</head><p>We show that the debias controlled method is appropriately debiasing according to r. We use Fraction Neutra (FN; <ref type="bibr" target="#b5">Dev et al., 2020)</ref> as the bias evaluation method. The FN method evaluates bias in the NLI by considering the percentage of neutral labels predicted by the model for the premise sentence (e.g. The driver owns a cabinet.) and the hypothesis sentence (e.g. The man owns a cabinet.) generated with the template. The FN method indicates that the lower the score, the more bias there is in the model. We evaluate PLMs trained on MNLI with FN method.</p><p>Figure <ref type="figure" target="#fig_2">2</ref> shows the bias scores of FN method for each debias controlled model. It can be seen that the bias of the model is decreasing with r. Therefore, the debias controlled method is able to debias the models according to r.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3</head><label></label><figDesc>https://quoradata.quora.com/ First-Quora-Dataset-Release-Question-Pairs 4 https://github.com/huggingface/transformers/ tree/main/examples/pytorch/text-classification</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Performance difference between original and debiased models by debias rate r. The vertical axis shows the performance difference between the original and debiased models, and the horizontal axis shows the debias rate.</figDesc><graphic coords="4,306.14,370.75,226.77,128.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Evaluation of debias controlled models using FN evaluation method. The vertical axis shows the bias score, and the horizontal axis shows r.</figDesc><graphic coords="8,70.87,70.87,226.77,129.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, QNLI (Ra-</figDesc><table><row><cell></cell><cell></cell><cell>CDA</cell><cell></cell><cell></cell><cell cols="2">Dropout</cell><cell></cell><cell></cell><cell cols="2">Context</cell></row><row><cell></cell><cell cols="2">All Female Male</cell><cell>Occ.</cell><cell cols="3">All Female Male</cell><cell>Occ.</cell><cell cols="3">All Female Male</cell><cell>Occ.</cell></row><row><cell>CoLA</cell><cell>-1.36</cell><cell cols="2">-3.42 -2.01 -1.45</cell><cell>0.42</cell><cell cols="4">-0.14 -0.21 -0.07 -0.32</cell><cell cols="2">-0.86 -0.71 -0.55</cell></row><row><cell>MNLI</cell><cell>-0.55</cell><cell cols="2">-0.90 -0.71 -0.63</cell><cell>0.23</cell><cell>0.13</cell><cell>0.01</cell><cell cols="2">0.05 -0.05</cell><cell cols="2">-0.47 -0.43 -0.32</cell></row><row><cell cols="2">MRPC -0.96</cell><cell cols="3">-1.28 -1.31 -1.03 -0.82</cell><cell cols="4">-1.12 -1.02 -1.04 -0.88</cell><cell cols="2">-1.01 -1.06 -0.92</cell></row><row><cell>QNLI</cell><cell>-1.13</cell><cell cols="3">-1.42 -1.19 -1.27 -1.01</cell><cell cols="3">-1.11 -1.07 -1.21</cell><cell>0.25</cell><cell cols="2">-0.19 -0.06 -0.04</cell></row><row><cell>QQP</cell><cell>-0.21</cell><cell cols="2">-0.69 -0.32 -0.25</cell><cell>0.53</cell><cell>0.13</cell><cell>0.47</cell><cell>0.30</cell><cell>0.14</cell><cell>-0.12</cell><cell>0.03 -0.05</cell></row><row><cell>RTE</cell><cell>-1.16</cell><cell cols="3">-1.21 -1.02 -1.13 -1.01</cell><cell cols="4">-1.24 -0.96 -1.13 -0.43</cell><cell cols="2">-0.65 -0.51 -0.73</cell></row><row><cell>SST-2</cell><cell>-0.11</cell><cell cols="2">-0.81 -0.34 -0.25</cell><cell>0.45</cell><cell>0.20</cell><cell>0.12</cell><cell>0.23</cell><cell>0.22</cell><cell cols="2">-0.15 -0.02 -0.12</cell></row><row><cell cols="2">STS-B -1.01</cell><cell cols="2">-1.95 -1.34 -1.10</cell><cell>0.21</cell><cell cols="4">0.09 -0.03 -0.11 -0.08</cell><cell cols="2">-0.31 -0.38 -0.34</cell></row><row><cell>WNLI</cell><cell>-2.82</cell><cell cols="3">-3.07 -2.82 -2.71 -2.01</cell><cell cols="4">-2.21 -2.01 -2.33 -1.52</cell><cell cols="2">-1.88 -1.52 -1.61</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance difference between the original model and debiased model for each dataset. Bolded values indicate the largest drop in performance of the debiased model. jpurkar et al., 2016), QQP 3 , RTE</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://huggingface.co/bert-base-cased</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/kanekomasahiro/ context-debias</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>In Appendix A, we show that the debias controlled method is able to debias the model according to r. (a) QQP. (b) MNLI. (c) QNLI.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>These research results were obtained from the commissioned research (No.<rs type="grantNumber">225</rs>) by <rs type="funder">National Institute of Information and Communications Technology (NICT), Japan</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_JfEW2A3">
					<idno type="grant-number">225</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The fifth pascal recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<editor>TAC. Citeseer</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Man is to computer programmer as woman is to homemaker? debiasing word embeddings</title>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">T</forename><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Toward gender-inclusive coreference resolution</title>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Trista</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.418</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4568" to="4595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iñigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S17-2001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note>SemEval-2017</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment: First PASCAL Machine Learning Challenges Workshop, MLCW 2005</title>
		<meeting><address><addrLine>Southampton, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2006. April 11-13, 2005</date>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On measuring and mitigating biased inferences of word embeddings</title>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">M</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><surname>Vivek</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i05.6267</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">Srikumar. 2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7659" to="7666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Harms of gender exclusivity and challenges in non-binary representation in language technologies</title>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masoud</forename><surname>Monajatipoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anaelia</forename><surname>Ovalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Subramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.150</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1968" to="1994" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing</title>
		<meeting>the Third International Workshop on Paraphrasing</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The third pascal recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing</title>
		<meeting>the ACL-PASCAL workshop on textual entailment and paraphrasing</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Autodebias: Debiasing masked language models with automated biased prompts</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Abbasi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.72</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1012" to="1023" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The second pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Bar Haim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Idan</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second PAS-CAL Challenges Workshop on Recognising Textual Entailment</title>
		<meeting>the Second PAS-CAL Challenges Workshop on Recognising Textual Entailment</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Debiasing pre-trained contextualised embeddings</title>
		<author>
			<persName><forename type="first">Masahiro</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.107</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unmasking the mask-evaluating social biases in masked language models</title>
		<author>
			<persName><forename type="first">Masahiro</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="11954" to="11962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Comparing intrinsic gender bias evaluation measures without using human annotated examples</title>
		<author>
			<persName><forename type="first">Masahiro</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 17th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Dubrovnik, Croatia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2857" to="2863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gender bias in masked language models for multiple languages</title>
		<author>
			<persName><forename type="first">Masahiro</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aizhan</forename><surname>Imankulova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.197</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Seattle, United States. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2740" to="2750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Measuring bias in contextualized word representations</title>
		<author>
			<persName><forename type="first">Keita</forename><surname>Kurita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nidhi</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-3823</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Gender Bias in Natural Language Processing</title>
		<meeting>the First Workshop on Gender Bias in Natural Language Processing<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="166" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName><forename type="first">Hector</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth international conference on the principles of knowledge representation and reasoning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An empirical survey of the effectiveness of debiasing techniques for pre-trained language models</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Meade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elinor</forename><surname>Poole-Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.132</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1878" to="1898" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">StereoSet: Measuring stereotypical bias in pretrained language models</title>
		<author>
			<persName><forename type="first">Moin</forename><surname>Nadeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Bethke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.416</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5356" to="5371" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CrowS-pairs: A challenge dataset for measuring social biases in masked language models</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasika</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.154</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1953" to="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">French CrowS-pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than English</title>
		<author>
			<persName><forename type="first">Aurélie</forename><surname>Névéol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoann</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Bezançon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karën</forename><surname>Fort</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.583</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8521" to="8531" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5446</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural network acceptability judgments</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00290</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="625" to="641" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Kellie</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06032</idno>
		<title level="m">Measuring and reducing gendered correlations in pre-trained models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gender bias in contextualized word embeddings</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1064</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Minnesota. Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="629" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sense embeddings are also biased -evaluating social biases in static and contextualised sense embeddings</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masahiro</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1924" to="1935" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology</title>
		<author>
			<persName><forename type="first">Ran</forename><surname>Zmigrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabrina</forename><forename type="middle">J</forename><surname>Mielke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1161</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1651" to="1661" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
