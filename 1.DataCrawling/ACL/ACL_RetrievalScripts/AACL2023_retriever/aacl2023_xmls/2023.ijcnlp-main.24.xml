<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking the Role of Entity Type in Relation Classification</title>
				<funder>
					<orgName type="full">Australian Government</orgName>
				</funder>
				<funder>
					<orgName type="full">CSIRO&apos;s Precision Health Future Science Platform (PH FSP</orgName>
				</funder>
				<funder>
					<orgName type="full">National Computational Infrastructure</orgName>
					<orgName type="abbreviated">NCI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CSIRO Data61</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sarvnaz</forename><surname>Karimi</surname></persName>
							<email>sarvnaz.karimi@csiro.au</email>
							<affiliation key="aff0">
								<orgName type="institution">CSIRO Data61</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><surname>Wan</surname></persName>
							<email>stephen.wan@csiro.au</email>
							<affiliation key="aff0">
								<orgName type="institution">CSIRO Data61</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking the Role of Entity Type in Relation Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B61B1A3210044758DE07349522711659</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Relation Classification (RC)-the task of identifying the relation between a pair of target entities-is a fundamental sub-task of information extraction. RC models built on top of entity information are prevalent, with different variants using entity information, especially entity type information, differently. However, RC models are often benchmarked on datasets that human annotators provide near-perfect entity information, and, state-of-the-art results are reported using gold entity type information. We believe there is a need to understand how the effectiveness of RC models is affected by the correctness of entity type information because in practice this information is provided by imperfect entity recognition models. Our results on six datasets across four domains show that although using gold entity type improves the effectiveness of RC models, incorrect entity types may cause large effectiveness drops on some (but not all) datasets. We propose using Pointwise Mutual Information (PMI) to identify datasets on which RC models may be negatively impacted by incorrect entity type information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction is a fundamental sub-task of information extraction that aims to extract structured information from unstructured text. It can be useful for many downstream applications, such as opinion mining, question answering, and knowledge graph construction <ref type="bibr" target="#b5">(Choi et al., 2006;</ref><ref type="bibr" target="#b15">Ji and Grishman, 2011;</ref><ref type="bibr" target="#b24">Nickel et al., 2015;</ref><ref type="bibr">Zhang et al., 2019a)</ref>. One common approach to relation extraction is pipelinebased, where Named Entity Recognition (NER) models are first used to identify entity names in text and then the identified entities are fed into a Relation Classification (RC) model, identifying the relation between a pair of target entities (See an example in Figure <ref type="figure">1</ref>).</p><p>Previous studies <ref type="bibr" target="#b30">(Soares et al., 2019;</ref><ref type="bibr" target="#b25">Peng et al., 2020;</ref><ref type="bibr">Zhong and Chen, 2021;</ref><ref type="bibr" target="#b48">Zhou and Chen,</ref> He received Rx for potassium to help with cramps .</p><p>TrAP (Treatment is administered for medical problem) Figure <ref type="figure">1</ref>: An example taken from I2B2-2010 <ref type="bibr" target="#b33">(Uzuner et al., 2011)</ref>. It contains the 'TrAP' relation between two entities: Rx (entity type: Treatment) and cramps (entity type: Problem). Note that this sentence contains another relation between 'potassium' and 'cramps'.</p><p>2022) show that adding information on entity position and type is critical for the RC models to learn useful relation representations, and the RC model heavily relies on the entity information, especially entity type information. However, the effectiveness of proposed methods of incorporating entity information into the RC model is largely benchmarked on datasets where human-annotated entity information is provided. Start-of-the-art results are reported with RC models using gold entity type information <ref type="bibr" target="#b21">(Lyu and Chen, 2021;</ref><ref type="bibr" target="#b48">Zhou and Chen, 2022;</ref><ref type="bibr" target="#b13">Han et al., 2022)</ref>. There is a gap in the literature to investigate how the effectiveness of RC models is affected by the correctness of entity type information. In other words, given that no NER model is perfect, how may the availability of accurate entity type information affect our choice of RC models?</p><p>To answer this research question, we present the following contributions:</p><p>• We compare different approaches of Transformer-based RC models that incorporate entity type information via minimal architecture change. Based on experimental results on six datasets across four domains, we find that incorporating gold entity type information using special markers outperforms other approaches using entity type embeddings or entity type as part of the initial decoder input.</p><p>• We conduct a sensitivity analysis of the RC model with respect to the correctness of entity type information. Our results show that entity type errors may cause a large effectiveness drop on some (but not all) datasets, and this phenomenon may change the decision of how to incorporate entity type information to RC models.</p><p>• We show that Pointwise Mutual Information (PMI) can be used to identify datasets on which RC models may be negatively impacted by incorrect entity types and help decide how to use entity type for the RC model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In earlier literature, Relation Classification (RC) models rely on manually defined features <ref type="bibr" target="#b6">(Craven and Kumlien, 1999;</ref><ref type="bibr" target="#b22">Mintz et al., 2009)</ref>, convolutional neural network <ref type="bibr" target="#b42">(Zeng et al., 2014;</ref><ref type="bibr" target="#b9">dos Santos et al., 2015)</ref>, recurrent neural network <ref type="bibr" target="#b44">(Zhang and Wang, 2015;</ref><ref type="bibr" target="#b23">Miwa and Bansal, 2016)</ref> or graph neural network <ref type="bibr" target="#b11">(Guo et al., 2019)</ref> to build relation representation. To effectively capture the interaction between entities, in addition to entity information, these models either explicitly make use of syntactic information <ref type="bibr" target="#b22">(Mintz et al., 2009)</ref> or use neural networks to learn context information <ref type="bibr" target="#b34">(Vu et al., 2016;</ref><ref type="bibr" target="#b31">Sorokin and Gurevych, 2017)</ref>.</p><p>After the introduction of BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>, the pre-training-then-fine-tuning paradigm dominated. RC models based on pre-trained language representation models have also gained significant success <ref type="bibr" target="#b37">(Wu and He, 2019;</ref><ref type="bibr" target="#b2">Alt et al., 2019;</ref><ref type="bibr" target="#b36">Wei et al., 2019)</ref>. Recent research can be divided into three groups. One research direction continues to improve pre-trained models via injecting factual and linguistic knowledge, usually with the help of external knowledge base consisting of relation tuples <ref type="bibr" target="#b27">(Peters et al., 2019;</ref><ref type="bibr" target="#b30">Soares et al., 2019;</ref><ref type="bibr">Zhang et al., 2019b;</ref><ref type="bibr" target="#b39">Yamada et al., 2020;</ref><ref type="bibr" target="#b35">Wang et al., 2021)</ref>. Another line of research designs specialised pre-training objectives to help better modelling spans, which usually refer to entities <ref type="bibr" target="#b16">(Joshi et al., 2020;</ref><ref type="bibr" target="#b18">Lin et al., 2021)</ref>. The last category focuses on the fine-tuning stage where modifications are proposed to incorporate syntactic features <ref type="bibr" target="#b0">(Adel and Strötgen, 2021)</ref> and entity information <ref type="bibr" target="#b3">(Bilan and Roth, 2018;</ref><ref type="bibr" target="#b10">Eberts and Ulges, 2020;</ref><ref type="bibr" target="#b17">Li et al., 2020;</ref><ref type="bibr" target="#b48">Zhou and Chen, 2022;</ref><ref type="bibr" target="#b13">Han et al., 2022)</ref>.</p><p>Our study falls in the last category, and we focus on analysing how the correctness of entity types may impact the effectiveness of RC models.</p><p>Although we focus on a pipeline-based approach for relation extraction, our work also relates to another group of relation extraction methods that model NER and RC jointly <ref type="bibr" target="#b23">(Miwa and Bansal, 2016;</ref><ref type="bibr" target="#b19">Lin et al., 2020;</ref><ref type="bibr" target="#b10">Eberts and Ulges, 2020;</ref><ref type="bibr" target="#b40">Yan et al., 2022)</ref>. On the one hand, both approaches employ methods of incorporating entity information into RC, and design options can be shared. On the other hand, although joint models aim to mitigate error propagation via modelling entity and relation representations together, they still rely on ground truth entity information for training the relation component. For example, Eberts and Ulges (2020) train the relation classifier via drawing negative examples from gold entity pairs that are not labelled with any relation. We believe our analysis can provide insights into designing components used in joint models when high-quality entity information is unavailable.</p><p>Debates about the usefulness of entity type information for RC models <ref type="bibr" target="#b25">Peng et al. (2020)</ref> observe that Context+EntityType-replacing entity names with their entity types-achieves comparable results on TACRED with Context+EntityName for BERT. They argue that using original entity names may be biased by the entity distributions in the training set and the RC models may not generalise well to unseen entities. By the same consideration, <ref type="bibr" target="#b45">Zhang et al. (2018)</ref>; <ref type="bibr" target="#b16">Joshi et al. (2020)</ref> use entity types to replace entity names. <ref type="bibr" target="#b48">Zhou and Chen (2022)</ref> argue that if the RC models should not consider entity names, it is unreasonable to suppose that they can be improved by external knowledge graphs, which is an active research area in the literature. They propose to insert special typed markers around original entity names (detailed in Section 3.2) and show that the proposed variant achieves state-of-the-art effectiveness on multiple datasets. However, Zhou and Chen notice that using entity type information brings smaller improvements on a clean test set than a noisy test set. They hypothesise that this result may be attributed to annotation biases. That is, some annotators may label the relation only based on target entities without reading the context. The paradigm proposed by <ref type="bibr" target="#b21">Lyu and Chen (2021)</ref> makes stronger assumptions about the correctness of entity types, which are used to filter candidate relations. A specific classifier is individually learned for each pair of entity types to predict a specific set of candidate relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries of RC Models</head><p>Problem Formulation Relation classification is framed as a task where given a text sequence X = [x 0 , • • • , x n ] and two entity names e 1 and e 2 , the RC model predicts either (1) a relation r ∈ R that holds between two entities; or, (2) NA relation (no relation or none of the pre-defined relation hold). We aim to investigate how to effectively incorporate entity types of e 1 and e 2 in RC models, and how the effectiveness of the RC model is affected by the correctness of these entity types.</p><p>In the following, we first group existing RC models into three categories: span-based, marker-based and prompt-based, and then describe how entity type can be incorporated into these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Span-based Models</head><p>The span-based RC models usually consist of three components: (1) a token encoder, (2) a relation encoder, and (3) a classifier. The token encoder takes X as input and generates a list of contextual token representations</p><formula xml:id="formula_0">H = [h 0 , • • • , h n ].</formula><p>After the contextual token presentations H are obtained from the token encoder, span-based models first build span (e.g., entity names, the context span between two target entities) representations from H. There are many options for the fusion function proposed in the literature. For example, Eberts and Ulges (2020) max-pool contextual token representations to obtain span embeddings; <ref type="bibr" target="#b37">Wu and He (2019)</ref> apply the average operation to obtain span embeddings; <ref type="bibr" target="#b41">Yu et al. (2020)</ref> use biaffine attention to build span embeddings; and, concatenating token representations corresponding to boundary tokens for span embeddings <ref type="bibr" target="#b16">(Joshi et al., 2020)</ref>. Our preliminary experiments find that max-pooling (Eberts and Ulges, 2020) performs best, although the difference between these variants is very small.</p><p>Eberts and Ulges (2020) propose to concatenate three span embeddings-corresponding to two entities and the context between them-as the relation representation ℏ. We also investigate concatenating more spans (e.g., context before the first entity names and context after the last entity names) or the hidden states corresponding to the [CLS] token, but find that these variants do not improve the effectiveness of the RC models. We denote the model variant by <ref type="bibr" target="#b10">Eberts and Ulges (2020)</ref> as SpU in experimental results (Table <ref type="table" target="#tab_3">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incorporating entity type information via segment embeddings</head><p>To provide the RC model with entity type information, we propose incorporating segment embedding into the input of the token encoder <ref type="bibr" target="#b31">(Sorokin and Gurevych, 2017)</ref>. We first mark each token in the entity names using their entity types. An embedding matrix, E ∈ R (c+1)×768 (c is the total number of entity types), is then used to convert these entity types into the segment embedding, and finally, we sum the segment, token, and position embeddings and feed them into the token encoder. In our preliminary experiments, we also investigate concatenating segment embeddings with token encoder outputs but find it underperforms the variant where segment embeddings are fed into the token encoder. We denote this variant as SpT in Table <ref type="table" target="#tab_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Marker-based Models</head><p>Methods belonging to this category usually modify the original text sequence by either inserting special markers or using special markers to replace original entity names. Then the hidden states corresponding to these special markers are used to build the relation representation. For example, Soares et al. ( <ref type="formula">2019</ref>) insert special markers, i.e., [E1], [/E1],</p><p>[E2] and [/E2], before and after target entities, and then concatenate hidden states corresponding to [E1] and [E2] as the relation representation. Arguing that these newly introduced markers, such as [E1] and [E2], are not well pre-trained, <ref type="bibr" target="#b48">Zhou and Chen (2022)</ref> propose to use punctuation markers such as @ and # to enclose target entities. Zhou and Chen also use special markers to incorporate the entity type. That is, they use * and ∧ to enclose entity type and prepend to entity names. We denote the variant of using untyped markers as MaU and MaU received @ Rx @ for potassium to help with # cramps # MaTi received @ * treatment * Rx @ for potassium to help with # ∧ problem ∧ cramps # MaTr received @ * treatment * @ for potassium to help with # ∧ problem ∧ #  [Z] are three sentinel tokens in the template, and we omit trainable prompt embeddings in the template for the sake of simplicity. We refer readers to <ref type="bibr" target="#b13">(Han et al., 2022)</ref> for more details.</p><p>the one using typed markers as MaTi in Table <ref type="table" target="#tab_3">3</ref>, and examples of the modified text are shown in Table <ref type="table" target="#tab_0">1</ref>. <ref type="bibr" target="#b46">Zhang et al. (2017)</ref>; <ref type="bibr" target="#b16">Joshi et al. (2020)</ref> replace entities by their entity types such as '[SUBJ-TYPE]' and '[OBJ-TYPE]' and predict the relation type using the hidden states of [CLS] token. We find concatenating hidden states corresponding to two beginning markers-punctuation markers instead of newly introduced markers-performs better, and we denote this variant as MaTr (See example in Table <ref type="table" target="#tab_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Prompt-based Models</head><p>Prompt-based models employ encoder-decoder architecture and convert the classification problem to a text generation problem <ref type="bibr" target="#b13">(Han et al., 2022;</ref><ref type="bibr" target="#b4">Chen et al., 2022;</ref><ref type="bibr" target="#b38">Xu et al., 2022)</ref>. That is, the original text sequence X is reformulated by adding a clozestyle phrase called template. The modified text sequence is then taken as the input of the seq2seq model, and the model generates a sequence of tokens called relation label verbalisation and can be mapped from relation labels R. See Figure <ref type="figure" target="#fig_0">2</ref> for a high-level illustration.</p><p>Instead of the handcrafted template (such as 'The relation between Rx and cramps is &lt;mask&gt;', <ref type="bibr" target="#b13">Han et al. (2022)</ref> design a method that uses a series of learnable continuous tokens as prompts. They copy target entity names after the original text sequence and use three sentinel tokens ([X], [Y], [Z]) to separate target entity names in the template (See example in Figure <ref type="figure" target="#fig_0">2</ref>). Then, the original text se-quence and the template are mapped to a sequence of continuous vectors via the token embedding layer. After this transformation, a few learnable vectors, which are jointly optimised by gradient descent, are inserted in front of token embeddings corresponding to these three sentinel tokens. Finally, the new sequence of token embeddings, which is summed together with the position embedding, is fed into the encoder.</p><p>To use entity type information to influence the choice of possible candidate relations, Han et al. append the entity type tokens as part of the initial decoder inputs. We denote this variant called GenPT as PrT and the variant without entity type information-the initial decoder input is '&lt;s&gt; [Z]'as PrU. them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Other Baselines</head><p>• Entity name only: we keep two target entities and remove all other tokens. Taking the sentence in Figure <ref type="figure">1</ref> as an example, the sentence becomes 'Rx cramps' and is fed as input to the sentence classifier.</p><p>• Entity type only: instead of entity names, we use entity type and remove all other tokens. The example sentence in Figure <ref type="figure">1</ref> becomes 'Treatment Problem'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Datasets and Experimental Setup</head><p>We choose six datasets-all in English-that are sampled from four different domains: TACRED and RETACRED by <ref type="bibr" target="#b46">(Zhang et al., 2017)</ref> and <ref type="bibr" target="#b32">(Stoica et al., 2021)</ref> are sampled from newswire and the web. Fortyone relations, such as per:date_of_birth and org:shareholders exist in TACRED. The original relation labels of TACRED are obtained by crowd-sourcing, and the later work <ref type="bibr" target="#b1">(Alt et al., 2020;</ref><ref type="bibr" target="#b32">Stoica et al., 2021)</ref> show that the quality of crowd-sourced annotations is a major factor contributing to the overall error rate of models on TACRED. The descriptive statistics of the datasets are listed in Table <ref type="table" target="#tab_1">2</ref>. On TACRED, RETACRED and RAD-GRAPH, we use the official train-dev-test split. We use the split of SCIERC from <ref type="bibr" target="#b12">Gururangan et al. (2020)</ref>; both DDI 2013 and I2B2-2010 from the BLUE benchmark <ref type="bibr" target="#b26">(Peng et al., 2019)</ref>.</p><p>We use ROBERTA-large as the backbone model in all our experiments except for the prompt-based models. We use BART-large in prompt-based experiments because BART is an encoder-decoder model that is pre-trained by reconstructing the original text from the corrupting text.</p><p>For each model variant, we fine-tune the whole model and perform a grid search to find the best combination of the number of training epochs and the learning rate on each development set. Once the best combination is found, we repeat all experiments three times using different random seeds, and medium test Micro and Macro F 1 scores are reported. In addition to evaluation results on all test examples, we follow <ref type="bibr" target="#b48">(Zhou and Chen, 2022)</ref> and report results on filtered test sets, where test examples containing entities observed in the training set are removed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head><p>The first observation from Table <ref type="table" target="#tab_3">3</ref> is that incorporating gold entity type information can improve the effectiveness of marker-based models. On four out of six datasets, inserting typed makers (MaTi) significantly outperforms using untyped markers (MaU) in terms of Micro F 1 , and the averaged improvement of Micro F 1 over all datasets is 1.4 (Macro of 2.4). Similarly, the averaged improvement of Micro F 1 using entity type information with prompt-based models is 0.6 and span-based is 0.3 (the averaged Macro F 1 of span-based models slightly decreases 0.1 when entity type information is incorporated). Secondly, we observe that marker-based models outperform span-based and prompt-based models on most of these evaluations, except on RETACRED and SCIERC, where prompt-based models achieve the highest Micro or Macro F 1 . Thirdly, replacing entity names using typed markers (MaTr) under-performs inserting typed markers before and after entity names (MaTi) with a large margin (on average 2.3 Micro F 1 ). Fi-nally, we observe that the benefits of incorporating entity type information seem to be dataset dependent. For example, MaTi significantly outperforms MaU on SCIERC, TACRED, and RADGRAPH in terms of both Micro and Macro F 1 scores. PrT significantly outperforms PrU on I2B2-2010, RE-TACRED, TACRED, and RADGRAPH in terms of both F 1 scores.</p><p>When RC models are evaluated on examples containing unseen entities (Table <ref type="table" target="#tab_4">4</ref>), we can see incorporating entity type information brings larger improvements compared to results on the complete test set (on average 1.0 vs 0.3 with span-based; 3.4 vs 1.4 with marker-based; and 4.4 vs 1.4 with prompt-based models). This result shows that using entity type information improves the generalisation of the RC models to unseen entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effectiveness Drop due to Incorrect Entity Type Information</head><p>After we investigate the benefits of incorporating gold entity type information to the RC model, the next question is: what will happen if incorrect entity type information is used during inference? We believe that gold (human-annotated) entity type information may be available on a small scale and can be used to train the RC model. However, it is impractical to expect entity type information to be always correct when the RC model is employed in the wild. Therefore, we focus on analysing the robustness of RC models against incorrect entity types and measuring how the effectiveness of trained RC models is affected by the correctness of entity type information during inference.</p><p>For each target entity in the test example, we use a binomial distribution, p ∈ [0, 1], to randomly decide whether its entity type should be corrupted.</p><p>If yes, we select another entity type-the incorrect type with the highest output probability based on a span-based NER model-as the replacement. Note that we also investigate randomly sampling erroneous entity types and observe a pattern similar to NER-based errors. We train span-based NER models (Zhong and Chen, 2021; <ref type="bibr" target="#b7">Dai and Karimi, 2022)</ref> separately on the corresponding training sets using entity annotations. The model enumerates all possible spans and determines whether a span is a valid entity and its entity type. The accuracy-given an entity name in the context, predict its entity type-of these trained NER models are high on DDI 2013 (93.4), I2B2-2010 (91.4), RADGRAPH (91.7) and relatively low on RETACRED (71.7), TACRED (71.4), SCIERC (71.6). Two possible factors are causing this accuracy divergence. On the one hand, this difference reflects that classifying entity names of different types has various levels of inherent difficulty (e.g., it may be easy to identify drug names in DDI 2013, but difficult to identify the metric names in SCIERC). On the other hand, the low accuracy on some datasets can be attributed to the scarcity of entity annotations or the noisy annotations (e.g., entity names in RETACRED and TA-CRED are not fully annotated).</p><p>The sensitivity analysis results show that spanbased models are robust against incorrect entity types (Figure <ref type="figure">3a</ref>). When entity types are incorrect, the RC models still maintain similar effectiveness as the gold entity types used. In contrast, incorrect entity types cause large effectiveness drop on some (but not all) datasets with mark-based models (Figure <ref type="figure">3b</ref>) and moderate drop with promptbased models (Figure <ref type="figure">3c</ref>). For example, when 10% of entity types are incorrect (p = 0.1), markerbased models have had great effectiveness drop on I2B2-2010 (8.7), RETACRED (9.1), TACRED (6.3), and RADGRAPH (6.7). We argue this result shows state-of-the-art models <ref type="bibr" target="#b48">(Zhou and Chen, 2022</ref>)-inserting typed markers before and after entity names-to be a questionable design option in practice, although they indeed achieve the highest F 1 scores on most of the evaluations when gold entity type information is used. It is also worth noting that on DDI 2013 and SCIERC, even when 50% of target entities have incorrect entity types, the drop of mark-based models is still very small (0.0 and 1.2, respectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">What can associations between relation and entity types tell us?</head><p>To understand why on some, but not all, datasets marker-based and prompt-based models have performance drop using incorrect entity types, we use Pointwise Mutual Information (PMI), an association measure to quantify the strength of associ- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>381</head><p>where |D| is the total number of examples, #(r) is the frequency of relation, #(e) is the frequency of entity type pair, #(r, e) is the frequency r and e occur.</p><p>The measured PMI values on different datasets are shown in Figure <ref type="figure" target="#fig_3">4</ref>. On DDI 2013 and SCIERC, possible combinations of relation and entity type pairs are more evenly distributed and centred at zero, indicating the strength of association on these datasets is weak. Therefore, even if a large portion of incorrect entity types are provided, the RC model is still able to make the correct prediction (see the negligible drop in Figure <ref type="figure">3</ref>). In contrast, values of other datasets have more imbalance distribution across a larger range. It indicates that relation types have stronger-either positive or negativeassociation with entity type pairs. Therefore, if incorrect entity types are provided, the RC model is more likely to make a wrong prediction (see the large drop in Figure <ref type="figure">3b</ref> and Figure <ref type="figure">3c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">A closer look at the examples</head><p>We provide a few representative examples in this section to demonstrate how (incorrect) entity type information might affect the effectiveness of RC models:</p><p>• The correct entity type information helps the relation prediction. For example, given the sentence taken from the SciERC dataset, 'Hitherto , smooth motion has been encouraged using a trajectory basis , yielding a hard combinatorial problem with time complexity growing exponentially in the number of frames .', the marker-based approach (MaU) predicts the relation between 'time complexity' and 'hard combinatorial problem' is 'FEATURE-OF' when no entity type information used. This prediction is likely to be influenced by the preposition 'with' between these two entity mentions. However, once the correct entity type information ('Metric' and 'Task') is given, the model (MaTi) correctly predicts the 'EVALUATE-FOR' relation.</p><p>• The incorrect entity type information causes erroneous predictions, whereas models without using entity type and models using correct entity type succeed. For example, given the sentence taken from the TACRED dataset, 'The troubled insurance giant , which has received multiple federal bailouts since September , said that it would give the New York Fed preferred stakes in two of the company 's crown jewels Asian-based American International Assurance , or AIA , and American Life Insurance Co. , or Alico , which operates in more than 50 countries .', both the model without using entity type information and the one using gold entity type information can predict correctly the relation between 'Alico' and 'American Life Insurance Co.' is 'org:alternate_names'. However, when the NER model makes a mistake and recognises 'Alico' as a person name, the relation model is negatively affected and predicts the relation 'org:top_members/employees', a common relation between a persona and an organisation.</p><p>• Incorrect entity type information does not cause erroneous relation predictions. For example, given the sentence taken from the SciERC dataset, 'Amorph recognises NE items in two stages : dictionary lookup and rule application .', models with incorrect entity type-NER model predicts both 'dictionary lookup' and 'rule application' as 'Task' instead of 'Method'-can still predict the relation between 'dictionary lookup' and 'rule ap-plication' as 'conjunction' relation due to the existence of the conjunction between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Implications</head><p>RC models are usually employed as subcomponents of IE systems, and entity type information is generated using an automated NER system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Relation Classification (RC) is an active area of research for a number of applications, such as knowledge base construction and biomedical text mining. The existing methods often heavily rely on entity information, especially entity type information. We conduct a comparison of methods of incorporating entity type information into the RC models on six datasets across four different domains.</p><p>Results show that when gold entity type information is available, inserting typed markers before and after target entities and using token representations corresponding to these typed markers for relation representation is effective. However, when entity types become inaccurate, methods that rely on typed markers become less effective on some (but not all) datasets. In contrast, span-based methods that use token representations to build span representation and then relation representation are robust when incorrect entity types are provided. The latter is a more realistic scenario, given NER models are practically never perfect. The promptbased method that uses entity type information as part of the initial decoder inputs is located in the middle of the spectrum. It is also affected by the incorrect entity types, but its performance drop is much smaller than the one with marker-based models.</p><p>We found that Pointwise Mutual Information, a measure to quantify the association between relation and entity type pairs, can explain why on some datasets entity type errors cause large effectiveness drops. We suggest it as a cheap yet effective tool to understand the dataset and help the decision about how to use entity type for the RC model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Our work is motivated by debates on the usefulness of entity type information for relation classification. We investigated how the effectiveness of relation classification models is affected by the correctness of entity type information. However, the effectiveness of relation classification models can be affected by other factors, such as entity names (whether the NER model can effectively identify entity boundaries) and surrounding context (whether there is sufficient context). We leave the investigation of other factors for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>There are no known ethical concerns associated with the findings of this work. However, we acknowledge that all datasets used are in English, which does not help mitigate the inequality of NLP research across languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: A high-level illustration of prompt-based RC model. Note that [X], [Y], [Z] are three sentinel tokens in the template, and we omit trainable prompt embeddings in the template for the sake of simplicity. We refer readers to<ref type="bibr" target="#b13">(Han et al., 2022)</ref> for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>DDI 2013 (</head><label>2013</label><figDesc><ref type="bibr" target="#b29">Segura-Bedmar et al., 2013)</ref> is sampled from biomedical publications. Four entity types-brand, drug, drug_n and groupand four relation types-advise, effect, int, and mechanism-are annotated in the dataset.I2B2-2010<ref type="bibr" target="#b33">(Uzuner et al., 2011)</ref> is sampled from clinical notes. Three entity types-test, problem and treatment-and eight relation types-TrWP, TrNAP, TeCP, TrCP, TrIP, TrAP, TeRP and PIP-are annotated in the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3: (a): negligible effectiveness drop due to incorrect entity types using span-based models; (b): large drop on some (but not all) datasets using marker-based models; (c): moderate drop using prompt-based models. High p values indicate more entity type errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Association between relation (e.g., 'TrAP') and entity type pair (e.g., ('Treatment', 'Problem')), measured using PMI values, on different datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Relation label verbalization</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>treatment</cell><cell>is</cell><cell></cell><cell>administered for medical problem</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Decoder</cell><cell></cell><cell></cell></row><row><cell>&lt;s&gt;</cell><cell>[X]</cell><cell cols="2">treatment</cell><cell>[Y]</cell><cell>problem</cell><cell>[Z]</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Encoder</cell><cell></cell><cell></cell></row><row><cell cols="3">He received Rx</cell><cell cols="3">for potassium to</cell><cell>help</cell><cell cols="2">with cramps</cell><cell>.</cell><cell>[X]</cell><cell>Rx</cell><cell>[Y]</cell><cell>cramps</cell><cell>[Z]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Template</cell></row></table><note><p>Examples of the modified input. MaU: untyped marker; MaTi: typed marker (insert); MaTr: typed marker (replace). The hidden states of underlined tokens are concatenated and used as the relation representation.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The descriptive statistics of the datasets.</figDesc><table><row><cell>• Random prediction: we count label distribu-</cell></row><row><cell>tion from the training set and assign labels to</cell></row><row><cell>test examples based on the obtained distribu-</cell></row><row><cell>tion.</cell></row><row><cell>• Sentence classification: we take the original</cell></row><row><cell>text sequence as input and use the hidden</cell></row><row><cell>states corresponding to the [CLS] token as</cell></row><row><cell>the relation representation. Since no entity</cell></row><row><cell>information is provided, the encoded relation</cell></row><row><cell>representation is sub-optimal. However, the</cell></row><row><cell>model may still learn heuristics that the sen-</cell></row><row><cell>tence mentions the relation (Rosenman et al.,</cell></row><row><cell>2020). It is also worth noting that if multiple</cell></row><row><cell>relations exist in the sentence, it is impossible</cell></row><row><cell>for this baseline model to distinguish between</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>A comparison of methods incorporating entity type information to the RC model. Both Micro F1 and Macro F1 are reported. Underlined results indicate the improvement due to the incorporation of entity types is statistically significant (Wilcoxon signed-rank test, p &lt; 0.05). The best Micro and Macro F1 results for each dataset are boldfaced.</figDesc><table><row><cell>Dataset</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Micro F 1 results on filtered test sets, where examples containing seen entities from the training sets are removed. Numbers in parentheses are the number of examples in the filtered test sets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Depending on the effectiveness of the NER and the association between relation type and entity type pairs, we suggest using different RC variants. If the association between relation and entity type pairs is weak (e.g., DDI 2013, SCIERC), we suggest using the marker-based model with entity type information used. If relation types have a strong association with entity types (e.g., I2B2-2010, RADGRAPH, TACRED, RETACRED), we suggest choosing prompt-based models if relatively accurate entity types are guaranteed or span-based models if entity types are prone to errors.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement This work is supported by the <rs type="funder">CSIRO's Precision Health Future Science Platform (PH FSP</rs>). This work is also undertaken with the assistance of resources and services from the <rs type="funder">National Computational Infrastructure (NCI)</rs>, which is supported by the <rs type="funder">Australian Government</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Enriched Attention for Robust Relation Extraction</title>
		<author>
			<persName><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jannik</forename><surname>Strötgen</surname></persName>
		</author>
		<idno>arXiv, 2104.10899</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Gabryszak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonhard</forename><surname>Hennig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Improving Relation Extraction by Pre-trained Language Representations</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Hübner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonhard</forename><surname>Hennig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In AKBC</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Position-aware Self-attention with Relative Positional Encodings for Slot Filling</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Bilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<idno>arXiv, 1807.03052</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multilingual Relation Classification via Efficient and Effective Prompting</title>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Harbecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonhard</forename><surname>Hennig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint Extraction of Entities and Relations for Opinion Recognition</title>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Breck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Constructing Biological Knowledge Bases by Extracting Information from Text Sources</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Kumlien</surname></persName>
		</author>
		<idno type="DOI">10.5555/645634.663209</idno>
	</analytic>
	<monogr>
		<title level="m">ISMB</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detecting Entities in the Astrophysics Literature: A Comparison of Word-based and Span-based Entity Recognition Methods</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarvnaz</forename><surname>Karimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WIESP@AACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Classifying Relations by Ranking with Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Bing</forename><surname>Cícero Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Span-based Joint Entity and Relation Extraction with Transformer Pre-training</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Eberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Ulges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention Guided Graph Convolutional Networks for Relation Extraction</title>
		<author>
			<persName><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Don&apos;t Stop Pretraining: Adapt Language Models to Domains and Tasks</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative Prompt Tuning for Relation Classification</title>
		<author>
			<persName><forename type="first">Jiale</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengkun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">RadGraph: Extracting Clinical Entities and Relations from Radiology Reports</title>
		<author>
			<persName><forename type="first">Saahil</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriel</forename><surname>Saporta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Du</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename><surname>Nguyen Duong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Chambon</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Matthew P Lungren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><forename type="middle">P</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName><surname>Rajpurkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Knowledge Base Population: Successful Approaches and Challenges</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Daniel S Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Span-BERT: Improving Pre-training by Representing and Predicting Spans</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-attention enhanced selective gate with entity-aware embedding for distantly supervised relation extraction</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">EntityBERT: Entity-centric Masking Strategy for Model Pretraining for the Clinical Domain</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Dligach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guergana</forename><surname>Savova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BioNLP@NAACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Joint Neural Model for Information Extraction with Global Features</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relation Classification with Entity Type Restriction</title>
		<author>
			<persName><forename type="first">Shengfei</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanhuan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-AFNLP</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures</title>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniy</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">104</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning from Context or Names? An Empirical Study on Neural Relation Extraction</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BioNLP@ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Knowledge enhanced contextual word representations</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vidur</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exposing Shallow Heuristics of Relation Extraction Models with Challenge Data</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Shachar Rosenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Jacovi</surname></persName>
		</author>
		<author>
			<persName><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Extraction of Drug-Drug Interactions from Biomedical Texts (DDIExtraction 2013)</title>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Segura-Bedmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paloma</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">María</forename><surname>Herrero-Zazo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval-2013 Task</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note>In SemEval</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><surname>Kwiatkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Context-Aware Representations for Knowledge Base Relation Extraction</title>
		<author>
			<persName><forename type="first">Daniil</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Re-TACRED: Addressing Shortcomings of the TACRED Dataset</title>
		<author>
			<persName><forename type="first">George</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Emmanouil Antonios Platanios, and Barnabás Póczos</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text</title>
		<author>
			<persName><forename type="first">Özlem</forename><surname>Uzuner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuying</forename><surname>Brett R South</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><surname>Duvall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMIA</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Combining Recurrent and Convolutional Neural Networks for Relation Classification</title>
		<author>
			<persName><forename type="first">Ngoc Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters</title>
		<author>
			<persName><forename type="first">Ruize</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cuihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Relation Extraction from Clinical Narratives Using Pre-trained Language Models</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongcheng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingcheng</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Firat</forename><surname>Tiryaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cui</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kirk</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Enriching pretrained language model with entity information for relation classification</title>
		<author>
			<persName><forename type="first">Shanchan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1145/3357384.3358119</idno>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards Realistic Lowresource Relation Extraction: A Benchmark with Empirical Baseline Study</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">LUKE: Deep Contextualized Entity Representations with Entityaware Self-attention</title>
		<author>
			<persName><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An Empirical Study of Pipeline vs. Joint Approaches to Entity and Relation Extraction</title>
		<author>
			<persName><forename type="first">Zhaohui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Named Entity Recognition as Dependency Parsing</title>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">OpenKI: Integrating Open Information Extraction and Knowledge Bases with Relation Inference</title>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Lockard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luna</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<idno>arXiv, 1508.01006</idno>
		<title level="m">Relation Classification via Recurrent Neural Network</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graph Convolution over Pruned Dependency Trees Improves Relation Extraction</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Position-aware attention and supervised data improve slot filling</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">ERNIE: Enhanced Language Representation with Informative Entities</title>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL. Zexuan Zhong and Danqi Chen. 2021. A Frustratingly Easy Approach for Entity and Relation Extraction</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>NAACL</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An Improved Baseline for Sentence-level Relation Extraction</title>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<ptr target="https://catalog.ldc.upenn.edu/LDC2018T24" />
		<title level="m">A Resources and Downloads TACRED</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<ptr target="https://github.com/ncbi-nlp/BLUE_BenchmarkI2B2-2010https://github.com/ncbi-nlp/BLUE_BenchmarkROBERTA-largehttps://huggingface.co/roberta-largeBART-largehttps://huggingface.co/facebook/bart-largeGenPThttps://github.com/hanjiale/GenPT" />
		<title level="m">RETACRED</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
