<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SPEC5G: A Dataset for 5G Cellular Network Protocol Analysis</title>
				<funder ref="#_CJ9Vxc7">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Imtiaz</forename><surname>Karim</surname></persName>
							<email>karim7@purdue.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Samin</forename><surname>Kazi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mirza</forename><forename type="middle">Masfiqur</forename><surname>Mubasshir</surname></persName>
							<email>kmubassh@purdue.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elisa</forename><surname>Rahman</surname></persName>
							<email>rahman75@purdue.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Bertino</surname></persName>
							<email>bertino@purdue.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SPEC5G: A Dataset for 5G Cellular Network Protocol Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">996819DDD90D1EF058A9BE4867BB31AA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>5G is the 5 th generation state-of-the-art cellular network protocol designed to connect virtually everyone and everything with increased speed and reduced latency. Therefore, its development, analysis, and security are critical. However, all approaches to the 5G protocol development and security analysis, e.g., property extraction, protocol summarization, and semantic analysis of the protocol specifications and implementations are completely manual. To reduce such manual efforts, in this paper, we curate SPEC5G-the first-ever public 5G dataset for NLP research. The dataset contains 3,547,587 sentences with 134M words, from 13094 cellular network specifications and 13 online websites. By leveraging large-scale pre-trained language models that have achieved state-of-the-art results on NLP tasks, we use this dataset for security-related text classification and summarization. Security-related text classification can be used to extract relevant security-related properties for protocol testing. On the other hand, summarization can help developers and practitioners understand the highlevel idea of the protocol, which is itself a daunting task. To ensure the research community can benefit from this work, all the datasets and accompanying codebase are made publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The deployment of the 5G cellular network protocol has generated a lot of enthusiasm in academia and industry, because of its promise of enabling innovative applications, such as autonomous vehicles <ref type="bibr" target="#b4">(Ahmad et al., 2020)</ref>, remote surgery <ref type="bibr">(sur, 2022)</ref>, industrial IoT <ref type="bibr" target="#b47">(Satyanarayanan, 2017)</ref>, augmented reality <ref type="bibr">(Zhang et al., 2017)</ref>, and multiplayer online gaming <ref type="bibr">(scr, 2022)</ref>. Therefore the security of 5G protocol is critical. Unfortunately, the 5G protocol development and analysis are all 1 Datasets and codebase for SPEC5G are publicly available at https://github.com/Imtiazkarimik23/SPEC5G completely manual tasks requiring domain expertise. We observe that for 5G there is an unutilized resource of information available in the form of specifications <ref type="bibr">(Spe, 2022)</ref> and numerous tutorials on the Internet. These resources have not yet been utilized.</p><p>Recently, a few approaches have been proposed that leverage natural language processing (NLP) and machine learning (ML) to detect risky operations in some of the specifications of 4G LTE <ref type="bibr" target="#b12">(Chen et al., 2021)</ref> and to analyze change requests <ref type="bibr" target="#b11">(Chen et al., 2022)</ref>. These approaches are very limited, not generalizable, and not open-source. Automatic and systematic analysis of 5G networks is still a difficult task. One major problem is the lack of highquality datasets to train ML models, which would enable the automation of different 5G-related downstream tasks e.g., security-related text classification, protocol summarization, semantic analysis, and automatic programming. In this paper, we address this need by introducing SPEC5G, a high-quality dataset of the 5G protocol specifications. 5G is not a single wireless technology, but an umbrella term used to categorize the fifth generation of wireless communication, including hundreds of different protocols at different layers of the protocol. Some of these protocols are VoWiFi, cellular IoT, IKE, and 5G-AKA. SPEC5G is a complete dataset that covers all these protocols and therefore, has the potential to impact different protocols affecting billions of devices. Such a high-quality dataset would be beneficial to numerous applications in different domains, such as security testing, policy enforcement, automatic code generation, and protocol summarization. It would encourage research and development in novel NLP tasks that are communication protocol-specific and critical for the security analysis of these protocols. Notable examples include formal model extraction from large-scale natural language documents and identifications of conflicting security guidelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training of Language Models</head><p>Fine-tuning of Language Models The use of ciphering in a network is an operator option. In this subclause, for the ease of description, it is assumed that ciphering is used, unless explicitly indicated otherwise. Operation of a network without ciphering is achieved by configuring the AMF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simplification Classification</head><p>Ciphering is operator dependent and operation of a network without ciphering is achieved by configuring the AMF. To show the viability of our SPEC5G dataset, we use it for two downstream tasks (shown in Figure <ref type="figure" target="#fig_1">1</ref>). First, we use it for security-text classification. In previous 5G security testing <ref type="bibr" target="#b5">(Basin et al., 2018;</ref><ref type="bibr" target="#b17">Cremers and Dehnel-Wild, 2019;</ref><ref type="bibr" target="#b21">Hussain et al., 2019)</ref> the properties are manually extracted from the specifications. Using security-text classification, we can automatically identify texts, which specify important security properties to be used for formal verification and other testing approaches. Second, we use SPEC5G for the paragraph summarization task. The 5G specification is large and complex with specialized jargon, mostly due to backward compatibility requirements. Therefore, it is really daunting for a software developer to understand the high-level ideas of the protocol specification. With the summarization task, we show that it is possible to summarize and simplify the high-level ideas of the protocol. To achieve those tasks, we created two expert-annotated datasets: one for summarization and one for classification. The summarization dataset contains 713 long articles and their concise summaries. The classification dataset contains 2401 sentences and their class labels (Non-Security, Security, Undefined). Both datasets were annotated by multiple domain experts to ensure quality and fairness. Along with SPEC5G, these two expert annotated datasets have been open-sourced to enhance research.</p><p>On the whole, our contributions are threefold. First, we create the first-ever novel 5G dataset (SPEC5G) of 3,547,587 sentences by preprocessing the 5G specification and scraping data from different 5G tutorials on the Internet. Second, we create two expert-annotated datasets for baseline security-text classification and summarization tasks. We conduct an extensive evaluation of these datasets using several NLP models on the downstream tasks. The results show that the models pre-trained on SPEC5G outperform all baseline models. Third, all these research artifacts have been made available via a public repository. To the best of our knowledge, this is the first-ever public 5G dataset created for NLP research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The introduction of the attention-based transformer architecture by <ref type="bibr" target="#b55">(Vaswani et al., 2017)</ref> beaconed the era of transformer-based Language Models (LM) in the field of NLP. A range of high-performing transformer-based language models have since been proposed, each with its own specific use cases. To train such LMs, high-quality large datasets are critical. In the following, we will discuss the research relevant to our work. Cellular Networks Research Using NLP. CREEK <ref type="bibr" target="#b11">(Chen et al., 2022)</ref> uses BERT models for detecting security-relevant change requests. For this, they pre-train BERT with a subset of 4G LTE specifications (1546 out of 13094). Moreover, in ATOMIC <ref type="bibr" target="#b12">(Chen et al., 2021)</ref> they design a framework to semantically analyze LTE documents using NLP to obtain a set of hazard indicators for generating test cases based on a given threat model. These are the first steps in applying NLP techniques to analyze cellular network specifications. In a technical blog post from <ref type="bibr">Erricson (err, 2022)</ref>, the authors adopt LMs for the telecom domain and create a telecom question-answering dataset. Though promising, these approaches do not generalize and are ad-hoc and closed-source, thus accentuating the need for a complete and public dataset for 5G. Summarization. Following BookCorpus and Wikidata, researchers have built summarization datasets such as Wikilarge <ref type="bibr" target="#b63">(Zhang and Lapata, 2017)</ref>, Wikismall <ref type="bibr" target="#b67">(Zhu et al., 2010)</ref>, and so on <ref type="bibr" target="#b16">(Coster and Kauchak, 2011;</ref><ref type="bibr" target="#b26">Kauchak, 2013)</ref>. Such datasets are widely used in the field of sentence summarization. Early summarization models mostly relied on statistical machine translation <ref type="bibr" target="#b57">(Wubben et al., 2012;</ref><ref type="bibr" target="#b38">Narayan and Gardent, 2014)</ref>. Improvements of the machine translation model to obtain a new summarization model are done by <ref type="bibr" target="#b40">(Nisioi et al., 2017)</ref> and investigations on how to simplify sentences to different difficulty levels are conducted after this <ref type="bibr" target="#b48">(Scarton and Specia, 2018;</ref><ref type="bibr" target="#b39">Nishihara et al., 2019)</ref>. Sentence alignment methods to improve sentence summarization are proposed by <ref type="bibr">(Štajner et al., 2017)</ref> and <ref type="bibr" target="#b25">(Jiang et al., 2020)</ref>. There are several corpora related to summarization. A large-scale, human-annotated scientific papers corpus is provided by <ref type="bibr" target="#b60">(Yasunaga et al., 2019)</ref>. This corpus provides over 1,000 papers in the ACL anthology with their citation networks (e.g., citation sentences, citation counts) and their comprehensive, manual summaries. There is another dataset that has been created for the Computational Linguistics Scientific Document Summarization Shared Task which started in 2014 as a pilot <ref type="bibr">(Jaidka et al., 2014)</ref> and which is now a well-developed challenge in its fourth year <ref type="bibr" target="#b24">(Jaidka et al., 2018</ref><ref type="bibr" target="#b23">(Jaidka et al., , 2017))</ref>. A new dataset for summarisation of computer science publications by exploiting a large resource of the author-provided summaries is introduced by <ref type="bibr" target="#b15">(Collins et al., 2017)</ref>.</p><p>Sentence Classification. The Corpus of Linguistic Acceptability (CoLA) <ref type="bibr" target="#b56">(Warstadt et al., 2018)</ref> consists of English acceptability judgments drawn from books and journal articles on linguistic theory. Each example is a sequence of words annotated with whether it is an English grammatical sentence. The Stanford Sentiment Treebank <ref type="bibr" target="#b51">(Socher et al., 2013)</ref> consists of sentences from movie reviews and human annotations of their sentiment. Sci-Cite <ref type="bibr">(Cohan et al., 2019a</ref>) is a large dataset of citation intents for the task of automated analysis of scientific papers by identifying the intent of a citation (e.g., background information, use of methods, comparing results). Researchers have also leveraged other large datasets such as DEFT <ref type="bibr" target="#b52">(Spala et al., 2019)</ref> and ACL-ARC <ref type="bibr" target="#b7">(Bird et al., 2008)</ref> for the sentence classification tasks. CSABSTRUCT <ref type="bibr">(Cohan et al., 2019b)</ref> is another new dataset of manually annotated sentences from computer science abstracts for Sequential Sentence Classification (SSC). Paper Field <ref type="bibr" target="#b50">(Sinha et al., 2015)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset Curation</head><p>In this section, we discuss the collection and preparation of our dataset. A significant amount of data was collected from the 3GPP website (Spe, 2022).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">3GPP</head><p>The 3rd Generation Partnership Project (3GPP) is an umbrella organization that hosts several organizations from different countries. 3GPP is globally considered the issuer of standards for cellular network protocols. These standards are publicized as releases, e.g., LTE standards were made public from Release 8 and 5G standards from Release 15. The current release is Release 19.</p><p>A large number of meeting minutes, Technical Reports (TR) can be found on the 3GPP FTP server <ref type="bibr">(Spe, 2022)</ref>. 3GPP releases a set of Technical Specifications (TS) as well, which subsequently add features and bug fixes. Figure <ref type="figure">2</ref> shows the count of specification documents per release.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset Collection</head><p>As stated earlier a significant portion of the dataset has been collected from the 3GPP FTP server. Automated NLP tasks have been hindered in the 5G domain because of noisy data in the standard doc-umentation. Often, the specification documents contain embedded codes, tables, and lists with definitions of varying terminologies, flow diagrams, finite state machines, and so on-which makes it hard to build models that reason and perform well on downstream applications.</p><p>Thus, to leverage downstream NLP tasks, we perform extensive preprocessing. Furthermore, we scrape data from 13 blogs, and forums of the internet. The web sources are listed in Table <ref type="table" target="#tab_6">4</ref> and details about the web sources can be found in Appendix C.5. We extract approximately 17 GB of text data from specification releases and web portals using python web scrapper and <ref type="bibr">Selenium (sel, 2022)</ref>. Later we apply a set of standard and domainspecific preprocessing to obtain the final dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Preprocessing</head><p>5G specifications and web data contain a variety of materials encompassing method and framework documentation, pseudocode, high-level implementations, numerous parameters, field constitution, and so on. At first, the raw data go through standard NLP preprocessing tasks, e.g., removing extra whitespaces, tabs, certain Unicode characters introduced from scrapping, HTML tags, etc. Later, we extend the preprocessing to handle special cases such as code snippets, tables, figures, references to other specification documents, etc. For the list of preprocessing tasks, we refer the reader to Appendix A.1. Finally, this dataset is used to pre-train baseline models for downstream applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Dataset Statistics</head><p>Our final processed dataset contains 3,547,587 sentences with a total of 134M words. Figure <ref type="figure" target="#fig_3">3</ref> shows the distribution of the number of sentences per document and Figure <ref type="figure">4</ref> shows the distribution of tokens per sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Annotation</head><p>To demonstrate the effectiveness of SPEC5G, we additionally create and annotate two datasets specific to two NLP tasks -summarization and sentence classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Summarization</head><p>To prepare the summarization dataset, we randomly select 1500 locations to retrieve articles from the SPEC5G dataset. An article is defined as a sequential collection of sentences. Here we apply another round of manual processing to ensure semantic correctness among the sentences of each of   For the rest of this paper, we refer to this annotated dataset as 5GSum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Security Classification</head><p>Similar to the summarization task, we randomly select and annotate 2401 sentences from our SPEC5G dataset to use for multi-class classification. We categorize the data into 3 classes-Non-Security (0), Security (1), and Undefined (2). To discard human bias, the dataset has been labeled by 9 domain experts. We do a 85-5-10 split for train, validation, and test data with 2040, 120, and 241 samples respectively. For the rest of the paper, we refer to this dataset as 5GSC.</p><p>Among the 3 classes, the least number of samples are from class 2 (Undefined: 484). Yet, the class with the highest number of samples (Non-Security: 1303) is about 3 times more than the class with the lowest number of samples. Therefore, the dataset is not highly imbalanced. Overall, this non-uniformity is expected, since most of the specification documents should not be related to Security issues and a high amount of Undefined statements in 5G specifications would rather mean inconsistencies in implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Tasks</head><p>In this section, we define the downstream tasks: summarization and security sentence classification. Moreover, we discuss the relevance of these downstream tasks with respect to 5G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task 1: Summarization</head><p>Text summarization is the simplification of the original text to a more understandable text while keeping the main meaning of the original text unchanged <ref type="bibr" target="#b54">(Štajner and Saggion, 2018;</ref><ref type="bibr" target="#b33">Maddela et al., 2020)</ref>. It can provide convenience for nonnative speakers <ref type="bibr" target="#b43">(Petersen and Ostendorf, 2007;</ref><ref type="bibr" target="#b20">Glavaš and Štajner, 2015;</ref><ref type="bibr" target="#b42">Paetzold and Specia, 2016)</ref>, non-expert readers <ref type="bibr" target="#b19">(Elhadad and Sutaria, 2007;</ref><ref type="bibr" target="#b49">Siddharthan and Katsos, 2010)</ref>. In the case of 5G standard documents, summarization can help developers and practitioners understand the highlevel idea of the protocol, which can be really timeconsuming without the summarization.</p><p>The document-level text summarization task can be defined as follows. Let C be an original com-plex article; suppose that C consists of n sentences, denoted as C = S 1 , S 2 , . . . , S n . Document-level summarization aims to simplify C into m sentences, which form the simplified article F , denoted as F = T 1 , T 2 , . . . , T m , where m is not necessarily equal to n. F retains the primary meaning of C and is more straightforward than C, making it easier for people to understand. The operations for sentence-level summarization include word reservation and deletion, synonym replacement <ref type="bibr" target="#b58">(Xu et al., 2016)</ref>. In our definition, document-level summarization should allow the loss of information but should not allow the loss of important information. The fact that sentence deletion is a prevalent phenomenon in document summarization is pointed out by <ref type="bibr" target="#b66">(Zhong et al., 2019)</ref>. We believe that information that has little relevance to the primary meaning should be removed to improve readability.</p><p>The objective is to simplify a paragraph without losing important information. Task 1 is more challenging when evaluating a model's ability to reason about unobserved effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Task 2: Sentence Classification</head><p>Text classification is a classic topic for natural language processing, in which one needs to assign predefined categories to free-text documents. The range of text classification research goes from designing the best features to choosing the best possible machine learning classifiers <ref type="bibr" target="#b35">(Mekala et al., 2021;</ref><ref type="bibr" target="#b32">Liu et al., 2021;</ref><ref type="bibr" target="#b65">Zhang et al., 2022)</ref>.</p><p>The multi-class sentence classification can be defined as follows. Given a sentence s ∈ S, where S is some high dimensional sentence space and a finite set of categories or classes C = {c 1 , c 2 , . . . , c n }, the objective of multi-class sentence classification is to find a function F mapping sentences to categories, formally,</p><formula xml:id="formula_0">F : S → C. Given a dataset D of m training samples {(s i , c i )} m i=1</formula><p>, we aim to learn the function F that approximates F.</p><p>For protocol analysis, an important step is property-guided testing <ref type="bibr" target="#b21">(Hussain et al., 2019)</ref>. Up to this point, the properties are manually extracted, and the testing is entirely manual. The security classification task aims to label the security-related sentences that in turn can be used as properties and enable semi-automated testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Evaluation</head><p>In this section we provide a comprehensive assessment of the proposed methodology through rigorous experimentation and evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setup</head><p>Baseline Models: For baseline models we use base versions of BERT <ref type="bibr" target="#b18">(Devlin et al., 2018)</ref>, RoBERTa <ref type="bibr" target="#b31">(Liu et al., 2019)</ref>, XLNet <ref type="bibr" target="#b66">(Yang et al., 2019)</ref>, BART <ref type="bibr" target="#b29">(Lewis et al., 2019)</ref>, GPT2 (Radford et al., 2019), T5 <ref type="bibr" target="#b45">(Raffel et al., 2019)</ref>, AL-BERT <ref type="bibr" target="#b27">(Lan et al., 2019)</ref>, CamemBERT <ref type="bibr" target="#b34">(Martin et al., 2019)</ref>, LongFormer <ref type="bibr" target="#b6">(Beltagy et al., 2020)</ref>, Pegasus <ref type="bibr" target="#b61">(Zhang et al., 2019)</ref>; large versions of GPT2 and mBART <ref type="bibr" target="#b30">(Liu et al., 2020)</ref>; medium version of GPT2; DistilGPT2 and DistilBERT <ref type="bibr" target="#b46">(Sanh et al., 2019)</ref>. Pre-trained Models: We pre-train three models-BERT-base, ROBERTa-base, and XLNet-base, on the SPEC5G dataset; we refer to them as BERT5G, ROBERTa5G, and XLNet5G respectively. The reason for training these three models is discussed in Section 7 (Choice of Pre-trained Models). We then fine-tune the pre-trained models for the downstream tasks. The details of the pre-training and fine-tuning are discussed in detail in Section A.2. Training Hardware: We use Google Colab Pro+ to pre-train and fine-tune the models. Around 3000 computing units (CU) of Premium GPU (A100) with high RAM configuration have been consumed to complete all our experiments. For details about CU and training time, we refer to Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Metric</head><p>To measure the performance of the sentence classification task we use standard performance metrics such as accuracy, precision, recall, and F1-score. We discuss the metrics for the summarization task here in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Summarization Metrics</head><p>To measure the quality of summarization we use both automatic and human evaluation metrics. For automatic evaluation, we use the commonly used ROUGE score. Human Evaluation Metric: Due to significant dissonance with human evaluation, automatic evaluation metrics are often considered unreliable for summarization quality evaluation. Hence, we resort to human evaluation metrics. The human annotator's rate on a scale from 1 (worst) to 5 (best) on Whether the generated or annotated inferences fit the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance</head><p>We now report the performance of the baseline language models to characterize our dataset. Summarization: We report the models' mid-score fmeasure of ROUGE-1, ROUGE-2, and ROUGE-L in Table <ref type="table">1</ref> and human evaluation scores in Table <ref type="table" target="#tab_3">3</ref> to show the comparison of the baseline models with the pre-trained models. BERT5G outperforms all the models, though BERT-base was not the bestperforming model. This shows the quality of our dataset for technical specification learning.</p><p>Sentence Classification: We report the performance of the models on the sentence classification task in Table <ref type="table" target="#tab_2">2</ref>. For sentence classification with relatively few classes, BERT, ROBERTa, and XLNet perform the best <ref type="bibr" target="#b10">(Chang et al., 2020)</ref>. Therefore, we pre-train 3 models-BERT-base, ROBERTa-base, and XLNet-base language models on the SPEC5G dataset. These 3 models along with other baselines are then fine-tuned on the 5GSC dataset to compare their classification performance. We observe that BERT5G, ROBERTa5G, and XLNet5G outperform their corresponding baselines by a significant margin. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Result Analysis</head><p>Performance Improvements Due to SPEC5G:</p><p>The primary objective of our work is to introduce an anchor 5G dataset that might pave the way for future NLP research in 5G and NLP. The models pre-trained on 5G and fine-tuned on respective tasks achieve significant performance improvements, suggesting that such dataset should be considered the gold standard for pre-training models before deploying them for more sophisticated, 5Goriented NLP applications.</p><p>Best Pre-trained Model: The scores of both tasks show that BERT5G is the best-performing model. It is not surprising that XLNet5G, the pre-trained version of more recent BERT variant XLNet, is a close competitor. While GPT2 is a good choice for our summarization task, we do not recommend it for security classification. Despite a high accuracy score, GPT2 failed to achieve a contending recall or F1-score. We observe that GPT2 could classify the Non-Security samples well (53 out of 70 test samples were correct) which dominates the dataset distribution, while poorly classifying samples from Security (11 out of 24 correct) and Undefined (4 out of 11 correct). This is the reason for its higher accuracy yet low recall and F1.</p><p>Results of Human Evaluation for Summarization: We randomly sample 40 inferences generated by each pre-trained model, their non-pre-trained versions, and corresponding gold inference. These inferences are then manually rated by three independent annotators based on the human-evaluation metrics. As shown in Table <ref type="table" target="#tab_3">3</ref>, we observe that the fine-tuned models perform similarly on SPEC5G but fail to reach gold annotation performance. Moreover, as expected, the pre-trained models significantly outperform their non-pre-trained counterparts. We provide some examples of the generated inferences in Figure <ref type="figure" target="#fig_5">6</ref>. Inspection of the model-generated inferences reveals that the usage of keywords from the technical specifications is more frequent in inferences generated by models pre-trained on SPEC5G. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Broader Impact of SPEC5G. Our dataset can offer valuable insights and applications that extend beyond the immediate scope. It can be very useful also for other specialized communication protocols (like IoT, Bluetooth, Bluetooth Low Energy, Vehicular Protocols, and WiFi). One popular methodology to evaluate the design of communication protocols is to manually extract a formal model, for example in terms of finite state machines, of the protocol and evaluate the model against the desired security and privacy properties <ref type="bibr" target="#b21">(Hussain et al., 2019)</ref>. One major issue with this approach is that the manual model extraction from the protocol-text is error-prone and not scalable. Therefore, communication protocols are analyzed partially or within a specific scope. The analysis paved by SPEC5G can yield a deeper understanding of network behavior, interference patterns, and potential optimizations that can be applied to a variety of wireless communication scenarios. This can lead to an ecosystem around our initial dataset, which could include models trained on SPEC5G and additional relevant datasets that could be combined with ours.</p><p>The versatility can spark innovation in the design and development of future-generation protocols. There is a lot of work to design technologies and protocols for interconnecting cellular networks and non-terrestrial networks (via for example, low earth orbit-LEO satellites); therefore creating datasets for these new protocols would only require addition to a modest amount of new data to SPEC5G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Article:</head><p>The PDCCH channel is used to send control information to the UEs on a per-slot basis. It is used to schedule downlink transmissions, uplink transmissions, MCS of those transmissions, and HARQ information. The PDCCH can appear on any subcarrier; so the jammer must decode the parameter CORESET freq-domain. The parameter CORESET-time-duration, which can take on values 1, 2, or 3, indicates how many OFDM symbols the PDCCH occupies in each slot. The PDCCH always starts in the first symbol of each slot, is QPSK modulated and uses polar coding.</p><p>Reference: PDCCH sends per-slot control information to UEs. Schedules downlink, uplink, MCS, and HARQ transmissions. PDCCH can exist on any subcarrier; the jammer must decode CORESET freq-domain. The CORESET-time-duration parameter shows how many OFDM symbols the PDCCH occupies each slot. The PDCCH starts at the first symbol of each slot and uses polar coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT-base:</head><p>The PDCCH channel is used to send control information to the UEs on a per-slot basis. The PDCCH can appear on any subcarrier; so the jammer must decode the parameter CORESET freq-domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT5G:</head><p>The PDCCH channel is used to send control information to the UEs on a per-slot basis. The parameter CORESET-time-duration, which can take on values 1, 2, or 3, indicates how many OFDM symbols the PDCCH occupies each slot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RoBERTa-base:</head><p>The PDCCH channel is used to send control information to the UEs on a per-slot basis. The PDCCH can appear on any subcarrier; so the jammer must decode the parameter CORESET freq-domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RoBERTa5G:</head><p>The PDCCH channel is used to send control information to the UEs on a per-slot basis. The parameter CORESET-timeduration, which can take on values 1, 2, or 3, indicates how many OFDM symbols the PDCCH occupies each slot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XLNet-base:</head><p>The PDCCH channel is used to send control information to the UEs on a per-slot basis. The PDCCH always starts in the first symbol of each slot, is QPSK modulated and uses polar coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XLNet5G:</head><p>The PDCCH channel is used to send control information to the UEs on a per-slot basis. The PDCCH always starts in the first symbol of each slot, is QPSK modulated and uses polar coding. Similarly, models trained on SPEC5G could also be tuned by using a modest amount of new data. By leveraging the insights gained from the interactions within the dataset, researchers and engineers can create protocols that can adapt to evolving communication landscapes. This adaptability will be essential as we move towards more interconnected and heterogeneous networks.</p><p>Furthermore, the utility of our dataset reaches beyond those exclusively working on 5G networks. As NLP research and natural language processing techniques continue to evolve, our dataset can serve as a foundation for various research avenues. For instance, the dataset can be employed to develop advanced predictive models, anomaly detection systems, and intelligent network management solutions. These applications are not limited to the realm of 5G but have the potential to influence and enhance NLP research across a broader spectrum.</p><p>Choice of Pre-trained Models. The motivation behind the choice of the machine learning models is to show the quality of SPEC5G. Hence we only use pre-existing models for the downstream tasks and do not measure the performance of simple baseline models like lead-3 extractive baseline (taking the first 3 sentences of the article as the summary) and the SummaRuNNer extractive model <ref type="bibr" target="#b37">(Nallapati et al., 2016)</ref>, nor improve the performance of the downstream tasks. We pick the models that perform well in both downstream tasks. Here the chosen models are all encoder-only to maintain consistency between the experiments. Nevertheless, encoder-decoder models or decoderonly models would also benefit from pre-training. It is well known that pre-training on domainspecific data can help to improve the performance of downstream tasks in the domain <ref type="bibr" target="#b28">(Lee et al., 2019;</ref><ref type="bibr" target="#b9">Chalkidis et al., 2020)</ref>. However, in our case after the first step of preprocessing, BERT only improves 2.73%, XLNet improvement is 1.96% and ROBERTa improves 0.061% in F1 score. Thus, although we commonly know that pre-training improves downstream tasks, evidently the preprocessing of the dataset signifies that process even more. The performance improvement of the base models after pre-training on our dataset indicates that the models could learn and sufficiently generalize their knowledge in technical specifications. We leave exploring the downstream tasks in detail and the criteria for the selection of different models on the technical specification domain as future work. Downstream Task Dataset Size. While the downstream task datasets may seem small, recent high-quality manually annotated datasets had similar sizes-COUGH dataset(1236 labeled sentences) <ref type="bibr" target="#b64">(Zhang et al., 2021)</ref> and YASO dataset (2215 labeled sentences) <ref type="bibr" target="#b41">(Orbach et al., 2021)</ref>. Thus, the current size is comparable to the contemporaries. To address the selection bias of the relatively short test set, the test points are randomly sampled on 3 different runs of each model, and the models are run on 3 different random seeds which show low standard deviation in performance metrics. Therefore, the randomness in the test set removes the selection bias. Moreover, this dataset can easily be used as a seed alongside our trained models for semi-automatic annotation with minimal human effort. Our work enables this direction of using language models in technical specification documents. In the case of the summarization dataset, it is only used as a test set for the models that can already summarize articles. Their performance on summarizing network protocol specification is measured using this test set. Project Maintenance. In the context of the 3GPP, major releases like 3G, 4G, and 5G are published every ten years. However, smaller, incremental functional changes are made each year within these larger frameworks. These updates are designed to be backward compatible and avoid conflicts with the previous releases, ensuring a smooth transition for existing infrastructure and devices. To address the concern with the new releases, we have devised a plan to maintain the quality and relevance of our dataset in the face of these protocol changes. After each major 3GPP release, we will analyze the changes and updates made to the protocol specifications. For each significant protocol update, we will review and re-evaluate the annotations in our dataset. This will involve identifying any modifications, additions, or clarifications in the protocol specifications. The Change Request (CR) procedure used by 3GPP to create revised versions of 3GPP specifications can be used to automatically identify the modifications. Our team will then update the dataset to accurately reflect these changes. Alongside these updates, we will provide summaries of the changes made in each protocol release. This will serve as a "TL;DR" version highlighting the key modifications that have taken place. This way, users can quickly understand what has changed in the context of each new release. Our commitment is to keep our dataset in sync with the evolving protocols and maintain its utility as a valuable resource for researchers, developers, and industry professionals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have created SPEC5G-a new dataset for 5G, 5GSum and 5GSCexpert annotated datasets for 5G protocol summarization and 5G security text classification respectively. To show the usefulness of SPEC5G in protocol specifications learning by the Language Models, we design security sentence classification and summarization tasks for state-of-the-art Language Models to solve. Future Work. Given the specialized nature of 5G terminology in the dataset, it could be utilized for domain adaptation tasks in NLP (for instance, adapting language models to understand and generate content in the context of 5G communications). The dataset could be used to create datasets for named entity recognition tasks, focusing on extracting and categorizing specific entities such as protocols, technologies, companies, and standards relevant to 5G. With the wealth of information present in the dataset, question-answering datasets could also be constructed, where models are trained to answer questions related to 5G concepts, protocols, and technologies. SPEC5G can be used to develop semantic role labeling datasets, assisting in understanding the roles and relationships of various elements in sentences discussing 5G. Datasets for document classification tasks, where the goal is to categorize entire documents or articles based on their content related to 5G concepts can also be created. With content from various sources, the dataset could be used to create parallel corpora for translating technical 5G content between different languages. SPEC5G can be utilized to develop datasets for dependency parsing tasks, improving syntactic analysis and understanding of relationships between words. Generating datasets for topic modeling tasks can help in identifying and categorizing prevalent topics within the 5G domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Limitations</head><p>Here we discuss some limitations we faced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Underspecifications in the standards</head><p>In this paper, we introduce SPEC5G, a dataset aimed at the automated analysis of the 5G protocol, and show the usefulness of SPEC5G in two downstream tasks. The performance of the two different downstream tasks on the dataset, in turn, depends on the 5G standards. In some cases, the standards are intentionally kept underspecified and contain ambiguities. The reason for such underspecifications and ambiguities is mainly to give vendors flexibility in the implementation design and performance enhancement. Nonetheless, the SPEC5G dataset can include some of the underspecified behaviors from the standards. These ambiguities existing in the text can be resolved using human expertise. This is precisely how we leverage human expertise for the two downstream tasks in the paper. However, this can be accomplished by using NLP methods that exploit unlabeled data and human knowledge. This is the direction we plan to pursue in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Automation</head><p>The aim of SPEC5G is to help automate the manual-intensive tasks of 5G protocol development, analysis, and testing using state-of-the-art NLP techniques. However, it is evident that it is still not possible to completely automate such tasks because of the manual annotation, which requires domain expertise. In spite of the limited annotated data, we show that it is still possible to achieve fairly good results in two downstream tasks. It may not be possible to completely automate the 5G related tasks, but we still hope it can help reduce the large manual efforts which is the current state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Ethical Considerations</head><p>In regards to the datasets being released, all information is in the public domain and is not subject to any copyrights. To pre-train, we use different language models. It has been reported that the pretrained masked language models encode unfair social biases such as gender, racial bias, and religious biases <ref type="bibr" target="#b8">(Bommasani et al., 2020)</ref>. In our case, as we are dealing with a technical domain, we believe these biases do not have any impact on our results. Moreover, we randomly evaluated the model's outputs and found no evidence of these biases. In the case of annotations, the annotators for SPEC5G are all Ph.D. students doing active research in the area of networks. They are provided with specific guidelines (discussed in detail in Appendix C) and are strictly asked not to write any toxic content (hateful or offensive toward any gender, race, sex, or religion) and to consider gender-neutral settings.</p><p>• Starting numbers, dots, interpuncts, and hyphens appearing from (un)ordered lists are removed. • Additional whitespaces after opening parentheses, curly braces, and brackets are removed. Similar to closing ones. • 3GPP specifications contain numerous mentions of specification documents (i.e. TS 24.301). These do not add any useful features for learning. Those are renamed as "specification document". • If a sentence contains a high amount of digits, they necessarily are from embedded codes.</p><p>If more than 20% are seen, we filter out the sentence. • Few special cases (for example: "e.g.,", "i.e.,") are handled to not be considered as the end of a sentence. • An additional newline is added after adding all texts from each of the documents/web pages. This is to ensure that certain downstream applications (e.g., summarization) do not get affected by unrelated texts from multiple documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training Details</head><p>To pre-train BERT Masked Language Model (MLM), we use the Adam optimizer with ϵ = 10 -8 and train the model for 10 epochs. The learning rate is 5 × 10 -5 , we set aside 10% of the data as validation to inspect the model performance at every 50k steps. BERTFastTokenizer has been used to tokenize the dataset. We use the same parameters to pre-train ROBERTa MLM for 5 epochs and ROBERTa BPE tokenizer to tokenize the dataset for this setting. For pre-training XLNet Permutation Language Model (PLM), we use the Adam optimizer in the same setting. Since XLNet requires approximately 5 times more than BERT or ROBERTa, we train the model for 1 epoch. We use the SentencePiece tokenizer in this case. When fine-tuning the classification models, we set the learning rate to be 2×10 -5 , weight decay to be 0.01, and batch size to be 16. The Huggingface standard pipeline with the Automodel class has been used for sequence classification. We train each model for 15 epochs.</p><p>We use the bert-extractive-summarizer <ref type="bibr" target="#b36">(Miller, 2019)</ref> to generate summaries using BERT-base. The Huggingface standard pipeline libraries has been used to generate summaries using sequenceto-sequence models i.e., PEGASUS and T5 that comes with default summarization capability. To generate summary using RoBERTa-base, RoBERTa5G, XLNet5G, and BERT5G, we use the Huggingface Automodel Library. We use another Huggingface library TransformerSummarizer to generate summary using XLNet, GPT2, GPT2base, GPT2-medium, GPT2-large and DistilGPT2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Compute Unit and Training Duration</head><p>A compute unit (CU) is the unit of measurement for the resources consumed. To calculate CUs, one needs to multiply two factors: (1) Memory (GB)size of the allocated server for task to run and ( <ref type="formula">2</ref>) Duration (hours) -how long the server is used. This means, 1 CU = 1 GB memory x 1 hour. We have used around 80-90% of the GPU during training time. By definition of computing units, we have used roughly 100 hours of 30GB GPU time.</p><p>Pre-training BERT takes around 36 hours in our experimental setup. Pre-training RoBERTa and XLNET takes around 24 hours each. Fine-tuning each model takes around 5-6 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Performance Evaluation of Downstream Tasks</head><p>We report the evaluation of performance and metrics used for it in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Performance Metrics</head><p>For automatic evaluation, we </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Annotation Guidelines</head><p>Below are the specific guidelines that we have given to the annotators to ensure the standard of annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Sentence Classification Guidelines</head><p>The annotators are given some general guidelines and are suggested to follow some steps to make the data annotation consistent. They are also provided with some rules and tips.</p><p>General Guidelines: For this task, an annotator is given a set of sentences. Based on the methods, fields, variables, and/or entities mentioned in the sentence, the annotator's objective is to identify if the sentence implies a potential security concern or sophisticated operation that might involve vulnerable consequences.</p><p>Steps:</p><p>1 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rules and Tips:</head><p>• Select all items in the sentence that have a security hazard. • If there are multiple such cases, you may choose any or all of them. • Optionally, you may provide a comment about your rationalization or feedback about the data (e.g., errors, unclear descriptions.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Summarization Guidelines</head><p>Similar to classification guidelines, the annotators are given general guidelines and suggested steps to annotate the summarization dataset. Again, they are also provided with some rules and tips. Below are the guidelines for annotation tasks for summarization.</p><p>General Guidelines: For this task, an annotator is given a set of articles. Based on the methods, fields, variables, and/or entities mentioned in the sentence, the annotator's objective is to summarize the article without losing important information, correctness and contextuality.</p><p>Steps:</p><p>1. Read the article carefully.</p><p>2. Identify the key points.</p><p>3. Summarize the article by doing the following: a) Deletion: Delete a sentence if it does not convey any important information.</p><p>b) Merge and shorten: Merge consecutive sentences if they convey continued information and make the merged sentences concise. c) Rephrase and shorten: Rephrase a sentence to make it simpler and make it shorter if possible. 4. If the sentences do not express any proper context, or are semantically incorrect, add a comment and proceed to the next sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rules and Tips:</head><p>• Select all items in the article that have important information. • Make the sentences simpler and concise keeping the important information. • Under each article is a comments box. Optionally, you can provide article-specific feedback in this box. This may include a rationalization of your choice, a description of an error within the article, or the justification of another answer which was also plausible. In general, any relevant feedback would be useful and will help in improving this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Annotator Agreement</head><p>In total nine annotators have annotated the summarization dataset. Each of them is given 70 nonoverlapping distinct articles. So there is no disagreement between annotators. Another round of manual cleaning has been done by two meta annotators who have gone through the whole dataset to ensure summarization quality and consistency, by addressing the comments and suggestions made by the annotators in the first round and making necessary changes(update/delete). For example in first round of annotation, annotators put comments like -"The paragraph is vague", "Independent sentences", "The paragraph does not have a logical flow. It cannot be further summarized", "It is not clear what the paragraph is talking about", etc. These comments are addressed by the meta annotators by manually correcting or removing the articles.</p><p>For the classification task, 3 annotators (we call them A1, A2, A3 here) separately annotate the dataset-A1 and A2 annotate 800 examples each and A3 annotates 801 examples. In the second step, they are assigned to reevaluate the annotations of each other (A1 reevaluating labels assigned by A3, A2 reevaluating labels assigned by A1, and A3 reevaluating A2). Such reevaluations bring forth disagreements on several labels which are finally resolved by their combined discussion. For example: "The AMF shall not indicate to the SMF to release the emergency PDU session.": A2 labels this as Security, while A3 assigns Undefined. This disagreement is later resolved by discussing their reasoning for the respective labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Examples</head><p>We are listing some example annotated data for both tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4.1 Sentence Classification:</head><p>Here we show a few examples of sentence classification-each containing a sentence and the correct label associated with it.</p><p>Sentence 1: If the positioning method parameter indicates both E-Cell ID and GNSS positioning, the eNB may use E-Cell ID measurement collection only if the UE does not provide GNSS-based location information. Here are a few examples, each containing an article and its summary.</p><p>Article 1: As indicated, 5G NR Meas Gap Length is not fixed and 3GPP specifications made it configurable. Having a fixed Meas Gap could cause unnecessary degradation of throughput in the serving cell. The SMTC window and window duration can be set to match SSB transmissions and accordingly, the MGL. For example, if we consider the SMTC window duration as 2 ms and the Meas Gap Length as 6 ms, here 4 ms segment would not be available for transmission, and reception of data in the serving cell will result in low DL/UL throughput.</p><p>Summary 1: 5G NR Meas Gap Length is adjustable per 3GPP specs. A fixed Meas Gap can degrade serving cell throughput. The SMTC window and duration can match SSB transmissions and the MGL. If the SMTC window duration is 2 ms and Meas Gap Length is 6 ms, a 4 ms segment is not accessible for transmission, resulting in limited DL/UL throughput.</p><p>Article 2: IMSI-catching attacks have threatened all generations (2G/3G/4G) of mobile telecommunication for decades. As a result of facilitating backward compatibility for legacy reasons, this privacy problem appears to have persisted. However, the 3GPP has now decided to address this issue, albeit at the cost of backward compatibility. In case of identification failure via a 5G-GUTI, unlike earlier generations, 5G security specifications do not allow plain-text transmissions of the SUPI over the radio interface. Instead, an Elliptic Curve Integrated Encryption Scheme (ECIES)-based privacypreserving identifier containing the concealed SUPI is transmitted. This concealed SUPI is known as SUCI (Subscription Concealed Identifier).</p><p>Summary 2: Unlike earlier generations, in the case of identification failure via a 5G-GUTI, 5G security specifications do not allow plain-text transmissions of SUPI over the radio interface. Instead, an Elliptic Curve Integrated Encryption Scheme (ECIES)-based privacy-preserving identifier containing the concealed SUPI (also known as SUCI) is transmitted.</p><p>Article 3: A SUPI is usually a string of 15 decimal digits. The first three digits represent the Mobile Country Code (MCC) while the next two or three form the Mobile Network Code (MNC), identifying the network operator. The remaining (nine or ten) digits are known as Mobile Subscriber Identification Numbers (MSIN) and represent the individual user of that particular operator. SUPI is equivalent to IMSI, which uniquely identifies the ME, and is also a string of 15 digits. Article 4: Next-generation 5G cellular systems will operate in frequencies ranging from around 500 MHz up to 100 GHz. Till now, with LTE and Wi-Fi technologies, we were operating below 6GHz and the channel models were designed and evaluated for operation at frequencies only as high as 6 GHz. The new 5G systems are to operate in bands above 6 GHz and existing channel models will not be valid, hence there is a need for accurate radio propagation models for these higher frequencies, which requires new channel models. The requirements of the new channel model that can support 5G operation across frequency bands up to 100 GHz are based on the existing 3GPP channel models along with extensions to cover additional 5G modeling requirements.</p><p>Summary 4: 5G will operate in frequencies ranging from around 500 MHz up to 100 GHz. Up to now 4G and WiFi were operating below 6GHz and the channel models were designed and evaluated for operation at frequencies only as high as 6GHz.</p><p>Article 5: Carrier Aggregation (CA) increases the bandwidth by combining several carriers. Each aggregated carrier is referred to as a Component Carrier (CC). 5G NR CA supports up to 16 contiguous and non-contiguous CCs with different numerologies in the FR1 band and in the FR2 band. A Carrier aggregation configuration includes the type of carrier aggregation (intra-band, contiguous or not, or inter-band), the number of bands, and the bandwidth class. CA Bandwidth Class is a series of alphabets that defines the minimum and maximum </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1.Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of our pre-training and fine-tuning on downstream tasks using SPEC5G</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Document distribution based on sentences. Documents with more than 1000 sentences were omitted from the figure for better visualization. Mean: 400.90, median: 356, min: 0, max: 1000, sd: 257.83, skewness: 0.52, kurtosis: -0.73</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure4: Sentence distribution based on tokens. Sentences with more than 200 tokens were omitted from the figure for better visualization.Mean: 34.89, median: 23,  min: 4, max: 200, sd: 34.91, skewness: 1.95, kurtosis:  4.10    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison of summarization task by pre-trained models and their base version. Brown colored lines denotes the base models inability to capture protocol specific sentences in summaries and teal colored lines donotes the sentences introduced by pre-trained models on SPEC5G that are more contextual.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Label 1 :</head><label>1</label><figDesc>SecuritySentence 2: SIGN_VAR shall be included in the channel quality report.Label 2: Non-Security Sentence 3: After performing the attach, the MS should activate PDP context(s) to replace any previously active PDP context(s). Label 3: Security Sentence 4: It switches the user from the UTRAN user plane to the GAN user plane Label 4: Undefined Sentence 5: If the BSIC cannot be decoded at the next available opportunities re attempts shall be made to decode this BSIC. Label 5: Non-Security Sentence 6: This might lead to an empty or even absent structure, if no parameter was modified. Label 6: Undefined C.4.2 Summarization:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Summary 3 :</head><label>3</label><figDesc>SUPI is a string of 15 decimal digits consisting of the Mobile Country Code, Mobile Network Code, and Mobile Subscriber Identification Number. SUPI is equivalent to IMSI which uniquely identifies the ME.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance of baseline models on classification task over the 5GSC dataset.</figDesc><table><row><cell>Model</cell><cell cols="2">Precision Recall</cell><cell>F1</cell><cell>Acc</cell></row><row><cell>ALBERT-base</cell><cell>0.6485</cell><cell cols="3">0.6587 0.6459 0.7034</cell></row><row><cell>BART-base</cell><cell>0.6458</cell><cell cols="3">0.6503 0.6432 0.7103</cell></row><row><cell>BERT-base</cell><cell>0.6113</cell><cell cols="3">0.6229 0.6157 0.6897</cell></row><row><cell>BERT5G</cell><cell>0.6972</cell><cell cols="3">0.6762 0.6856 0.7655</cell></row><row><cell>CamemBERT</cell><cell>0.6107</cell><cell cols="3">0.6272 0.6174 0.7034</cell></row><row><cell>DistilBERT</cell><cell>0.5819</cell><cell cols="3">0.5769 0.5731 0.6621</cell></row><row><cell>GPT2</cell><cell>0.6133</cell><cell cols="3">0.5567 0.5767 0.7973</cell></row><row><cell>LongFormer</cell><cell>06280</cell><cell cols="3">0.6281 0.6274 0.7034</cell></row><row><cell>mBART-large</cell><cell>0.6598</cell><cell cols="3">0.6642 0.6606 0.7241</cell></row><row><cell>ROBERTa-base</cell><cell>0.5752</cell><cell cols="3">0.5562 0.5631 0.6690</cell></row><row><cell>ROBERTa5G</cell><cell>0.5944</cell><cell cols="3">0.5696 0.5785 0.6966</cell></row><row><cell>XLNet-base</cell><cell>0.6260</cell><cell cols="3">0.6339 0.6297 0.7034</cell></row><row><cell>XLNet5G</cell><cell>0.6480</cell><cell cols="3">0.6829 0.6619 0.7103</cell></row></table><note><p>Additionally, BERT5G outperforms all other models in precision and F1 score. XLNet5G has the highest recall. Interestingly, the baseline GPT2 has the highest accuracy. Despite that, we do not choose GPT2 for pre-training. The reason behind this is discussed in Section 6.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Result of the human evaluation for SPEC5G.</figDesc><table><row><cell>Model</cell><cell cols="3">Simplicity Correctness Contextuality</cell></row><row><cell>Gold</cell><cell>4.37</cell><cell>4.77</cell><cell>4.56</cell></row><row><cell>BERT-base</cell><cell>3.2</cell><cell>3.87</cell><cell>3.3</cell></row><row><cell>BERT5G</cell><cell>3.96</cell><cell>4.32</cell><cell>3.92</cell></row><row><cell>RoBERTa-base</cell><cell>3.7</cell><cell>4.03</cell><cell>3.76</cell></row><row><cell>RoBERTa5G</cell><cell>4.02</cell><cell>4.2</cell><cell>3.94</cell></row><row><cell>XLNET-base</cell><cell>3.57</cell><cell>3.84</cell><cell>3.53</cell></row><row><cell>XLNET5G</cell><cell>3.97</cell><cell>4.31</cell><cell>3.91</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>We use the precision metric -which is calculated in almost the exact same way as recall, but rather than dividing by the reference n-gram count, it is divided by the model n-gram count.</figDesc><table><row><cell>use the com-monly used ROUGE score. We use the Python rouge_score library to calculate this. ROUGE Score: ROUGE-N measures the number of match-ing 'n-grams' between the model-generated text and a 'reference'. An n-gram is simply a group-ing of tokens/words. A unigram (1-gram) would consist of a single word. A bigram (2-gram) con-sists of two consecutive words. In ROUGE-N, N denotes the n-gram that is being used. For ROUGE-1 the match rate of unigrams between the model output and reference are measured. ROUGE-2 and ROUGE-3 would use bigrams and trigrams respec-tively. Recall: The recall counts the number of overlap-ping n-grams found in both the model output and reference, then divides this number by the total number of n-grams in the reference. recall = count n (gram n ) count(gram n ) Precision: precision = num of ngrams in model &amp; ref num of ngrams in model Now that both the recall and precision values are available, they can be used to calculate the ROUGE F1 score with the following formula: 2  *  precision  *  recall precision + recall ROUGE-L: ROUGE-L measures the longest com-mon subsequence (LCS) between the model output and the reference. With this metric, the number of tokens in the longest sequence shared between both are counted. The idea here is that a longer shared sequence would indicate more similarity between the two sequences. The recall and precision calcu-lations can be applied just like before -but this time the match is replaced with LCS. LCS(gram n ) recall = count(gram n )</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>List of blogs &amp; forums crawled as part of dataset collection</figDesc><table><row><cell>Portal Name</cell><cell>Web Address</cell><cell>Description</cell><cell># of Sen-</cell><cell># of Words</cell></row><row><cell></cell><cell></cell><cell></cell><cell>tences</cell><cell></cell></row><row><cell cols="2">Artiza Networks artizanetworks.com</cell><cell>ArtizaNetworks contains tutorials</cell><cell>383</cell><cell>5182</cell></row><row><cell></cell><cell></cell><cell>about 3G, 4G, and 5G Radio Ac-</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>cess Network (RAN) and Core Net-</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>work (CN).</cell><cell></cell><cell></cell></row><row><cell>Event Helix</cell><cell>eventhelix.com</cell><cell>Event Helix is a private corpora-</cell><cell>595</cell><cell>6691</cell></row><row><cell></cell><cell></cell><cell>tion based on Maryland. They de-</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>velop tools for networking and dis-</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>tributed systems and host numer-</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>ous blogs about 5G Radio, TCP/IP,</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>and so on.</cell><cell></cell><cell></cell></row><row><cell>3G LTE Info</cell><cell>3glteinfo.com</cell><cell>3G LTE Info offers tutorials and</cell><cell>790</cell><cell>9651</cell></row><row><cell></cell><cell></cell><cell>articles for network professionals.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>These articles encompass GSM,</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>GPRS, 3G, LTE, 5G, Bluetooth,</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>and so on.</cell><cell></cell><cell></cell></row><row><cell>4G 5G World</cell><cell>4g5gworld.com</cell><cell>Powered by NgnGuru Solutions</cell><cell>80</cell><cell>1069</cell></row><row><cell></cell><cell></cell><cell>Pvt. Ltd., 4G 5G World delivers</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>news, reports, and tutorials about</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>4G and 5G advanced technologies.</cell><cell></cell><cell></cell></row><row><cell>Info NR LTE</cell><cell>info-nrlte.com</cell><cell>Run by telecom experts, Info</cell><cell>508</cell><cell>7431</cell></row><row><cell></cell><cell></cell><cell>NR LTE delivers technology</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>overviews about NR LTE and NR</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>5G.</cell><cell></cell><cell></cell></row><row><cell>Resurchify</cell><cell>resurchify.com</cell><cell>Resurchify contains research gath-</cell><cell>138</cell><cell>1815</cell></row><row><cell></cell><cell></cell><cell>erings from conferences, journals,</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>symposiums, meetings from multi-</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>ple sectors.</cell><cell></cell><cell></cell></row><row><cell cols="2">Share Tech Note sharetechnote.com</cell><cell>ShareTechNote aims to be a ref-</cell><cell>8325</cell><cell>91575</cell></row><row><cell></cell><cell></cell><cell>erence guideline on numerous</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>fields, such as, programming lan-</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>guages, engineering, mathematics,</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>advanced technologies. 5G is one</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>of them.</cell><cell></cell><cell></cell></row><row><cell>Telecompedia</cell><cell>telecompedia.net</cell><cell>Telecompedia is a tutorial re-</cell><cell>2173</cell><cell>24997</cell></row><row><cell></cell><cell></cell><cell>source written by 4G, 5G, and ra-</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>dio experts from Rakuten Mobile</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>on different 5G related technolo-</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>gies such as D-RAN, Open-RAN,</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>power control etc.</cell><cell></cell><cell></cell></row><row><cell>RF Wireless</cell><cell>rfwireless-world.com</cell><cell>Following IEEE and 3GPP stan-</cell><cell>610</cell><cell>8032</cell></row><row><cell></cell><cell></cell><cell>dards, RF Wireless hosts articles,</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>tutorials, source code, terminolo-</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>gies about wireless technologies.</cell><cell></cell><cell></cell></row><row><cell>Tech Play On</cell><cell>techplayon.com</cell><cell>Tech Play On contains technology</cell><cell>5220</cell><cell>89035</cell></row><row><cell></cell><cell></cell><cell>news and guidelines on 5GNR and</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>LTE.</cell><cell></cell><cell></cell></row><row><cell>Telecom Hall</cell><cell>telecomhall.net</cell><cell>A forum to discuss the advances</cell><cell>7982</cell><cell>127712</cell></row><row><cell></cell><cell></cell><cell>on telecom domain and to guide</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>developers or practitioners.</cell><cell></cell><cell></cell></row><row><cell>How LTE Stuff</cell><cell>howltestuffworks.</cell><cell>Hosts numerous blogs about</cell><cell>4213</cell><cell>65520</cell></row><row><cell>Works</cell><cell>blogspot.com</cell><cell>5GNR and LTE.</cell><cell></cell><cell></cell></row><row><cell>Pro-Developer</cell><cell>prodevelopertutorial.</cell><cell>Delivers tutorials about C/C++,</cell><cell>3178</cell><cell>31708</cell></row><row><cell>Tutorial</cell><cell>com</cell><cell>Git, System design, 4G LTE,</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>5GNR, shell-scripting, etc.</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We thank the anonymous reviewers for their insightful comments and the annotators: <rs type="person">Adrian Li</rs>, <rs type="person">Charalampos Katsis</rs>, <rs type="person">Fabrizio Cicala</rs>, <rs type="person">Mengdie Huang</rs>, <rs type="person">Sonam Bhardwaj</rs>, <rs type="person">Yiwei Zhang</rs>, and <rs type="person">Zilin Shen</rs> from <rs type="affiliation">cyber2slab of Purdue University</rs> for their valuable time and effort in annotating the downstream task datasets. The work reported in this paper has been supported by <rs type="funder">NSF</rs> under grant <rs type="grantNumber">2112471</rs> "<rs type="projectName">AI Institute for Future Edge Networks and Distributed Intelligence (AI-EDGE</rs>)".</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_CJ9Vxc7">
					<idno type="grant-number">2112471</idno>
					<orgName type="project" subtype="full">AI Institute for Future Edge Networks and Distributed Intelligence (AI-EDGE</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Preprocessing and Training</head><p>In this section, we outline the preprocessing steps undertaken to clean and transform the data, as well as the training methodology employed to optimize the model's performance. By employing rigorous preprocessing and training techniques, we aim to ensure reliable and accurate results in our subsequent analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Preprocessing Details</head><p>The following preprocessing steps were performed- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Data Sources</head><p>Below we list the websites that were scrapped to create SPEC5G.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Specifications</forename><surname>Gpp</surname></persName>
		</author>
		<ptr target="https://www.3gpp.org/ftp/Specs" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">G-Powered Medical Robot Performs Remote Brain Surgery</title>
		<ptr target="https://www.automate.org/blogs/5g-powered-medical-robot-performs-remote-brain-surgery" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Adopting neural language models for the telecom domain</title>
		<ptr target="https://www.screenbeam.com/wifihelp/wifibooster/how-to-reduce-lag-for-gaming-and-improve-your-internet-speed/" />
		<imprint/>
	</monogr>
	<note>How to Reduce &amp; Fix Gaming Lag</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Selenium</surname></persName>
		</author>
		<ptr target="https://www.selenium.dev/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CarMap: Fast 3d feature map updates for automobiles</title>
		<author>
			<persName><forename type="first">Fawad</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><surname>Eells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Govindan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th USENIX Symposium on Networked Systems Design and Implementation (NSDI 20)</title>
		<meeting><address><addrLine>Santa Clara, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1063" to="1081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A formal analysis of 5g authentication</title>
		<author>
			<persName><forename type="first">David</forename><surname>Basin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jannik</forename><surname>Dreier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucca</forename><surname>Hirschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saša</forename><surname>Radomirovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Sasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Stettler</surname></persName>
		</author>
		<idno type="DOI">10.1145/3243734.3243846</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security, CCS &apos;18</title>
		<meeting>the 2018 ACM SIGSAC Conference on Computer and Communications Security, CCS &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1383" to="1396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno>CoRR, abs/2004.05150</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The ACL Anthology reference corpus: A reference dataset for bibliographic research in computational linguistics</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brett</forename><surname>Powley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee Fan</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)</title>
		<meeting>the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Interpreting pretrained contextualized representations via reductions to static embeddings</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelly</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4758" to="4781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos</title>
		<author>
			<persName><forename type="first">Ilias</forename><surname>Chalkidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manos</forename><surname>Fergadiotis</surname></persName>
		</author>
		<idno>CoRR, abs/2010.02559</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>LEGAL-BERT: the muppets straight out of law school</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Taming pretrained transformers for extreme multi-label text classification</title>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Seeing the forest for the trees: Understanding security hazards in the 3GPP ecosystem through intelligent analysis on change requests</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yepeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi-Aofeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongfang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st USENIX Security Symposium (USENIX Security</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="17" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bookworm game: Automatic discovery of lte vulnerabilities through documentation analysis</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yepeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoxu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/SP40001.2021.00104</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1197" to="1214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Structural scaffolds for citation intent classification in scientific publications</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Field</forename><surname>Cady</surname></persName>
		</author>
		<idno>CoRR, abs/1904.01608</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pretrained language models for sequential sentence classification</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavana</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Weld</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1383</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3693" to="3699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A supervised approach to extractive summarisation of scientific papers</title>
		<author>
			<persName><forename type="first">Ed</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K17-1021</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Vancouver, Canada. Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017. CoNLL 2017</date>
			<biblScope unit="page" from="195" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to simplify sentences using Wikipedia</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Coster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kauchak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on</title>
		<meeting>the Workshop on<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note type="report_type">Monolingual Text-To-Text Generation</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Component-based formal analysis of 5g-aka: Channel assumptions and session confusion</title>
		<author>
			<persName><forename type="first">Cas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Dehnel-Wild</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">In Biological, translational, and clinical language processing</title>
		<author>
			<persName><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Komal</forename><surname>Sutaria</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="49" to="56" />
			<pubPlace>Prague, Czech Republic</pubPlace>
		</imprint>
	</monogr>
	<note>Mining a lexicon of technical terms and lay equivalents</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simplifying lexical simplification: Do we need simplified corpora?</title>
		<author>
			<persName><forename type="first">Goran</forename><surname>Glavaš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Štajner</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-2011</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="63" to="68" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">5greasoner: A property-directed security and privacy analysis framework for 5g cellular network protocol</title>
		<author>
			<persName><forename type="first">Mitziu</forename><surname>Syed Rafiul Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imtiaz</forename><surname>Echeverria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Karim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><surname>Bertino</surname></persName>
		</author>
		<idno type="DOI">10.1145/3319535.3354263</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, CCS &apos;19</title>
		<meeting>the 2019 ACM SIGSAC Conference on Computer and Communications Security, CCS &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="669" to="684" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Diego Molla Aliod, Dragomir Radev, Francesco Ronzano, and Horacio Saggion. 2014. The computational linguistics summarization pilot task</title>
		<author>
			<persName><forename type="first">Kokil</forename><surname>Jaidka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muthu</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatriz</forename><surname>Fisas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Khanna</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The cl-scisumm shared task 2017: Results and key insights</title>
		<author>
			<persName><forename type="first">Kokil</forename><surname>Jaidka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muthu</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Insights from cl-scisumm 2016: the faceted scientific document summarization shared task</title>
		<author>
			<persName><forename type="first">Kokil</forename><surname>Jaidka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muthu</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sajal</forename><surname>Rustagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00799-017-0221-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal on Digital Libraries</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Neural CRF model for sentence alignment in text simplification</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mounica</forename><surname>Maddela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wuwei</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<idno>CoRR, abs/2005.02324</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving text simplification language modeling using unsimplified text data</title>
		<author>
			<persName><forename type="first">David</forename><surname>Kauchak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1537" to="1546" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">ALBERT: A lite BERT for selfsupervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno>CoRR, abs/1909.11942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Piyush Sharma, and Radu Soricut</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">So</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<idno>CoRR, abs/1901.08746</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>CoRR, abs/1910.13461</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>CoRR, abs/2001.08210</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep attention diffusion graph neural networks for text classification</title>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renchu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fausto</forename><surname>Giunchiglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanchun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyue</forename><surname>Feng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.642</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Online and Punta Cana, Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8142" to="8152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Controllable text simplification with explicit paraphrasing</title>
		<author>
			<persName><forename type="first">Mounica</forename><surname>Maddela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Alva-Manchego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<idno>CoRR, abs/2010.11004</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Éric Villemonte de la Clergerie, Djamé Seddah, and Benoît Sagot</title>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><forename type="middle">Ortiz</forename><surname>Suárez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoann</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
		<idno>CoRR, abs/1911.03894</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Camembert: a tasty french language model</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Coarse2fine: Fine-grained text classification on coarsely-grained annotated data</title>
		<author>
			<persName><forename type="first">Dheeraj</forename><surname>Mekala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Gangal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Leveraging BERT for extractive text summarization on lectures</title>
		<author>
			<persName><forename type="first">Derek</forename><surname>Miller</surname></persName>
		</author>
		<idno>CoRR, abs/1906.04165</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Summarunner: A recurrent neural network based sequence model for extractive summarization of documents</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR, abs/1611.04230</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Hybrid simplification using deep semantics and machine translation</title>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1041</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Controllable text simplification with lexical constraint loss</title>
		<author>
			<persName><forename type="first">Daiki</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoyuki</forename><surname>Kajiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuki</forename><surname>Arase</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-2036</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="260" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exploring neural text simplification models</title>
		<author>
			<persName><forename type="first">Sergiu</forename><surname>Nisioi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Štajner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liviu</forename><forename type="middle">P</forename><surname>Dinu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2014</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="85" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">YASO: A targeted sentiment analysis evaluation dataset for open-domain reviews</title>
		<author>
			<persName><forename type="first">Matan</forename><surname>Orbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orith</forename><surname>Toledo-Ronen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Spector</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranit</forename><surname>Aharonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Slonim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.721</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9154" to="9173" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised lexical simplification for non-native speakers</title>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Paetzold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v30i1.9885</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Text simplification for language learners: a corpus analysis</title>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">E</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>In SLaTE</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno>CoRR, abs/1910.01108</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The emergence of edge computing</title>
		<author>
			<persName><forename type="first">Mahadev</forename><surname>Satyanarayanan</surname></persName>
		</author>
		<idno type="DOI">10.1109/MC.2017.9</idno>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="39" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning simplifications for specific target audiences</title>
		<author>
			<persName><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2113</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="712" to="718" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Reformulating discourse connectives for non-expert readers</title>
		<author>
			<persName><forename type="first">Advaith</forename><surname>Siddharthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Napoleon</forename><surname>Katsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1002" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">An overview of microsoft academic service (mas) and applications</title>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrin</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-June</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/2740908.2742839</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="243" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">DEFT: A corpus for definition extraction in free-and semistructured text</title>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Spala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Dockhorn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-4015</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Linguistic Annotation Workshop</title>
		<meeting>the 13th Linguistic Annotation Workshop<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="124" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Sentence alignment methods for improving text simplification systems</title>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Štajner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Paolo Ponzetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiner</forename><surname>Stuckenschmidt</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2016</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Data-driven text simplification</title>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Štajner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics: Tutorial Abstracts</title>
		<meeting>the 27th International Conference on Computational Linguistics: Tutorial Abstracts<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="19" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>CoRR, abs/1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Amanpreet Singh, and Samuel R Bowman</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12471</idno>
	</analytic>
	<monogr>
		<title level="m">Neural network acceptability judgments</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Sentence simplification by monolingual machine translation</title>
		<author>
			<persName><forename type="first">Sander</forename><surname>Wubben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island</addrLine></address></meeting>
		<imprint>
			<publisher>Korea. Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1015" to="1024" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Optimizing statistical machine translation for text simplification</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00107</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="401" to="415" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/1906.08237</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">R</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<idno>CoRR, abs/1909.01716</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">PEGASUS: pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/1912.08777</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Towards efficient edge cloud augmentation for virtual reality mmogs</title>
		<author>
			<persName><forename type="first">Wuyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipankar</forename><surname>Raychaudhuri</surname></persName>
		</author>
		<idno type="DOI">10.1145/3132211.3134463</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second ACM/IEEE Symposium on Edge Computing, SEC &apos;17</title>
		<meeting>the Second ACM/IEEE Symposium on Edge Computing, SEC &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Sentence simplification with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1062</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="584" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">COUGH: A challenge dataset and models for COVID-19 FAQ retrieval</title>
		<author>
			<persName><forename type="first">Xinliang</forename><surname>Frederick Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.305</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3759" to="3769" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learn to adapt for generalized zero-shot text classification</title>
		<author>
			<persName><forename type="first">Yiwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caixia</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbin</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.39</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="517" to="527" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Discourse level factors for sentence deletion in text simplification</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi Jessy</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/1911.10384</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">A monolingual tree-based translation model for sentence simplification</title>
		<author>
			<persName><forename type="first">Zhemin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Delphine</forename><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1353" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
