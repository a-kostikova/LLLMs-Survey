<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multilingual Non-Autoregressive Machine Translation without Knowledge Distillation</title>
				<funder>
					<orgName type="full">Digital Research Alliance of Canada</orgName>
				</funder>
				<funder>
					<orgName type="full">Canada CIFAR AI Chair Program</orgName>
				</funder>
				<funder ref="#_wYnABDP">
					<orgName type="full">Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_KNGzusf #_JUDYjzt">
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
							<email>chenyangh@ualberta.ca</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computing Science</orgName>
								<orgName type="department" key="dep2">Alberta Machine Intelligence Institute (Amii)</orgName>
								<orgName type="institution">University of Alberta</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
							<email>huangfei382@163.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Damo Academy</orgName>
								<address>
									<settlement>Alibaba</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zaixiang</forename><surname>Zheng</surname></persName>
							<email>zhengzaixiang@bytedance.com</email>
							<affiliation key="aff2">
								<orgName type="department">ByteDance Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Osmar</forename><surname>Zaïane</surname></persName>
							<email>zaiane@ualberta.ca</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computing Science</orgName>
								<orgName type="department" key="dep2">Alberta Machine Intelligence Institute (Amii)</orgName>
								<orgName type="institution">University of Alberta</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
							<email>zhouhao@air.tsinghua.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute for AI Industry Research (AIR)</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computing Science</orgName>
								<orgName type="department" key="dep2">Alberta Machine Intelligence Institute (Amii)</orgName>
								<orgName type="institution">University of Alberta</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multilingual Non-Autoregressive Machine Translation without Knowledge Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">48657DAEE22F4324BCEEC594D50A80D9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multilingual neural machine translation (MNMT) aims at using one single model for multiple translation directions. Recent work applies non-autoregressive Transformers to improve the efficiency of MNMT, but requires expensive knowledge distillation (KD) processes. To this end, we propose an M-DAT approach to non-autoregressive multilingual machine translation. Our system leverages the recent advance of the directed acyclic Transformer (DAT), which does not require KD. We further propose a pivot back-translation (PivotBT) approach to improve the generalization to unseen translation directions. Experiments show that our M-DAT achieves state-of-the-art performance in non-autoregressive MNMT. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multilingual neural machine translation (MNMT) aims at using a single model for multiple translation directions <ref type="bibr" target="#b7">(Firat et al., 2016)</ref>. It has attracted the attention of the research community over the years <ref type="bibr" target="#b2">(Bapna and Firat, 2019;</ref><ref type="bibr" target="#b27">Zhang et al., 2021)</ref>, and has been widely applied in the industry <ref type="bibr" target="#b14">(Johnson et al., 2017)</ref>. Most state-of-the-art MNMT models are based on the autoregressive Transformer (AT, <ref type="bibr" target="#b26">Vaswani et al., 2017)</ref>. However, the inference of AT is slow, which results in significant latency in real-world applications <ref type="bibr" target="#b9">(Gu et al., 2018)</ref>.</p><p>Recent work applies the non-autoegressive Transformer (NAT, <ref type="bibr" target="#b9">Gu et al., 2018)</ref>, which generates target tokens in parallel for efficient inference. However, NAT often generates inconsistent sentences (e.g., with repetitive words). <ref type="bibr" target="#b21">Qian et al. (2021)</ref> propose the Glancing Transformer (GLAT), which is trained in a curriculum learning fashion. It tackles the weakness of NAT by focusing less on the training samples that lead to inconsistent generalization.</p><p>To accelerate multilingual non-autoregressive translation, <ref type="bibr">Song et al. (2022)</ref> propose a Switch-GLAT method, which is based on the Glancing Transformer, and is equipped with back-translation for data augmentation. To the best of our knowledge, Switch-GLAT is currently the only nonautoregressive system for multilingual translation. However, it suffers from two drawbacks. First, Switch-GLAT requires sequence-level knowledge distillation (KD, <ref type="bibr" target="#b15">Kim and Rush, 2016)</ref> in every translation direction, which is inconvenient for multilingual tasks. Second, Switch-GLAT is unable to generalize to unseen translation directions (zeroshot translation), which is an essential aspect of multilingual machine translation systems <ref type="bibr" target="#b14">(Johnson et al., 2017;</ref><ref type="bibr" target="#b3">Chen et al., 2017;</ref><ref type="bibr" target="#b11">Gu et al., 2019)</ref>.</p><p>In this work, we propose a multilingual Directed Acyclic Transformer (M-DAT) approach to nonautoregressive multilingual machine translation. Our system leverages the recent directed acyclic Transformer (DAT, <ref type="bibr">Huang et al., 2022b)</ref>, which does not rely on KD. In addition, we propose a pivot back-translation (PivotBT) approach for the multilingual translation task. Specifically, we backtranslate a target sentence to a randomly selected language to obtain an augmented source sentence. The newly generated source sentence and the original target sentence form a synthetic data sample. We observe that if the back-translation direction (e.g., German → Romanian) does not exist in the training set (i.e., zero-shot), the augmented source sentence will be of low quality. Therefore, our proposed PivotBT uses an intermediate language for the back translation (e.g., German → English → Romanian). Our PivotBT is efficient, as the inference of our non-autoregressive model is fast.</p><p>We evaluated M-DAT in both supervised and zero-shot translation directions. In the supervised setting, our M-DAT achieves 0.4 higher BLEU scores than the previous state-of-the-art Switch-GLAT, while maintaining fast inference. Moreover, our M-DAT does not require KD, and is convenient to be trained on multilingual datasets. In the zero-shot translation settings, our M-DAT is the first NAT model to effectively generalize to unseen translation directions, and even outperforms a strong autoregressive baseline, which is largely attributed to our proposed PivotBT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The non-autoregressive Transformer (NAT, <ref type="bibr" target="#b9">Gu et al., 2018)</ref> predicts all target words in parallel to achieve fast inference, and has been applied to various text generation tasks, such as machine translation <ref type="bibr" target="#b10">(Gu and Kong, 2021;</ref><ref type="bibr">Huang et al., 2022a)</ref>, summarization <ref type="bibr" target="#b25">(Su et al., 2021;</ref><ref type="bibr">Liu et al., 2022a,b)</ref>, and dialogue generation <ref type="bibr" target="#b30">(Zou et al., 2021;</ref><ref type="bibr">Qi et al., 2021)</ref>. However, the output quality of NAT models tends to be low <ref type="bibr" target="#b24">(Stern et al., 2019;</ref><ref type="bibr" target="#b8">Ghazvininejad et al., 2019)</ref>, and as a remedy, the Glancing Transformerthe Glancing Transformer (GLAT, <ref type="bibr" target="#b21">Qian et al., 2021)</ref> applies an adaptive training algorithm that allows the model to progressively learn more difficult data samples.</p><p>Sequence-level knowledge distillation (KD, <ref type="bibr" target="#b15">Kim and Rush, 2016)</ref> is commonly used to improve the translation quality of non-autoregressive models. As shown in <ref type="bibr" target="#b29">Zhou et al. (2020)</ref>, KD data are less complex compared with the original training set, which is easier for NAT models. However, <ref type="bibr" target="#b5">Ding et al. (2021)</ref> find that the KD process tends to miss low-frequency words (e.g., proper nouns), which results in worse translation for NAT models. Therefore, there is a need to remove the KD process for NAT models.</p><p>The most related work to ours is Switch-GLAT <ref type="bibr">(Song et al., 2022)</ref>. It combines the Glancing Transformer and knowledge distillation for multilingual machine translation tasks. In addition, Switch-GLAT is equipped with a back-translation technique to augment training data.</p><p>Our system is based on the directed acyclic  <ref type="bibr">(Huang et al., 2022b)</ref>. N dec is the number of decoding layers.</p><p>Transformer (DAT, <ref type="bibr">Huang et al., 2022b)</ref>, which expands the generation canvas to allow multiple plausible translation fragments. Then, DAT selects the fragments by predicting linkages, which eventually form an output sentence. In this way, M-DAT is more capable of handling complex data samples, and does not rely on KD.</p><p>We propose PivotBT to augment the training data to improve the generalization of M-DAT. Our PivotBT is inspired by online back-translation <ref type="bibr" target="#b28">(Zhang et al., 2020)</ref>, which extends the original back-translation (BT, <ref type="bibr" target="#b22">Sennrich et al., 2016)</ref> by randomly picking augmented directions. Different from the previous work, our approach applies pivot machine translation <ref type="bibr" target="#b4">(Cheng et al., 2017)</ref> to improve the reliability of the back-translation directions that are unseen in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Multilingual translation handles multiple translation directions with a single model. Suppose a data sample contains a source sentence x = (x 1 , • • • , x Tx ) and a target sentence y = (y 1 , • • • , y Ty ), where T x and T y denote the lengths. In addition, language tags l src and l tgt are given to indicate the languages of the source and target sentences, respectively. A multilingual machine translation dataset D can be represented by {(x (i) , l</p><formula xml:id="formula_0">(i) src , y (i) , l (i) tgt )} K i=1</formula><p>, where K is the size of the dataset.</p><p>Our multilingual Directed Acyclic Transformer (M-DAT) has an encoder-decoder architecture. The encoder of M-DAT takes in an input sentence x (i) and the target language tag l (i) tgt , whereas the decoder predicts the target-language words independently as the translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Directed Acyclic Transformer</head><p>To train our system without KD, we adapt the recent directed acyclic Transformer (DAT, <ref type="bibr">Huang et al., 2022b)</ref>, as it does not use KD but achieves comparable performance to autoregressive models on bilingual machine translation tasks (one direction per model).</p><p>In general, DAT expands its output canvas to generate multiple plausible translation fragments. Further, DAT predicts links to select the fragments, which form an output sentence. As seen in Figure <ref type="figure" target="#fig_0">1</ref>, DAT predicts extra words and forms the final generation "I feel good", using the predicted links.</p><p>Suppose the DAT decoder has S generation steps (S &gt; T y ). For each step s within 1 ≤ s ≤ S, DAT makes two predictions: word prediction p  </p><formula xml:id="formula_1">word (•) = softmax(W word h s )<label>(1)</label></formula><p>where W word is a learnable matrix. The link prediction p (s) link (•) computes the distribution over the subsequent steps of the sth step, determining which follow-up step should be linked to the sth step. Specifically, the link prediction leverages the attention mechanism <ref type="bibr" target="#b1">(Bahdanau et al., 2015)</ref>, which compares the sth step's hidden state h s with the states of subsequent generation steps (from s + 1 to S), given by</p><formula xml:id="formula_2">p (s) link (•) = softmax([k ⊤ s q s+1 ; • • • ; k ⊤ s q S ]) (2)</formula><p>where k s = W k h s and q s = W q h s . W k and W q are learnable matrices. The operation [; ] concatenates scalars into a column vector. Given a reference sequence y in the training set D, DAT selects T y of all S generation steps to generate the words in y, where the selected steps are connected by predicted links. We denote the indices of the selected steps by a = (a 1 , • • • , a Ty ), where 1 = a 1 &lt; • • • &lt; a Ty = S. We refer to each selection of the steps a as a path.</p><p>Consider a groundtruth sequence y 1:Ty . The joint probability of the sequence, together with some path a 1:Ty , is p(y 1:Ty , a 1:Ty ) = where p (a t-1 ) link (a t ) is the probability that the two generation steps a t-1 and a t are linked up. Specially, a 1 is set to 1, and is not considered as a random variable. p (at) word (y t ) is the probability of predicting the word y t at the a t th generation step.</p><p>Finally, the probability of generating the reference sentence p(y 1:Ty ) is obtained by the marginalization of all possible paths, given by p(y 1:Ty ) = where</p><formula xml:id="formula_3">Γ S,Ty = {a = (a 1 , • • • , a Ty )|1 = a 1 &lt; • • • &lt; a Ty = S} represents all paths of length T y .</formula><p>The computation of ( <ref type="formula">4</ref>) is efficient through dynamic programming. <ref type="foot" target="#foot_0">2</ref>Note that p </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pivot Back-Translation</head><p>We propose a pivot back-translation (PivotBT) approach to improve the robustness of M-DAT. Following <ref type="bibr" target="#b28">Zhang et al. (2020)</ref> and <ref type="bibr">Song et al. (2022)</ref>, we augment the training data with back-translation (BT, <ref type="bibr" target="#b22">Sennrich et al., 2016)</ref>. Specifically, a randomly selected language is chosen for such data augmentation.</p><p>We observe that when the back-translation direction is unseen (i.e., zero-shot), the synthesized source sentence will be of low quality, which results in a less meaningful synthetic training sample. To this end, we propose to handle the zero-shot scenario by PivotBT, which uses an intermediate language as a pivot and performs multi-step backtranslation.</p><p>Given a training sample (x, l src , y, l tgt ), we first randomly pick a language l aug from the set of languages in the multilingual training set D. If the back-translation direction l tgt → l aug is in the training set, we directly back-translate y to x of language l aug . Otherwise, we choose a pivot language l pivot (e.g., English) such that l tgt → l pivot and l pivot → l aug are both in the training set. <ref type="foot" target="#foot_1">3</ref>  In our PivotBT, the multi-step back-translation is conducted by M-DAT itself. Since M-DAT is fast in inference, the back-translation is also efficient.</p><p>We denote the loss of training the real samples in dataset D by L real and that of the synthetic samples by L PivotBT . The overall training loss of our proposed system is L = L real + λL PivotBT , where λ is a hyperparameter controlling the strength of the back-translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>We evaluated M-DAT on five datasets: WMT-EFD, WMT-EFZ, WMT-MANY, IWSLT, and Europarl. The three WMT corpora <ref type="bibr">(Song et al., 2022)</ref> only contain supervised directions in the test set, whereas the test sets of IWSLT and Europarl <ref type="bibr" target="#b16">(Liu et al., 2021)</ref> contain both supervised directions and unseen directions (zero-shot). The training hyperparameters and evaluate metrics are strictly following <ref type="bibr">Song et al. (2022)</ref> and <ref type="bibr" target="#b16">Liu et al. (2021)</ref>. We provide more details in Appendices A and B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>Supervised Translation. Table <ref type="table" target="#tab_0">1</ref> summarizes the BLEU scores of three WMT datasets. We evaluated M-DAT with two decoding algorithms: lookahead and n-gram beam search. The lookahead method directly decodes the generated words in parallel, and jointly maximizes the probability of the next position and predicted tokens, whereas n-gram beam search generates a few candidate sentences and ranks them with an n-gram language model. We observe that the generation quality with n-gram beam search is higher, which is consistent with <ref type="bibr">Huang et al. (2022b)</ref>.  We also see that M-DAT outperforms Switch-GLAT on average with both lookahead and beam search decoding methods. This makes our M-DAT the state of the art in non-autoregressive multilingual translation. In addition, our system does not rely on KD, which makes it convenient to be trained on multilingual datasets.</p><p>To compare M-DAT with the autoregressive multilingual Transformer (M-AT, <ref type="bibr" target="#b14">Johnson et al., 2017)</ref>, we include two autoregressive Transformer-based variants: 1) the standard layout <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref>, which has the same number of encoder layers; and 2) the layout with a shallow decoder, which moves all but one decoding layers to the encoder. The layout with a shallow decoder is suggested by <ref type="bibr">Kasai et al. (2021)</ref> as it achieves close results to the standard layout but is faster in inference. We observe that M-DAT is only slightly lower in BLEU scores compared with the M-AT models. This shows our M-DAT, in addition to its efficiency, largely closes the gap between AT and NAT in nonautoregressive multilingual translation tasks.</p><p>Zero-Shot Translation. The ability to generalize to unseen translation directions is important for multilingual models. In this setting, we do not compare our model with Switch-GLAT as it fails to achieve reasonable performance. <ref type="foot" target="#foot_2">4</ref> Instead, we compare M-DAT with the M-AT models <ref type="bibr" target="#b14">(Johnson et al., 2017)</ref>, and include a recent study, Residual M-AT <ref type="bibr" target="#b16">(Liu et al., 2021)</ref>, which relaxes the residual connections in the Transformer encoder to force the decoder to learn more generalized representations.<ref type="foot" target="#foot_3">5</ref>  As seen in Table <ref type="table" target="#tab_1">2</ref>, the M-ATs are incapable of zero-shot translation, and are largely outperformed by the Residual M-AT. On the other hand, our M-DAT with the lookahead decoding outperforms Residual M-AT by 0.86 BLEU on the IWSLT dataset, although it is 0.27 lower on the Europarl dataset. With n-gram beam search, the improvement is 1.68 BLEU on IWSLT and 1.31 on Europarl. Our M-DAT is the first non-autoregressive model to outperform a strong AT baseline in zeroshot multilingual translation. <ref type="foot" target="#foot_4">6</ref>Inference Speed. We compare the inference speed on the test set of WMT-EFD and present the results in Table <ref type="table" target="#tab_2">3</ref>. The batch size is set to 1 to mimic the real-world scenario, where the user requests come one after another <ref type="bibr" target="#b9">(Gu et al., 2018)</ref>. As seen, M-DAT with lookahead is about 16 times faster than the standard autoregressive baseline, and is about 4 times faster than M-AT with a shallow decoder. Compared with Switch-GLAT, M-DAT with lookahead is about the same efficiency. Admittedly, M-DAT with beam search is slower, but is still 5.2 times faster than the standard M-AT. In general, M-DAT obtains a good speed-quality trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head><p>Low-Frequency Words. We analyze the generated text (on the WMT-EFD test set) of our M-DAT and Switch-GLAT. We followed <ref type="bibr" target="#b5">Ding et al. (2021)</ref>, and computed the percentage of a word being preserved in the translated sentence (in the corresponding language); then we grouped the words by their frequencies in the dataset.<ref type="foot" target="#foot_5">7</ref> As seen in Figure <ref type="figure" target="#fig_6">2</ref>, M-DAT keeps more low-frequency words, which verifies our motivation to develop a non-autoregressive multilingual model without the help of knowledge distillation. In addition to the BLEU scores, this result further provides evidence that our M-DAT has better translation quality than Switch-GLAT.</p><p>Ablation Study. Table <ref type="table" target="#tab_3">4</ref> presents an ablation study on the pivot back-translation (PivotBT) of M-DAT using the IWSLT dataset. In addition to Piv-otBT, we consider 3 variants: 1) M-DAT rand-lang &amp; w/o pivot, which randomly selects an augmented source language but does not translate through a pivot language; 2) M-DAT src-lang &amp; w/o pivot, which directly back-translates the target sentence to the language of the source sentence; and 3) M-DAT w/o BT, which does not augment the training data with back-translation.</p><p>In the supervised setting, we observe that backtranslation improves the performance (Lines 1-3 vs. Line 4), which is consistent with the findings of previous work <ref type="bibr" target="#b14">(Johnson et al., 2017)</ref>. Among back-translation methods, src-lang &amp; w/o pivot performs the worst in the zero-shot setting (Line 3). We conjuncture that this is because only applying back-translation to the source language makes the model focus too much on the supervised directions, which degenerates the generalization to the zero-shot setting. On the other hand, our PivotBT outperforms the direct random back-translation (rand-lang &amp; w/o pivot) and the source-language back-translation (src-lang &amp; w/o pivot). This confirms that PivotBT provides the model with betteraugmented samples for the zero-shot translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose M-DAT to tackle nonautoregressive multilingual machine translation (MNMT). Our approach leverages the recent directed acyclic Transformer so that we do not need the knowledge distillation process, which is particularly inconvenient in the multilingual translation task. Further, we propose a pivot back-translation method to improve the robustness. Our M-DAT achieves state-of-the-art results on supervised and zero-shot settings for non-autoregressive MNMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitation</head><p>One possible limitation is that our M-DAT obtains slightly lower BLEU scores compared with autoregressive models in the supervised setting. However, this is not the drawback of this work, as it is understandable that non-autoregressive models trade quality with more efficiency. Nevertheless, our M-DAT outperforms the previous state-of-the-art NAT approach in both supervised and zero-shot settings, and is easier to be deployed since KD is not required. Our PivotBT, in principle, can also be applied to the training of multilingual autoregressive Transformer (M-AT). However, we do not include M-AT with PivotBT for two reasons: 1) the main focus of this research is on the non-autoregressive Transformer; and 2) the decoding of M-AT is much slower than our M-DAT, which makes the training of M-AT with PivotBT impractical. A Datasets WMT Datasets. WMT-EFZ, EFD, and MANY are specifically curated for multilingual machine translation; each is a mix of a few general bilingual machine translation corpora. <ref type="foot" target="#foot_6">8</ref> We list the translation directions of the three datasets in Table <ref type="table" target="#tab_4">5</ref>. As seen, both WMT-EFZ and WMT-EFD have 3 languages and 6 translation directions, whereas WMT-MANY has 5 languages and 10 directions. We strictly followed <ref type="bibr">Song et al. (2022)</ref> for data preparation, and we set the vocabulary size as 85K for WMT-EFD and WMT-EFZ, and 95K for WMT-MANY.</p><p>IWSLT Dataset. We followed Liu et al. ( <ref type="formula">2021</ref>) for the IWSLT dataset, and directly obtained the processed data from their published codebase. <ref type="foot" target="#foot_7">9</ref>The vocabulary size of IWSLT is 19K. The IWSLT test set contains 3 supervised language pairs: en↔ro, en↔it, and en↔nl. Additionally, it contains 3 zero-shot language pairs: ro↔it, ro↔nl, and it↔nl. The training set for each supervised direction includes 145K samples.</p><p>Europarl Dataset. We further include the Europarl dataset to evaluate the multilingual capability of the proposed method. We followed <ref type="bibr" target="#b16">Liu et al. (2021)</ref> for data preprocessing. The Europarl test set has 16 supervised directions (containing English) and 56 zero-shot directions (not containing English). The translation directions are detailed in Table <ref type="table">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Settings</head><p>Evaluation Metrics. We used BLEU <ref type="bibr" target="#b19">(Papineni et al., 2002)</ref> to evaluate the translation quality. To ensure a fair comparison with previous work, we applied two BLEU variants. For the WMT datasets, we followed <ref type="bibr">Song et al. (2022)</ref> and applied tokenized BLEU. For the IWSLT and Eu-roparl datasets, we followed <ref type="bibr" target="#b16">Liu et al. (2021) and</ref><ref type="bibr">adopted SacreBLEU (Post, 2018)</ref>.</p><p>We evaluated the latency on a single Tesla V100 GPU with a batch size of 1 to mimic the real-world scenario where the users' requests come one by one. Our evaluation scripts are also available from the released code.</p><p>WMT and Europarl Datasets. We used the Transformer-base configuration <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref> as the backbone. To train the model, we set the batch size such that it contains 64K tokens, and let the model train for 800K updates. We used the Adam optimizer. The learning rate was warmed up to 5e-4 using 10K updates, and was annealed with the inverse square roots scheduler. The backtranslation strength λ was set to 0.5. To balance the sizes of different translation directions, we set the upsampling ratio to 1/3. Since the validation set only contains supervised directions for the IWSLT dataset, we further applied the regularization on the encoder representations <ref type="bibr" target="#b0">(Arivazhagan et al., 2019)</ref> to prevent overfitting to the supervised directions.</p><p>Following the setting in most non-autoregressive machine translation studies <ref type="bibr" target="#b9">(Gu et al., 2018;</ref><ref type="bibr" target="#b10">Gu and Kong, 2021;</ref><ref type="bibr">Song et al., 2022;</ref><ref type="bibr">Huang et al., 2022a)</ref>, we evaluated both AT and NAT models by averaging the weights of the best 5 checkpoints, which were selected by their BLEU scores on the validation set.</p><p>For the neural architecture, both our M-DAT and the M-AT with the standard layout have 6 encoder layers and 6 decoder layers. The shallow-decoder M-AT has 12 encoder layers and 1 decoder layer.</p><p>IWSLT dataset. Most of the settings for IWSLT are the same as those for the WMT and Europarl datasets, but we made some adaptations. Since the IWSLT dataset is smaller, we set the batch size to 32K tokens. Further, we followed <ref type="bibr" target="#b16">Liu et al. (2021)</ref>, and set the number of encoders and decoders to 5 for our M-DAT and the M-AT with the standard layout. On the other hand, the M-AT with the shallowdecoder layout M-AT has 10 encoder layers and 1 decoder layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Detailed Results</head><p>We list the per-direction BLEU scores of the three WMT datasets in Table <ref type="table">6</ref>, IWSLT in Table <ref type="table">7</ref>, and Europarl in Table <ref type="table">8</ref>.</p><p>As seen, our M-DAT is only slightly outperformed by M-AT <ref type="bibr" target="#b14">(Johnson et al., 2017)</ref>, and the gap is small. However, our M-DAT outperforms</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of our PivotBT augmenting a German sentence y to Romanian x, where English is used as the pivot language. The training and back-translation steps are accomplished by DAT(Huang et al., 2022b). N dec is the number of decoding layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>word (•) gives the distribution over possible words by mapping DAT's sth decoder state h s to the probability distribution over the vocabulary, given by p (s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Ty t=2 p (a t-1 ) link (a t ) Ty t=1 p (at) word (y t ) (3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(s) word (•) and p (s) link (•) are independently predicted for different generation steps; thus, DAT is non-autoregressive and is fast in inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison between M-DAT and Switch-GLAT in the preservation ratio of low-frequency words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>BLEU scores on three WMT datasets. * Trained with sequence-level knowledge distillation.way, we are able to obtain an augmented source sentence x by first translating y to xpivot of the intermediate language l pivot , and then translating xpivot to the augmented source sentence x of language l aug . Finally, the newly synthesized sample (x, l aug , y, l tgt ) is added to the training.</figDesc><table><row><cell>In this</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>BLEU scores on IWSLT and Europarl.</figDesc><table><row><cell></cell><cell cols="2">IWSLT</cell><cell>Europarl</cell><cell></cell></row><row><cell>Model Variant</cell><cell cols="4">Supervised 0-Shot Supervised 0-Shot</cell></row><row><cell>M-AT w/ standard layout</cell><cell>30.00</cell><cell>12.87</cell><cell>35.79</cell><cell>15.84</cell></row><row><cell>M-AT w/ shallow decoder</cell><cell>29.23</cell><cell>4.95</cell><cell>34.95</cell><cell>8.11</cell></row><row><cell>Residual M-AT</cell><cell>29.72</cell><cell>17.67</cell><cell>35.18</cell><cell>26.13</cell></row><row><cell>M-DAT w/ lookahead</cell><cell>28.58</cell><cell>18.53</cell><cell>34.83</cell><cell>25.86</cell></row><row><cell>w/ n-gram beam search</cell><cell>29.42</cell><cell>19.35</cell><cell>35.48</cell><cell>27.44</cell></row><row><cell>Model Variant</cell><cell></cell><cell cols="3">Latency (ms) Speedup</cell></row><row><cell cols="2">M-AT w/ standard layout</cell><cell>352.4</cell><cell cols="2">1.0×</cell></row><row><cell cols="2">M-AT w/ shallow decoder</cell><cell>84.2</cell><cell cols="2">4.2×</cell></row><row><cell>Switch-GLAT</cell><cell></cell><cell>19.6</cell><cell cols="2">18.0×</cell></row><row><cell cols="2">M-DAT w/ lookahead</cell><cell>21.9</cell><cell cols="2">16.1×</cell></row><row><cell cols="2">w/ n-gram beam search</cell><cell>67.6</cell><cell cols="2">5.2×</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Latency and speedup on WMT-EFD.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on the IWSLT dataset. The results are generated with n-gram beam search.</figDesc><table><row><cell cols="2"># Model Variant</cell><cell cols="2">Supervised Zero-Shot</cell></row><row><cell cols="2">1 M-DAT (PivotBT)</cell><cell>29.42</cell><cell>19.35</cell></row><row><cell>2</cell><cell>rand-lang &amp; w/o pivot</cell><cell>29.33</cell><cell>18.10</cell></row><row><cell>3</cell><cell>src-lang &amp; w/o pivot</cell><cell>29.38</cell><cell>13.78</cell></row><row><cell>4</cell><cell>w/o BT</cell><cell>28.55</cell><cell>13.37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The translation directions of the WMT-EFD, WMT-EFZ, and WMT-MANY datasets.</figDesc><table><row><cell></cell><cell cols="3">Directions Languages Language pairs</cell></row><row><cell>WMT-EFZ</cell><cell>6</cell><cell>3</cell><cell>en↔fr, en↔zh</cell></row><row><cell>WMT-EFD</cell><cell>6</cell><cell>3</cell><cell>en↔fr, en↔de</cell></row><row><cell>WMT-MANY</cell><cell>10</cell><cell>6</cell><cell>en↔fr, en↔zh,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>en↔de, en↔ro,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>en↔ru</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We refer readers toHuang et al. (2022b).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Most multilingual translation datasets are English-centric, where using English as lpivot guarantees the connection of ltgt and laug, which is the case of this work. In general, we can use multiple pivot languages to connect ltgt and laug with multiple back-translation steps.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>As seen in the Table7ofSong et al. (2022), Switch-GLAT only obtained a 2.34 BLEU on a zero-shot dataset.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>The numbers of Residual M-AT are based on our replication, and are close to those in<ref type="bibr" target="#b16">Liu et al. (2021)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>In our experiments, the autoregressive baseline models are not equipped with PivotBT, as their inefficient inference (as seen in Table3) makes the training with back-translation impractical. In fact, our PivotBT is another way to take advantage of the fast inference of non-autoregressive models.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>  7  We applied the FastAlign alignment tool<ref type="bibr" target="#b6">(Dyer et al., 2013)</ref> to approximate word preservation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>The corpora are obtained from the WMT workshop: https://www.statmt.org/wmt17/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>https://github.com/nlp-dke/NMTGMinor/tree/ master/recipes/zero-shot</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank all reviewers and chairs for their comments. This research was supported in part by the <rs type="funder">Natural Science Foundation of China</rs> under Grant No. <rs type="grantNumber">62376133</rs>. This research was also supported in part by the <rs type="funder">Natural Sciences and Engineering Research Council of Canada (NSERC)</rs> under Grant Nos. <rs type="grantNumber">RGPIN-2020-04440</rs> and <rs type="grantNumber">RGPIN-2020-04465</rs>, the <rs type="institution">Amii Fellow Program</rs>, the <rs type="funder">Canada CIFAR AI Chair Program</rs>, the <rs type="institution">Alberta Innovates Program</rs>, and the <rs type="funder">Digital Research Alliance of Canada</rs> (alliancecan.ca).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wYnABDP">
					<idno type="grant-number">62376133</idno>
				</org>
				<org type="funding" xml:id="_KNGzusf">
					<idno type="grant-number">RGPIN-2020-04440</idno>
				</org>
				<org type="funding" xml:id="_JUDYjzt">
					<idno type="grant-number">RGPIN-2020-04465</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The missing ingredient in zero-shot neural machine translation</title>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Macherey</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07091</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simple, scalable adaptation for neural machine translation</title>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1538" to="1548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A teacher-student framework for zero-resource neural machine translation</title>
		<author>
			<persName><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1925" to="1935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint training for pivot-based neural machine translation</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/555</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3974" to="3980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rejuvenating low-frequency words: Making the most of parallel data in non-autoregressive translation</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuebo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3431" to="3441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of IBM Model 2</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter</title>
		<meeting>the 2013 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="644" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-way, multilingual neural machine translation with a shared attention mechanism</title>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="866" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mask-predict: Parallel decoding of conditional masked language models</title>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6112" to="6121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Non-autoregressive neural machine translation</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fully nonautoregressive neural machine translation: Tricks of the trade</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="120" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved zero-shot neural machine translation via ignoring spurious correlations</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1258" to="1268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">2022a. Non-autoregressive translation with layer-wise prediction and deep supervision</title>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Osmar R Zaïane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="page" from="10776" to="10784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">2022b. Directed acyclic Transformer for nonautoregressive machine translation</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Macduff</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">James</forename><surname>Cross</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2017. 2021</date>
			<biblScope unit="page" from="339" to="351" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sequencelevel knowledge distillation</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1317" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving zero-shot translation by disentangling positional information</title>
		<author>
			<persName><forename type="first">Danni</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1259" to="1273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">2022a. Learning non-autoregressive models from search for unsupervised sentence summarization</title>
		<author>
			<persName><forename type="first">Puyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7916" to="7929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">2022b. A character-level length-control algorithm for nonautoregressive sentence summarization</title>
		<author>
			<persName><forename type="first">Puyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="29101" to="29112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BLEU: A method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bang: Bridging autoregressive and non-autoregressive generation with large scale pretraining</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post ; Weizhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houqiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiusheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2021</date>
			<biblScope unit="page" from="8630" to="8639" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glancing transformer for non-autoregressive neural machine translation</title>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1993" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">2022. switch-GLAT: Multilingual parallel machine translation via code-switch decoder</title>
		<author>
			<persName><forename type="first">Zhenqiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanbo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Insertion transformer: Flexible sequence generation via insertion operations</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5976" to="5985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonautoregressive text generation with pre-trained language models</title>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<meeting>the 16th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Main Volume</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="234" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Share or not? Learning to schedule language-specific capacity for multilingual translation</title>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving massively multilingual neural machine translation and zero-shot translation</title>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1628" to="1639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Understanding knowledge distillation in nonautoregressive machine translation</title>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Thinking clearly, talking fast: Concept-guided non-autoregressive generation for open-domain dialogue systems</title>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingwu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2215" to="2226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m">Shot Setting AVG it-en it-ro nl-it nl-ro ro-it ro-nl</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M-</forename></persName>
		</author>
		<idno>AT 12.87 13.3 12.3 13.4 11.7 14.2 12.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M-</forename></persName>
		</author>
		<idno>AT 12.87 13.3 12.3 13.4 11.7 14.2 12.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M-Dat</forename></persName>
		</author>
		<idno>18.53 20.0 17.8 19.3 15.3 20.9 17.9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wmt-Many Avg En-De</forename><surname>De</surname></persName>
		</author>
		<idno>/ standard layout 30.36 24.67 32.16 41.67 38.00 32.86 35.65 24.22 30.47 20.66 23.27</idno>
		<imprint/>
	</monogr>
	<note type="report_type">en en-fr fr-en en-ro ro-en en-ru ru-en en-zh zh-en M-AT w</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M-</forename></persName>
		</author>
		<idno>AT w/ shallow decoder 29.19 25.41 31.38 41.98 37.88 30.18 34.16 21.87 26.92 20.90 21.27 Switch-GLAT 28.47 24.18 30.49 39.47 36.30 31.93 32.40 24.16 28.33 16.25 21.23</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M-Dat</forename></persName>
		</author>
		<idno>28.69 23.48 30.30 40.78 36.76 30.77 34.83 20.14 28.30 19.61 21.94</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">on all zero-shot translation directions of the IWSLT dataset and of most of the zero-shot directions of the Europarl dataset</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Direction M-AT w/ standard layout M-AT w/ shallow decoder Residual M-AT M-DAT w/ lookahead M-DAT</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Moreover, our proposed M-AT outperforms the strong Residual M-AT model</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m">Table 8: BLEU scores on the Europarl dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
