<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Localize, Retrieve and Fuse: A Generalized Framework for Free-Form Question Answering over Tables</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenting</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ye</forename><surname>Liu</surname></persName>
							<email>yeliu@salesforce.com</email>
							<affiliation key="aff1">
								<orgName type="department">Salesforce Research</orgName>
								<address>
									<settlement>Palo Alto</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yao</forename><surname>Wan</surname></persName>
							<email>wanyao@hust.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Sci. &amp; Tech</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yibo</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhongfen</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
							<email>psyu@uic.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Localize, Retrieve and Fuse: A Generalized Framework for Free-Form Question Answering over Tables</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DC14FCE6F70904BB7A9A4153B9E9A64A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Question answering on tabular data (a.k.a TableQA), which aims at generating answers to questions grounded on a provided table, has gained significant attention recently. Prior work primarily produces concise factual responses through information extraction from individual or limited table cells, lacking the ability to reason across diverse table cells. Yet, the realm of free-form TableQA, which demands intricate strategies for selecting relevant table cells and the sophisticated integration and inference of discrete data fragments, remains mostly unexplored. To this end, this paper proposes a generalized three-stage approach: Table-to-Graph conversion and cell localizing, external knowledge retrieval, and the fusion of table and text (called TAG-QA), to address the challenge of inferring long free-form answers in generative TableQA. In particular, TAG-QA (1) locates relevant table cells using a graph neural network to gather intersecting cells between relevant rows and columns, (2) leverages external knowledge from Wikipedia, and (3) generates answers by integrating both tabular data and natural linguistic information. Experiments showcase the superior capabilities of TAG-QA in generating sentences that are both faithful and coherent, particularly when compared to several state-of-the-art baselines. Notably, TAG-QA surpasses the robust pipelinebased baseline TAPAS by 17% and 14% in terms of BLEU-4 and PARENT F-score, respectively. Furthermore, TAG-QA outperforms the end-to-end model T5 by 16% and 12% on BLEU-4 and PARENT F-score, respectively. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question answering is to generate precise answers by interacting efficiently with unstructured, structured, or heterogeneous contexts, such as paragraphs, knowledge bases, tables, images, and various combinations thereof (Burke et al., 1997; Yao   1 Source code will be released at https://github. com/wentinghome/TAGQA. [TAPAS]: The Newcomers Manx Grand Prix race was won by the Spaniard in the first three places with a time of 1:28.22.2 seconds, 1:29.24.8 seconds and a time of 1:29.59.2.</p><p>[MATE]: Robert Dunlop won in the first three places, followed by Steve Hislop in the second.</p><p>[Ours]: The Newcomers Manx Grand Prix race was won by Robert Dunlop from Scotland Steve Hislop in 2nd place and Ian Lougher in 3rd place at 100.62 mph.</p><p>[Reference]: The Newcomers Manx Grand Prix race was won by Robert Dunlop from Steve Hislop in 2nd place and Ian Lougher in 3rd place at an race speed of 100.62.</p><p>[Q]: Who won in the first three places of The Newcomers Manx Grand Prix race?</p><p>google Figure 1: A motivating example to show the insights of our proposed approach when comparing with several state-of-the-art methods.</p><p>and <ref type="bibr" target="#b51">Van Durme, 2014;</ref><ref type="bibr" target="#b43">Talmor et al., 2021;</ref><ref type="bibr" target="#b14">Hao et al., 2017)</ref>. Among these, question answering on tabular data (TableQA) is a challenging task that requires the understanding of table semantics, as well as the ability to reason and infer over relevant table cells <ref type="bibr" target="#b16">(Herzig et al., 2021;</ref><ref type="bibr">Chen et al., 2020b</ref><ref type="bibr">Chen et al., , 2021b))</ref>.</p><p>For the task of TableQA, from our investigation, most current studies are focusing on the factoid TableQA, in which the answer is in a few words or a phrase copied directly from relevant table cells. In particular, current works on factoid TableQA are mainly categorized into two groups: (1) pipelinebased methods consisting of two stages, i.e., cell retrieval and answer reader <ref type="bibr" target="#b58">(Zhu et al., 2021;</ref><ref type="bibr">Chen et al., 2020a)</ref>; and (2) end-to-end neural networks such as a paradigm of sequence-to-sequence model that takes the context of question answering (e.g., question and table cells) as input to generate natural-language answers <ref type="bibr">(Li et al., 2021b;</ref><ref type="bibr" target="#b35">Pan et al., 2022;</ref><ref type="bibr" target="#b16">Herzig et al., 2021;</ref><ref type="bibr" target="#b34">Pan et al., 2021;</ref><ref type="bibr" target="#b2">Chen, 2023)</ref>.</p><p>Despite much progress made on factoid TableQA, a contradiction between the factoid TableQA and TableQA exists in real scenarios. In factoid TableQA, the answers are always in a short   form with a few words directly copied from the relevant table cells. However, in real-world scenarios, the answers are expected to be long and informative sentences in a free form, motivating us to target the free-form TableQA in this paper.</p><p>It is challenging to generate coherent and faithful free-form answers over tables. (1) The wellpreserved spatial structure of tables is critical for retrieving relevant table cells to the question. Different from factoid TableQA, free-form TableQA with sophisticated question shares less semantic similarities to the table content, while depending more on the spatial structure of tables to infer multiple related cells such that the related cells may be located in a relatively connected area, e.g., from either a few selected rows or columns. (2) The selected table cells, containing the key point, are insufficient for composing the entire coherent sentences. To generate fluent natural-language sentences as answers, external information such as the relevant background knowledge about the question is necessary. (3) It is expected to aggregate and reason from the question, retrieved table cells, and external knowledge to compose a reasonable answer. Given the heterogeneous information, a practical model should be capable of aggregating the information efficiently and generating a coherent and fluent free-form answer.</p><p>Figure <ref type="figure">1</ref> provides a motivating example to illustrate the insights of this paper. Given a table describing "the 1983 Manx Grand Prix Newcomers Junior Race Results" and a question "Who won in the first three places of The Newcomers Manx Grand Prix race?", the goal is to select relevant cells first and then generate a natural sentence as an answer. From this table, we can observe that the state-of-the-art model TAPAS and MATE only select the "rider" while missing the "rank" column, providing low cell selection coverage. For the overall generation quality, we can observe that both the end-to-end T5 <ref type="bibr" target="#b40">(Raffel et al., 2020)</ref> and the pipeline-based TAPAS <ref type="bibr" target="#b17">(Herzig et al., 2020)</ref> and MATE <ref type="bibr" target="#b10">(Eisenschlos et al., 2021)</ref> are missing key information from the table by merely mentioning part of the three riders. In addition, the TAPAS introduces a hallucinated rider named "Spaniard". These observations motivate us to design a model that can select the relevant cells more accurately and generate faithful answers grounded on the table given a question.</p><p>Based on the aforementioned insights, this paper designs a three-stage pipeline framework to tackle the problem of free-form TableQA. Even though the end-to-end TableQA models with high accuracy are prevalently ascribed to the suppression of error accumulated from one-stage training, the long table distracts the model from focusing on relevant table cells, resulting in irrelevant answers. On the other hand, the cell selection module provides a controllable and explainable perspective by extracting a small number of table cells as anchors for the model to generate answers. For the content selection stage, inspired by the recent success of graph models, we convert the table to a graph by designing the node linking and applying a Graph Neural Network (GNN) to aggregate node information and classify whether the table cell is relevant or not. In addition, to generate informative free-form answers, we employ a spare retrieval technique to explore extra knowledge from Wikipedia. Consequently, both the extra knowledge and relevant cells are taken into account to calibrate the pre-trained language model bias. Lastly, we adopt a fusion layer in the decoder to generate the final answer.</p><p>To summarize, the primary contributions of this paper are three-fold. (1) To the best of our knowledge, we are the first to convert a semi-structured table into a graph, and then design a graph neural network to retrieve relevant table cells. (2) External knowledge is leveraged to fill in the gap between the selected table cell and the long informative answer by providing background information. (3) Comprehensive experiments on a public dataset named FeTaQA <ref type="bibr" target="#b33">(Nan et al., 2022)</ref> are performed to verify the effectiveness of TAG-QA. Experimental results show that TAG-QA outperforms the strong baseline TAPAS by 17% and 14%, and outperforms the end-to-end T5 model by 16% and 12%, in terms of BLEU-4 and PARENT F-score, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TAG-QA Approach</head><p>In this section, we first formulate the problem of TableQA, and introduce the details of our proposed approach TAG-QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation</head><p>A free-form question-answering task is formulated as generating an answer a to a question q based on a semi-structured table T including table cell content and table meta information such as column, and row header. Different from the factoid table question answering task with a short answer, the free-form QA aims at generating informative and long answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Overview</head><p>Figure <ref type="figure">2</ref> illustrates the overall architecture of our proposed TAG-QA, which is composed of three stages, i.e., relevant </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Relevant Table Cell Localization</head><p>The initial phase of TAG-QA involves table content selection, a pivotal step that serves as the foundation for subsequent stages. Notably, this stage is of utmost importance as it supplies essential input to the subsequent processes. FeTaQA presents a formidable challenge as a dataset, with a Median/Avg percentage of relevant table cells at 10.7%/16.2%. In order to enhance the precision of the content selection stage, we design a tableto-graph converter to preserve the inherent spatial structure of the tables. We employ GNN to effectively aggregate information at the cell level and subsequently perform a classification task on the table cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table-to-Graph</head><p>Converter State-of-the-art models prefer to adopt the pre-trained Language Models (LMs) to make predictions by transforming the semi-structured table into natural sentences using a pre-defined template . However, they lose the table structure information and deteriorate the performance of downstream tasks.</p><p>TAG-QA designs a table-to-graph converter to transform a table into a graph, preserving the table structure by identifying the cell-to-cell relations. Figure <ref type="figure">3</ref> shows an example of transforming a table into a graph. For the i-th row, we add an empty row header as rhi which reflects the entire row information. All the table cells from the same row are fully connected, and all the table cells from the same column are also fully connected. Besides, we design two types of relations for the table graph, i.e., "of the same row" and "of the same column" relations. In particular, "of the same row" relation captures the entity information, while "of the same column" relation reveals the connection of the same attribute.</p><p>In addition, to incorporate the question node into the graph, we create a question node and assign a linking edge between the question and each table cell with the relation "question to cell".</p><p>TAG-QA Content Selection Inspired by QA-GNN <ref type="bibr" target="#b52">(Yasunaga et al., 2021)</ref>, we propose a content selection module (TAG-CS) that retrieves relevant table cells from the table-based graph. TAG-CS takes the converted table graph from Sec. 2.3 as input, and outputs the question-related table cells. TAG-CS reasons over the table cell level, and each graph node represents a table cell. To fully explore the table semantic and the spatial information, TAG-CS acquires the initial graph node embedding through a pre-trained LM e.g., BERT. Besides, the pre-trained LM and GNN are jointly trained to predict the selected cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GNN Architecture</head><p>We use Graph Attention Network (GAT) <ref type="bibr" target="#b45">(Veličković et al., 2017)</ref> which leverages masked self-attention layers and employs iterative message passing among neighbors is applied to predict the selected graph node. GAT follows Eq. 1 to update the i-th node feature h l i ∈ R D at layer l through gathering the weighted attention among its neighbors N i .</p><formula xml:id="formula_0">h l i = f g   sϵNt∪{t} α st m st   + h l-1 t (1)</formula><p>where α st and m st ∈ R N are the self-attention weight and the message passed from source node s to target node t respectively, and f g is a 2-layer Multi-Layer Perceptron (MLP) with batch normalization. The message m st ∈ R N from node v s to v t is computed using Eq. 2.</p><formula xml:id="formula_1">m st = f m (h l-1 s , u s , r st )<label>(2)</label></formula><p>where u s ∈ R T /2 is the source node s feature linearly transformed from the one hot vector node type u t . r st ∈ R T is the relation feature from source node s to target node t computed through a 2-layer MLP by taking relation type, source, and target node type into account. f m is a linear transformation.</p><p>The self-attention coefficient α st is updated in Eq. 3. Query and key vectors are linearly transformed by g q and g k , as node, edge feature, and the previous layer hidden state provided.</p><formula xml:id="formula_2">α st = exp(γ st ) t ′ ϵNs∪{s} exp(γ st ′ )) , γ st = Q T s K t √ N (3) Q s = g q (h l-1 s , u s , r st )<label>(4)</label></formula><formula xml:id="formula_3">K t = g k (h l-1 s , u t , r st )<label>(5)</label></formula><p>GNN Training and Inference Given a question q and a table T , TAG-CS reasons over a graph containing both the table cell nodes and the question node by making predictions on the row and column level. We observe that relevant table cells tend to show up in a relatively connected area, thus we make predictions over row and column headers and choose the intersection area. Compared to predicting over the cell level which results in low recall, our method gains a higher chance to capture relevant table cells. For the training stage, TAG-CS maximizes the cross entropy to predict the row and column for relevant cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">External Knowledge Retrieval</head><p>TAG-QA is the first attempt to leverage the external knowledge to address the table-based freeform QA task. TAG-QA adopts an effective and simple Spare Retrieval based on the TF/IDF approach to select a potentially relevant context from Wikipedia.</p><p>Sparse Retrieval For TAG-QA, the external knowledge is served as a complimentary background context for the next table and text fusion stage. We choose the spare retrieval method using BM25 <ref type="bibr" target="#b42">(Robertson and Zaragoza, 2009)</ref> as a ranking function to retrieve the most relevant text as supplementary information. Given a query q with m keywords k 1 , k 2 , . . . , k m , the BM25 ranking score p i for document d i is calculated by Eq. 6,</p><formula xml:id="formula_4">p i = m j=1 idf (q j ) × tf (q j , d i ) × (α + 1) tf (k j , d i ) + α(1 -β + β |d i | L D )<label>(6)</label></formula><p>Precision Recall F-1 TAPAS <ref type="bibr" target="#b17">(Herzig et al., 2020)</ref> 65.31 24.20 35.32 MATE <ref type="bibr" target="#b10">(Eisenschlos et al., 2021)</ref> 56 where idf is the Inverse Document Frequency (IDF), tf (k j , d i ) is the term frequency of the keyword k j in document d i , and L D is the average document length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Table-Text Fusion</head><p>After obtaining the predicted highlighted table cells from the table as well as the support context from Wikipedia, TAG-QA aggregates and combines the two information sources through a sequence-tosequence model Fusion-in-Decoder (FiD) <ref type="bibr" target="#b18">(Izacard and Grave, 2021)</ref>. FiD appends the question to each information source, encoding each component independently. It subsequently merges all source features and transmits them to the decoder.</p><p>Fusion in Decoder Fusion-in-Decoder based on T5 <ref type="bibr" target="#b40">(Raffel et al., 2020)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Analysis</head><p>In this section, we explore the following experimental questions: (1) Does proposed TAG-QA generate a more coherent and faithful answer compared with the baseline? (2) Is table cell selection, knowledge retrieval, and fusion necessary for the free-form TableQA? (3) Is it promising to keep enhancing the three modules of TAG-QA?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>This paper focuses on tackling the challenge of generating long free-form answers, rather than the short factoid responses. Consequently, we have opted for the utilization of the state-of-the-art dataset, FetaQA <ref type="bibr" target="#b33">(Nan et al., 2022)</ref>, as our testbed.</p><p>The training dataset comprises 7,327 instances, while the development and test sets encompass 1,002 and 2,004 examples, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>TAG-CS) TAG-CS applies BERT checkpoint "bert-based-uncased" to learn the table cell representation. For the BERT model, we set the learning rate to 1e-6 and impose a maximum token length of 35 for each cell. Subsequently, the acquired table cell-level embeddings serve as input node features for our GNN. Within the TAG-CS framework, our GNN module comprises 3 layers, each with node features of 200 dimensions. Additionally, we apply a dropout rate of 0.2 to each layer for regularization. We train our model on the FeTaQA dataset, configuring it to run for a maximum of 50 epochs. We employ the RAdam optimizer <ref type="bibr" target="#b26">(Liu et al., 2019)</ref> with a weight decay of 0.01, utilizing a powerful 24G memory Titan-RTX GPU. To optimize GPU memory usage, we set the maximum number of table cells as 200 and set the batch size as 1. The selection of the best checkpoint is based on the performance of the model on the development set, which is then used for decoding the test set. Additionally, to enhance efficiency, TAG-CS is employed to select intersection cells from the top 3 rows and 3 columns as the relevant cells, drawing upon our accumulated experience in this context. Sparse Retrieval) Our implementation relies on the PyTorch-based toolkit Pyserini, designed for reproducible information retrieval research using both sparse and dense representations. We utilize the question as the query to retrieve pertinent contextual information from Wikipedia, selecting the first sentence from the top results. We specifically employ the Lucene Indexes, denoted as "enwikiparagraphs"<ref type="foot" target="#foot_0">2</ref> . FiD) In the context of FiD, TAG-QA employs the Adam optimizer with a learning rate of 1e-5. We select the best checkpoint for inference purposes. In the inference phase, we utilize beam search with a beam size of 3 and apply a length penalty of 1 when generating answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baselines</head><p>To validate the effectiveness of TAG-QA, we choose two different types of methods as baselines, including end-to-end and pipeline-based models. Table <ref type="table">2</ref>: Results on FeTaQA dataset. "P/R/F" denotes the precision/recall/F score. We report end-to-end model UniLM, BART and T5, and the pipeline results. The results of various table cell selection strategies TAPAS, MATE and our proposed TAG with T5 as backbone generation model are noted as TAPAS-T5, MATE-T5 and TagQA-T5. To validate the effectiveness of proposed framework components, we test different combinations of source information to models where "Q" is question, "Retrieve" is the retrieved external knowledge, "fullTab" is full table, and "predCell" refers to the selected table cell. And the last row TAGQA-FiD is the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Overall Reference 4.94 UniLM [end-to-end] <ref type="bibr" target="#b9">(Dong et al., 2019)</ref> 3.88 BART [end-to-end] <ref type="bibr" target="#b19">(Lewis et al., 2020)</ref> 3.67 T5 [end-to-end] <ref type="bibr" target="#b40">(Raffel et al., 2020)</ref> 3.81 Tapas [pipeline] <ref type="bibr" target="#b17">(Herzig et al., 2020)</ref> 3.38 MATE [pipeline] <ref type="bibr" target="#b10">(Eisenschlos et al., 2021)</ref> 3.30 TAG-QA <ref type="bibr">[pipeline]</ref> 3.93</p><p>Table <ref type="table">3</ref>: Results of human evaluation for reference, end-to-end model and pipeline methods. TAG-QA outperforms the pipeline models by a large margin, and achieves performance on par with the strong end-to-end baseline model T5.</p><p>Firstly, we compare TAG-QA with strong stateof-the-art end-to-end pre-trained generative LMs. UniLM <ref type="bibr" target="#b9">(Dong et al., 2019)</ref>, BART <ref type="bibr" target="#b19">(Lewis et al., 2020)</ref>, and T5 <ref type="bibr" target="#b39">(Radford et al., 2019)</ref>. For the input format to the end-to-end model, we flatten the table by concatenating special token <ref type="bibr">[SEP]</ref> in between different table cells, and concatenate with the question as a natural sentence, e.g. "question [SEP] flattened table". Furthermore, we compare the performance of our proposed model with pipelinebased methods which include two stages: content selection and answer generation. Content selection makes predictions of relevant cells. We choose two table-based pre-training models: TAPAS <ref type="bibr" target="#b17">(Herzig et al., 2020)</ref> and MATE <ref type="bibr" target="#b10">(Eisenschlos et al., 2021)</ref>. Moreover, T5 is chosen as the baseline model's answer generation backbone due to the integration capacity for the table cell and retrieved knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Automatic Evaluation Metrics</head><p>We use various automatic metrics to evaluate the model performance. Due to the pipeline style of TAG-QA, we report two sets of metrics for content selection and answer generation stages respectively. Firstly, to evaluate the retrieval competency of the table semantic parser, we report Precision, Recall, and F1 scores. Besides, to evaluate the answer generation quality, we choose several automatic evaluation metrics, i.e., <ref type="bibr">BLEU-4 (Papineni et al., 2002)</ref>, ROUGE-L <ref type="bibr" target="#b24">(Lin, 2004)</ref> and METEOR <ref type="bibr" target="#b0">(Banerjee and Lavie, 2005)</ref>, to evaluate the n-gram match between the generated sentence and the reference answer. Considering the limitation that those metric fails to reflect the faithfulness answer to the fact from the table, we report PARENT <ref type="bibr" target="#b8">(Dhingra et al., 2019)</ref> and PARENT-T <ref type="bibr" target="#b50">(Wang et al., 2020)</ref> score. PARENT score takes the answer matching with both the reference answer and the table information into account, while PARENT-T focuses on the overlap between the generated answer with the corresponding table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results</head><p>We first evaluate the TAG-CS content selection stage table semantic parsing results, as shown in Table <ref type="table" target="#tab_4">1</ref>. For the F-1 score, TAG-QA outperforms the strong baseline model TAPAS and MATE by 9.9% and 13.27%. For recall, TAG-QA achieves the best result, demonstrating that TAG-QA retrieves more relevant table cells. For precision, the baseline model outperforms TAG-QA by retrieving fewer cells which includes more relevant cells. However, the low precision and high recall are a trade-off since the relevant cells make a stronger impact on the overall answer generation quality. Thus, we can tolerate a small amount of irrelevant cells and keep the correct cells as many as possible.</p><p>In addition, Table <ref type="table">2</ref> shows the measurements of generated answer quality using TAG-QA compared to previous both end-to-end and pipelinebased state-of-the-art models. From overlappingbased metrics BLEU-4, METEOR, and ROUGE-L, TAG-QA outperforms all the end-to-end and pipeline-based models. Specifically, TAG-QA gains 14.27%/1.86%/9.93% more than the best endto-end model UniLM in "Q-fullTab" while gains 14.76%/8.98%/13.88% in "Q-predCell" setting, more than the best pipeline-based model TAPAS. For faithfulness metric PARENT and PARETN-T, TAG-QA provides the best performance among the pipeline models by outperforming TAPAS on the "Q-predCell" setting by 13.92% and 3.88% on PARENT and PARENT-T. Compared with endto-end models, TAG-QA gives the best PARENT score while UniLM shows the best result regarding PARENT-T. It's explainable because TAG-QA incorporates information outside of the table to generate answers, achieving a trade-off between being grounded on the table and synthesizing informative answers.</p><p>Furthermore, to answer Question 2 "Are three stages of the framework necessary to generate highquality answer?", we conduct an experiment in Table <ref type="table">2</ref> by comparing the T5 model "Q-fullTab" with pipeline methods backend by T5 using "Q-predCell". The result shows proposed TAG for content selection TAGQA-T5 selecting 7% of table cell outperforms T5 with fullTab. This indicates the table cell selection is necessary since relevant cells provide an anchor to generate high answer generation. Moreover, to investigate the effect of retrieval knowledge, we show results in knowledge enhances model performance by providing background knowledge. The proposed model TAGQA-T5 provides the best result by integrating retrieval and informative selected cells. Lastly, our fusion module further enhanced the overall performance by aggregating tables and text efficiently.</p><p>Last but not least, to answer the question "Is there space to further enhance performance using this framework ?", we conduct an oracle experiment shown in "Oracle-T5". With the simple Retrieval technique, T5 backend generation, and oracle table cell, the BLEU-4 result is 31%, and PARENT, PARENT-T are over 30%. If a better retrieval and fusion model is used, the model performance can be further boosted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Analysis</head><p>To further evaluate the quality of generated answer by various state-of-the-art models when compared to the ground-truth answer, we perform an additional human evaluation. Besides, we conduct an ablation study for TAG-QA to validate the three building blocks: jointly training of LM and GNN for TAG-CS, external context retrieved from Wikipedia, and FiD model. Furthermore, a case study is presented which shows different answer qualities produced by various models.</p><p>Human Evaluation Following <ref type="bibr" target="#b33">(Nan et al., 2022)</ref>, we recruit three human annotators who pass the College English Test (CET-6)<ref type="foot" target="#foot_1">3</ref> to judge the quality of the generated sentence. We randomly draw 100 samples from test examples in FeTaQA dataset and collect answers from TAG-QA and baseline models. Then, we present the generated answers to three human annotators without revealing the name of the model, thus reducing human variance.</p><p>We provide instructions for human raters to evaluate the sentence quality from four aspects: faithfulness, fluency, correctness, and adequateness. For each aspect, an annotator is supposed to assign a score ranging from 1 (worst) to 5 (best) based on the answer quality. The "overall" column refers to the average ranking of the model. First, for fluency, the annotator checks if an answer is natural and grammatical. Second, for correctness, we compare the answer with the ground truth by checking if the predicted answer contains the correct information. Third, adequacy reflects if an answer contains all the aspects that are asked. Finally, faithfulness evaluates faithfulness if an answer is faithful and grounded to the contents of the highlighted table region such that it covers all the relevant information from the table while not including other key information outside of the table. From Table <ref type="table">3</ref>, we can see TAG-QA ranked the top among all models.</p><p>Ablation Study To figure out which building blocks are driving the improvements, we examine different ablated models to understand each component of TAG-QA, including joint training of BERT and GNN from TAG-CS, sparse retrieval, and FiD. Table <ref type="table" target="#tab_7">4</ref> presents the ablation results under different evaluation metrics. We can see that the model performance drops when any component is removed. Especially, ablating the sparse retrieval module results in the most drop in BLEU-4 and PARENT scores, while removing FiD causes the most significant drop in PARENT-T.</p><p>Case Study To inspect the effect of TAG-QA directly, we present a case study in Figure <ref type="figure">4</ref>, where a sampled table, question, ground-truth relevant table cells (highlighted in blue), the predicted answers of models, as well as the reference are provided. First, we find that the end-to-end model generally contains more information than pipeline models due to the more abundant table information while they suffer from hallucination. For example, T5 and BART identify the ranking position of "Leandro de Oliveira" as "17th" while it should be "73rd" from the table. Second, for pipeline models, they tend to generate irrelevant information e.g. MATE mentions the duration and points instead of answering the ranking position and the event. Third, both the end-to-end and pipeline models (TAPAS) fail to cover all the relevant information from the table, e.g. UniLM did not capture the event 12km, and TAPAS fails to mention the position 73rd. By contrast, TAG-QA provides the highest table coverage while keeping the fluency of sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>In this section, we review the related work to ours from the perspectives of TableQA, GNN for natural language processing, and knowledge-grounded text generation.</p><p>TableQA FeTaQA is the first TableQA dataset that addresses the significance of free-form answer generation, while most current research work including WikiTableQuestions <ref type="bibr" target="#b38">(Pasupat and Liang, 2015)</ref>, Spider <ref type="bibr" target="#b55">(Yu et al., 2018)</ref>, HybridQA <ref type="bibr">(Chen et al., 2020b)</ref>, OTT-QA <ref type="bibr">(Chen et al., 2020a)</ref>, and TAT-QA <ref type="bibr" target="#b58">(Zhu et al., 2021)</ref> focuses on the short factoid answer generation. The early solution <ref type="bibr" target="#b57">(Zhong et al., 2017;</ref><ref type="bibr" target="#b23">Liang et al., 2017)</ref> of addressing the TableQA is to parse the natural question into a machine-executable meaning representations that can be used to query the table. To reduce the laborintensive logical annotation, a semantic parser trained over weak supervision from denotations has been drawing attention. Plenty of Transformerbased table pre-traininig models demonstrate decent TableQA performance, e.g., TaPas <ref type="bibr" target="#b17">(Herzig et al., 2020)</ref>, MATE <ref type="bibr" target="#b10">(Eisenschlos et al., 2021)</ref>, TaBERT <ref type="bibr" target="#b53">(Yin et al., 2020)</ref>, StruG <ref type="bibr" target="#b7">(Deng et al., 2021)</ref>, GraPPa <ref type="bibr">(Yu et al., 2021)</ref>, and TaPEx <ref type="bibr">(Liu et al., 2022a)</ref>. In addition, rather than explore table structure, RCI <ref type="bibr" target="#b13">(Glass et al., 2021)</ref> assumes the row and column are independent, and predicts the probability of containing the answer to a question in each row and column of a table individually.</p><p>GNN for Natural Language Processing Apart from the extensively renowned causal language models that have showcased impressive results in various task <ref type="bibr" target="#b44">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b37">Parmar et al., 2018;</ref><ref type="bibr">Wang et al., 2023a</ref><ref type="bibr" target="#b47">Wang et al., , 2022</ref><ref type="bibr">Wang et al., , 2023b))</ref>, a rich variety of language processing tasks gain improvements from exploiting the power of GNN <ref type="bibr" target="#b22">(Li et al., 2015)</ref>. Tasks such as semantic parsing <ref type="bibr">(Chen et al., 2021a</ref>), text classification <ref type="bibr" target="#b25">(Lin et al., 2021</ref>), text generation <ref type="bibr" target="#b12">(Fei et al., 2021</ref>), question answering <ref type="bibr" target="#b46">(Wang et al., 2021;</ref><ref type="bibr" target="#b52">Yasunaga et al., 2021</ref>) can be expressed with a graph structure and handled with graph-based methods. In addition, researchers apply GNN to model the text generation from structured data tasks e.g. graph-to-sequence (Marcheggiani and Perez-Beltrachini, 2018), and AMR-totext <ref type="bibr" target="#b41">(Ribeiro et al., 2019)</ref>.</p><p>Knowledge-Grounded Text Generation Encoder-decoder-based models have been proposed to tackle the generation task by mapping the input to the output sequence. However, the input text is insufficient to provide knowledge to generate decent output due to the lack of commonsense, factual events, and semantic information. Knowledge-grounded text generation incorporating external knowledge such as linguistic features <ref type="bibr">(Liu et al., 2021c)</ref>, knowledge graph <ref type="bibr">(Liu et al., 2021b;</ref><ref type="bibr">Li et al., 2021a)</ref>, knowledge base <ref type="bibr" target="#b11">(Eric and Manning, 2017;</ref><ref type="bibr" target="#b15">He et al., 2017;</ref><ref type="bibr">Liu et al., 2022b)</ref>, and textual knowledge <ref type="bibr">(Liu et al., 2021a;</ref><ref type="bibr" target="#b56">Zhao et al., 2021)</ref> help to generate a more logical and informative answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper presents a generalized pipeline-based framework TAG-QA for free-form long answer generation for TableQA. The core idea of TAG-QA is to divide the answer generation process into three stages: (1) transform the table into a graph and jointly reason over the question-table graph to select relevant cells; (2) retrieve contextual knowledge from Wikipedia using sparse retrieval, and (3) integrate the selected cells with the content knowledge to predict the final answer. Extensive experiments on a public dataset FeTaQA are conducted to verify the generated answer quality from both the fluency and faithfulness aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>One limitation of TAG-CS, which accepts the entire table as input, arises when dealing with large tables, as training both BERT and the graph model simultaneously becomes challenging due to GPU memory constraints. Consequently, one promising avenue for future research involves the efficient modeling of large tables. Furthermore, it's worth noting that the availability of only one public dataset, FeTaQA, for free-form TableQA, has constrained our validation efforts to this single dataset. However, we are committed to expanding the scope of our research in the future by evaluating the performance of our pipeline model, TAG-QA, across multiple free-form TableQA datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Who won in the first three places of The Newcomers Manx Grand Prix race? [A]: The Newcomers Manx Grand Prix race was won by Robert Dunlop from Steve Hislop in 2nd place and Ian Lougher in 3rd place at an race speed of 100.62. Text Knowledge Retrieval( §2.3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Scotland's Steve Hislop finished second with 101.27 mph and Wales' Ian Lougher finished third with 100.62 mph.</figDesc><table><row><cell>Rank</cell><cell>Rider</cell><cell>Team</cell><cell>Speed</cell><cell>Time</cell></row><row><cell>1</cell><cell>Northern Ireland Robert D</cell><cell>Yamaha</cell><cell>102.46 mph</cell><cell>1:28.22.2</cell></row><row><cell>2</cell><cell>Scotland Steve Hislop</cell><cell>Yamaha</cell><cell>101.27</cell><cell>1:29.24.8</cell></row><row><cell>3</cell><cell>Wales Ian Lougher</cell><cell>Yamaha</cell><cell>100.62</cell><cell>1:29.59.2</cell></row><row><cell>[T5]:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table -</head><label>-</label><figDesc></figDesc><table><row><cell>to-Graph</cell><cell></cell></row><row><cell>Conversion ( §2.2)</cell><cell>Content Selection ( §2.2)</cell></row><row><cell></cell><cell>Fusion in Decoder( §2.4)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table Figure 2</head><label>Figure</label><figDesc>: An overview of TAG-QA. The input to TAG-QA is a combination of one table and question, while the output is an answer. The top box shows the content selection process which first converts the table to a graph and selects relevant nodes using GNN. The middle box shows the process of using the spare retrieval technique to retrieve relevant text as complementary information. The rightmost blue box is to integrate the selected cells and retrieved texts to generate the final answer.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Content selection results on FeTaQA dataset.</figDesc><table><row><cell></cell><cell>.93</cell><cell>22.21 31.95</cell></row><row><cell>TAG-QA (Ours)</cell><cell>47.60</cell><cell>43.06 45.22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Table 2 by concatenating "Retrieval" to the input. The retrieval Ablation study of the proposed model. We examine the ablated mode by removing the Joint Training (JT) of TAG-CS, Sparse Retrieval (SR), and FiD.</figDesc><table><row><cell>Model</cell><cell cols="4">BLEU METEOR PARENT PARENT-T</cell></row><row><cell>TAG-QA</cell><cell>31.84</cell><cell>30.16</cell><cell>29.59</cell><cell>28.26</cell></row><row><cell cols="2">TAG-QA w/o JT 31.35</cell><cell>29.65</cell><cell>28.93</cell><cell>27.48</cell></row><row><cell cols="2">TAG-QA w/o SR 18.93</cell><cell>24.95</cell><cell>21.57</cell><cell>27.95</cell></row><row><cell cols="2">TAG-QA w/o FiD 21.51</cell><cell>24.03</cell><cell>22.46</cell><cell>25.40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Leandro de Oliveira represented Brazil at the 2011 World Cross Country Championships and placed 73rd in the 12 km race. [REF]: Leandro de Oliveira represented Brazil at the 2011 World Cross Country Championships and placed 73rd in the 12 km race.Figure4: A case study from FeTaQA for qualitative analysis. The highlighted cells are the ground-truth relevant table cells. "RB" refers to "Representing Brazil". Hallucinated content from the predicted answer is marked in red and the correct content in blue.</figDesc><table><row><cell>Year</cell><cell>Competition</cell><cell>Venue</cell><cell>Pos</cell><cell>Event</cell></row><row><cell>RB</cell><cell>Representing Brazil</cell><cell>Representing Brazil</cell><cell>RB</cell><cell>RB</cell></row><row><cell>2011</cell><cell>World Cross Country Championships</cell><cell>Punta Umbría Spain</cell><cell>73rd</cell><cell>12 km</cell></row><row><cell>2011</cell><cell>World Cross Country Championships</cell><cell>Punta Umbría Spain</cell><cell>17th</cell><cell>Team -12 km</cell></row><row><cell>2011</cell><cell>South American Road Mile Championships</cell><cell>Belém Brazil</cell><cell>1st</cell><cell>One mile</cell></row><row><cell>2011</cell><cell>South American Championships</cell><cell>Buenos Aires Argentina</cell><cell>1st</cell><cell>1500m</cell></row><row><cell>2011</cell><cell>Pan American Games</cell><cell>Guadalajara México</cell><cell>18th</cell><cell>200m</cell></row><row><cell cols="5">[Q]: What country did Leandro de Oliveira represent at the 2011 World Cross</cell></row><row><cell cols="3">Country Championships and how did he place?</cell><cell></cell><cell></cell></row><row><cell cols="5">[T5-end-to-end]: Leandro de Oliveira represented Brazil at the 2011 World Cross</cell></row><row><cell cols="2">Country Championships and placed 17th.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">[Bart-end-to-end]: Landro de oliveira represented brazil at the 2011 World cross</cell></row><row><cell cols="3">country championships in the 12 km and finished 17th.</cell><cell></cell><cell></cell></row><row><cell cols="5">[Unilm-end-to-end]: Leandro de Oliveira was representing Brazil at the 2011</cell></row><row><cell cols="5">World Cross Country Championships in Punta Umbría , Spain , finishing 73rd .</cell></row><row><cell cols="5">[TAPAS-pipeline]: Leandro de Oliveira represented Brazil at the 2011 World</cell></row><row><cell cols="2">Cross Country Championships.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">[MATE-pipeline]: Leandro de Oliveira finished in 37:10 and finished with 326</cell></row><row><cell cols="2">points.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">[Ours-pipeline]:</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/castorini/pyserini</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>A national English as a foreign language test in China.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Question answering from frequently asked question files: Experiences with the faq finder system</title>
		<author>
			<persName><forename type="first">Robin D</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><forename type="middle">J</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Kulyukin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noriko</forename><surname>Steven L Lytinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Tomuro</surname></persName>
		</author>
		<author>
			<persName><surname>Schoenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="57" to="57" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large language models are few(1)shot table reasoners</title>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.findings-eacl.83</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EACL 2023</title>
		<meeting><address><addrLine>Dubrovnik, Croatia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1120" to="1130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10439</idno>
		<title level="m">Open question answering over tables and text</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07347</idno>
		<title level="m">Hybridqa: A dataset of multi-hop question answering over tabular and textual data</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">2021a. ShadowGNN: Graph projection neural network for text-to-SQL parser</title>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruisheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.441</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="5567" to="5577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">FinQA: A dataset of numerical reasoning over financial data</title>
		<author>
			<persName><forename type="first">Zhiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charese</forename><surname>Smiley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameena</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iana</forename><surname>Borova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Langdon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reema</forename><surname>Moussa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Beane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting-Hao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Routledge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.300</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3697" to="3711" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structure-grounded pretraining for text-to-SQL</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Hassan Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.105</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1337" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Handling divergent reference texts when evaluating table-to-text generation</title>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1483</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4884" to="4895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MATE: Multi-view attention for table transformer efficiency</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Eisenschlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maharshi</forename><surname>Gor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.600</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7606" to="7619" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A copyaugmented sequence-to-sequence architecture gives good performance on task-oriented dialogue</title>
		<author>
			<persName><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="468" to="473" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Iterative GNN-based decoder for question generation</title>
		<author>
			<persName><forename type="first">Zichu</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.201</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2573" to="2582" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Capturing row and column semantics in transformer based question answering over tables</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Canim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfio</forename><surname>Gliozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saneem</forename><surname>Chemmengath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishwajeet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishav</forename><surname>Chakravarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Sil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feifei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samarth</forename><surname>Bharadwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><forename type="middle">Rodolfo</forename><surname>Fauceglia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.96</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1212" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An endto-end model for question answering over knowledge base with cross-attention combining global knowledge</title>
		<author>
			<persName><forename type="first">Yanchao</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1021</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generating natural answers by incorporating copying and retrieving mechanisms in sequence-to-sequence learning</title>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1019</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="199" to="208" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Open domain question answering over tables via dense retrieval</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Syrine</forename><surname>Krichene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Eisenschlos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.43</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TaPas: Weakly supervised table parsing via pre-training</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Pawel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName><surname>Eisenschlos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.398</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4320" to="4333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.74</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="874" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">2021a. Knowledge-based review generation by coherence enhanced text planning</title>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Jing Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">2021b. Tsqa: tabular scenario based question answering</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yawei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="13297" to="13305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<title level="m">Gated graph sequence neural networks</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural symbolic machines: Learning semantic parsers on Freebase with weak supervision</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kenneth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1003</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="23" to="33" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05727</idno>
		<title level="m">Bertgcn: Transductive text classification by combining gcn and bert</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<title level="m">On the variance of the adaptive learning rate and beyond</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">TAPEX: Table pre-training via learning a neural SQL executor</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morteza</forename><surname>Ziyadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Caiming Xiong, and Philip Yu. 2021a. Dense hierarchical retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kg-bart: Knowledge graph-augmented bart for generative commonsense reasoning</title>
		<author>
			<persName><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="6418" to="6425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">2021c. Enriching non-autoregressive transformer with syntactic and semantic structures for neural machine translation</title>
		<author>
			<persName><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenting</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.105</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="1235" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Uniparser: Unified semantic parser for question answering on knowledge base and database</title>
		<author>
			<persName><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.emnlp-main.605</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="8858" to="8869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep graph convolutional encoders for structured data to text generation</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6501</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Natural Language Generation</title>
		<meeting>the 11th International Conference on Natural Language Generation<address><addrLine>Tilburg University</addrLine></address></meeting>
		<imprint>
			<publisher>The Netherlands. Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Caiming Xiong, Dragomir Radev, and Dragomir Radev</title>
		<author>
			<persName><forename type="first">Linyong</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiachun</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziming</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neha</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Kryściński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riley</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mutethia</forename><surname>Mutuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Rosand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="35" to="49" />
			<date type="published" when="2022">2022</date>
			<pubPlace>Isabel Trindade, Renusree Bandaru, Jacob Cunningham</pubPlace>
		</imprint>
	</monogr>
	<note>FeTaQA: Free-form Table Question Answering</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">CLTR: An end-to-end, transformer-based system for cell-level table retrieval and table question answering</title>
		<author>
			<persName><forename type="first">Feifei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Canim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfio</forename><surname>Gliozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Fox</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-demo.24</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="202" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">End-to-end table question answering via retrieval-augmented generation</title>
		<author>
			<persName><forename type="first">Feifei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Canim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfio</forename><surname>Gliozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hendler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.16714</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1142</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Enhancing AMR-to-text generation with dual graph representations</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Leonardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1314</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3183" to="3194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<title level="m">The probabilistic relevance framework: BM25 and beyond</title>
		<imprint>
			<publisher>Now Publishers Inc</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ori</forename><surname>Yoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amnon</forename><surname>Catav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Lahav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06039</idno>
		<title level="m">Multimodalqa: Complex question answering over text, tables and images</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations (ICLR</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.03192</idno>
		<title level="m">Gnn is a counter? revisiting gnn for question answering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Continuous prompt tuning based textual entailment model for e-commerce entity typing</title>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1383" to="1388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">2023a. Clickconversion multi-task model with position bias mitigation for sponsored search in ecommerce</title>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbing</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Musen</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenting</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<biblScope unit="page" from="1884" to="1888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">2023b. Named entity recognition via machine reading comprehension: A multi-task learning approach</title>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenting</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongfen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.11027</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Towards faithful neural table-to-text generation with content-matching constraints</title>
		<author>
			<persName><forename type="first">Zhenyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bang</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1072" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Information extraction over structured data: Question answering with Freebase</title>
		<author>
			<persName><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1090</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="956" to="966" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">QA-GNN: Reasoning with language models and knowledge graphs for question answering</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.45</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="535" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">TaBERT: Pretraining for joint understanding of textual and tabular data</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Tau Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dragomir Radev, richard socher, and Caiming Xiong. 2021. Gra{pp}a: Grammar-augmented pre-training for table semantic parsing</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chern Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingning</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanelle</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1425</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3911" to="3921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Attend, memorize and generate: Towards faithful table-to-text generation in few shots</title>
		<author>
			<persName><forename type="first">Wenting</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.347</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana</addrLine></address></meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4106" to="4117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Seq2sql: Generating structured queries from natural language using reinforcement learning</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.00103</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance</title>
		<author>
			<persName><forename type="first">Fengbin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youcheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiancheng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.254</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3277" to="3287" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
