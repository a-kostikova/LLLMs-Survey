<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study</title>
				<funder ref="#_neCZ9JQ">
					<orgName type="full">Natural Science Foundation of Guangdong</orgName>
				</funder>
				<funder ref="#_qzUhFVr">
					<orgName type="full">Key Technologies Research and Development Program of Shenzhen</orgName>
				</funder>
				<funder ref="#_jFTedAs #_bfaQTH2">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_BSDMjxp">
					<orgName type="full">Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
							<email>ruiwangnlp@outlook.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
							<email>xuruifeng@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">https://github.com/MilkWhite</orgName>
								<address>
									<country>LLMs_for_ Reference_Free_Text_Quality_Evaluation</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A556CF71E134B6D6F6D13055210BE684</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Evaluating the quality of generated text is a challenging task in NLP, due to the inherent complexity and diversity of text. Recently, large language models (LLMs) have garnered significant attention due to their impressive performance in various tasks. Therefore, we present this paper to investigate the effectiveness of LLMs, especially ChatGPT, and explore ways to optimize their use in assessing text quality. We compared three kinds of referencefree evaluation methods. The experimental results prove that ChatGPT is capable of evaluating text quality effectively from various perspectives without reference and demonstrates superior performance than most existing automatic metrics. In particular, the Explicit Score, which utilizes ChatGPT to generate a numeric score measuring text quality, is the most effective and reliable method among the three exploited approaches. However, directly comparing the quality of two texts may lead to suboptimal results. We believe this paper will provide valuable insights for evaluating text quality with LLMs and have released the used data 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated evaluation of text generation quality has posed a long-standing challenge in the field of natural language processing (NLP). On the one hand, the diverse forms of textual expression make it impossible for reference-based methods to account for all possible situations <ref type="bibr" target="#b39">(Zhang* et al., 2020;</ref><ref type="bibr" target="#b35">Yuan et al., 2021;</ref><ref type="bibr">Chen et al., 2022b)</ref>. On the other hand, devising reliable metrics without reference is not a straightforward task and can also be problematic <ref type="bibr" target="#b28">(Sun and Zhou, 2012;</ref><ref type="bibr">Niu et al., 2021;</ref><ref type="bibr" target="#b26">Shen et al., 2022)</ref>. Furthermore, different types of text necessitate evaluation of distinct aspects, e.g. coherence, fluency, and consistency <ref type="bibr">(Fabbri et al., 2021a;</ref><ref type="bibr">Mehri and Eskenazi, 2020a;</ref><ref type="bibr">Wang et al., 2023b)</ref>, which makes it hard to design metrics for each type of text and dimension separately.</p><p>Nowadays, large language models (LLMs) <ref type="bibr" target="#b0">(Brown et al., 2020;</ref><ref type="bibr" target="#b22">Ouyang et al., 2022;</ref><ref type="bibr" target="#b3">Chung et al., 2022;</ref><ref type="bibr" target="#b3">Chowdhery et al., 2022;</ref><ref type="bibr" target="#b38">Zhang et al., 2022;</ref><ref type="bibr">Touvron et al., 2023;</ref><ref type="bibr" target="#b5">Du et al., 2022)</ref> represented by ChatGPT<ref type="foot" target="#foot_0">2</ref> have revolutionized the field of NLP by achieving remarkable results in a wide range of NLP tasks <ref type="bibr" target="#b27">(Song et al., 2023;</ref><ref type="bibr">Chen et al., 2022a)</ref>. Recent studies <ref type="bibr" target="#b8">(Fu et al., 2023;</ref><ref type="bibr">Wang et al., 2023a;</ref><ref type="bibr" target="#b14">Kocmi and Federmann, 2023;</ref><ref type="bibr" target="#b12">Ji et al., 2023)</ref> have also demonstrated the potential of LLMs in evaluating the quality of generated texts. In this paper, we present an empirical study that compares different methods for text quality evaluation using LLMs in a reference-free mode. The key insights from our empirical findings are as follows:</p><p>• How accurately can ChatGPT assess text quality without references? ( §4.1) It is feasible for ChatGPT to evaluate text quality without reference, and it outperforms commonly used metrics even with a simple prompt design.</p><p>• What is the most effective approach to evaluate text quality using ChatGPT? ( §4) Generally, using ChatGPT to generate an explicit score for text quality is the best and most stable method among the three we compared. We suggest using greedy decoding for more reliable results.</p><p>• Why may directly comparing two texts using ChatGPT yield suboptimal results? ( §5.1) Due to its strict standard for "high-quality" text, ChatGPT often considers most generated texts unsatisfactory. Therefore, distinguishing between two subpar texts becomes challenging for ChatGPT.</p><p>• Why is Implicit Score generally less effective than Explicit Score? ( §5.2) Compared to generating an Explicit Score with ChatGPT, using the confidence of text-davinci models to determine text quality (Implicit Score) is less effective due to different distribution characteristics. Implicit Score has a narrow range and peak structure, while Explicit Score allows better differentiation with its smoother distribution.</p><p>• How can prompt design impact ChatGPT in generating an Explicit Score? ( §5.3) When prompting ChatGPT for an Explicit Score, it would be better to avoid detailed scoring criteria if such criteria lack clear definitions for each score range. A general description of the evaluation standard is enough. Also, making ChatGPT provide justifications in a "chain-of-thought" manner before scoring can lead it to prioritize its reasoning process over the text. These justifications tend to be templated and similar across different texts, reducing the discriminative power of the final score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>We explore two different reference-free paradigms, i.e., Individual Score and Pairwise Comparison for text evaluation using ChatGPT and text-davinci models. Individual Score assesses the quality of a single text by a numerical score, while Pairwise Comparison focuses on the relative quality of two texts and requires a direct comparison to determine which one is superior. Within the Individual Score paradigm, two methods are typically exploited: Explicit Score, obtained through direct text generation, and Implicit Score, obtained through the token probabilities outputted by the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Individual Score</head><p>Explicit Score Conditioned on a given input text (optional), we prompt ChatGPT to directly generate a score to measure the absolute quality of each text individually in terms of a specific aspect or the overall performance. An example prompt designed for scoring the overall quality of a storyline is shown as follows:</p><p>========= PROMPT FOR EXPLICIT SCORE ========= Score the following storyline given the beginning of the story on a continual scale from 0 (worst) to 100 (best), where a score of 0 means "The storyline makes no sense and is totally not understandable" and a score of 100 means "The storyline is perfect-written and highly consistent with the given beginning of the story".</p><p>The beginning of the story:</p><formula xml:id="formula_0">[Conditioned Text] Storyline: [Generated Text] Score:</formula><p>Implicit Score Given the LLM's potential insensitivity to numerical values and the lack of explicit instructions for aligning score intervals with specific criteria, score fluctuations may occur across different samples. Therefore, we propose an alternative approach by framing the problem as a binary Yes or No question, where the confidence level of answering "yes" serves as the Implicit Score. An illustrative example is presented below: ========= PROMPT FOR IMPLICIT SCORE ========= Consider the following storyline written according to the given beginning of the story:</p><p>The beginning of the story:</p><formula xml:id="formula_1">[Conditioned Text] Storyline: [Generated Text]</formula><p>Question: Is the storyline well-written and consistent with the beginning of the story? Answer:</p><p>Unfortunately, access to ChatGPT's token probabilities is currently unavailable. Text-davinci-003 is similar to ChatGPT in that they are both trained through supervised instruction tuning and Reinforcement Learning from Human Feedback (RLHF) based on GPT-3.5, and they both exhibit excellent performance in following and fulfilling human instructions. Therefore, we utilize textdavinci-003 to derive the Implicit Score as a baseline metric instead. To facilitate a more comprehensive comparison, we also obtain the Implicit Score from text-davinci-001, an earlier version of the text-davinci series model which is based on GPT-3 and has not been trained using RLHF. Due to a limitation of the OpenAI API, only the top 5 most probable tokens are returned with log probabilities. Therefore, we instead estimate the Implicit Score using the following formula:  (1)</p><formula xml:id="formula_2">p(yes) =</formula><p>Here, p(t) represents the probability of predicting token t immediately following the prompt "Answer:". The sets T yes and T no consist of the affirmative and negative response tokens, respectively, i.e., T yes = {" Yes", "Yes", " yes", "yes"}, and T no = {" No", "No", " no", "no"}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pairwise Comparison</head><p>Another paradigm to assess text quality is by directly comparing a pair of generated texts based on the same input. This method primarily focuses on the relative quality of the texts. For instance, a prompt for comparing the overall quality of two storylines written according to the same initial story beginning is shown as follows:</p><p>====== PROMPT FOR PAIRWISE COMPARISON ====== Consider the following two storylines written according to the given beginning of the story:</p><p>The beginning of the story:</p><formula xml:id="formula_3">[Conditioned Text] Storyline-1: [Generated Text-1] Storyline-2: [Generated Text-2]</formula><p>Question: Which storyline is better-written and more consistent with the beginning of the story? Please answer with one of the following options.</p><p>Options: (A) Storyline-1 (B) Storyline-2 (C) Both storylines are equally well-written and consistent with the beginning of the story.</p><p>Answer: I will choose Option 3 Experimental Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tasks and Datasets</head><p>We conduct experiments on four distinct natural language generation tasks: Text Summarization, Dialogue Response Generation, Story Generation, and Paraphrase Generation.</p><p>Text Summarization aims to summarize the key points of a given long text. SummEval <ref type="bibr">(Fabbri et al., 2021b</ref>) is a collection of human annotations for 16 model-generated summaries on 100 CNN/DaliyMail news over 4 dimensions: coherence (COH), fluency (FLU), consistency (CON), and relevance (REL). Due to the budget limit, we randomly sample 20 news and corresponding annotations from SummEval for evaluation.</p><p>Dialogue Response Generation aims to generate a response based on the preceding dialogue. We conduct experiments on the dialogue-level FED dataset <ref type="bibr">(Mehri and Eskenazi, 2020a)</ref>, which contains fine-grained human judgments for 124 conversations. The evaluation aspects include coherence (COH), error recovery (ERR), consistency (CON), diversity (DIV), topic depth (DEP), likeability (LIK), understanding (UND), flexibility (FLE), informativeness (INF), inquisitiveness (INQ) and overall performance (Overall). However, we do not include ERR in our evaluation since some annotations are missing.</p><p>Story Generation aims to automatically write a storyline based on a given beginning of the story. We employ OpenMEVA-ROC <ref type="bibr" target="#b11">(Guan et al., 2021)</ref> for evaluation, which contains 200 story beginnings and 5 corresponding machine-generated storylines for each beginning. Each storyline is manually annotated in terms of overall quality.</p><p>Paraphrase Generation aims to rephrase a sentence in different words or forms while preserving its original meaning. We use Twitter-Para <ref type="bibr" target="#b34">(Xu et al., 2014</ref><ref type="bibr" target="#b33">(Xu et al., , 2015) )</ref> for evaluation, containing 761 input sentences and each input has 9.41 paraphrase candidates on average. We adopt the test set <ref type="bibr" target="#b26">(Shen et al., 2022)</ref> extended from Twitter-Para by adding 20% of the input sentences as candidates, denoted as Twitter (Extend).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Chosen Metrics</head><p>Following the settings of previous works, we select baseline metrics from the following widely used metrics accordingly: ROUGE-1, ROUGE-2 and ROUGE-L <ref type="bibr" target="#b15">(Lin, 2004)</ref>; BERTScore <ref type="bibr" target="#b39">(Zhang* et al., 2020)</ref>; MoverScore <ref type="bibr" target="#b40">(Zhao et al., 2019)</ref>; PRISM <ref type="bibr" target="#b29">(Thompson and Post, 2020)</ref>; BARTScore and its enhanced versions, BARTScore+CNN and BARTScore+CNN+Para <ref type="bibr" target="#b35">(Yuan et al., 2021)</ref>; BERT-R (Ghazarian et al., 2019); GPT-2 <ref type="bibr" target="#b23">(Radford et al., 2019)</ref>; USR <ref type="bibr">(Mehri and Eskenazi, 2020b)</ref>; S-DiCoh <ref type="bibr" target="#b19">(Mesgar et al., 2020)</ref>; FED <ref type="bibr">(Mehri and Eskenazi, 2020a)</ref>; DynaEval <ref type="bibr" target="#b37">(Zhang et al., 2021)</ref>; SelfEval <ref type="bibr" target="#b16">(Ma et al., 2022)</ref>; PPL <ref type="bibr" target="#b11">(Guan et al., 2021)</ref>; iBLEU <ref type="bibr" target="#b28">(Sun and Zhou, 2012)</ref>; BERT-iBLEU <ref type="bibr">(Niu et al., 2021)</ref>; ParaScore <ref type="bibr" target="#b26">(Shen et al., 2022)</ref>. Note that, <ref type="bibr" target="#b26">Shen et al. (2022)</ref>  version of BERTScore and ParaScore, denoted as BERTScore.Free and ParaScore.Free.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Meta Evaluation</head><p>Individual Score In order to assess the reliability of Individual Scores, we utilize the Spearman <ref type="bibr" target="#b36">(Zar, 2005)</ref> and Pearson (Mukaka, 2012) correlation coefficients. As SummEval and OpenMEVA provide an equivalent number of model-generated results for each input, we present the sample-level correlations for these datasets. Whereas, for Twitter (Extend) and the dialog-level FED datasets, we report the dataset-level correlations instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pairwise Comparison</head><p>To avoid an excessive volume of requests when testing all permutations of pairwise comparisons in each dataset using Chat-GPT, we have opted to randomly sample 200 pairs from each dataset as an approximation. To estimate the reliability of metrics for pairwise comparison, Kendall's Tau-b <ref type="bibr" target="#b13">(Kendall, 1945)</ref>  It is worth noting that the Implicit Score based on text-davinci-003 also shows promising results. This suggests that LLMs' confidence level in determining whether a text meets a specific standard (yes or no) can reflect the text's quality to some extent. Besides, the Implicit Score based on textdavinci-003 performs better than that based on textdavinci-001 in most cases, perhaps due to RLHF, allowing text-davinci-003 to provide answers that align with human instructions better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pairwise Comparison</head><p>Scoring individual samples without providing detailed criteria for each score range may lead to inconsistent evaluation standards across different samples. Alternatively, we hypothesize that a direct comparison of quality between a pair of samples is more likely to yield reliable evaluation results from ChatGPT. However, our analysis in Tables 5 to 8 suggests that direct pairwise comparison is not as effective as expected, and eliminating the influence of sampling in decoding is not always advantageous for comparison.</p><p>We further categorize the texts for comparison into three levels of difficulty, namely hard, medium, and easy, based on the difference in human scores. The larger the score difference between a pair of texts, the easier it is to discern the better one. The performance of various metrics on distinct difficulty levels is shown in Tables <ref type="table">7</ref> and<ref type="table">8</ref>. Overall, the metrics exhibit an increasing trend in performance as the difficulty decreases. Moreover, our investigation indicates that the Implicit Score derived from text-davinci-003 outperforms or performs comparably to the Explicit Score based on ChatGPT when comparing hard text pairs. This finding may be attributed to the higher precision of the Implicit Score, which is based on the model's output token probability (a floating-point number), as opposed to the model's generated Explicit Score, which is limited to integer values ranging from 0 to 100. 5 Detailed Analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Why does the pairwise comparison paradigm perform worse?</head><p>In the main experiments, it is noteworthy that direct pairwise comparison using ChatGPT did not yield satisfactory results. To investigate whether this was caused by poorly designed prompts, alternative prompts were also evaluated. These prompts are briefly described in Table <ref type="table">9</ref>, with detailed information provided in Appendix B. Surprisingly, changing the prompt did not improve performance, but rather worsened it, as illustrated in Figure <ref type="figure" target="#fig_2">1</ref>.</p><p>To gain further insights, we examined the confusion matrices of results based on different prompts, as shown in Figure <ref type="figure">2</ref>. Our analysis revealed that, although we have provided the option of "both storylines equally good" in the default prompt (Prompt V1), ChatGPT still tended to choose one storyline that it deemed "better", as observed from Fig-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics</head><p>Kend.</p><p>COH ure 2(a). This could be attributed to the bias introduced by adding "Answer: I will choose Option" at the end of the prompt, which may have induced the model to make a biased choice at the beginning of the answer. To address this issue, we modified the prompt to require ChatGPT to present its reasoning process before making the final decision (Prompt V4). With this prompt, the model was more likely to choose the "tie" option, as indicated by the "s1=s2" column in Figure <ref type="figure">2(b)</ref>.</p><p>After analyzing ChatGPT's reasoning process, we discovered that ChatGPT frequently concludes that "the quality of the two given storylines is equally poor." As a result, we prompted ChatGPT to choose the "worse" storyline instead of the "better" one (Prompt V3). However, this questioning approach did not yield a better outcome. In addition, Figure <ref type="figure">2</ref>(c) shows that although Prompt V3 is a mirrored version of Prompt V1, which changes the prompt from selecting the better option to choosing the worse one, ChatGPT's results based on these two prompts are not always consistent. For example, in one case, ChatGPT selected Storyline-1 as better based on Prompt V1, but under the guidance of Prompt V3, it may not necessarily choose Storyline-2 as worse.</p><p>Overall, we speculate that the poor quality of the</p><formula xml:id="formula_4">PROMPTS FOR PAIRWISE COMPARISON ON STORY GENERATION PROMPT V1</formula><p>The default prompt where we first provide the beginning of the story and the corresponding two storylines for comparison before presenting the question. PROMPT V2 A revised version of Prompt V1 where we first propose the question, then provide the beginning of the story and present the two storylines to be compared in the form of options. PROMPT V3 A mirrored version of Prompt V1 where we instruct the model to choose "which one is worse" instead of "which one is better" from the two given storylines. PROMPT V4 A "chain-of-thought" version of Prompt V1 where we require the model to illustrate the reasoning process before presenting the final answer.</p><p>PROMPTS FOR EXPLICIT SCORE ON STORY GENERATION PROMPT V1 The default prompt where we only specify the rating criteria for zero and full marks. PROMPT V2 A rephrased version of Prompt V1. PROMPT V3 A simplified version of Prompt V1 where we only describe the dimensions that need to be evaluated. PROMPT V4 A detailed prompt where we divide the scores into 5 scales and list the corresponding evaluation criteria for each score scale. PROMPT V5 A "chain-of-thought" version of Prompt V1 where we require the model to first present the reasons for the evaluation, and then provide the final score.</p><p>Table <ref type="table">9</ref>: Prompts designed for Pairwise Comparison and Explicit Score for assessing the quality of storylines in story generation. Note that Prompt V4 of Explicit Score is cited from <ref type="bibr">(Wang et al., 2023a)</ref>.</p><formula xml:id="formula_5">VV V V V!V 0HWULF VV V V V!V +XPDQ (a) Prompt V1 (Default) VV V V V!V 0HWULF VV V V V!V +XPDQ (b) Prompt V4 ("Chain-of-Thought" ) VV V V V!V 0HWULF VV V V V!V +XPDQ (c) Prompt V3 (Mirrored)</formula><p>Figure <ref type="figure">2</ref>: Confusion matrices of pairwise comparisons on OpenMEVA based on different prompts using ChatGPT. Prompt V1 is the default prompt used in the main experiments. Prompt V4 and V3 are the "chain-of-thought" and "mirrored" versions of Prompt V1 respectively. Details of these prompts are presented in Table <ref type="table">9</ref> and Appendix B.</p><p>candidate texts used in our experiments is the main reason why comparing pairs directly with ChatGPT did not yield good results. ChatGPT perceives the candidate texts as generally low quality, making it to select a "better" or "worse" one from them. This might lead to ChatGPT's unstable decisions.</p><p>5.2 Why does Explicit Score generally perform better than Implicit Score?</p><p>In order to obtain the Explicit Score, we utilize ChatGPT to generate scores in a natural language format. However, as we do not have access to Chat-GPT's token probabilities, we instead rely on the confidence of text-davinci series models to determine the Implicit Score, which reflects how well a text meets a particular evaluation criterion. As stated in the Main Experiments ( §4), the Explicit  Score is generally more effective than the Implicit Score. This difference in effectiveness could be attributed not only to the variation in the models used but also to the distribution of the two scores. Figure <ref type="figure" target="#fig_3">3</ref> illustrates that the Implicit Score distribution has a peaked structure and is concentrated within a small range. In contrast, the Explicit Score distribution is smoother, allowing for greater discrimination between scores for different texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">How does the prompt design affect</head><p>Explicit Score?</p><p>We also investigate the impact of prompt design on the performance of rating Explicit Scores generated by ChatGPT. The detailed prompts are provided in Appendix A, and their main features and differences are summarized in Table <ref type="table">9</ref>. Our results, presented in Table <ref type="table" target="#tab_0">10</ref>, indicate that paraphrasing (V2) or simplifying (V3) the default prompt (V1) does not significantly affect the performance of Explicit Score based on ChatGPT. In contrast, refining scoring criteria (V4) or providing reasons before scoring (V5) results in a slight decrease in performance. The former may be due to the fact that the refined scoring rules in Prompt V4 do not fully match the standards used for actual manual annotation, and dividing scores into five scales reduces the distinction between scores for different samples. The latter may be due to the overall low quality of the dataset. Our observation indicates that ChatGPT's evaluations for each text are similar and mostly negative. After giving reasons before scoring, ChatGPT's scoring focuses more on the reasons rather than the text itself, resulting in lower scores for each text based on Prompt V5 and reducing the distinction between scores. The detailed distribution of scores derived from different prompts is demonstrated using a violin plot in Figure <ref type="figure" target="#fig_4">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>In the field of text quality evaluation, researchers have devised two main lines of approaches: reference-based and reference-free methods. The reference-based text evaluation aims to assess the quality by comparing outputs with ground truth, e.g. ROUGE <ref type="bibr" target="#b15">(Lin, 2004)</ref>, BERTScore <ref type="bibr" target="#b39">(Zhang* et al., 2020)</ref> and BARTScore <ref type="bibr" target="#b35">(Yuan et al., 2021)</ref>. However, due to the inherent complexity and diversity of text, it is impossible to obtain references covering the entire spectrum of potential outputs. This limitation has prompted researchers to explore reference-free evaluation methods without relying on predefined references e.g. iBLEU <ref type="bibr" target="#b28">(Sun and Zhou, 2012)</ref> and ParaScore <ref type="bibr" target="#b26">(Shen et al., 2022)</ref>. In this line, a reliable sentence representation model is required <ref type="bibr" target="#b9">(Gao et al., 2021;</ref><ref type="bibr">Shen et al., 2023a,b)</ref>.</p><p>Recent studies have indicated that LLM-based evaluation methods can exhibit good consistency with human evaluation in assessing text quality <ref type="bibr" target="#b8">(Fu et al., 2023;</ref><ref type="bibr">Wang et al., 2023a;</ref><ref type="bibr" target="#b14">Kocmi and Federmann, 2023;</ref><ref type="bibr" target="#b12">Ji et al., 2023)</ref>. However, most of these works are preliminary explorations or require gold references. On the contrary, we are the first to conduct extensive experiments to investigate the optimal evaluation approaches using LLMs without references, and moreover propose some clues for customized text evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper explores the feasibility of LLMs, specifically ChatGPT and text-davinci series models, for evaluating text quality in a reference-free mode.</p><p>Through an empirical study, we compare different methods for the evaluation of text quality and recommend the use of an Explicit Score generated by ChatGPT as the most effective and stable approach.</p><p>This paper also highlights the potential problem of directly comparing the quality of two texts using ChatGPT and the limitations of Implicit Scores obtained through the confidence of text-davinci series models. The prompt design is another crucial factor impacting the performance of the Explicit Score generated by ChatGPT. Overall, this paper demonstrates the potential of LLMs in evaluating text quality without reference and we hope it will provide useful insights for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations • Meta Evaluation Strategy</head><p>We primarily assess the reliability of metrics based on their correlation with human scores. However, it should be noted that the consistency between scores annotated by different raters may not always be high in certain datasets. Hence, the correlation with human ratings may not always reflect the performance of metrics appropriately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Coverage of Texts</head><p>We only conducted experiments on four textgeneration tasks. Additionally, the quality distribution of the evaluated texts may be non-uniform, potentially lacking in extremely high-quality texts.</p><p>Even if a metric performs well in evaluating a set of low-quality texts, it does not necessarily imply the same level of discrimination for high-quality texts, and vice versa. Furthermore, our evaluation has been limited to short texts, omitting the consideration of long-text generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Coverage of Models</head><p>We utilize OpenAI's API to access their language models, including ChatGPT (gpt3.5-turbo-0301), text-davinci-003, and text-davinci-001. However, these models may be updated over time, which can result in inconsistencies in experimental outcomes. Moreover, we have not considered a wider range of LLMs, such as text-babbage-001, text-curie-001, and the FLAN-T5 series. Regrettably, due to API limitations, we were unable to obtain results from the more powerful GPT4 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Prompt Design</head><p>Our exploration of prompts was limited to a few basic variations. Future research may benefit from more sophisticated prompt designs, such as incorporating few-shot demonstrations, providing more precise annotation guidelines, or guiding the model through multi-turn conversations to facilitate a more accurate assessment of text quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C An Explanation of Kendall's Tall-b</head><p>Kendall's Tau-b is a measure of the correlation between two variables, specifically designed to handle ties and ranks. The formula to calculate Kendall's Tau-b is as follows:</p><formula xml:id="formula_6">τB = P -Q (P + Q + T )(P + Q + U ) . (<label>2</label></formula><formula xml:id="formula_7">)</formula><p>where P is the number of concordant pairs, Q is the number of discordant pairs, T is the number of ties only in human judgments, and U is the number of ties only in the given metric. To better understand the calculation of P, Q, T, and U, we can refer to the following table: Metric s1 &lt; s2 s1 = s2 s1 &gt; s2 Human s1 &lt; s2</p><formula xml:id="formula_8">P U Q s1 = s2 T - T s1 &gt; s2 Q U P</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t∈T top5 ∩Tyes p(t), p(no) =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>t∈T top5 ∩Tno p(t), Implicit Score = max(p(yes), 1 -p(no)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Estimated Kendall's tau-b (Kend.) correlation of Pairwise Comparison using ChatGPT with different prompts on OpenMEVA. We use greedy decoding for Prompt V1∼V3. Whereas, for Prompt V4 we use Top-P sampling five times to obtain multiple results and vote for the final decision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distribution of different types of Individual Scores on OpenMEVA. The Implicit Score is rescaled into [0,100].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Distribution of Explicit Scores based on Chat-GPT with different prompts on OpenMEVA. For Prompt V4, the scores are normalized into [0, 100].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>also use a reference-free Sample-level Spearman (Spear.) correlation of different aspects on SummEval.</figDesc><table><row><cell>Metrics</cell><cell></cell><cell cols="2">Spear.</cell><cell></cell></row><row><cell></cell><cell cols="4">COH FLU CON REL</cell></row><row><cell>ROUGE-1</cell><cell>21.6</cell><cell>10.5</cell><cell>10.9</cell><cell>42.6</cell></row><row><cell>ROUGE-2</cell><cell>30.7</cell><cell>19.1</cell><cell>20.7</cell><cell>36.9</cell></row><row><cell>ROUGE-L</cell><cell>17.4</cell><cell>10.2</cell><cell>9.6</cell><cell>40.0</cell></row><row><cell>BERTScore</cell><cell>28.5</cell><cell>10.6</cell><cell>13.4</cell><cell>29.5</cell></row><row><cell>MoverScore</cell><cell>22.5</cell><cell>11.8</cell><cell>14.6</cell><cell>39.2</cell></row><row><cell>PRISM</cell><cell>23.7</cell><cell>17.5</cell><cell>35.2</cell><cell>16.9</cell></row><row><cell>BARTScore</cell><cell>33.4</cell><cell>20.9</cell><cell>34.8</cell><cell>24.8</cell></row><row><cell>+CNN</cell><cell>43.3</cell><cell>28.7</cell><cell>42.7</cell><cell>36.1</cell></row><row><cell>+CNN+Para</cell><cell>40.1</cell><cell>27.2</cell><cell>41.0</cell><cell>32.0</cell></row><row><cell>IMPLICIT SCORE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>text-davinci-001</cell><cell>-1.7</cell><cell>-5.6</cell><cell>19.7</cell><cell>8.4</cell></row><row><cell>text-davinci-003</cell><cell>57.4</cell><cell>32.9</cell><cell>35.2</cell><cell>28.1</cell></row><row><cell>EXPLICIT SCORE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ChatGPT (sampling)</cell><cell>45.8</cell><cell>22.1</cell><cell>41.2</cell><cell>39.2</cell></row><row><cell>ChatGPT (greedy)</cell><cell>52.2</cell><cell>19.3</cell><cell>43.3</cell><cell>46.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Extend), it is only outperformed by ParaScore and ParaScore.Free, which requires the use of reference or hyper-parameter adjustments on a dev set. Additionally, the performance of the Explicit Score further improves when we use greedy search instead of Top-P sampling for decoding.</figDesc><table><row><cell>is employed to</cell></row><row><cell>evaluate the correlation between two measured vari-</cell></row><row><cell>ables. A detailed explanation of Kendall's Tau-b is</cell></row><row><cell>shown in Appendix C.</cell></row><row><cell>4 Main Experiments</cell></row><row><cell>4.1 Individual Score</cell></row></table><note><p><p><p>Notably, as shown in Tables</p>1 to 4</p>, even without providing reference or calibration details for different score ranges, ChatGPT's Explicit Score has already correlated with human scores better than most commonly used automated metrics. On Twit-ter (</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Dataset-level Spearman (Spear.) correlation of different aspects on dialogue-level FED.</figDesc><table><row><cell>Metrics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Spear.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="10">COH CON DIV DEP LIK UND FLE INF INQ Overall</cell></row><row><cell>BERT-R</cell><cell>22.9</cell><cell>16.3</cell><cell cols="3">19.6 19.2 28.1</cell><cell>19.8</cell><cell cols="3">25.3 21.1 33.7</cell><cell>24.8</cell></row><row><cell>GPT-2</cell><cell>12.3</cell><cell>9.1</cell><cell>14.7</cell><cell>9.7</cell><cell>17.9</cell><cell>7.0</cell><cell cols="2">13.4 11.6</cell><cell>7.1</cell><cell>12.3</cell></row><row><cell>USR</cell><cell>19.4</cell><cell>16.9</cell><cell cols="3">24.2 34.1 22.1</cell><cell>17.2</cell><cell cols="3">20.9 28.8 18.8</cell><cell>28.8</cell></row><row><cell>S-DiCoh</cell><cell>3.8</cell><cell>1.7</cell><cell>5.9</cell><cell>4.6</cell><cell cols="2">-7.0 -10.0</cell><cell>4.4</cell><cell>2.8</cell><cell>-5.4</cell><cell>-7.3</cell></row><row><cell>FED</cell><cell>25.1</cell><cell>11.6</cell><cell cols="3">44.9 52.2 26.2</cell><cell>30.6</cell><cell cols="3">40.8 33.7 29.8</cell><cell>44.3</cell></row><row><cell>DynaEval</cell><cell>42.3</cell><cell>35.2</cell><cell cols="3">33.2 43.9 39.8</cell><cell>36.1</cell><cell cols="3">38.9 39.6 38.8</cell><cell>48.2</cell></row><row><cell>SelfEval</cell><cell>43.6</cell><cell>34.7</cell><cell cols="3">26.3 32.7 39.0</cell><cell>40.6</cell><cell cols="3">31.7 31.8 42.1</cell><cell>43.5</cell></row><row><cell>IMPLICIT SCORE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>text-davinci-001</cell><cell>37.9</cell><cell>33.0</cell><cell cols="3">36.1 26.2 35.0</cell><cell>57.5</cell><cell cols="3">39.5 54.8 45.0</cell><cell>39.4</cell></row><row><cell>text-davinci-003</cell><cell>46.8</cell><cell>43.8</cell><cell cols="3">24.9 53.4 57.3</cell><cell>57.6</cell><cell cols="3">45.0 55.1 59.0</cell><cell>58.0</cell></row><row><cell>EXPLICIT SCORE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ChatGPT (sampling)</cell><cell>57.8</cell><cell>47.8</cell><cell cols="3">44.5 51.5 47.2</cell><cell>61.7</cell><cell cols="3">49.4 61.7 42.8</cell><cell>55.8</cell></row><row><cell>ChatGPT (greedy)</cell><cell>62.4</cell><cell>47.5</cell><cell cols="3">48.3 55.5 55.4</cell><cell>60.0</cell><cell cols="3">54.8 62.0 42.3</cell><cell>54.2</cell></row><row><cell>Metrics</cell><cell cols="2">Spear. Pear.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ROUGE-1</cell><cell>1.4</cell><cell>2.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ROUGE-2</cell><cell>3.5</cell><cell>4.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ROUGE-L</cell><cell>1.3</cell><cell>2.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERTScore</cell><cell>14.0</cell><cell>12.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Perplexity</cell><cell>32.4</cell><cell>33.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BARTScore</cell><cell>-6.5</cell><cell>-8.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+CNN</cell><cell>4.9</cell><cell>2.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+CNN+Para</cell><cell>6.4</cell><cell>5.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IMPLICIT SCORE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>text-davinci-001</cell><cell>30.3</cell><cell>32.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>text-davinci-003</cell><cell>37.9</cell><cell>43.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EXPLICIT SCORE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ChatGPT (sampling)</cell><cell>47.6</cell><cell>49.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ChatGPT (greedy)</cell><cell>49.9</cell><cell>51.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Table 3: Sample-level Spearman (Spear.) and Pearson</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(Pear.) correlation on OpenMEVA.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Metrics</cell><cell cols="2">Spear. Pear.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>iBLEU</cell><cell>3.2</cell><cell>1.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERTScore</cell><cell>43.2</cell><cell>42.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERTScore.Free</cell><cell>41.9</cell><cell>31.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BARTScore+CNN+Para</cell><cell>27.6</cell><cell>28.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT-iBLEU</cell><cell>41.6</cell><cell>32.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ParaScore</cell><cell>53.0</cell><cell>52.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ParaScore.Free</cell><cell>49.5</cell><cell>49.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IMPLICIT SCORE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>text-davinci-001</cell><cell>15.8</cell><cell>15.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>text-davinci-003</cell><cell>44.4</cell><cell>40.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EXPLICIT SCORE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ChatGPT (sampling)</cell><cell>45.1</cell><cell>44.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ChatGPT (greedy)</cell><cell>46.5</cell><cell>45.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Metrics</cell><cell></cell><cell cols="2">Kend.</cell><cell></cell></row><row><cell></cell><cell cols="4">COH FLU CON REL</cell></row><row><cell>IMPLICIT SCORE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>text-davinci-001</cell><cell>-3.2</cell><cell>-4.3</cell><cell>9.3</cell><cell>12.9</cell></row><row><cell>text-davinci-003</cell><cell>46.9</cell><cell>24.5</cell><cell>35.3</cell><cell>29.1</cell></row><row><cell>EXPLICIT SCORE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ChatGPT (sampling)</cell><cell>50.3</cell><cell>8.6</cell><cell>31.7</cell><cell>44.3</cell></row><row><cell>ChatGPT (greedy)</cell><cell>43.7</cell><cell>16.8</cell><cell>32.8</cell><cell>52.5</cell></row><row><cell>COMPARISON</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ChatGPT (sampling)</cell><cell>22.6</cell><cell>7.8</cell><cell>24.2</cell><cell>30.5</cell></row><row><cell>ChatGPT (greedy)</cell><cell>34.5</cell><cell>17.4</cell><cell>22.0</cell><cell>34.0</cell></row><row><cell cols="5">Table 5: Estimated Kendall's tau-b (Kend.) correlation</cell></row><row><cell cols="3">of different aspects on SummEval.</cell><cell></cell><cell></cell></row></table><note><p><p>Dataset-level Spearman (Spear.)  </p>and Pearson (Pear.) correlation on Twitter (Extend).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>CON DIV DEP LIK UND FLE INF INQ Overall Estimated Kendall's tau-b (Kend.) correlation of different aspects on dialogue-level FED.</figDesc><table><row><cell cols="2">IMPLICIT SCORE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">text-davinci-001</cell><cell>33.3</cell><cell>32.0</cell><cell>29.6 25.1 25.6</cell><cell>49.9</cell><cell>32.8 44.8 49.5</cell><cell>33.6</cell></row><row><cell cols="2">text-davinci-003</cell><cell>28.8</cell><cell>30.5</cell><cell>18.8 36.9 41.9</cell><cell>43.2</cell><cell>34.0 45.8 43.0</cell><cell>36.7</cell></row><row><cell cols="2">EXPLICIT SCORE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ChatGPT (sampling)</cell><cell>48.4</cell><cell>44.1</cell><cell>32.4 47.5 46.7</cell><cell>48.0</cell><cell>36.2 45.6 45.9</cell><cell>44.2</cell></row><row><cell cols="2">ChatGPT (greedy)</cell><cell>50.2</cell><cell>39.6</cell><cell>45.5 53.5 50.8</cell><cell>53.7</cell><cell>50.5 47.7 38.1</cell><cell>41.7</cell></row><row><cell>COMPARISON</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ChatGPT (sampling)</cell><cell>28.3</cell><cell>16.1</cell><cell>28.5 31.5 43.0</cell><cell>27.5</cell><cell>55.5 35.2 24.5</cell><cell>38.6</cell></row><row><cell cols="2">ChatGPT (greedy)</cell><cell>24.3</cell><cell>13.7</cell><cell>28.5 33.8 41.9</cell><cell>27.5</cell><cell>55.5 34.1 25.6</cell><cell>37.5</cell></row><row><cell>Metrics</cell><cell></cell><cell cols="2">Kend.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Hard Medium Easy</cell><cell>All</cell><cell></cell><cell></cell></row><row><cell>IMPLICIT SCORE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>text-davinci-001</cell><cell>6.3</cell><cell>29.8</cell><cell cols="2">44.4 16.6</cell><cell></cell><cell></cell></row><row><cell>text-davinci-003</cell><cell>27.9</cell><cell>36.8</cell><cell cols="2">66.7 33.2</cell><cell></cell><cell></cell></row><row><cell>EXPLICIT SCORE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ChatGPT (sampling) 18.5</cell><cell>47.3</cell><cell cols="2">74.3 31.2</cell><cell></cell><cell></cell></row><row><cell>ChatGPT (greedy)</cell><cell>16.8</cell><cell>62.6</cell><cell cols="2">82.5 36.2</cell><cell></cell><cell></cell></row><row><cell>COMPARISON</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ChatGPT (sampling)</cell><cell>8.1</cell><cell>22.8</cell><cell cols="2">33.3 14.5</cell><cell></cell><cell></cell></row><row><cell>ChatGPT (greedy)</cell><cell>9.9</cell><cell>29.8</cell><cell cols="2">55.6 19.7</cell><cell></cell><cell></cell></row><row><cell cols="5">Table 7: Estimated Kendall's tau-b (Kend.) correlation</cell><cell></cell><cell></cell></row><row><cell>on OpenMEVA.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Metrics</cell><cell></cell><cell cols="2">Kend.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Hard Medium Easy</cell><cell>All</cell><cell></cell><cell></cell></row><row><cell>IMPLICIT SCORE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>text-davinci-001</cell><cell>21.6</cell><cell>34.6</cell><cell cols="2">13.6 20.4</cell><cell></cell><cell></cell></row><row><cell>text-davinci-003</cell><cell>25.5</cell><cell>19.2</cell><cell cols="2">59.1 28.6</cell><cell></cell><cell></cell></row><row><cell>EXPLICIT SCORE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ChatGPT (sampling) 27.8</cell><cell>40.0</cell><cell cols="2">53.8 34.9</cell><cell></cell><cell></cell></row><row><cell>ChatGPT (greedy)</cell><cell>15.3</cell><cell>38.5</cell><cell cols="2">57.0 31.2</cell><cell></cell><cell></cell></row><row><cell>COMPARISON</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ChatGPT (sampling) 14.6</cell><cell>31.0</cell><cell cols="2">68.3 31.3</cell><cell></cell><cell></cell></row><row><cell>ChatGPT (greedy)</cell><cell>10.0</cell><cell>22.2</cell><cell cols="2">65.1 26.3</cell><cell></cell><cell></cell></row><row><cell cols="5">Table 8: Estimated Kendall's tau-b (Kend.) correlation</cell><cell></cell><cell></cell></row><row><cell>on Twitter (Extend).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://openai.com/blog/chatgpt</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research was supported in part by the <rs type="funder">National Natural Science Foundation of China</rs>(<rs type="grantNumber">62006062</rs>, <rs type="grantNumber">62176076</rs>), the <rs type="funder">Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies</rs>(<rs type="grantNumber">2022B121201000 5</rs>), <rs type="funder">Natural Science Foundation of Guangdong</rs>(<rs type="grantNumber">2023A1515012922</rs>), and <rs type="funder">Key Technologies Research and Development Program of Shenzhen</rs> <rs type="grantNumber">JSGG20210802154400001</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jFTedAs">
					<idno type="grant-number">62006062</idno>
				</org>
				<org type="funding" xml:id="_bfaQTH2">
					<idno type="grant-number">62176076</idno>
				</org>
				<org type="funding" xml:id="_BSDMjxp">
					<idno type="grant-number">2022B121201000 5</idno>
				</org>
				<org type="funding" xml:id="_neCZ9JQ">
					<idno type="grant-number">2023A1515012922</idno>
				</org>
				<org type="funding" xml:id="_qzUhFVr">
					<idno type="grant-number">JSGG20210802154400001</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Different Prompts for Explicit Score on Story Generation</head><p>======= PROMPT FOR EXPLICIT SCORE V1 ======= Score the following storyline given the beginning of the story on a continual scale from 0 (worst) to 100 <ref type="bibr">(best)</ref>, where a score of 0 means "The storyline makes no sense and is totally not understandable" and a score of 100 means "The storyline is perfect-written and highly consistent with the given beginning of the story".</p><p>The beginning of the story:</p><p>On a scale of 0 to 100, evaluate the storyline based on the given beginning. A score of 0 indicates that the storyline is incomprehensible, while a score of 100 means that the storyline is flawlessly written and logically follows from the beginning of the story.</p><p>The beginning of the story:</p><p>Score the overall quality of the following storyline given the beginning of the story on a continual scale from 0 (worst) to 100 (best). Consider whether the storyline well-written and consistent with the given beginning of the story.</p><p>The beginning of the story:</p><p>Score:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Different Prompts for Pairwise Comparison on Story Generation</head><p>======= PROMPT FOR EXPLICIT SCORE V4 ======= Score the following storyline given the beginning of the story with one to five stars. Where</p><p>• one star means "Nonsense",</p><p>• two stars mean "The storyline has some connections with the beginning, but is not understandable",</p><p>• three stars mean "The storyline has some connections with the beginning and is understandable",</p><p>• four stars mean "The storyline is consistent with the beginning and possibly involves a few grammar mistakes", • and five stars mean "Perfect storyline and grammar".</p><p>The beginning of the story:</p><p>Stars (1-5):</p><p>======= PROMPT FOR EXPLICIT SCORE V5 ======= Score the following storyline given the beginning of the story on a continual scale from 0 (worst) to 100 (best), where a score of 0 means "The storyline makes no sense and is totally not understandable" and a score of 100 means "The storyline is perfect-written and highly consistent with the given beginning of the story". Please first give your reason carefully (indicated by "Reason:") and then decide your final score (indicated by "Score: 1-100").</p><p>The beginning of the story:</p><p>Consider the following two storylines written according to the given beginning of the story:</p><p>The beginning of the story:</p><p>Question: Which storyline is better-written and more consistent with the beginning of the story? Please answer with one of the following options.</p><p>Options: (A) Storyline-1 (B) Storyline-2 (C) Both storylines are equally well-written and consistent with the beginning of the story.</p><p>Answer: I will choose Option ===== PROMPT FOR PAIRWISE COMPARISON V2 ===== Question: Which storyline is better-written and more consistent with the beginning of the story? Please answer with one of the following options.</p><p>The beginning of the story: Storyline-1:</p><p>Question: Which storyline is better-written and more consistent with the beginning of the story? Please first give your reason carefully (indicated by "Reason:") and then choose one of the following options (indicated by "Answer: A/B/C").</p><p>Options: (A) Storyline-1 (B) Storyline-2 (C) Both storylines are equally well-written (poor-written) and consistent (inconsistent) with the beginning of the story.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Nuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.06869</idno>
		<title level="m">What would harry say? building dialogue agents for characters in a story</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mcpg: A flexible multi-level controllable framework for unsupervised paraphrase generation</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5948" to="5958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Palm: Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11416</idno>
		<title level="m">Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">GLM: General language model pretraining with autoregressive blank infilling</title>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.26</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="320" to="335" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">2021a. SummEval: Re-evaluating summarization evaluation</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">R</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Kryściński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mc-Cann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00373</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="391" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">2021b. Summeval: Re-evaluating summarization evaluation</title>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Alexander R Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Kryściński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Mc-Cann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="391" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">See-Kiong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Gptscore: Evaluate as you desire</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08821</idno>
		<title level="m">Simcse: Simple contrastive learning of sentence embeddings</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Better automatic evaluation of open-domain dialogue systems with contextualized embeddings</title>
		<author>
			<persName><forename type="first">Johnny</forename><surname>Sarik Ghazarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-2310</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</title>
		<meeting>the Workshop on Methods for Optimizing and Evaluating Neural Language Generation<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="82" to="89" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">OpenMEVA: A benchmark for evaluating open-ended story generation metrics</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhexin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoer</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changjie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.500</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6394" to="6407" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Exploring chatgpt&apos;s ability to rank content: A preliminary study on consistency with human preferences</title>
		<author>
			<persName><forename type="first">Yunjie</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiping</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiyan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baochang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangang</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The treatment of ties in ranking problems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="239" to="251" />
			<date type="published" when="1945">1945</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Large language models are state-of-the-art evaluators of translation quality</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kocmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SelF-eval: Self-supervised fine-grained dialogue evaluation</title>
		<author>
			<persName><forename type="first">Longxuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics</title>
		<meeting>the 29th International Conference on Computational Linguistics<address><addrLine>Gyeongju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="485" to="495" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised evaluation of interactive dialog with DialoGPT</title>
		<author>
			<persName><forename type="first">Shikib</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>st virtual meeting</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">USR: An unsupervised and reference free evaluation metric for dialog generation</title>
		<author>
			<persName><forename type="first">Shikib</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.64</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="681" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dialogue coherence assessment without explicit dialogue act labels</title>
		<author>
			<persName><forename type="first">Mohsen</forename><surname>Mesgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Bücker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.133</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1439" to="1450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A guide to appropriate use of correlation coefficient in medical research</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mavuto</surname></persName>
		</author>
		<author>
			<persName><surname>Mukaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Malawi medical journal</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="69" to="71" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Nitish Shirish Keskar, Huan Wang, and Caiming Xiong. 2021. Unsupervised paraphrasing with pretrained language models</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.417</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic</publisher>
			<biblScope unit="page" from="5136" to="5150" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">2023a. Sen2pro: A probabilistic perspective to sentence embedding from pre-trained language model</title>
		<author>
			<persName><forename type="first">Lingfeng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.02247</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">2023b. A simple and plug-and-play method for unsupervised sentence representation enhancement</title>
		<author>
			<persName><forename type="first">Lingfeng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.07824</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the evaluation metrics for paraphrase generation</title>
		<author>
			<persName><forename type="first">Lingfeng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3178" to="3190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Is chatgpt a good keyphrase generator? a preliminary study</title>
		<author>
			<persName><forename type="first">Mingyang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songfang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huafeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liping</forename><surname>Jing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.13001</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint learning of a dual SMT system for paraphrase generation</title>
		<author>
			<persName><forename type="first">Hong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island</addrLine></address></meeting>
		<imprint>
			<publisher>Korea. Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="38" to="42" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic machine translation evaluation in many languages via zero-shot paraphrasing</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.8</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="90" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jiaan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunlong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxiang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>a preliminary study</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">2023b. Retrieval-free knowledge injection through multi-document traversal for dialogue models</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhu</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongru</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SemEval-2015 task 1: Paraphrase and semantic similarity in Twitter (PIT)</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S15-2001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation (Se-mEval 2015)</title>
		<meeting>the 9th International Workshop on Semantic Evaluation (Se-mEval 2015)<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Extracting lexically divergent paraphrases from Twitter</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00194</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="435" to="448" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bartscore: Evaluating generated text as text generation</title>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="27263" to="27277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Jerrold</forename><forename type="middle">H</forename><surname>Zar</surname></persName>
		</author>
		<title level="m">Spearman rank correlation. Encyclopedia of biostatistics</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">DynaEval: Unifying turn and dialogue level evaluation</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D'</forename><surname>Haro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Friedrichs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grandee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.441</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5676" to="5689" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<title level="m">Opt: Open pre-trained transformer language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Peyrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><forename type="middle">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Eger</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1053</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="563" to="578" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
