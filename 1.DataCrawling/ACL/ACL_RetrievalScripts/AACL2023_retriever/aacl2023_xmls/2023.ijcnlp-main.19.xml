<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating and Answering Simple and Complex Questions from Text and from Knowledge Graphs</title>
				<funder ref="#_gcVhgFz">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kelvin</forename><surname>Han</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Claire</forename><surname>Gardent</surname></persName>
							<email>claire.gardent@loria.fr</email>
						</author>
						<author>
							<persName><roleName>CA</roleName><forename type="first">San</forename><surname>Francisco</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Usa</forename><forename type="middle">Morgan</forename><surname>Kaufmann</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kaustubh</forename><forename type="middle">D</forename><surname>Dhole</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Varun</forename><surname>Gangal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aadesh</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<persName><roleName>Saad</roleName><forename type="first">Zhenhao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Abinaya</forename><surname>Mahendiran</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>Mille</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ashish</forename><surname>Srivastava</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Samson</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tongshuang</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jinho</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">On- Drej</forename><surname>Dusek</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sajant</forename><surname>Anand</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Na- Gender</forename><surname>Aneja</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rabin</forename><surname>Banjade</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lisa</forename><surname>Barthe</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hanna</forename><surname>Behnke</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ian</forename><surname>Berlot-Attwell</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Connor</forename><surname>Boyle</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Car- Oline</forename><surname>Brun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Antonio</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sobrevilla</forename><surname>Cabezudo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Samuel</forename><surname>Cahyawijaya</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Emile</forename><surname>Chapuis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mukund</forename><surname>Choudhary</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christian</forename><surname>Clauss</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Colombo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Filip</forename><surname>Cornell</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gautier</forename><surname>Dagan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mayukh</forename><surname>Das</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tanay</forename><surname>Dixit</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Dopierre</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Paul-Alexis</forename><surname>Dray</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Su- Chitra</forename><surname>Dubey</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tatiana</forename><surname>Ekeinhor</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><forename type="middle">Di</forename><surname>Giovanni</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rishabh</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Louanes</forename><surname>Hamla</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sang</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fabrice</forename><surname>Harel-Canada</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Antoine</forename><surname>Honore</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ishan</forename><surname>Jindal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Przemyslaw</forename><forename type="middle">K</forename><surname>Joniak</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Denis</forename><surname>Kleyko</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Venelin</forename><surname>Kovatchev</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kalpesh</forename><surname>Krishna</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ashutosh</forename><surname>Ku- Mar</surname></persName>
						</author>
						<author>
							<persName><roleName>Seungjae</roleName><forename type="first">Stefan</forename><surname>Langer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ryan</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Corey</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hualou</forename><surname>Levinson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kaizhao</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhexiong</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andrey</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vukosi</forename><surname>Lukyanenko</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gerard</forename><surname>Marivate</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>De Melo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Maxime</forename><surname>Meoni</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Afnan</forename><surname>Meyer</surname></persName>
						</author>
						<author>
							<persName><roleName>Nafise</roleName><forename type="first">Sadat</forename><surname>Mir</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Niklas</forename><surname>Moosavi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Timo- Thy</forename><surname>Muennighoff</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hon</forename><surname>Sum</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kenton</forename><surname>Mun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marcin</forename><surname>Murray</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Maria</forename><surname>Namysl</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Priti</forename><surname>Obedkova</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nivranshu</forename><surname>Oli</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><surname>Pasricha</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Richard</forename><surname>Pfister</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vinay</forename><surname>Plant</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vasile</forename><surname>Prabhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Libo</forename><surname>Pais</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shahab</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName><roleName>Pawan</roleName><forename type="first">Kumar</forename><surname>Raji</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vikas</forename><surname>Rajpoot</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Roy</forename><surname>Raunak</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Rinberg</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Juan</forename><forename type="middle">Diego</forename><surname>Roberts</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Claude</forename><surname>Rodriguez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vasconcellos</forename><forename type="middle">P H S</forename><surname>Roux</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ananya</forename><forename type="middle">B</forename><surname>Sai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Robin</forename><forename type="middle">M</forename><surname>Schmidt</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Scialom</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tshephisho</forename><surname>Sefara</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Saqib</forename><forename type="middle">N</forename><surname>Shamsi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xudong</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Haoyue</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yiwen</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anna</forename><surname>Shvets</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nick</forename><surname>Siegel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Damien</forename><surname>Sileo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jamie</forename><surname>Simon</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chandan</forename><surname>Singh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ro- Man</forename><surname>Sitelew</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Priyank</forename><surname>Soni</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Taylor</forename><surname>Sorensen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">William</forename><surname>Soto</surname></persName>
						</author>
						<author>
							<persName><roleName>KV</roleName><forename type="first">Aman</forename><surname>Srivastava</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aditya</forename><surname>Srivatsa</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tony</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mukund</forename><surname>Varma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">A</forename><surname>Tabassum</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fiona</forename><forename type="middle">Anting</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ryan</forename><surname>Teehan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mo</forename><surname>Tiwari</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marie</forename><surname>Tolkiehn</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CNRS/LORIA</orgName>
								<orgName type="institution" key="instit2">Université de Lorraine</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Pub-lishers Inc</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Athena Wang</orgName>
								<address>
									<addrLine>Zijian Wang, Gloria Wang, Zijie J. Wang, Fuxuan Wei, Bryan Wilie, Genta Indra Winata, Xinyi Wu, Witold Wydmański, M. Yee, Jing Zhang, and Yue Zhang</addrLine>
									<postCode>2021</postCode>
									<settlement>Tianbao Xie, Usama Yaseen</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generating and Answering Simple and Complex Questions from Text and from Knowledge Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C133C53CAB4D317C871ABB3857B2E0F3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While both text and Knowledge Graphs (KG) may be used to answer a question, most current Question Answering and Generation models only work on a single modality. In this paper, we introduce a multi-task model such that questions can be generated and answered from both KG and text. The model has wide coverage and handles both simple (one KG fact) and complex (more than one KG fact) questions. Extensive internal, cross-modal and external consistency checks, and analysis of the quality of the generated questions, show that our approach outperforms previous work. Our data and modeling also leads to improvements in downstream tasks, including better performance with finetuning Open-Domain QA architectures and better correlation with human judgments than the Data-QuestEval metric which was previously proposed for evaluating the semantic adequacy of KG-to-Text generations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Previous work on question generation (QG) and question-answering (QA) mainly focused on a single modality such as Natural Language (NL) <ref type="bibr" target="#b21">(Lyu et al., 2021)</ref>, Knowledge Graphs (KG) <ref type="bibr" target="#b9">(Hu et al., 2019)</ref> or images <ref type="bibr" target="#b34">(Shah et al., 2019)</ref>. Although QG and QA should be able to operate consistently across semantically equivalent sources regardless of their modality, previous work is hampered by the lack of large-scale aligned cross-modal QA-QG data that also ensures wide QG coverage. As argued in <ref type="bibr" target="#b29">(Rebuffel et al., 2021)</ref>, such cross-modal QG-QA models can also be used to assess semantic consistency between KG and Text in KG-to-Text generation (i.e. Do questions on the generated text yield the same answer on the input graph and viceversa?). Finally, cross-modal KG/NL models are key for interacting with KGs in natural language.</p><p>Building on datasets pairing KG graphs with text, we develop a multitask, KG/NL model (QTT) that, given a text in English language or a subgraph from the Wikidata KG, can generate and answer simple and complex questions across the two modalities.</p><p>We evaluate the model in terms of QG coverage, internal, cross-modal and external QA consistency. We also examine the quality of the generated questions using human evaluation. The results show that our approach outperforms previous work across the board. We further demonstrate that our approach also brings improvements to two downstream tasks namely, better performance with fine-tuning Open-Domain QA architectures and better correlations with human judgments when used for the Data-QuestEval metric <ref type="bibr" target="#b29">(Rebuffel et al., 2021)</ref>. Our code, data and pretrained models are available at https://gitlab.inria. fr/hankelvin/quartet_qgqa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>QG and QA from KG. Early rules-or templatebased KGQG approaches <ref type="bibr" target="#b23">(Olney et al., 2012;</ref><ref type="bibr" target="#b33">Seyler et al., 2015;</ref><ref type="bibr" target="#b35">Song and Zhao, 2016)</ref> required significant human effort and faced issues with generalisation, making them difficult to deploy at scale. While neural KGQG models such as <ref type="bibr" target="#b30">(Reddy et al., 2017;</ref><ref type="bibr" target="#b32">Serban et al., 2016;</ref><ref type="bibr">Elsahar et al., 2018;</ref><ref type="bibr" target="#b19">Liu et al., 2019;</ref><ref type="bibr" target="#b8">Han et al., 2022)</ref> address some of the limitations, they -and KGQA ones such as <ref type="bibr">(Bordes et al., 2015;</ref><ref type="bibr" target="#b41">Wu et al., 2019;</ref><ref type="bibr" target="#b10">Huang et al., 2019)</ref> -mainly focused on the generation and answering of simple questions i.e., questions which verbalise a single KG fact. More recently, some researchers have started to address complex KGQG and KGQA i.e. questions on more than one KG fact <ref type="bibr" target="#b14">(Kumar et al., 2019;</ref><ref type="bibr" target="#b43">Zhang et al., 2022;</ref><ref type="bibr" target="#b31">Saha et al., 2018;</ref><ref type="bibr">Christmann et al., 2019;</ref><ref type="bibr" target="#b16">Lecorvé et al., 2022;</ref><ref type="bibr" target="#b24">Perez-Beltrachini et al., 2023)</ref>. However most of these efforts are focused on generating and answering questions from Knowledge Graphs only.</p><p>QG and QA from NL. A large body of work utilises the SQuAD dataset <ref type="bibr" target="#b28">(Rajpurkar et al., 2016)</ref> to develop neural models for text-based joint QA and QG <ref type="bibr" target="#b39">(Wang et al., 2017;</ref><ref type="bibr">Duan et al., 2017;</ref><ref type="bibr" target="#b21">Lyu et al., 2021;</ref><ref type="bibr" target="#b20">Luo et al., 2022)</ref>. Some work uses retrieval and conditions a pretrained language model on both the query and the retrieved documents to generate the answer <ref type="bibr" target="#b17">(Lewis et al., 2020;</ref><ref type="bibr" target="#b7">Guu et al., 2020;</ref><ref type="bibr" target="#b13">Khattab et al., 2021)</ref>. Other work has investigated the use of synthetically-generated data with round trip filtering techniques and shown improved QA performance <ref type="bibr" target="#b1">(Alberti et al., 2019;</ref><ref type="bibr" target="#b26">Puri et al., 2020;</ref><ref type="bibr" target="#b15">Kwiatkowski et al., 2019)</ref>. Similarly, we used data augmentation and round trip filtering to improve generalisation; however, we do this for QG and QA for both text and KG.</p><p>Cross-Modal text/graph QG and QA. An early direction <ref type="bibr" target="#b3">(Fader et al., 2014;</ref><ref type="bibr">Das et al., 2017)</ref> for leveraging both structured (KG, tables, lists etc) and unstructured (text) information used information extraction methods such as OpenIE <ref type="bibr" target="#b2">(Banko et al., 2007)</ref> and UniversalSchema <ref type="bibr" target="#b42">(Yao et al., 2012)</ref> to fill the coverage gaps of KGs so as to employ semantic parsing-or rules-based KGQA methods.</p><p>More recent work instead casts structured information as text to access their knowledge through TextQA methods. <ref type="bibr" target="#b0">(Agarwal et al., 2021)</ref> verbalise a large KG Wikidata <ref type="bibr" target="#b38">(Vrandečić and Krötzsch, 2014)</ref> to add to a retrieval LM corpus, obtaining performance improvements on benchmark QA datasets. <ref type="bibr" target="#b22">(Oguz et al., 2022)</ref> obtain improvements by adding Wikipedia tables and lists to the data mix.</p><p>Similar to our work, <ref type="bibr" target="#b29">(Rebuffel et al., 2021)</ref> created synthetic multimodal-QA/QG datasets, by using a QG model trained on SQuAD to generate questions on texts paired with graphs in existing datasets. They use this and SQuAD to train multimodal models and show that the models can be used to evaluate KG-to-Text generation models; they report better correlations between their measure, the Data-QuestEval metric, with human judgments of semantic adequacy than existing automatic metrics. They however do not provide a systematic evaluation of their models; in contrast, we provide a detailed evaluation of our model and compare it against theirs. We also examine how using our model instead impacts the Data-QuestEval metric's correlations with human judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our approach comprise the creation of graph-text aligned QG-QA datasets covering simple and complex questions (Section 4); and multimodal, multi-task training of a generative QG-QA model (Section 5) -we call this model QTT for the number (4, or QuarTeT) of its main fine-tuning tasks. Terminology and Notation. We use the term graphs (denoted by g) to refer to subgraphs of the Wikidata KG <ref type="bibr" target="#b38">(Vrandečić and Krötzsch, 2014)</ref> and texts (t) to refer to English texts. A KG graph is a set of triples (also called facts) of the form xsubject, predicate, objecty. We write X to denote the (text or graph) context of a question and X 1 to denote its semantically equivalent counterpart in the other modality; g 1 is a subgraph of g that corresponds to a question q and its answer; nf is the number of facts related to a given q (i.e. the size of its corresponding subgraph |g 1 | ); Ý Ñ q is a list of NL questions; and a X is an answer in X whereby a graph answer a g is either a subject or an object entity in g whereas a text answer a t is a span in t. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data</head><p>To train our QTT model, we derive a dataset of pg, a g , t, a t , g 1 , nf, qq tuples from two existing KG/NL datasets, KELM and WEBNLG.</p><p>‚ KELM <ref type="bibr" target="#b0">(Agarwal et al., 2021)</ref> is a dataset of 15M (Wikidata graph, English text) pairs where texts were generated from Wikidata graphs using a T5 <ref type="bibr" target="#b27">(Raffel et al., 2020)</ref> pre-trained model finetuned on TekGen, a large dataset of (g, t) pairs created using distant supervision. We use a subset of KELM filtered for (g, t) pairs where g has between 2 and 5 triples (as larger sizes lead to unnatural questions).<ref type="foot" target="#foot_0">1</ref> </p><p>‚ WEBNLG <ref type="bibr">(Gardent et al., 2017a</ref>) is comprised of 38,872 (g, t) pairs where the graphs are from Wikidata<ref type="foot" target="#foot_1">2</ref> and the texts were crowdsourced to match the graphs.</p><p>We derive our training data from KELM and WEBNLG in three steps. First, we create pg, a g , t, a t , g 1 , nf, qq tuples by applying textbased QG and QA on the texts and heuristically aligning text answers with the corresponding graph answers -we call the resulting datasets Q-KELM and Q-WEBNLG 0 . Second, we use Q-KELM to train two general multimodal QG models. Thirdly, we apply the models to WEBNLG and add to Q-WEBNLG 0 , thereby extending the coverage of the data for training QTT. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the process, and we describe the three steps below.</p><p>‚ Associating Graphs and Texts with Questions and Answers (Step 1). Given pg, tq, an instance of any aligned KG-to-Text dataset, we create synthetic Multimodal QG-QA Data by: (i) generating a question q from t using text-based QG; <ref type="bibr">(ii)</ref> extracting the text answer a t using QA to obtain pt, a t , qq;<ref type="foot" target="#foot_2">3</ref> and heuristically aligning a t with the corresponding graph answer a g . To improve quality, we filter out any questions that QA found unanswerable, or whose text answer cannot be aligned with a graph entity. We also heuristically align each generated question with the matching subgraph g 1 Ď g and label it with its size nf i.e., the number of facts each question denotes. Appendix C.1 contains the implementation details for these steps.</p><p>The size information permits distinguishing between simple (SQ) and complex (CQ) questions which allows us to take a differentiated approach in Step 2 and generate more QA-QG data.</p><p>Our filtering above comes however at the cost of question coverage -when our full data generation procedure is applied to WEBNLG, only 2,044 CQs remain (Table <ref type="table" target="#tab_3">2</ref>). Nonetheless, we keep these (as stated earlier, we call this set Q-WEBNLG 0 ) to have as wide coverage as possible. <ref type="foot" target="#foot_3">4</ref> Applying the procedure on KELM, we get much larger sets of SQs and CQs, which we call SQ-KELM and CQ-KELM, and which together form Q-KELM. Some examples of Q-KELM instances are in Table <ref type="table">9</ref> in Appendix A. Table <ref type="table" target="#tab_3">2</ref> shows the sizes of the resulting datasets.   <ref type="figure">Appendix C</ref>.1) at the start of the input sequence to train the CQ-GEN model allows us to controllably generate a complex question from input of the form pX, a X , a type , nf q. Here, a type is the semantic type of the answer entity detected by ELQ <ref type="bibr" target="#b18">(Li et al., 2020)</ref> or Duckling; it is retrieved from the 2021-12-29 Wikidata RDF dump (for entities) or the prediction from Duckling (for values). a type is added to improve QG. SQ-GEN is similar to CQ-GEN except that the question type (q type ) is used in the input to increase the number and variety of the generated questions.</p><formula xml:id="formula_0"># Facts Q-KELM Q-WEBNLG 0 QTT-DATA<label>1</label></formula><p>‚ Extending Q-WebNLG 0 (Step 3) By applying the controllable QG models from Step 2 to WEBNLG, we can extend Q-WEBNLG 0 from</p><p>Step 1. This gives us the final training data for QTT and we call it QTT-DATA.</p><p>Our QG models (CQ-GEN, SQ-GEN) generate a question given a context and an answer. Hence, a set of answers must first be selected from the context. For a given X in WEBNLG, we use the same answer selection method as <ref type="bibr" target="#b29">(Rebuffel et al., 2021)</ref>. For g, the set of possible graph answers is comprised of the subjects and objects in g. For t, it is the set of named entities (NEs) and noun phrases (NPs) detected in t using the spacy package.</p><p>For SQs from graphs, we follow <ref type="bibr" target="#b8">(Han et al., 2022)</ref>'s work on SQ generation from RDF triples and use their q type prediction model, which returns the set of plausible q type for an answer given its position in the triple and its semantic type.</p><p>Finally, we add an answerability+consistency filter on the generated questions by posing them to two QA models 5 and keep only questions where both QA models return an answer which (i) has a confidence score ě 0.7, and (ii) shares at least a token overlap with the other model's answer and with the answer used to condition QG.</p><p>In sum, by iterating over possible answers, nf from 1 to 4, and q type for SQ-GEN, our controllable approach to QG drastically increases the num-5 The deepset QA above and one based on DeBERTaV3.  ber of generated questions. A breakdown of QTT-DATA's composition is in Table <ref type="table" target="#tab_3">2</ref>.</p><p>5 QTT, a multimodal QG-QA Model</p><p>Our model (QTT) is trained in a multi-task manner to handle both QA and QG. It is based on the T5small checkpoint (60.5M parameters), allowing for direct comparison with <ref type="bibr" target="#b29">(Rebuffel et al., 2021)</ref>. We fine-tune on QTT-DATA using four main and four auxiliary tasks, all of which are cast in a sequenceto-sequence manner. Using a single Nvidia A40 GPU, it takes approximately 20 hours to fine-tune QTT. The four main and four auxiliary tasks are:</p><p>‚ QG from text/graph Given (X, a X , nf ), generate a set of questions Ý Ñ q . We obtain this set by first gathering together questions in QTT-DATA that were generated from a given context X, and which share the same size nf and answer, and then adding to these the questions generated from other "smaller" pieces of contexts (whose information is fully contained in X), and also sharing the same attributes (nf and answer). This gathering process is detailed in Appendix D.1.1.</p><p>‚ QA from text/graph Given (q, X), generate an answer âX . We leverage sets of pX, a X , qq from QTT-DATA for training these tasks. Additionally, to maximise the use of the data for training QA, we also associate questions answerable by a text t to larger pieces of text that semantically contain t (details in Appendix D.1.2). To allow QTT to abstain from an answer if the question cannot be answered from the context, we use two strategies (details in Appendix D.1.4) to generate negative unanswerable (q, ␣X) pairs.</p><p>‚ KG-to-Text/Text-to-KG These auxiliary tasks consist in either verbalising a graph or deriving a graph from a text. We instantiate each of the WEBNLG graph-text pairs as training instances.</p><p>‚ Entity typing on graph/text These auxiliary tasks combine entity detection and typing. Given a context X, the task identifies the entities/values mentioned in X, and their semantic types. We use BLINK <ref type="bibr" target="#b40">(Wu et al., 2020)</ref>, Duckling and Wikidata to obtain the target information for the tasks.</p><p>As the number of instances vary across QTT-DATA, we upsampled between modalities and tasks to balance them. Details of each task, and upsampling, can be found in Appendices D.2 and D.1.5. Also, when the input is a graph, we linearise it using the same format for data-to-text tasks in the GEM benchmark <ref type="bibr" target="#b6">(Gehrmann et al., 2021)</ref>. During training, we use cross-entropy loss as the objective. At inference, we generate with greedy search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We compare QTT to the original models (DQE, hereafter) used in the Data-QuestEval metric <ref type="bibr" target="#b29">(Rebuffel et al., 2021)</ref> in terms of QG coverage, QA accuracy and consistency as well as performance in two downstream tasks. In the following, we describe DQE, our evaluation data and methodology.</p><p>Baseline: the DQE models DQE comprises four T5-small <ref type="bibr" target="#b27">(Raffel et al., 2020</ref>) models fine-tuned for QG-QA from graph and text. For text, their QA model (DQE-TextQA) was fine-tuned on SQuAD 2.0 and the QG model (DQE-TextQG) on SQuAD 1.0. For graphs, both their QG (DQE-KGQG) and QA models (DQE-KGQA) were fine-tuned on a synthetic QG dataset of (g, a g , q) triples created by applying DQE-TextQG to a (g, t) corpus.</p><p>Evaluation data We reserved the test part of WEBNLG, which comprise 1,779 (graph, text) instances, for evaluation. The parallel (g, t) data here ensures that a question can be answered using graph or text, allowing us to check the models' cross-modal consistency (Section 6). We apply both DQE and our model to the test set and generate questions for every graph and text.</p><p>Evaluating QG Coverage We compare the coverage of QTT against DQE by measuring the number of unique questions they each generated on the WEBNLG test set. We also compare the semantic coverage of the questions using BERTScore (BSc) <ref type="bibr" target="#b44">(Zhang et al., 2020)</ref>, by taking one model's question for a given entry as prediction and the other's generated questions for the same entry as multireferences. This is repeated with both approaches swapped. The intuition is that if approach A scores Figure <ref type="figure">2</ref>: QA Accuracy. Bold lines denote QA comparisons within/between modalities and/or approaches. Dotted arrows indicate the context X or X 1 that the question (q X ) is posed against to obtain the answers.</p><p>higher with approach B's questions as references than vice-versa, A's questions are "contained" in B's and conversely, B has wider semantic coverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluating QA Consistency</head><p>In what follows, we refer to a X , the answer used to condition the generation of q X , as the ground truth (GT) or the reference answer. We use âX to denote a generated answer. For a given question, âX is the answer derived from modality X and âX 1 is from modality X 1 . We use superscripts (e.g. âA X and âB X ) to distinguish answers generated by different models for the same question q and input context X.</p><p>Accounting for the various ways in which a question can be answered (i.e. a X , âX , âX 1 ), we evaluate the quality of multimodal QG-QA models by computing three consistency metrics. <ref type="foot" target="#foot_4">6</ref> Internal Same-mod (GT) compares the generated answers âX against the reference answers a X , indicating the approach's self-consistency. Internal X-mod (GT) compares the ground truth a X with the answer derived from the other modality âX 1 . Finally, Internal X-mod (Gen Ans) compares âX 1 and âX , the answers derived from each modality.</p><p>We also investigate QA across approaches, allowing an external indication of each's QG-QA capabilities. Here, we examine the two answers that can be generated by approach B (â B X and âB X 1 ) when given q A X , a question generated by A. We do this on three levels: (i) X-Appr Same-mod (GT), by comparing âB X against a A X on the same modality that q A X came from; (ii) X-Appr X-mod (GT) comparing B's generated answer with the reference answer across modalities (i.e. âB X 1 vs a A X ); and (iii) X-Appr X-mod (Gen Ans) where âB X 1 is compared against âA X . A graphical overview of these QA consistency comparisons can be found in Figure <ref type="figure">2</ref>. Downstream: QA with FiD For another external verification of QTT's (and DQE's) QG, we conducted experiments with Fusion-in-Decoder (FiD) <ref type="bibr" target="#b11">(Izacard and Grave, 2021)</ref>. We use a checkpoint that was trained on TriviaQA <ref type="bibr" target="#b12">(Joshi et al., 2017)</ref> as the questions there are factual in nature, i.e. compatible with the texts in WebNLG. To investigate the quality of QTT-DATA, we also fine-tune the same FiD checkpoint using either QTT-DATA or DQE's training data and use these for QA. <ref type="foot" target="#foot_5">7</ref> Similar to X-Appr Same-mod (GT) above, we compare âB X against a A X , except that B in this case is a given fine-tuned (or not) FiD QA model while A is DQE or QTT. Though such a setting gives upper-bound FiD scores,<ref type="foot" target="#foot_6">8</ref> the differences in scores -when varying fine-tuning data and QG -independently validates QTT's QG vs DQE's and QTT-DATA too. Downstream: Data-QuestEval metric Since DQE was originally used in the Data-QuestEval metric, we also compare the correlation of the resulting Data-QuestEval metric with human judgments when DQE is replaced with QTT. For this, we compute the correlations with the judgments collected on 2,007 outputs from 9 participating systems in the WebNLG Challenge <ref type="bibr">(Gardent et al., 2017b)</ref>. Following <ref type="bibr" target="#b29">(Rebuffel et al., 2021)</ref>, we compute Pearson's r , but also report Spearman's ρ.<ref type="foot" target="#foot_7">9</ref> </p><p>Evaluation settings The following describes and explains the settings for our evaluations.</p><p>‚ Answer selection for QG inference To have a direct comparison with <ref type="bibr" target="#b29">(Rebuffel et al., 2021)</ref>'s models, we follow their use of the spacy pipeline for answer selection on t (see Section 4), and report results with this setting in Sections 6 and 7. However this approach is noisy and often segments NEs with nouns or NPs in them (e.g. 'English' being extracted from 'English Without Tears'), leading to ill-formed questions. We trained an answer selection model for texts (see Appendix D.2) using QTT-DATA, ensuring that the answers for QG are meaningful spans in t. We conducted our ablation experiments and human evaluation (Sections 8 and 9) using this answer selection method for text.</p><p>‚ Automatic metric Following <ref type="bibr" target="#b29">(Rebuffel et al., 2021)</ref>, we use BERTScore (BSc) for evaluation, to address the restrictiveness of token F1 for crossmodality QA. We use the same settings, except the following for a clearer analysis: (i) BScs were rescaled against the official BSc baseline for a wider spread, (ii) "unanswerable" strings were set to an empty string to avoid non-zero BSc for these and "over-counting" them, and (iii) lowercasing.</p><p>‚ Self-consistency filter Since both QTT and DQE are trained on synthetic data, some generated questions may be ill-formed and pose an impact on QA. We therefore filter from both QTT and DQE the questions: (i) which cannot be answered from their source context; or (ii) whose generated answer âX has a BSc &lt; 0.7 when compared against the reference a X . We focus our analysis for QA Consistency and the Downstream Evaluations on the results after filtering as this is the upper bound of the approaches' performance; the impact of removing this filter is included in our ablations (Section 8). For congruence with our QG Coverage analysis, if the filtering will leave a given approach A -and therefore B as well -with no QA pairs (i.e. no coverage), we keep one QA pair for A.</p><p>We note also that the self-consistency filter above is important in the downstream Data-QuestEval evaluation (see above) -given how the Data-QuestEval metric is computed (i.e. the generated answer is compared against the GT answer), if a generated question cannot be answered by the source context and yet still posed to the other modality, it will not capture the factuality comparison accurately (i.e. it will skew the metric and affect its reliability).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>QG Coverage QTT generates three times as many questions as DQE when the input is a text (14,141 vs 42,959) and 10 times more when it is a graph <ref type="bibr">(7,272 vs 75,906)</ref>. Figure <ref type="figure" target="#fig_1">3</ref> provides a finegrained view of the QG coverage by the (graphbased) size of the context. This higher coverage results from three modeling choices differentiating QTT from DQE: (i) QTT is trained to generate multiple questions from a given X; (ii) the enlarged size and coverage of QTT's training data by applying Q-KELM-trained QG models to WEBNLG; and (iii) the use of q type controls in SQ-GEN, permitting multiple SQs of various types to be generated from a single X.</p><p>The BScs are also higher with QTT's questions as references (89.7 vs 85.3 for text; 90.4 vs 82.5 for graph), suggesting that QTT is not just generating more questions but also ones that "contains" DQE's as well as semantically different ones from DQE's. Total and average number of generated questions is much higher for QTT across both modality and input size. The delta increases with the size of the input. <ref type="table" target="#tab_7">4</ref> summarises the comparisons between QTT and DQE on QA accuracy. A finer-grained analysis of QTT's Internal performance across question complexity can be found in Table <ref type="table" target="#tab_1">10</ref> in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QA Consistency Table</head><p>‚ QTT outperforms DQE on self-consistency Despite QTT and DQE both starting fine-tuning from the same T5-small checkpoint, QTT gains over DQE in Internal Same-mod (GT) (+8.0 BSc for text, +3.8 for graph). This shows that using QTT-DATA -which provides aligned widecoverage QA-QG data -for training in a multimodal multi-task manner enables QG and QA with greater internal roundtrip consistency.</p><p>‚ Our synthetic in-domain data improves performance QTT's self-consistency gains over  DQE for text also stems from our procedure for creating in-domain data. DQE-TextQG and DQE-TextQA were fine-tuned on SQuAD only, leading to a drop in scores when DQE-TextQA is applied on WEBNLG (vs DQE-KGQA's 95.0). This out-of-domain effect also shows when it answers a question QTT generated with text (95.4 to 56.8, Table <ref type="table" target="#tab_7">4</ref>); whereas, in the reverse case, the drop for QTT when it answers a DQE question generated is much less (87.4 to 81.4).</p><p>‚ Q-DATA with multi-task training improves cross-modality performance QTT outperforms DQE by at least 9.1 BSc (e.g. 69.7 vs 60.6 for T Ñ G) in Internal X-mod (GT) showing that it can more accurately answer questions cross-modally with respect to the reference answer, and/or generate questions that allow this. This is beneficial when using the QG-QA model(s) for evaluation such as the Data-QuestEval metric where this comparison (â X 1 and a X ) is relied on to assess the semantic concordance between the data input and generated text. Furthermore, a low discrepancy in the Internal X-mod (Gen Ans) performances between the cross-modal directions (i.e. G Ñ T vs T Ñ G) is ideal under our parallel data evaluation setting, as it shows that the QG-QA model(s) is able to reflect the agreement between the pg, tq pairs. QTT's 4.3 BSc gap here narrows by nearly two-thirds the 12.1 BSc gap faced by DQE (i.e. 76.4-72.1 vs 51.8-63.9). ‚ QTT's performance is consistent across question complexity and externally validated We find that QTT also performs consistently for all nf , (the number of facts q relates to) for text and for graph (Table <ref type="table" target="#tab_1">10</ref> in Appendix B). Our findings above on QTT's internal performance also hold when examined cross-approach (X-Appr, i.e. external); whenever QTT is used to answer questions generated by DQE, the drop in QA accuracy is significantly lower (or in some cases a gain) than vice-versa. Downstream Evaluations QTT also outperforms in two downstream tasks. Our FiD experiments (Table <ref type="table" target="#tab_9">5</ref>) show that the QTT's questions can be answered with higher accuracy than DQE's for both modalities -likely because QTT's training data permits it to generate more CQs than DQE, which is closer to the forms in TriviaQA. The combination of fine-tuning FiD with QTT-DATA and QTT QG also betters every other combination with DQE and/or its training data. These validate (i) our procedure for creating QTT-DATA, and (ii) the use of it with multimodal multi-task modeling for improved QG. Besides that, using QTT for computing the Data-QuestEval metric also boosts by &gt;10 points the score's correlation (Spearman's ρ) with human judgments of semantic adequacy (Table <ref type="table" target="#tab_10">6</ref>). This is likely due to the improved QG (quality and coverage) for both modalities, allowing QA-based evaluation to more accurately assess the information content of the data and the generated text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusion-in-Decoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QG:</head><p>DQE DQE DQE QTT QTT QTT QA:   </p><formula xml:id="formula_1">FiD 0 FiD D FiD Q FiD 0 FiD D FiD Q Same-mod (GT) T Ñ T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Ablation</head><p>We also studied the impact of four variations to the data and modeling: (i) X-Filt removes the question self-consistency filter (Section 6); (ii) Data uses a similar data setting as <ref type="bibr" target="#b29">(Rebuffel et al., 2021)</ref>  The ablations show that, other than the question self-consistency filter and using QTT-DATA, the other variations have relatively limited impact on QTT's Same-mod (GT). It also shows that using our data generation procedure leads to improvements in QA; especially in cross-modal settings (X-mod (GT) and X-mod (Gen Ans)).  We conducted a human evaluation on the quality of QTT's questions since there are no reference questions. To have a broad study, we sampled 10 questions each from bins combining these characteristics: (i) modality -whether QG from graph or text; (ii) complexity -the question's size (nf ); and (iii) QA accuracy, where the questions were separated into three quantiles based on their Internal Same-mod (GT) score. 10  Three doctoral candidates in NLP fluent in English, were shown the 240 questions and their contexts, and asked to rate each question on three aspects with binary choice: whether it is (i) consistent with the context; (ii) natural-sounding; and (iii) a CQ. For the last aspect, we report whether their rating matches the control (nf =1 or nf &gt;1) used for generation. The results can be found in Table <ref type="table" target="#tab_13">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Human evaluation</head><p>The overall agreement between the annotators is substantial (Fleiss' kappa: 0.65 for KGQG; 0.61 for TextQG). For TextQG, QTT scores ě 0.75 on all aspects. In KGQG, we observe lower scores for Consistency and Naturalness. This is likely due to the increased challenge when generating from graph (which is under-specified and requires a semantic gap to be overcome), and is compounded by unseen properties in the WEBNLG test set. There is also a difference in the performance for Complexity between the graph and text modalities; we found that this can be attributed to the challenge of accurately judging KG facts in text. 11</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusion</head><p>We propose an approach (QTT), which we show generates more questions that cover more information compared to previous work (DQE). Unlike existing approaches, our data and architecture allows us to generate multiple questions for a given input. Extensive internal, cross-modal and external checks show that QTT outperforms DQE on QA consistency. The quality of our generated questions was verified by human evaluation for semantic consistency, naturalness and adherence to our complexity controls. Finally, the use of our approach also leads to improvements against DQE in two downstream evaluations (QA with Fusion-in-Decoder and the Data-QuestEval metric).</p><p>Our main contributions are (i) a large multimodal general QG-QA dataset (Q-KELM) which will be made available on publication, (ii) a data generation procedure including two general multimodal QG models enabling controllable generation of indomain synthetic QG-QA datasets, and (iii) a multimodal multi-task QG-QA model that can generate 10 e.g. 1 bin is {G, nf =1, 1st quant.} i.e. SQs from graph, with BSc for âg vs ag in top 33% of all SQs from graph.</p><p>11 For e.g. the question "Who was born in Los Angeles in California?" generated from the text "The birthplace of X is Los Angeles in California." corresponds to the single KG fact x X; born in; Los Angeles, California y but may be judged as being a CQ of more than 1 fact (e.g. x X; born in; Los Angeles &gt; and &lt; Los Angeles; located in; California y).</p><p>and answer questions from text and from graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Acknowledgments</head><p>We thank the anonymous reviewers for their feedback. This research was supported by ANR Project QUANTUM (Project-ANR-19-CE23-0025). Experiments presented in this paper were carried out using the Grid'5000 testbed, supported by a scientific interest group hosted by Inria and including CNRS, RENATER and several Universities as well as other organizations (see https: //www.grid5000.fr).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">Limitations</head><p>Our data generation relies on the KELM dataset <ref type="bibr" target="#b0">(Agarwal et al., 2021)</ref>, which was generated using a single pretrained language model (T5large). As such, KELM -and hence the texts in our generated data (Q-KELM) too -reflects only the set of factual or linguistics characteristics in this model. For instance, there is certainly more than one way a KG graph (or set of facts) can be lexicalised in text, however KELM only contains one lexicalisation for each of the KG subgraph within it. In addition, we only use a single QG model for generating our initial sets of synthetic QA pairs (Q-KELM, Q-WEBNLG 0 ). Although by using Q-KELM to train our two general QG models, we may have introduced new varieties of questions into QTT-DATA, it is unlikely we have obtained the full range of questions possible for a given contextanswer pair. This limitation of KELM may however be alleviated by for example, ensembling the KELM dataset with generations from different LMs (following fine-tuning or with in-context learning) on the same subgraphs used to generate the KELM sentences/passages.</p><p>Secondly, so as to ensure QG and QA fidelity, we made sure to exclude questions generated from text in Step 3 of the data creation process (Section 4) when constructing the KGQG and KGQA instances for QTT-DATA (Section 5). This is because the answers for questions generated from text may not be constrained to a single KG entity. As a result of this, the sets of complex questions for these two tasks are smaller than for their text counterparts 12 . This impacts QTT's capabilities for generating multiple complex questions for a single input instance in the KGQG task 13 . This limitation could potentially be alleviated by using beam search (or variants of it such as diverse beam search <ref type="bibr" target="#b36">(Vijayakumar et al., 2016)</ref> and constrained beam search <ref type="bibr" target="#b25">(Post and Vilar, 2018)</ref>) to increase the generation of complex questions from graph using CQ-GEN.</p><p>Finally, although we used the agreement of two state-of-the-art QA models when checking for QG acceptability, we cannot be certain that questions rejected by the QA models are not actually valid 12 On the other hand, given the parallel nature of WEBNLG, the answer for a question generated from graph can be found in the text, allowing us to include such questions when constructing the TextQG and TextQA instances for QTT-DATA. 13 The input context here is g 1 P g, where 2 ď| g |ď 4</p><p>questions -in such cases, it means that the coverage of QA pairs in our datasets is constrained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13">Ethics Statement</head><p>As advancements in generative technologies accelerate in terms of capabilities, scale and public access, so too must the need for the the ability to understand if such machine-generated information are reliable. We believe that our KG/NL-aligned QG-QA data creation method and cross-modal QG-QA model has the potential to contribute positively in the following areas: (i) a QG-QA model with cross-modal consistency can aid in tasks that includes but are not limited to automated fact verification, KG-totext/text-to-KG quality estimation and knowledge graph completion; and (ii) the ability to generate indomain QA data can help improve downstream QA performance and dialogue systems to aid humanmachine interactions with KGs. On the other hand, a direct application of our method and model for a task such as fact verification could lead to a failure to capture misinformation, which have the potential for substantive societal harm. This risk arises because KELM is based on a snapshot of the Wikidata KG from circa 2019. Additionally, the T5 pretrained language models used in producing KELM, and also in all our models, were trained with data up to 2020. These constrain the extent and validity of facts in our model up to these points in time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Detailed results</head><p>Finer-grained analysis of QTT QA performance Table <ref type="table" target="#tab_1">10</ref> provides a finer-grained view of QTT's same-mod and cross-mod QA consistency performance; it is evaluated for QA performance on the set of questions relating to varying number of facts (1 ď nf ď 4).</p><p>C Implementation details: CQ-Gen and SQ-Gen</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Training data</head><p>Q-KELM is used as the training data for the CQ-GEN and SQ-GEN models (respectively using CQ-KELM and SQ-KELM from Q-KELM). Each training instance in CQ-GEN and SQ-GEN is assembled in the manner described in the following paragraphs; examples for them can be found in Table <ref type="table" target="#tab_1">11</ref> and<ref type="table" target="#tab_3">12</ref>.</p><p>Context X When generating from text t, the entire t is as input for both CQ-GEN and SQ-GEN. For generation from KG subgraph, the input to both models is g 1 , a subgraph of g of size nf where 1 ď nf ď 4. For SQ-GEN, the size of g 1 is always 1; for CQ-GEN, it is 2 ď nf ď 4 .</p><p>Answer a X For generation from text, a t the text answer that was extracted by the RoBERTa QA model (see Section 4) is used. When generating from KG subgraph, the graph answer a g (the entity/value in g that is aligned with a t (see below) is used.</p><p>‚ Aligning a g The graph answer a g is the entity in g which either exactly matches, contains or else has the smallest edit distance to a t . If a t cannot be matched to a graph entity, the (q, a t ) is rejected, together with those unanswerable by the QA model.</p><p>Answer semantic type a type The answer entity semantic type a type is used for QG when generating from text as well as from graph. It is obtained by using the graph entity (a g ) aligned with the answer by querying Wikidata for the set of entities/values that a g has the 'instance of' and/or 'subclass of' property with. The JNR Class DE15 is a diesel-hydraulic locomotive class made by Kawasaki Heavy Industries, which entered service in 1967.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KELM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q-KELM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance Description Example</head><p>Text (CQ) pt, q, a t q ‚ q: generated from t using Text QG (Step 1)</p><p>What is the name of the diesel-hydraulic locomotive class made by Kawasaki Heavy Industries? ‚ a t : answer derived from pt, qq using Text QA (Step 1) JNR Class DE15 Text (SQ) pt, q, a t q ‚ q: generated from t using Text QG (Step 1) When did the JNR Class DE15 enter service? ‚ a t : answer derived from pt, qq using Text QA (Step 1) 1967 Graph (CQ) pg 1 , q, a g q ‚ g 1 Ď g: a subgraph of g heuristically aligned with q xJNR Class DE15, subclass of, diesel-hydraulic locomotivey, xJNR Class DE15, instance of, locomotive classy, xJNR Class DE15, manufacturer, Kawasaki Heavy Industriesy ‚ q: generated from t using Text QG (Step 1) What is the name of the diesel-hydraulic locomotive class made by Kawasaki Heavy Industries? ‚ a g : graph answer i.e. entity in g 1 heuristically aligned with a t ‚ JNR Class DE15 Graph (SQ) pg 1|1| , q, a g q ‚ g 1|1| P g: a subgraph of g, of size one xJNR Class DE15, service entry, 00 1967y ‚ q: generated from t using Text QG (Step 1) When did the JNR Class DE15 enter service? ‚ a g : graph answer i.e. entity in g 1|1| heuristically aligned with a t 00 1967</p><p>Table <ref type="table">9</ref>: Q-KELM Dataset. For each pg, tq pairs in the filtered version of KELM (see Section 4), QA pairs are created for both t and g 1 Ď g. The question q is generated from t, heuristically aligned with the corresponding subgraph g 1 and both the text and the graph answer are extracted from t and g 1 respectively.</p><p>Question complexity control nf In CQ-KELM and SQ-KELM each (q X , a X ) pair is associated with a subgraph g 1 obtained with heuristic matching (see below). Since g and t are parallel, nf (the size of g 1 ) is used as the control for the complexity of the question for generation from both text and graph.</p><p>‚ Determining nf The size (nf ) of the question is determined by matching a question and its answer to the corresponding subgraph g 1 Ď g where g 1 is the set of triples xs, p, oy in g such that: either s and o have an overlap of ě 1 token with q `at , and/or they can be detected in q `at . TextQG and KGQG can be found in Tables <ref type="table" target="#tab_5">13</ref> and<ref type="table" target="#tab_7">14</ref>, complex and simple TextQA as well as KGQA can be found in Table <ref type="table" target="#tab_9">15</ref>, the KG-to-Text as well as Text-to-KG tasks can be found in Table <ref type="table" target="#tab_10">16</ref>; and the EntType tasks can be found in Table <ref type="table" target="#tab_11">17</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.1 Obtaining sequence of questions for</head><p>QTT QG</p><p>For questions grounded in text, since each WEBNLG text t is associated with a graph g, we use it to gather the set of questions in QTT-DATA generated from g itself and its sub-graphs (and the corresponding sub-texts of t that is present in WEBNLG). From this set, we gather all nf -sized questions which share an a t to form the set of questions associated with pt, a t , nf q. For questions grounded in graphs, two treatments are used to gather questions since the inputs to QTT differs for generation of CQs and SQs (see Appendix D.2). For CQs, given g 1 , which is a subgraph of g (a graph occurring in WEBNLG), we gather all questions with size nf and answer a g that are associated with g 1 . For SQs, given a WEBNLG graph g, we gather all questions of size one and answer a g which can be computed from a triple contained in g.</p><p>Finally, the target in the TextQG and KGQG tasks is typically a set of 3 questions drawn from Ý Ñ q without replacement. If | Ý Ñ q | &gt; 3, Ý Ñ q is padded to ensure | Ý Ñ q | % 3 " 0. If however | Ý Ñ q |ď 3 -as it happens in the complex KGQG settingthe sequence of questions in the target is simply Ý Ñ q .</p><p>Each set of ď 3 questions is then instantiated as a new training instance. This is done in order to train the QG models to generate multiple questions for a given context while still staying within the T5 model's maximum input length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.2 Deriving and maximising QA data</head><p>We derive QA data by creating for each pX, a X , Ý Ñ q q tuple in QTT-DATA as many QA training instances of the form pX, a X , qq as there are questions in Ý Ñ q .</p><p>In addition, if a question q with answer a t is answered by a text t (resp. graph g by a g ) which is strictly contained within another larger text t `(resp. g `) in WEBNLG, we also associate (q, a t ) with t `(resp. (q, a g ) with g `). Table <ref type="table" target="#tab_5">3</ref> shows the number of questions associated with the various input and nf size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.3 Creating EntType instances</head><p>The target for the EntType auxiliary task is the set of entities and values (eset) detected in the input context, and each of the e P eset are paired with their semantic types. For text, eset t is detected by applying the BLINK entity linker for text <ref type="bibr" target="#b40">(Wu et al., 2020)</ref>, and Duckling on the text t. For entities, the semantic type information is retrieved from Wikidata (from the RDF dump dated 29 December 2021). Any type that contains the strings "MediaWiki", "Wikimedia" or "disambiguation" are excluded. For values, we use the type information predicted by Duckling.</p><p>For graph, the set of entities/values (eset g ) that are present in g is used. For values in g, such as dates, times, monetary sums etc we leverage the predictions from Duckling (that was applied on the t that is associated with g). We identify from the Duckling prediction set the one: with the lowest edit distance to the value, which is an alphanumeric string, and has a normalised edit distance &lt; 0.5 (if there is one such).</p><p>Finally, we exclude the following from the training instances: (i) when an answer semantic type for an entity cannot be found; and (ii) when the difference in the number of entity/values sets between the g and its t is more than 2. The latter is so as to avoid semantically similar g and t instances having significantly different targets, which is particularly important since we train in a multi-task setting where all tasks are seen simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.4 Negative sampling for QA</head><p>For the TextQA and KGQA tasks, negative examples were created so as to allow the model to recognise questions that are unanswerable given the context. These samples are created using a random (i.e. 50-50) assignment to one of these two strategies:</p><p>Strategy 1: simple negatives for a given (X, q, a X ) instance are created by picking another (X other , q 1 , a 1 X ) instance in the QA training set where X and X other do not share any common entities/values between them. If X is a text, we use the graph that it is paired with in WEBNLG to check for common entities. A new instance (X, q 1 , "unanswerable") is then created in the training data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Procedure for generating Q-WebNLG.</figDesc><graphic coords="3,306.14,70.87,218.27,226.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparative QG coverage DQE vs. QTT.Total and average number of generated questions is much higher for QTT across both modality and input size. The delta increases with the size of the input.</figDesc><graphic coords="7,70.87,363.87,106.59,99.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Controls A textual prompt is added to both the input and the target to control the question genera-D Implementation details: QTT D.1 Training data QTT-DATA was used as the training data for QTT. Examples of the training instances for each task/subtask can be found in the following tables:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="5,306.14,70.86,218.27,255.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>QTT-DATA instances derived from WebNLG Data. Enclosed letters refer to the triple/text above.</figDesc><table><row><cell>WebNLG Data</cell><cell></cell></row><row><cell>Graph</cell><cell>rAs xAkita Museum of Art, floor count, 3y</cell></row><row><cell></cell><cell>rBs xAkita Museum of Art, opening date, 2013-09-28y</cell></row><row><cell></cell><cell>rCs xAkita Museum of Art, address, 1-4-2 Nakadoriy</cell></row><row><cell></cell><cell>rDs xAkita Museum of Art, floor area, 3746.66 (sqm)y</cell></row><row><cell>Text</cell><cell>rEs The Akita Museum of Art at 142 Nakadori has 3</cell></row><row><cell></cell><cell>floors with a total area of 3746.66 square metres and was</cell></row><row><cell></cell><cell>inaugurated on 28th September 2013.</cell></row><row><cell>QTT Data</cell><cell></cell></row><row><cell>X = graph</cell><cell>rBs, rCs</cell></row><row><cell>a X = g ent</cell><cell>Akita Museum of Art</cell></row><row><cell>Complex</cell><cell>{What museum opened in 2013-09-28 in Nakadori?</cell></row><row><cell>Questions</cell><cell>What is the name of the museum that opened in</cell></row><row><cell></cell><cell>2013-09-28 in Nakadori?}</cell></row><row><cell>X = graph</cell><cell>rBs</cell></row><row><cell>a X = g ent</cell><cell>2013-09-28</cell></row><row><cell>Simple</cell><cell>{In what year was the Akita Museum of Art opened?</cell></row><row><cell>Questions</cell><cell>Which year was the Akita Museum of Art opened?}</cell></row><row><cell>X = text</cell><cell>rEs</cell></row><row><cell>a X = t span</cell><cell>{The Akita Museum of Art}</cell></row><row><cell>Complex</cell><cell>{What is the name of the museum that has 3 floors with a</cell></row><row><cell>Questions</cell><cell>total area of 3746.66 square metres?}</cell></row><row><cell>X = text</cell><cell>rEs</cell></row><row><cell>a X = t span</cell><cell>{2013}</cell></row><row><cell>Simple</cell><cell>{What year was the Akita Museum of Art inaugurated?</cell></row><row><cell>Questions</cell><cell>Which year was the museum inaugurated?}</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>contains examples of these from the</cell></row><row><cell>QTT-DATA (Section 4) dataset we created; further</cell></row><row><cell>examples for Q-KELM (Section 4) can be found</cell></row><row><cell>in Table 9 in the Appendix.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Data Statistics.</figDesc><table><row><cell>Number of questions in the</cell></row><row><cell>QA datasets; nf : the size of the question (no. of facts).</cell></row><row><cell>Training multimodal QG models on Q-KELM and ap-</cell></row><row><cell>plying them to WEBNLG drastically enlarges the Q-</cell></row><row><cell>WEBNLG 0 training data.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>QTT DATA. Average, minimum and maximum number of questions for text and graph inputs of size nf (the size is the number of facts matched by the question)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Consistency Results Avg. of BScs between answers. In subscripts are std. dev. across 5 random runs; superscripts are the difference between X-Appr and Internal. X/Y indicates the QG/QA model used. QTT betters DQE on all consistency tests and for all modalities.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Consistency Results with FiD (Similar to Table 4.) FiD 0 is the public FiD checkpoint trained on Triv-iaQA. FiD D /FiD Q denotes that checkpoint fine-tuned on training data from DQE/QTT for that modality.</figDesc><table><row><cell>Measure</cell><cell>DQE</cell><cell>QTT</cell></row><row><cell cols="3">Spearman's ρ 47.9 (1.47e-104) 58.6 (4.81e-168)</cell></row><row><cell>Pearson's r</cell><cell cols="2">51.8 (2.69e-125) 61.8 (1.24e-191)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Correlations</figDesc><table /><note><p>with human judgments. Comparing when DQE and QTT are used in the computation of the Data-QuestEval metric. All (p-values) « 0.001.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Ablation results. All models here use the text answer selector. Comp. denotes consistency comparison, Mod. denotes QG and QA modalities respectively.</figDesc><table><row><cell>i.e.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Human evaluation for QG. Each score is the average of all annotators' ratings. Values in brackets are the Fleiss' kappa coefficient for that particular aspect.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We also filtered out the KELM (g, t) pairs that have: (i) properties not found in the Wikidata SPARQL endpoint or have a functional nature, e.g. containing terms such as 'identifier', 'image of', which tend to have superfluous t in KELM; and (ii) t with low fidelity to their g, using a contrastive losstrained similarity measure for RDF graph-text</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>pairs.2  In WEBNLG, the graphs are from the DBpedia KG. Here we use a version where some of the DBPedia graphs have been mapped to Wikidata<ref type="bibr" target="#b8">(Han et al., 2022)</ref>, or else removed of underscores and camelcase to align with the Wikidata</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>format.3  In our work, we used t5-base-e2e-qg, a T5-base QG model fine-tuned on SQuAD 1.0 data and the deepset RoBERTA-based QA model fine-tuned on SQuAD 2.0.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>This was not necessary for SQs since SQ-GEN in our next step (3) generates SQs across varying qtype and facts.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>QTT and DQE generates differing numbers of questions for a given X; to ensure a fair evaluation, when an Approach A generates more questions for X, we randomly sample from its set as many questions that Approach B generates for X.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>Here, when the context is a graph, we use the linearisation scheme from<ref type="bibr" target="#b22">(Oguz et al., 2022)</ref> to utilise FiD for KGQA.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>i.e. the information to answer the question is in a single document and this gold document is being provided to FiD.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>The latter may be appropriate since the system outputs for the WebNLG 2017 challenge evaluation were selected to cover a spread of automatic scores and are therefore unlikely to be normally distributed.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_8"><p>https://dl.fbaipublicfiles.com/FiD/ pretrained_models/tqa_reader_base.tar.gz</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>Nl-augmenter: A framework for task-sensitive natural language augmentation. <rs type="person">Nan Duan</rs>, <rs type="person">Duyu Tang</rs>, <rs type="person">Peng Chen</rs>, and <rs type="person">Ming Zhou</rs>. 2017. Question generation for question answering. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages <rs type="grantNumber">866-874</rs>, <rs type="institution">Copenhagen, Denmark. Association for Computational Linguistics</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gcVhgFz">
					<idno type="grant-number">866-874</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>tion; the prompt differs for CQ-GEN and SQ-GEN. SQs are those where q + a t contain only 2 entities, whereas we define CQs as those with at least 7 tokens and such that q + a t contain &gt; 2 entities and the size of the matching subgraph is at least 2 (i.e. nf ě 2). A set of special tokens (added to the T5 tokeniser vocabulary) are used to demarcate the components in the input and target. These prompts and tokens can be seen in the examples found in Tables <ref type="table">11</ref> and<ref type="table">12</ref>.</p><p>Question type q type The question type, which is only used in SQ-GEN, is detected using the question type filter in the NL-Augmenter framework <ref type="bibr" target="#b6">(Dhole et al., 2021)</ref>  14 .</p><p>Target The target is a single question for a given input for both CQ-GEN and SQ-GEN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Technical details</head><p>CQ-GEN and SQ-GEN are each T5-base pretrained models that were fine-tuned from their public checkpoints. Each of them was tuned for up to 10 epochs with early stopping (on the loss for the validation set for when it stops decreasing, with a patience of 3 epochs). A learning rate of 2e-4 was used, together with a linear warmup ratio on 10% of the total training steps, and an effective batch size of 144. For both models, we used the HuggingFace transformers library. We used the Lightning integration of the DeepSpeed framework for efficient training and used bf16 precision together with the DeepSpeedCPUAdam optimizer.   Strategy 2: hard negatives are in the following manner: a mapping M is first created ahead of time where every entity e that can be found in the training data is associated with the set of all the (X, q, a X ) instances where e is mentioned in X (e P X). If X is a text, we use the graph it is paired with in WEBNLG when creating M . For a given (X, q, a X ) instance, another instance, i.e. (X other , q 1 , a X other ) is randomly chosen using M and a random e P X. If a single token from the answer a X other overlaps with (i) any of the tokens for any e (an entity or value) found in X or (ii) any of the tokens of X, then this (X other , q 1 , a X other ) candidate is rejected. Otherwise, a new instance (X, q 1 , "unanswerable") is created in the training data using the instance and the process for (X, q, a X ) terminates. When a candidate instance is rejected, a new (X other 1 , q 1 , a X other 1 ) is drawn from M using another e P X. After 10 tries or when all e P X has been exhausted, a simple negative is created instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.5 Upsampling</head><p>We carry out upsampling on two levels when preparing QTT-DATA (the training data for QTT).</p><p>Between modalities for QG subtasks and between modalities for the same task For QG this is done between complex TextQG and complex KGQG, simple TextQG and simple KGQG to ensure that the QG subtasks are balanced. It is also done between modalities for the QA tasks (e.g. TextQA and KGQA) to ensure that the tasks are balanced between modalities. The negative samples for the QA tasks are also upsampled in the same way.</p><p>For instance, m 1 and m 2 are the sets of samples for modality 1 and modality 2 respectively, and suppose</p><p>.5, we upsample m 2 up to at most 1{3¨| m 1 |. This is to ensure that we do not overrepresent m 2 in the data and overfit on it during training.</p><p>Globally for certain tasks This was done for simple QG as a whole (i.e. after simple TextQG and simple KGQG have been consolidated as one), KG-to-Text, Text-to-KG, and EntType tasks. This is because they are significantly less instantiated samples of these tasks in the data than the rest. The number of samples for each of these tasks were tripled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Technical details</head><p>QA with FiD We fine-tune with the base version of the publicly released FiD checkpoint 15 that is trained on the TriviaQA dataset <ref type="bibr" target="#b12">(Joshi et al., 2017)</ref>. Using the training data for either QTT (i.e. QTT-DATA) or DQE (Section 6), we further fine-tune this checkpoint for 15,000 steps to give four different models (FiD D t trained on DQE's training data for text; FiD D g trained on DQE's synthetic training data for graph; FiD Q t and FiD Q g trained with QTT-DATA with the appropriate context X used, i.e. text and graph respectively). For evaluation, we use the same set of generated questions (q X ) and reference answers, i.e. the answer used to condition QG (a X ) produced by each of DQE and QTT; this is the same set of questions and reference answers used to compute the results in Table <ref type="table">4</ref> Correlations Following <ref type="bibr" target="#b29">(Rebuffel et al., 2021)</ref>, both the Spearman's ρ and Pearson's r correlations in this paper were computed with the SciPy python library <ref type="bibr" target="#b37">(Virtanen et al., 2020)</ref> Text answer selector model This is a T5-base model that is fine-tuned on the task of answer selection on text. The input to the model is a text t, and the target is a set of answer spans in QTT-DATA that are in t. The answer spans comprise: (i) the set of all a t in QTT-DATA for a given t, together with the set of all mention spans obtained from BLINK/Duckling (which were also used in the EntType auxiliary tasks). The answer spans are sorted by the sequence of appearance in t. The model was trained for 10 epochs with early stopping (patience of 3 epochs) on the development set loss. A batch size of 32 and a learning rate of 2e-4 (with a linear warmup of 10% of the training steps) was used.      </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training</title>
		<author>
			<persName><forename type="first">Oshin</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heming</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.278</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3554" to="3565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Synthetic QA corpora generation with roundtrip consistency</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1620</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6168" to="6173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Zero-shot question generation from knowledge graphs for unseen predicates and entity types</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Broadhead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1020</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI&apos;07</title>
		<title level="s">Long Papers</title>
		<meeting>the 20th International Joint Conference on Artifical Intelligence, IJCAI&apos;07<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2007">2007. 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="218" to="228" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2018 Conference of the North American Chapter. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Open question answering over curated and extracted knowledge bases</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<idno type="DOI">10.1145/2623330.2623677</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1156" to="1165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Creating training corpora for NLG micro-planners</title>
		<author>
			<persName><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1017</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The WebNLG challenge: Generating text from RDF data</title>
		<author>
			<persName><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-3518</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Natural Language Generation</title>
		<meeting>the 10th International Conference on Natural Language Generation<address><addrLine>Santiago de Compostela, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="124" to="133" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The GEM benchmark: Natural language generation, its evaluation and metrics</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tosin</forename><surname>Adewumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karmanya</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawan</forename><surname>Sasanka Ammanamanchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuoluwapo</forename><surname>Aremu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghavi</forename><surname>Khyathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miruna-Adriana</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Clinciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaustubh</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanyu</forename><surname>Dhole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esin</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Chinenye Emezue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Gangal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Garbacea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufang</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangfeng</forename><surname>Jhamtani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shailza</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Jolly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mounica</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khyati</forename><surname>Maddela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saad</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><surname>Mahamood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasad</forename><surname>Bodhisattwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Henrique</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><surname>Mille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moin</forename><surname>Emiel Van Miltenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Nadeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salomey</forename><surname>Niyongabo Rubungo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Osei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niranjan</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Raunak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sashank</forename><surname>Santhanam</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.gem-1.10</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Natural Language Generation</title>
		<editor>
			<persName><forename type="first">João</forename><surname>Sedoc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thibault</forename><surname>Sellam</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Samira</forename><surname>Shaikh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anastasia</forename><surname>Rina</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marco</forename><surname>Antonio Sobrevilla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hendrik</forename><surname>Cabezudo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nishant</forename><surname>Strobelt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wei</forename><surname>Subramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Diyi</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Akhila</forename><surname>Yang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jiawei</forename><surname>Yerukola</surname></persName>
		</editor>
		<editor>
			<persName><surname>Zhou</surname></persName>
		</editor>
		<meeting>the 1st Workshop on Natural Language Generation</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="96" to="120" />
		</imprint>
	</monogr>
	<note>Evaluation, and Metrics (GEM 2021</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Retrieval augmented language model pre-training</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="3929" to="3938" />
		</imprint>
	</monogr>
	<note>Panupong Pasupat, and Mingwei Chang</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generating questions from Wikidata triples</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thiago</forename><surname>Castro Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Language Resources and Evaluation Conference</title>
		<meeting>the Thirteenth Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="277" to="290" />
		</imprint>
	</monogr>
	<note>European Language Resources Association</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How question generation can help question answering over knowledge base</title>
		<author>
			<persName><forename type="first">Sen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32233-5_7</idno>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Chinese Computing</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="80" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding based question answering</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3289600.3290956</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.74</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<meeting>the 16th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="874" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1147</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Vancouver, Canada. Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Relevance-guided supervision for OpenQA with ColBERT</title>
		<author>
			<persName><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00405</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="929" to="944" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Difficulty-controllable multi-hop question generation from knowledge graphs</title>
		<author>
			<persName><forename type="first">Vishwajeet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuncheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-30793-6_22</idno>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Natural questions: A benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00276</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="452" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SPARQLto-text question generation for knowledge-based conversational applications</title>
		<author>
			<persName><forename type="first">Gwénolé</forename><surname>Lecorvé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Veyret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Brabant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><forename type="middle">M Rojas</forename><surname>Barahona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="131" to="147" />
		</imprint>
	</monogr>
	<note>Long Papers). Online only. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledgeintensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9459" to="9474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient onepass end-to-end entity linking for questions</title>
		<author>
			<persName><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.522</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6433" to="6441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generating questions for knowledge bases via incorporating diversified contexts and answer-aware loss</title>
		<author>
			<persName><forename type="first">Cao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaiqing</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1247</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2431" to="2441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cooperative self-training of machine reading comprehension</title>
		<author>
			<persName><forename type="first">Hongyin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><surname>Shang-Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghak</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Seattle, United States. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="244" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving unsupervised question answering via summarizationinformed question generation</title>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.340</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4134" to="4148" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">UniK-QA: Unified representations of structured and unstructured knowledge for open-domain question answering</title>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Peshterliev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: NAACL 2022</title>
		<meeting><address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1535" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Question generation from concept maps</title>
		<author>
			<persName><forename type="first">Arthur</forename><forename type="middle">C</forename><surname>Andrew M Olney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalie</forename><forename type="middle">K</forename><surname>Graesser</surname></persName>
		</author>
		<author>
			<persName><surname>Person</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue &amp; Discourse</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="75" to="99" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic parsing for conversational question answering over knowledge graphs</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parag</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilio</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Conference of the European Chapter</title>
		<meeting>the 17th Conference of the European Chapter<address><addrLine>Dubrovnik, Croatia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2507" to="2522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast lexically constrained decoding with dynamic beam allocation for neural machine translation</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Vilar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1119</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Training question answering models from synthetic data</title>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Spring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.468</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5811" to="5826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Data-QuestEval: A referenceless metric for datato-text semantic evaluation</title>
		<author>
			<persName><forename type="first">Clement</forename><surname>Rebuffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laure</forename><surname>Soulier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Piwowarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Lamprier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacopo</forename><surname>Staiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Scoutheeten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.633</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8029" to="8036" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generating natural language question-answer pairs from a knowledge graph using a RNN based question generation model</title>
		<author>
			<persName><forename type="first">Sathish</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachindra</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<title level="s">Long Papers</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="376" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph</title>
		<author>
			<persName><forename type="first">Amrita</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vardaan</forename><surname>Pahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitesh</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generating factoid questions with recurrent neural networks: The 30M factoid question-answer corpus</title>
		<author>
			<persName><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1056</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="588" to="598" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generating quiz questions from knowledge graphs</title>
		<author>
			<persName><forename type="first">Dominic</forename><surname>Seyler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Yahya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Berberich</surname></persName>
		</author>
		<idno type="DOI">10.1145/2740908.2742722</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="113" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cycle-consistency for robust visual question answering</title>
		<author>
			<persName><forename type="first">Meet</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6649" to="6658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Question generation from a knowledge base with web exploration</title>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.03807</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Diverse beam search: Decoding diverse solutions from neural sequence models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ashwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramprasath</forename><forename type="middle">R</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1610.02424</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scipy 1.0: fundamental algorithms for scientific computing in python</title>
		<author>
			<persName><forename type="first">Pauli</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Haberland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeni</forename><surname>Burovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pearu</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warren</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Bright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="261" to="272" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledge base</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrandečić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Krötzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A joint model for question answering and question generation</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Xingdi (eric) Yuan</surname></persName>
		</author>
		<author>
			<persName><surname>Trischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to generate natural language workshop</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scalable zeroshot entity linking with dense entity retrieval</title>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Josifoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6397" to="6407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning representation mapping for relation detection in knowledge base question answering</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongxiang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaixiang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1616</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6130" to="6139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Probabilistic databases of universal schema</title>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction (AKBC-WEKEX)</title>
		<meeting>the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction (AKBC-WEKEX)<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="116" to="121" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Meta-CQG: A meta-learning framework for complex question generation over knowledge bases</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunqi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuhui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics</title>
		<meeting>the 29th International Conference on Computational Linguistics<address><addrLine>Gyeongju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations. A Q-KELM examples Table 9 shows examples of instances from Q-KELM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
