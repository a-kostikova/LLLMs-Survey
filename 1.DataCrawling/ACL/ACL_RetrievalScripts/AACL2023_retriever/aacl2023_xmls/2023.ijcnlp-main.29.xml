<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Question Answer Generation in Bengali: Mitigating the scarcity of QA datasets in a low-resource language</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Md</roleName><forename type="first">Shihab</forename><surname>Shahriar</surname></persName>
							<email>shihabshahriar@iut-dhaka.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Islamic University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ahmad</forename><surname>Al</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Islamic University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fayad</forename><surname>Chowdhury</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Islamic University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Md</forename><surname>Amimul</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Islamic University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ehsan</forename><surname>Abu</surname></persName>
							<email>amimulehsan@iut-dhaka.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Islamic University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raihan</forename><surname>Kamal</surname></persName>
							<email>raihan.kamal@iut-dhaka.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Islamic University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Question Answer Generation in Bengali: Mitigating the scarcity of QA datasets in a low-resource language</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">30D542518492F83B753F58B67E065E3A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T14:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The scarcity of comprehensive, high-quality Question-Answering (QA) datasets in lowresource languages has greatly limited the progress of research on QA for these languages.</p><p>This has inspired research on Question-Answer Generation (QAG) which seeks to synthetically generate QA pairs and minimize the human effort required to compile labeled datasets. In this paper, we present the first QAG pipeline for the Bengali language, which consists of an answer span extraction model, a question generation model, and roundtrip consistency filtering to discard inconsistent QA pairs. To train our QAG pipeline, we translate SQuAD1.1 and SQuAD2.0 using the state-of-the-art NLLB machine translation model and accurately mark the answer spans using a novel embedding-based answer alignment algorithm to construct two Bengali QA datasets that we show are superior to the only two existing machine-translated datasets in terms of quality and quantity. We use our QAG pipeline to generate more than 170,000 QA pairs to build BanglaQA, a synthetic QA dataset from 16,000 Bengali news articles spanning 5 different news categories. We demonstrate the quality of BanglaQA by human evaluation on a variety of metrics. The bestperforming model among several baselines on our dataset achieves an F1 score of 86.14 falling behind human performance of 95.72 F1. Our codebase and curated datasets are publicly available at https://github.com/ shihabshahriar16/BengaliQAG.git.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretrained language models such as BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>, RoBERTa <ref type="bibr" target="#b16">(Liu et al., 2020)</ref> etc. have shown performance comparable to human agents on the Natural Language Processing (NLP) task of Question Answering (QA). However, this performance has been recorded in the case of well-  resourced languages that have extensive, publiclyavailable QA datasets to satisfy the incredible training requirements of these models. The scenario for low-resource languages is concerning as they have seen considerably less progress than their high-resource counterparts. This is primarily due to a scarcity of labeled data, which can be attributed to the massive amount of human effort and time required to create QA datasets. With a particular focus on the Bengali language for this work, we found mention of only two relevant Bengali QA datasets, namely Bengali-SQuAD <ref type="bibr" target="#b21">(Tahsin Mayeesha et al., 2021)</ref> and SQuAD_Bn <ref type="bibr">(Bhattacharjee et al., 2022a)</ref>.</p><p>One approach that has been explored is machine-translated datasets from a high-resource source language to a low-resource target language. Both Bengali-SQuAD and SQuAD_Bn are examples of this method. Bengali-SQuAD is a Google Cloud Translation 1 of SQuAD2.0 <ref type="bibr" target="#b19">(Rajpurkar et al., 2018)</ref> and SQuAD_Bn augments their translation of SQuAD2.0 with the Bengali subset of the popular TyDiQA <ref type="bibr" target="#b6">(Clark et al., 2020)</ref> dataset. However, we note that such datasets often present data quality issues. Despite being economical in terms of time and cost, a major issue in this translationbased approach is that the translated answer does not represent the correct answer span in the context which results in discarding data samples or degrading the quality of the datasets.</p><p>Question Answer Generation (QAG) is an alternative approach proposed to tackle the problem of a scarcity of QA datasets. QAG is the task of generating QA pairs consistent with the information in a provided context and has garnered great interest from the NLP communities in both industry and academia <ref type="bibr" target="#b25">(Zhao et al., 2018)</ref>. Earlier QAG models employed regular Recurrent Neural Networks (RNNs) and their attention-augmented variants. However, the inability of RNNs to capture semantic information in long sequences has pushed work towards the use of transformer-based architectures <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref>. <ref type="bibr" target="#b0">Alberti et al. (2019)</ref> and <ref type="bibr" target="#b5">Chan and Fan (2019)</ref> have proven the effectiveness of these models in generating synthetic QA data that can supplement existing data to train more robust and accurate QA models.</p><p>In this work, we present a Bengali QAG pipeline that can generate synthetic datasets to mitigate the dearth of comprehensive QA datasets in Bengali. Most efforts in curating QA datasets in Bengali have so far been limited to translations of English datasets or have involved a laborious human annotation process. Our work is the first of its kind in the Bengali language to explore this area of research. The QAG pipeline consists of an answer span extraction model, a question generation model, and a roundtrip consistency filtering mechanism to produce QA pairs. To train the pipeline, we translate SQuAD1.1 <ref type="bibr" target="#b20">(Rajpurkar et al., 2016)</ref> and SQuAD2.0 and generate two new translated QA datasets, namely SQuADBangla1.1 and SQuADBangla2.0. Witnessing its tremendous capabilities in machine translation as demonstrated by a 44% BLEU score improvement over the previous state-of-the-art model, we employ Meta AIs NLLB <ref type="bibr" target="#b17">(NLLB Team et al., 2022)</ref> model to translate the SQuAD datasets in Bengali. We then apply a novel embedding-based answer alignment al-1 https://cloud.google.com/translate gorithm to accurately identify answer spans in the translated contexts, since we identified this as an issue in existing datasets.</p><p>Further, to demonstrate the effectiveness of our QAG pipeline, we introduce BanglaQA, the first synthetic Bengali QA dataset, comprising more than 170,000 QA pairs. We use the BARD dataset <ref type="bibr">(Tanvir Alam and Mofijul Islam, 2018)</ref>, a collection of scraped Bengali news articles spanning five categories, and generate both answerable and unanswerable QA pairs, following SQuAD2.0. We present an assessment of the quality of this dataset via human evaluation on five criteria and establish baseline performance scores of three different models on it.</p><p>The contributions of this paper can be summarized as:</p><p>• We present the first Bengali QAG pipeline to produce synthetic QA datasets.</p><p>• We introduce two new translated QA datasets, SQuADBangla1.1 and SQuAD-Bangla2.0 which we show to be superior to existing Bengali QA datasets in terms of quality and quantity.</p><p>• We release BanglaQA, the first Bengali synthetic QA dataset, which also validates the effectiveness of our QAG pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Explorations in the field of QA in the Bengali language began with building factoid-based QA systems. <ref type="bibr" target="#b1">Banerjee et al. (2014)</ref> attempted to build the first Bengali factoid-based QA system, BFQA, which was an information retrieval system that classified questions, retrieved relevant sentences, ranked them, and extracted correct answers. <ref type="bibr" target="#b10">Hoque et al. (2015)</ref> built BQAS, a bilingual question-answering system that could generate and answer factoid-based questions from English and Bengali documents. Islam and Nurul Huda (2019) also implemented a similar question-answering system but based it entirely on time-related questions. However, none of the work before Tahsin Mayeesha et al. ( <ref type="formula">2021</ref>) employed deep learning techniques on SQuAD-like reading comprehension datasets in Bengali.</p><p>Question Generation (QG) is concerned with two questions -what to ask and how to ask. The first part, content selection, was tackled in the past by applying semantic or syntactic parsing of text sequences to obtain intermediate symbolic representations. The second part involves question construction which takes these representations and converts them to natural language questions either in a transformation-based or a template-based approach. <ref type="bibr" target="#b18">(Pan et al., 2019)</ref> The current deep learning frameworks follow the sequence-to-sequence approach and employ transformer-based architectures to learn the content selection via the encoder and the question construction via the decoder. These QG models differ only in certain factors like answer encoding (for answer-aware question generation), question word generation, and paragraph-level contexts. Recent works have solved the problem of answer encoding by either treating the answers position as an input feature <ref type="bibr" target="#b25">(Zhao et al., 2018)</ref>, by encoding the answer with a separate RNN <ref type="bibr" target="#b9">(Duan et al., 2017;</ref><ref type="bibr" target="#b12">Kim et al., 2018)</ref>, or a mixture of both via transformerbased architectures <ref type="bibr" target="#b14">(Lee et al., 2020;</ref><ref type="bibr" target="#b0">Alberti et al., 2019;</ref><ref type="bibr" target="#b5">Chan and Fan, 2019)</ref>.</p><p>BERT models have been used effectively by <ref type="bibr" target="#b0">Alberti et al. (2019)</ref> to generate synthetic QA pairs. The authors use three separate BERT models for the auxiliary tasks of answer extraction, question generation, and question answering. Coupled with roundtrip consistency which ensures that noisy context-question-answer tuples are removed, they show that QA models that are fully pretrained on QA datasets as well as synthetic QA pairs outperform those that are only fine-tuned on the QA datasets. Some works have also looked into different forms of encoding the answer as an input feature. <ref type="bibr" target="#b5">Chan and Fan (2019)</ref> show that their BERT-HLSQG model, which highlights the answer span within the context with special tokens can outperform previously suggested RNN and LSTM-based models. <ref type="bibr" target="#b15">Lewis et al. (2021)</ref> uses a pipeline consisting of four components to generate QA pairs. For passage selection, the authors fine-tune a RoBERTa model on known QA datasets to identify information-rich contexts. Then a BERTbased model or an NER-based approach is used to extract plausible answer text spans. Subsequently, a BART model conditioned on the answer-annotated passage produces relevant questions. Finally, an existing QA model evaluates the question-answer compatibility to omit contradictory pairs which they coin as global filtering.</p><p>Drawing inspiration from these works in the English language, we leverage transformer-based architectures pretrained on Bengali corpora and build a QAG pipeline to overcome the problem of QA dataset scarcity. We demonstrate in later sections that the resulting synthetic QA data is comparable to human-annotated QA datasets and can be used to supplement existing QA datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we describe our process of generating QA pairs in Bengali. In section 3.1, we describe the process of translating the questions, answers, and contexts of a QA dataset separately and then aligning and correcting the answer in the translated dataset. In section 3.2, we provide an overview of our QAG pipeline which consists of an answer span extraction model, a QG model, and a QA model for ensuring round-trip consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Translate QA dataset from a high-resource language</head><p>We denote the context as C, the question as Q and the answer as A. Before translating the context we tokenize the context into individual sen-</p><formula xml:id="formula_0">tences C = [c 1 , c 2 , • • • , c n ]</formula><p>using the sentence tokenizer from the Natural Language Toolkit (NLTK) library <ref type="bibr" target="#b4">(Bird et al., 2009)</ref>. Both SQuAD1.1 and SQuAD2.0 contain the answer start position for every answer. We use this position to identify the context sentence which has the answer, denoting it as c ans . Using the NLLB model, we independently translate C, Q and A and denote the translated context as C ′ , the translated question as Q ′ , and the translated answer as A ′ . We map c ans to its corresponding sentence in C ′ and call it c ′ ans .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Answer span alignment and correction</head><p>Following translation, we find the correct answer span in c ′ ans using an alignment algorithm. We first tokenize c ′</p><p>ans and the answer A ′ using the UToken<ref type="foot" target="#foot_0">2</ref> tokenizer. We choose this tokenizer over existing Bengali word tokenizers since it comes paired with a detokenizer that helps greatly with the reconstruction of the sentence after the alignment process. For each of n tokens in</p><formula xml:id="formula_1">c ′ ans = [c ′ a1 , c ′ a2 , • • • , c ′ an ] and m tokens in A ′ = [a ′ 1 , a ′ 2 , • • • , a ′ m ],</formula><p>we find the corresponding vector representations using fastText<ref type="foot" target="#foot_1">3</ref> word represen-  tation model. In fastText word embeddings, a vector representation is associated with each character n-gram of a word. This allows us to identify similar words written differently in different contexts such as in "সঙ্গীত িশে র" and "সঙ্গীত িশ ". In each of these sentences the word "িশ " is used differently.</p><formula xml:id="formula_2">' Q' A' A' A'' Train Given C', A'' Generate Q' Train Given C' Generate A'' Given C', Q' Find A'' Input New Context C x C x Q y A y Question-</formula><p>For each window of m tokens in c ′ ans , we compute the similarity using an alignment score, S, with the answer tokens using the following formula:</p><formula xml:id="formula_3">S = ∑ i+m k=i θ(c ′ ak , a ′ k ) m</formula><p>where i is the starting position of the window and θ is the function for cosine similarity.</p><p>The window of m tokens in c ′ ans which results in the maximum alignment score is selected as the correct answer span A ′′ . The m tokens are then detokonized using the same tokenizer to find the answer span starting character index in c ′ ans . In some cases, the correct span in c ′ ans has one or two more tokens than the number of tokens in A ′ . For example, the answer A, to a question in SQuAD2.0 is "99" which, translated to A ′ , is "৯৯ ডলার". However, we find a 3-token sequence "৯৯ মািকর্ ন ডলার" in c ′ ans to be the most appropriate match. To account for these cases, we also run the algorithm for window sizes of m + 1 and m + 2.</p><p>We also find, sometimes, the correct span in c ′ ans has a different ordering of tokens than A ′ .</p><p>Such as one answer in SQuAD2.0 is translated as "১২ েসে র, ২০০৬" whereas the correct span in c ′ ans is found as "২০০৬ সােলর ১২ েসে র". To alleviate this problem we calculate the alignment score for all permutations of A ′ for a window and the maximum among them is taken as the alignment score for that window. If S max is less than a specified S threshold , we back-translate each of the tokens of A ′ and c ′ ans separately to English. The back translation gives the tokens</p><formula xml:id="formula_4">A ′ b = [a ′ b1 , a ′ b2 , • • • , a ′ bm ] and c ′ b = [c ′ b1 , c ′ b2 , ...c ′ bn ] from A ′ and c ′ ans</formula><p>respectively. An answer span is again selected using the same algorithm. If the new alignment score is greater than the previous alignment score by a specified δ, we select the new answer span tokens from c ′ ans . A higher threshold and delta lead to a stricter alignment and hence more accurately translated QA pairs, but also generated more noise while a lower threshold and delta sacrificed some accuracy for reduced noise. Based on our experiments, we find that selecting a value of 0.6 for S threshold and 0.05 for δ provides a fair trade-off between accuracy and noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Question-answer generation with roundtrip consistency</head><p>The translated dataset, consisting of contexts denoted as C ′ , questions denoted as Q ′ and correctly aligned answers denoted as A ′′ , is used to train our proposed QAG pipeline. This pipeline consists of three key elements described below:</p><p>Answer span extraction model: We formulate the problem of identifying possible answer spans for question-answer pairs as a token classification problem 4 . However, modifying our translated dataset to a token classification problem dataset would require careful tokenization and labeling of each token in the context of whether they can be possible answer spans. To avoid such rigorous modification and labeling we fine-tune BanglaT5 <ref type="bibr">(Bhattacharjee et al., 2022b)</ref> in a conditional generation setting where the model receives the context sentence containing the answer c ′ ans as the input text and the answer A ′′ as the target text. During inference, the model receives as input a context C x and outputs an answer span A y . The two are passed along to the question generation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question generation model:</head><p>The question generation model is used to generate questions based on the given context and an answer span from the context. For our synthetic datasets, we generate both answerable and unanswerable questions following the format of SQuAD2.0. For both, we use BanglaT5 in a conditional generation setting.</p><p>• For answerable questions, the model is finetuned to receive as input c ′ ans as well as the answer A ′′ , denoted as X 1 , and output the question Q ′ . Since SQuAD1.1 has only answerable questions, we use our translation of SQuAD1.1 for fine-tuning in this case.</p><formula xml:id="formula_5">X 1 = [c ′ ans &lt; /sep &gt; A ′′ &lt; /s &gt;]</formula><p>• For unanswerable questions, the model is fine-tuned to receive as input c ′ ans and "impossible" keyword in place of A ′′ , denoted as X 2 , and output the question Q ′ . During training, we select only unanswerable Q ′ from our translation of SQuAD2.0.</p><formula xml:id="formula_6">X 2 = [c ′ ans &lt; /sep &gt; impossible &lt; /s &gt;]</formula><p>During inference, the model outputs Q y given C x and A y . In the case of unanswerable questions, A y is replaced with "impossible".</p><p>4 https://huggingface.co/tasks/ token-classification Roundtrip consistency filtering: In accordance with the work of <ref type="bibr" target="#b0">Alberti et al. (2019)</ref>, we adapt the roundtrip consistency filtering mechanism to discard QA pairs that are inconsistent. We fine-tune BanglaBert <ref type="bibr">(Bhattacharjee et al., 2022a)</ref> on our translated datasets with the question-answering objective. During inference, this model receives C x and Q y from the output of the question generation model and identifies an answer span A ′ y in C x . We then compare A y and A ′ y and retain the QA pair if they are exactly similar to one another. In the case of unanswerable questions, if the QA model outputs an empty string and A y is found to be "impossible", the QA pair is considered consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>SQuAD1.1 and SQuAD2.0: We use translations of popular question-answering datasets, SQuAD1.1 and SQuAD2.0 for training our QAG pipeline. SQuAD1.1 consists of paragraphs from Wikipedia<ref type="foot" target="#foot_2">5</ref> and crowdsourced question-answer pairs. There are 536 paragraphs divided into 23,215 contexts and 107,785 question-answer pairs in the dataset. The answers are a span of tokens or words within the texts. Since the questions are crowdsourced there is a diverse range of questions in the datasets. SQuAD2.0 adds over 50,000 unanswerable questions to the SQuAD1.1 dataset. Since the test sets of the SQuAD are not public, we only use the translations of the train and validation sets to produce SQuAD-Bangla1.1 and SQuADBangla2.0 after correcting the alignment of the answers and filtering out question-answer pairs with low alignment scores. For SQuADBangla1.1, we use the validation set of the original SQuAD1.1 as the test set, the first 400 paragraphs of the SQuAD1.1's train set as the train set, and the remaining 42 paragraphs of SQuAD1.1's train set as the validation set. We follow the same technique to produce the train, validation and test sets for SQuADBangla2.0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Detail</head><p>We use the Hugging Face<ref type="foot" target="#foot_3">6</ref> implementation and pretrained checkpoints from the Hugging Face library for all required models except for fastText word vector model. For translation, we use the checkpoint "facebook/nllb-200-3.3B" with 3.3 billion parameters from the Hugging Face library. For fastText word embeddings, we use the pretrained word vector model for Bengali, trained on Common Crawl<ref type="foot" target="#foot_4">7</ref> and Wikipedia. For back-translation, we use the "csebuetnlp/banglat5_nmt_bn_en" model checkpoint.</p><p>To train our answer span extraction model, we use the model checkpoint "csebuetnlp/banglat5" with 247 million parameters.</p><p>We fine-tune the answer span extraction model for 3 epochs with a batch size of 8, a learning rate of 3e-5, max length of input text 128, and a max length of output text 30. This required around 3 GPU training hours. For our QA models, we use the pretrained model checkpoints "bert-basemultilingual-uncased", "xlm-roberta-base" and "csebuetnlp/banglabert" with 180, 270, and 110 million parameters respectively. We fine-tune 3 epochs with a batch size of 16 and a learning rate of 2e-5. This required around 2 GPU training hours. For each dataset, we fine-tune the QA models only once to reduce the carbon footprint. For QG, we use the model checkpoint "csebuetnlp/banglat5" which is fine-tuned for 3 epochs with a learning rate of 2e-4 and a batch size of 16. This takes around 2 to 3 GPU hours. The max input length for QG is taken to be 512 and the max output length is taken to be 64. We use an Nvidia GeForce RTX3090 GPU with 24 GB VRAM for all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics</head><p>In accordance with prior literature, we use the EM and F1 scores to establish a benchmark of baseline scores on our synthetically generated QA dataset. We also quantitatively assess our translated datasets by fine-tuning QA models on them and evaluating them on other test sets.</p><p>To assess the quality of our synthetic dataset, we choose five different criteria as outlined below:</p><p>• Grammatical accuracy: We consider a QA pair to be grammatically accurate only if the question and the answer both had no grammatical errors.</p><p>• Relevance to context: If the question is based on the context and the answer can be derived from the context, the QA pair is considered relevant. We discard unanswerable questions for this criterion since they are impossible to answer from the information in the context.</p><p>• Consistency of QA pairs: We consider a QA pair to be consistent if the answer span actually answers the question. Unanswerable QA pairs are deemed to be consistent if there are no answers to them.</p><p>Table <ref type="table">1</ref>: Benchmark scores of different models fine-tuned and tested on different datasets. The scores emphasized in bold in every column are the top 2 EM/F1 scores for that particular dataset in our experiments.</p><p>• Conciseness of answers: An answer span is considered concise if it contained no words or characters beyond the actual answer to the question. Unanswerable questions are discarded for this criterion since they have no answer to assess.</p><p>• Diversity of questions: To quantify diversity, we opt for a binary mark of 1 or 0. We ask assessors at the end of each article whether the questions for that article are diverse in nature spanning different question types like "why", "where", "how", "who", "when" etc.</p><p>5 Experimental Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Translated datasets</head><p>A comparison of different translated QA datasets along with SQuADBangla1.1 and SQuAD-Bangla2.0 is shown in Table <ref type="table">1</ref>. Of all three models assessed, we found BanglaBERT to significantly outperform the other two, XLM-RoBERTa <ref type="bibr" target="#b7">(Conneau et al., 2020)</ref> and mBERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>. This is because XLM-RoBERTa and mBERT are both multilingual language models trained on huge corpora comprising multiple languages, whereas BanglaBERT is trained on specifically Bengali corpora.</p><p>Aligned with this, we find that fine-tuning BanglaBERT on SQuADBangla2.0 results in consistently good performances on all datasets. On SQuAD_Bn, this combination posts an EM of 69.81 and an F1 score of 75.38 and on SQuAD-Bangla1.1, it scores 62.18 EM and 77.87 F1. Testing this combinations performance on SQuAD-Bangla2.0 itself, we find an EM of 65.08 and an F1 score of 71.05. Fine-tuning on SQuADBangla2.0 yields consistently high performance across all datasets, establishing SQuADBangla2.0 as a robust and comprehensive Bengali QA dataset. We observe the lowest EM and F1 scores with Bengali-SQuAD and attribute it to discrepancies in the translation and answer span marking, as identified previously. The highest EM and F1 scores posted on Bengali-SQuAD are 48.81 and 54.37 respectively by mBERT fine-tuned on Bengali-SQuAD itself. Even so, fine-tuning mBERT and XLM-RoBERTa on SQuADBangla2.0 results in comparable performance at <ref type="bibr">44.61 EM and 52.87 F1 and 42.87 EM and 51.66</ref> F1 respectively for each model. We also find that models fine-tuned on SQuADBangla1.1 do not perform well on other datasets. This is primarily because SQuAD-Bangla1.1 does not consist of unanswerable questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Synthetic dataset: BanglaQA</head><p>In order to assess the performance of our QAG pipeline on native Bengali text, we use the pipeline on the BARD dataset, a collection of news articles written by native Bengali speakers scraped from trusted, popular online news portals, to generate our synthetic QA dataset, BanglaQA. The distribution of articles from each of the five categories to  We extracted a random sampling of 2,100 QA pairs across 204 articles and asked a group of seven university students with a firm grasp of Bengali to assess them as per the criteria. Each student assessed 300 QA pairs. The summary of results is presented in Table <ref type="table" target="#tab_6">4</ref>. BanglaQA achieves a humanevaluated score of 98% in terms of grammatical accuracy. We attribute this to the syntactical accuracy of the translated SQuADBangla datasets that we used to train the QAG pipeline as well as the BARD dataset that we took the articles from. The QA pairs in BanglaQA are also mostly relevant to the context and consistent within themselves.</p><p>Acknowledging the general scarcity of humanannotated Bengali QA data, we show the use of BanglaQA as a standalone synthetic dataset. To that end, we provide a benchmark of baseline  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Noting that this is the first work of its kind in the Bengali language, our work is not without its limitations. While the NLLB model achieves stateof-the-art English-to-Bengali performance, we acknowledge the possibility of residual errors in the translation and alignment of the translated SQuAD dataset. However, the results in Table <ref type="table">1</ref> demonstrate that models trained on our SQuAD-Bangla datasets achieve strong performance when evaluated on other datasets. Furthermore, the QAG pipeline is given human-written Bengali text as input contexts and the QA pairs are synthetically generated from them without any further need for translation or alignment. Given the relatively high human evaluation scores for our BanglaQA dataset, we deduce these errors minimally impact the quality of the generated QA pairs.</p><p>Our method for answer span extraction in the QAG pipeline is not suited for multi-hop QA and deeper logical reasoning, which have garnered great interest recently, resulting in datasets like HotpotQA <ref type="bibr" target="#b24">(Yang et al., 2018)</ref> and NarrativeQA <ref type="bibr" target="#b13">(Kočiský et al., 2018</ref>). The answer spans extracted are from text sequences exactly as they are present in the contexts and the generated questions reflect this in their nature.</p><p>The embedding-based alignment algorithm that we used to identify the answer spans posttranslation may not work as is for other languages because of syntactic and semantic differences. However, the principle should be easily adaptable to these languages as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations</head><p>In compliance with the Copyright Act, 2008<ref type="foot" target="#foot_5">8</ref> , Bangladesh, we are publicly releasing all the translated and synthetic datasets generated as a result of this work. There are also no concerns about copyright infringement issues since all of the datasets that we use are already publicly available for noncommercial research usage.</p><p>In appreciation of their efforts in assessing the quality of BanglaQA, the seven students we selected for human evaluation were given appropriate remunerations at standard rates.</p><p>BanglaQA being a synthetic dataset based on Bengali news articles may also be prone to negative bias. This is because news articles may often highlight negative incidents, political biases, and certain stereotypes. This is not a serious issue since it is very specific to the domain of news articles, which, by nature, revolve around such content. However, we can not guarantee that there will not be any serious biases from synthetic datasets generated by this work since this is heavily dependent on the choice of source contexts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of synthetically generated QA pairs from a given context using our QAG pipeline. For more examples see Appendix A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Bengali-SQuAD: Tahsin Mayeesha et al. (2021) used the Google Cloud Translation API to translate 294 paragraphs from SQuAD2.0 to produce the translated dataset Bengali-SQuAD. The authors use random splitting to choose 235 paragraphs with 73,812 QA pairs as the training set and the remaining 59 paragraphs with 17,607 QA pairs as the validation set. For the test set, they collected Bengali Wikipedia articles and made 300 QA pairs which they did not make publicly available. For our purpose of comparison, we use the validation set of Bengali-SQuAD as the test set and split the train set to use the first 200 articles as training data and the rest as validation data. SQuAD_Bn: SQuAD_Bn was presented by Bhattacharjee et al. (2022a) combining translated SQuAD2.0 and the Bengali portion of TyDiQA as part of a natural language understanding benchmark in Bengali. The authors use the translations of both the train and validation sets of SQuAD2.0 as the train set of SQuAD_Bn and use the Bengali portion of TyDiQA as validation and test sets. The train set of SQuAD_Bn consists of 477 paragraphs with 118,117 QA pairs. The validation set has 1,221 paragraphs with only 2,502 QA pairs and the test set has 1,282 paragraphs with only 2,504 QA pairs. BARD: Tanvir Alam and Mofijul Islam (2018) present BARD, a Bengali article classification dataset, in their work on the task of document classification in Bengali. BARD consists of around 376,226 articles collected from different Bengali news portals. The authors consider only the news articles that fall within five categories: state, international, economy, entertainment, and sports. We use a subset of the BARD dataset's articles to generate QA pairs for BanglaQA, our synthetic QA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Such genes are typically shorter and simpler in structure than most eukaryotic genes, with few if any introns.এই ধরেনর জন সাধারণত বিশরভাগ ইউকািরয়ট জেনর ত লনায় সংি এবং সহজ কাঠােমার হয়, যিদ কানও থােক তেব খু ব কম ইন ন থােক।</figDesc><table><row><cell></cell><cell></cell><cell cols="2">এই ধরেনর জন সাধারণত বিশরভাগ ইউকািরয়ট জেনর ত লনায় সংি</cell><cell>এবং সহজ কাঠােমার</cell><cell>C</cell></row><row><cell></cell><cell></cell><cell></cell><cell>হয়, যিদ কানও থােক তেব খু ব কম ইন ন থােক।</cell></row><row><cell>How do the lengths of orphan genes compare to most eukaryotic genes?</cell><cell>NLLB</cell><cell cols="2">অনাথ জেনর দঘ অিধকাংশ ইউকািরয়ট জেনর সােথ ত লনা করেল কমন হয়?</cell></row><row><cell>simpler in structure</cell><cell></cell><cell>কাঠােমা সহজ</cell></row><row><cell></cell><cell></cell><cell>θ2</cell><cell>θ2</cell></row><row><cell></cell><cell></cell><cell cols="2">কাঠােমা সহজ</cell></row><row><cell></cell><cell></cell><cell cols="2">কাঠােমা সহজ</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Distribution of news categories in the articles and QA data in BanglaQA.use as contexts and the number of QA pairs under each category is shown in Table2.Table3shows the distribution of articles and QA pairs for our train, validation, and test sets. We retained 80% of the dataset for our training set resulting in 142,536 QA pairs across 12,797 articles. Of the remainder, 10% was allocated to the validation set, resulting in 17,861 QA pairs across 1,600 articles, and 10% to the test set, resulting in 17,615 QA pairs across 1,600 articles. BanglaQA has more than 170,000 QA pairs in total whereas the previously available datasets Bengali-SQuAD and SQuAD_Bn have roughly 90,000 QA pairs and 123,000 QA pairs respectively.</figDesc><table><row><cell>Categories</cell><cell>Articles</cell><cell>QA Pairs</cell><cell>Unanswerable Questions</cell></row><row><cell>State</cell><cell>3090</cell><cell>29901</cell><cell>5910</cell></row><row><cell>Economy</cell><cell>2660</cell><cell>24964</cell><cell>4903</cell></row><row><cell>International</cell><cell>3530</cell><cell>38782</cell><cell>7778</cell></row><row><cell>Sports</cell><cell>3308</cell><cell>40731</cell><cell>8299</cell></row><row><cell>Entertainment</cell><cell>3409</cell><cell>43634</cell><cell>8941</cell></row><row><cell>Total</cell><cell>15997</cell><cell>178012</cell><cell>35831</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Statistics of BanglaQA train, validation, and test sets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Results of human evaluation of BanglaQA's quality on five different metrics. scores on BanglaQA. We show the performance of mBERT, XLM-RoBERTa, and BanglaBERT on BanglaQA in Table5. Consistent with the fact that BanglaBERT is the only language model pretrained on a solely Bengali corpus of the 3 models tested, it performs the best scoring 75.70 EM points and 86.14 F1. Furthermore, we examine the performance of models trained on BanglaQA on the test set of the SQuAD_Bn dataset. This particular test set is derived from the Bengali section of TyDiQA, which provides a human-annotated benchmark for evaluating model performance. Future work on extending these QA generation techniques to additional tasks such as logical reasoning and multihop QA may further advance the capabilities of Bengali QA. We hope the BanglaQA dataset and the QAG framework presented here will inspire continued research into QA for other low-resource languages.</figDesc><table><row><cell>Model</cell><cell>BanglaQA EM/F1</cell><cell>SQuAD_Bn EM/F1</cell></row><row><cell>mBERT</cell><cell>68.70/78.01</cell><cell>57.03/61.51</cell></row><row><cell>XLMRoBERTa</cell><cell>74.18/84.58</cell><cell>57.59/64.17</cell></row><row><cell>BanglaBERT</cell><cell>75.70/86.14</cell><cell>57.74/65.41</cell></row><row><cell cols="3">Table 5: Benchmark scores of different models trained</cell></row><row><cell>on BanglaQA</cell><cell></cell><cell></cell></row><row><cell cols="3">6 Conclusion and Future Work</cell></row><row><cell cols="3">Prior research on QA for the Bengali language</cell></row><row><cell cols="3">has been significantly hindered by a lack of large-</cell></row><row><cell cols="3">scale Bengali QA datasets. Given the laborious</cell></row><row><cell cols="3">nature of human annotation, the only solution</cell></row><row><cell cols="3">explored so far has involved machine-translated</cell></row><row><cell cols="3">versions of popular QA datasets. In this work,</cell></row><row><cell cols="3">we propose an alternative approach through a</cell></row><row><cell cols="3">QAG pipeline tailored for Bengali and demon-</cell></row><row><cell cols="3">strate its effectiveness by generating the synthetic</cell></row><row><cell cols="3">BanglaQA dataset. We also produce two empiri-</cell></row><row><cell cols="3">cally better translated datasets, SQuADBangla1.1</cell></row><row><cell cols="3">and SQuADBangla2.0, to train our QAG pipeline.</cell></row><row><cell cols="3">Further, we assess the quality of BanglaQA by</cell></row><row><cell cols="3">human evaluation on five different metrics and</cell></row><row><cell cols="3">establish it as a benchmark Bengali QA dataset,</cell></row><row><cell cols="3">reporting the baseline performance of three dif-</cell></row><row><cell cols="3">ferent QA models on it. The BanglaQA dataset</cell></row><row><cell cols="3">should provide new opportunities to develop and</cell></row><row><cell cols="3">evaluate Bengali QA systems, helping address the</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/uhermjakob/utoken</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://fasttext.cc/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>https://www.wikipedia.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>https://huggingface.co</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>https://commoncrawl.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>http://bdlaws.minlaw.gov.bd/act-details-846. html</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>তাঁ র পিরবেতর্ 'মারদািন' ছিবর অিভেনতা তািহর রাজ ভািসনেক েনওয়া হেব বেলও েশানা যাি ল।</head><p>Question:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>মারদািন" ছিবর অিভেনতা েক িছেলন?</head><p>Answer: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>তািহর রাজ ভািসনেক</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>েভা ােক তার নাম, িঠকানা এবং ইউিনক েকাড উে খ কের িক করেত হেব?</head><p>Answer:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>িতনিট ে র উ র িদেত হেব</head><p>Context:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ৈবদু য্িতক শটর্ সািকর্ ট েথেক এ অি কাে র সূ পাত হেয়েছ বেল িনি ত হওয়া েগেছ।</head><p>Question:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>েকান কারেণ এই আগুেনর সূ পাত হেয়িছল বেল জানা েগেছ?</head><p>Answer:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ৈবদু য্িতক শটর্ সািকর্ ট</head><p>Context:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>েছাটেদর মািসক সামিয়কী িকেশার আেলার জ বািষর্ কী উপলেক্ষ ওই েমলার আেয়াজন করা হেয়েছ।</head><p>Question:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>েকান পি কার জ িদন উপলেক্ষ এই েমলা অনু ি ত হয়?</head><p>Answer:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>িকেশার আেলার</head><p>Context:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>রাজধানীর েতজগাঁ ওেয় গতকাল বু ধবার িবএসিটআইেয়র ধান কাযর্ ালেয় এ চারিট িত ােনর িতিনিধর কােছ সনদ হ া র কেরন িত ানিটর মহাপিরচালক ইকরামু ল হক।</head><p>Question:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>িবএসিটআই এর ধান েক িছেলন?</head><p>Answer:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ইকরামু ল হক</head><p>Context:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>এঁ েদর মেধয্ মু ি য়া মু রািলধরন ও েশন ওয়ােনর্ র উইেকটসংখয্া হাজােরর ওপর।</head><p>Question:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>েকান দু ইজন েখেলায়াড় ১০০০ এর েবিশ উইেকট িনেয়েছন?</head><p>Answer:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>মু ি য়া মু রািলধরন ও েশন ওয়ােনর্ র</head><p>Context:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>আর িকউবার িতিনিধদলিটর েনতৃ ে েদশিটর পররা ম ণালেয়র যু রা িবষয়ক পিরচালক েহােসিফনা িভদাল।</head><p>Question:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>িকউবার পররা ম ণালেয়র যু রা িবষয়ক পিরচালক েক িছেলন?</head><p>Answer:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>েহােসিফনা িভদাল</head><p>Context:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>েকননা, বাংলােদেশর যত র ািন হয়, এর ৫০ শতাংেশর মেতা ইউেরাপীয় ইউিনয়েন হয়।</head><p>Question:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>কত শতাংশ র ািন ইউেরাপীয় ইউিনয়েন হয়?</head><p>Answer: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>৫০ শতাংেশর</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>েক ভারতীয় েটিনস তারকােক ইউএস ওেপেনর িশেরাপা িজতেত সাহাযয্ কেরেছ?</head><p>Answer: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>েনা েসােরেসর</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Synthetic QA corpora generation with roundtrip consistency</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1620</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6168" to="6173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bfqa: A bengali factoid question answering system</title>
		<author>
			<persName><forename type="first">Somnath</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudip</forename><surname>Kumar Naskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text, Speech and Dialogue</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2022a. BanglaBERT: Language model pretraining and benchmarks for low-resource language understanding evaluation in Bangla</title>
		<author>
			<persName><forename type="first">Abhik</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tahmid</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wasi</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazi</forename><surname>Samin Mubasshir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Saiful Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anindya</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sohel Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rifat</forename><surname>Shahriyar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-naacl.98</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: NAACL 2022</title>
		<meeting><address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
	<note>United States. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Wasi Uddin Ahmad, and Rifat Shahriyar. 2022b. Banglanlg: Benchmarks and resources for evaluating low-resource natural language generation in bangla</title>
		<author>
			<persName><forename type="first">Abhik</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tahmid</forename><surname>Hasan</surname></persName>
		</author>
		<idno>CoRR, abs/2205.11081</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<title level="m">Natural Language Processing with Python</title>
		<imprint>
			<publisher>Reilly Media Inc</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A recurrent BERT-based model for question generation</title>
		<author>
			<persName><forename type="first">Ying-Hong</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao-Chung</forename><surname>Fan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5821</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Machine Reading for Question Answering</title>
		<meeting>the 2nd Workshop on Machine Reading for Question Answering<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="154" to="162" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00317</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="454" to="470" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Question generation for question answering</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1090</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="866" to="874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bqas: A bilingual question answering system</title>
		<author>
			<persName><forename type="first">Sanjidul</forename><surname>Hoque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shamsul Arefin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><forename type="middle">Moshiul</forename><surname>Hoque</surname></persName>
		</author>
		<idno type="DOI">10.1109/EICT.2015.7392020</idno>
	</analytic>
	<monogr>
		<title level="m">2015 2nd International Conference on Electrical Information and Communication Technologies (EICT)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Design and development of question answering system in bangla language from multiple documents</title>
		<author>
			<persName><forename type="first">Tasnia</forename><surname>Samina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huda</forename><surname>Nurul</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASERT.2019.8934447</idno>
	</analytic>
	<monogr>
		<title level="m">2019 1st International Conference on Advances in Science, Engineering and Robotics Technology (ICASERT)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving neural question generation using answer separation</title>
		<author>
			<persName><forename type="first">Yanghoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwanhee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joongbo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyomin</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The NarrativeQA reading comprehension challenge</title>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Kočiský</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00023</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="317" to="328" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generating diverse and consistent QA pairs from contexts with information-maximizing hierarchical conditional VAEs</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Bok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Seanie</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tae</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghwan</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Hwang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.20</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="208" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">PAQ: 65 million probably-asked questions and what you can do with them</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00415</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1098" to="1115" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">Ro{bert}a: A robustly optimized {bert} pretraining approach</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Nllb Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Costa-Jussà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maha</forename><surname>Çelebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Elbayad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elahe</forename><surname>Heffernan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janice</forename><surname>Kalbassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Licht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skyler</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bapi</forename><surname>Youngblood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Akula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><forename type="middle">Mejia</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prangthip</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hansanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semarley</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ram</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Sadagopan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shannon</forename><surname>Rowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chau</forename><surname>Spruit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Necip</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Fazil Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedanuj</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Mourachko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Safiyyah</forename><surname>Ropers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Saleem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2207.04672</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>No language left behind: Scaling human-centered machine translation</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Recent advances in neural question generation</title>
		<author>
			<persName><forename type="first">Liangming</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<idno>ArXiv, abs/1905.08949</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for SQuAD</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2124</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning based question answering system in bengali</title>
		<author>
			<persName><forename type="first">Abdullah</forename><forename type="middle">Md</forename><surname>Tasmiah Tahsin Mayeesha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashedur</forename><forename type="middle">M</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName><surname>Rahman</surname></persName>
		</author>
		<idno type="DOI">10.1080/24751839.2020.1833136</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Information and Telecommunication</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="178" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bard: Bangla article classification using a new comprehensive dataset</title>
		<idno type="DOI">10.1109/ICBSLP.2018.8554382</idno>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on Bangla Speech and Language Processing (ICBSLP)</title>
		<editor>
			<persName><forename type="first">Md</forename><surname>Tanvir</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alam</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Md Mofijul</forename><surname>Islam</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">HotpotQA: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1259</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2369" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Paragraph-level neural question generation with maxout pointer and gated self-attention networks</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochuan</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifa</forename><surname>Ke</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1424</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3901" to="3910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A Appendix Some examples of QA pairs generated using our QAG pipeline are shown in table 6. These QA pairs are generated using articles from BARD</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
