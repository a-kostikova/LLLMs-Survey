<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Glass Ceiling of Automatic Evaluation in Natural Language Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Pierre</forename><surname>Colombo</surname></persName>
							<email>colombo.pierre@centralesupelec.fr</email>
						</author>
						<author>
							<persName><forename type="first">Maxime</forename><surname>Peyrard</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">EPFL</orgName>
								<address>
									<addrLine>4 Telecom Paris 5 ILLS</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">CNRS -CentraleSupélec</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nathan</forename><surname>Noiry</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>West</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">EPFL</orgName>
								<address>
									<addrLine>4 Telecom Paris 5 ILLS</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">CNRS -CentraleSupélec</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pablo</forename><surname>Piantanida</surname></persName>
						</author>
						<author>
							<persName><surname>Mics -Centralesupélec</surname></persName>
						</author>
						<author>
							<persName><surname>Equall</surname></persName>
						</author>
						<title level="a" type="main">The Glass Ceiling of Automatic Evaluation in Natural Language Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E6FB2DE299F5E07CFD031A7FAA08F555</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic evaluation metrics capable of replacing human judgments are critical to allowing fast development of new methods. Thus, numerous research efforts have focused on crafting such metrics. In this work, we take a step back and analyze recent progress by comparing the body of existing automatic metrics and human metrics altogether. As metrics are used based on how they rank systems, we compare metrics in the space of system rankings. Our extensive statistical analysis reveals surprising findings: automatic metrics -old and neware much more similar to each other than to humans. Automatic metrics are not complementary and rank systems similarly. Strikingly, human metrics predict each other much better than the combination of all automatic metrics used to predict a human metric. It is surprising because human metrics are often designed to be independent, to capture different aspects of quality, e.g. content fidelity or readability. We provide a discussion of these findings and recommendations for future work in the field of evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Crafting automatic evaluation metrics (AEM) able to replace human judgments is critical to guide progress in natural language generation (NLG), as such automatic metrics allow for cheap, fast, and large-scale development of new ideas. The NLG fields are then heavily influenced by the set of AEM used to decide which systems are valuable. Therefore, a large body of work has focused on improving the ability of AEM to predict human judgments.</p><p>Human judgment data is typically employed to decide which metric to select based on correlation analysis with human annotations <ref type="bibr" target="#b26">(Rankel et al., 2013;</ref><ref type="bibr" target="#b18">Owczarzak et al., 2012;</ref><ref type="bibr" target="#b9">Graham, 2015)</ref>. In this work, we take a step back and investigate the relationship between existing AEM and human judgments globally. We do not make metric recommendation but reflect upon the global progress in the field of automatic evaluation. Our work is motivated by the findings of Fig. <ref type="figure" target="#fig_0">1</ref>. It depicts the improvement over time, when new metrics were introduced, in the ability to fit human judgments when using all existing metrics as features. The fit is measured by the correlation with humans of a trained classifier in a 5-fold cross-validation setup. Remarkably, our observations indicate that there have been only minor incremental improvements, and the progress in recent years appears to be reaching a saturation point.</p><p>Recent works emphasized the importance of viewing metrics in terms of how they rank systems instead of just comparing score values <ref type="bibr" target="#b17">(Novikova et al., 2018;</ref><ref type="bibr" target="#b22">Peyrard et al., 2021;</ref><ref type="bibr" target="#b4">Colombo et al., 2022)</ref>. Indeed, not only ranking is a more robust framework of comparison, it is also more aligned with the way metrics are used: identifying and extracting the "best system". Thus, we perform our analysis in the space of rankings. i.e., how do metrics rank systems? By analyzing 9 datasets covering 4 tasks and 270k scores, we made the following observations: Findings. (i) Automatic metrics are much more similar to each other, in terms of how they rank systems, than they are to human metrics. It means that AEM, even the more recent transformer-based ones are similar to the older ones when used in practice (ROUGE and BLEU). (ii) This lack of complementarity results in the inability to fit human judgments even when all these metrics are taken together as features for a classifier predicting humans. (iii) Quite surprisingly, different human dimensionsdifferent annotations guidelines such as readability, or content fidelity -are very predictive of each other, whereas AEM are much less predictive of humans. This finding is striking because human metrics are designed to capture different and independent aspects of quality whereas AEM have been selected precisely for their ability to match humans. We would expect human metrics to be uncorrelated and automatic metric to be highly correlated with humans but we observe the opposite. First, it casts serious doubt about the ability of AEM to replace human judgments. Then, the correlation between independent human annotations of quality hints at some latent inherent goodness of systems: good systems are good in different aspect whereas bad systems are bad across all aspects.</p><p>Our findings have several consequences that can inform future research. Newly introduced metrics are not complementary to previous ones, resulting in small global improvements. As a way forward, we propose that research, instead of crafting metrics that maximize correlation with humans, focus on making metrics that also aim to be explicitly complementary to the set of existing metrics. This would enforce maximal marginal gain and ensure that the field, as a whole, makes progress towards capturing the complexity of human annotations.</p><p>For practitioners, it is common practice to report several AEM in the hope to get a better view of system performances. However, reporting several metrics that all produce similar rankings does not bring useful additional information. With our proposal, reporting a set of complementary metrics would better serve the intended purpose.</p><p>To help research build upon our work and use our measure of complementarity, we make our code available at github.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Terminology. Let X be the space of possible outputs for an NLG task. An NLG metric is a function m : X ×X → R + which, from a given textual candidate C ∈ X and corresponding reference R ∈ X , computes a score m(C, R) reflecting the properties that C should satisfy (e.g. fluency, fidelity...). Of course, it is illusory to summarize subtle semantic properties by a single scalar and one is rather seeking for metrics that are able to discriminate between different systems. In fact, crafted AEM are evaluated by comparison to human judgments: one usually computes ranking correlations such as the Kendall's τ . Higher correlations indicate that the AEM is a better replacement for the human metrics.</p><p>Encoding metrics with rankings. Since the usage of NLG metrics is to rank systems, we choose to represent an NLG metric, automatic or human, by the ranking it induces on a set of systems or utterances. More formally, for S ≥ 1 NLG systems evaluated on a dataset made of U ≥ 1 utterances, there exists a natural ranking representations of m:</p><p>Each utterance u ∈ {1, . . . ,U} induces a ranking σ m u ∈ R S of the S systems seen as a vector σ m u , where σ m u [s] is the rank of system s ∈ {1, . . . , S}. For a system s, the representation of a metric m, noted σ m [s], is sum of rankings over the utterances:</p><formula xml:id="formula_0">σ m [s] := U u=1 σ m u (s) ∈ R N .<label>(1)</label></formula><p>We call this System level representation. Symmetrically, each system s ∈ {1, . . . , S} induces a ranking ρ m s ∈ R U of the U utterances, where ρ m s [u] is the rank of utterance u. The Utterance level representation of m is sum of rankings over the systems:</p><formula xml:id="formula_1">ρ m [u] := S s=1 ρ n s ∈ R K . (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>Using the space of rankings has been shown to be more robust than the raw scores as it is less sensitive to outliers and statistical variations <ref type="bibr" target="#b16">(Novikova et al., 2017;</ref><ref type="bibr" target="#b22">Peyrard et al., 2021;</ref><ref type="bibr" target="#b4">Colombo et al., 2022)</ref>. Furthermore, this representation is closely tied to Borda counts, which enjoys theoretical properties: the ranking induced by σ m,S is a 5approximation of the Kemeny-consensus which is a good notion of average in the symmetric group <ref type="bibr" target="#b11">(Kemeny, 1959;</ref><ref type="bibr" target="#b27">Young and Levenglick, 1978;</ref><ref type="bibr" target="#b5">Coppersmith et al., 2006)</ref>. It is moreover the fastest approximation of the Kemeny-consensus whose computation is NP-hard <ref type="bibr" target="#b0">(Ali and Meilȃ, 2012)</ref>.</p><p>Complementarity. We measure the complementarity between two metrics -humans or automatic -by the average over utterances of the distance between their rankings of systems. Formally, for two metrics m 0 and m 1 , complementarity is given by:</p><formula xml:id="formula_3">C(m 0 , m 1 ) := 1 U U u=1 d τ (σ m 0 u , σ m 1 u ),<label>(3)</label></formula><p>where d τ is the normalized Kendall's distance between the vectors of rank. It is related to the Kendall's rank correlation τ by: τ = 1 -2d τ .</p><p>Similarly, we define the complementarity between a metric m 0 and a set of other metrics m := {m i } i=1,...,l , as the average pairwise complementarity:</p><formula xml:id="formula_4">C(m 0 , m) = 1 l i=1,...,l C(m 0 , m i ).<label>(4)</label></formula><p>Complementarity measures the extent to which a metric ranks systems differently than another metrics or a set of other metrics. Whether comparing two metrics or a metric with set, it is a number between 0 and 1 where 0 indicates that the metrics rank systems in the exact same order and 1 indicates the exact opposite order. In between, it counts the number of inversions between the two rank lists normalized by the number of possible pairs of systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset description</head><p>To ensure a wide coverage of NLG we focus on four different problems i.e., dialogue generation (using PersonaChat (PC) and TopicalChat (TC) <ref type="bibr" target="#b14">(Mehri and Eskenazi, 2020)</ref>), image description (relying on FLICKR <ref type="bibr" target="#b28">(Young et al., 2014)</ref>), summary evaluation (via TAC08 (Dang and <ref type="bibr" target="#b6">Owczarzak, 2008)</ref>, TAC10, TAC11 <ref type="bibr" target="#b19">(Owczarzak and Dang, 2011)</ref>, RSUM <ref type="bibr" target="#b2">(Bhandari et al., 2020)</ref>  Figure <ref type="figure">2</ref>: Complementarity: For each dataset, the pairwise complementarity between each pair of metrics as computed by Eq. 3 both human and automatic. In these matrix plot, symmetric by design, we ordered metrics to have the human one first and the automatic ones after, the red lines trace the limit between humans and AEM. <ref type="bibr" target="#b15">(Ng and Abrecht, 2015)</ref>), BERTScore <ref type="bibr" target="#b29">(Zhang et al., 2019)</ref>, MoverScore <ref type="bibr" target="#b30">(Zhao et al., 2019)</ref>. For MLQE we solely consider several version of BERTScore, MoverScore and ContrastScore. The human evalutions criterion are specific to each dataset and will be identified by starting with an H:. Overall, our final datasets gather over 270k scores.<ref type="foot" target="#foot_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Finding 1: Automatic metrics are similar to each other much more than they are to human metric. In Fig. <ref type="figure">2</ref>, we report the pairwise complementarity between each pair of metrics as computed by Eq. 3 for both human and AEM. When aggregated over pairs and over datasets, we obtain an average complementarity between: (i) two human metrics of .16 ± .01, (ii) two AEM of .20 ± .01 and (iii) a human and an automatic metric of .35 ± .02. Importantly, we observe across datasets low complementarity, i.e., strong similarity, between AEM, low complementarity between human metrics but high complementarity, i.e., low similarity, between automatic and human metrics.</p><formula xml:id="formula_5">D i a l o g -P C D i a l o g -T C S U M -E V A L T A C -0 8 T A C -0 9 T A C -1 1 0.0 0.</formula><p>We draw two conclusions from this analysis: (i) AEM rank systems similarly but (ii) differently than humans. There is some nuances across datasets. The effect described above is particularly strong in the Dialog, MLQE and SUM-Eval datasets. In particular, we notice that TAC datasets, from the summarization task, have lower complementarity in general, meaning that all metrics, human and automatic, are more similar. Indeed, a lot of works have relied on these datasets to develop new metrics. The more recent REAL-SUM and SUM-Eval reveal much lower metric similarity.</p><p>Finding 2: Automatic metrics even all combined do not explain human metrics. If AEM are rather different than human metrics, we might wonder whether it is possible to get a good approximation of human judgments by combining existing AEM together. To account for possible correlations, we rely on XGBoost regressors with 5-fold crossvalidation to predict human judgments. The training is performed on three different features space: (i) AEM only, (ii) other human metrics only and (iii) both sets of metrics combined. We compute the Kendall's τ between predictions and ground truths and report the results in Fig. <ref type="figure">3</ref>. The plot confirms that AEM struggle to capture human judgment subtlety: correlation rarely exceeds .4 on held-out data. In contrast, human metrics are much more predictive of each others, even if they are often supposed to capture different concepts. Finally, it is worth noting that adding AEM to hu-man ones do not marginally improve the prediction power. These findings cast shadows over recent progress in the field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Our analysis reveals that studied automatic metrics are not complementary, and recent automatic metrics actually capture the same properties of human judgments as older ones. Furthermore, the studied metrics are not strong predictors of human judgments. Quite surprisingly, other human metrics which are often designed to be independent of each other end-up being more predictive of each other than automatic metrics. This predictability of human metrics from one another can be explained due to the available datasets: when a system is good at extracting content, it is also often good at making the content readable, when a system is bad it is often bad across the board in all human metrics. However, the fact the considered automatic metrics are less predictive than other human dimensions casts some shadow over recent progress in the field. It shows that the current strategy of crafting metrics with slightly better correlation than baselines with one of the human metrics has reached its limit and some qualitative change would be needed.A promising strategy to address the limitations of automatic metrics is to report several of them, hoping that they will together give a more robust overview of system performance. However, this makes sense only if automatic metrics measure different aspects of human judgments, i.e., if they are complementary. In this work, we have seen that metrics are in fact not complementary, as they produce similar rankings of systems.</p><p>Proposition for future work To foster meaningful progress in the field of automatic evaluation, we propose that future research craft new metrics not only to maximize correlation with human judgments but also to minimize the similarity with the body of existing automatic metrics. This would ensure that the field progresses as whole by focusing on capturing aspects of human judgments that are not already captured by existing metrics. Furthermore, the reporting of several metrics that have been demonstrated to be complementary could become again a valid heuristic to get a robust overview of model performance. In practice, researchers could re-use our code and analysis to enforce complementarity by, for example, enforcing new metrics to have low complementarity as measured by Eq. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations</head><p>Even though we have considered a representative set of automatic evaluation metrics, new ones are constantly introduced and could be added to such an analysis. Similarly, new datasets could be added to the analysis and impact the results. In an effort to make our findings relevant in the long run, we release an easy-to-use code base to replicate our analysis with new metrics and datasets.</p><p>Like the majority of analysis on automatic evaluation metrics, ours rely on the assumption that human judgments are valid and meaningful. However, some works have questioned the quality of human judgments in standard datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Correlation with humans over time considering all existing metrics combined. On the x-axis: evaluation metrics ordered by their release time; y-axis: utterance-level Kendall's τ with human when training a model to fit human judgments with all metrics available at the time (5-Fold cross-validation with XGBoost regressor). The dotted lines represent different human annotations and datasets. Different variants of the same metrics (like ROUGE-1 and ROUGE-2) are averaged. The datasets and metrics are described in Sec. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Human metrics are significantly more predictive of each other than AEM. On this plot, we report the 5-fold cross-validated result of fitting an XG-Boost regressor on various feature sets: (i) all available AEM, (ii) other human metrics when available, and (iii) both automatic and human metrics. The fit is measured as the average instance-level correlation in the test set.</figDesc><table><row><cell></cell><cell></cell><cell>Automatic metrics</cell></row><row><cell></cell><cell>0.8</cell><cell>Other human metrics</cell></row><row><cell>Correlation with humans</cell><cell>0.2 0.3 0.4 0.5 0.6 0.7</cell><cell>Both combined</cell></row><row><cell></cell><cell>1</cell><cell></cell></row><row><cell cols="2">Figure 3:</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The selection of these metrics was driven by their widespread usage and recognition, as reported in numerous research papers. In order to streamline our analysis and address practical considerations, we opted to exclude recent metrics, such as those based on GPT-3/4, due to their expensive evaluation requirements on large benchmarks and reliance on proprietary models with undisclosed datasets.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Experiments with kemeny ranking: What works when?</title>
		<author>
			<persName><forename type="first">Alnur</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marina</forename><surname>Meilȃ</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Social Sciences</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="40" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The computational difficulty of manipulating an election</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">J</forename><surname>Bartholdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><forename type="middle">A</forename><surname>Tovey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Trick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Choice and Welfare</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="227" to="241" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Re-evaluating evaluation in text summarization</title>
		<author>
			<persName><forename type="first">Manik</forename><surname>Bhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Gour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atabak</forename><surname>Ashfaq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast unfolding of communities in large networks</title>
		<author>
			<persName><forename type="first">Jean-Loup</forename><surname>Vincent D Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renaud</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Lambiotte</surname></persName>
		</author>
		<author>
			<persName><surname>Lefebvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of statistical mechanics: theory and experiment</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page">10008</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Noiry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekhine</forename><surname>Irurozki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stéphan</forename><surname>Clémençon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03799</idno>
		<title level="m">What are the best systems? new perspectives on nlp benchmarking</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ordering by weighted number of wins gives a good ranking for weighted tournaments</title>
		<author>
			<persName><forename type="first">Don</forename><surname>Coppersmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Fleischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventeenth annual ACM-SIAM symposium on Discrete algorithm</title>
		<meeting>the seventeenth annual ACM-SIAM symposium on Discrete algorithm</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="776" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Overview of the tac 2008 update summarization task</title>
		<author>
			<persName><forename type="first">Trang</forename><surname>Hoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karolina</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><surname>Owczarzak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>In TAC</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rank aggregation methods for the web</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moni</forename><surname>Naor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandapani</forename><surname>Sivakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international conference on World Wide Web</title>
		<meeting>the 10th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="613" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Summeval: Re-evaluating summarization evaluation</title>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Alexander R Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Kryściński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Mc-Cann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="391" to="409" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Re-evaluating Automatic Summarization with BLEU and 192 Shades of ROUGE</title>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1013</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="128" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Principal component analysis: a review and recent developments</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Jolliffe</surname></persName>
		</author>
		<author>
			<persName><surname>Cadima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</title>
		<imprint>
			<biblScope unit="volume">374</biblScope>
			<biblScope unit="page">20150202</biblScope>
			<date type="published" when="2016">2016. 2065</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mathematics without numbers</title>
		<author>
			<persName><forename type="first">G</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><surname>Kemeny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Daedalus</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="577" to="591" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An information-theoretic approach to automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Main Conference</title>
		<meeting>the Human Language Technology Conference of the NAACL, Main Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="463" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Usr: An unsupervised and reference free evaluation metric for dialog generation</title>
		<author>
			<persName><forename type="first">Shikib</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00456</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Jun-Ping</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktoria</forename><surname>Abrecht</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06034</idno>
		<title level="m">Better summarization evaluation with word embeddings for rouge</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Why we need new evaluation metrics for NLG</title>
		<author>
			<persName><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><forename type="middle">Cercas</forename><surname>Curry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1238</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2241" to="2252" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">RankME: Reliable human ratings for natural language generation</title>
		<author>
			<persName><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2012</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An Assessment of the Accuracy of Automatic Evaluation in Summarization</title>
		<author>
			<persName><forename type="first">Karolina</forename><surname>Owczarzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoa</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Evaluation Metrics and System Comparison for Automatic Summarization</title>
		<meeting>Workshop on Evaluation Metrics and System Comparison for Automatic Summarization</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Overview of the tac 2011 summarization track: Guided task and aesop task</title>
		<author>
			<persName><forename type="first">Karolina</forename><surname>Owczarzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Text Analysis Conference (TAC 2011)</title>
		<meeting>the Text Analysis Conference (TAC 2011)<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11">2011. November</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th ACL</title>
		<meeting>the 40th ACL<address><addrLine>Philadelphia, Pennsylvania, USA. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to score system summaries for better content selection evaluation</title>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Peyrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teresa</forename><surname>Botschen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on New Frontiers in Summarization</title>
		<meeting>the Workshop on New Frontiers in Summarization</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Peyrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Eger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>West</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.10746</idno>
		<title level="m">Better than average: Paired evaluation of nlp systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">chrf++: words helping character n-grams</title>
		<author>
			<persName><forename type="first">Maja</forename><surname>Popović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second WMT</title>
		<meeting>the second WMT</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="612" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08771</idno>
		<title level="m">A call for clarity in reporting bleu scores</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">An exploratory analysis of multilingual word-level quality estimation with cross-lingual transformers</title>
		<author>
			<persName><forename type="first">Tharindu</forename><surname>Ranasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantin</forename><surname>Orasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Mitkov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00143</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</title>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">A</forename><surname>Rankel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Hoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="131" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A consistent extension of condorcet&apos;s election principle</title>
		<author>
			<persName><forename type="first">Peyton</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Levenglick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="285" to="300" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<title level="m">Bertscore: Evaluating text generation with bert</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Peyrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><forename type="middle">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Eger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
