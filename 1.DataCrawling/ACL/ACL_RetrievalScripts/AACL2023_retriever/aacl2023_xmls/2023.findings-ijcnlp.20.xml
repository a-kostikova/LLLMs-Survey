<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Machine Reading Comprehension through A Simple Masked-Training Scheme</title>
				<funder ref="#_YMembQA #_FEYNTEE">
					<orgName type="full">Australian Research Council Linkage Project</orgName>
				</funder>
				<funder ref="#_HHAvTaM">
					<orgName type="full">Australian Research Council Discovery Project</orgName>
				</funder>
				<funder>
					<orgName type="full">University of Wollongong</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xun</forename><surname>Yao</surname></persName>
							<email>yaoxun@wtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Computing and Information Technology</orgName>
								<orgName type="department" key="dep3">Faculty of Information Technology</orgName>
								<orgName type="institution" key="instit1">Wuhan Textile University</orgName>
								<orgName type="institution" key="instit2">University of Wollongong</orgName>
								<orgName type="institution" key="instit3">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junlong</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Computing and Information Technology</orgName>
								<orgName type="department" key="dep3">Faculty of Information Technology</orgName>
								<orgName type="institution" key="instit1">Wuhan Textile University</orgName>
								<orgName type="institution" key="instit2">University of Wollongong</orgName>
								<orgName type="institution" key="instit3">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinrong</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Computing and Information Technology</orgName>
								<orgName type="department" key="dep3">Faculty of Information Technology</orgName>
								<orgName type="institution" key="instit1">Wuhan Textile University</orgName>
								<orgName type="institution" key="instit2">University of Wollongong</orgName>
								<orgName type="institution" key="instit3">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
							<email>jiey@uow.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Computing and Information Technology</orgName>
								<orgName type="department" key="dep3">Faculty of Information Technology</orgName>
								<orgName type="institution" key="instit1">Wuhan Textile University</orgName>
								<orgName type="institution" key="instit2">University of Wollongong</orgName>
								<orgName type="institution" key="instit3">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
							<email>yuanfang.li@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Computing and Information Technology</orgName>
								<orgName type="department" key="dep3">Faculty of Information Technology</orgName>
								<orgName type="institution" key="instit1">Wuhan Textile University</orgName>
								<orgName type="institution" key="instit2">University of Wollongong</orgName>
								<orgName type="institution" key="instit3">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lisbon</forename><surname>Oceanarium</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Computing and Information Technology</orgName>
								<orgName type="department" key="dep3">Faculty of Information Technology</orgName>
								<orgName type="institution" key="instit1">Wuhan Textile University</orgName>
								<orgName type="institution" key="instit2">University of Wollongong</orgName>
								<orgName type="institution" key="instit3">Monash University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Machine Reading Comprehension through A Simple Masked-Training Scheme</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">122F69BE63327477BDDC8FF4B5A60E3F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Extractive Question Answering (EQA) is a fundamental problem in Natural Language Understanding, aiming at answering given questions via extracting a contiguous sequence or span of words from a passage. Recent work on EQA has achieved promising performance with the help of pre-trained language models, for which Masked Language Modeling (MLM) is usually adopted as a pre-training task to predict masked tokens. This paper revisits MLM and proposes a simple yet effective method to improve the EQA performance, termed the [Mask]-for-Answering method (M4A). Specifically, three masking strategies are first introduced, which produce masked copies of the original passages. Instead of predicting masked tokens as in MLM, both original samples and masked copies are utilized simultaneously for training the EQA model. Importantly, a discrepancy loss is further incorporated to ensure that masked copies remain semantically close to the originals. As such, M4A is able to produce robust embeddings for both original and masked samples and infer correct answers even with masked context. Experimental study on several highly-competitive benchmarks consistently demonstrates the superiority of our proposed method over existing methods. M4A also achieves strong performance in low-resource settings and out-of-domain generalization. * corresponding author Limit Context: Portugal has the largest aquarium in Europe, the Lisbon Oceanarium, and the Portuguese have several other notable organizations focused on science-related exhibits and divulgation, like the state agency CiÃªncia Viva, a programme of the Portuguese Ministry of Science and Technology to the promotion of a scientific and technological culture among the Portuguese population, the Science Museum of the University of Coimbra, the National Museum of Natural History at the University of Lisbon, and the Visionarium.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Extractive Question Answering (EQA), a fundamental task for Natural Language Understanding, refers to the process of identifying the answer span (a sequence of continuous words) over the given question and passage. Past years have witnessed a dramatically increasing interest in applications of Pre-trained Language Models (PLMs) for EQA, where PLMs are usually adopted as encoders to form contextual/semantic embeddings for the Figure <ref type="figure">1</ref>: An illustration case from SQuAD (1.1) <ref type="bibr" target="#b2">(Fisch et al., 2019)</ref>, using the proposed [Mask]-for-Answering algorithm (M4A) to infer answer(s) from samples with limit (unmasked) context. question-passage pair. Abundant evidences indicate that the strong encoding capability of PLMs has rapidly advanced the EQA performance, compared to traditional word embedding methods, such as GloVe and Word2Vec <ref type="bibr" target="#b1">(Devlin et al., 2019;</ref><ref type="bibr" target="#b4">Joshi et al., 2020;</ref><ref type="bibr" target="#b8">Liu et al., 2019)</ref>. Recently, how humans approaching reading comprehension becomes a major source of inspiration for enhancing EQA. There is a rich literature to incorporate the human-like reading comprehension strategy with PLMs and achieve remarkable success beyond the vanilla models, as later shown in Section 2.</p><p>Notably, when experienced human readers perform question reasoning, they could infer the correct answer using only few sentences (even some parts of one sentence), instead of the entire passage <ref type="bibr" target="#b10">(Paris et al., 1983;</ref><ref type="bibr" target="#b17">Yu et al., 2017)</ref>. As illustrated in Fig. <ref type="figure">1</ref>, even with the limit (unmasked) contexts, one could still predict the correct answer of "Lisbon Oceanarium" to the given question (of "the name of the largest European aquarium"). The observation of humans approaching reading comprehension with limit (unmasked) contexts is the major source of inspiration for this paper.</p><p>Accordingly, we propose a simple yet effective mask-training scheme, termed [Mask]-for-Answering (M4A). Specifically, M4A introduces three masking strategies to substitute non-answer tokens from the original training passage with [Mask] tokens. Additionally, semantic similarity is utilized to maintain the semantic closeness between masked samples and originals. The effectiveness of our training scheme can be intuitively explained from two perspectives: (1) providing strong training signals by strategically masking out potentially-irrelevant contents (non-answer tokens) and (2) data augmentation by simply perturbing the original training samples without additional annotations.</p><p>Our method differs from existing methods in the following perspectives. (1) The <ref type="bibr">[Mask]</ref> token is traditionally utilized to hold out a portion of the input tokens in pre-training PLMs to predict missing tokens <ref type="bibr" target="#b19">(Ãzkan Tan et al., 2023;</ref><ref type="bibr" target="#b16">Yang et al., 2023)</ref>. Several studies leverage <ref type="bibr">[Mask]</ref> to produce pseudo passage-question pairs <ref type="bibr" target="#b11">(Ram et al., 2021;</ref><ref type="bibr" target="#b0">Bian et al., 2021)</ref>, which is limited by parts of speech (POS) of masked tokens (usually nouns) and the objective is still for predicting masked tokens (the in-domain pre-training). In contrast, M4A directly employs samples with masked tokens in directly optimizing the downstream task objective. Additionally, in M4A non-answer tokens can be masked regardless their POS. (2) Masked samples also play a role of data augmentation, and existing augmentation methods either replace words with synonyms <ref type="bibr" target="#b9">(Ng et al., 2020)</ref> or perturb input embeddings <ref type="bibr" target="#b6">(Lee et al., 2021)</ref>. The former is limited by the set of available synonyms, while our method is independent from synonyms. The latter adds noise on the embedding level (for all tokens) with a prior assumption of a multivariate Gaussian distribution. In contrast, M4A performs masking on the token level, and ensures (ground-truth) answer tokens unmasked.</p><p>The main contributions of our proposed work are summarized as follows:</p><formula xml:id="formula_0">â¢ A novel [Mask]-for-Answering method is</formula><p>proposed to produce robust features and incorporate human comprehension skills to infer answers from samples with masked tokens. â¢ Three masking strategies are introduced to produce masked samples, that are trained simultaneously with original inputs. â¢ The semantic similarity between original-andmasked pairs is applied to minimize the noise conveyed in masked samples. â¢ Empirically, our proposed M4A method outperforms recent strong baselines on six standard benchmarks. Intensive ablation studies are also conducted to understand the impact from masking strategies and ratios. Moreover, M4A also demonstrates a strong generalizability in the low-resource training and zero-shot domain adaptation setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>The Extractive Question Answering (EQA) task requires a model to learn informative representation from the context passage, and return a span (continuous words from this passage) that matches the given question. Usually, Pre-trained Language Models (PLMs) are adopted as the encoder to estimate embeddings for the pair of question-passage, which is followed by a decision layer (i.e., two binary classifiers to identify the start and end position of the answer span respectively). Due to the capability of forming semantic representation for input questions and passages, PLMs have significantly advanced the EQA frontier <ref type="bibr" target="#b1">(Devlin et al., 2019;</ref><ref type="bibr" target="#b4">Joshi et al., 2020;</ref><ref type="bibr" target="#b7">Liu et al., 2022)</ref>.</p><p>Inspired by the remarkable success of PLMs, a variety of improvement methods have been proposed to further enhance PLMs with human-reading strategies. A block based attention method is proposed in <ref type="bibr" target="#b12">(Seonwoo et al., 2020)</ref>, which predicts highlyrelevant context about answers. Another similar work is found in <ref type="bibr" target="#b3">(Guan, 2022)</ref>, that introduces the Block-Skim strategy to identify and skim irrelevant context blocks by utilizing CNN to for EQA. In <ref type="bibr" target="#b13">Sun et al. (2019)</ref>, different-level attention mechanisms are implemented to simulate the process of back-and-forth reading, highlighting, and selfassessment, while <ref type="bibr" target="#b18">Zhang et al. (2020)</ref> considers the reading strategy for multi-round reasoning phrases of reading-attending-excluding, that is, through initial scan reading, followed by attended intensive reading, and concluding with answer exclusion.</p><p>In addition to attention-based work, a few studies employed the data-augmentation strategy to fine-tune the EQA task, including synthetic question-answer generation, external knowledge, and input perturbation. As an example, the work from <ref type="bibr" target="#b11">(Ram et al., 2021;</ref><ref type="bibr" target="#b0">Bian et al., 2021)</ref> produces synthetic pairs by masking specific words from the passage and training a model to answer questions related to those masked words. Yet, generation-based methods usually suffer from costly computational resource and have limitations on masked words (usually nouns) and question types (mainly cloze-like). KALA <ref type="bibr" target="#b5">(Kang et al., 2022)</ref> is proposed to integrate the contextual representation of intermediate PLM layers with related entity and relational representations (from the external Knowledge Graph). With knowledge-augmented representations, KALA improves the performance of the vanilla PLM on various EQA tasks. Additionally, input perturbation is also considered via the word deletion or synonym replacement <ref type="bibr" target="#b15">(Wei and Zou, 2019)</ref>. SSMBA <ref type="bibr" target="#b9">(Ng et al., 2020)</ref> corrupts input sequences via substituting existing tokens with [Mask] and then reconstructing them. SWEP <ref type="bibr" target="#b6">(Lee et al., 2021)</ref> directly perturbs input embeddings via adjustable Gaussian noises. Using both original and perturbed samples, models are trained to extract token representations to facilitate the subsequent answering task. By comparison, our proposed method neither recovers masked tokens nor perturbs input embeddings. Instead, augmented samples (with [Mask] tokens) are directly utilized for the model training while ensuring their semantic similarity with originals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed method</head><p>The proposed [Mask]-for-Answering (M4A) algorithm is detailed in this section. It consists of three main modules: source training, <ref type="bibr">[Mask]</ref> training, and semantic alignment. Specifically, the second module enables the model to answer questions with incomplete (masked) contents, while the third module ensures semantic correlations between original and masked samples (shown in Fig <ref type="figure" target="#fig_0">2</ref>). Notably, the [Mask] training module is not utilized during the inference stage, as our ultimate aim is to fine-tune a resilient encoder that can handle masked samples and generate robust features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Source training</head><p>Given the input pair of question (q) and passage (p), the EQA task aims to identify the start and end positions of the correct answer span (a s/e ) from p.</p><p>Specifically, the input of EQA is a tokenized sequence, X =</p><formula xml:id="formula_1">[CLS]q 1 q 2 â¢â¢â¢ q |q| [SEP]p 1 p 2 â¢â¢â¢ p |p| [SEP],</formula><p>where q i and p j represents the i-th and j-th token from q and p, respectively. Then the encoder (F, usually a PLM such as BERT <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref>) is applied to induce the following probability distribution:</p><formula xml:id="formula_2">p(p i = a s/e ) â exp(MLP s/e (F(p i ))) |p| j exp(MLP s/e (F(p j )))</formula><p>,</p><p>where F(p i ) represents the extracted feature of p i , and MLP s/e represents a multilayer perceptron network (usually with one-hidden layer) for predicting the start and end of the answer span, respectively. Accordingly, the loss function for EQA is defined as follows:</p><formula xml:id="formula_4">L EQA â - |p| i 1(p i = a s/e )logp(p i = a s/e ),<label>(2)</label></formula><p>where 1(â¢) is the indicator function that returns 1 if the condition is true and returns 0 otherwise. Overall, the source training module is to minimize the L EQA loss using labeled original samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">[Mask] training</head><p>The traditional mask-based training (or maskedlanguage modeling (MLM)) aims to predict a set fraction of [Mask] tokens given the remaining unmasked text. This fraction is defined as the masking budget (b M ), and tokens for masking are chosen by a uniform (or random) sampling until b M is met. This previous mask-and-predict task is different from ours. In contrast, samples with masked tokens are utilized for training the EQA model directly, while the mask-prediction task is less emphasized.</p><p>Given the tokenized input X, this module substitutes tokens from the passage p (the second part of X) with [Mask] to generate the masked copy X M . * Specifically, three masking strategies, Gaussian, U-shaped, and Uniform, are adopted, which are differentiated by the distributions of masked tokens. A reference index r is introduced (r â [1,|p|]), where r is explicitly determined by the index of the start token from the ground-truth answer span. Then, Gaussian favors masking tokens around r, while U-shaped differs from Gaussian by masking more away from r (towards the begin or end of p). Uniform masks tokens with a uniform probability (similar to the MLM). Note that for all three masking strategies, tokens belonging to the ground-truth answer span are assigned with zero masking probability (never masked), and only non-answer tokens can be masked.</p><p>To estimate the masking rate for each token within the passage, probability density functions (PDFs) under different masking strategies are firstly introduced. Assume that PDFs are defined within a range of [-â, â] (where â &gt; 0 is a hyper-parameter). Followed by the standard Gaussian distribution N (0, 1), the PDF for the Gaussian masking is accordingly defined as</p><formula xml:id="formula_5">Gau(x) = e -x 2 2 â 2Ï , -â â¤ x â¤ â.</formula><p>Similarly the PDF for the U-shaped distribution is defined as:</p><formula xml:id="formula_6">Ush(x) = Gau(x-â) if 0 â¤ x â¤ â Gau(x+â) if -â â¤ x &lt; 0 .</formula><p>To be consistent with Gaussian and U-shaped, the PDF for Uniform is simply given by</p><formula xml:id="formula_7">Uni(x) = 1 â-(-â) = 1 2Ãâ , -â â¤ x â¤ â.</formula><p>Next, given the reference index r, the following mapping function is adopted to project the t-th token into this range of [-â,â] by:</p><formula xml:id="formula_8">Map(t) = â(t-r) max(r,|X|-r) .<label>(3)</label></formula><p>At last, given the masking budget (the fraction of masked tokens) b M , the masking probability for this t-th token is further given by:</p><formula xml:id="formula_9">p(t) = b m Ã|X|Ã f (Map(t)) |X| i f (Map(i)) ,<label>(4)</label></formula><p>where f (â¢) represents Gau(â¢), Ush(â¢), and Uni(â¢) for the case of Gaussian, U-shaped, and Uniform, respectively. An illustrating example of the three masking strategies is provided in Fig. <ref type="figure" target="#fig_1">3</ref>. At last, masked passages are combined with the given question to form X M , and the resultant loss L [Mask]  EQA is obtained by replacing X with X M in Eq. (2). Presumably, the ground-truth answer token is with the 7 th index. Specifically, Gaussian prefers to mask tokens around the reference index of r=7, while more tokens from the begin and end are masked in the U-shaped strategy. By contrast, Uniform marks tokens with the equally distributed way. Notably, ground-truth answer tokens will never be masked across three cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Semantic alignment</head><p>The <ref type="bibr">[Mask]</ref> training module enables the model to answer questions with masked samples X M , which could introduce noise from masked tokens. To reduce the impact of noises, we design the semantic alignment module to suppress noisy signals conveyed in X M , and enforce the semantics of original inputs is preserved even after masking parts of the passage. This is done by minimizing the difference between the distribution of individual-token score (i.e., the probability of being correct-answer tokens) obtained from X and that of the corresponding X M .</p><p>To begin with, let C and C M represent feature embedding of original and masked samples, i.e., C = F(X) and C M = F(X M ), where C/C M â R |X|Ãl and l is the hidden dimension. Given two classifiers for identifying the start/end token (MLP s/e ) from Eq. ( <ref type="formula" target="#formula_3">1</ref>), the score distribution of individual tokens from C and C M is estimated by:</p><formula xml:id="formula_10">d s/e C â softmax(MLP s/e (C)) d s/e C M â softmax(MLP s/e (C M )),<label>(5)</label></formula><p>The Jensen-Shannon divergence (D JS ) is then employed to measure the distribution similarity with the following objective:</p><formula xml:id="formula_11">L ALI â D JS (d s C ,d s C M )+D JS (d e C ,d e C M ). (6)</formula><p>As such, this loss minimization encourages F to produce semantically similar representations between the original input X and its masked version X M . Note that this alignment loss is different from that of <ref type="bibr">[Mask]</ref> training, as it does not require supervision signals, but only reduces the token-score difference obtained by X and X M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Overall objective function</head><p>In summary, in the proposed M4A, samples with masked tokens are trained simultaneously with originals. Additionally, the semantic alignment loss further ensures masked samples remaining semantically close to original ones. The following joint loss is utilized:</p><formula xml:id="formula_12">L = 1 2 L EQA + 1 2 L [Mask] EQA +Î»L ALI , (<label>7</label></formula><formula xml:id="formula_13">)</formula><p>where Î» is a penalty regularizer. During the inference, the [Mask] training and semantic alignment modules are discarded, and testing samples follow the traditional steps to extract latent representation via the trained encoder (F), before applying the classifiers (MLP s/e ) to identify the start and end position of answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Experiments and analysis are carried on a collection of high-competitive EQA tasks. Specifically, six benchmarking datasets from MRQA 2019 <ref type="bibr" target="#b2">(Fisch et al., 2019)</ref> are employed, including SQuAD (1.1), HotpotQA, NewsQA, NaturalQ, TriviaQA, and SearchQA. Their statistics are provided in Appendix A.1.</p><p>The RoBERTa-base model <ref type="bibr" target="#b8">(Liu et al., 2019</ref>) is adopted as the encoder. In addition, with both Gaussian and U-shaped masking, the reference index r is set as the beginning location of the ground-truth answer token (from training samples), while ground-truth answer tokens are assigned with zero masking probability (never masked). The hyper-parameter is set as â = 2 for Eq. (3). More training details are provided in Appendix A.2. The F1-evaluation metric, measured by the number of overlapping tokens between the predicted and ground-truth answers, is adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main results</head><p>Our proposed method is compared with the following models. We re-implement them using provided source codes and results are competing with the reported.</p><p>â¢ Base <ref type="bibr" target="#b8">(Liu et al., 2019)</ref> is implemented using the pre-trained vanilla model (RoBERTa-base) and fine-tuned as described in Section 3.1;</p><p>â¢ BLANC <ref type="bibr" target="#b12">(Seonwoo et al., 2020)</ref> applies a block-attention strategy to predict answers and supporting contexts (spans surrounding around answers) simultaneously;</p><p>â¢ SSMBA <ref type="bibr" target="#b9">(Ng et al., 2020)</ref> randomly substitutes tokens with [Mask] and recovers them to produce new samples for data augmentation;</p><p>â¢ SWEP <ref type="bibr" target="#b6">(Lee et al., 2021)</ref> augments the data by perturbing the input embedding with an adjustable Gaussian noise;</p><p>â¢ KALA <ref type="bibr" target="#b5">(Kang et al., 2022)</ref> augments the original contextual representation using related entity and relational representation from the external Knowledge Graph.</p><p>For M4A we set Î» = 0.5 (the ablation study of Î» is provided later), the masking budget (or the the fraction of masked tokens) as b m = 20% with the U-shaped masking strategy (the impact of different masking strategies are also offered in the ablation study). The comparison results in terms of the mean value and standard deviation over 10 runs are shown in Table <ref type="table" target="#tab_0">1</ref>, in which the best result for each dataset is bolded. M4A consistently improves existing models across employed EQA tasks. For instance, the strongest baseline SWEP model achieves an approximately 0.8 absolute-point improvement compared to the vanilla model, while our M4A model enhance a further 1.1 absolute point over SWEP. Thus, it amounts to a comparable improvement and demonstrates the superiority of the proposed masking approach.</p><p>Notably, in terms of the computational complexity, the proposed algorithm has a same scale of model-training parameters as the vanilla model (RoBERTa-base). Specifically, M4A reuses two MLPs (MLP s/e for classifying the start/end answer token from Eq. ( <ref type="formula" target="#formula_3">1</ref>)) in the <ref type="bibr">[Mask]</ref> training and semantic alignment modules (Eq. ( <ref type="formula" target="#formula_10">5</ref>)). As such, the proposed method does not require extra model parameters. Additionally, during the inference, both the [Mask] training and semantic alignment modules are discarded as M4A only requires the trained encoder. As an example, M4A needs only 0.02 seconds per question on the SQuAD testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation study</head><p>On the encoder flexibility. To start with, we evaluate the impact from the fundamental encoder towards the proposed M4A. Specifically, the BERTbase encoder <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref> is implemented as the Base, and the strongest baseline SWEP from Table <ref type="table" target="#tab_0">1</ref> is also employed for comparison purposes. Most of the experimental settings, such as the batch size and the sequence length, are in line with the above RoBERTa-base model, except the learning rate is fixed as 3e -5 and the training epoch is 2 (consistent with SWEP). The comparison results over 10 runs are illustrated in Fig. <ref type="figure" target="#fig_2">4</ref>, and our M4A method achieves the best F1 score across all datasets, outperforming SWEP. This experiment provides further evidence for M4A's stability on the underlying encoder: it outperforms the current best model SWEP on both RoBERTa and BERT as the encoder. Without explicitly mentioning, the following ablation studies are conducted using the RoBERTa-base encoder.</p><p>On the masking strategy. The following ablation experiments are performed using SQuAD to evaluate the proposed masking strategies, i.e., Gaussian, U-shaped, and Uniform, whose main difference lies in the distribution of masked tokens. Given Î» = 0.5, â = 2, and b m = 20%, the reference index r, again, for Gaussian and U-shaped is set as the start location of the ground-truth answer token(s), respectively. As for Uniform, the token masking probability is set as b m . Table <ref type="table" target="#tab_1">2</ref> compares the three proposed masking strategies, all of which obtain the similar, and higher accuracy than the Base model. The result clearly demonstrates the effectiveness of M4A to utilize masked samples for solving EQA. In addition, the U-shaped masking strategy, which masks less tokens around the ground-truth answer(s) but more towards the begin/end of the entire passage, performs the best on the SQuAD dataset. This finding is consistent with BLANC <ref type="bibr" target="#b12">(Seonwoo et al., 2020)</ref>, from which supporting contexts (spans surrounding around answers) are highlighted via a predetermined soft label. In contrast, our method encourages to remove (via masking) tokens that are far away from answer tokens. Accordingly, the Ushaped masking strategy is adopted for all following experiments. However, we need to point out that the differences among these three masking distributions are relatively moderate (e.g., Â±0.25 F1). The contributor to this effectiveness is explored later on the module breakdown part.</p><p>On the masking budget. The impact from the masking budget (or the the fraction of masked tokens) b m is validated hereafter. Obviously, more tokens are masked out with a higher value of b m , which also leads to more noisy samples (due to the increase of masked or missing tokens). Specifically, experiments are conduced by varying b m from 10% to 100% on the SQuAD dataset. Notably, with b M =100%, all tokens except ground-truth ones are removed.</p><p>10% 20% 30% 40% 50% 60% 70% 80% 90% 100% The comparison is shown in Fig 5 <ref type="figure">,</ref> and<ref type="figure">results</ref> show that the proposed method is relatively insensitive to b m . For instance, with SQuAD the highest performance is observed as 92.2Â±0.1 (b m = 20%), which decreases to 90.7Â±0.2 (b m = 100%). Surprisingly, even with b M =100%, M4A still obtains a slightly better performance than the Base model (90.3 from RoBERTa-base), which shows the lower bound of M4A is the vanilla model. That is, given the very incomplete (masked) passages, M4A is still capable of identifying correct answers, which empirically confirms the robustness and stability of our proposed masking strategy. Furthermore, we argue that such a stable performance (with high masking rate) is a result of the semantic alignment. In the extreme case of b M =100%, for instance, only ground-truth tokens are maintained in the masked sample, and presumably the contribution from the [Mask] training loss becomes limited; yet, the semantic alignment loss enforces the encoder to produce robust features for sample with/out masking masking. We further investigate this hypothesis in the model breakdown study below.</p><p>On the module breakdown. The following experiment examines the effectiveness of M4A from aspects of masked training and semantic alignment. Specifically, the comparison is considered with the following variants: Base represents the model trained using only original samples (w.r.t. the first term of Eq. ( <ref type="formula" target="#formula_12">7</ref>)); Mask only employs U-shaped masking samples for the model training (w.r.t. the second term of Eq. ( <ref type="formula" target="#formula_12">7</ref>)); and Alignment maximizes the semantic similarity between the originalmasked pairs (w.r.t. the last term of Eq. ( <ref type="formula" target="#formula_12">7</ref>)). Table <ref type="table" target="#tab_2">3</ref> shows contributions from individual modules using SQuAD with Î» = 0.5 and b m = 0.2. To begin with, both the proposed masked training and alignment modules stably improve the performance of the Base model. Training with only masked samples, as observed, the Mask variant achieves the worst result; the reason is mainly due to the noise brought by the incomplete (or masked) passage(s). Yet, Base+Mask achieves the accuracy of 90.7 for SQuAD, which shows the benefit of employing masked copies as the data augmentation. Furthermore, Alignment brings a bigger performance boost in comparison with Mask. The former achieves the averaged improvement of 1.2 points on top of Base, while the latter only obtains a boost of 0.4 points. More importantly, that is also evidenced by the second-place score from Base+Alignment, which trains the model without masked samples but only enforces the semantic similarity between the original and masked samples. In other words, with the presence of masked samples, the key contributor to M4A is to restrict their semantic alignment with the original ones (not the way of producing those masked samples, such as the masking budget and the masking strategy). The alignment loss regulates the encoder to generate robust features so to improve the model robustness and performance.</p><p>It is worth noting that, the utilization of the masking model proves essential not only for generating supplementary samples that complement the originals, but also for facilitating the Semantic Alignment module to align these masked samples with the original ones. In other words, masked samples form the foundational element on which the alignment depends, although the key contributor to M4A is the semantic alignment. Yet, the proposed masking strategy is also cost-effective, which stands in stark contrast to existing data augmentation techniques that often demand prior domain knowledge and/or resources.</p><p>On the penalty regularizer Î». On the data augmentation. This section validates the proposed model from the aspect of the low-resource training, as samples with masked tokens could play a role of the data augmentation. Accordingly, only a small amount (say m) of samples (randomly selected from the training set) are utilized for masking and further fine-tuning the model, where m = [20%, 40%, 60%, 80%, 100%] (m = 100% represents the full dataset). Table <ref type="table" target="#tab_4">4</ref> shows the accuracy as a function of the sample size using the SQuAD dataset. Compared to the strongest baseline (SWEP from Table <ref type="table" target="#tab_0">1</ref>), M4A consistently improves the model performance under all percentages of the training samples. With 40% of labeled data, M4A has achieved even higher accuracy than SWEP with 60% samples. Empirically, the result clearly demonstrates the superiority of produced samples with <ref type="bibr">[Mask]</ref> tokens as an effective data augmentation.</p><p>On out-of-domain generalizability. In this experiment, we measure M4A's ability via adapting to unseen domains (i.e., datasets) in a zero-shot manner. Following SSMBA and SWEP, the model is first trained on a single source dataset (SQuAD in this case), and further evaluated on the other two target dataset (i.e., HotpotQA and NaturalQ) without any fine-tuning â  .  Averaged performance from our proposed and other methods is presented in Fig. <ref type="figure" target="#fig_4">7</ref>. The Base method (directly applying the RoBERTa-base model trained from SQuAD) achieves the worst F1 (60.1 on average) on three subsequent target datasets, which reveals the different data distribution among diverse datasets. Therefore, a direct application of the Base model to downstream dataset(s) is ineffective as shown by its low generalizability. Surprisingly, SSMBA achieves even worse results than Base on HotpotQA and NaturalQ. Notably, SSMBA reconstructs masked tokens to produced augmented samples. Yet, new substituted tokens may still cause the semantic drift and lead to poor performance on test sets. In contrast, the average performance of M4A (62.58) is highest across three target datasets. Due to masked samples, M4A is encouraged to produce more robust features, which contributes to the model generalizability to unseen datasets. The comparison indicates that M4A can be regarded as a supplementary pre-training method, along with the traditional Masked Language Modeling (MLM), to offer a robust starting point for fine-tuning unseen datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper proposes a [Mask]-for-Answering algorithm (M4A) to tackle the Extractive Question Answering (EQA) task, via simulating humancomprehension skills to infer answers with limit â  Notably these three datasets share the same Wikipedia domain background, as shown in Table <ref type="table" target="#tab_6">5</ref> from Appendix A.1. context. Specifically, three different masking strategies are introduced to produce masked copies of original samples. Those masked samples are directly utilized for the model training, and regularized by an alignment loss to ensure their semantic similarity with originals. Empirically, M4A achieves statistically better performance than current methods on several benchmark EQA datasets. In addition, M4A also demonstrates the strong capability in the settings of the low-resource training and zero-shot domain adaptation. At last, this masking-then-training strategy, explored by M4A, is also agnostic to downstream tasks, and we will incorporate it with other downstream applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>[Mask] tokens play a critical role, for our proposed M4A, in improving the model inference capability even with masked contents. Yet, the proposed masking strategies mainly focus on the distribution of masked tokens; intuitively, it still cannot be guaranteed that all question-irrelevant contents are masked (or removed). Therefore, masking strategies can be further refined, such as by incorporating external/prior knowledge about passages/questions, masking (stop) words with specific POS, or introducing dependency parsing for identifying trivial words.</p><p>Additionally, regarding the Jensen-Shannon divergence employed in this paper, we acknowledge its crucial role in semantic alignment. However, other methods exist for calculating semantic similarity. We defer exploring these alternatives to future work, as they hold potential to enhance the performance further At last, while current Large Language Models (LLMs), such as GPT-3.5 and Vicuna, achieve impressive achievements across different tasks, including EQA, the proposed method is much more lightweight, based on models that are orders of magnitude smaller (e.g., 110M parameters for BERTbase compared to 7B for Vicuna). Nevertheless, our ablation study highlights the adaptability of our approach as it can be applied to other encoder-decoder language models in a plug-and-play manner. We anticipate enhanced performance by adopting our proposed method using LLMs like Vicuna for EQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Statement</head><p>In terms of reproducibility, we have made the experimental source code available anonymously.</p><p>Additionally, the benchmark datasets used in our study are publicly accessible. However, it is important to acknowledge the potential presence of hidden biases from Pre-trained Language Models, stemming from biased data they were trained on. Despite utilizing these pre-trained language models for encoding, we did not encounter any biased outcomes. Nevertheless, we carefully considered the low-risk nature of our specific domain during the study. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the proposed M4A for EQA. Original inputs are masked to produce additional training samples, while their representation similarity is later maximized to remain the semantic alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Comparison of proposed masking strategies, where b m = 20%, the length of the passage is 20 and â=2. Presumably, the ground-truth answer token is with the 7 th index. Specifically, Gaussian prefers to mask tokens around the reference index of r=7, while more tokens from the begin and end are masked in the U-shaped strategy. By contrast, Uniform marks tokens with the equally distributed way. Notably, ground-truth answer tokens will never be masked across three cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Impact analysis from the underlying encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance comparison as a function of the masking budgets b m .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Transferring the EQA model trained by SQuAD to unseen datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="4">: Comparison among M4A and existing</cell></row><row><cell>methods.</cell><cell cols="3">Specifically, M4A achieves 1.66e -16 ,</cell></row><row><cell cols="4">4.47e -7 , 9.91e -19 , 6.92e -19 , 2.58e -9 , and 5.14e -13</cell></row><row><cell cols="4">for SQuAD, HotpotQA, NewsQA, NaturalQ, TriviaQA</cell></row><row><cell cols="4">and SearchQA, respectively, in terms of the p-values</cell></row><row><cell cols="4">from the T-tests. This statistical significance testing</cell></row><row><cell cols="3">confirms the stability of M4A.</cell><cell></cell></row><row><cell>Model</cell><cell cols="3">SQuAD HotpotQA NewsQA</cell></row><row><cell>Base</cell><cell>90.3Â±0.2</cell><cell>78.7Â±0.3</cell><cell>69.8Â±0.4</cell></row><row><cell cols="2">BLANC 91.1Â±0.2</cell><cell>77.8Â±0.1</cell><cell>70.7Â±0.5</cell></row><row><cell cols="2">SSMBA 90.1Â±0.3</cell><cell>77.3Â±0.4</cell><cell>69.2Â±0.2</cell></row><row><cell>SWEP</cell><cell>91.0Â±0.1</cell><cell>78.8Â±0.1</cell><cell>71.7Â±0.1</cell></row><row><cell>KALA</cell><cell>90.9Â±0.4</cell><cell>77.3Â±0.5</cell><cell>72.7Â±0.3</cell></row><row><cell>M4A</cell><cell>92.2Â±0.1</cell><cell>79.2Â±0.1</cell><cell>72.8Â±0.2</cell></row><row><cell>Model</cell><cell cols="3">NaturalQ TriviaQA SearchQA</cell></row><row><cell>Base</cell><cell>79.6Â±0.2</cell><cell>74.0Â±0.4</cell><cell>81.5Â±0.3</cell></row><row><cell cols="2">BLANC 80.3Â±0.1</cell><cell>75.1Â±0.1</cell><cell>82.6Â±0.1</cell></row><row><cell cols="2">SSMBA 79.8Â±0.2</cell><cell>74.8Â±0.2</cell><cell>82.1Â±0.1</cell></row><row><cell>SWEP</cell><cell>80.2Â±0.3</cell><cell>75.3Â±0.2</cell><cell>82.5Â±0.2</cell></row><row><cell>KALA</cell><cell>80.1Â±0.4</cell><cell>75.5Â±0.3</cell><cell>82.4Â±0.4</cell></row><row><cell>M4A</cell><cell>81.4Â±0.1</cell><cell>76.3Â±0.2</cell><cell>83.5Â±0.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the answering performance achieved by different masking strategies.</figDesc><table><row><cell>Base</cell><cell>Gaussian U-shaped Uniform</cell></row><row><cell cols="2">90.3Â±0.2 91.7Â±0.1 92.2Â±0.1 92.1Â±0.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Examination on individual modules of training with masked copies and aligning original-and-masked semantics.</figDesc><table><row><cell>Base</cell><cell>Mask</cell><cell>Base+Mask</cell></row><row><cell>90.3</cell><cell>90.0</cell><cell>90.7</cell></row><row><cell cols="2">Base+Alignment Mask+Alignment</cell><cell>Full</cell></row><row><cell>91.5</cell><cell>91.0</cell><cell>92.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The model accuracy is also evaluated by varying values of Î» from 0.1 to 1.0. The comparison is shown in Fig6with SQuAD. The result illustrates that the proposed method is robust to Î», as M4A achieves a stable performance for different scenarios (with an average F1 difference of Â±0.35 approximately).</figDesc><table><row><cell>92.4</cell><cell>SQuAD</cell></row><row><cell>92.0 92.2 Performance</cell><cell></cell></row><row><cell>91.8</cell><cell></cell></row><row><cell>91.6</cell><cell>0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Penalty regularizer</cell></row><row><cell cols="2">Figure 6: Performance comparison as a function of</cell></row><row><cell>different Î».</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Averaged performance (measured by F1) obtained by SWEP/M4A under different percentages of training samples.</figDesc><table><row><cell cols="3">SQuAD 20% 40% 60% 80% 100%</cell></row><row><cell>SWEP</cell><cell>88.4 89.6 90.1 90.7</cell><cell>91.0</cell></row><row><cell>M4A</cell><cell>88.6 90.4 91.5 91.9</cell><cell>92.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Employed datasets for the EQA task, where Domain represents the passage resource, and #Train and #Test is the number of training and test samples, respectively. value of 2e -5 . The training is performed with batches of 8 sequences of length 512. The maximal number of training epoch is 10. For each dataset, 1,000 samples are randomly selected from the training set to form the validation set, and training stops when the validation accuracy fails to improve for one epoch. At last, the proposed model is trained on a machine with four Tesla K80 GPUs.</figDesc><table><row><cell>Dataset</cell><cell>Domain</cell><cell>#Train</cell><cell>#Test</cell></row><row><cell>SQuAD(1.1)</cell><cell>Wikipedia</cell><cell cols="2">86,588 10,507</cell></row><row><cell>HotpotQA</cell><cell>Wikipedia</cell><cell>72,928</cell><cell>5,904</cell></row><row><cell>NewsQA</cell><cell cols="2">News articles 74,160</cell><cell>4,212</cell></row><row><cell>NaturalQ</cell><cell>Wikipedia</cell><cell cols="2">104,071 12,836</cell></row><row><cell>TriviaQA</cell><cell cols="2">Web snippets 61,688</cell><cell>7,785</cell></row><row><cell>SearchQA</cell><cell cols="3">Web snippets 117,384 16,980</cell></row><row><cell>to a minimum</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>* Some masking techniques (such as removing continues words<ref type="bibr" target="#b4">(Joshi et al., 2020)</ref> or words with dependency connections<ref type="bibr" target="#b14">(Tian et al., 2022)</ref>) may be beneficial to produce more diverse masked copies, and we leave the investigation to future work.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We express our sincere gratitude to the anonymous reviewers for their invaluable feedback, which has significantly enhanced the depth and presentation of our research. This work was partially supported by the <rs type="funder">Australian Research Council Discovery Project</rs> (<rs type="grantNumber">DP210101426</rs>), the <rs type="funder">Australian Research Council Linkage Project</rs> (<rs type="grantNumber">LP200201035</rs>), and <rs type="grantName">AEGiS Advance Grant</rs>(<rs type="grantNumber">888/008/268</rs>), <rs type="funder">University of Wollongong</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HHAvTaM">
					<idno type="grant-number">DP210101426</idno>
				</org>
				<org type="funding" xml:id="_YMembQA">
					<idno type="grant-number">LP200201035</idno>
					<orgName type="grant-name">AEGiS Advance Grant</orgName>
				</org>
				<org type="funding" xml:id="_FEYNTEE">
					<idno type="grant-number">888/008/268</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Employed benchmarks</head><p>The statistics for employed datasets are provided in Table <ref type="table">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training Details</head><p>The RoBERTa-base model <ref type="bibr" target="#b8">(Liu et al., 2019</ref>) is adopted as the encoder. The dropout rate across all layers is set as 0.1. The Adam optimizer with a dynamic learning rate is adopted, for which the learning rate is warmed up for 10 thousand steps to a maximum value of 1e -4 before decaying linearly</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bridging the Gap between Language Model and Reading Comprehension: Unsupervised MRC via Self-Supervision</title>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Ning Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2107.08582</idno>
		<idno>2107.08582</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MRQA 2019 Shared Task: Evaluating generalization in reading comprehension</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5801</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Machine Reading for Question Answering</title>
		<meeting>the 2nd Workshop on Machine Reading for Question Answering<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Block-skim: Efficient question answering for transformer</title>
		<author>
			<persName><forename type="first">Li</forename><forename type="middle">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="10710" to="10719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SpanBERT: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00300</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">KALA: knowledge-augmented language model adaptation</title>
		<author>
			<persName><forename type="first">Minki</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinheon</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.379</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Seattle, United States. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5144" to="5167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to perturb word embeddings for out-of-distribution QA</title>
		<author>
			<persName><forename type="first">Seanie</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minki</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.434</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5583" to="5595" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Seeing the wood for the trees: a contrastive regularization method for the lowresource knowledge base question answering</title>
		<author>
			<persName><forename type="first">Junping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinrong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-naacl.82</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: NAACL 2022</title>
		<meeting><address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1085" to="1094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1907.11692</idno>
		<idno>abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SSMBA: Self-supervised manifold based data augmentation for improving out-of-domain robustness</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzyeh</forename><surname>Ghassemi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.97</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1268" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Becoming a strategic reader</title>
		<author>
			<persName><forename type="first">G</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjorie</forename><forename type="middle">Y</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><forename type="middle">K</forename><surname>Lipson</surname></persName>
		</author>
		<author>
			<persName><surname>Wixson</surname></persName>
		</author>
		<idno type="DOI">10.1016/0361-476X(83)90018-8</idno>
	</analytic>
	<monogr>
		<title level="j">Contemporary Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="316" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Few-shot question answering by pretraining span selection</title>
		<author>
			<persName><forename type="first">Ori</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Kirstain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.239</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3066" to="3079" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Context-aware answer extraction in question answering</title>
		<author>
			<persName><forename type="first">Yeon</forename><surname>Seonwoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Hoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.189</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2418" to="2428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving machine reading comprehension with general reading strategies</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1270</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2633" to="2643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving relation extraction through syntax-induced pretraining with dependency masking</title>
		<author>
			<persName><forename type="first">Yuanhe</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-acl.147</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1875" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">EDA: Easy data augmentation techniques for boosting performance on text classification tasks</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1670</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6382" to="6388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning better masking for better language model pre-training</title>
		<author>
			<persName><forename type="first">Dongjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.acl-long.400</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7255" to="7267" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to skim text</title>
		<author>
			<persName><forename type="first">Adams</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hongrae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1172</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1880" to="1890" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Read, attend, and exclude: Multi-choice reading comprehension by mimicking human reasoning process</title>
		<author>
			<persName><forename type="first">Chenbin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congjian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zenglin</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401326</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;20</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1945" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Input-length-shortening and text generation via attention values</title>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">Yuxuan</forename><surname>NeÅet Ãzkan Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Bensemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hartill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gahegan</surname></persName>
		</author>
		<author>
			<persName><surname>Witbrock</surname></persName>
		</author>
		<idno>arXiv, 2303.07585</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
