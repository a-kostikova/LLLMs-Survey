<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Query-Focused Meeting Summarization with Query-Relevant Knowledge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tiezheng</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Artificial Intelligence Research (CAiRE) Department of Electronic and Computer Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Clear Water Bay</addrLine>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziwei</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Artificial Intelligence Research (CAiRE) Department of Electronic and Computer Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Clear Water Bay</addrLine>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
							<email>pascale@ece.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Artificial Intelligence Research (CAiRE) Department of Electronic and Computer Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<addrLine>Clear Water Bay</addrLine>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Query-Focused Meeting Summarization with Query-Relevant Knowledge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9609F136AC93578969B7DA21AAC56160</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Query-Focused Meeting Summarization (QFMS) aims to generate a summary of a given meeting transcript conditioned upon a query. The main challenges for QFMS are the long input text length and sparse query-relevant information in the meeting transcript. In this paper, we propose a knowledge-enhanced two-stage framework called Knowledge-Aware Summarizer (KAS) to tackle the challenges. In the first stage, we introduce knowledge-aware scores to improve the query-relevant segment extraction. In the second stage, we incorporate query-relevant knowledge in the summary generation.</p><p>Experimental results on the QMSum dataset show that our approach achieves state-of-the-art performance. Further analysis proves the competency of our methods in generating relevant and faithful summaries. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Meetings are an essential part of human collaboration and communication. Especially in recent years, the outbreak of Covid-19 has led people to meet online, where most meetings are automatically recorded and transcribed. Query-Focused Meeting Summarization (QFMS) <ref type="bibr" target="#b21">(Zhong et al., 2021)</ref> aims to summarize the given meeting transcript conditioned upon a query, which helps people efficiently catch up to the specific part of the meeting they want to know.</p><p>There are two main challenges for QFMS: Firstly, meeting transcripts can be so long that current deep learning models cannot encode them at once. Even for models <ref type="bibr" target="#b1">(Beltagy et al., 2020;</ref><ref type="bibr" target="#b16">Xiong et al., 2022)</ref> that accept long text input, the cost of computational complexity is enormous. Secondly, query-relevant content is sparsely scattered in the meeting transcripts, meaning a significant part of the transcripts is noisy information when given a 1 The code is released at: https://github.com/ TysonYu/KA-QFMS </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query-Relevant Knowledge Triple</head><p>Query Figure <ref type="figure">1</ref>: An example of query-relevant knowledge triple extracted from meeting transcripts. The knowledge triple can be used in query-relevant segment extraction as well as summary generation. particular query. Therefore, the models need to reduce the noisy information's impact effectively.</p><p>In this paper, we focus on the two-stage framework, extracting query-relevant segments from the meeting transcripts and then generating the summary based on the selected content. Compared to the end-to-end approaches <ref type="bibr" target="#b22">(Zhu et al., 2020;</ref><ref type="bibr" target="#b11">Pagnoni et al., 2022)</ref> that directly encode the entire meeting transcripts, the two-stage framework is better at keeping computational efficiency and easier to scale up to longer inputs. Specifically, we propose Knowledge-Aware Summarizer (KAS) that incorporates query-relevant knowledge in both stages. In the first stage, we extract knowledge triples from the text segments and introduce knowledge-aware scores to improve segment ranking. In the second stage, the extracted query-relevant knowledge triples are utilized as extra input for the summary generation. We conduct experiments on a QFMS dataset named QMSum <ref type="bibr" target="#b21">(Zhong et al., 2021)</ref> and achieve state-of-the-art performance. We further investigate how the different numbers of extracted segments affect the final performance. In addition, we manually evaluate the generation quality regarding fluency, relevance and factual correctness.</p><p>Our contributions in this work are threefold: (1) Our work demonstrates the effectiveness of leveraging query-relevant knowledge in QFMS. (2) We  propose KAS, a two-stage framework incorporating query-relevant knowledge for QFMS.</p><p>(3) Experimental results show that our approach achieves state-of-the-art performance on a QFMS dataset (QMSum). Further analysis and human evaluation indicate the advantage of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Existing QFMS methods can be divided into two categories <ref type="bibr" target="#b15">(Vig et al., 2022)</ref>: two-stage approaches and end-to-end approaches. Two-stage approaches <ref type="bibr" target="#b21">(Zhong et al., 2021;</ref><ref type="bibr" target="#b15">Vig et al., 2022;</ref><ref type="bibr" target="#b19">Zhang et al., 2022)</ref> first extract query-relevant snippets and then generate the summary upon the extracted snippets, while end-to-end approaches <ref type="bibr" target="#b22">(Zhu et al., 2020;</ref><ref type="bibr" target="#b20">Zhong et al., 2022;</ref><ref type="bibr" target="#b11">Pagnoni et al., 2022)</ref> directly generate summaries from the whole meeting transcripts. However, both types of methods have their disadvantages. For example, most of the two-stage approaches select query-relevant content based on utterance, which ignores the contextual information between multiple utterances. As for the end-to-end approaches, the computational and memory requirements will increase rapidly when the input text becomes longer, making models challenging to adapt to long meeting transcripts.</p><p>To our knowledge, we are the first to incorporate query-relevant knowledge in QFMS and demonstrate its effectiveness. Besides, we include multiple utterances in each segment to preserve the contextual information, and our approach can easily extend to process long input text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>This section presents our two-stage framework. Firstly, we introduce the extractor, which extracts query-relevant segments and knowledge from the source document. Then, a generative model synthesizes the query, extracted segments, and knowledge into the final summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Knowledge-aware Extractor</head><p>Meeting Transcripts Segmentation To preserve the contextual information between utterances, we split the meeting transcripts into segments, and each segment could contain multiple utterances. To do so, given an input meeting transcripts T , we will separate it into n segments S = {S 1 , S 2 , ..., S n } where each S i is fewer than l tokens. Specifically, we feed meeting transcripts to the segment utterance by utterance until it reaches l.</p><p>Knowledge-aware Ranking The knowledgeaware ranking approach will select top-k segments according to a combination of semantic search scores and knowledge-aware scores. We apply Multi-QA MPNet <ref type="bibr" target="#b12">(Song et al., 2020)</ref> to calculate semantic search scores. Multi-QA MPNet is trained on 215M question-answer pairs from various sources and domains, including Stack Exchange, MS MARCO <ref type="bibr" target="#b10">(Nguyen et al., 2016)</ref>, WikiAnswers <ref type="bibr" target="#b2">(Fader et al., 2014</ref>) and many more.</p><p>Given the query and the segments, the model outputs 768-dimension vectors to represent them. The semantic search score for each segment is computed according to the cosine similarity:</p><formula xml:id="formula_0">Score se = M P N et(Q) • M P N et(S i ) ∥M P N et(Q)∥ ∥M P N et(S i )∥ (1)</formula><p>To compute the knowledge-aware score for each segment, we first use OpenIE <ref type="bibr" target="#b0">(Angeli et al., 2015)</ref> to extractive knowledge triples. Then, to filter out the triples irrelevant to the query, we only keep the triples that contain overlapping words with the query. The knowledge-aware score is obtained by L2 normalizing the number of the remaining triples m i in each segment:</p><formula xml:id="formula_1">Score ka = m i n i=1 |m i | 2<label>(2)</label></formula><p>Finally, we calculate the ranking score by summing the semantic search score and the knowledgeaware score:</p><formula xml:id="formula_2">Score rank = Score se + Score ka<label>(3)</label></formula><p>The segment with the top-k ranking score will be selected for the next stage of summary generation. We denote the remaining segments as S = {S 1 , S 2 , ..., S k }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generator</head><p>We choose BART-large <ref type="bibr" target="#b5">(Lewis et al., 2020)</ref>, a transformer-based <ref type="bibr" target="#b14">(Vaswani et al., 2017)</ref> generative pre-trained language model, as our backbone model for the generator because of its remarkable performance on text summarization benchmarks. Following the idea of Fusion-in-Decoder (FiD) and its applications in generation tasks <ref type="bibr" target="#b3">(Izacard and Grave, 2021;</ref><ref type="bibr" target="#b13">Su et al., 2022;</ref><ref type="bibr" target="#b15">Vig et al., 2022)</ref>, we employ FiD-BART for encoding multiple segments independently in the encoder and fuse information from all segments in the decoder jointly through the encoder-decoder attention.</p><p>To incorporate the extracted knowledge in the summary generation, we use the knowledge as extra inputs other than the query and segments. In detail, we remove the stop words in the knowledge triples and then merge all the remaining words of knowledge triples in each segment as a set of knowledge phrases. The concatenation of each segment S i , knowledge phrases K i and the query Q will be processed by FiD-BART encoder:</p><formula xml:id="formula_3">h i enc = Encoder(Q ⊕ K i ⊕ S i )<label>(4)</label></formula><p>Finally, the decoder performs encoder-decoder attention over the concatenation of all segments' encoder outputs. In this way, the computational complexity grows linearly with the number of segments rather than quadratically, while jointly processing all segments in the decoder enables the model to aggregate information from multiple segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We choose the top 12 text segments in the segment selection, with each fewer than 512 tokens. For the summary generator, inspired by the effectiveness of pre-finetuning models on relevant datasets to transfer task-related knowledge in the abstractive summarization <ref type="bibr" target="#b18">(Yu et al., 2021;</ref><ref type="bibr" target="#b15">Vig et al., 2022)</ref>, we initialize our BART-large model using the checkpoint<ref type="foot" target="#foot_0">2</ref> pre-finetuned on WikiSum <ref type="bibr" target="#b7">(Liu et al., 2018)</ref>.  See Appendix A and B for more details of the experimental setup and baselines.</p><p>We evaluate the models on the QMSum <ref type="bibr" target="#b21">(Zhong et al., 2021)</ref> dataset, which consists of 1,808 querysummary pairs over 232 meetings from product design, academic, and political committee domains. We report ROUGE 3 <ref type="bibr" target="#b6">(Lin, 2004)</ref> as the automatic evaluation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>We compare our proposed model with strong baselines and previous state-of-the-art models. As shown in Table <ref type="table" target="#tab_3">1</ref>, our approach outperforms both the two-stage and end-to-end methods by a large margin on all evaluation metrics. We also investigate how the number of selected segments in the first stage affects the final performance. As shown in Table <ref type="table">2</ref>, when the number of selected segments grows, the quality of the final summary also improves. Previous end-to-end approaches usually outperform two-stage approaches because they encode the entire meeting transcripts, which conduct lots of computing resources. Our approach performs better by encoding 12 segments of input (6,144 tokens) in the second stage generation than the state-of-the-art end-to-end approach that directly encodes 16384 tokens. Therefore, we strike a balance between performance and efficiency, which is essential in real-world applications. Besides, our approach can easily extend to processing long input text since the ranking method is unsupervised. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We conduct an ablation study to investigate the contribution of the knowledge-aware modules by removing the knowledge-based scoring in the ranking (w/o KA in ranking) and the knowledge input in the summary generation (w/o KA in generator), respectively. Besides, we evaluate the entity-level factual consistency <ref type="bibr" target="#b9">(Nan et al., 2021)</ref> of the summary to test the effectiveness of our knowledgeaware modules in keeping the knowledge entities in generated summaries. Specifically, we report the F-1 scores of the entity overlap (Entity F-1) between the source and the generated summary. Table <ref type="table" target="#tab_5">3</ref> presents that the model's performance decreases in both metrics, especially the Entity F-1, without the knowledge-aware module, which indicates the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Human Evaluation</head><p>We further conduct a human evaluation to assess the models on fluency, relevance, and factual correctness. We randomly select 50 samples from the QMSum test set and ask three annotators to score the summary from one to five, with higher scores being better. We compare SegEnc-W because it is the best publicly available model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a two-stage framework named KAS that incorporates query-relevant knowledge in both stages. Extensive experimental results on the QMSum dataset show the effectiveness of our method. We further conduct de-tailed analysis and human evaluation prove our method's capacity to generate fluency, relevant and faithful summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>Our method is trained and tested on the only publicly available QFMS dataset named QMSum. QM-Sum consists of three different domains (Academic, Committee and Product), which makes the evaluation robust. However, QMSum only contains 1,808 samples, which is relatively small. We hope larger QFMS datasets be proposed to accelerate the development of this field.</p><p>In the first stage of our approach, we extract a fixed length (6,144 tokens) of the meeting transcripts as the second stage input text. Therefore, the model's performance could be affected since some query-relevant information could be cut off in the first stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Ethics Statement</head><p>Although our approach can generate query-focused meeting summaries and achieve a much better factual correctness score than other models, we can not avoid generating hallucinated content from the generative models. We recommend that when our approach is deployed in real-world applications, additional post-processing should be carried out to remove unreliable summaries. In addition, we will indicate that the summary we provide is for reference only, and users need to check the original meeting transcripts to get accurate information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental Details</head><p>We use the off-the-shelf model Multi-QA MPNet<ref type="foot" target="#foot_1">4</ref> for semantic searching. We initialize the generator from the off-the-shelf BART-large model through Hugging Face. We use learning rates 6e -5 following <ref type="bibr" target="#b5">(Lewis et al., 2020)</ref> and Adam optimizer (Kingma and Ba, 2014) to fine-tune KAS. We use a batch size of one and train all models on one RTX 3090 for ten epochs. During decoding, we use beam search with a beam size of five and decode until an end-of-sequence token is emitted. The final results are the average test set performance on the best three checkpoints in the validation set. In addition, the QMSum dataset has MIT License, so we can freely use it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Baselines Details</head><p>We conduct both two-stage and end-to-end baselines for comparison. MaRGE <ref type="bibr" target="#b17">(Xu and Lapata, 2021)</ref> used Masked ROUGE extractor in the first stage. DYLE <ref type="bibr" target="#b8">(Mao et al., 2022</ref>) is a dynamic latent extraction approach for abstractive long-input summarization. SUMM N <ref type="bibr" target="#b19">(Zhang et al., 2022</ref>) is a multi-stage summarization framework for long input dialogues and documents. RelReg (RELevance REGression) <ref type="bibr" target="#b15">(Vig et al., 2022</ref>) is similar to MaRGE, which trains a relevance prediction model directly on QFS data using the original, nonmasked query. RelReg-W <ref type="bibr" target="#b15">(Vig et al., 2022)</ref> uses the same framework as RelReg but initializes the generator from the WikiSum pre-trained checkpoint. LED <ref type="bibr" target="#b1">(Beltagy et al., 2020)</ref> is an encoderdecoder transformer-based model that employs efficient attention to process long input text. Di-alogLM <ref type="bibr" target="#b20">(Zhong et al., 2022)</ref> is a pre-trained neural encoder-decoder model for long dialogue understanding and summarization, and it uses a hybrid attention approach by combining sparse local attention with dense global attention. SegEnc-W <ref type="bibr" target="#b15">(Vig et al., 2022</ref>) is a fusion-in-decoder method that initialized the model from WikiSum <ref type="bibr" target="#b7">(Liu et al., 2018)</ref>. BART-LS <ref type="bibr" target="#b16">(Xiong et al., 2022)</ref> used poolingaugmented blockwise attention to improve the efficiency and pre-trains the model with a maskedspan prediction task with spans of varying lengths. SOCRATIC <ref type="bibr" target="#b11">(Pagnoni et al., 2022)</ref> uses a questiondriven, unsupervised pretraining objective to improve controllability in summarization tasks, and it</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Segment 1 Query Segment 2 Segment 3 Segment n</head><label></label><figDesc></figDesc><table><row><cell>Meeting Transcripts</cell></row><row><cell>•••</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Query-Relevant Knowledge Segment 1 Segment 2 Segment k ••• Query Knowledge 1 Query Knowledge 2 Query Knowledge k FiD-BART Query-Focused Meeting Summary</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Knowledge Extraction</cell><cell></cell><cell></cell><cell>Knowledge-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Aware</cell></row><row><cell></cell><cell>buttons</cell><cell>add</cell><cell>cost</cell><cell>Ranking</cell></row><row><cell>••••••</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Project Manager: Push-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>buttons are buttons not the</cell><cell>Segmentation</cell><cell></cell><cell></cell></row><row><cell>most expensive , but</cell><cell></cell><cell></cell><cell></cell></row><row><cell>do add extra cost .</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Industrial Designer: Okay.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>••••••</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Figure 2: An overview of our proposed framework. We first extract query-relevant knowledge triples based on the</cell></row><row><cell cols="5">query and meeting transcripts and select top-k segments through knowledge-aware ranking. Then we generate the</cell></row><row><cell cols="5">query-focused meeting summary by FiD-BART from query, knowledge and selected meeting transcripts.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Results on QMSum test set. The previous works can be divided into two-stage and end-to-end approaches.</figDesc><table><row><cell>Model</cell><cell cols="4">ROUGE-1 ROUGE-2 ROUGE-L Average</cell></row><row><cell>MaRGE (Xu and Lapata, 2021)</cell><cell>31.99</cell><cell>8.97</cell><cell>27.93</cell><cell>22.96</cell></row><row><cell>DYLE (Mao et al., 2022)</cell><cell>34.42</cell><cell>9.71</cell><cell>30.10</cell><cell>24.74</cell></row><row><cell>SUMM N (Zhang et al., 2022)</cell><cell>34.03</cell><cell>9.28</cell><cell>29.48</cell><cell>24.26</cell></row><row><cell>RelReg (Vig et al., 2022)</cell><cell>34.91</cell><cell>11.91</cell><cell>30.73</cell><cell>25.85</cell></row><row><cell>RelReg-W (Vig et al., 2022)</cell><cell>36.45</cell><cell>12.81</cell><cell>32.28</cell><cell>27.18</cell></row><row><cell>End-to-end</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LED (Beltagy et al., 2020)</cell><cell>31.60</cell><cell>7.80</cell><cell>20.50</cell><cell>19.97</cell></row><row><cell>DialogLM (Zhong et al., 2022)</cell><cell>34.50</cell><cell>9.92</cell><cell>30.27</cell><cell>24.90</cell></row><row><cell>SegEnc-W (Vig et al., 2022)</cell><cell>37.80</cell><cell>13.43</cell><cell>33.38</cell><cell>28.20</cell></row><row><cell>BART-LS (Xiong et al., 2022)</cell><cell>37.90</cell><cell>12.10</cell><cell>33.10</cell><cell>27.70</cell></row><row><cell>SOCRATIC (Pagnoni et al., 2022)</cell><cell>38.06</cell><cell>13.74</cell><cell>33.51</cell><cell>28.44</cell></row><row><cell>KAS (Ours)</cell><cell>38.80</cell><cell>14.01</cell><cell>34.26</cell><cell>29.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablation results on the QMSum test set without knowledge-Aware (KA). Our KA modules improve the performance of KAS on both stages.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Human evaluation results on QMSum test set.</figDesc><table><row><cell>3 https://github.com/pltrdy/files2rouge</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>illus-</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/salesforce/ query-focused-sum</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>https://huggingface. co/sentence-transformers/ multi-qa-mpnet-base-cos-v1</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>follows the SegEnc framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Evaluation</head><p>In Table <ref type="table">4</ref>, we conduct a human evaluation of the fluency, relevancy and factual correctness of the generated summaries from the QMSum dataset. In detail, we randomly sample 50 summaries from KAS, SegEnc-W and the ground truth summary from the same meeting transcripts for comparison. Assessments are scored on a scale of one to five, with higher scores being better. Fluency means the summary is grammatically correct and contextually coherent. Relevancy means the summary is relevant to the query. Factual correctness means the summary is faithful according to the information given in the meeting transcripts and does not contain hallucinations. We assign each summary to three annotators and take the average score as the final result. In total, we used three annotators from Hong Kong, and all annotators voluntarily participated in the human evaluation. All annotators agree to have their evaluation results included as part of this paper's results.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Leveraging linguistic structure for open domain information extraction</title>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><forename type="middle">Jose</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Johnson</forename><surname>Premkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="344" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Open question answering over curated and extracted knowledge bases</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1156" to="1165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Édouard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="874" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bart</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10198</idno>
		<title level="m">Generating wikipedia by summarizing long sequences</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dyle: Dynamic latent extraction for abstractive longinput summarization</title>
		<author>
			<persName><forename type="first">Ziming</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Henry</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ansong</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Budhaditya</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1687" to="1698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Entity-level factual consistency of abstractive text summarization</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henghui</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<meeting>the 16th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Main Volume</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2727" to="2733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">choice</title>
		<imprint>
			<biblScope unit="page">660</biblScope>
			<date type="published" when="2016">2016. 2640</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Socratic pretraining: Question-driven pretraining for controllable summarization</title>
		<author>
			<persName><forename type="first">Artidoro</forename><surname>Pagnoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Alexander R Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Kryściński</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.10449</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mpnet: Masked and permuted pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="16857" to="16867" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Read before generate! faithful long form question answering with machine reading</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jindi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="744" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploring neural models for query-focused summarization</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">Richard</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Kryściński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: NAACL 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1455" to="1468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adapting pretrained text-to-text models for long text sequences</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anchit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.10052</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generating query focused summaries from query-free resources</title>
		<author>
			<persName><forename type="first">Yumo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6096" to="6109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptsum: Towards low-resource domain adaptation for abstractive summarization</title>
		<author>
			<persName><forename type="first">Tiezheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5892" to="5904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Summn: A multi-stage summarization framework for long input dialogues and documents: A multi-stage summarization framework for long input dialogues and documents</title>
		<author>
			<persName><forename type="first">Yusen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ansong</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziming</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Henry</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Budhaditya</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1592" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dialoglm: Pre-trained model for long dialogue understanding and summarization</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="11765" to="11773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Qmsum: A new benchmark for query-based multi-domain meeting summarization</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Zaidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mutethia</forename><surname>Mutuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5905" to="5921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A hierarchical network for abstractive meeting summarization with cross-domain pretraining</title>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="194" to="203" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
