<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KoBigBird-large: Transformation of Transformer for Korean Language Understanding</title>
				<funder>
					<orgName type="full">Korea Agency for Infrastructure Technology Advancement</orgName>
					<orgName type="abbreviated">KAIA</orgName>
				</funder>
				<funder ref="#_3xZ34fd">
					<orgName type="full">Ministry of Land, Infrastructure and Transport</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Kisu</forename><surname>Yang</surname></persName>
							<email>ksyang@vaiv.kr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab Korea University NLP&amp;AI Lab</orgName>
								<orgName type="institution">VAIV Company AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">KoBigBird-large: Transformation of Transformer for Korean Language Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C8868516287814091E6CB98300943854</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-05-29T13:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work presents KoBigBird-large, a large size of Korean BigBird that achieves state-ofthe-art performance and allows long sequence processing for Korean language understanding. Without further pretraining, we only transform the architecture and extend the positional encoding with our proposed Tapered Absolute Positional Encoding Representations (TAPER). In experiments, KoBigBird-large shows stateof-the-art overall performance on Korean language understanding benchmarks and the best performance on document classification and question answering tasks for longer sequences against the competitive baseline models. We publicly release our model here 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Research on minority languages is a crucial area that extends beyond the scope of English language representations. Even though multilingual models for natural language understanding (NLU) <ref type="bibr" target="#b3">(Devlin et al., 2019;</ref><ref type="bibr" target="#b2">Conneau et al., 2020)</ref> have widely shown moderate performance, they still fall short of expectations for Korean NLU tasks. This deficiency highlights the need to develop language-specific models that can effectively handle the distinctive characteristics of the language. Currently, several Korean NLU models <ref type="bibr" target="#b7">(Lee et al., 2020;</ref><ref type="bibr" target="#b11">Park, 2020)</ref> have been proposed, among which KLUE-RoBERTa <ref type="bibr">(Park et al., 2021)</ref> has shown promising performance on general tasks. It has been designed to reflect the characteristics of the language and pretrained with various model sizes including a large size. Despite its strengths, the vanilla Transformer-based architecture prevents Figure <ref type="figure">1</ref>: An illustration of building KoBigBird-large process. Based on the architecture of KoBigBird-base and the parameters of RoBERTa-large, our proposed TAPER method is applied to build KoBigBird-large.</p><p>itself from processing long sequences over a certain length, which affects its overall performance because important information could be lost <ref type="bibr" target="#b1">(Beltagy et al., 2020)</ref>. Following BigBird which is proposed to process longer inputs <ref type="bibr" target="#b19">(Zaheer et al., 2020)</ref>, a Korean version of BigBird has been publicly released with a base size <ref type="bibr">(Park and Kim, 2021)</ref>. However, owing to the limited size, it fails to match the performance of other large models. As a result, the absence of a large size of Korean BigBird forces open-source users to choose either the competitive performance or long text processing although both are desirable features <ref type="bibr" target="#b6">(Lee et al., 2022)</ref>. To overcome this limitation, we present KoBigBirdlarge, a large size of Korean BigBird to simultaneously achieve state-of-the-art performance and longer sequence processing for Korean NLU tasks. It is initialized with the large size of KLUE-RoBERTa to take advantage of the mentioned strengths and then transformed into the BigBird architecture with the Tapered Absolute Positional Encoding Representations (TAPER) which could extend position embeddings. Noteworthily, no further pretraining or corpus is required to build it, but just the transformation of modules is all we need. Details regarding the modifications for short sequences are described in Sec-tion 3.1 while the methodology to improve extrapolation for extended sequences is in Section 3.2. This approach without further pretraining allows us to clarify the impact of differences in model structure and reduce carbon footprints in line with current ethical issues <ref type="bibr" target="#b14">(Patterson et al., 2021)</ref>. In experiments, KoBigBird-large achieves state-ofthe-art overall performance for all tasks on Korean NLU benchmarks <ref type="bibr">(Park et al., 2021)</ref> with an average gap of more than 0.4% points compared to the previous records. Also, the perplexity measurement of KoBigBird-large for longer inputs demonstrates that TAPER helps to improve the extrapolation of language models <ref type="bibr" target="#b15">(Press et al., 2021)</ref>. In additional experiments for long sequences, ours performs best on document classification (NIKL, 2020) and question answering tasks against the competitive baseline models. More details for experimental results are described in Section 4 and 5.</p><p>2 Related Work KLUE-RoBERTa Inspired by RoBERTa <ref type="bibr" target="#b9">(Liu et al., 2019)</ref>, KLUE-RoBERTa <ref type="bibr">(Park et al., 2021)</ref> has been proposed for Korean language processing. As a pretraining corpus, a subset of ten corpora has been selected based on criteria such as diversity, modernity, privacy, or toxicity concerns. The corpus has been pseudonymized with the Faker 2 library and morphologically analyzed <ref type="bibr" target="#b5">(Kudo, 2005)</ref> before its tokenizer has constructed the vocabulary using byte-pair encoding <ref type="bibr" target="#b17">(Sennrich et al., 2015)</ref>. The large size of KLUE-RoBERTa has been pretrained with a batch size of 2048 and a fixed learning rate of 1e-4 for 500k steps. In short, its strength lies in the use of a qualified corpus and a tokenizer with expensive pertaining, rather than the architecture itself. Thus, we transplant its well-tuned parameters into ours and promote the architecture to be KoBigBird-large.</p><p>KoBigBird-base BigBird <ref type="bibr" target="#b19">(Zaheer et al., 2020)</ref> is a pretrained model that has been proposed to handle longer sequences. With dilated sliding window attention and different window sizes across the layers, BigBird allows the model to process 8 times longer tokens than BERT <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref>. It improves the computational efficiency in the long text by replacing the self-attention layer with sparse attention. Based on English BigBird, KoBigBird-base, a Ko-2 https://github.com/joke2k/faker rean pretrained model for longer sequences, has been released <ref type="bibr">(Park and Kim, 2021)</ref>. It is pretrained on multiple corpora encompassing Korean public corpus<ref type="foot" target="#foot_0">3</ref> , Korean Wikipedia<ref type="foot" target="#foot_1">4</ref> and Common Crawl<ref type="foot" target="#foot_2">5</ref> . They adopt the WordPiece tokenizer and start pretraining from their own pretrained BERT weights. The model employs a batch size of 32 and a max sequence length of 4096, alongside a peak learning rate of 1e-4 which is coupled with a warmup phase of 20k steps, amounting to a total of 2M steps. The optimization is handled by the AdamW optimizer <ref type="bibr" target="#b10">(Loshchilov and Hutter, 2017)</ref>. The total number of pretraining tokens is less than that of KLUE-RoBERTa. Moreover, it is only available in the base size and does not incorporate morpheme analysis, which hinders its practical use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Transformation of Transformer</head><p>In this section, we provide a detailed description of our KoBigBird-large as a target model M tgt transformed from a source model M src . We choose KLUE-RoBERTa-large as M src because it features a morpheme-aware tokenizer tailored to Korean language characteristics, has been pretrained on ethically curated corpora, and stands out for its performance among language models for general Korean NLU tasks. When the input length is the same as or shorter than the predefined length l src of M src , KoBigBirdlarge operates in the full attention mode. During this mode, details of the embeddings and structure are upgraded to foster improvements in performance after fine-tuning, while ensuring output consistency with M src at the initial state. Details in this mode are elaborated in Section 3.1. On the other hand, M tgt operates in the sparse attention mode when the input length exceeds l src . In this case, since M tgt has an expanded input length l tgt that is greater than l src , this extension defaults to generate randomly initialized l tgt -l src absolute position embeddings (APE) unless otherwise handled. If used as is, they would provide inappropriate representations for positions. To mitigate this problem, we propose a novel method, Tapered Absolute Positional Encoding Representations (TAPER), for the extended APEs so that they show better extrapolation performance for language modeling. The sparse attention mode with our newly proposed method is in Section 3.2. Model hyperparameters are provided in Table <ref type="table" target="#tab_0">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Full Attention Mode</head><p>KoBigBird-large incorporates an enhanced version of the embeddings and structure, initially adopting all parameters from M src . Despite the modifications, it has been structured to ensure that, at the initial state, identical inputs yield consistent logits when compared to those produced by M src . Differences detailed below and further specifics can be verified in the released implementation.</p><p>Distinct Segment Type M src restricts the number of segment types to one as it is only trained with Masked Language Modeling (MLM) without Next Sentence Prediction (NSP). However, distinguishing separate input sentences can be significant in tasks such as Semantic Textual Similarity (STS), Natural Language Inference (NLI), and Machine Reading Comprehension (MRC). Thus, we created two segment type embeddings, duplicating the first segment type embeddings for the second. As the segment type embeddings remain untrained even during the pretraining of M src , they are constituted by zeros.</p><p>Revised Positional Encoding When constructing absolute position embeddings for KoBigBirdlarge, we adopted the l src APEs of M src . However, a bug in the implementation of it is observed, which starts position id counting from 2, neglecting position ids 0 and 1. Thus, during transformation, we extracted the l src APEs from the range [2, l src + 2).</p><p>Ordered Layer Normalization While M src applies layer normalization before dropout in the word embedding layer, it adopts the dropout before layer normalization order elsewhere. To ensure a consistent architecture, we apply dropout before layer normalization to the word embeddings layer. This alteration results in different logit values from M src and M tgt for the same input during the training mode, leading to different learning processes. However, they still return the exactly same logit values for the same input during inference at the initial state because the dropout is off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sparse Attention Mode</head><p>For input lengths exceeding l src , KoBigBird-large switches to the Internal Transformer Construction (ITC) mode <ref type="bibr" target="#b19">(Zaheer et al., 2020)</ref>. It incorporates global, sliding, and random attention to handle inputs up to l tgt . Alongside such structural changes for extended inputs, the extension generates untrained additional l tgt -l src APEs. To address this, we introduce the TAPER method, which extends the originally trained l src APEs by applying attenuation to generate additional l tgt -l src APEs.</p><p>TAPER The motivation for TAPER stems from the operational characteristics of ALiBi <ref type="bibr" target="#b15">(Press et al., 2021)</ref>. When attending to a target token in an attention layer, ALiBi predominantly focuses on source tokens close to the target token. The attention score diminishes significantly with increasing distance, thereby having minimal influence on distant tokens. Based on the characteristics, we hypothesize that even if the relationships with far-off tokens are unknown, repeating the pattern of pretrained position embeddings that are well-attuned to the relationships with nearby tokens should work well for a language modeling task. However, simply repeating the pretrained l src APEs into an identical pattern r = l tgt /l src times encounters the duplication problem: The APE for any arbitrary position x becomes indistinguishable from the APE at position x + l src . To address this, we apply the attenuated amplitude of each repetition.</p><formula xml:id="formula_0">P tgt = r-1 ∥ i=0 P src • τ • r -i τ • r<label>(1)</label></formula><p>In Equation <ref type="formula" target="#formula_0">1</ref>, the source position embeddings P src are extended to the target position embeddings P tgt . The number of repetitions, r, represents an integer quotient of l tgt divided by l src . A temperature, τ , determines the degree of attenuation applied to the amplitude of the APEs of each repetition, thereby making the extended positions distinguishable (see Figure <ref type="figure" target="#fig_0">2</ref>). As τ increases, the difference in the APEs of each repetition diminishes, and if it becomes too large, the extended APEs become almost identical to the repeated ones.</p><p>We set τ = 2.0 for KoBigBird-large. The value of τ should be adjusted so that the extended position embeddings show the best extrapolation performance.</p><p>The attenuated embeddings are concatenated along the dimension representing the position id.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">KLUE Benchmark</head><p>The Korean Language Understanding Evaluation (KLUE) benchmark <ref type="bibr">(Park et al., 2021)</ref>, designed to foster Korean language processing research, encompasses eight varied Korean NLU tasks including topic classification, semantic textual similarity, and more. To enable equitable model comparisons in Korean NLP, KLUE provides benchmark datasets, task-specific evaluation metrics and pretrained language models like KLUE-RoBERTa. Data statistics are served in Table <ref type="table">1</ref>.</p><p>YNAT This dataset comprises news headlines from online articles circulated by Yonhap News Agency, categorized into seven topics for Topic Classification (TC): politics, economy, society, culture, world, IT/science, and sports. Macro F1 score was adopted as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KLUE-STS</head><p>This dataset contains annotations of Semantic Textual Similarity (STS) between two sentences, rated from 0 to 5. Evaluation can be performed using the Pearson correlation coefficient between labels and predictions and the F1 score determined after converting them to binary using a threshold of 3.0.</p><p>KLUE-NLI This Natural Language Inference (NLI) dataset includes pairs of sentences and corresponding labels of three types: entailment, contradiction, and neutral. These denote the relation between premise and hypothesis sentences. The performance is gauged by its classification accuracy. which assesses if all slots are predicted accurately, and Slot F1 score, which measures the prediction accuracy for each individual slot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KLUE-NER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>Apart from KLUE-RoBERTa and KoBigBird models, we additionally assess two multilingual language models and two Korean monolingual language models for the benchmark.</p><p>mBERT This is a multilingual BERT model put forth and made publicly available. It is trained on a multilingual corpus that includes 104 languages, Korean included, using both the Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) objectives <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref>.</p><p>XLM-R A variant of RoBERTa trained on a vast multilingual corpus using the MLM objective <ref type="bibr" target="#b9">(Liu et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KR-BERT</head><p>A publicly available Korean language model at the character level, based on BERT. The KR-BERT character WordPiece tokenizer incorporates a vocabulary of 16,424 unique tokens <ref type="bibr" target="#b7">(Lee et al., 2020)</ref>.</p><p>KoELECTRA An open-source Korean language model, trained with the MLM and replaced token detection objectives <ref type="bibr" target="#b11">(Park, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Settings</head><p>The learning rate of 2e-5 with the AdamW optimizer is used for all KLUE benchmark tasks, influenced by prior research in which adjustments were made in the range of 1e-5 to 5e-5, adopting a consistent value for easy reimplementation. Instead, the batch size is selected from {8, 16, 32}. Maximum sequence lengths are 128 for YNAT, KLUE-STS, KLUE-NLI, 256 for KLUE-RE and KLUE-DP, and 512 for KLUE-NER, KLUE-MRC, WoS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Benchmark Results</head><p>Table <ref type="table" target="#tab_2">3</ref> presents interesting results regarding the performance of different models on Korean NLU tasks. Performances marked with an asterisk (*) were borrowed from previous studies <ref type="bibr">(Park et al., 2021)</ref>, providing a valuable benchmark for comparison.</p><p>Firstly, multilingual models demonstrate weak performance in Korean NLU tasks, affirming the importance of language-specific models for these tasks. Among the base size models, KLUE-RoBERTa-base and KoBigBird-base show superior performance compared to other publicly available Korean NLU models. When comparing the base and large size models, there is a performance increase when scaling up. Specifically, KLUE-RoBERTa-large achieves a 2.07% points improvement over its base counterpart, and our KoBigBird-large shows a 2.38% points increase over KoBigBird-base. When comparing large models, KoBigBird-large performs 0.41% points better than KLUE-RoBERTa-large, demonstrating its potential for more advanced NLU tasks. An interesting anomaly is observed in the YNAT dataset where, in line with previous research <ref type="bibr">(Park et al., 2021)</ref>, base models outperform large models. This might be due to the nature of the YNAT task, which involves classifying topics based on titles.</p><p>The data for this task may be too easy to classify, leading to rapid overfitting during training with larger models.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effects of Distinct Segment Type</head><p>Table <ref type="table" target="#tab_3">4</ref> presents findings on the role of segment type embeddings in our KoBigBird-large model which includes two segment type embeddings to distinguish between different types of text segments. These embeddings are initially identical, but they are separately adapted during fine-tuning. Our experiments were conducted on tasks that require multiple text inputs, specifically Semantic Textual Similarity (STS), Natural Language Inference (NLI), and Machine Reading Comprehension (MRC) tasks.</p><p>The results in Table <ref type="table" target="#tab_3">4</ref> show performance differences depending on whether or not segment types are distinguished. It was observed that distinguishing segment types tended to offer performance advantages in tasks involving multiple text inputs. This evidence indicates the potential benefit of employing distinct segment type embeddings in tasks with multiple text inputs, emphasizing the adaptability and flexibility of the KoBigBird-large model in handling complex NLU tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Extrapolation</head><p>We investigate the impact of position embeddings and the temperature parameter τ on the model's perplexity (PPL) as in Figure <ref type="figure" target="#fig_1">3</ref>. As one of the met- rics to evaluate language models, it represents the uncertainty of the model, so a lower score indicates higher performance. We measure the PPL on preprocessed Korean Wikipedia corpus<ref type="foot" target="#foot_3">6</ref> . Inputs to a language model are packed with full sequences sampled contiguously from one or more documents, with separate tokens inserted between them to delimit individual documents like <ref type="bibr" target="#b9">Liu et al. (2019)</ref>. Without random token replacement <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref>, 15% of the input tokens are replaced with masked tokens. The PPL is measured only for these masked tokens, and the average is calculated.</p><p>In Figure <ref type="figure" target="#fig_1">3</ref>, "Vanilla" refers to the model prior to the application of TAPER, where the newly extended position embeddings are randomly initialized. On the other hand, "Repeated" signifies a scenario where the same value is repeated without the application of a temperature parameter (τ ).</p><p>Our findings show that the Vanilla model struggles to make predictions for lengths exceeding the pretraining limit, indicating the necessity of a more nuanced approach for longer sequences. When a temperature (τ ) of 1.0 is applied, the PPL diverges after a certain point, suggesting a limit to the model's capacity to handle long sequences effectively in this configuration.</p><p>The model performs best with a temperature (τ ) of 2.0, achieving the lowest PPL, suggesting that this temperature setting allows the model to handle longer sequences more effectively. Beyond this point, however, as the temperature increases, the PPL slightly rises, indicating that too high a temperature close to the repetition of APEs may have a negative impact on the model's performance. These results suggest that careful tuning of the temperature parameter and adequate handling of position embeddings are crucial for optimizing the model's performance, particularly for long sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Long Text Classification</head><p>In this part, we focus on longer text processing. For a single text NLU task, we adopt the Sentiment Analysis dataset from Modu Corpus (NIKL, 2020) built by the National Institute of the Korean Language. It consists of a total of 2,081 documents based on blogs or social media posts, and its topics are related to products, movies, and travel. As represented in Table <ref type="table" target="#tab_5">5</ref>, we divide the total data into training and validation sets with a 4:1 ratio. Although some documents exceed the maximum token length of the models, the models utilize tokens within a predetermined length. Each document has five sentiment labels: strong negative, negative, neutral, positive, and strong positive. Macro F1 score is used as the evaluation metric for the classification task.  of around 10% points. When we do not apply TAPER to the KoBigBirdlarge, and the extended position embeddings initiate randomly, the model suffers a significant performance decline even after fine-tuning. Strikingly, in sequences longer than 2048, the large model without TAPER underperforms compared to the base model, highlighting the substantial role of TAPER in sustaining performance. Overall, these results affirm the necessity of TAPER application in optimizing the performance of the KoBigBird-large, emphasizing its crucial role in handling longer sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Question Answering for Longer Context</head><p>We venture to address the machine reading comprehension (MRC) task for a longer text-pair NLU task, which derives an appropriate response through the extraction of pertinent context spans answering a question. Despite the availability of Korean datasets such as KorQuAD 1.0 <ref type="bibr" target="#b8">(Lim et al., 2019)</ref> and KorQuAD 2.0 <ref type="bibr" target="#b4">(Kim et al., 2019)</ref>, modeled after the notable English SQuAD dataset <ref type="bibr" target="#b16">(Rajpurkar et al., 2016)</ref>, they presented significant limitations. The former offers too short input lengths while the latter employs HTML formats, so both are unfit for evaluating long input NLU. To circumvent these drawbacks, we select the KLUE-MRC dataset for our experiment, discussed in Section 4.1, characterized by its adequate input length primarily composed of KLUE's licensing policy. This grants open-source users the freedom to copy, redistribute, alter, and build upon the material for any purpose, including commercial endeavors, provided that they distribute their derivative works under an identical license (CC BY-SA 4.0). We anticipate that this approach will significantly enhance future endeavors in NLP research and development.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustrations showcasing the TAPER method's extension of position embeddings by multiplying the source embeddings with variables unique to each corresponding iteration, enhancing the informativeness and distinguishability of the extended positional representations.</figDesc><graphic coords="3,306.14,70.87,218.24,133.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Perplexity scores on Korean Wikipedia corpus for extrapolation measurement</figDesc><graphic coords="6,306.14,245.67,218.24,186.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Comparison of hyperparameters for representative Korean NLU models.</figDesc><table><row><cell>Dataset</cell><cell cols="5">|Train| |Dev| L min L avg L max</cell></row><row><cell>YNAT</cell><cell cols="2">45,678 9,107</cell><cell>4</cell><cell>27</cell><cell>44</cell></row><row><cell>KLUE-STS</cell><cell>11,668</cell><cell>519</cell><cell>17</cell><cell>66</cell><cell>207</cell></row><row><cell>KLUE-NLI</cell><cell cols="2">24,998 3,000</cell><cell>29</cell><cell>70</cell><cell>170</cell></row><row><cell cols="3">KLUE-NER 21,008 5,000</cell><cell>25</cell><cell>71</cell><cell>222</cell></row><row><cell>KLUE-RE</cell><cell cols="2">32,740 7,765</cell><cell>17</cell><cell>93</cell><cell>432</cell></row><row><cell>KLUE-DP</cell><cell cols="2">10,000 2,000</cell><cell>16</cell><cell>48</cell><cell>140</cell></row><row><cell cols="3">KLUE-MRC 26,128 8,643</cell><cell cols="3">209 1,052 2,070</cell></row><row><cell>WoS</cell><cell cols="2">8,000 1,000</cell><cell>110</cell><cell cols="2">522 1,429</cell></row><row><cell cols="6">Table 1: Data statistics of KLUE benchmark. It shows</cell></row><row><cell cols="6">the number of samples and the minimum, average, and</cell></row><row><cell cols="6">maximum length of input characters of each develop-</cell></row><row><cell cols="6">ment set. We count the sum of two sentences for the</cell></row><row><cell cols="6">tasks involving multiple sentences (KLUE-STS, KLUE-</cell></row><row><cell cols="2">NLI and KLUE-MRC).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">organization, date, time, and quantity. The evalua-</cell></row><row><cell cols="6">tion involves entity-level and character-level macro</cell></row><row><cell>F1 scores.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">KLUE-RE This Relation Extraction (RE) dataset</cell></row><row><cell cols="6">contains annotations for semantic relationships</cell></row><row><cell cols="6">between subject and object entities in sentences.</cell></row><row><cell cols="6">It includes 30 relationship classes, including a</cell></row><row><cell cols="6">"no_relation" label. Evaluation involves Micro F1</cell></row><row><cell cols="6">score without the "no_relation" class and the Area</cell></row><row><cell cols="5">Under the Precision-Recall Curve (AUPRC).</cell><cell></cell></row><row><cell cols="6">KLUE-DP This dataset annotates Dependency</cell></row><row><cell cols="6">Parsing (DP) in sentences using syntax and func-</cell></row><row><cell cols="6">tion tags. The performance is evaluated using Un-</cell></row><row><cell cols="6">labeled Attachment Score (UAS) for function tags</cell></row><row><cell cols="6">and Labeled Attachment Score (LAS) for both func-</cell></row><row><cell cols="2">tion and syntax tags.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">KLUE-MRC This Machine Reading Compre-</cell></row><row><cell cols="6">hension (MRC) dataset contains text, questions,</cell></row><row><cell cols="6">and answer spans indicating where the answers are</cell></row><row><cell cols="6">located in the text. The model's performance is</cell></row><row><cell cols="6">evaluated based on the Exact Match (EM) of find-</cell></row><row><cell cols="6">ing the answer location accurately at the character</cell></row><row><cell cols="6">level and the ROUGE-W score, which measures</cell></row><row><cell cols="6">the similarity between the predicted and actual an-</cell></row><row><cell cols="6">swers using the longest common consecutive sub-</cell></row><row><cell>sequence.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">WoS Wizard of Seoul (WoS) is a Dialog State</cell></row><row><cell cols="6">Tracking (DST) dataset that labels slot-value pairs</cell></row><row><cell cols="6">in dialogues between humans (travelers) and com-</cell></row></table><note><p>This Named Entity Recognition (NER) dataset annotates entity classes for each character in a sentence, including person, location, puters (information sources). Slots represent categories (e.g., hotel type), while values represent possible options (e.g., hotel, guest house). The evaluation measures include Joint Goal Accuracy (JGA),</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparative experiments of our model to other Korean models on KLUE benchmark. The scores in bold indicate the best score, and the underline indicate the second best score.</figDesc><table><row><cell></cell><cell cols="2">KLUE-STS</cell><cell>KLUE-NLI</cell><cell cols="2">KLUE-MRC</cell></row><row><cell>Segment Type</cell><cell>R P</cell><cell>F1</cell><cell>ACC</cell><cell cols="2">EM ROUGE</cell></row><row><cell>Uniform</cell><cell cols="2">93.36 87.58</cell><cell>89.53</cell><cell>75.04</cell><cell>81.07</cell></row><row><cell>Distinct</cell><cell cols="2">93.69 88.37</cell><cell>89.57</cell><cell>75.57</cell><cell>81.44</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The experimental results based on different segment type embeddings. In the task of KLUE-STS, KLUE-NLI, and KLUE-MRC, distinct types achieve higher scores than uniform types.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>presents the experimental results of the</cell></row><row><cell>KLUE-RoBERTa and KoBigBird models for long</cell></row><row><cell>document classification, including cases where the</cell></row><row><cell>TAPER method is not applied to the KoBigBird-</cell></row><row><cell>large as the ablation study. In our experiments, the</cell></row><row><cell>KoBigBird-large not only achieves its peak perfor-</cell></row><row><cell>mance at a sequence length of 512 but also retains</cell></row><row><cell>the highest performance in longer sequences up to</cell></row><row><cell>a length of 4096. This underscores the effective en-</cell></row><row><cell>hancements in both full attention mode and sparse</cell></row><row><cell>attention mode. Scaling from the base to the large</cell></row><row><cell>model size yields a notable performance increase</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Data statistics of Modu Sentiment dataset for long text classification. It presents the number of samples and the minimum, quartiles, and maximum lengths of input characters of each split.</figDesc><table><row><cell cols="3">Single Document Classification</cell><cell></cell></row><row><cell>Model</cell><cell>512</cell><cell cols="3">1024 2048 4096</cell></row><row><cell cols="2">KLUE-RoBERTa-base 42.61</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">KLUE-RoBERTa-large 52.30</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>KoBigBird-base</cell><cell cols="4">45.81 46.14 47.47 44.90</cell></row><row><cell cols="5">KoBigBird-large (ours) 55.32 53.44 58.22 52.06</cell></row><row><cell>-TAPER</cell><cell>-</cell><cell cols="3">47.44 43.58 43.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Macro F1 scores for the 5-class classification task on Modu Sentiment datasets. Models perform sentiment analysis with various token lengths.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>https://corpus.korean.go.kr/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>https://dumps.wikimedia.org/kowiki/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>https://commoncrawl.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>https://ratsgo.github.io/embedding/ downloaddata.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is supported by the <rs type="funder">Korea Agency for Infrastructure Technology Advancement (KAIA)</rs> grant funded by the <rs type="funder">Ministry of Land, Infrastructure and Transport</rs> (Grant <rs type="grantNumber">RS-2022-00143336</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3xZ34fd">
					<idno type="grant-number">RS-2022-00143336</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>natural language. We provide the data statistics of KLUE-MRC in Table <ref type="table">7</ref>.</p><p>The experimental results for MRC are shown in Table <ref type="table">8</ref>. We evaluated the performance with Exact Match (EM) and ROUGE-W scores. Similar to the single document classification task, KoBigBirdlarge exhibits the best performance across all length segments. It demonstrates superior performance compared to KLUE-RoBERTa-large, illustrating the efficacy of adjustments made in the full attention mode. While KoBigBird-base shows improved performance with inputs longer than 512 tokens, KoBigBird-large in spite of TAPER slightly regresses because it is crucial to find the exact positions of pertinent spans in the MRC task. Nonetheless, it still outperforms other models. Our ablation study, without TAPER applied, shows a performance drop within 3% points. This indicates that employing TAPER with KoBigBird-large is effective, particularly when handling long inputs, thereby affirming its instrumental role in enhancing the performance in processing extensive texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>NLU modules remain highly applicable in fields where neural network throughput and output regularity are critical <ref type="bibr" target="#b18">(Yamada et al., 2021;</ref><ref type="bibr" target="#b0">Baradaran et al., 2022)</ref>. By presenting KoBigBird-large, enabling the simultaneous achievement of state-of-the-art performance and long input processing, this paper contributes to the Korean research community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>In this study, we ensure strict adherence to ethical considerations, particularly in environmental sustainability and data privacy, following the best practices in AI research. We minimize environmental impact by using transformation-only on publicly available models, negating further pretraining on parameters and reducing computational load.</p><p>For data privacy, we apply pseudonymization techniques during preprocessing, anonymizing all identifiable information in our training corpus. We responsibly transfer parameters from models trained on this pseudonymized corpus, aligning with our dedication to anonymity and ethical AI usage.</p><p>Our research methodology, upholding transparency, accountability, and privacy, represents our commitment to the highest ethical conduct and we welcome constructive discourse for continuous improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>The position embeddings of KoBigBird-large, expanded using the TAPER technique, exhibit lower extrapolation performance compared to embeddings obtained through more resource-intensive pretraining. We have endeavored to minimize unnecessary training by transforming the parameters of the existing model, aiming to reduce the carbon footprint associated with the pretraining process. However, this approach has led to a trade-off in the representativeness of the position embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Risks</head><p>If users become overly dependent on the model's predictions or suggestions, they might not critically consider or evaluate the generated content, which could lead to the propagation of misinformation or skewed perspectives. There is a potential security risk of adversarial attacks where malicious actors could manipulate the model's output for their purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Licenses</head><p>KoBigBird-large is distributed under the terms of the CC BY-SA 4.0 license in accordance with</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey on machine reading comprehension systems</title>
		<author>
			<persName><forename type="first">Razieh</forename><surname>Baradaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razieh</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Amirkhani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="683" to="732" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised crosslingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Édouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Korquad 2.0: Korean qa dataset for web document machine comprehension</title>
		<author>
			<persName><forename type="first">Youngmin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungyoung</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunjeong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soyoon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myungji</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Human and Language Technology</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
	<note>Human and Language Technology</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mecab: Yet another part-of-speech and morphological analyzer</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<ptr target="http://mecab.sourceforge.net/" />
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Littlebird: Efficient faster &amp; longer transformer for question answering</title>
		<author>
			<persName><forename type="first">Minchul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kijong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myeong Cheol</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11870</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Sangah</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hansol</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunmee</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzi</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyopil</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03979</idno>
		<title level="m">Kr-bert: A small-scale korean-specific language model</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Korquad1. 0: Korean qa dataset for machine reading comprehension</title>
		<author>
			<persName><forename type="first">Seungyoung</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myungji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jooyoul</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07005</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101.NIKL.2020</idno>
		<ptr target="https://corpus.korean.go.kr" />
		<title level="m">National institute of korean languages corpora</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Decoupled weight decay regularization</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Jangwon</forename><surname>Park</surname></persName>
		</author>
		<ptr target="https://github.com/monologg/KoELECTRA" />
		<title level="m">Koelectra: Pretrained electra model for korean</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Jangwon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donggyu</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5654154</idno>
		<title level="m">Kobigbird: Pretrained bigbird model for korean</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Klue: Korean language understanding evaluation</title>
		<author>
			<persName><forename type="first">Sungjoon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihyung</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Won Ik</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Yoon Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jangwon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chisung</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junseong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngsook</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taehwan</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Round</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluis-Miquel</forename><surname>Munguia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rothchild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maud</forename><surname>Texier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10350</idno>
		<title level="m">Carbon emissions and large neural network training</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Train short, test long: Attention with linear biases enables input length extrapolation</title>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.12409</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<title level="m">Squad: 100,000+ questions for machine comprehension of text</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Efficient passage retrieval with hashing for open-domain question answering</title>
		<author>
			<persName><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00882</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinava</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17283" to="17297" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
