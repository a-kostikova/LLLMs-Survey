[
    {
        "title": "Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement",
        "authors": [
            "Xin Quan",
            "Marco Valentino",
            "Louise Dennis",
            "Andre Freitas"
        ],
        "published": "2024",
        "summary": "An increasing amount of research in Natural Language Inference (NLI) focuses on the application and evaluation of Large Language Models (LLMs) and their reasoning capabilities. Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains. In this paper, we focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can enhance the logical validity and alignment of ethical explanations produced by LLMs. Specifically, we present an abductive-deductive framework named Logic-Explainer, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy. An extensive empirical analysis demonstrates that Logic-Explainer can improve explanations generated via in-context learning methods and Chain-of-Thought (CoT) on challenging ethical NLI tasks, while, at the same time, producing formal proofs describing and supporting models’ reasoning. As ethical NLI requires commonsense reasoning to identify underlying moral violations, our results suggest the effectiveness of neuro-symbolic methods for multi-step NLI more broadly, opening new opportunities to enhance the logical consistency, reliability, and alignment of LLMs.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.1.pdf",
        "keywords": [
            "explanations",
            "ethical explanations",
            "ethical",
            "natural language explanations",
            "logical consistency",
            "abductive",
            "commonsense reasoning",
            "explainer",
            "symbolic refinement",
            "inference"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains.\""
    },
    {
        "title": "Anisotropy Is Inherent to Self-Attention in Transformers",
        "authors": [
            "Nathan Godey",
            "Éric Clergerie",
            "Benoît Sagot"
        ],
        "published": "2024",
        "summary": "The representation degeneration problem is a phenomenon that is widely observed among self-supervised learning methods based on Transformers. In NLP, it takes the form of anisotropy, a singular property of hidden representations which makes them unexpectedly close to each other in terms of angular distance (cosine-similarity). Some recent works tend to show that anisotropy is a consequence of optimizing the cross-entropy loss on long-tailed distributions of tokens. We show in this paper that anisotropy can also be observed empirically in language models with specific objectives that should not suffer directly from the same consequences. We also show that the anisotropy problem extends to Transformers trained on other modalities. Our observations tend to demonstrate that anisotropy might actually be inherent to Transformers-based models.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.3.pdf",
        "keywords": [
            "representation degeneration",
            "anisotropy"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We show in this paper that anisotropy can also be observed empirically in language models with specific objectives that should not suffer directly from the same consequences. Our observations tend to demonstrate that anisotropy might actually be inherent to Transformers-based models.\"\n\nThis rating is given because the paper extensively discusses a limitation (anisotropy) of Transformers-based models, including LLM",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We show in this paper that anisotropy can also be observed empirically in language models with specific objectives that should not suffer directly from the same consequences. Our observations tend to demonstrate that anisotropy might actually be inherent to Transformers-based models.\"\n\nThis rating is given because the paper extensively discusses a limitation (anisotropy) of Transformers-based models, including LLM"
    },
    {
        "title": "Generating Benchmarks for Factuality Evaluation of Language Models",
        "authors": [
            "Dor Muhlgay",
            "Ori Ram",
            "Inbal Magar",
            "Yoav Levine",
            "Nir Ratner",
            "Yonatan Belinkov",
            "Omri Abend",
            "Kevin Leyton-Brown",
            "Amnon Shashua",
            "Yoav Shoham"
        ],
        "published": "2024",
        "summary": "Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain. Existing methods for factuality evaluation of LLM generation focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent domain specific or rare facts. We propose FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality. FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM’s propensity to generate true facts from the corpus vs. similar but incorrect statements. We use our framework to create three benchmarks: Wiki-FACTOR, News-FACTOR and Expert-FACTOR. We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score and perplexity do not always agree on model ranking; (iii) when perplexity and benchmark score disagree, the latter better reflects factuality in open-ended generation, as measured by human annotators.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.4.pdf",
        "keywords": [
            "factuality",
            "factual assessment",
            "factual corpus",
            "benchmarks",
            "perplexity",
            "language models",
            "factuality evaluation",
            "benchmark score disagree",
            "lm factuality",
            "scalable",
            "expert factor",
            "lm",
            "ranking",
            "s propensity"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing methods for factuality evaluation of LLM generation focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent domain specific or rare facts.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Existing methods for factuality evaluation of LLM generation focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent domain specific or rare facts.\""
    },
    {
        "title": "Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs",
        "authors": [
            "Simone Balloccu",
            "Patrícia Schmidtová",
            "Mateusz Lango",
            "Ondrej Dusek"
        ],
        "published": "2024",
        "summary": "Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of indirect data leaking, where modelsare iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using OpenAI’s GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination. By analysing 255 papers and considering OpenAI’s data usage policy, we extensively document the amount of data leaked to these models during the first year after the model’s release. We report that these models have been globally exposed to ∼4.7M samples from 263 benchmarks. At the same time, we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and reproducibility issues. We release our results as a collaborative project on https://leak-llm.github.io/, where other researchers can contribute to our efforts.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.5.pdf",
        "keywords": [
            "natural language processing",
            "reproducibility",
            "cheat"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers.\"; \"We report that these models have been globally exposed to ∼4.7M samples from 263 benchmarks.\"; \"At the same time, we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and repro",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers.\"; \"We report that these models have been globally exposed to ∼4.7M samples from 263 benchmarks.\"; \"At the same time, we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and repro"
    },
    {
        "title": "Archer: A Human-Labeled Text-to-SQL Dataset with Arithmetic, Commonsense and Hypothetical Reasoning",
        "authors": [
            "Danna Zheng",
            "Mirella Lapata",
            "Jeff Pan"
        ],
        "published": "2024",
        "summary": "We present Archer, a challenging bilingual text-to-SQL dataset specific to complex reasoning, including arithmetic, commonsense and hypothetical reasoning. It contains 1,042 English questions and 1,042 Chinese questions, along with 521 unique SQL queries, covering 20 English databases across 20 domains. Notably, this dataset demonstrates a significantly higher level of complexity compared to existing publicly available datasets. Our evaluation shows that Archer challenges the capabilities of current state-of-the-art models, with a high-ranked model on the Spider leaderboard achieving only 6.73% execution accuracy on Archer test set. Thus, Archer presents a significant challenge for future research in this field.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.6.pdf",
        "keywords": [
            "hypothetical reasoning",
            "commonsense",
            "archer",
            "arithmetic",
            "sql"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.\n\n(Note: The paper does not explicitly mention LLMs, but it does discuss the limitations of state-of-the-art models in the field of text-to-SQL, which may include LLMs. However, based on the provided instructions, I will not consider this as a discussion of LLM limitations.)",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs.\n\n(Note: The paper does not explicitly mention LLMs, but it does discuss the limitations of state-of-the-art models in the field of text-to-SQL, which may include LLMs. However, based on the provided instructions, I will not consider this as a discussion of LLM limitations.)"
    },
    {
        "title": "GEAR: Augmenting Language Models with Generalizable and Efficient Tool Resolution",
        "authors": [
            "Yining Lu",
            "Haoping Yu",
            "Daniel Khashabi"
        ],
        "published": "2024",
        "summary": "Augmenting large language models (LLM) to use external tools enhances their performance across a variety of tasks. However, prior works over-rely on task-specific demonstration of tool use that limits their generalizability and computational cost due to making many calls to large-scale LLMs. We introduce GEAR, a computationally efficient query-tool grounding algorithm that is generalizable to various tasks that require tool use while not relying on task-specific demonstrations. GEAR achieves better efficiency by delegating tool grounding and execution to small language models (SLM) and LLM, respectively; while leveraging semantic and pattern-based evaluation at both question and answer levels for generalizable tool grounding. We evaluate GEAR on 14 datasets across 6 downstream tasks, demonstrating its strong generalizability to novel tasks, tools and different SLMs. Despite offering more efficiency, GEAR achieves higher precision in tool grounding compared to prior strategies using LLM prompting, thus improving downstream accuracy at a reduced computational cost. For example, we demonstrate that GEAR-augmented GPT-J and GPT-3 outperform counterpart tool-augmented baselines because of better tool use.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.7.pdf",
        "keywords": [
            "generalizable",
            "gear",
            "generalizable tool grounding",
            "small language models",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, prior works over-rely on task-specific demonstration of tool use that limits their generalizability and computational cost due to making many calls to large-scale LLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, prior works over-rely on task-specific demonstration of tool use that limits their generalizability and computational cost due to making many calls to large-scale LLMs.\""
    },
    {
        "title": "LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons using Large Language Models",
        "authors": [
            "Adian Liusie",
            "Potsawee Manakul",
            "Mark Gales"
        ],
        "published": "2024",
        "summary": "Current developments in large language models (LLMs) have enabled impressive zero-shot capabilities across various natural language tasks. An interesting application of these systems is in the automated assessment of natural language generation (NLG), a highly challenging area with great practical benefit. In this paper, we explore two options for exploiting the emergent abilities of LLMs for zero-shot NLG assessment: absolute score prediction, and comparative assessment which uses relative comparisons between pairs of candidates. Though comparative assessment has not been extensively studied in NLG assessment, we note that humans often find it more intuitive to compare two options rather than scoring each one independently. This work examines comparative assessment from multiple perspectives: performance compared to absolute grading; positional biases in the prompt; and efficient ranking in terms of the number of comparisons. We illustrate that LLM comparative assessment is a simple, general and effective approach for NLG assessment. For moderate-sized open-source LLMs, such as FlanT5 and Llama2-chat, comparative assessment is superior to prompt scoring, and in many cases can achieve performance competitive with state-of-the-art methods. Additionally, we demonstrate that LLMs often exhibit strong positional biases when making pairwise comparisons, and we propose debiasing methods that can further improve performance.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.8.pdf",
        "keywords": [
            "comparative assessment",
            "natural language generation",
            "nlg assessment",
            "large language models",
            "pairwise comparisons",
            "automated assessment",
            "zero shot nlg assessment",
            "zero shot nlg evaluation",
            "absolute score prediction"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Additionally, we demonstrate that LLMs often exhibit strong positional biases when making pairwise comparisons, and we propose debiasing methods that can further improve performance.\"\n\nThis abstract mentions a limitation of LLMs (positional biases) but does not explore it in depth and focuses more on the proposed solution and its effectiveness.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Additionally, we demonstrate that LLMs often exhibit strong positional biases when making pairwise comparisons, and we propose debiasing methods that can further improve performance.\"\n\nThis abstract mentions a limitation of LLMs (positional biases) but does not explore it in depth and focuses more on the proposed solution and its effectiveness."
    },
    {
        "title": "Parameter-Efficient Conversational Recommender System as a Language Processing Task",
        "authors": [
            "Mathieu Ravaut",
            "Hao Zhang",
            "Lu Xu",
            "Aixin Sun",
            "Yong Liu"
        ],
        "published": "2024",
        "summary": "Conversational recommender systems (CRS) aim to recommend relevant items to users by eliciting user preference through natural language conversation. Prior work often utilizes external knowledge graphs for items’ semantic information, a language model for dialogue generation, and a recommendation module for ranking relevant items. This combination of multiple components suffers from a cumber-some training process, and leads to semantic misalignment issues between dialogue generation and item recommendation. In this paper, we represent items in natural language and formulate CRS as a natural language processing task. Accordingly, we leverage the power of pre-trained language models to encode items, understand user intent via conversation, perform item recommendation through semantic matching, and generate dialogues. As a unified model, our PECRS (Parameter-Efficient CRS), can be optimized in a single stage, without relying on non-textual metadata such as a knowledge graph. Experiments on two benchmark CRS datasets, ReDial and INSPIRED, demonstrate the effectiveness of PECRS on recommendation and conversation. Our code is available at: https://github.com/Ravoxsg/efficient_unified_crs.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.9.pdf",
        "keywords": [
            "natural language processing",
            "conversational recommender system",
            "parameter efficient"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"semantic misalignment issues between dialogue generation and item recommendation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"semantic misalignment issues between dialogue generation and item recommendation.\""
    },
    {
        "title": "OpenPI2.0: An Improved Dataset for Entity Tracking in Texts",
        "authors": [
            "Li Zhang",
            "Hainiu Xu",
            "Abhinav Kommula",
            "Chris Callison-Burch",
            "Niket Tandon"
        ],
        "published": "2024",
        "summary": "Much texts describe a changing world (e.g., procedures, stories, newswires), and understanding them requires tracking how entities change. An earlier dataset, OpenPI, provided crowdsourced annotations of entity state changes in text. However, a major limitation was that those annotations were free-form and did not identify salient changes, hampering model evaluation. To overcome these limitations, we present an improved dataset, OpenPI2.0, where entities and attributes are fully canonicalized and additional entity salience annotations are added. On our fairer evaluation setting, we find that current state-of-the-art language models are far from competent. We also show that using state changes of salient entities as a chain-of-thought prompt, downstream performance is improved on tasks such as question answering and classical planning, outperforming the setting involving all related entities indiscriminately. We offer OpenPI2.0 for the continued development of models that can understand the dynamics of entities in text.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.10.pdf",
        "keywords": [
            "entity tracking"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"On our fairer evaluation setting, we find that current state-of-the-art language models are far from competent.\"\n\nThis rating is chosen because the abstract mentions a limitation of current state-of-the-art language models in a specific task (entity tracking in texts), but the primary focus of the paper is on introducing an improved dataset and a new approach to improve performance on this task.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"On our fairer evaluation setting, we find that current state-of-the-art language models are far from competent.\"\n\nThis rating is chosen because the abstract mentions a limitation of current state-of-the-art language models in a specific task (entity tracking in texts), but the primary focus of the paper is on introducing an improved dataset and a new approach to improve performance on this task."
    },
    {
        "title": "Few-Shot Data Synthesis for Open Domain Multi-Hop Question Answering",
        "authors": [
            "Mingda Chen",
            "Xilun Chen",
            "Wen-tau Yih"
        ],
        "published": "2024",
        "summary": "Few-shot learning for open domain multi-hop question answering typically relies on the in-context learning capability of large language models (LLMs). While powerful, these LLMs usually contain tens or hundreds of billions of parameters, making them rather inefficient at inference time. To improve performance of smaller language models, we propose a data synthesis framework for multi-hop question answering that requires less than 10 human-annotated question answer pairs. Our framework depends only on rich, naturally-occurring relationships among documents and is built upon the data generation functions parameterized by LLMs and prompts. We synthesize millions of multi-hop questions and claims to finetune language models, evaluated on popular benchmarks for multi-hop question answering and fact verification. Empirically, our approach improves model performance significantly, allowing the finetuned models to be competitive with GPT-3.5 based approaches while being almost one-third the size in parameter count.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.12.pdf",
        "keywords": [
            "multi hop question answering",
            "synthesis",
            "data synthesis"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While powerful, these LLMs usually contain tens or hundreds of billions of parameters, making them rather inefficient at inference time.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While powerful, these LLMs usually contain tens or hundreds of billions of parameters, making them rather inefficient at inference time.\""
    },
    {
        "title": "Language Models as Inductive Reasoners",
        "authors": [
            "Zonglin Yang",
            "Li Dong",
            "Xinya Du",
            "Hao Cheng",
            "Erik Cambria",
            "Xiaodong Liu",
            "Jianfeng Gao",
            "Furu Wei"
        ],
        "published": "2024",
        "summary": "Inductive reasoning is a core component of human intelligence. In the past research of inductive reasoning within computer science, formal language is used as representations of knowledge (facts and rules, more specifically). However, formal language can cause systematic problems for inductive reasoning such as disability of handling raw input such as natural language, sensitiveness to mislabeled data, and incapacity to handle ambiguous input. To this end, we propose a new paradigm (task) for inductive reasoning, which is to induce natural language rules from natural language facts, and create a dataset termed DEER containing 1.2k rule-fact pairs for the task, where rules and facts are written in natural language. New automatic metrics are also proposed and analysed for the evaluation of this task. With DEER, we investigate a modern approach for inductive reasoning where we use natural language as representation for knowledge instead of formal language and use pretrained language models as ”reasoners”. Moreover, we provide the first and comprehensive analysis of how well pretrained language models can induce natural language rules from natural language facts. We also propose a new framework drawing insights from philosophy literature for this task, which we show in the experiment section that surpasses baselines in both automatic and human evaluations. We discuss about our future perspectives for inductive reasoning in Section 7. Dataset and code are available at https://github.com/ZonglinY/Inductive_Reasoning.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.13.pdf",
        "keywords": [
            "inductive reasoning",
            "natural language",
            "formal language",
            "language models",
            "knowledge"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, formal language can cause systematic problems for inductive reasoning such as disability of handling raw input such as natural language, sensitiveness to mislabeled data, and incapacity to handle ambiguous input.\"\n\nNote that the paper mentions limitations of formal language, not LLMs directly. However, the paper discusses using LLMs as \"reasoners\" and evaluates their performance in in",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, formal language can cause systematic problems for inductive reasoning such as disability of handling raw input such as natural language, sensitiveness to mislabeled data, and incapacity to handle ambiguous input.\"\n\nNote that the paper mentions limitations of formal language, not LLMs directly. However, the paper discusses using LLMs as \"reasoners\" and evaluates their performance in in"
    },
    {
        "title": "SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects",
        "authors": [
            "David Adelani",
            "Hannah Liu",
            "Xiaoyu Shen",
            "Nikita Vassilyev",
            "Jesujoba Alabi",
            "Yanke Mao",
            "Haonan Gao",
            "En-Shiun Lee"
        ],
        "published": "2024",
        "summary": "Despite the progress in building multilingual language models, evaluation is often limited to a few languages with available datasets which excludes a large number of low-resource languages. In this paper, we create SIB-200—a large-scale open-sourced benchmark dataset for topic classification in 205 languages and dialects to address the lack of evaluation dataset for Natural Language Understanding (NLU). For many of the languages covered in SIB-200, this is the first publicly available evaluation dataset for NLU. The dataset is based on Flores-200 machine translation corpus. We annotated the English portion of the dataset and extended the sentence-level annotation to the remaining 204 languages covered in the corpus. Despite the simplicity of this task, our evaluation in full-supervised setting, cross-lingual transfer setting and prompting of large language model setting show that there is still a large gap between the performance of high-resource and low-resource languages when multilingual evaluation is scaled to numerous world languages. We found that languages unseen during the pre-training of multilingual language models, languages from under-represented families (like Nilotic and Altantic-Congo), and languages from the regions of Africa, Americas, Oceania and South East Asia, often have the lowest performance on our topic classification dataset. We hope our dataset %will encourages a more inclusive evaluation of multilingual language models on a more diverse set of languages.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.14.pdf",
        "keywords": [
            "languages",
            "topic classification",
            "language models",
            "natural language understanding",
            "resource languages",
            "evaluation",
            "world languages",
            "evaluation dataset",
            "dialects",
            "big evaluation dataset",
            "machine"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the simplicity of this task, our evaluation in full-supervised setting, cross-lingual transfer setting and prompting of large language model setting show that there is still a large gap between the performance of high-resource and low-resource languages when multilingual evaluation is scaled to numerous world languages.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Despite the simplicity of this task, our evaluation in full-supervised setting, cross-lingual transfer setting and prompting of large language model setting show that there is still a large gap between the performance of high-resource and low-resource languages when multilingual evaluation is scaled to numerous world languages.\""
    },
    {
        "title": "Like a Good Nearest Neighbor: Practical Content Moderation and Text Classification",
        "authors": [
            "Luke Bates",
            "Iryna Gurevych"
        ],
        "published": "2024",
        "summary": "Few-shot text classification systems have impressive capabilities but are infeasible to deploy and use reliably due to their dependence on prompting and billion-parameter language models. SetFit (Tunstall, 2022) is a recent, practical approach that fine-tunes a Sentence Transformer under a contrastive learning paradigm and achieves similar results to more unwieldy systems. Inexpensive text classification is important for addressing the problem of domain drift in all classification tasks, and especially in detecting harmful content, which plagues social media platforms. Here, we propose Like a Good Nearest Neighbor (LaGoNN), a modification to SetFit that introduces no learnable parameters but alters input text with information from its nearest neighbor, for example, the label and text, in the training data, making novel data appear similar to an instance on which the model was optimized. LaGoNN is effective at flagging undesirable content and text classification, and improves SetFit’s performance. To demonstrate LaGoNN’s value, we conduct a thorough study of text classification systems in the context of content moderation under four label distributions, and in general and multilingual classification settings.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.17.pdf",
        "keywords": [
            "classification",
            "text classification",
            "nearest neighbor",
            "content moderation",
            "good nearest neighbor",
            "multilingual classification"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Few-shot text classification systems have impressive capabilities but are infeasible to deploy and use reliably due to their dependence on prompting and billion-parameter language models.\"\n\nThis paper mentions a limitation of LLMs (dependence on prompting and large parameters) but only briefly, and the primary focus is on proposing a modification to a practical approach for text classification.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Few-shot text classification systems have impressive capabilities but are infeasible to deploy and use reliably due to their dependence on prompting and billion-parameter language models.\"\n\nThis paper mentions a limitation of LLMs (dependence on prompting and large parameters) but only briefly, and the primary focus is on proposing a modification to a practical approach for text classification."
    },
    {
        "title": "Zero-shot Sentiment Analysis in Low-Resource Languages Using a Multilingual Sentiment Lexicon",
        "authors": [
            "Fajri Koto",
            "Tilman Beck",
            "Zeerak Talat",
            "Iryna Gurevych",
            "Timothy Baldwin"
        ],
        "published": "2024",
        "summary": "Improving multilingual language models capabilities in low-resource languages is generally difficult due to the scarcity of large-scale data in those languages. In this paper, we relax the reliance on texts in low-resource languages by using multilingual lexicons in pretraining to enhance multilingual capabilities. Specifically, we focus on zero-shot sentiment analysis tasks across 34 languages, including 6 high/medium-resource languages, 25 low-resource languages, and 3 code-switching datasets. We demonstrate that pretraining using multilingual lexicons, without using any sentence-level sentiment data, achieves superior zero-shot performance compared to models fine-tuned on English sentiment datasets, and large language models like GPT–3.5, BLOOMZ, and XGLM. These findings are observable for unseen low-resource languages to code-mixed scenarios involving high-resource languages.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.18.pdf",
        "keywords": [
            "resource languages",
            "low resource languages",
            "language models",
            "multilingual sentiment lexicon",
            "multilingual language models",
            "multilingual",
            "zero shot sentiment analysis",
            "high medium resource languages"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Improving multilingual language models capabilities in low-resource languages is generally difficult due to the scarcity of large-scale data in those languages.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Improving multilingual language models capabilities in low-resource languages is generally difficult due to the scarcity of large-scale data in those languages.\""
    },
    {
        "title": "CEAN: Contrastive Event Aggregation Network with LLM-based Augmentation for Event Extraction",
        "authors": [
            "Zihao Meng",
            "Tao Liu",
            "Heng Zhang",
            "Kai Feng",
            "Peng Zhao"
        ],
        "published": "2024",
        "summary": "Event Extraction is a crucial yet arduous task in natural language processing (NLP), as its performance is significantly hindered by laborious data annotation. Given this challenge, recent research has predominantly focused on two approaches: pretraining task-oriented models for event extraction and employing data augmentation techniques. These methods involve integrating external knowledge, semantic structures, or artificially generated samples using large language models (LLMs). However, their performances can be compromised due to two fundamental issues. Firstly, the alignment between the introduced knowledge and event extraction knowledge is crucial. Secondly, the introduction of data noise during the augmentation is unavoidable and can mislead the model’s convergence. To address these issues, we propose a Contrastive Event Aggregation Network with LLM-based Augmentation to promote low-resource learning and reduce data noise for event extraction. Different from the existing methods introducing linguistic knowledge into data augmentation, an event aggregation network is established to introduce event knowledge into supervised learning by constructing adaptively-updated semantic representation for trigger and argument. For LLM-based augmentation, we design a new scheme including a multi-pattern rephrasing paradigm and a data-free composing paradigm. Instead of directly using augmentation samples in the supervised task, we introduce span-level contrastive learning to reduce data noise. Experiments on the ACE2005 and ERE-EN demonstrate that our proposed approach achieves new state-of-the-art results on both of the two datasets.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.19.pdf",
        "keywords": [
            "event extraction",
            "event aggregation",
            "data augmentation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their performances can be compromised due to two fundamental issues. Firstly, the alignment between the introduced knowledge and event extraction knowledge is crucial. Secondly, the introduction of data noise during the augmentation is unavoidable and can mislead the model’s convergence.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, their performances can be compromised due to two fundamental issues. Firstly, the alignment between the introduced knowledge and event extraction knowledge is crucial. Secondly, the introduction of data noise during the augmentation is unavoidable and can mislead the model’s convergence.\""
    },
    {
        "title": "EXPLORER: Exploration-guided Reasoning for Textual Reinforcement Learning",
        "authors": [
            "Kinjal Basu",
            "Keerthiram Murugesan",
            "Subhajit Chaudhury",
            "Murray Campbell",
            "Kartik Talamadupula",
            "Tim Klinger"
        ],
        "published": "2024",
        "summary": "Text-based games (TBGs) have emerged as an important collection of NLP tasks, requiring reinforcement learning (RL) agents to combine natural language understanding with reasoning. A key challenge for agents attempting to solve such tasks is to generalize across multiple games and demonstrate good performance on both seen and unseen objects. Purely deep-RL-based approaches may perform well on seen objects; however, they fail to showcase the same performance on unseen objects. Commonsense-infused deep-RL agents may work better on unseen data; unfortunately, their policies are often not interpretable or easily transferable. To tackle these issues, in this paper, we present EXPLORER which is an exploration-guided reasoning agent for textual reinforcement learning. EXPLORER is neuro-symbolic in nature, as it relies on a neural module for exploration and a symbolic module for exploitation. It can also learn generalized symbolic policies and perform well over unseen data. Our experiments show that EXPLORER outperforms the baseline agents on Text-World cooking (TW-Cooking) and Text-World Commonsense (TWC) games.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.24.pdf",
        "keywords": [
            "reinforcement learning",
            "exploration guided reasoning",
            "text based games",
            "textual reinforcement learning",
            "exploration guided reasoning agent"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents",
        "authors": [
            "Simeng Sun",
            "Yang Liu",
            "Shuohang Wang",
            "Dan Iter",
            "Chenguang Zhu",
            "Mohit Iyyer"
        ],
        "published": "2024",
        "summary": "Strategies such as chain-of-thought prompting improve the performance of large language models (LLMs) on complex reasoning tasks by decomposing input examples into intermediate steps. However, it remains unclear how to apply such methods to reason over long input documents, in which both the decomposition and the output of each intermediate step are non-trivial to obtain. In this work, we propose PEARL, a prompting framework to improve reasoning over long documents, which consists of three stages: action mining, plan formulation, and plan execution. More specifically, given a question about a long document, PEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE, FIND_EVENT, FIND_RELATION) and then executes them over the document to obtain the answer. Each stage of PEARL is implemented via zero-shot or few-shot prompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate PEARL on a challenging subset of the QuALITY dataset, which contains questions that require complex reasoning over long narrative texts. PEARL outperforms zero-shot and chain-of-thought prompting on this dataset, and ablation experiments show that each stage of PEARL is critical to its performance. Overall, PEARL is a first step towards leveraging LLMs to reason over long documents.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.29.pdf",
        "keywords": [
            "language",
            "prompting",
            "action mining",
            "input examples",
            "language models",
            "plan",
            "plan formulation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it remains unclear how to apply such methods to reason over long input documents, in which both the decomposition and the output of each intermediate step are non-trivial to obtain.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, it remains unclear how to apply such methods to reason over long input documents, in which both the decomposition and the output of each intermediate step are non-trivial to obtain.\""
    },
    {
        "title": "LAraBench: Benchmarking Arabic AI with Large Language Models",
        "authors": [
            "Ahmed Abdelali",
            "Hamdy Mubarak",
            "Shammur Chowdhury",
            "Maram Hasanain",
            "Basel Mousi",
            "Sabri Boughorbel",
            "Samir Abdaljalil",
            "Yassine El Kheir",
            "Daniel Izham",
            "Fahim Dalvi",
            "Majd Hawasly",
            "Nizi Nazar",
            "Youssef Elshahawy",
            "Ahmed Ali",
            "Nadir Durrani",
            "Natasa Milic-Frayling",
            "Firoj Alam"
        ],
        "published": "2024",
        "summary": "Recent advancements in Large Language Models (LLMs) have significantly influenced the landscape of language and speech research. Despite this progress, these models lack specific benchmarking against state-of-the-art (SOTA) models tailored to particular languages and tasks. LAraBench addresses this gap for Arabic Natural Language Processing (NLP) and Speech Processing tasks, including sequence tagging and content classification across different domains. We utilized models such as GPT-3.5-turbo, GPT-4, BLOOMZ, Jais-13b-chat, Whisper, and USM, employing zero and few-shot learning techniques to tackle 33 distinct tasks across 61 publicly available datasets. This involved 98 experimental setups, encompassing ~296K data points, ~46 hours of speech, and 30 sentences for Text-to-Speech (TTS). This effort resulted in 330+ sets of experiments. Our analysis focused on measuring the performance gap between SOTA models and LLMs. The overarching trend observed was that SOTA models generally outperformed LLMs in zero-shot learning, with a few exceptions. Notably, larger computational models with few-shot learning techniques managed to reduce these performance gaps. Our findings provide valuable insights into the applicability of LLMs for Arabic NLP and speech processing tasks.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.30.pdf",
        "keywords": [
            "language models",
            "benchmarking",
            "tts",
            "arabic nlp",
            "arabic natural"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The overarching trend observed was that SOTA models generally outperformed LLMs in zero-shot learning, with a few exceptions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"The overarching trend observed was that SOTA models generally outperformed LLMs in zero-shot learning, with a few exceptions.\""
    },
    {
        "title": "Finding a Needle in the Adversarial Haystack: A Targeted Paraphrasing Approach For Uncovering Edge Cases with Minimal Distribution Distortion",
        "authors": [
            "Aly Kassem",
            "Sherif Saad"
        ],
        "published": "2024",
        "summary": "Adversarial attacks against Language models (LMs) are a significant concern. In particular, adversarial samples exploit the model’s sensitivity to small input changes. While these changes appear insignificant on the semantics of the input sample, they result in significant decay in model performance. In this paper, we propose Targeted Paraphrasing via RL (TPRL), an approach to automatically learn a policy to generate challenging samples that improve the model’s performance. TPRL leverages FLAN-T5, a language model, as a generator and employs a self-learned policy using a proximal policy optimization to generate the adversarial examples automatically. TPRL’s reward is based on the confusion induced in the classifier, preserving the original text meaning through a Mutual Implication score. We demonstrate & evaluate TPRL’s effectiveness in discovering natural adversarial attacks and improving model performance through extensive experiments on four diverse NLP classification tasks via Automatic & Human evaluation. TPRL outperforms strong baselines, exhibits generalizability across classifiers and datasets, and combines the strengths of language modeling and reinforcement learning to generate diverse and influential adversarial examples.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.33.pdf",
        "keywords": [
            "adversarial haystack",
            "adversarial attacks",
            "language model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Adversarial attacks against Language models (LMs) are a significant concern. In particular, adversarial samples exploit the model’s sensitivity to small input changes. While these changes appear insignificant on the semantics of the input sample, they result in significant decay in model performance.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Adversarial attacks against Language models (LMs) are a significant concern. In particular, adversarial samples exploit the model’s sensitivity to small input changes. While these changes appear insignificant on the semantics of the input sample, they result in significant decay in model performance.\""
    },
    {
        "title": "NNOSE: Nearest Neighbor Occupational Skill Extraction",
        "authors": [
            "Mike Zhang",
            "Rob van der Goot",
            "Min-Yen Kan",
            "Barbara Plank"
        ],
        "published": "2024",
        "summary": "The labor market is changing rapidly, prompting increased interest in the automatic extraction of occupational skills from text. With the advent of English benchmark job description datasets, there is a need for systems that handle their diversity well. We tackle the complexity in occupational skill datasets tasks—combining and leveraging multiple datasets for skill extraction, to identify rarely observed skills within a dataset, and overcoming the scarcity of skills across datasets. In particular, we investigate the retrieval-augmentation of language models, employing an external datastore for retrieving similar skills in a dataset-unifying manner. Our proposed method, Nearest Neighbor Occupational Skill Extraction (NNOSE) effectively leverages multiple datasets by retrieving neighboring skills from other datasets in the datastore. This improves skill extraction without additional fine-tuning. Crucially, we observe a performance gain in predicting infrequent patterns, with substantial gains of up to 30% span-F1 in cross-dataset settings.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.35.pdf",
        "keywords": [
            "skills",
            "occupational skills",
            "skill extraction",
            "language models",
            "labor market",
            "nnose"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"retrieval-augmentation of language models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"retrieval-augmentation of language models\""
    },
    {
        "title": "MAFIA: Multi-Adapter Fused Inclusive Language Models",
        "authors": [
            "Prachi Jain",
            "Ashutosh Sathe",
            "Varun Gumma",
            "Kabir Ahuja",
            "Sunayana Sitaram"
        ],
        "published": "2024",
        "summary": "Pretrained Language Models (PLMs) are widely used in NLP for various tasks. Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases. However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion. Moreover, the methods typically involve finetuning the full model in order to maintain the performance on the downstream task. In this work, we aim to modularly debias a pre-trained language model across multiple dimensions. Previous works extensively explored debiasing PLMs by using limited US-centric counterfactual data augmentation (CDA). We use structured knowledge and a large generative model to build a diverse CDA across multiple bias dimensions in a semi-automated way. We highlight how existing debiasing methods do not consider interactions between multiple societal biases and propose a debiasing model that exploits the synergy amongst various societal biases and enables multi-bias debiasing simultaneously. An extensive evaluation on multiple tasks and languages demonstrates the efficacy of the approach.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.37.pdf",
        "keywords": [
            "language models",
            "inclusive language models",
            "multi adapter",
            "counterfactual data augmentation",
            "diverse"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion.\""
    },
    {
        "title": "Generation-driven Contrastive Self-training for Zero-shot Text Classification with Instruction-following LLM",
        "authors": [
            "Ruohong Zhang",
            "Yau-Shian Wang",
            "Yiming Yang"
        ],
        "published": "2024",
        "summary": "The remarkable performance of large language models (LLMs) in zero-shot language understanding has garnered significant attention.However, employing LLMs for large-scale inference or domain-specific fine-tuning requires immense computational resources due to their substantial model size. To overcome these limitations, we introduce a novel method, namely GenCo, which leverages the strong generative power of LLMs to assist in training a smaller and more adaptable language model. In our method, an LLM plays an important role in the self-training loop of a smaller model in two important ways. Firstly, we utilize an LLM to generate multiple augmented texts for each input instance to enhance its semantic meaning for better understanding. Secondly, we additionally generate high-quality training instances conditioned on predicted labels, ensuring the generated texts are relevant to the labels. In this way, GenCo not only corrects the errors of predicted labels during self-training but also eliminates the need for extensive unlabeled texts. In our experiments, GenCo outperforms previous state-of-the-art methods when only limited (<5% of original) in-domain text data is available. Notably, our approach surpasses Alpaca-7B with human instructions, highlighting the significance of self-training.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.39.pdf",
        "keywords": [
            "zero shot text classification",
            "self training"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"employing LLMs for large-scale inference or domain-specific fine-tuning requires immense computational resources due to their substantial model size.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"employing LLMs for large-scale inference or domain-specific fine-tuning requires immense computational resources due to their substantial model size.\""
    },
    {
        "title": "Examining Gender and Racial Bias in Large Vision–Language Models Using a Novel Dataset of Parallel Images",
        "authors": [
            "Kathleen Fraser",
            "Svetlana Kiritchenko"
        ],
        "published": "2024",
        "summary": "Following on recent advances in large language models (LLMs) and subsequent chat models, a new wave of large vision–language models (LVLMs) has emerged. Such models can incorporate images as input in addition to text, and perform tasks such as visual question answering, image captioning, story generation, etc. Here, we examine potential gender and racial biases in such systems, based on the perceived characteristics of the people in the input images. To accomplish this, we present a new dataset PAIRS (PArallel Images for eveRyday Scenarios). The PAIRS dataset contains sets of AI-generated images of people, such that the images are highly similar in terms of background and visual content, but differ along the dimensions of gender (man, woman) and race (Black, white). By querying the LVLMs with such images, we observe significant differences in the responses according to the perceived gender or race of the person depicted.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.41.pdf",
        "keywords": [
            "parallel images",
            "vision–language",
            "racial bias"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"By querying the LVLMs with such images, we observe significant differences in the responses according to the perceived gender or race of the person depicted.\"\n\nThis paper extensively analyzes biases in LVLMs, which are a type of LLMs, underlining a major limitation of these models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"By querying the LVLMs with such images, we observe significant differences in the responses according to the perceived gender or race of the person depicted.\"\n\nThis paper extensively analyzes biases in LVLMs, which are a type of LLMs, underlining a major limitation of these models."
    },
    {
        "title": "ConstraintChecker: A Plugin for Large Language Models to Reason on Commonsense Knowledge Bases",
        "authors": [
            "Quyet V. Do",
            "Tianqing Fang",
            "Shizhe Diao",
            "Zhaowei Wang",
            "Yangqiu Song"
        ],
        "published": "2024",
        "summary": "Reasoning over Commonsense Knowledge Bases (CSKB), i.e. CSKB reasoning, has been explored as a way to acquire new commonsense knowledge based on reference knowledge in the original CSKBs and external prior knowledge.Despite the advancement of Large Language Models (LLM) and prompt engineering techniques in various reasoning tasks, they still struggle to deal with CSKB reasoning.One of the problems is that it is hard for them to acquire explicit relational constraints in CSKBs from only in-context exemplars, due to a lack of symbolic reasoning capabilities (CITATION).To this end, we proposed **ConstraintChecker**, a plugin over prompting techniques to provide and check explicit constraints.When considering a new knowledge instance, ConstraintChecker employs a rule-based module to produce a list of constraints, then it uses a zero-shot learning module to check whether this knowledge instance satisfies all constraints.The acquired constraint-checking result is then aggregated with the output of the main prompting technique to produce the final output.Experimental results on CSKB Reasoning benchmarks demonstrate the effectiveness of our method by bringing consistent improvements over all prompting methods.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.42.pdf",
        "keywords": [
            "commonsense",
            "large language models",
            "commonsense knowledge bases",
            "prompting technique"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"they still struggle to deal with CSKB reasoning... due to a lack of symbolic reasoning capabilities\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"they still struggle to deal with CSKB reasoning... due to a lack of symbolic reasoning capabilities\""
    },
    {
        "title": "Comparing Template-based and Template-free Language Model Probing",
        "authors": [
            "Sagi Shaier",
            "Kevin Bennett",
            "Lawrence Hunter",
            "Katharina von der Wense"
        ],
        "published": "2024",
        "summary": "The differences between cloze-task language model (LM) probing with 1) expert-made templates and 2) naturally-occurring text have often been overlooked. Here, we evaluate 16 different LMs on 10 probing English datasets – 4 template-based and 6 template-free – in general and biomedical domains to answer the following research questions: (RQ1) Do model rankings differ between the two approaches? (RQ2) Do models’ absolute scores differ between the two approaches? (RQ3) Do the answers to RQ1 and RQ2 differ between general and domain-specific models? Our findings are: 1) Template-free and template-based approaches often rank models differently, except for the top domain- specific models. 2) Scores decrease by up to 42% Acc@1 when comparing parallel template-free and template-based prompts. 3) Perplexity is negatively correlated with accuracy in the template-free approach, but, counter-intuitively, they are positively correlated for template-based probing. 4) Models tend to predict the same answers frequently across prompts for template-based probing, which is less common when employing template-free techniques.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.46.pdf",
        "keywords": [
            "template free language model",
            "free",
            "template free",
            "task language model",
            "template based probing"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Scores decrease by up to 42% Acc@1 when comparing parallel template-free and template-based prompts.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Scores decrease by up to 42% Acc@1 when comparing parallel template-free and template-based prompts.\""
    },
    {
        "title": "Scaling up Discovery of Latent Concepts in Deep NLP Models",
        "authors": [
            "Majd Hawasly",
            "Fahim Dalvi",
            "Nadir Durrani"
        ],
        "published": "2024",
        "summary": "Despite the revolution caused by deep NLP models, they remain black boxes, necessitating research to understand their decision-making processes. A recent work by Dalvi et al. (2022) carried out representation analysis through the lens of clustering latent spaces within pre-trained models (PLMs), but that approach is limited to small scale due to the high cost of running Agglomerative hierarchical clustering. This paper studies clustering algorithms in order to scale the discovery of encoded concepts in PLM representations to larger datasets and models. We propose metrics for assessing the quality of discovered latent concepts and use them to compare the studied clustering algorithms. We found that K-Means-based concept discovery significantly enhances efficiency while maintaining the quality of the obtained concepts. Furthermore, we demonstrate the practicality of this newfound efficiency by scaling latent concept discovery to LLMs and phrasal concepts.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.48.pdf",
        "keywords": [
            "clustering latent spaces",
            "latent concept discovery",
            "concept discovery",
            "clustering",
            "scaling up discovery",
            "pre trained models",
            "clustering algorithms",
            "nlp"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the revolution caused by deep NLP models, they remain black boxes, necessitating research to understand their decision-making processes.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite the revolution caused by deep NLP models, they remain black boxes, necessitating research to understand their decision-making processes.\""
    },
    {
        "title": "AnthroScore: A Computational Linguistic Measure of Anthropomorphism",
        "authors": [
            "Myra Cheng",
            "Kristina Gligoric",
            "Tiziano Piccardi",
            "Dan Jurafsky"
        ],
        "published": "2024",
        "summary": "Anthropomorphism, or the attribution of human-like characteristics to non-human entities, has shaped conversations about the impacts and possibilities of technology. We present AnthroScore, an automatic metric of implicit anthropomorphism in language. We use a masked language model to quantify how non-human entities are implicitly framed as human by the surrounding context. We show that AnthroScore corresponds with human judgments of anthropomorphism and dimensions of anthropomorphism described in social science literature. Motivated by concerns of misleading anthropomorphism in computer science discourse, we use AnthroScore to analyze 15 years of research papers and downstream news articles. In research papers, we find that anthropomorphism has steadily increased over time, and that papers related to language models have the most anthropomorphism. Within ACL papers, temporal increases in anthropomorphism are correlated with key neural advancements. Building upon concerns of scientific misinformation in mass media, we identify higher levels of anthropomorphism in news headlines compared to the research papers they cite. Since AnthroScore is lexicon-free, it can be directly applied to a wide range of text sources.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.49.pdf",
        "keywords": [
            "anthropomorphism",
            "computational linguistic measure"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Motivated by concerns of misleading anthropomorphism in computer science discourse, we use AnthroScore to analyze 15 years of research papers and downstream news articles.\"\n\nThis abstract mentions a limitation of LLMs (misleading anthropomorphism) but does not elaborate on it in detail.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Motivated by concerns of misleading anthropomorphism in computer science discourse, we use AnthroScore to analyze 15 years of research papers and downstream news articles.\"\n\nThis abstract mentions a limitation of LLMs (misleading anthropomorphism) but does not elaborate on it in detail."
    },
    {
        "title": "Unsupervised Contrast-Consistent Ranking with Language Models",
        "authors": [
            "Niklas Stoehr",
            "Pengxiang Cheng",
            "Jing Wang",
            "Daniel Preotiuc-Pietro",
            "Rajarshi Bhowmik"
        ],
        "published": "2024",
        "summary": "Language models contain ranking-based knowledge and are powerful solvers of in-context ranking tasks. For instance, they may have parametric knowledge about the ordering of countries by size or may be able to rank product reviews by sentiment. We compare pairwise, pointwise and listwise prompting techniques to elicit a language model’s ranking knowledge. However, we find that even with careful calibration and constrained decoding, prompting-based techniques may not always be self-consistent in the rankings they produce. This motivates us to explore an alternative approach that is inspired by an unsupervised probing method called Contrast-Consistent Search (CCS). The idea is to train a probe guided by a logical constraint: a language model’s representation of a statement and its negation must be mapped to contrastive true-false poles consistently across multiple statements. We hypothesize that similar constraints apply to ranking tasks where all items are related via consistent, pairwise or listwise comparisons. To this end, we extend the binary CCS method to Contrast-Consistent Ranking (CCR) by adapting existing ranking methods such as the Max-Margin Loss, Triplet Loss and an Ordinal Regression objective. Across different models and datasets, our results confirm that CCR probing performs better or, at least, on a par with prompting.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.54.pdf",
        "keywords": [
            "consistent ranking",
            "ranking",
            "contrast consistent search",
            "language models",
            "unsupervised contrast consistent ranking",
            "context ranking",
            "unsupervised probing",
            "regression"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, we find that even with careful calibration and constrained decoding, prompting-based techniques may not always be self-consistent in the rankings they produce.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, we find that even with careful calibration and constrained decoding, prompting-based techniques may not always be self-consistent in the rankings they produce.\""
    },
    {
        "title": "Entity-level Factual Adaptiveness of Fine-tuning based Abstractive Summarization Models",
        "authors": [
            "Jongyoon Song",
            "Nohil Park",
            "Bongkyu Hwang",
            "Jaewoong Yun",
            "Seongho Joe",
            "Youngjune Gwon",
            "Sungroh Yoon"
        ],
        "published": "2024",
        "summary": "Abstractive summarization models often generate factually inconsistent content particularly when the parametric knowledge of the model conflicts with the knowledge in the input document. In this paper, we analyze the robustness of fine-tuning based summarization models to the knowledge conflict, which we call factual adaptiveness. We utilize pre-trained language models to construct evaluation sets and find that factual adaptiveness is not strongly correlated with factual consistency on original datasets. Furthermore, we introduce a controllable counterfactual data augmentation method where the degree of knowledge conflict within the augmented data can be adjustable. Our experimental results on two pre-trained language models (PEGASUS and BART) and two fine-tuning datasets (XSum and CNN/DailyMail) demonstrate that our method enhances factual adaptiveness while achieving factual consistency on original datasets on par with the contrastive learning baseline.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.55.pdf",
        "keywords": [
            "summarization",
            "factual adaptiveness",
            "fine tuning",
            "factual consistency",
            "knowledge conflict",
            "abstractive summarization"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Abstractive summarization models often generate factually inconsistent content particularly when the parametric knowledge of the model conflicts with the knowledge in the input document.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (generating factually inconsistent content) but only briefly and does not explore it in depth, focusing instead on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Abstractive summarization models often generate factually inconsistent content particularly when the parametric knowledge of the model conflicts with the knowledge in the input document.\"\n\nThis rating is given because the abstract mentions a limitation of LLMs (generating factually inconsistent content) but only briefly and does not explore it in depth, focusing instead on the proposed solution."
    },
    {
        "title": "LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions",
        "authors": [
            "Minghao Wu",
            "Abdul Waheed",
            "Chiyu Zhang",
            "Muhammad Abdul-Mageed",
            "Alham Fikri Aji"
        ],
        "published": "2024",
        "summary": "Large language models (LLMs) with instruction fine-tuning demonstrate superior generative capabilities. However, these models are resource-intensive. To alleviate this issue, we explore distilling knowledge from instruction-tuned LLMs into much smaller ones. While other similar works have been done, they are often conducted on a limited set of (usually still large) models and are not accompanied by proper evaluations. To this end, we carefully develop a large set of 2.58M instructions based on both existing and newly-generated instructions. In addition to being sizable, we design our instructions to cover a broad set of topics to ensure diversity. Extensive analysis of our instruction dataset confirms its diversity, and we generate responses for these instructions using gpt-3.5-turbo. Leveraging these instructions, we fine-tune a diverse herd of models, collectively referred to as LaMini-LM, which includes models from both the encoder-decoder and decoder-only families, with varying sizes. We evaluate the performance of our models using automatic metrics on 15 different natural language processing (NLP) benchmarks, as well as through human assessment. We also assess the model for hallucination and toxicity, and for the former, we introduce a new benchmark dataset for hallucination-inducing QA. The results demonstrate that our proposed LaMini-LM models are comparable to strong baselines while being much smaller in size.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.57.pdf",
        "keywords": [
            "hallucination",
            "lamini lm",
            "diverse herd",
            "large language models",
            "instruction tuned"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these models are resource-intensive.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these models are resource-intensive.\""
    },
    {
        "title": "Cross-Lingual Transfer from Related Languages: Treating Low-Resource Maltese as Multilingual Code-Switching",
        "authors": [
            "Kurt Micallef",
            "Nizar Habash",
            "Claudia Borg",
            "Fadhl Eryani",
            "Houda Bouamor"
        ],
        "published": "2024",
        "summary": "Although multilingual language models exhibit impressive cross-lingual transfer capabilities on unseen languages, the performance on downstream tasks is impacted when there is a script disparity with the languages used in the multilingual model’s pre-training data. Using transliteration offers a straightforward yet effective means to align the script of a resource-rich language with a target language thereby enhancing cross-lingual transfer capabilities. However, for mixed languages, this approach is suboptimal, since only a subset of the language benefits from the cross-lingual transfer while the remainder is impeded. In this work, we focus on Maltese, a Semitic language, with substantial influences from Arabic, Italian, and English, and notably written in Latin script. We present a novel dataset annotated with word-level etymology. We use this dataset to train a classifier that enables us to make informed decisions regarding the appropriate processing of each token in the Maltese language. We contrast indiscriminate transliteration or translation to mixing processing pipelines that only transliterate words of Arabic origin, thereby resulting in text with a mixture of scripts. We fine-tune the processed data on four downstream tasks and show that conditional transliteration based on word etymology yields the best results, surpassing fine-tuning with raw Maltese or Maltese processed with non-selective pipelines.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.61.pdf",
        "keywords": [
            "multilingual language models",
            "transliteration",
            "semitic language",
            "conditional transliteration",
            "code switching"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although multilingual language models exhibit impressive cross-lingual transfer capabilities on unseen languages, the performance on downstream tasks is impacted when there is a script disparity with the languages used in the multilingual model’s pre-training data.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although multilingual language models exhibit impressive cross-lingual transfer capabilities on unseen languages, the performance on downstream tasks is impacted when there is a script disparity with the languages used in the multilingual model’s pre-training data.\""
    },
    {
        "title": "Where Do We Go From Here? Multi-scale Allocentric Relational Inferencefrom Natural Spatial Descriptions",
        "authors": [
            "Tzuf Paz-Argaman",
            "John Palowitch",
            "Sayali Kulkarni",
            "Jason Baldridge",
            "Reut Tsarfaty"
        ],
        "published": "2024",
        "summary": "The concept of acquired spatial knowledge is crucial in spatial cognitive research, particularly when it comes to communicating routes. However, NLP navigation studies often overlook the impact of acquired knowledge on textual descriptions. Current navigation studies concentrate on egocentric local descriptions (e.g., ‘it will be on your right’) that require reasoning over the agent’s local perception. These instructions are typically given in a sequence of steps, with each action-step explicitly mentioned and followed by a landmark that the agent can use to verify that they are on the correct path (e.g., ‘turn right and then you will see...’). In contrast, descriptions based on knowledge acquired through a map provide a complete view of the environment and capture its compositionality. These instructions typically contain allocentric relations, are non-sequential, with implicit actions and multiple spatial relations without any verification (e.g., ‘south of Central Park and a block north of a police station’). This paper introduces the Rendezvous (RVS) task and dataset, which includes 10,404 examples of English geospatial instructions for reaching a target location using map-knowledge. Our analysis reveals that RVS exhibits a richer use of spatial allocentric relations, and requires resolving more spatial relations simultaneously compared to previous text-based navigation benchmarks.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.62.pdf",
        "keywords": [
            "knowledge"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of LLMs."
    },
    {
        "title": "REFINER: Reasoning Feedback on Intermediate Representations",
        "authors": [
            "Debjit Paul",
            "Mete Ismayilzada",
            "Maxime Peyrard",
            "Beatriz Borges",
            "Antoine Bosselut",
            "Robert West",
            "Boi Faltings"
        ],
        "published": "2024",
        "summary": "Language models (LMs) have recently shown remarkable performance on reasoning tasks by explicitly generating intermediate inferences,e.g., chain-of-thought prompting. However, these intermediate inference steps may be inappropriate deductions from the initial contextand lead to incorrect final predictions. Here we introduce REFINER, a framework for finetuning LMs to explicitly generate intermediate reasoning steps while interacting with a critic model that provides automated feedback on the reasoning. Specifically, the critic provides structured feedback that the reasoning LM uses to iteratively improve its intermediate arguments. Empirical evaluations of REFINER on three diverse reasoning tasks show significant improvements over baseline LMs of comparable scale. Furthermore, when using GPT-3.5 or ChatGPT as the reasoner, the trained critic significantly improves reasoning without finetuning the reasoner. Finally, our critic model is trained without expensive human-in-the-loop data but can be substituted with humans at inference time.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.67.pdf",
        "keywords": [
            "reasoner",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these intermediate inference steps may be inappropriate deductions from the initial context and lead to incorrect final predictions.\"\n\nThis abstract mentions a limitation of LLMs (generating intermediate inference steps that may be inappropriate deductions) in passing, but does not elaborate on it further and focuses on the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, these intermediate inference steps may be inappropriate deductions from the initial context and lead to incorrect final predictions.\"\n\nThis abstract mentions a limitation of LLMs (generating intermediate inference steps that may be inappropriate deductions) in passing, but does not elaborate on it further and focuses on the proposed solution."
    },
    {
        "title": "HumBEL: A Human-in-the-Loop Approach for Evaluating Demographic Factors of Language Models in Human-Machine Conversations",
        "authors": [
            "Anthony Sicilia",
            "Jennifer Gates",
            "Malihe Alikhani"
        ],
        "published": "2024",
        "summary": "While demographic factors like age and gender change the way people talk, and in particular, the way people talk to machines, there is little investigation into how large pre-trained language models (LMs) can adapt to these changes. To remedy this gap, we consider how demographic factors in LM language skills can be measured to determine compatibility with a target demographic. We suggest clinical techniques from Speech Language Pathology, which has norms for acquisition of language skills in humans. We conduct evaluation with a domain expert (i.e., a clinically licensed speech language pathologist), and also propose automated techniques to complement clinical evaluation at scale. Empirically, we focus on age, finding LM capability varies widely depending on task: GPT-3.5 mimics the ability of humans ranging from age 6-15 at tasks requiring inference, and simultaneously, outperforms a typical 21 year old at memorization. GPT-3.5 also has trouble with social language use, exhibiting less than 50% of the tested pragmatic skills. Findings affirm the importance of considering demographic alignment and conversational goals when using LMs as public-facing tools. Code, data, and a package will be available.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.68.pdf",
        "keywords": [
            "language",
            "language models",
            "demographic factors",
            "human machine conversations",
            "social language",
            "speech language"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"GPT-3.5 also has trouble with social language use, exhibiting less than 50% of the tested pragmatic skills.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"GPT-3.5 also has trouble with social language use, exhibiting less than 50% of the tested pragmatic skills.\""
    },
    {
        "title": "LOCOST: State-Space Models for Long Document Abstractive Summarization",
        "authors": [
            "Florian Le Bronnec",
            "Song Duong",
            "Mathieu Ravaut",
            "Alexandre Allauzen",
            "Nancy Chen",
            "Vincent Guigue",
            "Alberto Lumbreras",
            "Laure Soulier",
            "Patrick Gallinari"
        ],
        "published": "2024",
        "summary": "State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of 𝒪(L log L), this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.69.pdf",
        "keywords": [
            "summarization",
            "document",
            "locost",
            "state space",
            "inference",
            "performance level",
            "conditional text generation",
            "state space models",
            "book summarization"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Improving Generalization in Semantic Parsing by Increasing Natural Language Variation",
        "authors": [
            "Irina Saparina",
            "Mirella Lapata"
        ],
        "published": "2024",
        "summary": "Text-to-SQL semantic parsing has made significant progress in recent years, with various models demonstrating impressive performance on the challenging Spider benchmark. However, it has also been shown that these models often struggle to generalize even when faced with small perturbations of previously (accurately) parsed expressions. This is mainly due to the linguistic form of questions in Spider which are overly specific, unnatural, and display limited variation. In this work, we use data augmentation to enhance the robustness of text-to-SQL parsers against natural language variations. Existing approaches generate question reformulations either via models trained on Spider or only introduce local changes. In contrast, we leverage the capabilities of large language models to generate more realistic and diverse questions. Using only a few prompts, we achieve a two-fold increase in the number of questions in Spider. Training on this augmented dataset yields substantial improvements on a range of evaluation sets, including robustness benchmarks and out-of-domain data.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.71.pdf",
        "keywords": [
            "semantic parsing",
            "sql parsers",
            "robustness benchmarks",
            "language",
            "generalization"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This is mainly due to the linguistic form of questions in Spider which are overly specific, unnatural, and display limited variation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"This is mainly due to the linguistic form of questions in Spider which are overly specific, unnatural, and display limited variation.\""
    },
    {
        "title": "Text-to-Code Generation with Modality-relative Pre-training",
        "authors": [
            "Fenia Christopoulou",
            "Guchun Zhang",
            "Gerasimos Lampouras"
        ],
        "published": "2024",
        "summary": "Large pre-trained language models have recently been expanded and applied to programming language tasks with great success, often through further pre-training of a strictly-natural language model–where training sequences typically contain both natural and (linearised) programming language. Such approaches effectively map both modalities of the sequence into the same embedding space. However, programming language keywords (e.g. “while”) often have very strictly defined semantics. As such, transfer learning from their natural language usage may not necessarily be beneficial to their code application and vise versa. Assuming an already pre-trained language model, in this work we investigate how sequence tokens can be adapted and represented differently, depending on which modality they belong to, and to the ultimate benefit of the downstream task. We experiment with separating embedding spaces between modalities during further model pre-training with modality-relative training objectives. We focus on text-to-code generation and observe consistent improvements across two backbone models and two test sets, measuring pass@k and a novel incremental variation.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.72.pdf",
        "keywords": [
            "modalities",
            "language",
            "natural language",
            "text to code generation",
            "transfer learning",
            "language models",
            "modality relative training",
            "modality relative pre training",
            "incremental variation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, programming language keywords (e.g. “while”) often have very strictly defined semantics. As such, transfer learning from their natural language usage may not necessarily be beneficial to their code application and vise versa.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, programming language keywords (e.g. “while”) often have very strictly defined semantics. As such, transfer learning from their natural language usage may not necessarily be beneficial to their code application and vise versa.\""
    },
    {
        "title": "Quantifying Stereotypes in Language",
        "authors": [
            "Yang Liu"
        ],
        "published": "2024",
        "summary": "A stereotype is a generalized perception of a specific group of humans. It is often potentially encoded in human language, which is more common in texts on social issues. Previous works simply define a sentence as stereotypical and anti-stereotypical. However, the stereotype of a sentence may require fine-grained quantification. In this paper, to fill this gap, we quantify stereotypes in language by annotating a dataset. We use the pre-trained language models (PLMs) to learn this dataset to predict stereotypes of sentences. Then, we discuss stereotypes about common social issues such as hate speech, sexism, sentiments, and disadvantaged and advantaged groups. We demonstrate the connections and differences between stereotypes and common social issues, and all four studies validate the general findings of the current studies. In addition, our work suggests that fine-grained stereotype scores are a highly relevant and competitive dimension for research on social issues. The models and datasets used in this paper are available at https://anonymous.4open.science/r/quantifying_stereotypes_in_language.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.74.pdf",
        "keywords": [
            "language",
            "trained language models",
            "social issues",
            "stereotypes"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but \"pre-trained language models (PLMs)\" is mentioned, however, there is no discussion of limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but \"pre-trained language models (PLMs)\" is mentioned, however, there is no discussion of limitations."
    },
    {
        "title": "Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model",
        "authors": [
            "Andrew Brown",
            "Jiading Zhu",
            "Mohamed Abdelwahab",
            "Alec Dong",
            "Cindy Wang",
            "Jonathan Rose"
        ],
        "published": "2024",
        "summary": "Large Foundational Language Models are capable of performing many tasks at a high level but are difficult to deploy in many applications because of their size and proprietary ownership. Many will be motivated to distill specific capabilities of foundational models into smaller models that can be owned and controlled. In the development of a therapeutic chatbot, we wish to distill a capability known as reflective listening, in which a therapist produces reflections of client speech. These reflections either restate what a client has said, or connect what was said to a relevant observation, idea or guess that encourages and guides the client to continue contemplation. In this paper, we present a method for distilling the generation of reflections from a Foundational Language Model (GPT-4) into smaller models. We first show that GPT-4, using zero-shot prompting, can generate reflections at near 100% success rate, superior to all previous methods. Using reflections generated by GPT-4, we fine-tune different sizes of the GPT-2 family. The GPT-2-small model achieves 83% success on a hold-out test set and the GPT-2 XL achieves 90% success. We also show that GPT-4 can help in the labor-intensive task of evaluating the quality of the distilled models, using it as a zero-shot classifier. Using triple-human review as a guide, the classifier achieves a Cohen-Kappa of 0.66, a substantial inter-rater reliability figure.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.75.pdf",
        "keywords": [
            "foundational language model",
            "foundational models",
            "distillation",
            "generation",
            "reflections generated",
            "family"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large Foundational Language Models are capable of performing many tasks at a high level but are difficult to deploy in many applications because of their size and proprietary ownership.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large Foundational Language Models are capable of performing many tasks at a high level but are difficult to deploy in many applications because of their size and proprietary ownership.\""
    },
    {
        "title": "Plan-Grounded Large Language Models for Dual Goal Conversational Settings",
        "authors": [
            "Diogo Glória-Silva",
            "Rafael Ferreira",
            "Diogo Tavares",
            "David Semedo",
            "Joao Magalhaes"
        ],
        "published": "2024",
        "summary": "Training Large Language Models (LLMs) to follow user instructions has shown to supply the LLM with ample capacity to converse fluently while being aligned with humans. Yet, it is not completely clear how an LLM can lead a plan-grounded conversation in mixed-initiative settings where instructions flow in both directions of the conversation, i.e. both the LLM and the user provide instructions to one another. In this paper, we tackle a dual goal mixed-initiative conversational setting where the LLM not only grounds the conversation on an arbitrary plan but also seeks to satisfy both a procedural plan and user instructions. The LLM is then responsible for guiding the user through the plan and, at the same time, adapting to new circumstances, answering questions, and activating safety guardrails when needed. We propose a novel LLM that grounds the dialogue on a procedural plan, can take the dialogue initiative, and enforces guardrails on the system’s behavior, while also improving the LLM’s responses to unexpected user behavior. Experiments in controlled settings and with real users show that the best-performing model, which we call PlanLLM, achieves a 2.1x improvement over a strong baseline. Moreover, experiments also show good generalization to unseen domains.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.77.pdf",
        "keywords": [
            "language models",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Yet, it is not completely clear how an LLM can lead a plan-grounded conversation in mixed-initiative settings...\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Yet, it is not completely clear how an LLM can lead a plan-grounded conversation in mixed-initiative settings...\""
    },
    {
        "title": "CEV-LM: Controlled Edit Vector Language Model for Shaping Natural Language Generations",
        "authors": [
            "Samraj Moorjani",
            "Adit Krishnan",
            "Hari Sundaram"
        ],
        "published": "2024",
        "summary": "As large-scale language models become the standard for text generation, there is a greater need to tailor the generations to be more or less concise, targeted, and informative, depending on the audience/application. Existing control approaches primarily adjust the semantic (e.g., emotion, topics), structural (e.g., syntax tree, parts-of-speech), and lexical (e.g., keyword/phrase inclusion) properties of text, but are insufficient to accomplish complex objectives such as pacing which control the complexity and readability of the text. In this paper, we introduce CEV-LM - a lightweight, semi-autoregressive language model that utilizes constrained edit vectors to control three complementary metrics (speed, volume, and circuitousness) that quantify the shape of text (e.g., pacing of content). We study an extensive set of state-of-the-art CTG models and find that CEV-LM provides significantly more targeted and precise control of these three metrics while preserving semantic content, using less training data, and containing fewer parameters.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.80.pdf",
        "keywords": [
            "generations",
            "edit",
            "shaping natural language generations",
            "edit vector language model",
            "language models",
            "semi autoregressive language model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Existing control approaches... are insufficient to accomplish complex objectives such as pacing which control the complexity and readability of the text.\"\n\nThis rating is given because the abstract mentions a limitation of existing control approaches for LLMs, but it is not the primary focus of the paper and is used to motivate the proposed solution.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Existing control approaches... are insufficient to accomplish complex objectives such as pacing which control the complexity and readability of the text.\"\n\nThis rating is given because the abstract mentions a limitation of existing control approaches for LLMs, but it is not the primary focus of the paper and is used to motivate the proposed solution."
    },
    {
        "title": "HiGen: Hierarchy-Aware Sequence Generation for Hierarchical Text Classification",
        "authors": [
            "Vidit Jain",
            "Mukund Rungta",
            "Yuchen Zhuang",
            "Yue Yu",
            "Zeyu Wang",
            "Mu Gao",
            "Jeffrey Skolnick",
            "Chao Zhang"
        ],
        "published": "2024",
        "summary": "Hierarchical text classification (HTC) is a complex subtask under multi-label text classification, characterized by a hierarchical label taxonomy and data imbalance. The best-performing models aim to learn a static representation by combining document and hierarchical label information. However, the relevance of document sections can vary based on the hierarchy level, necessitating a dynamic document representation. To address this, we propose HiGen, a text-generation-based framework utilizing language models to encode dynamic text representations. We introduce a level-guided loss function to capture the relationship between text and label name semantics. Our approach incorporates a task-specific pretraining strategy, adapting the language model to in-domain knowledge and significantly enhancing performance for classes with limited examples. Furthermore, we present a new and valuable dataset called ENZYME, designed for HTC, which comprises articles from PubMed with the goal of predicting Enzyme Commission (EC) numbers. Through extensive experiments on the ENZYME dataset and the widely recognized WOS and NYT datasets, our methodology demonstrates superior performance, surpassing existing approaches while efficiently handling data and mitigating class imbalance. We release our code and dataset here: https://github.com/viditjain99/HiGen.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.82.pdf",
        "keywords": [
            "text generation",
            "hierarchical text classification",
            "hierarchy aware sequence generation",
            "language models",
            "pretraining strategy",
            "class imbalance"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"utilizing language models to encode dynamic text representations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"utilizing language models to encode dynamic text representations.\""
    },
    {
        "title": "M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection",
        "authors": [
            "Yuxia Wang",
            "Jonibek Mansurov",
            "Petar Ivanov",
            "Jinyan Su",
            "Artem Shelmanov",
            "Akim Tsvigun",
            "Chenxi Whitehouse",
            "Osama Mohammed Afzal",
            "Tarek Mahmoud",
            "Toru Sasaki",
            "Thomas Arnold",
            "Alham Fikri Aji",
            "Nizar Habash",
            "Iryna Gurevych",
            "Preslav Nakov"
        ],
        "published": "2024",
        "summary": "Large language models (LLMs) have demonstrated remarkable capability to generate fluent responses to a wide variety of user queries. However, this has also raised concerns about the potential misuse of such texts in journalism, education, and academia. In this study, we strive to create automated systems that can detect machine-generated texts and pinpoint potential misuse. We first introduce a large-scale benchmark M4, which is a multi-generator, multi-domain, and multi-lingual corpus for machine-generated text detection. Through an extensive empirical study of this dataset, we show that it is challenging for detectors to generalize well on instances from unseen domains or LLMs. In such cases, detectors tend to misclassify machine-generated text as human-written. These results show that the problem is far from solved and that there is a lot of room for improvement. We believe that our dataset will enable future research towards more robust approaches to this pressing societal problem. The dataset is available at https://github.com/mbzuai-nlp/M4",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.83.pdf",
        "keywords": [
            "generator",
            "machine generated text detection",
            "black"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, this has also raised concerns about the potential misuse of such texts in journalism, education, and academia.\"; \"Through an extensive empirical study of this dataset, we show that it is challenging for detectors to generalize well on instances from unseen domains or LLMs. In such cases, detectors tend to misclassify machine-generated text as human-written.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, this has also raised concerns about the potential misuse of such texts in journalism, education, and academia.\"; \"Through an extensive empirical study of this dataset, we show that it is challenging for detectors to generalize well on instances from unseen domains or LLMs. In such cases, detectors tend to misclassify machine-generated text as human-written.\""
    },
    {
        "title": "A Truly Joint Neural Architecture for Segmentation and Parsing",
        "authors": [
            "Danit Yshaayahu Levi",
            "Reut Tsarfaty"
        ],
        "published": "2024",
        "summary": "Contemporary multilingual dependency parsers can parse a diverse set of languages, but for Morphologically Rich Languages (MRLs), performance is attested to be lower than other languages. The key challenge is that, due to high morphological complexity and ambiguity of the space-delimited input tokens, the linguistic units that act as nodes in the tree are not known in advance. Pre-neural dependency parsers for MRLs subscribed to the joint morpho-syntactic hypothesis, stating that morphological segmentation and syntactic parsing should be solved jointly, rather than as a pipeline where segmentation precedes parsing. However, neural state-of-the-art parsers to date use a strict pipeline. In this paper we introduce a joint neural architecture where a lattice-based representation preserving all morphological ambiguity of the input is provided to an arc-factored model, which then solves the morphological segmentation and syntactic parsing tasks at once. Our experiments on Hebrew, a rich and highly ambiguous MRL, demonstrate state-of-the-art performance on parsing, tagging and segmentation of the Hebrew section of UD, using a single model. This proposed architecture is LLM-based and language agnostic, providing a solid foundation for MRLs to obtain further performance improvements and bridge the gap with other languages.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.84.pdf",
        "keywords": [
            "segmentation",
            "dependency parsers",
            "parsers",
            "pipeline",
            "syntactic parsing",
            "tagging",
            "languages",
            "morphological segmentation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Contemporary multilingual dependency parsers can parse a diverse set of languages, but for Morphologically Rich Languages (MRLs), performance is attested to be lower than other languages.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Contemporary multilingual dependency parsers can parse a diverse set of languages, but for Morphologically Rich Languages (MRLs), performance is attested to be lower than other languages.\""
    },
    {
        "title": "Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation",
        "authors": [
            "Kun Zhou",
            "Yifan Li",
            "Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2024",
        "summary": "Recently, continuous diffusion models (CDM) have been introduced into non-autoregressive (NAR) text-to-text generation. However, the discrete nature of text increases the difficulty of CDM to generate coherent and fluent texts, and also causes the incompatibility problem between CDM and advanced NLP techniques, especially the popular pre-trained language models (PLMs).To solve it, we propose Diffusion-NAT, which introduces discrete diffusion models (DDM) into NAR text-to-text generation and integrates BART to improve the performance.By revising the decoding process of BART and the typical settings of DDM, we unify the inference process of BART and the denoising process of DDM into the same NAR masked tokens recovering task.In this way, DDM can rely on BART to perform denoising, which can benefit from both the rich pre-learned knowledge of BART and the iterative refining paradigm of DDM.Besides, we also propose the iterative self-prompting strategy to further improve the generation quality.Experimental results on 7 datasets show that our approach can outperform competitive NAR methods, and even surpass autoregressive methods.Our code and data are released at https://github.com/RUCAIBox/DiffusionNAT.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.86.pdf",
        "keywords": [
            "self prompting",
            "self prompting strategy",
            "diffusion nat",
            "diffusion",
            "discrete diffusion"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the incompatibility problem between CDM and advanced NLP techniques, especially the popular pre-trained language models (PLMs)\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the incompatibility problem between CDM and advanced NLP techniques, especially the popular pre-trained language models (PLMs)\""
    },
    {
        "title": "Predicting Client Emotions and Therapist Interventions in Psychotherapy Dialogues",
        "authors": [
            "Tobias Mayer",
            "Neha Warikoo",
            "Amir Eliassaf",
            "Dana Atzil-Slonim",
            "Iryna Gurevych"
        ],
        "published": "2024",
        "summary": "Natural Language Processing (NLP) can advance psychotherapy research by scaling up therapy dialogue analysis as well as by allowing researchers to examine client-therapist interactions in detail. Previous studies have mainly either explored the clients’ behavior or the therapists’ intervention in dialogues. Yet, modelling conversations from both dialogue participants is crucial to understanding the therapeutic interaction. This study explores speaker contribution-based dialogue acts at the utterance-level; i.e, the therapist - Intervention Prediction (IP) and the client - Emotion Recognition (ER) in psychotherapy using a pan-theoretical schema. We perform experiments with fine-tuned language models and light-weight adapter solutions on a Hebrew dataset. We deploy the results from our ER model predictions in investigating the coherence between client self-reports on emotion and the utterance-level emotions. Our best adapters achieved on-par performance with fully fine-tuned models, at 0.64 and 0.66 micro F1 for IP and ER, respectively. In addition, our analysis identifies ambiguities within categorical clinical coding, which can be used to fine-tune the coding schema. Finally, our results indicate a positive correlation between client self-reports and utterance-level emotions.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.88.pdf",
        "keywords": [
            "emotion",
            "client emotion recognition",
            "client self reports",
            "natural language processing",
            "predicting client emotions",
            "psychotherapy",
            "psychotherapy dialogues",
            "level",
            "adapters"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Style-News: Incorporating Stylized News Generation and Adversarial Verification for Neural Fake News Detection",
        "authors": [
            "Wei-Yao Wang",
            "Yu-Chieh Chang",
            "Wen-Chih Peng"
        ],
        "published": "2024",
        "summary": "With the improvements in generative models, the issues of producing hallucinations in various domains (e.g., law, writing) have been brought to people’s attention due to concerns about misinformation. In this paper, we focus on neural fake news, which refers to content generated by neural networks aiming to mimic the style of real news to deceive people. To prevent harmful disinformation spreading fallaciously from malicious social media (e.g., content farms), we propose a novel verification framework, Style-News, using publisher metadata to imply a publisher’s template with the corresponding text types, political stance, and credibility. Based on threat modeling aspects, a style-aware neural news generator is introduced as an adversary for generating news content conditioning for a specific publisher, and style and source discriminators are trained to defend against this attack by identifying which publisher the style corresponds with, and discriminating whether the source of the given news is human-written or machine-generated. To evaluate the quality of the generated content, we integrate various dimensional metrics (language fluency, content preservation, and style adherence) and demonstrate that Style-News significantly outperforms the previous approaches by a margin of 0.35 for fluency, 15.24 for content, and 0.38 for style at most. Moreover, our discriminative model outperforms state-of-the-art baselines in terms of publisher prediction (up to 4.64%) and neural fake news detection (+6.94% 31.72%). We plan to release our Style-News publicly, with the aim of improving neural fake news detection.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.92.pdf",
        "keywords": [
            "news",
            "fake news",
            "fake news detection",
            "fluency",
            "language fluency",
            "verification",
            "style news",
            "publisher",
            "content preservation",
            "threat modeling",
            "publisher metadata",
            "stylized news",
            "baselines"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"With the improvements in generative models, the issues of producing hallucinations in various domains (e.g., law, writing) have been brought to people’s attention due to concerns about misinformation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"With the improvements in generative models, the issues of producing hallucinations in various domains (e.g., law, writing) have been brought to people’s attention due to concerns about misinformation.\""
    },
    {
        "title": "Translate to Disambiguate: Zero-shot Multilingual Word Sense Disambiguation with Pretrained Language Models",
        "authors": [
            "Haoqiang Kang",
            "Terra Blevins",
            "Luke Zettlemoyer"
        ],
        "published": "2024",
        "summary": "Pretrained Language Models (PLMs) learn rich cross-lingual knowledge and perform well on diverse tasks such as translation and multilingual word sense disambiguation (WSD) when finetuned. However, they often struggle at disambiguating word sense in a zero-shot setting. To better understand this contrast, we present a new study investigating how well PLMs capture cross-lingual word sense with Contextual Word-Level Translation (C-WLT), an extension of word-level translation that prompts the model to translate a given word in context. We find that as the model size increases, PLMs encode more cross-lingual word sense knowledge and better use context to improve WLT performance. Building on C-WLT, we introduce a zero-shot prompting approach for WSD, tested on 18 languages from the XL-WSD dataset. Our method outperforms fully supervised baselines on recall for many evaluation languages without additional training or finetuning. This study presents a first step towards understanding how to best leverage the cross-lingual knowledge inside PLMs for robust zero-shot reasoning in any language.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.94.pdf",
        "keywords": [
            "word sense",
            "word sense disambiguation",
            "language",
            "language models",
            "word level translation",
            "finetuning",
            "zero shot reasoning",
            "wlt"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, they often struggle at disambiguating word sense in a zero-shot setting.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, they often struggle at disambiguating word sense in a zero-shot setting.\""
    },
    {
        "title": "Anchor Points: Benchmarking Models with Much Fewer Examples",
        "authors": [
            "Rajan Vivek",
            "Kawin Ethayarajh",
            "Diyi Yang",
            "Douwe Kiela"
        ],
        "published": "2024",
        "summary": "Modern language models often exhibit powerful but brittle behavior, leading to the development of larger and more diverse benchmarks to reliably assess their behavior. Here, we suggest that model performance can be benchmarked and elucidated with much smaller evaluation sets. We first show that in six popular language classification benchmarks, model confidence in the correct class on many pairs of points is strongly correlated across models. We build upon this phenomenon to propose Anchor Point Selection, a technique to select small subsets of datasets that capture model behavior across the entire dataset. Anchor points reliably rank models: across 87 diverse language model-prompt pairs, evaluating models using 1-30 anchor points outperforms uniform sampling and other baselines at accurately ranking models. Moreover, just a dozen anchor points can be used to estimate model per-class predictions on all other points in a dataset with low error, sufficient for gauging where the model is likely to fail. Lastly, we present Anchor Point Maps for visualizing these insights and facilitating comparisons of the performance of different models on various regions within the dataset distribution.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.95.pdf",
        "keywords": [
            "language classification benchmarks",
            "anchor point selection",
            "language model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Modern language models often exhibit powerful but brittle behavior\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Modern language models often exhibit powerful but brittle behavior\""
    },
    {
        "title": "Discovering and Articulating Frames of Communication from Social Media Using Chain-of-Thought Reasoning",
        "authors": [
            "Maxwell Weinzierl",
            "Sanda Harabagiu"
        ],
        "published": "2024",
        "summary": "Frames of Communication (FoCs) are ubiquitous in social media discourse. They define what counts as a problem, diagnose what is causing the problem, elicit moral judgments and imply remedies for resolving the problem. Most research on automatic frame detection involved the recognition of the problems addressed by frames, but did not consider the articulation of frames. Articulating an FoC involves reasoning with salient problems, their cause and eventual solution. In this paper we present a method for Discovering and Articulating FoCs (DA-FoC) that relies on a combination of Chain-of-Thought prompting of large language models (LLMs) with In-Context Active Curriculum Learning. Very promising evaluation results indicate that 86.72% of the FoCs encoded by communication experts on the same reference dataset were also uncovered by DA-FoC. Moreover, DA-FoC uncovered many new FoCs, which escaped the experts. Interestingly, 55.1% of the known FoCs were judged as being better articulated than the human-written ones, while 93.8% of the new FoCs were judged as having sound rationale and being clearly articulated.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.97.pdf",
        "keywords": [
            "frames",
            "social media",
            "thought reasoning",
            "frames of communication",
            "communication",
            "articulating"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, but \"Chain-of-Thought prompting of large language models (LLMs)\" implies the use of LLMs in the proposed method.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, but \"Chain-of-Thought prompting of large language models (LLMs)\" implies the use of LLMs in the proposed method."
    },
    {
        "title": "WSC+: Enhancing The Winograd Schema Challenge Using Tree-of-Experts",
        "authors": [
            "Pardis Sadat Zahraei",
            "Ali Emami"
        ],
        "published": "2024",
        "summary": "The Winograd Schema Challenge (WSC) serves as a prominent benchmark for evaluating machine understanding. While Large Language Models (LLMs) excel at answering WSC questions, their ability to generate such questions remains less explored. In this work, we propose Tree-of-Experts (ToE), a novel prompting method which enhances the generation of WSC instances (50% valid cases vs. 10% in recent methods). Using this approach, we introduce WSC+, a novel dataset comprising 3,026 LLM-generated sentences. Notably, we extend the WSC framework by incorporating new ‘ambiguous’ and ‘offensive’ categories, providing a deeper insight into model overconfidence and bias. Our analysis reveals nuances in generation-evaluation consistency, suggesting that LLMs may not always outperform in evaluating their own generated questions when compared to those crafted by other models. On WSC+, GPT-4, the top-performing LLM, achieves an accuracy of 68.7%, significantly below the human benchmark of 95.1%.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.99.pdf",
        "keywords": [
            "winograd schema challenge"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Notably, we extend the WSC framework by incorporating new ‘ambiguous’ and ‘offensive’ categories, providing a deeper insight into model overconfidence and bias. Our analysis reveals nuances in generation-evaluation consistency, suggesting that LLMs may not always outperform in evaluating their own generated questions when compared to those crafted by other models. On WSC+, GPT-4",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Notably, we extend the WSC framework by incorporating new ‘ambiguous’ and ‘offensive’ categories, providing a deeper insight into model overconfidence and bias. Our analysis reveals nuances in generation-evaluation consistency, suggesting that LLMs may not always outperform in evaluating their own generated questions when compared to those crafted by other models. On WSC+, GPT-4"
    },
    {
        "title": "Kardeş-NLU: Transfer to Low-Resource Languages with the Help of a High-Resource Cousin – A Benchmark and Evaluation for Turkic Languages",
        "authors": [
            "Lütfi Kerem Senel",
            "Benedikt Ebing",
            "Konul Baghirova",
            "Hinrich Schuetze",
            "Goran Glavaš"
        ],
        "published": "2024",
        "summary": "Cross-lingual transfer (XLT) driven by massively multilingual language models (mmLMs) has been shown largely ineffective for low-resource (LR) target languages with little (or no) representation in mmLM’s pretraining, especially if they are linguistically distant from the high-resource (HR) source language. Much of the recent focus in XLT research has been dedicated to LR language families, i.e., families without any HR languages (e.g., families of African languages or indigenous languages of the Americas). In this work, in contrast, we investigate a configuration that is arguably of practical relevance for more of the world’s languages: XLT to LR languages that do have a close HR relative. To explore the extent to which a HR language can facilitate transfer to its LR relatives, we (1) introduce Kardeş-NLU, an evaluation benchmark with language understanding datasets in five LR Turkic languages: Azerbaijani, Kazakh, Kyrgyz, Uzbek, and Uyghur; and (2) investigate (a) intermediate training and (b) fine-tuning strategies that leverage Turkish in XLT to these target languages. Our experimental results show that both - integrating Turkish in intermediate training and in downstream fine-tuning - yield substantial improvements in XLT to LR Turkic languages. Finally, we benchmark cutting-edge instruction-tuned large language models on Kardeş-NLU, showing that their performance is highly task- and language-dependent.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.100.pdf",
        "keywords": [
            "language",
            "low resource languages",
            "s languages",
            "xlt",
            "turkic languages",
            "turkic",
            "language models",
            "lr turkic languages",
            "evaluation benchmark",
            "a"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Cross-lingual transfer (XLT) driven by massively multilingual language models (mmLMs) has been shown largely ineffective for low-resource (LR) target languages with little (or no) representation in mmLM’s pretraining, especially if they are linguistically distant from the high-resource (HR) source language.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Cross-lingual transfer (XLT) driven by massively multilingual language models (mmLMs) has been shown largely ineffective for low-resource (LR) target languages with little (or no) representation in mmLM’s pretraining, especially if they are linguistically distant from the high-resource (HR) source language.\""
    },
    {
        "title": "Learning to Retrieve In-Context Examples for Large Language Models",
        "authors": [
            "Liang Wang",
            "Nan Yang",
            "Furu Wei"
        ],
        "published": "2024",
        "summary": "Large language models (LLMs) have demonstrated their ability to learn in-context, allowing them to perform various tasks based on a few input-output examples. However, the effectiveness of in-context learning is heavily reliant on the quality of the selected examples. In this paper, we propose a novel framework to iteratively train dense retrievers that can identify high-quality in-context examples for LLMs. Our framework initially trains a reward model based on LLM feedback to evaluate the quality of candidate examples, followed by knowledge distillation to train a bi-encoder based dense retriever. Our experiments on a suite of 30 tasks demonstrate that our framework significantly enhances in-context learning performance. Furthermore, we show the generalization ability of our framework to unseen tasks during training. An in-depth analysis reveals that our model improves performance by retrieving examples with similar patterns, and the gains are consistent across LLMs of varying sizes.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.105.pdf",
        "keywords": [
            "language models"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the effectiveness of in-context learning is heavily reliant on the quality of the selected examples.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, the effectiveness of in-context learning is heavily reliant on the quality of the selected examples.\""
    },
    {
        "title": "Unsupervised stance detection for social media discussions: A generic baseline",
        "authors": [
            "Maia Sutter",
            "Antoine Gourru",
            "Amine Trabelsi",
            "Christine Largeron"
        ],
        "published": "2024",
        "summary": "With the ever-growing use of social media to express opinions on the national and international stage, unsupervised methods of stance detection are increasingly important to handle the task without costly annotation of data. The current unsupervised state-of-the-art models are designed for specific network types, either homophilic or heterophilic, and they fail to generalize to both. In this paper, we first analyze the generalization ability of recent baselines to these two very different network types. Then, we conduct extensive experiments with a baseline model based on text embeddings propagated with a graph neural network that generalizes well to heterophilic and homophilic networks. We show that it outperforms, on average, other state-of-the-art methods across the two network types. Additionally, we show that combining textual and network information outperforms using text only, and that the language model size has only a limited impact on the model performance.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.107.pdf",
        "keywords": [
            "social media",
            "social media discussions",
            "unsupervised",
            "stance detection",
            "graph neural network"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Aligning Large and Small Language Models via Chain-of-Thought Reasoning",
        "authors": [
            "Leonardo Ranaldi",
            "Andre Freitas"
        ],
        "published": "2024",
        "summary": "Chain-of-Thought (CoT) prompting empowersthe reasoning abilities of Large Language Models (LLMs), eliciting them to solve complexreasoning tasks in a step-wise manner. However, these capabilities appear only in models with billions of parameters, which represent an entry barrier for many users who are constrained to operate on a smaller model scale, i.e., Small Language Models (SLMs). Although many companies are releasing LLMs of the same family with fewer parameters, these models tend not to preserve all the reasoning capabilities of the original models, including CoT reasoning.In this paper, we propose a method for aligning and transferring reasoning abilities between larger to smaller Language Models. By using an Instruction-tuning-CoT method, that is, an Instruction-tuning designed around CoT-Demonstrations, we enable the SLMs to generate multi-step controlled reasoned answers when they are elicited with the CoT mechanism. Hence, we instruct a smaller Language Model using outputs generated by more robust models belonging to the same family or not, evaluating the impact across different types of models. Results obtained on question-answering and mathematical reasoning benchmarks show that LMs instructed via the Instruction-tuning CoT method produced by LLMs outperform baselines within both in-domain and out-domain scenarios.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.109.pdf",
        "keywords": [
            "thought reasoning",
            "small language models",
            "aligning",
            "smaller language models",
            "large language models",
            "chain of thought"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these capabilities appear only in models with billions of parameters, which represent an entry barrier for many users who are constrained to operate on a smaller model scale, i.e., Small Language Models (SLMs). Although many companies are releasing LLMs of the same family with fewer parameters, these models tend not to preserve all the reasoning capabilities of the original models, including CoT",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, these capabilities appear only in models with billions of parameters, which represent an entry barrier for many users who are constrained to operate on a smaller model scale, i.e., Small Language Models (SLMs). Although many companies are releasing LLMs of the same family with fewer parameters, these models tend not to preserve all the reasoning capabilities of the original models, including CoT"
    },
    {
        "title": "Uncovering Stereotypes in Large Language Models: A Task Complexity-based Approach",
        "authors": [
            "Hari Shrawgi",
            "Prasanjit Rath",
            "Tushar Singhal",
            "Sandipan Dandapat"
        ],
        "published": "2024",
        "summary": "Recent Large Language Models (LLMs) have unlocked unprecedented applications of AI. As these models continue to transform human life, there are growing socio-ethical concerns around their inherent stereotypes that can lead to bias in their applications. There is an urgent need for holistic bias evaluation of these LLMs. Few such benchmarks exist today and evaluation techniques that do exist are either non-holistic or may provide a false sense of security as LLMs become better at hiding their biases on simpler tasks. We address these issues with an extensible benchmark - LLM Stereotype Index (LSI). LSI is grounded on Social Progress Index, a holistic social benchmark. We also test the breadth and depth of bias protection provided by LLMs via a variety of tasks with varying complexities. Our findings show that both ChatGPT and GPT-4 have strong inherent prejudice with respect to nationality, gender, race, and religion. The exhibition of such issues becomes increasingly apparent as we increase task complexity. Furthermore, GPT-4 is better at hiding the biases, but when displayed it is more significant. Our findings highlight the harms and divide that these LLMs can bring to society if we do not take very diligent care in their use.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.111.pdf",
        "keywords": [
            "task complexity",
            "language models",
            "holistic",
            "holistic bias evaluation",
            "holistic social benchmark",
            "benchmarks",
            "socio ethical",
            "social progress index",
            "test",
            "stereotypes"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"There is an urgent need for holistic bias evaluation of these LLMs. Few such benchmarks exist today and evaluation techniques that do exist are either non-holistic or may provide a false sense of security as LLMs become better at hiding their biases on simpler tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"There is an urgent need for holistic bias evaluation of these LLMs. Few such benchmarks exist today and evaluation techniques that do exist are either non-holistic or may provide a false sense of security as LLMs become better at hiding their biases on simpler tasks.\""
    },
    {
        "title": "CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain Performance and Calibration",
        "authors": [
            "Rachneet Sachdeva",
            "Martin Tutek",
            "Iryna Gurevych"
        ],
        "published": "2024",
        "summary": "In recent years, large language models (LLMs) have shown remarkable capabilities at scale, particularly at generating text conditioned on a prompt. In our work, we investigate the use of LLMs to augment training data of smaller language models (SLMs) with automatically generated counterfactual (CF) instances – i.e. minimally altered inputs – in order to improve out-of-domain (OOD) performance of SLMs in the extractive question answering (QA) setup. We show that, across various LLM generators, such data augmentation consistently enhances OOD performance and improves model calibration for both confidence-based and rationale-augmented calibrator models. Furthermore, these performance improvements correlate with higher diversity of CF instances in terms of their surface form and semantic content. Finally, we show that CF augmented models which are easier to calibrate also exhibit much lower entropy when assigning importance, indicating that rationale-augmented calibrators prefer concise explanations.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.113.pdf",
        "keywords": [
            "extractive question answering",
            "counterfactual augmented training",
            "out of domain",
            "calibration",
            "out of domain performance"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper discusses using LLMs to augment training data for smaller language models to improve their performance, but does not mention any limitations of LLMs themselves.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper discusses using LLMs to augment training data for smaller language models to improve their performance, but does not mention any limitations of LLMs themselves."
    },
    {
        "title": "UP5: Unbiased Foundation Model for Fairness-aware Recommendation",
        "authors": [
            "Wenyue Hua",
            "Yingqiang Ge",
            "Shuyuan Xu",
            "Jianchao Ji",
            "Zelong Li",
            "Yongfeng Zhang"
        ],
        "published": "2024",
        "summary": "Recent advances in Foundation Models such as Large Language Models (LLMs) have propelled them to the forefront of Recommender Systems (RS). Despite their utility, there is a growing concern that LLMs might inadvertently perpetuate societal stereotypes, resulting in unfair recommendations. Since fairness is critical for RS as many users take it for decision-making and demand fulfillment, this paper focuses on user-side fairness for LLM-based recommendation where the users may require a recommender system to be fair on specific sensitive features such as gender or age. In this paper, we dive into the extent of unfairness exhibited by LLM-based recommender models based on both T5 and LLaMA backbones, and discuss appropriate methods for promoting equitable treatment of users in LLM-based recommendation models. We introduce a novel Counterfactually-Fair-Prompt (CFP) method towards Unbiased Foundation mOdels (UFO) for fairness-aware LLM-based recommendation. Experiments are conducted on two real-world datasets, MovieLens-1M and Insurance, and compared with both matching-based and sequential-based fairness-aware recommendation models. Results show that CFP achieves better recommendation performance with a high level of fairness.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.114.pdf",
        "keywords": [
            "fairness",
            "recommender",
            "fairness aware recommendation",
            "unfair recommendations",
            "recommender systems"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite their utility, there is a growing concern that LLMs might inadvertently perpetuate societal stereotypes, resulting in unfair recommendations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Despite their utility, there is a growing concern that LLMs might inadvertently perpetuate societal stereotypes, resulting in unfair recommendations.\""
    },
    {
        "title": "Human Temporal Inferences Go Beyond Aspectual Class",
        "authors": [
            "Katarzyna Pruś",
            "Mark Steedman",
            "Adam Lopez"
        ],
        "published": "2024",
        "summary": "Past work in NLP has proposed the task of classifying English verb phrases into situation aspect categories, assuming that these categories play an important role in tasks requiring temporal reasoning. We investigate this assumption by gathering crowd-sourced judgements about aspectual entailments from non-expert, native English participants. The results suggest that aspectual class alone is not sufficient to explain the response patterns of the participants. We propose that looking at scenarios which can feasibly accompany an action description contributes towards a better explanation of the participants’ answers. A further experiment using GPT-3.5 shows that its outputs follow different patterns than human answers, suggesting that such conceivable scenarios cannot be fully accounted for in the language alone. We release our dataset to support further research.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.115.pdf",
        "keywords": [
            "temporal reasoning"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"A further experiment using GPT-3.5 shows that its outputs follow different patterns than human answers, suggesting that such conceivable scenarios cannot be fully accounted for in the language alone.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"A further experiment using GPT-3.5 shows that its outputs follow different patterns than human answers, suggesting that such conceivable scenarios cannot be fully accounted for in the language alone.\""
    },
    {
        "title": "Investigating Agency of LLMs in Human-AI Collaboration Tasks",
        "authors": [
            "Ashish Sharma",
            "Sudha Rao",
            "Chris Brockett",
            "Akanksha Malhotra",
            "Nebojsa Jojic",
            "Bill Dolan"
        ],
        "published": "2024",
        "summary": "Agency, the capacity to proactively shape events, is central to how humans interact and collaborate. While LLMs are being developed to simulate human behavior and serve as human-like agents, little attention has been given to the Agency that these models should possess in order to proactively manage the direction of interaction and collaboration. In this paper, we investigate Agency as a desirable function of LLMs, and how it can be measured and managed. We build on social-cognitive theory to develop a framework of features through which Agency is expressed in dialogue – indicating what you intend to do (Intentionality), motivating your intentions (Motivation), having self-belief in intentions (Self-Efficacy), and being able to self-adjust (Self-Regulation). We collect a new dataset of 83 human-human collaborative interior design conversations containing 908 conversational snippets annotated for Agency features. Using this dataset, we develop methods for measuring Agency of LLMs. Automatic and human evaluations show that models that manifest features associated with high Intentionality, Motivation, Self-Efficacy, and Self-Regulation are more likely to be perceived as strongly agentive.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.119.pdf",
        "keywords": [
            "llms",
            "conversational",
            "agency",
            "collaboration",
            "measuring agency",
            "cognitive theory",
            "self regulation",
            "self efficacy"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While LLMs are being developed to simulate human behavior and serve as human-like agents, little attention has been given to the Agency that these models should possess in order to proactively manage the direction of interaction and collaboration.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"While LLMs are being developed to simulate human behavior and serve as human-like agents, little attention has been given to the Agency that these models should possess in order to proactively manage the direction of interaction and collaboration.\""
    },
    {
        "title": "SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking",
        "authors": [
            "Atharva Kulkarni",
            "Bo-Hsiang Tseng",
            "Joel Ruben Antony Moniz",
            "Dhivya Piraviperumal",
            "Hong Yu",
            "Shruti Bhargava"
        ],
        "published": "2024",
        "summary": "In-context learning with Large Language Models (LLMs) has emerged as a promising avenue of research in Dialog State Tracking (DST). However, the best-performing in-context learning methods involve retrieving and adding similar examples to the prompt, requiring access to labeled training data. Procuring such training data for a wide range of domains and applications is time-consuming, expensive, and, at times, infeasible. While zero-shot learning requires no training data, it significantly lags behind the few-shot setup. Thus, ‘Can we efficiently generate synthetic data for any dialogue schema to enable few-shot prompting?' Addressing this question, we propose , a data generation framework tailored for DST, utilizing LLMs. Our approach only requires the dialogue schema and a few hand-crafted dialogue templates to synthesize natural, coherent, and free-flowing dialogues with DST annotations. Few-shot learning using data from results in 4-5% improvement in Joint Goal Accuracy over the zero-shot baseline on MultiWOZ 2.1 and 2.4. Remarkably, our few-shot learning approach recovers nearly 98% of the performance compared to the few-shot setup using human-annotated training data.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.120.pdf",
        "keywords": [
            "synthetic data",
            "dialog state tracking",
            "shot dialog state tracking",
            "synthdst"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Procuring such training data for a wide range of domains and applications is time-consuming, expensive, and, at times, infeasible.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Procuring such training data for a wide range of domains and applications is time-consuming, expensive, and, at times, infeasible.\""
    },
    {
        "title": "Argument Mining as a Text-to-Text Generation Task",
        "authors": [
            "Masayuki Kawarada",
            "Tsutomu Hirao",
            "Wataru Uchida",
            "Masaaki Nagata"
        ],
        "published": "2024",
        "summary": "Argument Mining (AM) aims to uncover the argumentative structures within a text. Previous methods require several subtasks, such as span identification, component classification, and relation classification. Consequently, these methods need rule-based postprocessing to derive argumentative structures from the output of each subtask. This approach adds to the complexity of the model and expands the search space of the hyperparameters. To address this difficulty, we propose a simple yet strong method based on a text-to-text generation approach using a pretrained encoder-decoder language model. Our method simultaneously generates argumentatively annotated text for spans, components, and relations, eliminating the need for task-specific postprocessing and hyperparameter tuning. Furthermore, because it is a straightforward text-to-text generation method, we can easily adapt our approach to various types of argumentative structures.Experimental results demonstrate the effectiveness of our method, as it achieves state-of-the-art performance on three different types of benchmark datasets: the Argument-annotated Essays Corpus (AAEC), AbstRCT, and the Cornell eRulemaking Corpus (CDCP).",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.121.pdf",
        "keywords": [
            "argumentative",
            "argument mining",
            "classification"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "An Empirical Analysis of Diversity in Argument Summarization",
        "authors": [
            "Michiel Van Der Meer",
            "Piek Vossen",
            "Catholijn Jonker",
            "Pradeep Murukannaiah"
        ],
        "published": "2024",
        "summary": "Presenting high-level arguments is a crucial task for fostering participation in online societal discussions. Current argument summarization approaches miss an important facet of this task—capturing diversity—which is important for accommodating multiple perspectives. We introduce three aspects of diversity: those of opinions, annotators, and sources. We evaluate approaches to a popular argument summarization task called Key Point Analysis, which shows how these approaches struggle to (1) represent arguments shared by few people, (2) deal with data from various sources, and (3) align with subjectivity in human-provided annotations. We find that both general-purpose LLMs and dedicated KPA models exhibit this behavior, but have complementary strengths. Further, we observe that diversification of training data may ameliorate generalization in zero-shot cases. Addressing diversity in argument summarization requires a mix of strategies to deal with subjectivity.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.123.pdf",
        "keywords": [
            "argument summarization",
            "diversity",
            "diversification",
            "empirical analysis",
            "key point analysis"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We evaluate approaches to a popular argument summarization task called Key Point Analysis, which shows how these approaches struggle to (1) represent arguments shared by few people, (2) deal with data from various sources, and (3) align with subjectivity in human-provided annotations.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We evaluate approaches to a popular argument summarization task called Key Point Analysis, which shows how these approaches struggle to (1) represent arguments shared by few people, (2) deal with data from various sources, and (3) align with subjectivity in human-provided annotations.\""
    },
    {
        "title": "Approximate Attributions for Off-the-Shelf Siamese Transformers",
        "authors": [
            "Lucas Moeller",
            "Dmitry Nikolaev",
            "Sebastian Padó"
        ],
        "published": "2024",
        "summary": "Siamese encoders such as sentence transformers are among the least understood deep models.Established attribution methods cannot tackle this model class since it compares two inputs rather than processing a single one. To address this gap, we have recently proposed an attribution method specifically for Siamese encoders (Möller et al., 2023). However, it requires models to be adjusted and fine-tuned and therefore cannot be directly applied to off-the-shelf models. In this work, we reassess these restrictions and propose (i) a model with exact attribution ability that retains the original model’s predictive performance and (ii) a way to compute approximate attributions for off-the-shelf models.We extensively compare approximate and exact attributions and use them to analyze the models’ attendance to different linguistic aspects. We gain insights into which syntactic roles Siamese transformers attend to, confirm that they mostly ignore negation, explore how they judge semantically opposite adjectives, and find that they exhibit lexical bias.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.125.pdf",
        "keywords": [
            "siamese encoders",
            "siamese transformers",
            "exact attribution",
            "syntactic roles siamese transformers"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it requires models to be adjusted and fine-tuned and therefore cannot be directly applied to off-the-shelf models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, it requires models to be adjusted and fine-tuned and therefore cannot be directly applied to off-the-shelf models.\""
    },
    {
        "title": "Tracing the Roots of Facts in Multilingual Language Models: Independent, Shared, and Transferred Knowledge",
        "authors": [
            "Xin Zhao",
            "Naoki Yoshinaga",
            "Daisuke Oba"
        ],
        "published": "2024",
        "summary": "Acquiring factual knowledge for language models (LMs) in low-resource languages poses a serious challenge, thus resorting to cross-lingual transfer in multilingual LMs (ML-LMs). In this study, we ask how ML-LMs acquire and represent factual knowledge. Using the multilingual factual knowledge probing dataset, mLAMA, we first conducted a neuron investigation of ML-LMs (specifically, multilingual BERT). We then traced the roots of facts back to the knowledge source (Wikipedia) to identify the ways in which ML-LMs acquire specific facts. We finally identified three patterns of acquiring and representing facts in ML-LMs: language-independent, cross-lingual shared and transferred, and devised methods for differentiating them. Our findings highlight the challenge of maintaining consistent factual knowledge across languages, underscoring the need for better fact representation learning in ML-LMs.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.127.pdf",
        "keywords": [
            "language models",
            "roots of facts",
            "knowledge source"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Acquiring factual knowledge for language models (LMs) in low-resource languages poses a serious challenge... Our findings highlight the challenge of maintaining consistent factual knowledge across languages, underscoring the need for better fact representation learning in ML-LMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Acquiring factual knowledge for language models (LMs) in low-resource languages poses a serious challenge... Our findings highlight the challenge of maintaining consistent factual knowledge across languages, underscoring the need for better fact representation learning in ML-LMs.\""
    },
    {
        "title": "LegalLens: Leveraging LLMs for Legal Violation Identification in Unstructured Text",
        "authors": [
            "Dor Bernsohn",
            "Gil Semo",
            "Yaron Vazana",
            "Gila Hayat",
            "Ben Hagag",
            "Joel Niklaus",
            "Rohit Saha",
            "Kyryl Truskovskyi"
        ],
        "published": "2024",
        "summary": "In this study, we focus on two main tasks, the first for detecting legal violations within unstructured textual data, and the second for associating these violations with potentially affected individuals. We constructed two datasets using Large Language Models (LLMs) which were subsequently validated by domain expert annotators. Both tasks were designed specifically for the context of class-action cases. The experimental design incorporated fine-tuning models from the BERT family and open-source LLMs, and conducting few-shot experiments using closed-source LLMs. Our results, with an F1-score of 62.69% (violation identification) and 81.02% (associating victims), show that our datasets and setups can be used for both tasks. Finally, we publicly release the datasets and the code used for the experiments in order to advance further research in the area of legal natural language processing (NLP).",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.130.pdf",
        "keywords": [
            "legal natural language processing",
            "legal violation identification",
            "source llms"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit mention of limitations of LLMs, only mentions using LLMs as a tool for the study.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit mention of limitations of LLMs, only mentions using LLMs as a tool for the study."
    },
    {
        "title": "Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of Question Answering Models",
        "authors": [
            "Lukáš Mikula",
            "Michal Štefánik",
            "Marek Petrovič",
            "Petr Sojka"
        ],
        "published": "2024",
        "summary": "While the Large Language Models (LLMs) dominate a majority of language understanding tasks, previous work shows that some of these results are supported by modelling spurious correlations of training datasets. Authors commonly assess model robustness by evaluating their models on out-of-distribution (OOD) datasets of the same task, but these datasets might share the bias of the training dataset. We propose a simple method for measuring a scale of models’ reliance on any identified spurious feature and assess the robustness towards a large set of known and newly found prediction biases for various pre-trained models and debiasing methods in Question Answering (QA). We find that the reported OOD gains of debiasing methods can not be explained by mitigated reliance on biased features, suggesting that biases are shared among different QA datasets. We further evidence this by measuring that performance of OOD models depends on bias features comparably to the ID model. Our findings motivate future work to refine the reports of LLMs’ robustness to a level of known spurious features.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.133.pdf",
        "keywords": [
            "question answering",
            "debiasing methods",
            "language models",
            "efficiency",
            "prediction"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"previous work shows that some of these results are supported by modelling spurious correlations of training datasets... We find that the reported OOD gains of debiasing methods can not be explained by mitigated reliance on biased features, suggesting that biases are shared among different QA datasets.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"previous work shows that some of these results are supported by modelling spurious correlations of training datasets... We find that the reported OOD gains of debiasing methods can not be explained by mitigated reliance on biased features, suggesting that biases are shared among different QA datasets.\""
    },
    {
        "title": "Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models",
        "authors": [
            "Natalie Shapira",
            "Mosh Levy",
            "Seyed Hossein Alavi",
            "Xuhui Zhou",
            "Yejin Choi",
            "Yoav Goldberg",
            "Maarten Sap",
            "Vered Shwartz"
        ],
        "published": "2024",
        "summary": "The escalating debate on AI’s capabilities warrants developing reliable metrics to assess machine “intelligence.” Recently, many anecdotal examples were used to suggest that newer Large Language Models (LLMs) like ChatGPT and GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached conflicting conclusions regarding those abilities. We investigate the extent of LLMs’ N-ToM through an extensive evaluation of 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust ToM abilities. We caution against drawing conclusions from anecdotal examples, limited benchmark testing, and using human-designed psychological tests to evaluate models.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.138.pdf",
        "keywords": [
            "large language models",
            "social reasoning",
            "neural theory of mind"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We investigate the extent of LLMs’ N-ToM through an extensive evaluation of 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust.\" and \"We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We investigate the extent of LLMs’ N-ToM through an extensive evaluation of 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust.\" and \"We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather"
    },
    {
        "title": "NevIR: Negation in Neural Information Retrieval",
        "authors": [
            "Orion Weller",
            "Dawn Lawrie",
            "Benjamin Van Durme"
        ],
        "published": "2024",
        "summary": "Negation is a common everyday phenomena and has been a consistent area of weakness for language models (LMs). Although the Information Retrieval (IR) community has adopted LMs as the backbone of modern IR architectures, there has been little to no research in understanding how negation impacts neural IR. We therefore construct a straightforward benchmark on this theme: asking IR models to rank two documents that differ only by negation. We show that the results vary widely according to the type of IR architecture: cross-encoders perform best, followed by late-interaction models, and in last place are bi-encoder and sparse neural architectures. We find that most current information retrieval models do not consider negation, performing similarly or worse than randomly ranking. We show that although the obvious approach of continued fine-tuning on a dataset of contrastive documents containing negations increases performance (as does model size), there is still a large gap between machine and human performance.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.139.pdf",
        "keywords": [
            "negation",
            "information retrieval",
            "neural information retrieval",
            "language models",
            "nevir"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Negation is a common everyday phenomena and has been a consistent area of weakness for language models (LMs).\"; \"We find that most current information retrieval models do not consider negation, performing similarly or worse than randomly ranking.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Negation is a common everyday phenomena and has been a consistent area of weakness for language models (LMs).\"; \"We find that most current information retrieval models do not consider negation, performing similarly or worse than randomly ranking.\""
    },
    {
        "title": "“According to . . . ”: Prompting Language Models Improves Quoting from Pre-Training Data",
        "authors": [
            "Orion Weller",
            "Marc Marone",
            "Nathaniel Weir",
            "Dawn Lawrie",
            "Daniel Khashabi",
            "Benjamin Van Durme"
        ],
        "published": "2024",
        "summary": "Large Language Models (LLMs) may hallucinate and generate fake information, despite pre-training on factual data. Inspired by the journalistic device of “according to sources”, we propose according-to prompting: directing LLMs to ground responses against previously observed text. To quantify this grounding, we propose a novel evaluation metric (QUIP-Score) that measures the extent to which model-produced answers are directly found in underlying text corpora. We illustrate with experiments on three corpora (Wikipedia, PubMed, and the U.S. legal tax code) that these prompts improve grounding under our metrics, with the additional benefit of often improving end-task performance. Furthermore, prompts that ask the model to decrease grounding (or to ground to other corpora) indeed decrease QUIP-Score, indicating the ability of LLMs to increase or decrease grounded generations on request.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.140.pdf",
        "keywords": [
            "wikipedia",
            "language models",
            "generations",
            "quip score",
            "evaluation metric"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large Language Models (LLMs) may hallucinate and generate fake information, despite pre-training on factual data.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs (hallucination and generation of fake information) but does not elaborate on it and focuses on the proposed solution to address this limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Large Language Models (LLMs) may hallucinate and generate fake information, despite pre-training on factual data.\"\n\nThis rating is chosen because the abstract mentions a limitation of LLMs (hallucination and generation of fake information) but does not elaborate on it and focuses on the proposed solution to address this limitation."
    },
    {
        "title": "SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models",
        "authors": [
            "Xiang Gao",
            "Jiaxin Zhang",
            "Lalla Mouatadid",
            "Kamalika Das"
        ],
        "published": "2024",
        "summary": "In recent years, large language models (LLMs) have become increasingly prevalent, offering remarkable text generation capabilities. However, a pressing challenge is their tendency to make confidently wrong predictions, highlighting the critical need for uncertainty quantification (UQ) in LLMs. While previous works have mainly focused on addressing aleatoric uncertainty, the full spectrum of uncertainties, including epistemic, remains inadequately explored. Motivated by this gap, we introduce a novel UQ method, sampling with perturbation for UQ (SPUQ), designed to tackle both aleatoric and epistemic uncertainties. The method entails generating a set of perturbations for LLM inputs, sampling outputs for each perturbation, and incorporating an aggregation module that generalizes the sampling uncertainty approach for text generation tasks. Through extensive experiments on various datasets, we investigated different perturbation and aggregation techniques. Our findings show a substantial improvement in model uncertainty calibration, with a reduction in Expected Calibration Error (ECE) by 50% on average. Our findings suggest that our proposed UQ method offers promising steps toward enhancing the reliability and trustworthiness of LLMs.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.143.pdf",
        "keywords": [
            "uncertainty quantification",
            "uncertainty",
            "perturbation",
            "large language models",
            "epistemic uncertainties",
            "reliability",
            "aggregation techniques"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, a pressing challenge is their tendency to make confidently wrong predictions, highlighting the critical need for uncertainty quantification (UQ) in LLMs. While previous works have mainly focused on addressing aleatoric uncertainty, the full spectrum of uncertainties, including epistemic, remains inadequately explored.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, a pressing challenge is their tendency to make confidently wrong predictions, highlighting the critical need for uncertainty quantification (UQ) in LLMs. While previous works have mainly focused on addressing aleatoric uncertainty, the full spectrum of uncertainties, including epistemic, remains inadequately explored.\""
    },
    {
        "title": "Advancing Precise Outline-Conditioned Text Generation with Task Duality and Explicit Outline Control",
        "authors": [
            "Yunzhe Li",
            "Qian Chen",
            "Weixiang Yan",
            "Wen Wang",
            "Qinglin Zhang",
            "Hari Sundaram"
        ],
        "published": "2024",
        "summary": "Existing works on outline-conditioned text generation typically aim to generate text using provided outlines as rough sketches, such as keywords and phrases. However, these approaches make it challenging to control the quality of text generation and assess consistency between outlines and generated texts due to lack of clarity and rationality of the rough outlines. In this paper, we introduce a novel text generation task called Precise Outline-conditioned Generation, which requires generating stories based on specific, sentence-level outlines. To facilitate research on this task, we construct two new datasets, WPOG and CDM. We provide strong baselines based on fine-tuning models such as BART and GPT-2, and evaluating zero-shot performance of models such as ChatGPT and Vicuna. Furthermore, we identify an issue of imbalanced utilization of the outline information in the precise outline-conditioned generation, which is ubiquitously observed across fine-tuned models and zero-shot inference models. To address this issue, we propose an explicit outline utilization control approach and a novel framework that leverages the task duality between summarization and generation. Experimental results show that the proposed approaches effectively alleviate the issue of imbalanced outline utilization and enhance the quality of precise outline-conditioned text generation for both fine-tuning and zero-shot settings.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.145.pdf",
        "keywords": [
            "text generation",
            "outline conditioned text generation",
            "precise outline conditioned generation",
            "rough outlines",
            "rough sketches",
            "outline utilization control",
            "explicit outline control",
            "duality",
            "summarization",
            "task duality"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we identify an issue of imbalanced utilization of the outline information in the precise outline-conditioned generation, which is ubiquitously observed across fine-tuned models and zero-shot inference models.\"\n\nNote: The limitation mentioned is related to the utilization of outline information in text generation, which is a specific domain of text generation with outlines.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"we identify an issue of imbalanced utilization of the outline information in the precise outline-conditioned generation, which is ubiquitously observed across fine-tuned models and zero-shot inference models.\"\n\nNote: The limitation mentioned is related to the utilization of outline information in text generation, which is a specific domain of text generation with outlines."
    },
    {
        "title": "Localization vs. Semantics: Visual Representations in Unimodal and Multimodal Models",
        "authors": [
            "Zhuowan Li",
            "Cihang Xie",
            "Benjamin Van Durme",
            "Alan Yuille"
        ],
        "published": "2024",
        "summary": "Despite the impressive advancements achieved through vision-and-language pretraining, it remains unclear whether multi-modal learning can help understand each individual modality. In this work, we conduct a comparative analysis of the visual representations in existing vision-and-language models and vision-only models by probing on a broad range of tasks. Five probing tasks are evaluated in order to assess the quality of the learned representations in a nuanced manner. Our results on five probing tasks suggest vision-and-language models are better at label prediction tasks like object and attribute prediction, while vision-only models are stronger at dense prediction tasks that require more localized information. We hope our study sheds light on the role of language in visual learning, and serves as an empirical guide for various pretrained models.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.146.pdf",
        "keywords": [
            "multimodal models",
            "semantics",
            "localization",
            "unimodal"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the impressive advancements achieved through vision-and-language pretraining, it remains unclear whether multi-modal learning can help understand each individual modality.\"\n\nThis abstract mentions a limitation of multimodal models (which include LLMs) in passing, but does not elaborate on it.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite the impressive advancements achieved through vision-and-language pretraining, it remains unclear whether multi-modal learning can help understand each individual modality.\"\n\nThis abstract mentions a limitation of multimodal models (which include LLMs) in passing, but does not elaborate on it."
    },
    {
        "title": "Creating Suspenseful Stories: Iterative Planning with Large Language Models",
        "authors": [
            "Kaige Xie",
            "Mark Riedl"
        ],
        "published": "2024",
        "summary": "Automated story generation has been one of the long-standing challenges in NLP. Among all dimensions of stories, *suspense* is very common in human-written stories but relatively under-explored in AI-generated stories. While recent advances in large language models (LLMs) have greatly promoted language generation in general, state-of-the-art LLMs are still unreliable when it comes to suspenseful story generation. We propose a novel iterative-prompting-based planning method that is grounded in two theoretical foundations of story suspense from cognitive psychology and narratology. This theory-grounded method works in a fully zero-shot manner and does not rely on any supervised story corpora. To the best of our knowledge, this paper is the first attempt at suspenseful story generation with LLMs. Extensive human evaluations of the generated suspenseful stories demonstrate the effectiveness of our method.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.147.pdf",
        "keywords": [
            "suspenseful",
            "suspenseful stories",
            "suspenseful story generation",
            "language models",
            "story generation",
            "planning",
            "automated story generation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While recent advances in large language models (LLMs) have greatly promoted language generation in general, state-of-the-art LLMs are still unreliable when it comes to suspenseful story generation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While recent advances in large language models (LLMs) have greatly promoted language generation in general, state-of-the-art LLMs are still unreliable when it comes to suspenseful story generation.\""
    },
    {
        "title": "Ask, Assess, and Refine: Rectifying Factual Consistency and Hallucination in LLMs with Metric-Guided Feedback Learning",
        "authors": [
            "Dongyub Lee",
            "Eunhwan Park",
            "Hodong Lee",
            "Heuiseok Lim"
        ],
        "published": "2024",
        "summary": "Recent advancements in Large Language Models (LLMs) have heralded unprecedented capabilities in information-seeking and text generation, as evidenced by applications like Bing Chat and perplexity.ai. Despite these strides, challenges on hallucination and factual inconsistency continue to impede their wider real-world adoption. Contemporary methods, including retrieval-augmented LLMs and feedback-based learning, serve as alternatives to mitigate these challenges. However, challenges remain, particularly regarding referencing erroneous evidence (citation errors) and generating information not present in the evidence (hallucination). In this paper, we introduce the 𝖠2𝖱 framework: Ask, Assess, and Refine. Our approach utilizes an explicit evaluation paradigm, incorporating metrics specifically tailored to assess citation errors and hallucination, aiming to address these prevalent challenges robustly. Capitalizing on these evaluations, we devise a strategy to formulate actionable natural language feedback, enabling iterative refinements that yield improved factual consistency and reduced hallucinations in responses. Our experiments on ASQA, ELI5, and QAMPARI datasets demonstrate our method’s superiority in enhancing correctness, fluency, and citation quality.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.149.pdf",
        "keywords": [
            "hallucination",
            "factual consistency",
            "rectifying factual consistency",
            "citation",
            "citation errors",
            "fluency",
            "llms"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite these strides, challenges on hallucination and factual inconsistency continue to impede their wider real-world adoption.\"; \"However, challenges remain, particularly regarding referencing erroneous evidence (citation errors) and generating information not present in the evidence (hallucination).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Despite these strides, challenges on hallucination and factual inconsistency continue to impede their wider real-world adoption.\"; \"However, challenges remain, particularly regarding referencing erroneous evidence (citation errors) and generating information not present in the evidence (hallucination).\""
    },
    {
        "title": "Effective Controllable Bias Mitigation for Classification and Retrieval using Gate Adapters",
        "authors": [
            "Shahed Masoudian",
            "Cornelia Volaucnik",
            "Markus Schedl",
            "Navid Rekabsaz"
        ],
        "published": "2024",
        "summary": "Bias mitigation of Language Models has been the topic of many studies with a recent focus on learning separate modules like adapters for on-demand debiasing. Besides optimizing for a modularized debiased model, it is often critical in practice to control the degree of bias reduction at inference time, e.g., in order to tune for a desired performance-fairness trade-off in search results or to control the strength of debiasing in classification tasks. In this paper, we introduce Controllable Gate Adapter (ConGater), a novel modular gating mechanism with adjustable sensitivity parameters, %In addition to better perseverance of task performance and enhanced information removal, which allows for a gradual transition from the biased state of the model to the fully debiased version at inference time. We demonstrate ConGater performance by (1) conducting adversarial debiasing experiments with three different models on three classification tasks with four protected attributes, and (2) reducing the bias of search results through fairness list-wise regularization to enable adjusting a trade-off between performance and fairness metrics. Our experiments on the classification tasks show that compared to baselines of the same caliber, ConGater can maintain higher task performance while containing less information regarding the attributes. Our results on the retrieval task show that the fully debiased ConGater can achieve the same fairness performance while maintaining more than twice as high task performance than recent strong baselines. Overall, besides strong performance ConGater enables the continuous transitioning between biased and debiased states of models, enhancing personalization of use and interpretability through controllability.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.150.pdf",
        "keywords": [
            "adapters",
            "gate adapters",
            "debiasing",
            "classification",
            "controllable gate adapter",
            "controllable bias",
            "bias mitigation",
            "modular gating"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Bias mitigation of Language Models has been the topic of many studies\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Bias mitigation of Language Models has been the topic of many studies\""
    },
    {
        "title": "A RelEntLess Benchmark for Modelling Graded Relations between Named Entities",
        "authors": [
            "Asahi Ushio",
            "Jose Camacho-Collados",
            "Steven Schockaert"
        ],
        "published": "2024",
        "summary": "Relations such as “is influenced by”, “is known for” or “is a competitor of” are inherently graded: we can rank entity pairs based on how well they satisfy these relations, but it is hard to draw a line between those pairs that satisfy them and those that do not. Such graded relations play a central role in many applications, yet they are typically not covered by existing Knowledge Graphs. In this paper, we consider the possibility of using Large Language Models (LLMs) to fill this gap. To this end, we introduce a new benchmark, in which entity pairs have to be ranked according to how much they satisfy a given graded relation. The task is formulated as a few-shot ranking problem, where models only have access to a description of the relation and five prototypical instances. We use the proposed benchmark to evaluate state-of-the-art relation embedding strategies as well as several publicly available LLMs and closed conversational models such as GPT-4. We find that smaller language models struggle to outperform a naive baseline. Overall, the best results are obtained with the 11B parameter Flan-T5 model and the 13B parameter OPT model, where further increasing the model size does not seem to be beneficial. For all models, a clear gap with human performance remains.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.152.pdf",
        "keywords": [
            "graded",
            "graded relations",
            "modelling graded relations",
            "language models",
            "art relation embedding",
            "ranked",
            "benchmark"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Overall, the best results are obtained with the 11B parameter Flan-T5 model and the 13B parameter OPT model, where further increasing the model size does not seem to be beneficial. For all models, a clear gap with human performance remains.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Overall, the best results are obtained with the 11B parameter Flan-T5 model and the 13B parameter OPT model, where further increasing the model size does not seem to be beneficial. For all models, a clear gap with human performance remains.\""
    },
    {
        "title": "Graph Guided Question Answer Generation for Procedural Question-Answering",
        "authors": [
            "Hai Pham",
            "Isma Hadji",
            "Xinnuo Xu",
            "Ziedune Degutyte",
            "Jay Rainey",
            "Evangelos Kazakos",
            "Afsaneh Fazly",
            "Georgios Tzimiropoulos",
            "Brais Martinez"
        ],
        "published": "2024",
        "summary": "In this paper, we focus on task-specific question answering (QA). To this end, we introduce a method for generating exhaustive and high-quality training data, which allows us to train compact (e.g., run on a mobile device), task-specific QA models that are competitive against GPT variants. The key technological enabler is a novel mechanism for automatic question-answer generation from procedural text which can ingest large amounts of textual instructions and produce exhaustive in-domain QA training data. While current QA data generation methods can produce well-formed and varied data, their non-exhaustive nature is sub-optimal for training a QA model. In contrast, we leverage the highly structured aspect of procedural text and represent each step and the overall flow of the procedure as graphs. We then condition on graph nodes to automatically generate QA pairs in an exhaustive and controllable manner. Comprehensive evaluations of our method show that: 1) small models trained with our data achieve excellent performance on the target QA task, even exceeding that of GPT3 and ChatGPT despite being several orders of magnitude smaller. 2) semantic coverage is the key indicator for downstream QA performance. Crucially, while large language models excel at syntactic diversity, this does not necessarily result in improvements on the end QA model. In contrast, the higher semantic coverage provided by our method is critical for QA performance.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.154.pdf",
        "keywords": [
            "procedural question answering",
            "question answer generation",
            "task specific question answering",
            "coverage"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Crucially, while large language models excel at syntactic diversity, this does not necessarily result in improvements on the end QA model.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Crucially, while large language models excel at syntactic diversity, this does not necessarily result in improvements on the end QA model.\""
    },
    {
        "title": "Contrastive Decoding Reduces Hallucinations in Large Multilingual Machine Translation Models",
        "authors": [
            "Jonas Waldendorf",
            "Barry Haddow",
            "Alexandra Birch"
        ],
        "published": "2024",
        "summary": "In Neural Machine Translation (NMT), models will sometimes generate repetitive or fluent output that is not grounded in the source sentence. This phenomenon is known as hallucination and is a problem even in large-scale multilingual translation models. We propose to use Contrastive Decoding, an algorithm developed to improve generation from unconditional language models, to mitigate hallucinations in NMT. Specifically, we maximise the log-likelihood difference between a model and the same model with reduced contribution from the encoder outputs. Additionally, we propose an alternative implementation of Contrastive Decoding that dynamically weights the difference based on the maximum probability in the output distribution to reduce the effect of CD when the model is confident of its prediction. We evaluate our methods using the Small (418M) and Medium (1.2B) M2M models across 21 low and medium-resource language pairs. Our results show a 14.6 ± 0.5 and 11.0 ± 0.6 maximal increase in the mean COMET scores for the Small and Medium models on those sentences for which the M2M models initially generate a hallucination., respectively.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.155.pdf",
        "keywords": [
            "hallucination",
            "comet",
            "machine translation",
            "neural machine translation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"In Neural Machine Translation (NMT), models will sometimes generate repetitive or fluent output that is not grounded in the source sentence. This phenomenon is known as hallucination and is a problem even in large-scale multilingual translation models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"In Neural Machine Translation (NMT), models will sometimes generate repetitive or fluent output that is not grounded in the source sentence. This phenomenon is known as hallucination and is a problem even in large-scale multilingual translation models.\""
    },
    {
        "title": "Leveraging fine-tuned Large Language Models with LoRA for Effective Claim, Claimer, and Claim Object Detection",
        "authors": [
            "Sotiris Kotitsas",
            "Panagiotis Kounoudis",
            "Eleni Koutli",
            "Haris Papageorgiou"
        ],
        "published": "2024",
        "summary": "Misinformation and disinformation phenomena existed long before the advent of digital technologies. The exponential use of social media platforms, whose information feeds have created the conditions for many to many communication and instant amplification of the news has accelerated the diffusion of inaccurate and misleading information. As a result, the identification of claims have emerged as a pivotal technology for combating the influence of misinformation and disinformation within news media. Most existing work has concentrated on claim analysis at the sentence level, neglecting the crucial exploration of supplementary attributes such as the claimer and the claim object of the claim or confining it by limiting its scope to a predefined list of topics. Furthermore, previous research has been mostly centered around political debates, Wikipedia articles, and COVID-19 related content. By leveraging the advanced capabilities of Large Language Models (LLMs) in Natural Language Understanding (NLU) and text generation, we propose a novel architecture utilizing LLMs finetuned with LoRA to transform the claim, claimer and claim object detection task into a Question Answering (QA) setting. We evaluate our approach in a dataset of 867 scientific news articles of 3 domains (Health, Climate Change, Nutrition) (HCN), which are human annotated with the major claim, the claimer and the object of the major claim. We also evaluate our proposed model in the benchmark dataset of NEWSCLAIMS. Experimental and qualitative results showcase the effectiveness of the proposed approach. We make our dataset publicly available to encourage further research.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.156.pdf",
        "keywords": [
            "claimer",
            "claim object detection",
            "question answering",
            "finetuned",
            "newsclaims",
            "lora",
            "social media",
            "nutrition",
            "natural"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation of LLMs is mentioned in the abstract, but it mentions \"neglecting the crucial exploration of supplementary attributes\" and \"limiting its scope to a predefined list of topics\" as limitations of previous research, which may indirectly imply limitations of LLMs in handling certain tasks, but it is not explicitly stated.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation of LLMs is mentioned in the abstract, but it mentions \"neglecting the crucial exploration of supplementary attributes\" and \"limiting its scope to a predefined list of topics\" as limitations of previous research, which may indirectly imply limitations of LLMs in handling certain tasks, but it is not explicitly stated."
    },
    {
        "title": "GUMsley: Evaluating Entity Salience in Summarization for 12 English Genres",
        "authors": [
            "Jessica Lin",
            "Amir Zeldes"
        ],
        "published": "2024",
        "summary": "As NLP models become increasingly capable of understanding documents in terms of coherent entities rather than strings, obtaining the most salient entities for each document is not only an important end task in itself but also vital for Information Retrieval (IR) and other downstream applications such as controllable summarization. In this paper, we present and evaluate GUMsley, the first entity salience dataset covering all named and non-named salient entities for 12 genres of English text, aligned with entity types, Wikification links and full coreference resolution annotations. We promote a strict definition of salience using human summaries and demonstrate high inter-annotator agreement for salience based on whether a source entity is mentioned in the summary. Our evaluation shows poor performance by pre-trained SOTA summarization models and zero-shot LLM prompting in capturing salient entities in generated summaries. We also show that predicting or providing salient entities to several model architectures enhances performance and helps derive higher-quality summaries by alleviating the entity hallucination problem in existing abstractive summarization.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.158.pdf",
        "keywords": [
            "summarization",
            "entity hallucination",
            "entity salience",
            "genres",
            "non named salient entities",
            "resolution"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our evaluation shows poor performance by pre-trained SOTA summarization models and zero-shot LLM prompting in capturing salient entities in generated summaries.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Our evaluation shows poor performance by pre-trained SOTA summarization models and zero-shot LLM prompting in capturing salient entities in generated summaries.\""
    },
    {
        "title": "Sensitivity, Performance, Robustness: Deconstructing the Effect of Sociodemographic Prompting",
        "authors": [
            "Tilman Beck",
            "Hendrik Schuff",
            "Anne Lauscher",
            "Iryna Gurevych"
        ],
        "published": "2024",
        "summary": "Annotators’ sociodemographic backgrounds (i.e., the individual compositions of their gender, age, educational background, etc.) have a strong impact on their decisions when working on subjective NLP tasks, such as toxic language detection. Often, heterogeneous backgrounds result in high disagreements. To model this variation, recent work has explored sociodemographic prompting, a technique, which steers the output of prompt-based models towards answers that humans with specific sociodemographic profiles would give. However, the available NLP literature disagrees on the efficacy of this technique — it remains unclear for which tasks and scenarios it can help, and the role of the individual factors in sociodemographic prompting is still unexplored. We address this research gap by presenting the largest and most comprehensive study of sociodemographic prompting today. We use it to analyze its influence on model sensitivity, performance and robustness across seven datasets and six instruction-tuned model families. We show that sociodemographic information affects model predictions and can be beneficial for improving zero-shot learning in subjective NLP tasks.However, its outcomes largely vary for different model types, sizes, and datasets, and are subject to large variance with regards to prompt formulations. Most importantly, our results show that sociodemographic prompting should be used with care when used for data annotation or studying LLM alignment.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.159.pdf",
        "keywords": [
            "robustness",
            "sociodemographic",
            "sociodemographic prompting",
            "sociodemographic information",
            "subjective nlp",
            "scenarios"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, its outcomes largely vary for different model types, sizes, and datasets, and are subject to large variance with regards to prompt formulations. Most importantly, our results show that sociodemographic prompting should be used with care when used for data annotation or studying LLM alignment.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, its outcomes largely vary for different model types, sizes, and datasets, and are subject to large variance with regards to prompt formulations. Most importantly, our results show that sociodemographic prompting should be used with care when used for data annotation or studying LLM alignment.\""
    },
    {
        "title": "Frequency Explains the Inverse Correlation of Large Language Models’ Size, Training Data Amount, and Surprisal’s Fit to Reading Times",
        "authors": [
            "Byung-Doh Oh",
            "Shisen Yue",
            "William Schuler"
        ],
        "published": "2024",
        "summary": "Recent studies have shown that as Transformer-based language models become larger and are trained on very large amounts of data, the fit of their surprisal estimates to naturalistic human reading times degrades. The current work presents a series of analyses showing that word frequency is a key explanatory factor underlying these two trends. First, residual errors from four language model families on four corpora show that the inverse correlation between model size and fit to reading times is the strongest on the subset of least frequent words, which is driven by excessively accurate predictions of larger model variants. Additionally, training dynamics reveal that during later training steps, all model variants learn to predict rare words and that larger model variants do so more accurately, which explains the detrimental effect of both training data amount and model size on fit to reading times. Finally, a feature attribution analysis demonstrates that larger model variants are able to accurately predict rare words based on both an effectively longer context window size as well as stronger local associations compared to smaller model variants. Taken together, these results indicate that Transformer-based language models’ surprisal estimates diverge from human-like expectations due to the superhumanly complex associations they learn for predicting rare words.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.162.pdf",
        "keywords": [
            "surprisal",
            "surprisal estimates",
            "surprisal estimates diverge",
            "inverse correlation",
            "reading times",
            "language models",
            "human reading times",
            "model variants",
            "s fit",
            "frequency"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent studies have shown that as Transformer-based language models become larger and are trained on very large amounts of data, the fit of their surprisal estimates to naturalistic human reading times degrades... Taken together, these results indicate that Transformer-based language models’ surprisal estimates diverge from human-like expectations due to the superhumanly complex associations they learn for predicting rare words.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Recent studies have shown that as Transformer-based language models become larger and are trained on very large amounts of data, the fit of their surprisal estimates to naturalistic human reading times degrades... Taken together, these results indicate that Transformer-based language models’ surprisal estimates diverge from human-like expectations due to the superhumanly complex associations they learn for predicting rare words.\""
    },
    {
        "title": "Presentations by the Humans and For the Humans: Harnessing LLMs for Generating Persona-Aware Slides from Documents",
        "authors": [
            "Ishani Mondal",
            "Shwetha S",
            "Anandhavelu Natarajan",
            "Aparna Garimella",
            "Sambaran Bandyopadhyay",
            "Jordan Boyd-Graber"
        ],
        "published": "2024",
        "summary": "Scientific papers and slides are two different representations of the same underlying information, but both require substantial work to prepare. While there had been prior efforts on automating document-to-slides generation, there is still a pressing need of customizing the presentation of content aligning with the persona of target audience or duration of presentation. This paper first introduces the concept of end-user specification-aware document to slides conversion that incorporates end-user specifications into the conversion process. For this, we initially introduce a new dataset reuse the existing SciDuet dataset consisting of pairs of papers and corresponding slides decks from recent years’ *ACL conferences to create four persona-aware configurations. Secondly, we present Persona-Aware-D2S, a novel approach by finetuning LLMs using target audience feedback to create persona-aware slides from scientific documents. Our evaluation on both automated metrics and qualitative human evaluation suggests that by incorporating end-user specifications into the conversion process, our model can create presentations that are not only informative but also tailored to expectations and cognitive abilities of target audience.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.163.pdf",
        "keywords": [
            "the presentation",
            "slides conversion",
            "end user",
            "end user specification",
            "generating"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but mentions \"finetuning LLMs\" which implies that the paper discusses LLMs, but does not explicitly mention any limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but mentions \"finetuning LLMs\" which implies that the paper discusses LLMs, but does not explicitly mention any limitations."
    },
    {
        "title": "ToPro: Token-Level Prompt Decomposition for Cross-Lingual Sequence Labeling Tasks",
        "authors": [
            "Bolei Ma",
            "Ercong Nie",
            "Shuzhou Yuan",
            "Helmut Schmid",
            "Michael Färber",
            "Frauke Kreuter",
            "Hinrich Schuetze"
        ],
        "published": "2024",
        "summary": "Prompt-based methods have been successfully applied to multilingual pretrained language models for zero-shot cross-lingual understanding. However, most previous studies primarily focused on sentence-level classification tasks, and only a few considered token-level labeling tasks such as Named Entity Recognition (NER) and Part-of-Speech (POS) tagging. In this paper, we propose Token-Level Prompt Decomposition (ToPro), which facilitates the prompt-based method for token-level sequence labeling tasks. The ToPro method decomposes an input sentence into single tokens and applies one prompt template to each token. Our experiments on multilingual NER and POS tagging datasets demonstrate that ToPro-based fine-tuning outperforms Vanilla fine-tuning and Prompt-Tuning in zero-shot cross-lingual transfer, especially for languages that are typologically different from the source language English. Our method also attains state-of-the-art performance when employed with the mT5 model. Besides, our exploratory study in multilingual large language models shows that ToPro performs much better than the current in-context learning method. Overall, the performance improvements show that ToPro could potentially serve as a novel and simple benchmarking method for sequence labeling tasks.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.164.pdf",
        "keywords": [
            "prompt decomposition",
            "sequence labeling",
            "token level sequence labeling"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions \"multilingual large language models\" and \"multilingual pretrained language models\", indicating a discussion of LLMs, but without mentioning any limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions \"multilingual large language models\" and \"multilingual pretrained language models\", indicating a discussion of LLMs, but without mentioning any limitations."
    },
    {
        "title": "Small Language Models Improve Giants by Rewriting Their Outputs",
        "authors": [
            "Giorgos Vernikos",
            "Arthur Brazinskas",
            "Jakub Adamek",
            "Jonathan Mallinson",
            "Aliaksei Severyn",
            "Eric Malmi"
        ],
        "published": "2024",
        "summary": "Despite the impressive performance of large language models (LLMs), theyoften lag behind specialized models in various tasks. LLMs only use a fractionof the existing training data for in-context learning, while task-specificmodels harness the full dataset for fine-tuning. In this work, we tackle theproblem of leveraging training data to improve the performance of LLMs withoutfine-tuning. Our approach directly targets LLM predictions without requiringaccess to their weights. We create a pool of candidates from the LLM throughfew-shot prompting and we employ a compact model, the LM-corrector (LMCor),specifically trained to merge these candidates to produce an enhanced output.Our experiments on four natural language generation tasks demonstrate that evena small LMCor model (250M) substantially improves the few-shot performance ofLLMs (62B), matching and even outperforming standard fine-tuning. Furthermore,we illustrate the robustness of LMCor against different prompts, therebyminimizing the need for extensive prompt engineering. Finally, we show thatLMCor can be seamlessly integrated with different LLMs at inference, serving asa plug-and-play module to improve their performance.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.165.pdf",
        "keywords": [
            "small language"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the impressive performance of large language models (LLMs), they often lag behind specialized models in various tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite the impressive performance of large language models (LLMs), they often lag behind specialized models in various tasks.\""
    },
    {
        "title": "System-Level Natural Language Feedback",
        "authors": [
            "Weizhe Yuan",
            "Kyunghyun Cho",
            "Jason Weston"
        ],
        "published": "2024",
        "summary": "Natural language (NL) feedback offers rich insights into user experience. While existing studies focus on an instance-level approach, where feedback is used to refine specific examples, we introduce a framework for system-level use of NL feedback. We show how to use feedback to formalize system-level design decisions in a human-in-the-loop-process – in order to produce better models. In particular this is done through: (i) metric design for tasks; and (ii) language model prompt design for refining model responses. We conduct two case studies of this approach for improving search query and dialog response generation, demonstrating the effectiveness of system-level feedback. We show the combination of system-level and instance-level feedback brings further gains, and that human written instance-level feedback results in more grounded refinements than GPT-3.5 written ones, underlying the importance of human feedback for building systems.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.169.pdf",
        "keywords": [
            "natural language",
            "natural language feedback",
            "human feedback"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"human written instance-level feedback results in more grounded refinements than GPT-3.5 written ones, underlying the importance of human feedback for building systems.\"\n\n(Note: Although the abstract mentions a limitation related to the effectiveness of a specific model (GPT-3.5) in comparison to human feedback, it is not a primary focus of the paper and is only mentioned briefly.)",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"human written instance-level feedback results in more grounded refinements than GPT-3.5 written ones, underlying the importance of human feedback for building systems.\"\n\n(Note: Although the abstract mentions a limitation related to the effectiveness of a specific model (GPT-3.5) in comparison to human feedback, it is not a primary focus of the paper and is only mentioned briefly.)"
    },
    {
        "title": "Syntactic Preposing and Discourse Relations",
        "authors": [
            "Yunfang Dong",
            "Xixian Liao",
            "Bonnie Webber"
        ],
        "published": "2024",
        "summary": "Over 15 years ago, Ward & Birner (2006) suggested that non-canonical constructions in English can serve both to mark information status and to structure the information flow of discourse. One such construction is preposing, where a phrasal constituent appears to the left of its canonical position, typically sentence-initially. But computational work on discourse has, to date, ignored non-canonical syntax. We take account of non-canonical syntax by providing quantitative evidence relating NP/PP preposing to discourse relations. The evidence comes from an LLM mask-filling task that compares the predictions when a mask is inserted between the arguments of an implicit inter-sentential discourse relation — first, when the right-hand argument (Arg2) starts with a preposed constituent, and again, when that constituent is in canonical (post-verbal) position. Results show that (1) the top-ranked mask-fillers in the preposed case agree more often with “gold” annotations in the Penn Discourse TreeBank than they do in the latter case, and (2) preposing in Arg2 can affect the distribution of discourse-relational senses.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.170.pdf",
        "keywords": [
            "right hand argument",
            "relations",
            "discourse relations"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but it mentions the use of an LLM mask-filling task, which implies some limitations in the model's ability to handle certain syntactic constructions, however, the focus is not on the limitations but rather on using the LLM to provide evidence for a linguistic phenomenon.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None, but it mentions the use of an LLM mask-filling task, which implies some limitations in the model's ability to handle certain syntactic constructions, however, the focus is not on the limitations but rather on using the LLM to provide evidence for a linguistic phenomenon."
    },
    {
        "title": "Can we obtain significant success in RST discourse parsing by using Large Language Models?",
        "authors": [
            "Aru Maekawa",
            "Tsutomu Hirao",
            "Hidetaka Kamigaito",
            "Manabu Okumura"
        ],
        "published": "2024",
        "summary": "Recently, decoder-only pre-trained large language models (LLMs), with several tens of billion parameters, have significantly impacted a wide range of natural language processing (NLP) tasks. While encoder-only or encoder-decoder pre-trained language models have already proved to be effective in discourse parsing, the extent to which LLMs can perform this task remains an open research question. Therefore, this paper explores how beneficial such LLMs are for Rhetorical Structure Theory (RST) discourse parsing. Here, the parsing process for both fundamental top-down and bottom-up strategies is converted into prompts, which LLMs can work with. We employ Llama 2 and fine-tune it with QLoRA, which has fewer parameters that can be tuned. Experimental results on three benchmark datasets, RST-DT, Instr-DT, and the GUM corpus, demonstrate that Llama 2 with 70 billion parameters in the bottom-up strategy obtained state-of-the-art (SOTA) results with significant differences. Furthermore, our parsers demonstrated generalizability when evaluated on RST-DT, showing that, in spite of being trained with the GUM corpus, it obtained similar performances to those of existing parsers trained with RST-DT.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.171.pdf",
        "keywords": [
            "language",
            "language models",
            "decoder",
            "encoder decoder",
            "rhetorical structure theory",
            "parsers",
            "encoder",
            "parsing",
            "natural language processing",
            "discourse parsing"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper implies that the extent to which LLMs can perform discourse parsing is an open research question, suggesting that there may be limitations to their ability to do so.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper implies that the extent to which LLMs can perform discourse parsing is an open research question, suggesting that there may be limitations to their ability to do so."
    },
    {
        "title": "Gradient-Based Language Model Red Teaming",
        "authors": [
            "Nevan Wichers",
            "Carson Denison",
            "Ahmad Beirami"
        ],
        "published": "2024",
        "summary": "Red teaming is a common strategy for identifying weaknesses in generative language models (LMs) by producing adversarial prompts that trigger models to generate unsafe responses. Red teaming is instrumental for both model alignment and evaluation, but is labor-intensive and difficult to scale when done by humans. In this paper, we present Gradient-Based Red Teaming (GBRT), a novel red teaming method for automatically generating diverse prompts that are likely to cause an LM to output unsafe responses. GBRT is a form of prompt learning, trained by scoring an LM response with a safety classifier and then backpropagating through the frozen safety classifier and LM to update the prompt. To improve the coherence of input prompts, we introduce two variants that add a realism loss and fine-tune a pretrained model to generate the prompts instead of learning the prompts directly. Our experiments show that GBRT is more effective at finding prompts that trigger an LM to generate unsafe responses than a strong reinforcement learning-based red teaming approach and works even when the LM has been fine-tuned to produce safer outputs.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.175.pdf",
        "keywords": [
            "teaming",
            "red teaming",
            "language model",
            "generative language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Red teaming is a common strategy for identifying weaknesses in generative language models (LMs) by producing adversarial prompts that trigger models to generate unsafe responses.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Red teaming is a common strategy for identifying weaknesses in generative language models (LMs) by producing adversarial prompts that trigger models to generate unsafe responses.\""
    },
    {
        "title": "Do Moral Judgment and Reasoning Capability of LLMs Change with Language? A Study using the Multilingual Defining Issues Test",
        "authors": [
            "Aditi Khandelwal",
            "Utkarsh Agarwal",
            "Kumar Tanmay",
            "Monojit Choudhury"
        ],
        "published": "2024",
        "summary": "This paper explores the moral judgment and moral reasoning abilities exhibited by Large Language Models (LLMs) across languages through the Defining Issues Test. It is a well known fact that moral judgment depends on the language in which the question is asked. We extend the work of beyond English, to 5 new languages (Chinese, Hindi, Russian, Spanish and Swahili), and probe three LLMs – ChatGPT, GPT-4 and Llama2Chat-70B – that shows substantial multilingual text processing and generation abilities. Our study shows that the moral reasoning ability for all models, as indicated by the post-conventional score, is substantially inferior for Hindi and Swahili, compared to Spanish, Russian, Chinese and English, while there is no clear trend for the performance of the latter four languages. The moral judgments too vary considerably by the language.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.176.pdf",
        "keywords": [
            "hindi",
            "moral reasoning",
            "moral judgment",
            "moral reasoning ability",
            "multilingual text",
            "languages",
            "language models",
            "multilingual defining"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our study shows that the moral reasoning ability for all models, as indicated by the post-conventional score, is substantially inferior for Hindi and Swahili, compared to Spanish, Russian, Chinese and English, while there is no clear trend for the performance of the latter four languages.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Our study shows that the moral reasoning ability for all models, as indicated by the post-conventional score, is substantially inferior for Hindi and Swahili, compared to Spanish, Russian, Chinese and English, while there is no clear trend for the performance of the latter four languages.\""
    },
    {
        "title": "Analyzing the Evaluation of Cross-Lingual Knowledge Transfer in Multilingual Language Models",
        "authors": [
            "Sara Rajaee",
            "Christof Monz"
        ],
        "published": "2024",
        "summary": "Recent advances in training multilingual language models on large datasets seem to have shown promising results in knowledge transfer across languages and achieve high performance on downstream tasks. However, we question to what extent the current evaluation benchmarks and setups accurately measure zero-shot cross-lingual knowledge transfer. In this work, we challenge the assumption that high zero-shot performance on target tasks reflects high cross-lingual ability by introducing more challenging setups involving instances with multiple languages. Through extensive experiments and analysis, we show that the observed high performance of multilingual models can be largely attributed to factors not requiring the transfer of actual linguistic knowledge, such as task- and surface-level knowledge. More specifically, we observe what has been transferred across languages is mostly data artifacts and biases, especially for low-resource languages. Our findings highlight the overlooked drawbacks of existing cross-lingual test data and evaluation setups, calling for a more nuanced understanding of the cross-lingual capabilities of multilingual models.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.177.pdf",
        "keywords": [
            "knowledge transfer",
            "language models",
            "multilingual language models",
            "multilingual models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we show that the observed high performance of multilingual models can be largely attributed to factors not requiring the transfer of actual linguistic knowledge, such as task- and surface-level knowledge. More specifically, we observe what has been transferred across languages is mostly data artifacts and biases, especially for low-resource languages.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"we show that the observed high performance of multilingual models can be largely attributed to factors not requiring the transfer of actual linguistic knowledge, such as task- and surface-level knowledge. More specifically, we observe what has been transferred across languages is mostly data artifacts and biases, especially for low-resource languages.\""
    },
    {
        "title": "MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks",
        "authors": [
            "Lei Zhang",
            "Yuge Zhang",
            "Kan Ren",
            "Dongsheng Li",
            "Yuqing Yang"
        ],
        "published": "2024",
        "summary": "The field of machine learning (ML) has gained widespread adoption, leading to significant demand for adapting ML to specific scenarios, which is yet expensive and non-trivial. The predominant approaches towards the automation of solving ML tasks (e.g., AutoML) are often time-consuming and hard to understand for human developers. In contrast, though human engineers have the incredible ability to understand tasks and reason about solutions, their experience and knowledge are often sparse and difficult to utilize by quantitative approaches. In this paper, we aim to bridge the gap between machine intelligence and human knowledge by introducing a novel framework MLCopilot, which leverages the state-of-the-art large language models to develop ML solutions for novel tasks. We showcase the possibility of extending the capability of LLMs to comprehend structured inputs and perform thorough reasoning for solving novel ML tasks. And we find that, after some dedicated design, the LLM can (i) observe from the existing experiences of ML tasks and (ii) reason effectively to deliver promising results for new tasks. The solution generated can be used directly to achieve high levels of competitiveness.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.179.pdf",
        "keywords": [
            "language models",
            "machine learning",
            "machine learning tasks",
            "large language models"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper mentions the potential of LLMs to \"comprehend structured inputs and perform thorough reasoning for solving novel ML tasks\" but does not discuss any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper mentions the potential of LLMs to \"comprehend structured inputs and perform thorough reasoning for solving novel ML tasks\" but does not discuss any limitations of LLMs."
    },
    {
        "title": "Text-Guided Image Clustering",
        "authors": [
            "Andreas Stephan",
            "Lukas Miklautz",
            "Kevin Sidak",
            "Jan Philip Wahle",
            "Bela Gipp",
            "Claudia Plant",
            "Benjamin Roth"
        ],
        "published": "2024",
        "summary": "Image clustering divides a collection of images into meaningful groups, typically interpreted post-hoc via human-given annotations. Those are usually in the form of text, begging the question of using text as an abstraction for image clustering. Current image clustering methods, however, neglect the use of generated textual descriptions. We, therefore, propose Text-Guided Image Clustering, i.e., generating text using image captioning and visual question-answering (VQA) models and subsequently clustering the generated text. Further, we introduce a novel approach to inject task- or domain knowledge for clustering by prompting VQA models. Across eight diverse image clustering datasets, our results show that the obtained text representations often outperform image features. Additionally, we propose a counting-based cluster explainability method. Our evaluations show that the derived keyword-based explanations describe clusters better than the respective cluster accuracy suggests. Overall, this research challenges traditional approaches and paves the way for a paradigm shift in image clustering, using generated text.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.180.pdf",
        "keywords": [
            "image clustering",
            "cluster",
            "guided image clustering",
            "text guided image clustering"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Current image clustering methods, however, neglect the use of generated textual descriptions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Current image clustering methods, however, neglect the use of generated textual descriptions.\""
    },
    {
        "title": "CCPrefix: Counterfactual Contrastive Prefix-Tuning for Many-Class Classification",
        "authors": [
            "Yang Li",
            "Canran Xu",
            "Guodong Long",
            "Tao Shen",
            "Chongyang Tao",
            "Jing Jiang"
        ],
        "published": "2024",
        "summary": "Recently, prefix-tuning was proposed to efficiently adapt pre-trained language models to a broad spectrum of natural language classification tasks. It leverages soft prefix as task-specific indicators and language verbalizers as categorical-label mentions to narrow the formulation gap from pre-training language models. However, when the label space increases considerably (i.e., many-class classification), such a tuning technique suffers from a verbalizer ambiguity problem since the many-class labels are represented by semantic-similar verbalizers in short language phrases. To overcome this, inspired by the human-decision process that the most ambiguous classes would be mulled over for an instance, we propose a brand-new prefix-tuning method, Counterfactual Contrastive Prefix-tuning (CCPrefix), for many-class classification. Basically, an instance-dependent soft prefix, derived from fact-counterfactual pairs in the label space, is leveraged to complement the language verbalizers in many-class classification. We conduct experiments on many-class benchmark datasets in both the fully supervised setting and the few-shot setting, which indicates that our model outperforms former baselines.",
        "pdf_link": "https://aclanthology.org/2024.eacl-long.181.pdf",
        "keywords": [
            "tuning",
            "prefix tuning",
            "counterfactual contrastive prefix tuning",
            "soft prefix",
            "language verbalizers"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, when the label space increases considerably (i.e., many-class classification), such a tuning technique suffers from a verbalizer ambiguity problem since the many-class labels are represented by semantic-similar verbalizers in short language phrases.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, when the label space increases considerably (i.e., many-class classification), such a tuning technique suffers from a verbalizer ambiguity problem since the many-class labels are represented by semantic-similar verbalizers in short language phrases.\""
    },
    {
        "title": "French GossipPrompts: Dataset For Prevention of Generating French Gossip Stories By LLMs",
        "authors": [
            "Msvpj Sathvik",
            "Abhilash Dowpati",
            "Revanth Narra"
        ],
        "published": "2024",
        "summary": "The realm of Large Language Models (LLMs) is undergoing a continuous and dynamic transformation. These state-of-the-art LLMs showcase an impressive ability to craft narratives based on contextual cues, highlighting their skill in comprehending and producing text resembling human writing. However, there exists a potential risk: the potential inclination of LLMs to create gossips when prompted with specific contexts. These LLMs possess the capacity to generate stories rooted in the context provided by the prompts. Yet, this very capability carries a risk of generating gossips given the context as input. To mitigate this, we introduce a dataset named “French GossipPrompts” designed for identifying prompts that lead to the creation of gossipy content in the French language. This dataset employs binary classification, categorizing whether a given prompt generates gossip or not. The dataset comprises a total of 7253 individual prompts. We have developed classification models and achieved an accuracy of 89.95%.",
        "pdf_link": "https://aclanthology.org/2024.eacl-short.1.pdf",
        "keywords": [
            "classification",
            "prevention",
            "french language",
            "inclination"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, there exists a potential risk: the potential inclination of LLMs to create gossips when prompted with specific contexts.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, there exists a potential risk: the potential inclination of LLMs to create gossips when prompted with specific contexts.\""
    },
    {
        "title": "Mitigating Hallucinations and Off-target Machine Translation with Source-Contrastive and Language-Contrastive Decoding",
        "authors": [
            "Rico Sennrich",
            "Jannis Vamvas",
            "Alireza Mohammadshahi"
        ],
        "published": "2024",
        "summary": "Hallucinations and off-target translation remain unsolved problems in MT, especially for low-resource languages and massively multilingual models. In this paper, we introduce two related methods to mitigate these failure cases with a modified decoding objective, without either requiring retraining or external models. In source-contrastive decoding, we search for a translation that is probable given the correct input, but improbable given a random input segment. In language-contrastive decoding, we search for a translation that is probable, but improbable given the wrong language indicator token. Experiments on the massively multilingual models M2M-100 (418M) and SMaLL-100 show that these methods suppress hallucinations and off-target translations, reducing the number of translations with segment-level chrF2 below 10 by 67-83% on average across 57 tested translation directions. In a proof of concept on out-of-English translation, we also show that we can suppress off-target translations with large language models. We release code upon acceptance.",
        "pdf_link": "https://aclanthology.org/2024.eacl-short.4.pdf",
        "keywords": [
            "language models",
            "off target machine translation"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Hallucinations and off-target translation remain unsolved problems in MT, especially for low-resource languages and massively multilingual models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Hallucinations and off-target translation remain unsolved problems in MT, especially for low-resource languages and massively multilingual models.\""
    },
    {
        "title": "Injecting Wiktionary to improve token-level contextual representations using contrastive learning",
        "authors": [
            "Anna Mosolova",
            "Marie Candito",
            "Carlos Ramisch"
        ],
        "published": "2024",
        "summary": "While static word embeddings are blind to context, for lexical semantics tasks context is rather too present in contextual word embeddings, vectors of same-meaning occurrences being too different (Ethayarajh, 2019). Fine-tuning pre-trained language models (PLMs) using contrastive learning was proposed, leveraging automatically self-augmented examples (Liu et al., 2021b). In this paper, we investigate how to inject a lexicon as an alternative source of supervision, using the English Wiktionary. We also test how dimensionality reduction impacts the resulting contextual word embeddings. We evaluate our approach on the Word-In-Context (WiC) task, in the unsupervised setting (not using the training set). We achieve new SoTA result on the original WiC test set. We also propose two new WiC test sets for which we show that our fine-tuning method achieves substantial improvements. We also observe improvements, although modest, for the semantic frame induction task. Although we experimented on English to allow comparison with related work, our method is adaptable to the many languages for which large Wiktionaries exist.",
        "pdf_link": "https://aclanthology.org/2024.eacl-short.5.pdf",
        "keywords": [
            "dimensionality reduction",
            "contrastive learning",
            "contextual word embeddings",
            "self augmented examples",
            "language",
            "lexical semantics",
            "wic test"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While static word embeddings are blind to context, for lexical semantics tasks context is rather too present in contextual word embeddings, vectors of same-meaning occurrences being too different\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While static word embeddings are blind to context, for lexical semantics tasks context is rather too present in contextual word embeddings, vectors of same-meaning occurrences being too different\""
    },
    {
        "title": "Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains",
        "authors": [
            "Sanjana Ramprasad",
            "Kundan Krishna",
            "Zachary Lipton",
            "Byron Wallace"
        ],
        "published": "2024",
        "summary": "Recent work has shown that large language models (LLMs) are capable of generating summaries zero-shot—i.e., without explicit supervision—that, under human assessment, are often comparable or even preferred to manually composed reference summaries. However, this prior work has focussed almost exclusively on evaluating news article summarization. How do zero-shot summarizers perform in other (potentially more specialized) domains?In this work we evaluate zero-shot generated summaries across specialized domains including: biomedical articles, and legal bills (in addition to standard news benchmarks for reference). We focus especially on the factuality of outputs. We acquire annotations from domain experts to identify inconsistencies in summaries and systematically categorize these errors. We analyze whether the prevalence of a given domain in the pretraining corpus affects extractiveness and faithfulness of generated summaries of articles in this domain. We release all collected annotations to facilitate additional research toward measuring and realizing factually accurate summarization, beyond news articles (The dataset can be downloaded from https://anonymous.4open.science/r/zero_shot_faceval_domains-9B83)",
        "pdf_link": "https://aclanthology.org/2024.eacl-short.7.pdf",
        "keywords": [
            "factuality",
            "extractiveness",
            "summarization",
            "news article summarization"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We acquire annotations from domain experts to identify inconsistencies in summaries and systematically categorize these errors.\"\n\nThis paper mentions the limitation of LLMs in generating factually accurate summaries, but it is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We acquire annotations from domain experts to identify inconsistencies in summaries and systematically categorize these errors.\"\n\nThis paper mentions the limitation of LLMs in generating factually accurate summaries, but it is not the primary focus of the paper."
    },
    {
        "title": "Characterizing the Confidence of Large Language Model-Based Automatic Evaluation Metrics",
        "authors": [
            "Rickard Stureborg",
            "Dimitris Alikaniotis",
            "Yoshi Suhara"
        ],
        "published": "2024",
        "summary": "There has recently been a growing interest in using Large Language Models (LLMs) to evaluate NLP tasks automatically. Considerable research effort has been put into improving such systems towards achieving high correlations with human judgement. However, it is still unclear what level of correlation is good enough for practical applications of LLM-based automatic evaluation systems. This paper characterizes these LLM evaluators’ confidence in ranking candidate NLP models and develops a configurable Monte Carlo simulation method. We show that even automatic metrics with low correlation with human judgement can reach high-confidence rankings of candidate models with reasonable evaluation set sizes (100s of examples). Further, we describe tradeoff curves between the LLM evaluator performance (i.e., correlation with humans) and evaluation set size; loss in correlation can be compensated with modest increases in the evaluation set size. We validate our results on RoSE, a text summarization dataset, and find our estimates of confidence align with empirical observations.Code available at https://github.com/rickardstureborg/llm-eval-confidence",
        "pdf_link": "https://aclanthology.org/2024.eacl-short.9.pdf",
        "keywords": [
            "automatic evaluation metrics",
            "automatic metrics",
            "confidence",
            "language model"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, it is still unclear what level of correlation is good enough for practical applications of LLM-based automatic evaluation systems.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, it is still unclear what level of correlation is good enough for practical applications of LLM-based automatic evaluation systems.\""
    },
    {
        "title": "Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance",
        "authors": [
            "Adrian Theuma",
            "Ehsan Shareghi"
        ],
        "published": "2024",
        "summary": "Large language models (LLMs) have exhibited an array of reasoning capabilities but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount. We explore the potential of language model augmentation with external tools to mitigate these limitations and offload certain reasoning steps to external tools that are more suited for the task, instead of solely depending on the LLM’s inherent abilities. More concretely, using financial domain question answering datasets, we apply supervised finetuning on a LLAMA-2 13B CHAT model to act both as a task router and task solver. The task router dynamically directs a question to either be answered internally by the LLM or externally via the right tool from the tool set. Our tool-equipped SFT model, RAVEN, demonstrates an improvement of 35.2% and 5.06% over the base model and SFT-only baselines, respectively, and is highly competitive with strong GPT-3.5 results. To the best of our knowledge, our work is the first that investigates tool augmentation of language models for the finance domain.",
        "pdf_link": "https://aclanthology.org/2024.eacl-short.10.pdf",
        "keywords": [
            "hallucination",
            "finance",
            "language model",
            "tabular data analysis",
            "equipping language",
            "large language models",
            "tool use"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models (LLMs) have exhibited an array of reasoning capabilities but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Large language models (LLMs) have exhibited an array of reasoning capabilities but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount.\""
    },
    {
        "title": "On the Benefits of Fine-Grained Loss Truncation: A Case Study on Factuality in Summarization",
        "authors": [
            "Lorenzo Jaime Flores",
            "Arman Cohan"
        ],
        "published": "2024",
        "summary": "Text summarization and simplification are among the most widely used applications of AI. However, such models are often prone to hallucination, which can result from training models on unaligned data. One efficient approach to address this issue is Loss Truncation (Kang and Hashimoto, 2020), an approach to modify the standard log loss to adaptively remove noisy examples during training. However, we find that LT alone yields a considerable number of hallucinated entities on various datasets. We study the behavior of the underlying losses between factual and non-factual examples, to understand and refine the performance of LT. We demonstrate that LT’s performance is limited when the underlying assumption that noisy targets have higher NLL loss is not satisfied, and find that word-level NLL among entities provides better signal for distinguishing factuality. We then leverage this to propose a fine-grained NLL loss and fine-grained data cleaning strategies, and observe improvements in hallucination reduction across some datasets. Our work is available at https://github.com/yale-nlp/Simplification-Projects.",
        "pdf_link": "https://aclanthology.org/2024.eacl-short.13.pdf",
        "keywords": [
            "factuality",
            "hallucination",
            "loss truncation",
            "summarization",
            "lt",
            "log loss",
            "nll loss",
            "text summarization",
            "fine"
        ],
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.\n\nPlease provide the next abstract for me to evaluate.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs.\n\nPlease provide the next abstract for me to evaluate."
    },
    {
        "title": "Evaluating Unsupervised Argument Aligners via Generation of Conclusions of Structured Scientific Abstracts",
        "authors": [
            "Yingqiang Gao",
            "Nianlong Gu",
            "Jessica Lam",
            "James Henderson",
            "Richard Hahnloser"
        ],
        "published": "2024",
        "summary": "Scientific abstracts provide a concise summary of research findings, making them a valuable resource for extracting scientific arguments. In this study, we assess various unsupervised approaches for extracting arguments as aligned premise-conclusion pairs: semantic similarity, text perplexity, and mutual information. We aggregate structured abstracts from PubMed Central Open Access papers published in 2022 and evaluate the argument aligners in terms of the performance of language models that we fine-tune to generate the conclusions from the extracted premise given as input prompts. We find that mutual information outperforms the other measures on this task, suggesting that the reasoning process in scientific abstracts hinges mostly on linguistic constructs beyond simple textual similarity.",
        "pdf_link": "https://aclanthology.org/2024.eacl-short.14.pdf",
        "keywords": [
            "mutual information",
            "unsupervised argument",
            "argument aligners",
            "reasoning"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that mutual information outperforms the other measures on this task, suggesting that the reasoning process in scientific abstracts hinges mostly on linguistic constructs beyond simple textual similarity.\"\n\nThis abstract mentions LLMs but does not discuss any limitations of LLMs explicitly. The evidence is about the performance of LLMs on a specific task, but it does not point to any limitation of",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"We find that mutual information outperforms the other measures on this task, suggesting that the reasoning process in scientific abstracts hinges mostly on linguistic constructs beyond simple textual similarity.\"\n\nThis abstract mentions LLMs but does not discuss any limitations of LLMs explicitly. The evidence is about the performance of LLMs on a specific task, but it does not point to any limitation of"
    },
    {
        "title": "Over-Reasoning and Redundant Calculation of Large Language Models",
        "authors": [
            "Cheng-Han Chiang",
            "Hung-yi Lee"
        ],
        "published": "2024",
        "summary": "Large language models (LLMs) can solve problems step-by-step.While this chain-of-thought (CoT) reasoning boosts LLMs’ performance, it is unclear if LLMs know when to use CoT and whether those CoT are always necessary to answer the question. This paper shows that LLMs tend to generate redundant calculations and reasoning on a manually constructed math QA dataset, GSM8K-Zero.GSM8K-Zero is constructed such that the questions can be answered without any calculations, but LLMs, including Llama-2 models and Claude-2, tend to generate lengthy and unnecessary calculations to answer the questions.We also conduct experiments to explain why LLMs generate redundant calculations and reasonings.",
        "pdf_link": "https://aclanthology.org/2024.eacl-short.15.pdf",
        "keywords": [
            "redundant",
            "redundant calculation",
            "language models",
            "over reasoning",
            "large language models",
            "reasoning"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"This paper shows that LLMs tend to generate redundant calculations and reasoning on a manually constructed math QA dataset, GSM8K-Zero.GSM8K-Zero is constructed such that the questions can be answered without any calculations, but LLMs, including Llama-2 models and Claude-2, tend to generate lengthy and unnecessary calculations to answer the questions.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"This paper shows that LLMs tend to generate redundant calculations and reasoning on a manually constructed math QA dataset, GSM8K-Zero.GSM8K-Zero is constructed such that the questions can be answered without any calculations, but LLMs, including Llama-2 models and Claude-2, tend to generate lengthy and unnecessary calculations to answer the questions.\""
    },
    {
        "title": "The Parrot Dilemma: Human-Labeled vs. LLM-augmented Data in Classification Tasks",
        "authors": [
            "Anders Giovanni Møller",
            "Arianna Pera",
            "Jacob Dalsgaard",
            "Luca Aiello"
        ],
        "published": "2024",
        "summary": "In the realm of Computational Social Science (CSS), practitioners often navigate complex, low-resource domains and face the costly and time-intensive challenges of acquiring and annotating data. We aim to establish a set of guidelines to address such challenges, comparing the use of human-labeled data with synthetically generated data from GPT-4 and Llama-2 in ten distinct CSS classification tasks of varying complexity. Additionally, we examine the impact of training data sizes on performance. Our findings reveal that models trained on human-labeled data consistently exhibit superior or comparable performance compared to their synthetically augmented counterparts. Nevertheless, synthetic augmentation proves beneficial, particularly in improving performance on rare classes within multi-class tasks. Furthermore, we leverage GPT-4 and Llama-2 for zero-shot classification and find that, while they generally display strong performance, they often fall short when compared to specialized classifiers trained on moderately sized training sets.",
        "pdf_link": "https://aclanthology.org/2024.eacl-short.17.pdf",
        "keywords": [
            "shot classification",
            "parrot dilemma",
            "computational social science"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Nevertheless, synthetic augmentation proves beneficial, particularly in improving performance on rare classes within multi-class tasks. Furthermore, we leverage GPT-4 and Llama-2 for zero-shot classification and find that, while they generally display strong performance, they often fall short when compared to specialized classifiers trained on moderately sized training sets.\"\n\nThis rating is based on the fact that the paper discusses some limitations",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Nevertheless, synthetic augmentation proves beneficial, particularly in improving performance on rare classes within multi-class tasks. Furthermore, we leverage GPT-4 and Llama-2 for zero-shot classification and find that, while they generally display strong performance, they often fall short when compared to specialized classifiers trained on moderately sized training sets.\"\n\nThis rating is based on the fact that the paper discusses some limitations"
    },
    {
        "title": "Language Model Sentence Completion with a Parser-Driven Rhetorical Control Method",
        "authors": [
            "Joshua Zingale",
            "Jugal Kalita"
        ],
        "published": "2024",
        "summary": "Controlled text generation (CTG) seeks to guide large language model (LLM) output, that statistical language generation would conform to desired criteria. The current study presents a novel CTG algorithm that enforces adherence toward specific rhetorical relations in an LLM sentence-completion context by a parser-driven decoding scheme that requires no model fine-tuning. The method is validated both with automatic and human evaluation.",
        "pdf_link": "https://aclanthology.org/2024.eacl-short.18.pdf",
        "keywords": [
            "sentence completion",
            "rhetorical control",
            "language model sentence completion",
            "language generation",
            "language model",
            "parser",
            "algorithm"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but the need for controlled text generation implies that LLMs may not always produce desired output on their own.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None explicitly mentioned, but the need for controlled text generation implies that LLMs may not always produce desired output on their own."
    },
    {
        "title": "From Partial to Strictly Incremental Constituent Parsing",
        "authors": [
            "Ana Ezquerro",
            "Carlos Gómez-Rodríguez",
            "David Vilares"
        ],
        "published": "2024",
        "summary": "We study incremental constituent parsers to assess their capacity to output trees based on prefix representations alone. Guided by strictly left-to-right generative language models and tree-decoding modules, we build parsers that adhere to a strong definition of incrementality across languages. This builds upon work that asserted incrementality, but that mostly only enforced it on either the encoder or the decoder. Finally, we conduct an analysis against non-incremental and partially incremental models.",
        "pdf_link": "https://aclanthology.org/2024.eacl-short.21.pdf",
        "keywords": [
            "parsers",
            "prefix representations",
            "trees",
            "tree decoding",
            "incremental",
            "definition",
            "languages",
            "generative language models",
            "incremental models",
            "constituent parsing",
            "capacity"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Guided by strictly left-to-right generative language models\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Guided by strictly left-to-right generative language models\""
    },
    {
        "title": "Predict the Next Word: <Humans exhibit uncertainty in this task and language models _____>",
        "authors": [
            "Evgenia Ilia",
            "Wilker Aziz"
        ],
        "published": "2024",
        "summary": "Language models (LMs) are statistical models trained to assign probability to human-generated text. As such, it is reasonable to question whether they approximate linguistic variability exhibited by humans well. This form of statistical assessment is difficult to perform at the passage level, for it requires acceptability judgments (i.e., human evaluation) or a robust automated proxy (which is non-trivial). At the word level, however, given some context, samples from an LM can be assessed via exact matching against a prerecorded dataset of alternative single-word continuations of the available context. We exploit this fact and evaluate the LM’s ability to reproduce variability that humans (in particular, a population of English speakers) exhibit in the ‘next word prediction’ task. This can be seen as assessing a form of calibration, which, in the context of text classification, Baan et al. (2022) termed calibration to human uncertainty. We assess GPT2, BLOOM and ChatGPT and find that they exhibit fairly low calibration to human uncertainty. We also verify the failure of expected calibration error (ECE) to reflect this, and as such, advise the community against relying on it in this setting.",
        "pdf_link": "https://aclanthology.org/2024.eacl-short.22.pdf",
        "keywords": [
            "language models"
        ],
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We assess GPT2, BLOOM and ChatGPT and find that they exhibit fairly low calibration to human uncertainty.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We assess GPT2, BLOOM and ChatGPT and find that they exhibit fairly low calibration to human uncertainty.\""
    },
    {
        "title": "A Prompt Response to the Demand for Automatic Gender-Neutral Translation",
        "authors": [
            "Beatrice Savoldi",
            "Andrea Piergentili",
            "Dennis Fucci",
            "Matteo Negri",
            "Luisa Bentivogli"
        ],
        "published": "2024",
        "summary": "Gender-neutral translation (GNT) that avoids biased and undue binary assumptions is a pivotal challenge for the creation of more inclusive translation technologies. Advancements for this task in Machine Translation (MT), however, are hindered by the lack of dedicated parallel data, which are necessary to adapt MT systems to satisfy neutral constraints. For such a scenario, large language models offer hitherto unforeseen possibilities, as they come with the distinct advantage of being versatile in various (sub)tasks when provided with explicit instructions. In this paper, we explore this potential to automate GNT by comparing MT with the popular GPT-4 model. Through extensive manual analyses, our study empirically reveals the inherent limitations of current MT systems in generating GNTs and provides valuable insights into the potential and challenges associated with prompting for neutrality.",
        "pdf_link": "https://aclanthology.org/2024.eacl-short.23.pdf",
        "keywords": [
            "machine translation",
            "translation technologies",
            "gender neutral translation",
            "automatic gender neutral translation",
            "language models",
            "response"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"our study empirically reveals the inherent limitations of current MT systems in generating GNTs and provides valuable insights into the potential and challenges associated with prompting for neutrality.\"\n\nThis rating is chosen because the abstract mentions the limitations of current MT systems (which includes LLMs) in generating GNTs, but it does not elaborate on these limitations in detail.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"our study empirically reveals the inherent limitations of current MT systems in generating GNTs and provides valuable insights into the potential and challenges associated with prompting for neutrality.\"\n\nThis rating is chosen because the abstract mentions the limitations of current MT systems (which includes LLMs) in generating GNTs, but it does not elaborate on these limitations in detail."
    },
    {
        "title": "Smaller Language Models are Better Zero-shot Machine-Generated Text Detectors",
        "authors": [
            "Niloofar Mireshghallah",
            "Justus Mattern",
            "Sicun Gao",
            "Reza Shokri",
            "Taylor Berg-Kirkpatrick"
        ],
        "published": "2024",
        "summary": "As large language models are becoming more embedded in different user-facing services, it is important to be able to distinguish between human-written and machine-generated text to verify the authenticity of news articles, product reviews, etc. Thus, in this paper we set out to explore whether it is possible to use one language model to identify machine-generated text produced by another language model, in a zero-shot way, even if the two have different architectures and are trained on different data. We find that overall, smaller models are better universal machine-generated text detectors: they can more precisely detect text generated from both small and larger models, without the need for any additional training/data. Interestingly, we find that whether or not the detector and generator models were trained on the same data is not critically important to the detection success. For instance the OPT-125M model has an AUC of 0.90 in detecting GPT4 generations, whereas a larger model from the GPT family, GPTJ-6B, has AUC of 0.65.",
        "pdf_link": "https://aclanthology.org/2024.eacl-short.25.pdf",
        "keywords": [
            "machine generated text",
            "machine generated text detectors",
            "language model"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Interestingly, we find that whether or not the detector and generator models were trained on the same data is not critically important to the detection success. For instance the OPT-125M model has an AUC of 0.90 in detecting GPT4 generations, whereas a larger model from the GPT family, GPTJ-6B, has AUC of 0.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Interestingly, we find that whether or not the detector and generator models were trained on the same data is not critically important to the detection success. For instance the OPT-125M model has an AUC of 0.90 in detecting GPT4 generations, whereas a larger model from the GPT family, GPTJ-6B, has AUC of 0."
    },
    {
        "title": "CharSpan: Utilizing Lexical Similarity to Enable Zero-Shot Machine Translation for Extremely Low-resource Languages",
        "authors": [
            "Kaushal Maurya",
            "Rahul Kejriwal",
            "Maunendra Desarkar",
            "Anoop Kunchukuttan"
        ],
        "published": "2024",
        "summary": "We address the task of machine translation (MT) from extremely low-resource language (ELRL) to English by leveraging cross-lingual transfer from *closely-related* high-resource language (HRL). The development of an MT system for ELRL is challenging because these languages typically lack parallel corpora and monolingual corpora, and their representations are absent from large multilingual language models. Many ELRLs share lexical similarities with some HRLs, which presents a novel modeling opportunity. However, existing subword-based neural MT models do not explicitly harness this lexical similarity, as they only implicitly align HRL and ELRL latent embedding space. To overcome this limitation, we propose a novel, CharSpan, approach based on character-span noise augmentation into the training data of HRL. This serves as a regularization technique, making the model more robust to lexical divergences between the HRL and ELRL, thus facilitating effective cross-lingual transfer. Our method significantly outperformed strong baselines in zero-shot settings on closely related HRL and ELRL pairs from three diverse language families, emerging as the state-of-the-art model for ELRLs.",
        "pdf_link": "https://aclanthology.org/2024.eacl-short.26.pdf",
        "keywords": [
            "lexical similarity",
            "low resource languages",
            "machine translation",
            "zero shot machine translation",
            "character span noise augmentation",
            "subword"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"their representations are absent from large multilingual language models.\"\n\nThis abstract mentions a limitation of large multilingual language models in representing extremely low-resource languages, but it is a minor detail and not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"their representations are absent from large multilingual language models.\"\n\nThis abstract mentions a limitation of large multilingual language models in representing extremely low-resource languages, but it is a minor detail and not the primary focus of the paper."
    },
    {
        "title": "Less is More for Long Document Summary Evaluation by LLMs",
        "authors": [
            "Yunshu Wu",
            "Hayate Iso",
            "Pouya Pezeshkpour",
            "Nikita Bhutani",
            "Estevam Hruschka"
        ],
        "published": "2024",
        "summary": "Large Language Models (LLMs) have shown promising performance in summary evaluation tasks, yet they face challenges such as high computational costs and the Lost-in-the-Middle problem where important information in the middle of long documents is often overlooked. To address these issues, this paper introduces a novel approach, Extract-then-Evaluate, which involves extracting key sentences from a long source document and then evaluating the summary by prompting LLMs. The results reveal that the proposed method not only significantly reduces evaluation costs but also exhibits a higher correlation with human evaluations. Furthermore, we provide practical recommendations for optimal document length and sentence extraction methods, contributing to the development of cost-effective yet more accurate methods for LLM-based text generation evaluation.",
        "pdf_link": "https://aclanthology.org/2024.eacl-short.29.pdf",
        "keywords": [
            "summary evaluation",
            "text generation evaluation",
            "document summary evaluation",
            "language models"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"yet they face challenges such as high computational costs and the Lost-in-the-Middle problem where important information in the middle of long documents is often overlooked.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"yet they face challenges such as high computational costs and the Lost-in-the-Middle problem where important information in the middle of long documents is often overlooked.\""
    },
    {
        "title": "Leveraging ChatGPT in Pharmacovigilance Event Extraction: An Empirical Study",
        "authors": [
            "Zhaoyue Sun",
            "Gabriele Pergola",
            "Byron Wallace",
            "Yulan He"
        ],
        "published": "2024",
        "summary": "With the advent of large language models (LLMs), there has been growing interest in exploring their potential for medical applications. This research aims to investigate the ability of LLMs, specifically ChatGPT, in the context of pharmacovigilance event extraction, of which the main goal is to identify and extract adverse events or potential therapeutic events from textual medical sources. We conduct extensive experiments to assess the performance of ChatGPT in the pharmacovigilance event extraction task, employing various prompts and demonstration selection strategies. The findings demonstrate that while ChatGPT demonstrates reasonable performance with appropriate demonstration selection strategies, it still falls short compared to fully fine-tuned small models. Additionally, we explore the potential of leveraging ChatGPT for data augmentation. However, our investigation reveals that the inclusion of synthesized data into fine-tuning may lead to a decrease in performance, possibly attributed to noise in the ChatGPT-generated labels. To mitigate this, we explore different filtering strategies and find that, with the proper approach, more stable performance can be achieved, although constant improvement remains elusive.",
        "pdf_link": "https://aclanthology.org/2024.eacl-short.30.pdf",
        "keywords": [
            "pharmacovigilance event extraction",
            "chatgpt"
        ],
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, our investigation reveals that the inclusion of synthesized data into fine-tuning may lead to a decrease in performance, possibly attributed to noise in the ChatGPT-generated labels.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, our investigation reveals that the inclusion of synthesized data into fine-tuning may lead to a decrease in performance, possibly attributed to noise in the ChatGPT-generated labels.\""
    },
    {
        "title": "A Comparative Analysis of Conversational Large Language Models in Knowledge-Based Text Generation",
        "authors": [
            "Phillip Schneider",
            "Manuel Klettner",
            "Elena Simperl",
            "Florian Matthes"
        ],
        "published": "2024",
        "summary": "Generating natural language text from graph-structured data is essential for conversational information seeking. Semantic triples derived from knowledge graphs can serve as a valuable source for grounding responses from conversational agents by providing a factual basis for the information they communicate. This is especially relevant in the context of large language models, which offer great potential for conversational interaction but are prone to hallucinating, omitting, or producing conflicting information. In this study, we conduct an empirical analysis of conversational large language models in generating natural language text from semantic triples. We compare four large language models of varying sizes with different prompting techniques. Through a series of benchmark experiments on the WebNLG dataset, we analyze the models’ performance and identify the most common issues in the generated predictions. Our findings show that the capabilities of large language models in triple verbalization can be significantly improved through few-shot prompting, post-processing, and efficient fine-tuning techniques, particularly for smaller models that exhibit lower zero-shot performance.",
        "pdf_link": "https://aclanthology.org/2024.eacl-short.31.pdf",
        "keywords": [
            "language models",
            "natural language text",
            "large language models",
            "knowledge based text generation",
            "knowledge graphs"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"which offer great potential for conversational interaction but are prone to hallucinating, omitting, or producing conflicting information.\"\n\nDoes this paper discuss LLM limitations? Yes, it does, mentioning that they are prone to hallucinating, omitting, or producing conflicting information, but this limitation is only briefly mentioned and not the main focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"which offer great potential for conversational interaction but are prone to hallucinating, omitting, or producing conflicting information.\"\n\nDoes this paper discuss LLM limitations? Yes, it does, mentioning that they are prone to hallucinating, omitting, or producing conflicting information, but this limitation is only briefly mentioned and not the main focus of the paper."
    },
    {
        "title": "Corpus-Steered Query Expansion with Large Language Models",
        "authors": [
            "Yibin Lei",
            "Yu Cao",
            "Tianyi Zhou",
            "Tao Shen",
            "Andrew Yates"
        ],
        "published": "2024",
        "summary": "Recent studies demonstrate that query expansions generated by large language models (LLMs) can considerably enhance information retrieval systems by generating hypothetical documents that answer the queries as expansions. However, challenges arise from misalignments between the expansions and the retrieval corpus, resulting in issues like hallucinations and outdated information due to the limited intrinsic knowledge of LLMs. Inspired by Pseudo Relevance Feedback (PRF), we introduce Corpus-Steered Query Expansion (CSQE) to promote the incorporation of knowledge embedded within the corpus. CSQE utilizes the relevance assessing capability of LLMs to systematically identify pivotal sentences in the initially-retrieved documents. These corpus-originated texts are subsequently used to expand the query together with LLM-knowledge empowered expansions, improving the relevance prediction between the query and the target documents. Extensive experiments reveal that CSQE exhibits strong performance without necessitating any training, especially with queries for which LLMs lack knowledge.",
        "pdf_link": "https://aclanthology.org/2024.eacl-short.34.pdf",
        "keywords": [
            "query expansions",
            "corpus steered query expansion",
            "relevance assessing",
            "relevance prediction"
        ],
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, challenges arise from misalignments between the expansions and the retrieval corpus, resulting in issues like hallucinations and outdated information due to the limited intrinsic knowledge of LLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, challenges arise from misalignments between the expansions and the retrieval corpus, resulting in issues like hallucinations and outdated information due to the limited intrinsic knowledge of LLMs.\""
    },
    {
        "title": "SOCIALITE-LLAMA: An Instruction-Tuned Model for Social Scientific Tasks",
        "authors": [
            "Gourab Dey",
            "Adithya V Ganesan",
            "Yash Kumar Lal",
            "Manal Shah",
            "Shreyashee Sinha",
            "Matthew Matero",
            "Salvatore Giorgi",
            "Vivek Kulkarni",
            "H. Andrew Schwartz"
        ],
        "published": "2024",
        "summary": "Social science NLP tasks, such as emotion or humor detection, are required to capture the semantics along with the implicit pragmatics from text, often with limited amounts of training data. Instruction tuning has been shown to improve the many capabilities of large language models (LLMs) such as commonsense reasoning, reading comprehension, and computer programming. However, little is known about the effectiveness of instruction tuning on the social domain where implicit pragmatic cues are often needed to be captured. We explore the use of instruction tuning for social science NLP tasks and introduce Socialite-Llama — an open-source, instruction-tuned Llama. On a suite of 20 social science tasks, Socialite-Llama improves upon the performance of Llama as well as matches or improves upon the performance of a state-of-the-art, multi-task finetuned model on a majority of them. Further, Socialite-Llama also leads to improvement on 5 out of 6 related social tasks as compared to Llama, suggesting instruction tuning can lead to generalized social understanding. All resources including our code, model and dataset can be found through [bit.ly/socialitellama](https://bit.ly/socialitellama/).",
        "pdf_link": "https://aclanthology.org/2024.eacl-short.40.pdf",
        "keywords": [
            "social science",
            "instruction tuned",
            "social science nlp",
            "language models",
            "social scientific tasks",
            "open source",
            "socialite llama"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract discusses the effectiveness of instruction tuning on social science NLP tasks, but does not mention any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract discusses the effectiveness of instruction tuning on social science NLP tasks, but does not mention any limitations of LLMs."
    },
    {
        "title": "Dynamic Masking Rate Schedules for MLM Pretraining",
        "authors": [
            "Zachary Ankner",
            "Naomi Saphra",
            "Davis Blalock",
            "Jonathan Frankle",
            "Matthew Leavitt"
        ],
        "published": "2024",
        "summary": "Most works on transformers trained with the Masked Language Modeling (MLM) objective use the original BERT model’s fixed masking rate of 15%. We propose to instead dynamically schedule the masking rate throughout training. We find that linearly decreasing the masking rate over the course of pretraining improves average GLUE accuracy by up to 0.46% and 0.25% in BERT-base and BERT-large, respectively, compared to fixed rate baselines. These gains come from exposure to both high and low masking rate regimes, providing benefits from both settings. Our results demonstrate that masking rate scheduling is a simple way to improve the quality of masked language models, achieving up to a 1.89x speedup in pretraining for BERT-base as well as a Pareto improvement for BERT-large.",
        "pdf_link": "https://aclanthology.org/2024.eacl-short.42.pdf",
        "keywords": [
            "masking",
            "masking rate",
            "masking rate scheduling",
            "dynamic masking rate schedules",
            "pretraining"
        ],
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Most works on transformers trained with the Masked Language Modeling (MLM) objective use the original BERT model’s fixed masking rate of 15%.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"Most works on transformers trained with the Masked Language Modeling (MLM) objective use the original BERT model’s fixed masking rate of 15%.\""
    },
    {
        "title": "GPTs Are Multilingual Annotators for Sequence Generation Tasks",
        "authors": [
            "Juhwan Choi",
            "Eunju Lee",
            "Kyohoon Jin",
            "YoungBin Kim"
        ],
        "published": "2024",
        "summary": "Data annotation is an essential step for constructing new datasets. However, the conventional approach of data annotation through crowdsourcing is both time-consuming and expensive. In addition, the complexity of this process increases when dealing with low-resource languages owing to the difference in the language pool of crowdworkers. To address these issues, this study proposes an autonomous annotation method by utilizing large language models, which have been recently demonstrated to exhibit remarkable performance. Through our experiments, we demonstrate that the proposed method is not just cost-efficient but also applicable for low-resource language annotation. Additionally, we constructed an image captioning dataset using our approach and are committed to open this dataset for future study. We have opened our source code for further study and reproducibility.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.2.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, only mentions their \"remarkable performance\".",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, only mentions their \"remarkable performance\"."
    },
    {
        "title": "FlexiQA: Leveraging LLM’s Evaluation Capabilities for Flexible Knowledge Selection in Open-domain Question Answering",
        "authors": [
            "Yuhan Chen",
            "Shuqi Li",
            "Rui Yan"
        ],
        "published": "2024",
        "summary": "Nowadays, large language models (LLMs) have demonstrated their ability to be a powerful knowledge generator of generate-then-read paradigm for open-domain question answering (ODQA). However this new paradigm mainly suffers from the “hallucination” and struggles to handle time-sensitive issue because of its expensive knowledge update costs. On the other hand, retrieve-then-read, as a traditional paradigm, is more limited by the relevance of acquired knowledge to the given question. In order to combine the strengths of both paradigms, and overcome their respective shortcomings, we design a new pipeline called “FlexiQA”, in which we utilize the diverse evaluation capabilities of LLMs to select knowledge effectively and flexibly. First, given a question, we prompt a LLM as a discriminator to identify whether it is time-sensitive. For time-sensitive questions, we follow the retrieve-then-read paradigm to obtain the answer. For the non time-sensitive questions, we further prompt the LLM as an evaluator to select a better document from two perspectives: factuality and relevance. Based on the selected document, we leverage a reader to get the final answer. We conduct extensive experiments on three widely-used ODQA benchmarks, the experimental results fully confirm the effectiveness of our approach.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.4.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However this new paradigm mainly suffers from the “hallucination” and struggles to handle time-sensitive issue because of its expensive knowledge update costs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However this new paradigm mainly suffers from the “hallucination” and struggles to handle time-sensitive issue because of its expensive knowledge update costs.\""
    },
    {
        "title": "NavHint: Vision and Language Navigation Agent with a Hint Generator",
        "authors": [
            "Yue Zhang",
            "Quan Guo",
            "Parisa Kordjamshidi"
        ],
        "published": "2024",
        "summary": "The existing work on vision and language navigation mainly relies on navigation-related losses to establish the connection between vision and language modalities, neglecting aspects of helping the navigation agent build a deep understanding of the visual environment.In our work, we provide indirect supervision to the navigation agent through a hint generator that provides detailed visual descriptions.The hint generator assists the navigation agent in developing a global understanding of the visual environment. It directs the agent’s attention toward related navigation details, including the relevant sub-instruction, potential challenges in recognition and ambiguities in grounding, and the targeted viewpoint description. To train the hint generator, we construct a synthetic dataset based on landmarks in the instructions and visible and distinctive objects in the visual environment.We evaluate our method on the R2R and R4R datasets and achieve state-of-the-art on several metrics. The experimental results demonstrate that generating hints not only enhances the navigation performance but also helps improve the agent’s interpretability.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.7.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Where are we Still Split on Tokenization?",
        "authors": [
            "Rob van der Goot"
        ],
        "published": "2024",
        "summary": "Many Natural Language Processing (NLP) tasks are labeled on the token level, forthese tasks, the first step is to identify the tokens (tokenization). Becausethis step is often considered to be a solved problem, gold tokenization iscommonly assumed. In this paper, we propose an efficient method fortokenization with subword-based language models, and reflect on the status ofperformance on the tokenization task by evaluating on 122 languages in 20different scripts. We show that our proposed model performs on par with thestate-of-the-art, and that tokenization performance is mainly dependent on theamount and consistency of annotated data. We conclude that besidesinconsistencies in the data and exceptional cases the task can be consideredsolved for Latin languages for in-dataset settings (>99.5 F1). However,performance is 0.75 F1 point lower on average for datasets in other scripts andperformance deteriorates in cross-dataset setups.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.9.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, performance is 0.75 F1 point lower on average for datasets in other scripts and performance deteriorates in cross-dataset setups.\"\n\nThis rating is given because the paper discusses a limitation of LLMs in the context of tokenization tasks, specifically the performance deterioration in cross-dataset setups and for non-Latin scripts. The limitation is discussed in moderate detail, providing",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, performance is 0.75 F1 point lower on average for datasets in other scripts and performance deteriorates in cross-dataset setups.\"\n\nThis rating is given because the paper discusses a limitation of LLMs in the context of tokenization tasks, specifically the performance deterioration in cross-dataset setups and for non-Latin scripts. The limitation is discussed in moderate detail, providing"
    },
    {
        "title": "A Methodology for Generative Spelling Correction via Natural Spelling Errors Emulation across Multiple Domains and Languages",
        "authors": [
            "Nikita Martynov",
            "Mark Baushenko",
            "Anastasia Kozlova",
            "Katerina Kolomeytseva",
            "Aleksandr Abramov",
            "Alena Fenogenova"
        ],
        "published": "2024",
        "summary": "Large language models excel in text generation and generalization, however they face challenges in text editing tasks, especially in correcting spelling errors and mistyping.In this paper, we present a methodology for generative spelling correction (SC), tested on English and Russian languages and potentially can be extended to any language with minor changes. Our research mainly focuses on exploring natural spelling errors and mistyping in texts and studying how those errors can be emulated in correct sentences to enrich generative models’ pre-train procedure effectively. We investigate the effects of emulations in various text domains and examine two spelling corruption techniques: 1) first one mimics human behavior when making a mistake through leveraging statistics of errors from a particular dataset, and 2) second adds the most common spelling errors, keyboard miss clicks, and some heuristics within the texts.We conducted experiments employing various corruption strategies, models’ architectures, and sizes in the pre-training and fine-tuning stages and evaluated the models using single-domain and multi-domain test sets. As a practical outcome of our work, we introduce SAGE (Spell checking via Augmentation and Generative distribution Emulation).",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.10.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"however they face challenges in text editing tasks, especially in correcting spelling errors and mistyping.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"however they face challenges in text editing tasks, especially in correcting spelling errors and mistyping.\""
    },
    {
        "title": "How Does In-Context Learning Help Prompt Tuning?",
        "authors": [
            "Simeng Sun",
            "Yang Liu",
            "Dan Iter",
            "Chenguang Zhu",
            "Mohit Iyyer"
        ],
        "published": "2024",
        "summary": "Fine-tuning large language models is becoming ever more impractical due to their rapidly-growing scale. This motivates the use of parameter-efficient adaptation methods such as prompt tuning (PT), which adds a small number of tunable embeddings to an otherwise frozen model, and in-context learning (ICL), in which demonstrations of the task are provided to the model in natural language without any additional training. Recently, (CITATION) propose “instruction prompt tuning” (IPT), which combines PT with ICL by concatenating a natural language demonstration with learned prompt embeddings. While all of these methods have proven effective on different tasks, how they interact with each other remains unexplored. In this paper, we empirically study when and how in-context examples improve prompt tuning by measuring the effectiveness of ICL, PT, and IPT on five text generation tasks with multiple base language models. We observe that (1) IPT does not always outperform PT, and in fact requires the in-context demonstration to be semantically similar to the test input to yield improvements; (2) PT is unstable and exhibits high variance, but combining PT and ICL (into IPT) consistently reduces variance across all five tasks; and(3) prompts learned for a specific source task via PT exhibit positive transfer when paired with in-context examples of a different target task. Our results offer actionable insights on choosing a suitable parameter-efficient adaptation method for a given task.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.11.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Fine-tuning large language models is becoming ever more impractical due to their rapidly-growing scale.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Fine-tuning large language models is becoming ever more impractical due to their rapidly-growing scale.\""
    },
    {
        "title": "Large Language Models for Psycholinguistic Plausibility Pretesting",
        "authors": [
            "Samuel Amouyal",
            "Aya Meltzer-Asscher",
            "Jonathan Berant"
        ],
        "published": "2024",
        "summary": "In psycholinguistics, the creation of controlled materials is crucial to ensure that research outcomes are solely attributed to the intended manipulations and not influenced by extraneous factors. To achieve this, psycholinguists typically pretest linguistic materials, where a common pretest is to solicit plausibility judgments from human evaluators on specific sentences. In this work, we investigate whether Language Models (LMs) can be used to generate these plausibility judgements. We investigate a wide range of LMs across multiple linguistic structures and evaluate whether their plausibility judgements correlate with human judgements. We find that GPT-4 plausibility judgements highly correlate with human judgements across the structures we examine, whereas other LMs correlate well with humans on commonly used syntactic structures. We then test whether this correlation implies that LMs can be used instead of humans for pretesting. We find that when coarse-grained plausibility judgements are needed, this works well, but when fine-grained judgements are necessary, even GPT-4 does not provide satisfactory discriminative power.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.12.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that when coarse-grained plausibility judgements are needed, this works well, but when fine-grained judgements are necessary, even GPT-4 does not provide satisfactory discriminative power.\"\n\nThis paper discusses the limitations of LLMs in the context of generating plausibility judgments, specifically highlighting the inability of even a strong model like GPT-4 to",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We find that when coarse-grained plausibility judgements are needed, this works well, but when fine-grained judgements are necessary, even GPT-4 does not provide satisfactory discriminative power.\"\n\nThis paper discusses the limitations of LLMs in the context of generating plausibility judgments, specifically highlighting the inability of even a strong model like GPT-4 to"
    },
    {
        "title": "Inconsistent dialogue responses and how to recover from them",
        "authors": [
            "Mian Zhang",
            "Lifeng Jin",
            "Linfeng Song",
            "Haitao Mi",
            "Dong Yu"
        ],
        "published": "2024",
        "summary": "One critical issue for chat systems is to stay consistent about preferences, opinions, beliefs and facts of itself, which has been shown a difficult problem. In this work, we study methods to assess and bolster utterance consistency of chat systems. A dataset is first developed for studying the inconsistencies, where inconsistent dialogue responses, explanations of the inconsistencies, and recovery utterances are authored by annotators. This covers the life span of inconsistencies, namely introduction, understanding, and resolution. Building on this, we introduce a set of tasks centered on dialogue consistency, specifically focused on its detection and resolution. Our experimental findings indicate that our dataset significantly helps the progress in identifying and resolving conversational inconsistencies, and current popular large language models like ChatGPT which are good at resolving inconsistencies however still struggle with detection.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.16.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"current popular large language models like ChatGPT which are good at resolving inconsistencies however still struggle with detection.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"current popular large language models like ChatGPT which are good at resolving inconsistencies however still struggle with detection.\""
    },
    {
        "title": "MUG: Interactive Multimodal Grounding on User Interfaces",
        "authors": [
            "Tao Li",
            "Gang Li",
            "Jingjie Zheng",
            "Purple Wang",
            "Yang Li"
        ],
        "published": "2024",
        "summary": "We present MUG, a novel interactive task for multimodal grounding where a user and an agent work collaboratively on an interface screen. Prior works modeled multimodal UI grounding in one round: the user gives a command and the agent responds to the command. Yet, in a realistic scenario, a user command can be ambiguous when the target action is inherently difficult to articulate in natural language. MUG allows multiple rounds of interactions such that upon seeing the agent responses, the user can give further commands for the agent to refine or even correct its actions. Such interaction is critical for improving grounding performances in real-world use cases. To investigate the problem, we create a new dataset that consists of 77,820 sequences of human user-agent interaction on mobile interfaces in which 20% involves multiple rounds of interactions. To establish benchmark, we experiment with a range of modeling variants and evaluation strategies, including both offline and online evaluation—the online strategy consists of both human evaluation and automatic with simulators. Our experiments show that iterative interaction significantly improves the absolute task completion by 18% over the entire test set and 31% over the challenging split. Our results lay the foundation for further investigation of the problem.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.17.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "PRILoRA: Pruned and Rank-Increasing Low-Rank Adaptation",
        "authors": [
            "Nadav Benedek",
            "Lior Wolf"
        ],
        "published": "2024",
        "summary": "With the proliferation of large pre-trained language models (PLMs), fine-tuning all model parameters becomes increasingly inefficient, particularly when dealing with numerous downstream tasks that entail substantial training and storage costs. Several approaches aimed at achieving parameter-efficient fine-tuning (PEFT) have been proposed. Among them, Low-Rank Adaptation (LoRA) stands out as an archetypal method, incorporating trainable rank decomposition matrices into each target module. Nevertheless, LoRA does not consider the varying importance of each layer. To address these challenges, we introduce PRILoRA, which linearly allocates a different rank for each layer, in an increasing manner, and performs pruning throughout the training process, considering both the temporary magnitude of weights and the accumulated statistics of the input to any given layer. We validate the effectiveness of PRILoRA through extensive experiments on eight GLUE benchmarks, setting a new state of the art.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.18.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"With the proliferation of large pre-trained language models (PLMs), fine-tuning all model parameters becomes increasingly inefficient, particularly when dealing with numerous downstream tasks that entail substantial training and storage costs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"With the proliferation of large pre-trained language models (PLMs), fine-tuning all model parameters becomes increasingly inefficient, particularly when dealing with numerous downstream tasks that entail substantial training and storage costs.\""
    },
    {
        "title": "mPLM-Sim: Better Cross-Lingual Similarity and Transfer in Multilingual Pretrained Language Models",
        "authors": [
            "Peiqin Lin",
            "Chengzhi Hu",
            "Zheyu Zhang",
            "Andre Martins",
            "Hinrich Schuetze"
        ],
        "published": "2024",
        "summary": "Recent multilingual pretrained language models (mPLMs) have been shown to encode strong language-specific signals, which are not explicitly provided during pretraining. It remains an open question whether it is feasible to employ mPLMs to measure language similarity, and subsequently use the similarity results to select source languages for boosting cross-lingual transfer. To investigate this, we propose mPLM-Sim, a language similarity measure that induces the similarities across languages from mPLMs using multi-parallel corpora. Our study shows that mPLM-Sim exhibits moderately high correlations with linguistic similarity measures, such as lexicostatistics, genealogical language family, and geographical sprachbund. We also conduct a case study on languages with low correlation and observe that mPLM-Sim yields more accurate similarity results. Additionally, we find that similarity results vary across different mPLMs and different layers within an mPLM. We further investigate whether mPLM-Sim is effective for zero-shot cross-lingual transfer by conducting experiments on both low-level syntactic tasks and high-level semantic tasks. The experimental results demonstrate that mPLM-Sim is capable of selecting better source languages than linguistic measures, resulting in a 1%-2% improvement in zero-shot cross-lingual transfer performance.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.20.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We also find that similarity results vary across different mPLMs and different layers within an mPLM.\"\n\nThis rating is based on the fact that the paper mentions a limitation of mPLMs (variance in similarity results across different models and layers) in passing, but does not elaborate on it and focuses on the proposed solution and its effectiveness.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We also find that similarity results vary across different mPLMs and different layers within an mPLM.\"\n\nThis rating is based on the fact that the paper mentions a limitation of mPLMs (variance in similarity results across different models and layers) in passing, but does not elaborate on it and focuses on the proposed solution and its effectiveness."
    },
    {
        "title": "A Comprehensive Evaluation of Inductive Reasoning Capabilities and Problem Solving in Large Language Models",
        "authors": [
            "Chen Bowen",
            "Rune Sætre",
            "Yusuke Miyao"
        ],
        "published": "2024",
        "summary": "Inductive reasoning is fundamental to both human and artificial intelligence. The inductive reasoning abilities of current Large Language Models (LLMs) are evaluated in this research.We argue that only considering induction of rules is too narrow and unrealistic, since inductive reasoning is usually mixed with other abilities, like rules application, results/rules validation, and updated information integration.We probed the LLMs with a set of designed symbolic tasks and found that even state-of-the-art (SotA) LLMs fail significantly, showing the inability of LLMs to perform these intuitively simple tasks.Furthermore, we found that perfect accuracy in a small-size problem does not guarantee the same accuracy in a larger-size version of the same problem, provoking the question of how we can assess the LLMs’ actual problem-solving capabilities.We also argue that Chain-of-Thought prompts help the LLMs by decomposing the problem-solving process, but the LLMs still learn limitedly.Furthermore, we reveal that few-shot examples assist LLM generalization in out-of-domain (OOD) cases, albeit limited. The LLM starts to fail when the problem deviates from the provided few-shot examples.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.22.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "5",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"even state-of-the-art (SotA) LLMs fail significantly, showing the inability of LLMs to perform these intuitively simple tasks\"; \"perfect accuracy in a small-size problem does not guarantee the same accuracy in a larger-size version of the same problem, provoking the question of how we can assess the LLMs’ actual problem-solving capabilities\"; \"we",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 5\nEvidence: \"even state-of-the-art (SotA) LLMs fail significantly, showing the inability of LLMs to perform these intuitively simple tasks\"; \"perfect accuracy in a small-size problem does not guarantee the same accuracy in a larger-size version of the same problem, provoking the question of how we can assess the LLMs’ actual problem-solving capabilities\"; \"we"
    },
    {
        "title": "Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach",
        "authors": [
            "Zhuang Li",
            "Levon Haroutunian",
            "Raj Tumuluri",
            "Philip Cohen",
            "Reza Haf"
        ],
        "published": "2024",
        "summary": "Post-editing has proven effective in improving the quality of text generated by large language models (LLMs) such as GPT-3.5 or GPT-4, particularly when direct updating of their parameters to enhance text quality is infeasible or expensive. However, relying solely on smaller language models for post-editing can limit the LLMs’ ability to generalize across domains. Moreover, the editing strategies in these methods are not optimally designed for text generation tasks. To address these limitations, we propose a neural programmer-interpreter approach that preserves the domain generalization ability of LLMs while editing their output. The editing actions in this framework are specifically devised for text generation. Extensive experiments demonstrate that the programmer-interpreter significantly enhances GPT-3.5’s performance in logical form-to-text conversion and low-resource machine translation, surpassing other state-of-the-art (SOTA) LLM post-editing methods in cross-domain settings.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.24.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, relying solely on smaller language models for post-editing can limit the LLMs’ ability to generalize across domains. Moreover, the editing strategies in these methods are not optimally designed for text generation tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, relying solely on smaller language models for post-editing can limit the LLMs’ ability to generalize across domains. Moreover, the editing strategies in these methods are not optimally designed for text generation tasks.\""
    },
    {
        "title": "Large Language Models for Scientific Information Extraction: An Empirical Study for Virology",
        "authors": [
            "Mahsa Shamsabadi",
            "Jennifer D’Souza",
            "Sören Auer"
        ],
        "published": "2024",
        "summary": "In this paper, we champion the use of structured and semantic content representation of discourse-based scholarly communication, inspired by tools like Wikipedia infoboxes or structured Amazon product descriptions. These representations provide users with a concise overview, aiding scientists in navigating the dense academic landscape. Our novel automated approach leverages the robust text generation capabilities of LLMs to produce structured scholarly contribution summaries, offering both a practical solution and insights into LLMs’ emergent abilities.For LLMs, the prime focus is on improving their general intelligence as conversational agents. We argue that these models can also be applied effectively in information extraction (IE), specifically in complex IE tasks within terse domains like Science. This paradigm shift replaces the traditional modular, pipelined machine learning approach with a simpler objective expressed through instructions. Our results show that finetuned FLAN-T5 with 1000x fewer parameters than the state-of-the-art GPT-davinci is competitive for the task.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.26.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the abstract does not mention any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the abstract does not mention any limitations of LLMs."
    },
    {
        "title": "(Chat)GPT v BERT Dawn of Justice for Semantic Change Detection",
        "authors": [
            "Francesco Periti",
            "Haim Dubossarsky",
            "Nina Tahmasebi"
        ],
        "published": "2024",
        "summary": "In the universe of Natural Language Processing, Transformer-based language models like BERT and (Chat)GPT have emerged as lexical superheroes with great power to solve open research problems. In this paper, we specifically focus on the temporal problem of semantic change, and evaluate their ability to solve two diachronic extensions of the Word-in-Context (WiC) task: TempoWiC and HistoWiC. In particular, we investigate the potential of a novel, off-the-shelf technology like ChatGPT (and GPT) 3.5 compared to BERT, which represents a family of models that currently stand as the state-of-the-art for modeling semantic change. Our experiments represent the first attempt to assess the use of (Chat)GPT for studying semantic change. Our results indicate that ChatGPT performs significantly worse than the foundational GPT version. Furthermore, our results demonstrate that (Chat)GPT achieves slightly lower performance than BERT in detecting long-term changes but performs significantly worse in detecting short-term changes.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.29.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our results indicate that ChatGPT performs significantly worse than the foundational GPT version. Furthermore, our results demonstrate that (Chat)GPT achieves slightly lower performance than BERT in detecting long-term changes but performs significantly worse in detecting short-term changes.\"\n\nThis rating is given because the abstract discusses the limitations of ChatGPT, a type of LLM, in detecting semantic change,",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Our results indicate that ChatGPT performs significantly worse than the foundational GPT version. Furthermore, our results demonstrate that (Chat)GPT achieves slightly lower performance than BERT in detecting long-term changes but performs significantly worse in detecting short-term changes.\"\n\nThis rating is given because the abstract discusses the limitations of ChatGPT, a type of LLM, in detecting semantic change,"
    },
    {
        "title": "On the Relationship between Sentence Analogy Identification and Sentence Structure Encoding in Large Language Models",
        "authors": [
            "Thilini Wijesiriwardene",
            "Ruwan Wickramarachchi",
            "Aishwarya Naresh Reganti",
            "Vinija Jain",
            "Aman Chadha",
            "Amit Sheth",
            "Amitava Das"
        ],
        "published": "2024",
        "summary": "The ability of Large Language Models (LLMs) to encode syntactic and semantic structures of language is well examined in NLP. Additionally, analogy identification, in the form of word analogies are extensively studied in the last decade of language modeling literature. In this work we specifically look at how LLMs’ abilities to capture sentence analogies (sentences that convey analogous meaning to each other) vary with LLMs’ abilities to encode syntactic and semantic structures of sentences. Through our analysis, we find that LLMs’ ability to identify sentence analogies is positively correlated with their ability to encode syntactic and semantic structures of sentences. Specifically, we find that the LLMs which capture syntactic structures better, also have higher abilities in identifying sentence analogies.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.31.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None explicitly mentioned, but the paper implies that LLMs may have varying abilities in capturing sentence analogies and encoding syntactic and semantic structures, suggesting potential limitations in these areas.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: None explicitly mentioned, but the paper implies that LLMs may have varying abilities in capturing sentence analogies and encoding syntactic and semantic structures, suggesting potential limitations in these areas."
    },
    {
        "title": "Contextualization Distillation from Large Language Model for Knowledge Graph Completion",
        "authors": [
            "Dawei Li",
            "Zhen Tan",
            "Tianlong Chen",
            "Huan Liu"
        ],
        "published": "2024",
        "summary": "While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the Contextualization Distillation strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks—reconstruction and contextualization—allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pipelines or architectures. Moreover, our analysis makes our method more explainable and provides insight into how to generate high-quality corpora for KGC, as well as the selection of suitable distillation tasks.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.32.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models.\""
    },
    {
        "title": "Differentially Private Natural Language Models: Recent Advances and Future Directions",
        "authors": [
            "Lijie Hu",
            "Ivan Habernal",
            "Lei Shen",
            "Di Wang"
        ],
        "published": "2024",
        "summary": "Recent developments in deep learning have led to great success in various natural language processing (NLP) tasks. However, these applications may involve data that contain sensitive information. Therefore, how to achieve good performance while also protecting the privacy of sensitive data is a crucial challenge in NLP. To preserve privacy, Differential Privacy (DP), which can prevent reconstruction attacks and protect against potential side knowledge, is becoming a de facto technique for private data analysis. In recent years, NLP in DP models (DP-NLP) has been studied from different perspectives, which deserves a comprehensive review. In this paper, we provide the first systematic review of recent advances in DP deep learning models in NLP. In particular, we first discuss some differences and additional challenges of DP-NLP compared with the standard DP deep learning. Then, we investigate some existing work on DP-NLP andpresent its recent developments from three aspects: gradient perturbation based methods, embedding vector perturbation based methods, and ensemble model based methods. We also discuss some challenges and future directions.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.33.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.\n\nNote that the paper does not mention Large Language Models (LLMs) at all, it talks about Differential Privacy (DP) in NLP models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs.\n\nNote that the paper does not mention Large Language Models (LLMs) at all, it talks about Differential Privacy (DP) in NLP models."
    },
    {
        "title": "Learning to Compare Financial Reports for Financial Forecasting",
        "authors": [
            "Ross Koval",
            "Nicholas Andrews",
            "Xifeng Yan"
        ],
        "published": "2024",
        "summary": "Public companies in the US are required to publish annual reports that detail their recent financial performance, present the current state of ongoing business operations, and discuss future prospects. However, they typically contain over 25,000 words across all sections, large amounts of industry and legal jargon, and a high percentage of boilerplate content that does not change much year-to-year. These unique characteristics present challenges for many generic pretrained language models because it is likely that only a small percentage of the long report that reflects salient information contains meaningful signal about the future prospects of the company. In this work, we curate a large-scale dataset of paired financial reports and introduce two novel, challenging tasks of predicting long-horizon company risk and correlation that evaluate the ability of the model to recognize cross-document relationships with complex, nuanced signals. We explore and present a comprehensive set of methods and experiments, and establish strong baselines designed to learn to identify subtle similarities and differences between long documents. Furthermore, we demonstrate that it is possible to predict company risk and correlation solely from the text of their financial reports and further that modeling the cross-document interactions at a fine-grained level provides significant benefit. Finally, we probe the best performing model through quantitative and qualitative interpretability methods to reveal some insight into the underlying task signal.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.34.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"These unique characteristics present challenges for many generic pretrained language models because it is likely that only a small percentage of the long report that reflects salient information contains meaningful signal about the future prospects of the company.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"These unique characteristics present challenges for many generic pretrained language models because it is likely that only a small percentage of the long report that reflects salient information contains meaningful signal about the future prospects of the company.\""
    },
    {
        "title": "Knowledge Generation for Zero-shot Knowledge-based VQA",
        "authors": [
            "Rui Cao",
            "Jing Jiang"
        ],
        "published": "2024",
        "summary": "Previous solutions to knowledge-based visual question answering (K-VQA) retrieve knowledge from external knowledge bases and use supervised learning to train the K-VQA model.Recently pre-trained LLMs have been used as both a knowledge source and a zero-shot QA model for K-VQA and demonstrated promising results.However, these recent methods do not explicitly show the knowledge needed to answer the questions and thus lack interpretability.Inspired by recent work on knowledge generation from LLMs for text-based QA, in this work we propose and test a similar knowledge-generation-based K-VQA method, which first generates knowledge from an LLM and then incorporates the generated knowledge for K-VQA in a zero-shot manner. We evaluate our method on two K-VQA benchmarks and found that our method performs better than previous zero-shot K-VQA methods and our generated knowledge is generally relevant and helpful.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.36.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, these recent methods do not explicitly show the knowledge needed to answer the questions and thus lack interpretability.\"\n\nThis evidence indicates a limitation of LLMs, but it is mentioned briefly and is not the primary focus of the paper.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, these recent methods do not explicitly show the knowledge needed to answer the questions and thus lack interpretability.\"\n\nThis evidence indicates a limitation of LLMs, but it is mentioned briefly and is not the primary focus of the paper."
    },
    {
        "title": "Bootstrap Your Own PLM: Boosting Semantic Features of PLMs for Unsuperivsed Contrastive Learning",
        "authors": [
            "Yoo Hyun Jeong",
            "Myeong Soo Han",
            "Dong-Kyu Chae"
        ],
        "published": "2024",
        "summary": "This paper aims to investigate the possibility of exploiting original semantic features of PLMs (pre-trained language models) during contrastive learning in the context of SRL (sentence representation learning). In the context of feature modification, we identified a method called IFM (implicit feature modification), which reduces the tendency of contrastive models for VRL (visual representation learning) to rely on feature-suppressing shortcut solutions. We observed that IFM did not work well for SRL, which may be due to differences between the nature of VRL and SRL. We propose BYOP, which boosts well-represented features, taking the opposite idea of IFM, under the assumption that SimCSE’s dropout-noise-based augmentation may be too simple to modify high-level semantic features, and that the features learned by PLMs are semantically meaningful and should be boosted, rather than removed. Extensive experiments lend credence to the logic of BYOP, which considers the nature of SRL.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.38.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We observed that IFM did not work well for SRL, which may be due to differences between the nature of VRL and SRL.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of a method (IFM) when applied to SRL, which is related to the semantic features of PLMs. However, the limitation is not the primary focus of",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We observed that IFM did not work well for SRL, which may be due to differences between the nature of VRL and SRL.\"\n\nThis rating is based on the fact that the abstract mentions a limitation of a method (IFM) when applied to SRL, which is related to the semantic features of PLMs. However, the limitation is not the primary focus of"
    },
    {
        "title": "Personalized Abstractive Summarization by Tri-agent Generation Pipeline",
        "authors": [
            "Wen Xiao",
            "Yujia Xie",
            "Giuseppe Carenini",
            "Pengcheng He"
        ],
        "published": "2024",
        "summary": "Tailoring outputs from large language models, like ChatGPT, to implicit user preferences remains a challenge despite their impressive generative capabilities. In this paper, we propose a tri-agent generation pipeline comprising a generator, an instructor, and an editor to enhance output personalization. The generator produces an initial output, the instructor automatically generates editing instructions based on user preferences, and the editor refines the output to align with those preferences. The inference-only large language model (ChatGPT) serves as both the generator and editor, with a smaller model acting as the instructor to guide output generation. We train the instructor using editor-steered reinforcement learning, leveraging feedback from a large-scale editor model to optimize instruction generation. Experimental results on two abstractive summarization datasets demonstrate the effectiveness of our approach in generating outputs that better meet user expectations.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.39.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Tailoring outputs from large language models, like ChatGPT, to implicit user preferences remains a challenge despite their impressive generative capabilities.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Tailoring outputs from large language models, like ChatGPT, to implicit user preferences remains a challenge despite their impressive generative capabilities.\""
    },
    {
        "title": "Reward Engineering for Generating Semi-structured Explanation",
        "authors": [
            "Jiuzhou Han",
            "Wray Buntine",
            "Ehsan Shareghi"
        ],
        "published": "2024",
        "summary": "Semi-structured explanation depicts the implicit process of a reasoner with an explicit representation. This explanation highlights how available information in a specific query is utilised and supplemented with information a reasoner produces from its internal weights towards generating an answer. Despite the recent improvements in generative capabilities of language models, producing structured explanations to verify a model’s true reasoning capabilities remains a challenge. This issue is particularly pronounced for not-so-large LMs (e.g., FLAN-T5-XXL). In this work, we first underscore the limitations of supervised fine-tuning (SFT) in tackling this challenge, and then introduce a carefully crafted reward engineering method in reinforcement learning (RL) to better address this problem. We investigate multiple reward aggregation methods and provide a detailed discussion which sheds light on the promising potential of RL for future research. Our proposed method on two semi-structured explanation generation benchmarks (ExplaGraph and COPA-SSE) achieves new state-of-the-art results.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.41.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite the recent improvements in generative capabilities of language models, producing structured explanations to verify a model’s true reasoning capabilities remains a challenge. This issue is particularly pronounced for not-so-large LMs (e.g., FLAN-T5-XXL)\"; \"we first underscore the limitations of supervised fine-tuning (SFT) in tackling this challenge\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Despite the recent improvements in generative capabilities of language models, producing structured explanations to verify a model’s true reasoning capabilities remains a challenge. This issue is particularly pronounced for not-so-large LMs (e.g., FLAN-T5-XXL)\"; \"we first underscore the limitations of supervised fine-tuning (SFT) in tackling this challenge\""
    },
    {
        "title": "Capturing the Relationship Between Sentence Triplets for LLM and Human-Generated Texts to Enhance Sentence Embeddings",
        "authors": [
            "Na Min An",
            "Sania Waheed",
            "James Thorne"
        ],
        "published": "2024",
        "summary": "Deriving meaningful sentence embeddings is crucial in capturing the semantic relationship between texts. Recent advances in building sentence embedding models have centered on replacing traditional human-generated text datasets with those generated by LLMs. However, the properties of these widely used LLM-generated texts remain largely unexplored. Here, we evaluate the quality of the LLM-generated texts from four perspectives (Positive Text Repetition, Length Difference Penalty, Positive Score Compactness, and Negative Text Implausibility) and find that there exists an inherent difference between human and LLM-generated datasets. To further enhance sentence embeddings using both human and LLM-generated datasets, we propose a novel loss function that incorporates Positive-Negative sample Augmentation (PNA) within the contrastive learning objective. Our results demonstrate that PNA effectively mitigates the sentence anisotropy problem in Wikipedia corpus (-7% compared to CLHAIF) and simultaneously improves the Spearman’s correlation in standard Semantic Textual Similarity (STS) tasks (+1.47% compared to CLHAIF).",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.43.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the properties of these widely used LLM-generated texts remain largely unexplored... we find that there exists an inherent difference between human and LLM-generated datasets.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, the properties of these widely used LLM-generated texts remain largely unexplored... we find that there exists an inherent difference between human and LLM-generated datasets.\""
    },
    {
        "title": "Morality is Non-Binary: Building a Pluralist Moral Sentence Embedding Space using Contrastive Learning",
        "authors": [
            "Jeongwoo Park",
            "Enrico Liscio",
            "Pradeep K. Murukannaiah"
        ],
        "published": "2024",
        "summary": "Recent advances in NLP show that language models retain a discernible level of knowledge in deontological ethics and moral norms. However, existing works often treat morality as binary, ranging from right to wrong. This simplistic view does not capture the nuances of moral judgment. Pluralist moral philosophers argue that human morality can be deconstructed into a finite number of elements, respecting individual differences in moral judgment. In line with this view, we build a pluralist moral sentence embedding space via a state-of-the-art contrastive learning approach. We systematically investigate the embedding space by studying the emergence of relationships among moral elements, both quantitatively and qualitatively. Our results show that a pluralist approach to morality can be captured in an embedding space. However, moral pluralism is challenging to deduce via self-supervision alone and requires a supervised approach with human labels.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.45.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, moral pluralism is challenging to deduce via self-supervision alone and requires a supervised approach with human labels.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, moral pluralism is challenging to deduce via self-supervision alone and requires a supervised approach with human labels.\""
    },
    {
        "title": "Exploring the Potential of ChatGPT on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations",
        "authors": [
            "Chunkit Chan",
            "Cheng Jiayang",
            "Weiqi Wang",
            "Yuxin Jiang",
            "Tianqing Fang",
            "Xin Liu",
            "Yangqiu Song"
        ],
        "published": "2024",
        "summary": "This paper aims to quantitatively evaluate the performance of ChatGPT, an interactive large language model, on inter-sentential relations such as temporal relations, causal relations, and discourse relations. Given ChatGPT’s promising performance across various tasks, we proceed to carry out thorough evaluations on the whole test sets of 11 datasets, including temporal and causal relations, PDTB2.0-based, and dialogue-based discourse relations. To ensure the reliability of our findings, we employ three tailored prompt templates for each task, including the zero-shot prompt template, zero-shot prompt engineering (PE) template, and in-context learning (ICL) prompt template, to establish the initial baseline scores for all popular sentence-pair relation classification tasks for the first time. Through our study, we discover that ChatGPT exhibits exceptional proficiency in detecting and reasoning about causal relations, albeit it may not possess the same level of expertise in identifying the temporal order between two events. While it is capable of identifying the majority of discourse relations with existing explicit discourse connectives, the implicit discourse relation remains a formidable challenge. Concurrently, ChatGPT demonstrates subpar performance in the dialogue discourse parsing task that requires structural understanding in a dialogue before being aware of the discourse relation.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.47.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While it is capable of identifying the majority of discourse relations with existing explicit discourse connectives, the implicit discourse relation remains a formidable challenge. Concurrently, ChatGPT demonstrates subpar performance in the dialogue discourse parsing task that requires structural understanding in a dialogue before being aware of the discourse relation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"While it is capable of identifying the majority of discourse relations with existing explicit discourse connectives, the implicit discourse relation remains a formidable challenge. Concurrently, ChatGPT demonstrates subpar performance in the dialogue discourse parsing task that requires structural understanding in a dialogue before being aware of the discourse relation.\""
    },
    {
        "title": "Investigating grammatical abstraction in language models using few-shot learning of novel noun gender",
        "authors": [
            "Priyanka Sukumaran",
            "Conor Houghton",
            "Nina Kazanina"
        ],
        "published": "2024",
        "summary": "Humans can learn a new word and infer its grammatical properties from very few examples. They have an abstract notion of linguistic properties like grammatical gender and agreement rules that can be applied to novel syntactic contexts and words. Drawing inspiration from psycholinguistics, we conduct a noun learning experiment to assess whether an LSTM and a decoder-only transformer can achieve human-like abstraction of grammatical gender in French. Language models were tasked with learning the gender of a novel noun embedding from a few examples in one grammatical agreement context and predicting agreement in another, unseen context. We find that both language models effectively generalise novel noun gender from one to two learning examples and apply the learnt gender across agreement contexts, albeit with a bias for the masculine gender category. Importantly, the few-shot updates were only applied to the embedding layers, demonstrating that models encode sufficient gender information within the word-embedding space. While the generalisation behaviour of models suggests that they represent grammatical gender as an abstract category, like humans, further work is needed to explore the details of how exactly this is implemented. For a comparative perspective with human behaviour, we conducted an analogous one-shot novel noun gender learning experiment, which revealed that native French speakers, like language models, also exhibited a masculine gender bias and are not excellent one-shot learners either.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.50.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While the generalisation behaviour of models suggests that they represent grammatical gender as an abstract category, like humans, further work is needed to explore the details of how exactly this is implemented.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While the generalisation behaviour of models suggests that they represent grammatical gender as an abstract category, like humans, further work is needed to explore the details of how exactly this is implemented.\""
    },
    {
        "title": "Style Vectors for Steering Generative Large Language Models",
        "authors": [
            "Kai Konen",
            "Sophie Jentzsch",
            "Diaoulé Diallo",
            "Peer Schütt",
            "Oliver Bensch",
            "Roxanne El Baff",
            "Dominik Opitz",
            "Tobias Hecking"
        ],
        "published": "2024",
        "summary": "This research explores strategies for steering the output of large language models (LLMs) towards specific styles, such as sentiment, emotion, or writing style, by adding style vectors to the activations of hidden layers during text generation. We show that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches. Through a series of experiments, we demonstrate the effectiveness of activation engineering using such style vectors to influence the style of generated text in a nuanced and parameterisable way, distinguishing it from prompt engineering. The presented research constitutes a significant step towards developing more adaptive and effective AI-empowered interactive systems.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.52.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs, but mentions \"steering the output of large language models (LLMs) towards specific styles\" which implies that LLMs may not be able to achieve specific styles on their own.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No evidence of discussion of limitations of LLMs, but mentions \"steering the output of large language models (LLMs) towards specific styles\" which implies that LLMs may not be able to achieve specific styles on their own."
    },
    {
        "title": "Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage",
        "authors": [
            "Hanyin Shao",
            "Jie Huang",
            "Shen Zheng",
            "Kevin Chang"
        ],
        "published": "2024",
        "summary": "The advancement of large language models (LLMs) brings notable improvements across various applications, while simultaneously raising concerns about potential private data exposure. One notable capability of LLMs is their ability to form associations between different pieces of information, but this raises concerns when it comes to personally identifiable information (PII). This paper delves into the association capabilities of language models, aiming to uncover the factors that influence their proficiency in associating information. Our study reveals that as models scale up, their capacity to associate entities/information intensifies, particularly when target pairs demonstrate shorter co-occurrence distances or higher co-occurrence frequencies. However, there is a distinct performance gap when associating commonsense knowledge versus PII, with the latter showing lower accuracy. Despite the proportion of accurately predicted PII being relatively small, LLMs still demonstrate the capability to predict specific instances of email addresses and phone numbers when provided with appropriate prompts. These findings underscore the potential risk to PII confidentiality posed by the evolving capabilities of LLMs, especially as they continue to expand in scale and power.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.54.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, there is a distinct performance gap when associating commonsense knowledge versus PII, with the latter showing lower accuracy.\"; \"These findings underscore the potential risk to PII confidentiality posed by the evolving capabilities of LLMs, especially as they continue to expand in scale and power.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, there is a distinct performance gap when associating commonsense knowledge versus PII, with the latter showing lower accuracy.\"; \"These findings underscore the potential risk to PII confidentiality posed by the evolving capabilities of LLMs, especially as they continue to expand in scale and power.\""
    },
    {
        "title": "Probing Critical Learning Dynamics of PLMs for Hate Speech Detection",
        "authors": [
            "Sarah Masud",
            "Mohammad Aflah Khan",
            "Vikram Goyal",
            "Md Shad Akhtar",
            "Tanmoy Chakraborty"
        ],
        "published": "2024",
        "summary": "Despite the widespread adoption, there is a lack of research into how various critical aspects of pretrained language models (PLMs) affect their performance in hate speech detection. Through five research questions, our findings and recommendations lay the groundwork for empirically investigating different aspects of PLMs’ use in hate speech detection. We deep dive into comparing different pretrained models, evaluating their seed robustness, finetuning settings, and the impact of pretraining data collection time. Our analysis reveals early peaks for downstream tasks during pretraining, the limited benefit of employing a more recent pretraining corpus, and the significance of specific layers during finetuning. We further call into question the use of domain-specific models and highlight the need for dynamic datasets for benchmarking hate speech detection.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.55.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We further call into question the use of domain-specific models and highlight the need for dynamic datasets for benchmarking hate speech detection.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We further call into question the use of domain-specific models and highlight the need for dynamic datasets for benchmarking hate speech detection.\""
    },
    {
        "title": "PromptExplainer: Explaining Language Models through Prompt-based Learning",
        "authors": [
            "Zijian Feng",
            "Hanzhang Zhou",
            "Zixiao Zhu",
            "Kezhi Mao"
        ],
        "published": "2024",
        "summary": "Pretrained language models have become workhorses for various natural language processing (NLP) tasks, sparking a growing demand for enhanced interpretability and transparency. However, prevailing explanation methods, such as attention-based and gradient-based strategies, largely rely on linear approximations, potentially causing inaccuracies such as accentuating irrelevant input tokens. To mitigate the issue, we develop PromptExplainer, a novel method for explaining language models through prompt-based learning. PromptExplainer aligns the explanation process with the masked language modeling (MLM) task of pretrained language models and leverages the prompt-based learning framework for explanation generation. It disentangles token representations into the explainable embedding space using the MLM head and extracts discriminative features with a verbalizer to generate class-dependent explanations. Extensive experiments demonstrate that PromptExplainer significantly outperforms state-of-the-art explanation methods.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.60.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, prevailing explanation methods, such as attention-based and gradient-based strategies, largely rely on linear approximations, potentially causing inaccuracies such as accentuating irrelevant input tokens.\"\n\nThis paper discusses LLMs but does not explicitly mention any limitations of the models themselves. The limitation mentioned is related to prevailing explanation methods, not the LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, prevailing explanation methods, such as attention-based and gradient-based strategies, largely rely on linear approximations, potentially causing inaccuracies such as accentuating irrelevant input tokens.\"\n\nThis paper discusses LLMs but does not explicitly mention any limitations of the models themselves. The limitation mentioned is related to prevailing explanation methods, not the LLMs."
    },
    {
        "title": "Do-Not-Answer: Evaluating Safeguards in LLMs",
        "authors": [
            "Yuxia Wang",
            "Haonan Li",
            "Xudong Han",
            "Preslav Nakov",
            "Timothy Baldwin"
        ],
        "published": "2024",
        "summary": "With the rapid evolution of large language models (LLMs), new and hard-to-predict harmful capabilities are emerging. This requires developers to identify potential risks through the evaluation of “dangerous capabilities” in order to responsibly deploy LLMs. Here we aim to facilitate this process. In particular, we collect an open-source dataset to evaluate the safeguards in LLMs, to facilitate the deployment of safer open-source LLMs at a low cost. Our dataset is curated and filtered to consist only of instructions that responsible language models should not follow. We assess the responses of six popular LLMs to these instructions, and we find that simple BERT-style classifiers can achieve results that are comparable to GPT-4 on automatic safety evaluation. Our data and code are available at https://github.com/Libr-AI/do-not-answer",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.61.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"With the rapid evolution of large language models (LLMs), new and hard-to-predict harmful capabilities are emerging. This requires developers to identify potential risks through the evaluation of “dangerous capabilities” in order to responsibly deploy LLMs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"With the rapid evolution of large language models (LLMs), new and hard-to-predict harmful capabilities are emerging. This requires developers to identify potential risks through the evaluation of “dangerous capabilities” in order to responsibly deploy LLMs.\""
    },
    {
        "title": "Do Language Models Know When They’re Hallucinating References?",
        "authors": [
            "Ayush Agrawal",
            "Mirac Suzgun",
            "Lester Mackey",
            "Adam Kalai"
        ],
        "published": "2024",
        "summary": "State-of-the-art language models (LMs) are notoriously susceptible to generating hallucinated information. Such inaccurate outputs not only undermine the reliability of these models but also limit their use and raise serious concerns about misinformation and propaganda. In this work, we focus on hallucinated book and article references and present them as the “model organism” of language model hallucination research, due to their frequent and easy-to-discern nature. We posit that if a language model cites a particular reference in its output, then it should ideally possess sufficient information about its authors and content, among other relevant details. Using this basic insight, we illustrate that one can identify hallucinated references without ever consulting any external resources, by asking a set of direct or indirect queries to the language model about the references. These queries can be considered as “consistency checks.” Our findings highlight that while LMs, including GPT-4, often produce inconsistent author lists for hallucinated references, they also often accurately recall the authors of real references. In this sense, the LM can be said to “know” when it is hallucinating references. Furthermore, these findings show how hallucinated references can be dissected to shed light on their nature.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.62.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"State-of-the-art language models (LMs) are notoriously susceptible to generating hallucinated information. Such inaccurate outputs not only undermine the reliability of these models but also limit their use and raise serious concerns about misinformation and propaganda.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"State-of-the-art language models (LMs) are notoriously susceptible to generating hallucinated information. Such inaccurate outputs not only undermine the reliability of these models but also limit their use and raise serious concerns about misinformation and propaganda.\""
    },
    {
        "title": "Rethinking STS and NLI in Large Language Models",
        "authors": [
            "Yuxia Wang",
            "Minghan Wang",
            "Preslav Nakov"
        ],
        "published": "2024",
        "summary": "Recent years, have seen the rise of large language models (LLMs), where practitioners use task-specific prompts; this was shown to be effective for a variety of tasks. However, when applied to semantic textual similarity (STS) and natural language inference (NLI), the effectiveness of LLMs turns out to be limited by low-resource domain accuracy, model overconfidence, and difficulty to capture the disagreements between human judgements. With this in mind, here we try to rethink STS and NLI in the era of LLMs. We first evaluate the performance of STS and NLI in the clinical/biomedical domain, and then we assess LLMs’ predictive confidence and their capability of capturing collective human opinions. We find that these old problems are still to be properly addressed in the era of LLMs.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.65.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, when applied to semantic textual similarity (STS) and natural language inference (NLI), the effectiveness of LLMs turns out to be limited by low-resource domain accuracy, model overconfidence, and difficulty to capture the disagreements between human judgements.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"However, when applied to semantic textual similarity (STS) and natural language inference (NLI), the effectiveness of LLMs turns out to be limited by low-resource domain accuracy, model overconfidence, and difficulty to capture the disagreements between human judgements.\""
    },
    {
        "title": "Learning High-Quality and General-Purpose Phrase Representations",
        "authors": [
            "Lihu Chen",
            "Gael Varoquaux",
            "Fabian Suchanek"
        ],
        "published": "2024",
        "summary": "Phrase representations play an important role in data science and natural language processing, benefiting various tasks like Entity Alignment, Record Linkage, Fuzzy Joins, and Paraphrase Classification.The current state-of-the-art method involves fine-tuning pre-trained language models for phrasal embeddings using contrastive learning. However, we have identified areas for improvement. First, these pre-trained models tend to be unnecessarily complex and require to be pre-trained on a corpus with context sentences.Second, leveraging the phrase type and morphology gives phrase representations that are both more precise and more flexible.We propose an improved framework to learn phrase representations in a context-free fashion.The framework employs phrase type classification as an auxiliary task and incorporates character-level information more effectively into the phrase representation.Furthermore, we design three granularities of data augmentation to increase the diversity of training samples.Our experiments across a wide range of tasks reveal that our approach generates superior phrase embeddings compared to previous methods while requiring a smaller model size.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.66.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, we have identified areas for improvement. First, these pre-trained models tend to be unnecessarily complex and require to be pre-trained on a corpus with context sentences.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, we have identified areas for improvement. First, these pre-trained models tend to be unnecessarily complex and require to be pre-trained on a corpus with context sentences.\""
    },
    {
        "title": "Explaining Language Model Predictions with High-Impact Concepts",
        "authors": [
            "Ruochen Zhao",
            "Tan Wang",
            "Yongjie Wang",
            "Shafiq Joty"
        ],
        "published": "2024",
        "summary": "To encourage fairness and transparency, there exists an urgent demand for deriving reliable explanations for large language models (LLMs). One promising solution is concept-based explanations, i.e., human-understandable concepts from internal representations. However, due to the compositional nature of languages, current methods mostly discover correlational explanations instead of causal features. Therefore, we propose a novel framework to provide impact-aware explanations for users to understand the LLM’s behavior, which are robust to feature changes and influential to the model’s predictions. Specifically, we extract predictive high-level features (concepts) from the model’s hidden layer activations. Then, we innovatively optimize for features whose existence causes the output predictions to change substantially. Extensive experiments on real and synthetic tasks demonstrate that our method achieves superior results on predictive impact, explainability, and faithfulness compared to the baselines, especially for LLMs.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.67.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, due to the compositional nature of languages, current methods mostly discover correlational explanations instead of causal features.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, due to the compositional nature of languages, current methods mostly discover correlational explanations instead of causal features.\""
    },
    {
        "title": "IndiVec: An Exploration of Leveraging Large Language Models for Media Bias Detection with Fine-Grained Bias Indicators",
        "authors": [
            "Luyang Lin",
            "Lingzhi Wang",
            "Xiaoyan Zhao",
            "Jing Li",
            "Kam-Fai Wong"
        ],
        "published": "2024",
        "summary": "This study focuses on media bias detection, crucial in today’s era of influential social media platforms shaping individual attitudes and opinions. In contrast to prior work that primarily relies on training specific models tailored to particular datasets, resulting in limited adaptability and subpar performance on out-of-domain data, we introduce a general bias detection framework, IndiVec, built upon large language models. IndiVec begins by constructing a fine-grained media bias database, leveraging the robust instruction-following capabilities of large language models and vector database techniques. When confronted with new input for bias detection, our framework automatically selects the most relevant indicator from the vector database and employs majority voting to determine the input’s bias label. IndiVec excels compared to previous methods due to its adaptability (demonstrating consistent performance across diverse datasets from various sources) and explainability (providing explicit top-k indicators to interpret bias predictions). Experimental results on four political bias datasets highlight IndiVec’s significant superiority over baselines. Furthermore, additional experiments and analysis provide profound insights into the framework’s effectiveness.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.70.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"In contrast to prior work that primarily relies on training specific models tailored to particular datasets, resulting in limited adaptability and subpar performance on out-of-domain data\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"In contrast to prior work that primarily relies on training specific models tailored to particular datasets, resulting in limited adaptability and subpar performance on out-of-domain data\""
    },
    {
        "title": "Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?",
        "authors": [
            "Rishav Hada",
            "Varun Gumma",
            "Adrian Wynter",
            "Harshita Diddee",
            "Mohamed Ahmed",
            "Monojit Choudhury",
            "Kalika Bali",
            "Sunayana Sitaram"
        ],
        "published": "2024",
        "summary": "Large Language Models (LLMs) excel in various Natural Language Processing (NLP) tasks, yet their evaluation, particularly in languages beyond the top 20, remains inadequate due to existing benchmarks and metrics limitations. Employing LLMs as evaluators to rank or score other models’ outputs emerges as a viable solution, addressing the constraints tied to human annotators and established benchmarks. In this study, we explore the potential of LLM-based evaluators in enhancing multilingual evaluation by calibrating them against 20K human judgments across three text-generation tasks, five metrics, and eight languages. Our analysis reveals a bias in LLM-based evaluators towards higher scores, underscoring the necessity of calibration with native speaker judgments, especially in low-resource and non-Latin script languages, to ensure accurate evaluation of LLM performance across diverse languages.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.71.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our analysis reveals a bias in LLM-based evaluators towards higher scores, underscoring the necessity of calibration with native speaker judgments, especially in low-resource and non-Latin script languages, to ensure accurate evaluation of LLM performance across diverse languages.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Our analysis reveals a bias in LLM-based evaluators towards higher scores, underscoring the necessity of calibration with native speaker judgments, especially in low-resource and non-Latin script languages, to ensure accurate evaluation of LLM performance across diverse languages.\""
    },
    {
        "title": "Why Generate When You Can Discriminate? A Novel Technique for Text Classification using Language Models",
        "authors": [
            "Sachin Pawar",
            "Nitin Ramrakhiyani",
            "Anubhav Sinha",
            "Manoj Apte",
            "Girish Palshikar"
        ],
        "published": "2024",
        "summary": "In this paper, we propose a novel two-step technique for text classification using autoregressive Language Models (LM). In the first step, a set of perplexity and log-likelihood based numeric features are elicited from an LM for a text instance to be classified. Then, in the second step, a classifier based on these features is trained to predict the final label. The classifier used is usually a simple machine learning classifier like Support Vector Machine (SVM) or Logistic Regression (LR) and it is trained using a small set of training examples. We believe, our technique presents a whole new way of exploiting the available training instances, in addition to the existing ways like fine-tuning LMs or in-context learning. Our approach stands out by eliminating the need for parameter updates in LMs, as required in fine-tuning, and does not impose limitations on the number of training examples faced while building prompts for in-context learning. We evaluate our technique across 5 different datasets and compare with multiple competent baselines.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.74.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"does not impose limitations on the number of training examples faced while building prompts for in-context learning.\"\n\nThis evidence is brief and mentions a limitation of in-context learning with LMs, but the primary focus of the paper is on the proposed technique rather than the limitation.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"does not impose limitations on the number of training examples faced while building prompts for in-context learning.\"\n\nThis evidence is brief and mentions a limitation of in-context learning with LMs, but the primary focus of the paper is on the proposed technique rather than the limitation."
    },
    {
        "title": "IndiFoodVQA: Advancing Visual Question Answering and Reasoning with a Knowledge-Infused Synthetic Data Generation Pipeline",
        "authors": [
            "Pulkit Agarwal",
            "Settaluri Sravanthi",
            "Pushpak Bhattacharyya"
        ],
        "published": "2024",
        "summary": "Large Vision Language Models (VLMs) like GPT-4, LLaVA, and InstructBLIP exhibit extraordinary capabilities for both knowledge understanding and reasoning. However, the reasoning capabilities of such models on sophisticated problems that require external knowledge of a specific domain have not been assessed well, due to the unavailability of necessary datasets. In this work, we release a first-of-its-kind dataset called IndiFoodVQA with around 16.7k data samples, consisting of explicit knowledge-infused questions, answers, and reasons. We also release IndiFoodKG, a related Knowledge Graph (KG) with 79k triples. The data has been created with minimal human intervention via an automated pipeline based on InstructBlip and GPT-3.5. We also present a methodology to extract knowledge from the KG and use it to both answer and reason upon the questions. We employ different models to report baseline zero-shot and fine-tuned results. Fine-tuned VLMs on our data showed an improvement of ~25% over the corresponding base model, highlighting the fact that current VLMs need domain-specific fine-tuning to excel in specialized settings. Our findings reveal that (1) explicit knowledge infusion during question generation helps in making questions that have more grounded knowledge, and (2) proper knowledge retrieval can often lead to better-answering potential in such cases. The data and code is available at https://github.com/SLSravanthi/IndifoodVQA.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.78.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, the reasoning capabilities of such models on sophisticated problems that require external knowledge of a specific domain have not been assessed well, due to the unavailability of necessary datasets.\"; \"Fine-tuned VLMs on our data showed an improvement of ~25% over the corresponding base model, highlighting the fact that current VLMs need domain-specific fine-tuning to excel in specialized settings",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, the reasoning capabilities of such models on sophisticated problems that require external knowledge of a specific domain have not been assessed well, due to the unavailability of necessary datasets.\"; \"Fine-tuned VLMs on our data showed an improvement of ~25% over the corresponding base model, highlighting the fact that current VLMs need domain-specific fine-tuning to excel in specialized settings"
    },
    {
        "title": "Exploring efficient zero-shot synthetic dataset generation for Information Retrieval",
        "authors": [
            "Tiago Almeida",
            "Sérgio Matos"
        ],
        "published": "2024",
        "summary": "The broad integration of neural retrieval models into Information Retrieval (IR) systems is significantly impeded by the high cost and laborious process associated with the manual labelling of training data. Similarly, synthetic training data generation, a potential workaround, often requires expensive computational resources due to the reliance on large language models. This work explored the potential of small language models for efficiently creating high-quality synthetic datasets to train neural retrieval models. We aim to identify an optimal method to generate synthetic datasets, enabling training neural reranking models in document collections where annotated data is unavailable. We introduce a novel methodology, grounded in the principles of information theory, to select the most appropriate documents to be used as context for question generation. Then, we employ a small language model for zero-shot conditional question generation, supplemented by a filtering mechanism to ensure the quality of generated questions. Extensive evaluation on five datasets unveils the potential of our approach, outperforming unsupervised retrieval methods such as BM25 and pretrained monoT5. Our findings indicate that an efficiently generated “silver-standard” dataset allows effective training of neural rerankers in unlabeled scenarios. To ensure reproducibility and facilitate wider application, we will release a code repository featuring an accessible API for zero-shot synthetic question generation.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.81.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Similarly, synthetic training data generation, a potential workaround, often requires expensive computational resources due to the reliance on large language models.\"\n\nThis abstract mentions a limitation of LLMs (high computational cost) but only briefly, and focuses on proposing a solution using small language models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Similarly, synthetic training data generation, a potential workaround, often requires expensive computational resources due to the reliance on large language models.\"\n\nThis abstract mentions a limitation of LLMs (high computational cost) but only briefly, and focuses on proposing a solution using small language models."
    },
    {
        "title": "Clarifying the Path to User Satisfaction: An Investigation into Clarification Usefulness",
        "authors": [
            "Hossein A. Rahmani",
            "Xi Wang",
            "Mohammad Aliannejadi",
            "Mohammadmehdi Naghiaei",
            "Emine Yilmaz"
        ],
        "published": "2024",
        "summary": "Clarifying questions are an integral component of modern information retrieval systems, directly impacting user satisfaction and overall system performance. Poorly formulated questions can lead to user frustration and confusion, negatively affecting the system’s performance. This research addresses the urgent need to identify and leverage key features that contribute to the classification of clarifying questions, enhancing user satisfaction. To gain deeper insights into how different features influence user satisfaction, we conduct a comprehensive analysis, considering a broad spectrum of lexical, semantic, and statistical features, such as question length and sentiment polarity. Our empirical results provide three main insights into the qualities of effective query clarification: (1) specific questions are more effective than generic ones; (2) the subjectivity and emotional tone of a question play a role; and (3) shorter and more ambiguous queries benefit significantly from clarification. Based on these insights, we implement feature-integrated user satisfaction prediction using various classifiers, both traditional and neural-based, including random forest, BERT, and large language models. Our experiments show a consistent and significant improvement, particularly in traditional classifiers, with a minimum performance boost of 45%. This study presents invaluable guidelines for refining the formulation of clarifying questions and enhancing both user satisfaction and system performance.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.84.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"including random forest, BERT, and large language models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"including random forest, BERT, and large language models.\""
    },
    {
        "title": "Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning",
        "authors": [
            "Lifu Tu",
            "Jin Qu",
            "Semih Yavuz",
            "Shafiq Joty",
            "Wenhao Liu",
            "Caiming Xiong",
            "Yingbo Zhou"
        ],
        "published": "2024",
        "summary": "Cross-lingual transfer of language models trained on high-resource languages like English has been widely studied for many NLP tasks, but focus on conversational tasks has been rather limited. This is partly due to the high cost of obtaining non-English conversational data, which results in limited coverage. In this work, we introduce for cross-lingual alignment pretraining, a parallel and large-scale multilingual conversation dataset that we created by translating the English-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into 105 other languages. XSGD contains about 330k utterances per language. To facilitate aligned cross-lingual representations, we develop an efficient prompt-tuning-based method for learning alignment prompts. We also investigate two different classifiers: NLI-based and vanilla classifiers, and test cross-lingual capability enabled by the aligned prompts. We evaluate our model’s cross-lingual generalization capabilities on two conversation tasks: slot-filling and intent classification. Our results demonstrate strong and efficient modeling ability of NLI-based classifiers and the large cross-lingual transfer improvements achieved by our aligned prompts, particularly in few-shot settings. We also conduct studies on large language models (LLMs) such as text-davinci-003 and ChatGPT in both zero- and few-shot settings. While LLMs exhibit impressive performance in English, their cross-lingual capabilities in other languages, particularly low-resource ones, are limited.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.85.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"While LLMs exhibit impressive performance in English, their cross-lingual capabilities in other languages, particularly low-resource ones, are limited.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"While LLMs exhibit impressive performance in English, their cross-lingual capabilities in other languages, particularly low-resource ones, are limited.\""
    },
    {
        "title": "Correcting Language Model Outputs by Editing Salient Layers",
        "authors": [
            "Kshitij Mishra",
            "Tamer Soliman",
            "Anil Ramakrishna",
            "Aram Galstyan",
            "Anoop Kumar"
        ],
        "published": "2024",
        "summary": "Large language models can accumulate incorrect or outdated knowledge as the real world evolves. Compared to typical solutions such as retraining, retrieval augmented generation, model editing offers an effective yet low cost solution to address this issue. However, existing model editing algorithms employ manual selection of edit layers, which requires prior domain knowledge or expensive architecture-specific empirical layer selection methods, such as causal tracing. In this work, we propose SaLEM (Salient Layers Editing Model), an efficient solution for data driven layer selection for the model editing task. Our solution utilizes layer-wise saliency maps for layer selection, and matches the accuracy of prior approaches but with only 1/3 of their edits, enabling efficient updates to the parametric knowledge in large language models.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.86.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Large language models can accumulate incorrect or outdated knowledge as the real world evolves.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Large language models can accumulate incorrect or outdated knowledge as the real world evolves.\""
    },
    {
        "title": "Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback",
        "authors": [
            "Nikhil Mehta",
            "Milagro Teruel",
            "Xin Deng",
            "Sergio Figueroa Sanz",
            "Ahmed Awadallah",
            "Julia Kiseleva"
        ],
        "published": "2024",
        "summary": "Many approaches to Natural Language Processing tasks often treat them as single-step problems, where an agent receives an instruction, executes it, and is evaluated based on the final outcome. However, language is inherently interactive, as evidenced by the back-and-forth nature of human conversations. In light of this, we posit that human-AI collaboration should also be interactive, with humans monitoring the work of AI agents and providing feedback that the agent can understand and utilize. Further, the AI agent should be able to detect when it needs additional information and proactively ask for help. Enabling this scenario would lead to more natural, efficient, and engaging human-AI collaboration.In this paper, we investigate these directions using the challenging task established by the IGLU competition, an interactive grounded language understanding task in a MineCraft-like world. We delve into multiple types of help players can give to the AI to guide it and analyze the impact of this help on behavior, resulting in performance improvements and an end-to-end interactive system.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.87.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Syllable-level lyrics generation from melody exploiting character-level language model",
        "authors": [
            "Zhe Zhang",
            "Karol Lasocki",
            "Yi Yu",
            "Atsuhiro Takasu"
        ],
        "published": "2024",
        "summary": "The generation of lyrics tightly connected to accompanying melodies involves establishing a mapping between musical notes and syllables of lyrics. This process requires a deep understanding of music constraints and semantic patterns at syllable-level, word-level, and sentence-level semantic meanings. However, pre-trained language models specifically designed at the syllable level are publicly unavailable. To solve these challenging issues, we propose to exploit fine-tuning character-level language models for syllable-level lyrics generation from symbolic melody. In particular, our method aims to fine-tune a character-level pre-trained language model, allowing to incorporation of linguistic knowledge of the language model into the beam search process of a syllable-level Transformer generator network. Besides, by exploring ChatGPT-based evaluation of generated lyrics in addition to human subjective evaluation, we prove that our approach improves the coherence and correctness of generated lyrics, without the need to train expensive new language models.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.89.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, pre-trained language models specifically designed at the syllable level are publicly unavailable.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, pre-trained language models specifically designed at the syllable level are publicly unavailable.\""
    },
    {
        "title": "Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca",
        "authors": [
            "Pinzhen Chen",
            "Shaoxiong Ji",
            "Nikolay Bogoychev",
            "Andrey Kutuzov",
            "Barry Haddow",
            "Kenneth Heafield"
        ],
        "published": "2024",
        "summary": "Foundational large language models (LLMs) can be instruction-tuned to perform open-domain question answering, facilitating applications like chat assistants. While such efforts are often carried out in a single language, we empirically analyze cost-efficient strategies for multilingual scenarios. Our study employs the Alpaca dataset and machine translations of it to form multilingual data, which is then used to tune LLMs through either low-rank adaptation or full-parameter training. Under a controlled computation budget, comparisons show that multilingual tuning is on par or better than tuning a model for each language. Furthermore, multilingual tuning with downsampled data can be as powerful and more robust. Our findings serve as a guide for expanding language support through instruction tuning.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.90.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper discusses the effectiveness of different instruction-tuning strategies for LLMs in multilingual scenarios but does not mention any limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper discusses the effectiveness of different instruction-tuning strategies for LLMs in multilingual scenarios but does not mention any limitations of LLMs."
    },
    {
        "title": "Prompt Perturbation Consistency Learning for Robust Language Models",
        "authors": [
            "Yao Qiang",
            "Subhrangshu Nandi",
            "Ninareh Mehrabi",
            "Greg Ver Steeg",
            "Anoop Kumar",
            "Anna Rumshisky",
            "Aram Galstyan"
        ],
        "published": "2024",
        "summary": "Large language models (LLMs) have demonstrated impressive performance on a number of natural language processing tasks, such as question answering and text summarization. However, their performance on sequence labeling tasks such as intent classification and slot filling (IC-SF), which is a central component in personal assistant systems, lags significantly behind discriminative models. Furthermore, there is a lack of substantive research on robustness of LLMs to various perturbations in the input prompts. The contributions of this paper are three-fold. First, we show that fine-tuning sufficiently large LLMs can produce IC-SF performance comparable to discriminative models. Next, we systematically analyze the performance deterioration of those fine-tuned models due to three distinct yet relevant types of input perturbations - oronyms, synonyms, and paraphrasing. Finally, we propose an efficient mitigation approach, Prompt Perturbation Consistency Learning (PPCL), which works by regularizing the divergence between losses from clean and perturbed samples. Our experiments show that PPCL can recover on an average 59% and 69% of the performance drop for IC and SF tasks, respectively. Furthermore, PPCL beats data augmentation approach while using ten times fewer augmented data samples.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.91.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their performance on sequence labeling tasks such as intent classification and slot filling (IC-SF), which is a central component in personal assistant systems, lags significantly behind discriminative models. Furthermore, there is a lack of substantive research on robustness of LLMs to various perturbations in the input prompts.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"However, their performance on sequence labeling tasks such as intent classification and slot filling (IC-SF), which is a central component in personal assistant systems, lags significantly behind discriminative models. Furthermore, there is a lack of substantive research on robustness of LLMs to various perturbations in the input prompts.\""
    },
    {
        "title": "Enhancing Society-Undermining Disinformation Detection through Fine-Grained Sentiment Analysis Pre-Finetuning",
        "authors": [
            "Tsung-Hsuan Pan",
            "Chung-Chi Chen",
            "Hen-Hsen Huang",
            "Hsin-Hsi Chen"
        ],
        "published": "2024",
        "summary": "In the era of the digital world, while freedom of speech has been flourishing, it has also paved the way for disinformation, causing detrimental effects on society. Legal and ethical criteria are insufficient to address this concern, thus necessitating technological intervention. This paper presents a novel method leveraging pre-finetuning concept for efficient detection and removal of disinformation that may undermine society, as deemed by judicial entities. We argue the importance of detecting this type of disinformation and validate our approach with real-world data derived from court orders. Following a study that highlighted four areas of interest for rumor analysis, our research proposes the integration of a fine-grained sentiment analysis task in the pre-finetuning phase of language models, using the GoEmotions dataset. Our experiments validate the effectiveness of our approach in enhancing performance significantly. Furthermore, we explore the application of our approach across different languages using multilingual language models, showing promising results. To our knowledge, this is the first study that investigates the role of sentiment analysis pre-finetuning in disinformation detection.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.92.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "No explicit limitation of LLMs is mentioned, but the abstract mentions the need for technological intervention to address disinformation, implying that current LLMs may not be sufficient to handle this issue.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: No explicit limitation of LLMs is mentioned, but the abstract mentions the need for technological intervention to address disinformation, implying that current LLMs may not be sufficient to handle this issue."
    },
    {
        "title": "Minimal Distillation Schedule for Extreme Language Model Compression",
        "authors": [
            "Chen Zhang",
            "Yang Yang",
            "Qifan Wang",
            "Jiahao Liu",
            "Jingang Wang",
            "Wei Wu",
            "Dawei Song"
        ],
        "published": "2024",
        "summary": "Recent studies have revealed that language model distillation can become less effective when there is a significant capacity gap between the teacher and the student models. In order to bridge the gap, teacher assistant-based distillation has been introduced, in which the selection of the teacher assistant plays a crucial role in transferring knowledge from the teacher to the student. However, existing approaches for teacher assistant-based distillation require numerous trials to find the optimal teacher assistant.In this paper, we propose a novel approach called Minimal Distillation Schedule (MiniDisc), which enables the scheduling of an optimal teacher assistant in just one trial for extreme model compression (e.g, to 5% scale). In particular, we empirically show that the performance of the student is positively correlated with the scale-performance tradeoff of the teacher assistant. We then introduce a new 𝜆-tradeoff metric that quantifies the optimality of the teacher assistant without the need for trial distillation to the student. By employing a sandwich framework, MiniDisc can select the optimal teacher assistant with the best 𝜆-tradeoff.We extensively evaluate MiniDisc through a series of experiments on the GLUE benchmark. The results demonstrate that our approach achieved an improved efficiency compared to various state-of-the-art baselines. Furthermore, we showcase the scalability of MiniDisc by applying it to a language model with billions of parameters.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.93.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Recent studies have revealed that language model distillation can become less effective when there is a significant capacity gap between the teacher and the student models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Recent studies have revealed that language model distillation can become less effective when there is a significant capacity gap between the teacher and the student models.\""
    },
    {
        "title": "High-quality Data-to-Text Generation for Severely Under-Resourced Languages with Out-of-the-box Large Language Models",
        "authors": [
            "Michela Lorandi",
            "Anya Belz"
        ],
        "published": "2024",
        "summary": "The performance of NLP methods for severely under-resourced languages cannot currently hope to match the state of the art in NLP methods for well resourced languages. We explore the extent to which pretrained large language models (LLMs) can bridge this gap, via the example of data-to-text generation for Irish, Welsh, Breton and Maltese. We test LLMs on these under-resourced languages and English, in a range of scenarios. We find that LLMs easily set the state of the art for the under-resourced languages by substantial margins, as measured by both automatic and human evaluations. For all our languages, human evaluation shows on-a-par performance with humans for our best systems, but BLEU scores collapse compared to English, casting doubt on the metric’s suitability for evaluating non-task-specific systems. Overall, our results demonstrate the great potential of LLMs to bridge the performance gap for under-resourced languages.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.98.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"For all our languages, human evaluation shows on-a-par performance with humans for our best systems, but BLEU scores collapse compared to English, casting doubt on the metric’s suitability for evaluating non-task-specific systems.\"\n\nThis rating is given because the paper mentions a limitation of LLMs, but it is a minor detail and not the primary focus of the abstract. The limitation mentioned is",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"For all our languages, human evaluation shows on-a-par performance with humans for our best systems, but BLEU scores collapse compared to English, casting doubt on the metric’s suitability for evaluating non-task-specific systems.\"\n\nThis rating is given because the paper mentions a limitation of LLMs, but it is a minor detail and not the primary focus of the abstract. The limitation mentioned is"
    },
    {
        "title": "Predicting Machine Translation Performance on Low-Resource Languages: The Role of Domain Similarity",
        "authors": [
            "Eric Khiu",
            "Hasti Toossi",
            "David Anugraha",
            "Jinyu Liu",
            "Jiaxu Li",
            "Juan Flores",
            "Leandro Roman",
            "A. Seza Doğruöz",
            "En-Shiun Lee"
        ],
        "published": "2024",
        "summary": "Fine-tuning and testing a multilingual large language model is a challenge for low-resource languages (LRLs) since it is an expensive process. While previous studies have predicted the performance of natural language processing (NLP) tasks using machine learning methods, they primarily focus on high-resource languages, overlooking LRLs and shifts across domains. Focusing on LRLs, we investigate three factors (the size of the fine-tuning corpus, domain similarity between fine-tuning and testing corpora, and language similarity between source and target languages), which can potentially impact the model performance by using classical regression models. Our results indicate that domain similarity has the most important impact on predicting the performance of Machine Translation models.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.100.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Fine-tuning and testing a multilingual large language model is a challenge for low-resource languages (LRLs) since it is an expensive process.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Fine-tuning and testing a multilingual large language model is a challenge for low-resource languages (LRLs) since it is an expensive process.\""
    },
    {
        "title": "Does CLIP Bind Concepts? Probing Compositionality in Large Image Models",
        "authors": [
            "Martha Lewis",
            "Nihal Nayak",
            "Peilin Yu",
            "Jack Merullo",
            "Qinan Yu",
            "Stephen Bach",
            "Ellie Pavlick"
        ],
        "published": "2024",
        "summary": "Large-scale neural network models combining text and images have made incredible progress in recent years. However, it remains an open question to what extent such models encode compositional representations of the concepts over which they operate, such as correctly identifying ‘red cube’ by reasoning over the constituents ‘red’ and ‘cube’. In this work, we focus on the ability of a large pretrained vision and language model (CLIP) to encode compositional concepts and to bind variables in a structure-sensitive way (e.g., differentiating ‘cube behind sphere’ from ‘sphere behind cube’). To inspect the performance of CLIP, we compare several architectures from research on compositional distributional semantics models (CDSMs), a line of research that attempts to implement traditional compositional linguistic structures within embedding spaces. We benchmark them on three synthetic datasets – single-object, two-object, and relational – designed to test concept binding. We find that CLIP can compose concepts in a single-object setting, but in situations where concept binding is needed, performance drops dramatically. At the same time, CDSMs also perform poorly, with best performance at chance level.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.101.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that CLIP can compose concepts in a single-object setting, but in situations where concept binding is needed, performance drops dramatically.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We find that CLIP can compose concepts in a single-object setting, but in situations where concept binding is needed, performance drops dramatically.\""
    },
    {
        "title": "Code-Switching and Back-Transliteration Using a Bilingual Model",
        "authors": [
            "Daniel Weisberg Mitelman",
            "Nachum Dershowitz",
            "Kfir Bar"
        ],
        "published": "2024",
        "summary": "The challenges of automated transliteration and code-switching–detection in Judeo-Arabic texts are addressed. We introduce two novel machine-learning models, one focused on transliterating Judeo-Arabic into Arabic, and another aimed at identifying non-Arabic words, predominantly Hebrew and Aramaic. Unlike prior work, our models are based on a bilingual Arabic-Hebrew language model, providing a unique advantage in capturing shared linguistic nuances. Evaluation results show that our models outperform prior solutions for the same tasks. As a practical contribution, we present a comprehensive pipeline capable of taking Judeo-Arabic text, identifying non-Arabic words, and then transliterating the Arabic portions into Arabic script. This work not only advances the state of the art but also offers a valuable toolset for making Judeo-Arabic texts more accessible to a broader Arabic-speaking audience.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.102.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Reading Between the Tweets: Deciphering Ideological Stances of Interconnected Mixed-Ideology Communities",
        "authors": [
            "Zihao He",
            "Ashwin Rao",
            "Siyi Guo",
            "Negar Mokhberian",
            "Kristina Lerman"
        ],
        "published": "2024",
        "summary": "Recent advances in NLP have improved our ability to understand the nuanced worldviews of online communities. Existing research focused on probing ideological stances treats liberals and conservatives as separate groups. However, this fails to account for the nuanced views of the organically formed online communities and the connections between them. In this paper, we study discussions of the 2020 U.S. election on Twitter to identify complex interacting communities. Capitalizing on this interconnectedness, we introduce a novel approach that harnesses message passing when finetuning language models (LMs) to probe the nuanced ideologies of these communities. By comparing the responses generated by LMs and real-world survey results, our method shows higher alignment than existing baselines, highlighting the potential of using LMs in revealing complex ideologies within and across interconnected mixed-ideology communities.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.104.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, the paper mentions LMs but does not discuss any limitations of the models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, the paper mentions LMs but does not discuss any limitations of the models."
    },
    {
        "title": "Unified Embeddings for Multimodal Retrieval via Frozen LLMs",
        "authors": [
            "Ziyang Wang",
            "Heba Elfardy",
            "Markus Dreyer",
            "Kevin Small",
            "Mohit Bansal"
        ],
        "published": "2024",
        "summary": "In this work, We present Unified Embeddings for Multimodal Retrieval (UniMuR), a simple but effective approach that embeds multimodal inputs and retrieves visual and textual outputs via frozen Large Language Models (LLMs). Specifically, UniMuR jointly retrieves multimodal outputs via a unified multimodal embedding and applies dual alignment training to account for both visual and textual semantics. Thus, unlike previous approaches, UniMuR significantly reduces LLM’s modality bias towards generating text-only outputs. Meanwhile, the proposed unified multimodal embedding mitigates the inconsistency between visual and textual outputs and provides coherent multimodal outputs. Furthermore, benefiting from the joint training of visual and textual semantics, UniMuR also achieves strong image/text retrieval ability. Compared to existing approaches, UniMuR achieves better zero-shot multimodal response retrieval performance on MMDialog, improving the overall R@1 by 6.5% while boosting the image retrieval rate and having better cross-modal consistency on multimodal outputs. UniMuR also achieves 2.4% and 3.9% improvement on context-based image retrieval tasks on MMDialog and VisDial respectively when compared to previous approaches, validating its generalization ability across multiple tasks.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.105.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"unlike previous approaches, UniMuR significantly reduces LLM’s modality bias towards generating text-only outputs.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"unlike previous approaches, UniMuR significantly reduces LLM’s modality bias towards generating text-only outputs.\""
    },
    {
        "title": "Assessing the Portability of Parameter Matrices Trained by Parameter-Efficient Finetuning Methods",
        "authors": [
            "Mohammed Sabry",
            "Anya Belz"
        ],
        "published": "2024",
        "summary": "As the cost of training ever larger language models has grown, so has the interest in reusing previously learnt knowledge. Transfer learning methods have shown how reusing non-task-specific knowledge can help in subsequent task-specific learning.In this paper, we investigate the inverse: porting whole functional modules that encode task-specific knowledge from one model to another. We designed a study comprising 1,440 training/testing runs to test the portability of modules trained by parameter-efficient finetuning (PEFT) techniques, using sentiment analysis as an example task. We test portability in a wide range of scenarios, involving different PEFT techniques and different pretrained host models, among other dimensions. We compare the performance of ported modules with that of equivalent modules trained (i) from scratch, and (ii) from parameters sampled from the same distribution as the ported module.We find that the ported modules far outperform the two alternatives tested, but that there are interesting differences between the four PEFT techniques tested.We conclude that task-specific knowledge in the form of structurally modular sets of parameters as produced by PEFT techniques is highly portable, but that degree of success depends on type of PEFT and on differences between originating and receiving pretrained models.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.106.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We conclude that task-specific knowledge in the form of structurally modular sets of parameters as produced by PEFT techniques is highly portable, but that degree of success depends on type of PEFT and on differences between originating and receiving pretrained models.\"\n\nThis rating is given because the abstract mentions a limitation of Large Language Models in passing, specifically that the degree of success in porting modules depends on",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We conclude that task-specific knowledge in the form of structurally modular sets of parameters as produced by PEFT techniques is highly portable, but that degree of success depends on type of PEFT and on differences between originating and receiving pretrained models.\"\n\nThis rating is given because the abstract mentions a limitation of Large Language Models in passing, specifically that the degree of success in porting modules depends on"
    },
    {
        "title": "GrounDial: Human-norm Grounded Safe Dialog Response Generation",
        "authors": [
            "Siwon Kim",
            "Shuyang Dai",
            "Mohammad Kachuee",
            "Shayan Ray",
            "Tara Taghavi",
            "Sungroh Yoon"
        ],
        "published": "2024",
        "summary": "Current conversational AI systems based on large language models (LLMs) are known to generate unsafe responses agreeing to offensive user input or including toxic content. Previous research aimed to alleviate the toxicity by fine-tuning LLM with manually annotated safe dialogue histories. However, the dependency on additional tuning requires substantial costs. To remove the dependency, we propose GrounDial, where response safety is achieved by grounding responses to commonsense social rules without requiring fine-tuning. A hybrid approach of in-context learning and human-norm-guided decoding of GrounDial enables the response to be quantitatively and qualitatively safer even without additional data or tuning.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.109.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Current conversational AI systems based on large language models (LLMs) are known to generate unsafe responses agreeing to offensive user input or including toxic content.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Current conversational AI systems based on large language models (LLMs) are known to generate unsafe responses agreeing to offensive user input or including toxic content.\""
    },
    {
        "title": "Low-Resource Counterspeech Generation for Indic Languages: The Case of Bengali and Hindi",
        "authors": [
            "Mithun Das",
            "Saurabh Pandey",
            "Shivansh Sethi",
            "Punyajoy Saha",
            "Animesh Mukherjee"
        ],
        "published": "2024",
        "summary": "With the rise of online abuse, the NLP community has begun investigating the use of neural architectures to generate counterspeech that can “counter” the vicious tone of such abusive speech and dilute/ameliorate their rippling effect over the social network. However, most of the efforts so far have been primarily focused on English. To bridge the gap for low-resource languages such as Bengali and Hindi, we create a benchmark dataset of 5,062 abusive speech/counterspeech pairs, of which 2,460 pairs are in Bengali, and 2,602 pairs are in Hindi. We implement several baseline models considering various interlingual transfer mechanisms with different configurations to generate suitable counterspeech to set up an effective benchmark. We observe that the monolingual setup yields the best performance. Further, using synthetic transfer, language models can generate counterspeech to some extent; specifically, we notice that transferability is better when languages belong to the same language family.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.111.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Further, using synthetic transfer, language models can generate counterspeech to some extent; specifically, we notice that transferability is better when languages belong to the same language family.\"\n\nThis rating is given because the paper mentions a limitation of language models in generating counterspeech for low-resource languages, but it is not the primary focus of the paper and is only briefly mentioned.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Further, using synthetic transfer, language models can generate counterspeech to some extent; specifically, we notice that transferability is better when languages belong to the same language family.\"\n\nThis rating is given because the paper mentions a limitation of language models in generating counterspeech for low-resource languages, but it is not the primary focus of the paper and is only briefly mentioned."
    },
    {
        "title": "Teaching Probabilistic Logical Reasoning to Transformers",
        "authors": [
            "Aliakbar Nafar",
            "K. Brent Venable",
            "Parisa Kordjamshidi"
        ],
        "published": "2024",
        "summary": "In this paper, we evaluate the capability of transformer-based language models in making inferences over uncertain text that includes uncertain rules of reasoning. We cover both Pre-trained Language Models (PLMs) and generative Large Language Models (LLMs). Our evaluation results show that both generations of language models struggle with reasoning over uncertain text. We propose a novel end-to-end fine-tuning approach, Probabilistic Constraint Training (PCT), that utilizes probabilistic logical rules as constraints in the fine-tuning phase without relying on these rules in the inference stage. To assess the effectiveness of PCT, we utilize the related corpora and, additionally, create a new and more challenging benchmark that, unlike the previous ones, uses instance-specific rules. Our study demonstrates that PCT improves the transformer-based language model’s intrinsic reasoning and makes their probabilistic logical reasoning process more explicit and explainable. Furthermore, PCT equips these models to effectively handle novel situations, including higher reasoning depth, new domains, and complex probabilistic structures.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.112.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our evaluation results show that both generations of language models struggle with reasoning over uncertain text.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Our evaluation results show that both generations of language models struggle with reasoning over uncertain text.\""
    },
    {
        "title": "SENSE-LM : A Synergy between a Language Model and Sensorimotor Representations for Auditory and Olfactory Information Extraction",
        "authors": [
            "Cédric Boscher",
            "Christine Largeron",
            "Véronique Eglin",
            "Elöd Egyed-Zsigmond"
        ],
        "published": "2024",
        "summary": "The five human senses – vision, taste, smell, hearing, and touch – are key concepts that shape human perception of the world. The extraction of sensory references (i.e., expressions that evoke the presence of a sensory experience) in textual corpus is a challenge of high interest, with many applications in various areas. In this paper, we propose SENSE-LM, an information extraction system tailored for the discovery of sensory references in large collections of textual documents. Based on the novel idea of combining the strength of large language models and linguistic resources such as sensorimotor norms, it addresses the task of sensory information extraction at a coarse-grained (sentence binary classification) and fine-grained (sensory term extraction) level.Our evaluation of SENSE-LM for two sensory functions, Olfaction and Audition, and comparison with state-of-the-art methods emphasize a significant leap forward in automating these complex tasks.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.119.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions \"large language models\" as a component of the proposed system, implying their use without discussing any limitations.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions \"large language models\" as a component of the proposed system, implying their use without discussing any limitations."
    },
    {
        "title": "In-Contextual Gender Bias Suppression for Large Language Models",
        "authors": [
            "Daisuke Oba",
            "Masahiro Kaneko",
            "Danushka Bollegala"
        ],
        "published": "2024",
        "summary": "Despite their impressive performance in a wide range of NLP tasks, Large Language Models (LLMs) have been reported to encode worrying-levels of gender biases. Prior work has proposed debiasing methods that require human labelled examples, data augmentation and fine-tuning of LLMs, which are computationally costly. Moreover, one might not even have access to the model parameters for performing debiasing such as in the case of closed LLMs such as GPT-4. To address this challenge, we propose bias suppression that prevents biased generations of LLMs by simply providing textual preambles constructed from manually designed templates and real-world statistics, without accessing to model parameters. We show that, using CrowsPairs dataset, our textual preambles covering counterfactual statements can suppress gender biases in English LLMs such as LLaMA2. Moreover, we find that gender-neutral descriptions of gender-biased objects can also suppress their gender biases. Moreover, we show that bias suppression has acceptable adverse effect on downstream task performance with HellaSwag and COPA.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.121.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Despite their impressive performance in a wide range of NLP tasks, Large Language Models (LLMs) have been reported to encode worrying-levels of gender biases.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Despite their impressive performance in a wide range of NLP tasks, Large Language Models (LLMs) have been reported to encode worrying-levels of gender biases.\""
    },
    {
        "title": "Parameter-Efficient Fine-Tuning: Is There An Optimal Subset of Parameters to Tune?",
        "authors": [
            "Max Ploner",
            "Alan Akbik"
        ],
        "published": "2024",
        "summary": "The ever-growing size of pretrained language models (PLM) presents a significant challenge for efficiently fine-tuning and deploying these models for diverse sets of tasks within memory-constrained environments.In light of this, recent research has illuminated the possibility of selectively updating only a small subset of a model’s parameters during the fine-tuning process.Since no new parameters or modules are added, these methods retain the inference speed of the original model and come at no additional computational cost. However, an open question pertains to which subset of parameters should best be tuned to maximize task performance and generalizability. To investigate, this paper presents comprehensive experiments covering a large spectrum of subset selection strategies. We comparatively evaluate their impact on model performance as well as the resulting model’s capability to generalize to different tasks.Surprisingly, we find that the gains achieved in performance by elaborate selection strategies are, at best, marginal when compared to the outcomes obtained by tuning a random selection of parameter subsets. Our experiments also indicate that selection-based tuning impairs generalizability to new tasks.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.122.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The ever-growing size of pretrained language models (PLM) presents a significant challenge for efficiently fine-tuning and deploying these models for diverse sets of tasks within memory-constrained environments.\"; \"Our experiments also indicate that selection-based tuning impairs generalizability to new tasks.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"The ever-growing size of pretrained language models (PLM) presents a significant challenge for efficiently fine-tuning and deploying these models for diverse sets of tasks within memory-constrained environments.\"; \"Our experiments also indicate that selection-based tuning impairs generalizability to new tasks.\""
    },
    {
        "title": "Contextualized Topic Coherence Metrics",
        "authors": [
            "Hamed Rahimi",
            "David Mimno",
            "Jacob Hoover",
            "Hubert Naacke",
            "Camelia Constantin",
            "Bernd Amann"
        ],
        "published": "2024",
        "summary": "This article proposes a new family of LLM-based topic coherence metrics called Contextualized Topic Coherence (CTC) and inspired by standard human topic evaluation methods. CTC metrics simulate human-centered coherence evaluation while maintaining the efficiency of other automated methods. We compare the performance of our CTC metrics and five other baseline metrics on seven topic models and show that CTC metrics better reflect human judgment, particularly for topics extracted from short text collections by avoiding highly scored topics that are meaningless to humans.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.123.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"avoiding highly scored topics that are meaningless to humans.\"\n\nThis paper discusses LLMs and mentions a limitation in passing, but it is not the primary focus of the abstract.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"avoiding highly scored topics that are meaningless to humans.\"\n\nThis paper discusses LLMs and mentions a limitation in passing, but it is not the primary focus of the abstract."
    },
    {
        "title": "ProMISe: A Proactive Multi-turn Dialogue Dataset for Information-seeking Intent Resolution",
        "authors": [
            "Yash Butala",
            "Siddhant Garg",
            "Pratyay Banerjee",
            "Amita Misra"
        ],
        "published": "2024",
        "summary": "Users of AI-based virtual assistants and search systems encounter challenges in articulating their intents while seeking information on unfamiliar topics, possibly due to complexity of the user’s intent or the lack of meta-information on the topic. We posit that an iterative suggested question-answering (SQA) conversation can improve the trade-off between the satisfaction of the user’s intent while keeping the information exchange natural and cognitive load of the interaction minimal on the users. In this paper, we evaluate a novel setting ProMISe by means of a sequence of interactions between a user, having a predefined information-seeking intent, and an agent that generates a set of SQA pairs at each step to aid the user to get closer to their intent. We simulate this two-player setting to create a multi-turn conversational dataset of SQAs and user choices (1025 dialogues comprising 4453 turns and 17812 SQAs) using human-feedback, chain-of-thought prompting and web-retrieval augmented large language models. We evaluate the quality of the SQs in the dataset on attributes such as diversity, specificity, grounding, etc, and benchmark the performance of different language models for the task of replicating user behavior.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.124.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"using human-feedback, chain-of-thought prompting and web-retrieval augmented large language models.\"\n\nThis abstract mentions LLMs but does not discuss any limitations of LLMs. The mention of LLMs is brief and primarily used to describe the method used to create the dataset.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"using human-feedback, chain-of-thought prompting and web-retrieval augmented large language models.\"\n\nThis abstract mentions LLMs but does not discuss any limitations of LLMs. The mention of LLMs is brief and primarily used to describe the method used to create the dataset."
    },
    {
        "title": "Non-Exchangeable Conformal Language Generation with Nearest Neighbors",
        "authors": [
            "Dennis Ulmer",
            "Chrysoula Zerva",
            "Andre Martins"
        ],
        "published": "2024",
        "summary": "Quantifying uncertainty in automatically generated text is important for letting humans check potential hallucinations and making systems more reliable. Conformal prediction is an attractive framework to provide predictions imbued with statistical guarantees, however, its application to text generation is challenging since any i.i.d. assumptions are not realistic. In this paper, we bridge this gap by leveraging recent results on *non-exchangeable* conformal prediction, which still ensures bounds on coverage. The result, *non-exchangeable conformal nucleus sampling*, is a novel extension of the conformal prediction framework to generation based on nearest neighbors. Our method can be used post-hoc for an arbitrary model without extra training and supplies token-level, calibrated prediction sets equipped with statistical guarantees. Experiments in machine translation and language modeling show encouraging results in generation quality. By also producing tighter prediction sets with good coverage, we thus give a more theoretically principled way to perform sampling with conformal guarantees.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.129.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Quantifying uncertainty in automatically generated text is important for letting humans check potential hallucinations and making systems more reliable.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Quantifying uncertainty in automatically generated text is important for letting humans check potential hallucinations and making systems more reliable.\""
    },
    {
        "title": "Self-training Strategies for Sentiment Analysis: An Empirical Study",
        "authors": [
            "Haochen Liu",
            "Sai Rallabandi",
            "Yijing Wu",
            "Parag Dakle",
            "Preethi Raghavan"
        ],
        "published": "2024",
        "summary": "Sentiment analysis is a crucial task in natural language processing that involves identifying and extracting subjective sentiment from text. Self-training has recently emerged as an economical and efficient technique for developing sentiment analysis models by leveraging a small amount of labeled data and a large amount of unlabeled data. However, given a set of training data, how to utilize them to conduct self-training makes a significant difference in the final performance of the model. We refer to this methodology as the self-training strategy. In this paper, we present an empirical study of various self-training strategies for sentiment analysis. First, we investigate the influence of the self-training strategy and hyper-parameters on the performance of traditional small language models (SLMs) in various few-shot settings. Second, we also explore the feasibility of leveraging large language models (LLMs) to help self-training. We propose and empirically compare several self-training strategies with the intervention of LLMs. Extensive experiments are conducted on three real-world sentiment analysis datasets.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.131.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We propose and empirically compare several self-training strategies with the intervention of LLMs.\"\n\nThis rating is based on the fact that the abstract mentions LLMs but does not discuss any limitations of LLMs. The mention of LLMs is brief and used to justify the exploration of self-training strategies with their intervention.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"We propose and empirically compare several self-training strategies with the intervention of LLMs.\"\n\nThis rating is based on the fact that the abstract mentions LLMs but does not discuss any limitations of LLMs. The mention of LLMs is brief and used to justify the exploration of self-training strategies with their intervention."
    },
    {
        "title": "Language is All a Graph Needs",
        "authors": [
            "Ruosong Ye",
            "Caiqi Zhang",
            "Runhui Wang",
            "Shuyuan Xu",
            "Yongfeng Zhang"
        ],
        "published": "2024",
        "summary": "The emergence of large-scale pre-trained language models has revolutionized various AI research domains. Transformers-based Large Language Models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with independent data like images, videos or texts, graphs usually contain rich structural and relational information. Meanwhile, languages, especially natural language, being one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph problems into the generative language modeling framework remains very limited. Considering the rising prominence of LLMs, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model) with highly scalable prompts based on natural language instructions. We use natural language to describe multi-scale geometric structure of the graph and then instruction finetune an LLM to perform graph tasks, which enables Generative Graph Learning. Our method surpasses all GNN baselines on ogbn-arxiv, Cora and PubMed datasets, underscoring its effectiveness and sheds light on generative LLMs as new foundation model for graph machine learning. Our code is available at https://github.com/agiresearch/InstructGLM.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.132.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, existing work on incorporating graph problems into the generative language modeling framework remains very limited.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, existing work on incorporating graph problems into the generative language modeling framework remains very limited.\""
    },
    {
        "title": "When do Generative Query and Document Expansions Fail? A Comprehensive Study Across Methods, Retrievers, and Datasets",
        "authors": [
            "Orion Weller",
            "Kyle Lo",
            "David Wadden",
            "Dawn Lawrie",
            "Benjamin Van Durme",
            "Arman Cohan",
            "Luca Soldaini"
        ],
        "published": "2024",
        "summary": "Using large language models (LMs) for query or document expansion can improve generalization in information retrieval. However, it is unknown whether these techniques are universally beneficial or only effective in specific settings, such as for particular retrieval models, dataset domains, or query types. To answer this, we conduct the first comprehensive analysis of LM-based expansion. We find that there exists a strong negative correlation between retriever performance and gains from expansion: expansion improves scores for weaker models, but generally harms stronger models. We show this trend holds across a set of eleven expansion techniques, twelve datasets with diverse distribution shifts, and twenty-four retrieval models. Through qualitative error analysis, we hypothesize that although expansions provide extra information (potentially improving recall), they add additional noise that makes it difficult to discern between the top relevant documents (thus introducing false positives). Our results suggest the following recipe: use expansions for weaker models or when the target dataset significantly differs from training corpus in format; otherwise, avoid expansions to keep the relevance signal clear.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.134.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We find that there exists a strong negative correlation between retriever performance and gains from expansion: expansion improves scores for weaker models, but generally harms stronger models... Through qualitative error analysis, we hypothesize that although expansions provide extra information (potentially improving recall), they add additional noise that makes it difficult to discern between the top relevant documents (thus introducing false positives).\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"We find that there exists a strong negative correlation between retriever performance and gains from expansion: expansion improves scores for weaker models, but generally harms stronger models... Through qualitative error analysis, we hypothesize that although expansions provide extra information (potentially improving recall), they add additional noise that makes it difficult to discern between the top relevant documents (thus introducing false positives).\""
    },
    {
        "title": "Can Large Language Models Understand Context?",
        "authors": [
            "Yilun Zhu",
            "Joel Ruben Antony Moniz",
            "Shruti Bhargava",
            "Jiarui Lu",
            "Dhivya Piraviperumal",
            "Site Li",
            "Yuan Zhang",
            "Hong Yu",
            "Bo-Hsiang Tseng"
        ],
        "published": "2024",
        "summary": "Understanding context is key to understanding human language, an ability which Large Language Models (LLMs) have been increasingly seen to demonstrate to an impressive extent. However, though the evaluation of LLMs encompasses various domains within the realm of Natural Language Processing, limited attention has been paid to probing their linguistic capability of understanding contextual features. This paper introduces a context understanding benchmark by adapting existing datasets to suit the evaluation of generative models. This benchmark comprises of four distinct tasks and nine datasets, all featuring prompts designed to assess the models’ ability to understand context. First, we evaluate the performance of LLMs under the in-context learning pretraining scenario. Experimental results indicate that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models. Second, as LLM compression holds growing significance in both research and real-world applications, we assess the context understanding of quantized models under in-context-learning settings. We find that 3-bit post-training quantization leads to varying degrees of performance reduction on our benchmark. We conduct an extensive analysis of these scenarios to substantiate our experimental results.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.135.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Experimental results indicate that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Experimental results indicate that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models.\""
    },
    {
        "title": "Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models",
        "authors": [
            "Younghun Lee",
            "Dan Goldwasser",
            "Laura Schwab Reese"
        ],
        "published": "2024",
        "summary": "Understanding the dynamics of counseling conversations is an important task, yet it is a challenging NLP problem regardless of the recent advance of Transformer-based pre-trained language models. This paper proposes a systematic approach to examine the efficacy of domain knowledge and large language models (LLMs) in better representing conversations between a crisis counselor and a help seeker. We empirically show that state-of-the-art language models such as Transformer-based models and GPT models fail to predict the conversation outcome. To provide richer context to conversations, we incorporate human-annotated domain knowledge and LLM-generated features; simple integration of domain knowledge and LLM features improves the model performance by approximately 15%. We argue that both domain knowledge and LLM-generated features can be exploited to better characterize counseling conversations when they are used as an additional context to conversations.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.137.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We empirically show that state-of-the-art language models such as Transformer-based models and GPT models fail to predict the conversation outcome.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"We empirically show that state-of-the-art language models such as Transformer-based models and GPT models fail to predict the conversation outcome.\""
    },
    {
        "title": "Testing the Depth of ChatGPT’s Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5’s Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking",
        "authors": [
            "David Bayani"
        ],
        "published": "2024",
        "summary": "In the months since its release, ChatGPT and its underlying model, GPT3.5, have garnered massive attention, due to their potent mix of capability and accessibility. While a niche industry of papers have emerged examining the scope of capabilities these models possess, language — whether natural or stylized like code — has been the vehicle to exchange information with the network. Drawing inspiration from the multi-modal knowledge we’d expect an agent with true understanding to possess, we examine GPT3.5’s aptitude for visual tasks, where the inputs feature ASCII-art without overt distillation into a lingual summary. In particular, we scrutinize its performance on carefully designed image recognition and generation tasks. An extended version of this write-up is available at: https://arxiv.org/abs/2307.16806 .",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.139.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"we examine GPT3.5’s aptitude for visual tasks, where the inputs feature ASCII-art without overt distillation into a lingual summary.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"we examine GPT3.5’s aptitude for visual tasks, where the inputs feature ASCII-art without overt distillation into a lingual summary.\""
    },
    {
        "title": "Cross-lingual Editing in Multilingual Language Models",
        "authors": [
            "Himanshu Beniwal",
            "Kowsik D",
            "Mayank Singh"
        ],
        "published": "2024",
        "summary": "The training of large language models (LLMs) necessitates substantial data and computational resources, and updating outdated LLMs entails significant efforts and resources. While numerous model editing techniques (METs) have emerged to efficiently update model outputs without retraining, their effectiveness in multilingual LLMs, where knowledge is stored in diverse languages, remains an underexplored research area. This research paper introduces the cross-lingual model editing (XME) paradigm, wherein a fact is edited in one language, and the subsequent update propagation is observed across other languages. To investigate the XME paradigm, we conducted experiments using BLOOM, mBERT, and XLM-RoBERTa using the two writing scripts: Latin (English, French, and Spanish) and Indic (Hindi, Gujarati, and Bengali). The results reveal notable performance limitations of state-of-the-art METs under the XME setting, mainly when the languages involved belong to two distinct script families. These findings highlight the need for further research and development of XME techniques to address these challenges. For more comprehensive information, the dataset used in this research and the associated code are publicly available at the following [URL](https://github.com/lingo-iitgn/XME).",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.140.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"The results reveal notable performance limitations of state-of-the-art METs under the XME setting, mainly when the languages involved belong to two distinct script families.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"The results reveal notable performance limitations of state-of-the-art METs under the XME setting, mainly when the languages involved belong to two distinct script families.\""
    },
    {
        "title": "Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large Language Models for Dynamic Inference",
        "authors": [
            "Parsa Kavehzadeh",
            "Mojtaba Valipour",
            "Marzieh Tahaei",
            "Ali Ghodsi",
            "Boxing Chen",
            "Mehdi Rezagholizadeh"
        ],
        "published": "2024",
        "summary": "Large language models (LLMs) have revolutionized natural language processing (NLP) by excelling at understanding and generating human-like text. However, their widespread deployment can be prohibitively expensive. SortedNet is a recent training technique for enabling dynamic inference by leveraging the modularity in networks and sorting sub-models based on computation/accuracy in a nested manner. We extend SortedNet to generative NLP tasks, making large language models dynamic without any Pre-Training and by only replacing Standard Fine-Tuning (SFT) with Sorted Fine-Tuning (SoFT). Our approach boosts model efficiency, eliminating the need for multiple models for various scenarios during inference. We show that this approach can unlock the potential of intermediate layers of transformers in generating the target output. Our sub-models remain integral components of the original model, minimizing storage requirements and transition costs between different computational/latency budgets. The efficacy of our proposed method was demonstrated by applying it to tune LLaMA 2 13B on the Stanford Alpaca dataset for instruction following and TriviaQA for closed-book question answering. Our results show the superior performance of sub-models in comparison to Standard Fine-Tuning and SFT+ICT (Early-Exit), all achieved with very efficient tuning and without additional memory usage during inference.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.141.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, their widespread deployment can be prohibitively expensive.\"\n\nThis paper mentions a limitation of LLMs (high deployment cost) but does not elaborate on it and focuses on the proposed solution to address this issue.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"However, their widespread deployment can be prohibitively expensive.\"\n\nThis paper mentions a limitation of LLMs (high deployment cost) but does not elaborate on it and focuses on the proposed solution to address this issue."
    },
    {
        "title": "Hierarchical and Dynamic Prompt Compression for Efficient Zero-shot API Usage",
        "authors": [
            "Yichen Jiang",
            "Marco Vecchio",
            "Mohit Bansal",
            "Anders Johannsen"
        ],
        "published": "2024",
        "summary": "No abstract available",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.143.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "Fine-tuning CLIP Text Encoders with Two-step Paraphrasing",
        "authors": [
            "Hyunjae Kim",
            "Seunghyun Yoon",
            "Trung Bui",
            "Handong Zhao",
            "Quan Tran",
            "Franck Dernoncourt",
            "Jaewoo Kang"
        ],
        "published": "2024",
        "summary": "Contrastive language-image pre-training (CLIP) models have demonstrated considerable success across various vision-language tasks, such as text-to-image retrieval, where the model is required to effectively process natural language input to produce an accurate visual output. However, current models still face limitations in dealing with linguistic variations in input queries, such as paraphrases, making it challenging to handle a broad range of user queries in real-world applications. In this study, we introduce a straightforward fine-tuning approach to enhance the representations of CLIP models for paraphrases. Our approach involves a two-step paraphrase generation process, where we automatically create two categories of paraphrases from web-scale image captions by leveraging large language models. Subsequently, we fine-tune the CLIP text encoder using these generated paraphrases while freezing the image encoder. Our resulting model, which we call ParaCLIP, exhibits significant improvements over baseline CLIP models across various tasks, including paraphrased retrieval (with rank similarity scores improved by up to 7.6% and 9.6%), Visual Genome Relation and Attribution, as well as seven semantic textual similarity tasks.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.144.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"However, current models still face limitations in dealing with linguistic variations in input queries, such as paraphrases, making it challenging to handle a broad range of user queries in real-world applications.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"However, current models still face limitations in dealing with linguistic variations in input queries, such as paraphrases, making it challenging to handle a broad range of user queries in real-world applications.\""
    },
    {
        "title": "Dive into the Chasm: Probing the Gap between In- and Cross-Topic Generalization",
        "authors": [
            "Andreas Waldis",
            "Yufang Hou",
            "Iryna Gurevych"
        ],
        "published": "2024",
        "summary": "Pre-trained language models (PLMs) perform well in In-Topic setups, where training and testing data come from the same topics. However, they face challenges in Cross-Topic scenarios where testing data is derived from distinct topics. This paper analyzes various PLMs with three probing-based experiments to better understand the reasons behind such generalization gaps. For the first time, we demonstrate that the extent of these generalization gaps and the sensitivity to token-level interventions vary significantly across PLMs. By evaluating large language models (LLMs), we show the usefulness of our analysis for these recent models. Overall, we observe diverse pre-training objectives and architectural regularization contribute to more robust PLMs and mitigate generalization gaps. Our research contributes to a deeper understanding and comparison of language models across different generalization scenarios.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.146.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "3",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Pre-trained language models (PLMs) perform well in In-Topic setups, where training and testing data come from the same topics. However, they face challenges in Cross-Topic scenarios where testing data is derived from distinct topics.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 3\nEvidence: \"Pre-trained language models (PLMs) perform well in In-Topic setups, where training and testing data come from the same topics. However, they face challenges in Cross-Topic scenarios where testing data is derived from distinct topics.\""
    },
    {
        "title": "LLM-GEm: Large Language Model-Guided Prediction of People’s Empathy Levels towards Newspaper Article",
        "authors": [
            "Md Rakibul Hasan",
            "Md Zakir Hossain",
            "Tom Gedeon",
            "Shafin Rahman"
        ],
        "published": "2024",
        "summary": "Empathy – encompassing the understanding and supporting others’ emotions and perspectives – strengthens various social interactions, including written communication in healthcare, education and journalism. Detecting empathy using AI models by relying on self-assessed ground truth through crowdsourcing is challenging due to the inherent noise in such annotations. To this end, we propose a novel system, named Large Language Model-Guided Empathy _(LLM-GEm)_ prediction system. It rectifies annotation errors based on our defined annotation selection threshold and makes the annotations reliable for conventional empathy prediction models, e.g., BERT-based pre-trained language models (PLMs). Previously, demographic information was often integrated numerically into empathy detection models. In contrast, our _LLM-GEm_ leverages GPT-3.5 LLM to convert numerical data into semantically meaningful textual sequences, enabling seamless integration into PLMs. We experiment with three _NewsEmpathy_ datasets involving people’s empathy levels towards newspaper articles and achieve state-of-the-art test performance using a RoBERTa-based PLM. Code and evaluations are publicly available at [https://github.com/hasan-rakibul/LLM-GEm](https://github.com/hasan-rakibul/LLM-GEm).",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.147.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "None, but the paper mentions \"inherent noise in such annotations\" which is not directly related to LLM limitations but to the challenging task of detecting empathy using AI models.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: None, but the paper mentions \"inherent noise in such annotations\" which is not directly related to LLM limitations but to the challenging task of detecting empathy using AI models."
    },
    {
        "title": "ICE-Score: Instructing Large Language Models to Evaluate Code",
        "authors": [
            "Terry Yue Zhuo"
        ],
        "published": "2024",
        "summary": "Recent advancements in the field of natural language generation have facilitated the use of large language models to assess the quality of generated text. Although these models have shown promising results in tasks such as machine translation and summarization, their applicability in code intelligence tasks remains limited without human involvement. The complexity of programming concepts required for such tasks makes it difficult to develop evaluation metrics that align with human judgment. Token-matching-based metrics, such as BLEU, have demonstrated weak correlations with human practitioners in code intelligence tasks. Moreover, utilizing human-written test suites to evaluate functional correctness can be challenging in domains with low resources. To overcome these obstacles, we propose ICE-Score, a new evaluation metric via instructing large language models (LLMs) for code assessments. Our metric addresses the limitations of existing approaches by achieving superior correlations with functional correctness and human preferences, without the need for test oracles or references. We evaluate the efficacy of our metric on two different aspects (human preference and execution success) and four programming languages. Our results demonstrate that our metric surpasses state-of-the-art metrics for code generation, delivering high levels of accuracy and consistency across various programming languages and tasks. We also make our evaluation metric and datasets available to the public, encouraging further research in evaluating code intelligence tasks.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.148.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Although these models have shown promising results in tasks such as machine translation and summarization, their applicability in code intelligence tasks remains limited without human involvement.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Although these models have shown promising results in tasks such as machine translation and summarization, their applicability in code intelligence tasks remains limited without human involvement.\""
    },
    {
        "title": "CReSE: Benchmark Data and Automatic Evaluation Framework for Recommending Eligibility Criteria from Clinical Trial Information",
        "authors": [
            "Siun Kim",
            "Jung-Hyun Won",
            "David Lee",
            "Renqian Luo",
            "Lijun Wu",
            "Tao Qin",
            "Howard Lee"
        ],
        "published": "2024",
        "summary": "Eligibility criteria (EC) refer to a set of conditions an individual must meet to participate in a clinical trial, defining the study population and minimizing potential risks to patients. Previous research in clinical trial design has been primarily focused on searching for similar trials and generating EC within manual instructions, employing similarity-based performance metrics, which may not fully reflect human judgment. In this study, we propose a novel task of recommending EC based on clinical trial information, including trial titles, and introduce an automatic evaluation framework to assess the clinical validity of the EC recommendation model. Our new approach, known as CReSE (Contrastive learning and Rephrasing-based and Clinical Relevance-preserving Sentence Embedding), represents EC through contrastive learning and rephrasing via large language models (LLMs). The CReSE model outperforms existing language models pre-trained on the biomedical domain in EC clustering. Additionally, we have curated a benchmark dataset comprising 3.2M high-quality EC-title pairs extracted from 270K clinical trials available on ClinicalTrials.gov. The EC recommendation models achieve commendable performance metrics, with 49.0% precision@1 and 44.2% MAP@5 on our evaluation framework. We expect that our evaluation framework built on the CReSE model will contribute significantly to the development and assessment of the EC recommendation models in terms of clinical validity.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.149.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "1",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"represents EC through contrastive learning and rephrasing via large language models (LLMs)\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 1\nEvidence: \"represents EC through contrastive learning and rephrasing via large language models (LLMs)\""
    },
    {
        "title": "BMX: Boosting Natural Language Generation Metrics with Explainability",
        "authors": [
            "Christoph Leiter",
            "Hoa Nguyen",
            "Steffen Eger"
        ],
        "published": "2024",
        "summary": "State-of-the-art natural language generation evaluation metrics are based on black-box language models. Hence, recent works consider their explainability with the goals of better understandability for humans and better metric analysis, including failure cases. In contrast, we explicitly leverage explanations to boost the metrics’ performance. In particular, we perceive feature importance explanations as word-level scores, which we convert, via power means, into a segment-level score. We then combine this segment-level score with the original metric to obtain a better metric. Our tests show improvements for multiple metrics across MT and summarization datasets. While improvements on machine translation are small, they are strong for summarization. Notably, BMX with the LIME explainer and preselected parameters achieves an average improvement of 0.087 points in Spearman correlation on the system-level evaluation of SummEval.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.150.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"State-of-the-art natural language generation evaluation metrics are based on black-box language models.\"\n\nNote that this abstract mentions a limitation of language models (being \"black-box\") but does not elaborate on it, and focuses on the proposed solution instead.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"State-of-the-art natural language generation evaluation metrics are based on black-box language models.\"\n\nNote that this abstract mentions a limitation of language models (being \"black-box\") but does not elaborate on it, and focuses on the proposed solution instead."
    },
    {
        "title": "Joint Inference of Retrieval and Generation for Passage Re-ranking",
        "authors": [
            "Wei Fang",
            "Yung-Sung Chuang",
            "James Glass"
        ],
        "published": "2024",
        "summary": "Passage retrieval is a crucial component of modern open-domain question answering (QA) systems, providing information for downstream QA components to generate accurate and transparent answers. In this study we focus on passage re-ranking, proposing a simple yet effective method, Joint Passage Re-ranking (JPR), that optimizes the mutual information between query and passage distributions, integrating both cross-encoders and generative models in the re-ranking process. Experimental results demonstrate that JPR outperforms conventional re-rankers and language model scorers in both open-domain QA retrieval settings and diverse retrieval benchmarks under zero-shot settings.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.151.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "0",
        "Talks about LLMs_Llama-3.1-70b": "No",
        "Evidence_Llama-3.1-70b": "No evidence of discussion of limitations of LLMs.",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: No\nRate Limitations of LLMs: 0\nEvidence: No evidence of discussion of limitations of LLMs."
    },
    {
        "title": "DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI",
        "authors": [
            "Jianguo Zhang",
            "Kun Qian",
            "Zhiwei Liu",
            "Shelby Heinecke",
            "Rui Meng",
            "Ye Liu",
            "Zhou Yu",
            "Huan Wang",
            "Silvio Savarese",
            "Caiming Xiong"
        ],
        "published": "2024",
        "summary": "Despite advancements in conversational AI, language models encounter challenges to handle diverse conversational tasks, and existing dialogue dataset collections often lack diversity and comprehensiveness. To tackle these issues, we introduce DialogStudio: the largest and most diverse collection of dialogue datasets, unified under a consistent format while preserving their original information. Our collection encompasses data from open-domain dialogues, task-oriented dialogues, natural language understanding, conversational recommendation, dialogue summarization, and knowledge-grounded dialogues, making it an incredibly rich and diverse resource for dialogue research and model training.To further enhance the utility of DialogStudio, we identify the licenses for each dataset, design external knowledge and domain-aware prompts for selected dialogues to facilitate instruction-aware fine-tuning. To improve transparency and support dataset and task-based research, as well as language model pre-training, all datasets, licenses, codes, and models associated with DialogStudio will be made publicly accessible.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.152.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"language models encounter challenges to handle diverse conversational tasks\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"language models encounter challenges to handle diverse conversational tasks\""
    },
    {
        "title": "The Queen of England is not England’s Queen: On the Lack of Factual Coherency in PLMs",
        "authors": [
            "Paul Youssef",
            "Jörg Schlötterer",
            "Christin Seifert"
        ],
        "published": "2024",
        "summary": "Factual knowledge encoded in Pre-trained Language Models (PLMs) enriches their representations and justifies their use as knowledge bases. Previous work has focused on probing PLMs for factual knowledge by measuring how often they can correctly predict an _object_ entity given a subject and a relation, and improving fact retrieval by optimizing the prompts used for querying PLMs. In this work, we consider a complementary aspect, namely the coherency of factual knowledge in PLMs, i.e., how often can PLMs predict the _subject_ entity given its initial prediction of the object entity. This goes beyond evaluating how much PLMs know, and focuses on the internal state of knowledge inside them. Our results indicate that PLMs have low coherency using manually written, optimized and paraphrased prompts, but including an evidence paragraph leads to substantial improvement. This shows that PLMs fail to model inverse relations and need further enhancements to be able to handle retrieving facts from their parameters in a coherent manner, and to be considered as knowledge bases.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.155.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Our results indicate that PLMs have low coherency using manually written, optimized and paraphrased prompts, but including an evidence paragraph leads to substantial improvement. This shows that PLMs fail to model inverse relations and need further enhancements to be able to handle retrieving facts from their parameters in a coherent manner, and to be considered as knowledge bases.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"Our results indicate that PLMs have low coherency using manually written, optimized and paraphrased prompts, but including an evidence paragraph leads to substantial improvement. This shows that PLMs fail to model inverse relations and need further enhancements to be able to handle retrieving facts from their parameters in a coherent manner, and to be considered as knowledge bases.\""
    },
    {
        "title": "HierarchyNet: Learning to Summarize Source Code with Heterogeneous Representations",
        "authors": [
            "Minh Huynh Nguyen",
            "Nghi D. Q. Bui",
            "Truong Son Hy",
            "Long Tran Thanh",
            "Tien N. Nguyen"
        ],
        "published": "2024",
        "summary": "Code representation is important to machine learning models in the code-related applications. Existing code summarization approaches primarily leverage Abstract Syntax Trees (ASTs) and sequential information from source code to generate code summaries while often overlooking the critical consideration of the interplay of dependencies among code elements and code hierarchy. However, effective summarization necessitates a holistic analysis of code snippets from three distinct aspects: lexical, syntactic, and semantic information. In this paper, we propose a novel code summarization approach utilizing Heterogeneous Code Representations (HCRs) and our specially designed HierarchyNet. HCRs adeptly capture essential code features at lexical, syntactic, and semantic levels within a hierarchical structure. HierarchyNet processes each layer of the HCR separately, employing a Heterogeneous Graph Transformer, a Tree-based CNN, and a Transformer Encoder. In addition, HierarchyNet demonstrates superior performance compared to fine-tuned pre-trained models, including CodeT5, and CodeBERT, as well as large language models that employ zero/few-shot settings, such as CodeLlama, StarCoder, and CodeGen. Implementation details can be found at https://github.com/FSoft-AI4Code/HierarchyNet.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.156.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"HierarchyNet demonstrates superior performance compared to fine-tuned pre-trained models, including... large language models that employ zero/few-shot settings, such as CodeLlama, StarCoder, and CodeGen.\"\n\nThis rating is based on the fact that the abstract mentions the limitations of large language models (CodeLlama, StarCoder, and CodeGen) in a specific task (code summar",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"HierarchyNet demonstrates superior performance compared to fine-tuned pre-trained models, including... large language models that employ zero/few-shot settings, such as CodeLlama, StarCoder, and CodeGen.\"\n\nThis rating is based on the fact that the abstract mentions the limitations of large language models (CodeLlama, StarCoder, and CodeGen) in a specific task (code summar"
    },
    {
        "title": "Understanding the effects of language-specific class imbalance in multilingual fine-tuning",
        "authors": [
            "Vincent Jung",
            "Lonneke Plas"
        ],
        "published": "2024",
        "summary": "We study the effect of one type of imbalance often present in real-life multilingual classification datasets: an uneven distribution of labels across languages. We show evidence that fine-tuning a transformer-based Large Language Model (LLM) on a dataset with this imbalance leads to worse performance, a more pronounced separation of languages in the latent space, and the promotion of uninformative features. We modify the traditional class weighing approach to imbalance by calculating class weights separately for each language and show that this helps mitigate those detrimental effects. These results create awareness of the negative effects of language-specific class imbalance in multilingual fine-tuning and the way in which the model learns to rely on the separation of languages to perform the task.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.157.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "4",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"We show evidence that fine-tuning a transformer-based Large Language Model (LLM) on a dataset with this imbalance leads to worse performance, a more pronounced separation of languages in the latent space, and the promotion of uninformative features.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 4\nEvidence: \"We show evidence that fine-tuning a transformer-based Large Language Model (LLM) on a dataset with this imbalance leads to worse performance, a more pronounced separation of languages in the latent space, and the promotion of uninformative features.\""
    },
    {
        "title": "NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries",
        "authors": [
            "Wei Zhao",
            "Zhitao Hou",
            "Siyuan Wu",
            "Yan Gao",
            "Haoyu Dong",
            "Yao Wan",
            "Hongyu Zhang",
            "Yulei Sui",
            "Haidong Zhang"
        ],
        "published": "2024",
        "summary": "Writing formulas on spreadsheets, such as Microsoft Excel and Google Sheets, is a widespread practice among users performing data analysis. However, crafting formulas on spreadsheets remains a tedious and error-prone task for many end-users, particularly when dealing with complex operations. To alleviate the burden associated with writing spreadsheet formulas, this paper introduces a novel benchmark task called NL2Formula, with the aim to generate executable formulas that are grounded on a spreadsheet table, given a Natural Language (NL) query as input. To accomplish this, we construct a comprehensive dataset consisting of 70,799 paired NL queries and corresponding spreadsheet formulas, covering 21,670 tables and 37 types of formula functions. We realize the NL2Formula task by providing a sequence-to-sequence baseline implementation called fCoder. Experimental results validate the effectiveness of fCoder, demonstrating its superior performance compared to the baseline models. Furthermore, we also compare fCoder with an initial GPT-3.5 model (i.e., text-davinci-003). Lastly, through in-depth error analysis, we identify potential challenges in the NL2Formula task and advocate for further investigation.",
        "pdf_link": "https://aclanthology.org/2024.findings-eacl.158.pdf",
        "source": "eacl2024",
        "Rate_Llama-3.1-70b": "2",
        "Talks about LLMs_Llama-3.1-70b": "Yes",
        "Evidence_Llama-3.1-70b": "\"Lastly, through in-depth error analysis, we identify potential challenges in the NL2Formula task and advocate for further investigation.\"",
        "Full_response_rate_Llama-3.1-70b": "Does it talk about LLMs: Yes\nRate Limitations of LLMs: 2\nEvidence: \"Lastly, through in-depth error analysis, we identify potential challenges in the NL2Formula task and advocate for further investigation.\""
    }
]